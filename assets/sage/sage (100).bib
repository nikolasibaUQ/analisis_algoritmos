@article{doi:10.2203/dose-response.09-030.Beam,
author = {Andrew L. Beam and Alison A. Motsinger-Reif},
title = {Optimization of Nonlinear Dose- and Concentration-Response Models Utilizing Evolutionary Computation},
journal = {Dose-Response},
volume = {9},
number = {3},
pages = {dose-response.09-030.Beam},
year = {2011a},
doi = {10.2203/dose-response.09-030.Beam},
note = {PMID:22013401},
URL = {https://doi-org.crai.referencistas.com/10.2203/dose-response.09-030.Beam},
eprint = {https://doi-org.crai.referencistas.com/10.2203/dose-response.09-030.Beam},
abstract = {An essential part of toxicity and chemical screening is assessing the concentrated related effects of a test article. Most often this concentration-response is a nonlinear, necessitating sophisticated regression methodologies. The parameters derived from curve fitting are essential in determining a test article’s potency (EC50) and efficacy (Emax) and variations in model fit may lead to different conclusions about an article’s performance and safety. Previous approaches have leveraged advanced statistical and mathematical techniques to implement nonlinear least squares (NLS) for obtaining the parameters defining such a curve. These approaches, while mathematically rigorous, suffer from initial value sensitivity, computational intensity, and rely on complex and intricate computational and numerical techniques. However if there is a known mathematical model that can reliably predict the data, then nonlinear regression may be equally viewed as parameter optimization. In this context, one may utilize proven techniques from machine learning, such as evolutionary algorithms, which are robust, powerful, and require far less computational framework to optimize the defining parameters. In the current study we present a new method that uses such techniques, Evolutionary Algorithm Dose Response Modeling (EADRM), and demonstrate its effectiveness compared to more conventional methods on both real and simulated data.}
}

@article{doi:10.1177/001440291007600403,
author = {Brian A. Bottge and Enrique Rueda and Timothy S. Grant and Ana C. Stephens and Perry T. Laroque},
title = {Anchoring Problem-Solving and Computation Instruction in Context-Rich Learning Environments},
journal = {Exceptional Children},
volume = {76},
number = {4},
pages = {417–437},
year = {2010b},
doi = {10.1177/001440291007600403},
URL = {https://doi-org.crai.referencistas.com/10.1177/001440291007600403},
eprint = {https://doi-org.crai.referencistas.com/10.1177/001440291007600403},
abstract = {Middle school students with learning disabilities in math (MLD) used two versions of Enhanced Anchored Instruction (EAI). In one condition, students learned how to compute with fractions on an as-needed basis while they worked to solve the EAI problems. In the other condition, teachers used a computer-based instructional module in place of one of the EAI problems to deliver formal fraction instruction. The results indicated that students in both instructional formats improved their fraction computational skills and that formal instruction provided an added benefit. Both instructional conditions improved students’ problem-solving skills by about the same amount. The findings suggest that combining formal fraction instruction with EAI is a viable way to improve the problem-solving and computational skills of students with MLD.}
}

@article{doi:10.1177/01655515211060530,
author = {Gustavo Candela and María-Dolores Sáez and Pilar Escobar and Manuel Marco-Such},
title = {A benchmark of Spanish language datasets for computationally driven research},
journal = {Journal of Information Science},
volume = {49},
number = {6},
pages = {1451–1461},
year = {2023c},
doi = {10.1177/01655515211060530},
URL = {https://doi-org.crai.referencistas.com/10.1177/01655515211060530},
eprint = {https://doi-org.crai.referencistas.com/10.1177/01655515211060530},
abstract = {In the domain of Galleries, Libraries, Archives and Museums (GLAM) institutions, creative and innovative tools and methodologies for content delivery and user engagement have recently gained international attention. New methods have been proposed to publish digital collections as datasets amenable to computational use. Standardised benchmarks can be useful to broaden the scope of machine-actionable collections and to promote cultural and linguistic diversity. In this article, we propose a methodology to select datasets for computationally driven research applied to Spanish text corpora. This work seeks to encourage Spanish and Latin American institutions to publish machine-actionable collections based on best practices and avoiding common mistakes.}
}

@article{doi:10.1177/1177932217712471,
author = {Larissa Catharina and Carlyle Ribeiro Lima and Alexander Franca and Ana Carolina Ramos Guimarães and Marcelo Alves-Ferreira and Pierre Tuffery and Philippe Derreumaux and Nicolas Carels},
title = {A Computational Methodology to Overcome the Challenges Associated With the Search for Specific Enzyme Targets to Develop Drugs Against Leishmania major},
journal = {Bioinformatics and Biology Insights},
volume = {11},
number = { },
pages = {1177932217712471},
year = {2017d},
doi = {10.1177/1177932217712471},
note = {PMID:28638238},
URL = {https://doi-org.crai.referencistas.com/10.1177/1177932217712471},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1177932217712471},
abstract = {We present an approach for detecting enzymes that are specific of Leishmania major compared with Homo sapiens and provide targets that may assist research in drug development. This approach is based on traditional techniques of sequence homology comparison by similarity search and Markov modeling; it integrates the characterization of enzymatic functionality, secondary and tertiary protein structures, protein domain architecture, and metabolic environment. From 67 enzymes represented by 42 enzymatic activities classified by AnEnPi (Analogous Enzymes Pipeline) as specific for L major compared with H sapiens, only 40 (23 Enzyme Commission [EC] numbers) could actually be considered as strictly specific of L major and 27 enzymes (19 EC numbers) were disregarded for having ambiguous homologies or analogies with H sapiens. Among the 40 strictly specific enzymes, we identified sterol 24-C-methyltransferase, pyruvate phosphate dikinase, trypanothione synthetase, and RNA-editing ligase as 4 essential enzymes for L major that may serve as targets for drug development.}
}

@article{doi:10.1177/00207020241276532,
author = {Xiujuan Chen and Ye Han and Zhiqiang Zhang},
title = {Chinese and American Think Tanks: Comparing International Social Media Communication Characteristics},
journal = {International Journal},
volume = {79},
number = {3},
pages = {344–368},
year = {2024e},
doi = {10.1177/00207020241276532},
URL = {https://doi-org.crai.referencistas.com/10.1177/00207020241276532},
eprint = {https://doi-org.crai.referencistas.com/10.1177/00207020241276532},
abstract = {Think tanks can leverage global influence to manifest the soft power of their home countries. International social media is a useful channel for Chinese think tanks to spread their messages abroad and enhance their influence. In this study, we examine the think tank index report released by the University of Pennsylvania’s Think Tanks and Civil Societies Program (TTCSP). Through comparative analysis and content analysis, this study analyzes the differences between Chinese and American think tanks’ usage of and presence on Twitter (now called X). Compared with American think tanks, Chinese think tanks have much room for improvement. We recommend that Chinese think tanks should: construct network infrastructure to support the development of international social media in China; increase their participation on diverse international social media platforms; actively participate in globalized issues and promote Chinese issues; cultivate professionals with the skills required to manage international social media accounts for think tanks; and incorporate metrics on social media into their think tank evaluation mechanisms.}
}

@article{doi:10.1111/poms.13971,
author = {Doug J. Chung and Kyoungwon Seo and Reo Song},
title = {Efficient computation of discrete games: Estimating the effect of Apple on market structure},
journal = {Production and Operations Management},
volume = {32},
number = {7},
pages = {2245–2263},
year = {2023f},
doi = {10.1111/poms.13971},
URL = {https://doi-org.crai.referencistas.com/10.1111/poms.13971},
eprint = {https://doi-org.crai.referencistas.com/10.1111/poms.13971},
abstract = {Discrete games provide the means to analyze market dynamics with limited data. However, computing such games with many players—especially in a complete information setting—is computationally infeasible because the strategy space increases exponentially with the number of players. This study presents a novel and practical method to compute and estimate discrete games. To do so, the study introduces two methodological innovations. First, we develop an efficient simulator that requires fewer random draws to evaluate the likelihood of discrete games with multiple equilibria. The augmented simulator avoids random draws that are not compatible with the observed equilibrium outcome and, thus, efficiently uses all draws to evaluate the likelihood. Second, we utilize general‐purpose computing on graphics‐processing unit (GPGPU), using multiple processing cores in a graphics‐processing unit, to increase computational speed. The two features allow us to estimate the model significantly faster compared to traditional methods. The study’s empirical application examines the effect of Apple’s company‐owned stores on the retail market structure. The results show that agglomeration effects exist between Apple and upscale firms. The presence of an Apple store attracts high‐income customers, promoting the entry of upscale firms and the exit of discount firms.}
}

@article{doi:10.1177/00222194241263646,
author = {Christian T. Doabler and Megan Rojo and Jenna A. Gersib and Anna-Maria Fall and Maria A. Longhi and Gail E. Lovette and Greg Roberts and Jasmine Uy and Katharina Johnson and Shadi Ghafghazi et al.},
title = {Do Mathematics and Reading Skills Impact Student Science Outcomes?},
journal = {Journal of Learning Disabilities},
volume = {0},
number = {0},
pages = {00222194241263646},
year = {2024g},
doi = {10.1177/00222194241263646},
note = {PMID:39056893},
URL = {https://doi-org.crai.referencistas.com/10.1177/00222194241263646},
eprint = {https://doi-org.crai.referencistas.com/10.1177/00222194241263646},
abstract = {Establishing validated science programs for students with or at risk for learning disabilities requires testing treatment effects and exploring differential response patterns. This study explored whether students’ initial mathematics and reading skills influenced their treatment response to a whole-class, second-grade science program called Scientific Explorers (Sci2). The original Sci2 study employed a cluster randomized controlled design and included 294 students from 18 second-grade classrooms. Differential effects of the program by initial mathematics and reading skill levels were not observed for an interactive science assessment and a distal science outcome measure. However, based on initial reading skill levels, moderation results were found on a science vocabulary measure, suggesting the effects of Sci2 were greatest for students with higher initial reading skills. Similar results were found using initial mathematics skill levels as a predictor of differential response such that students with higher mathematics skills reaped stronger treatment effects on the vocabulary measure. Further, we found initial mathematics skills also influenced outcomes on the proximal science content assessment, where students with higher initial mathematics skills led to higher outcomes. Overall, findings suggest Sci2 produced robust effects for all students (g = 0.24–1.23), regardless of initial skill proficiencies. Implications for exploring differential response in science intervention research are discussed.}
}

@article{doi:10.1260/026635103322987968,
author = {A. M. Horr and S. R. Asadsajadi and M. Safi},
title = {Design of Large Space Structures with Imperfection Using ANN-Based Simulator},
journal = {International Journal of Space Structures},
volume = {18},
number = {4},
pages = {235–255},
year = {2003h},
doi = {10.1260/026635103322987968},
URL = {https://doi-org.crai.referencistas.com/10.1260/026635103322987968},
eprint = {https://doi-org.crai.referencistas.com/10.1260/026635103322987968},
abstract = {Based on the advanced computational plasticity and an artificial neural network (ANN) simulator, a new design strategy has been presented for large space structures with imperfections. Nonlinear system identification approach has also been greatly spread among the researchers and engineers in the past few years. The neural network simulators as a non-parametric system identification approach present a robust and efficient way to simulate the nonlinear behaviour of engineering systems. In the paper herein an artificial neural network (ANN) simulator, a general back error propagating perceptron, is use to simulate random imperfection for nonlinear dynamic analysis of large space structures. It is also desirable to search for a procedure for wind pressure calculation with accuracy and reliability. In this respect, attention is paid to the advanced computational fluid dynamics (CFD). The use of the advanced CFD analysis can help engineers to estimate the wind pressure for the design of large space structures with complex geometries. The characteristics of the new design method have been shown graphically using a full documented numerical example, which highlights the efficiency of the new simulation method. The purpose of this paper is to present a new design method, which takes into account the effects of imperfection on the resulting dynamic responses of large space structures under gravity, temperature and wind loadings.}
}

@article{doi:10.1177/109434209300700204,
author = {Mark T. Jones and Paul E. Plassmann},
title = {Computation of Equilibrium Vortex Structures for Type-II Superconductors},
journal = {The International Journal of Supercomputing Applications},
volume = {7},
number = {2},
pages = {129–143},
year = {1993i},
doi = {10.1177/109434209300700204},
URL = {https://doi-org.crai.referencistas.com/10.1177/109434209300700204},
eprint = {https://doi-org.crai.referencistas.com/10.1177/109434209300700204},
abstract = {A model of the vortex structure and configuration for homogeneous type-II superconductors can be deter mined by minimization of the Ginzburg-Landau free energy functional. A generalization of this formulation, the Lawrence-Doniach model, can be used for layered systems. A finite-dimensional approximation of this functional leads to a very large, sparse optimization problem. A number of algorithms have been used to attempt to solve this problem. The most successful of these is a damped, inexact Newton algorithm. Its com putational kernel is the iterative solution of a large, sparse, linear system that is nearly singular. We present computational results for solving these three- dimensional problems on the Intel DELTA. Using a general-purpose, scalable, iterative solver based on the preconditioned conjugate gradient method, we ob tained sustained computational rates of up to 4.26 GFLOPS on 512 processors. We find improvements of over a factor of 100 in the total execution time when compared with the same problem run on the CRAY-2. This computational tool has given us the first compu tational view of three-dimensional vortex phenomena such as vortex locking.}
}

@article{doi:10.1057/ivs.2009.27,
author = {David J. Kasik and David Ebert and Guy Lebanon and Haesun Park and William M. Pottenger},
title = {Data Transformations and Representations for Computation and Visualization},
journal = {Information Visualization},
volume = {8},
number = {4},
pages = {275–285},
year = {2009j},
doi = {10.1057/ivs.2009.27},
URL = {https://doi-org.crai.referencistas.com/10.1057/ivs.2009.27},
eprint = {https://doi-org.crai.referencistas.com/10.1057/ivs.2009.27},
abstract = {At the core of successful visual analytics systems are computational techniques that transform data into concise, human comprehensible visual representations. The general process often requires multiple transformation steps before a final visual representation is generated. This article characterizes the complex raw data to be analyzed and then describes two different sets of transformations and representations. The first set transforms the raw data into more concise representations that improve the performance of sophisticated computational methods. The second transforms internal representations into visual representations that provide the most benefit to an interactive user. The end result is a computing system that enhances an end user’s analytic process with effective visual representations and interactive techniques. While progress has been made on improving data transformations and representations, there is substantial room for improvement.}
}

@article{doi:10.1080/01650250143000409,
author = {Horst Krist},
title = {Book Review: Emerging minds: The process of change in children’s thinking},
journal = {International Journal of Behavioral Development},
volume = {26},
number = {5},
pages = {475–476},
year = {2002k},
doi = {10.1080/01650250143000409},
URL = {https://doi-org.crai.referencistas.com/10.1080/01650250143000409},
eprint = {https://doi-org.crai.referencistas.com/10.1080/01650250143000409}
}

@article{doi:10.1177/0008068319750110,
author = {Ru-Ying Lee and I. R. Goodman},
title = {Computation of Density for A Linear Combination of –Generalized Normal Random Variables},
journal = {Calcutta Statistical Association Bulletin},
volume = {24},
number = {1–4},
pages = {101–116},
year = {1975l},
doi = {10.1177/0008068319750110},
URL = {https://doi-org.crai.referencistas.com/10.1177/0008068319750110},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0008068319750110},
abstract = {A computational procedure is presented for the approximation of the density of a linear combination of univariate -generalized normal random variables. (The -generalized normal random variable generalizes the ordinary normal one by replacing the power two in the exponent of the density by an arbitrary positive number.) The procedure applies a truncated form of the Fourier Inversion Theorem to the power series expansion of the characteristic function of a -generalized normal random variable. Because of the unimodal nature of -generalized normal characteristic functions for ⩽ 2 and the oscillatory nature for > 2, much of the computational procedure divides into two corresponding parts. Complete error analysis and accuracy control in all computations are also presented.}
}

@article{doi:10.1177/0894439303262565,
author = {David D. McFarland},
title = {Adding Machines and Logarithms: Franklin H. Giddings and Computation for the Exact Science of Sociology},
journal = {Social Science Computer Review},
volume = {22},
number = {2},
pages = {249–255},
year = {2004m},
doi = {10.1177/0894439303262565},
URL = {https://doi-org.crai.referencistas.com/10.1177/0894439303262565},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0894439303262565},
abstract = {Sociologists of earlier eras had much more limited computational tools available, but even in the founding era, there were slide rules and printed tables of mathematical functions, mechanical calculators were becoming more common, and punched card tabulating equipmentwas being developed. This article focuses on one particular founder, Franklin H. Giddings, and the kind of computations he did or did not do, using or not using various computational devices then available. Giddings’s reputation, as the leading quantitative sociologist among the founders of the discipline, is compared with his published work. Giddings himself, although a prolific author, did very little original quantitative research. His main contribution to quantitative sociology consisted of his programmatic statements, which helped provide legitimation for subsequent scholars who actually performed quantitative research.}
}

@article{doi:10.1177/09637214221078325,
author = {Bob McMurray and Keith S. Apfelbaum and J. Bruce Tomblin},
title = {The Slow Development of Real-Time Processing: Spoken-Word Recognition as a Crucible for New Thinking About Language Acquisition and Language Disorders},
journal = {Current Directions in Psychological Science},
volume = {31},
number = {4},
pages = {305–315},
year = {2022n},
doi = {10.1177/09637214221078325},
URL = {https://doi-org.crai.referencistas.com/10.1177/09637214221078325},
eprint = {https://doi-org.crai.referencistas.com/10.1177/09637214221078325},
abstract = {Words are fundamental to language, linking sound, articulation, and spelling to meaning and syntax; and lexical deficits are core to communicative disorders. Work in language acquisition commonly focuses on how lexical knowledge—knowledge of words’ sound patterns and meanings—is acquired. But lexical knowledge is insufficient to account for skilled language use. Sophisticated real-time processes must decode the sound pattern of words and interpret them appropriately. We review work that bridges this gap by using sensitive real-time measures (eye tracking in the visual world paradigm) of school-age children’s processing of highly familiar words. This work reveals that the development of word recognition skills can be characterized by changes in the rate at which decisions unfold in the lexical system (the activation rate). Moreover, contrary to the standard view that these real-time skills largely develop during infancy and toddlerhood, they develop slowly, at least through adolescence. In contrast, language disorders can be linked to differences in the ultimate degree to which competing interpretations are suppressed (competition resolution), and these differences can be mechanistically linked to deficits in inhibition. These findings have implications for real-world problems such as reading difficulties and second-language acquisition. They suggest that developing accurate, flexible, and efficient processing is just as important a developmental goal as is acquiring language knowledge.}
}

@article{doi:10.3102/0013189X016001022,
author = {Seymour Papert},
title = {Information Technology and Education: Computer Criticism vs. Technocentric Thinking},
journal = {Educational Researcher},
volume = {16},
number = {1},
pages = {22–30},
year = {1987o},
doi = {10.3102/0013189X016001022},
URL = {https://doi-org.crai.referencistas.com/10.3102/0013189X016001022},
eprint = {https://doi-org.crai.referencistas.com/10.3102/0013189X016001022}
}

@article{doi:10.1177/20438206241264631,
author = {Reuben Rose-Redwood and CindyAnn Rose-Redwood and Elia Apostolopoulou and Tyler Blackman and Han Cheng and Anindita Datta and Sharon Dias and Federico Ferretti and Wil Patrick and James Riding et al.},
title = {Re-imagining the futures of geographical thought and praxis},
journal = {Dialogues in Human Geography},
volume = {14},
number = {2},
pages = {177–191},
year = {2024p},
doi = {10.1177/20438206241264631},
URL = {https://doi-org.crai.referencistas.com/10.1177/20438206241264631},
eprint = {https://doi-org.crai.referencistas.com/10.1177/20438206241264631},
abstract = {The question of geography’s future has recurred throughout the history of geographical thought, and responses to it often presume a linear trajectory from the past and present to a possible future. Yet one of the major contributions that geographers have made to understanding spatio-temporality is reconceiving both space and time as plural, fluid, and co-constituted through multiple space–time trajectories simultaneously. Amidst the ongoing crises of the present, this article opens the current special issue with a call to pluralize geography’s futures by diversifying the voices speaking in the name of ‘geography’ and broadening the horizon of possibilities for the futures of geographical thought and praxis. We have assembled the contributions in this collection with the aim of raising important theoretical, methodological, and empirical questions about how geography’s past and present shape the conditions of possibility for its potential futures. In doing so, we seek to demonstrate how the worlding of geography’s futures is fundamentally a matter of transforming its disciplinary reproduction in the here-and-now.}
}

@article{doi:10.1177/14687984231212723,
author = {Ben Rydal Shapiro and Deborah Silvis},
title = {Animated movements, animating methods: An interaction geography approach to space and affect in early childhood education},
journal = {Journal of Early Childhood Literacy},
volume = {0},
number = {0},
pages = {14687984231212724},
year = {2023q},
doi = {10.1177/14687984231212723},
URL = {https://doi-org.crai.referencistas.com/10.1177/14687984231212723},
eprint = {https://doi-org.crai.referencistas.com/10.1177/14687984231212723},
abstract = {This article uses a comparative case study to examine how a methodological approach called interaction geography provides alternative ways to animate space, movement, and affect within the context of early childhood education. We take animation to incorporate the methods for representing space, movement, and affect; the social-material environment which animates the people and things we study; and the lively, energetic talk-in-interaction that takes place as people and things move. Our first case uses interaction geography to animate what we call gestural energies and choreographies between a teacher, students, and materials in a bilingual kindergarten classroom activity. Our second case uses interaction geography to animate a young child’s excitement for learning and teaching through movement in a cultural heritage museum. Together, our analysis demonstrates how interaction geography provides alternative ways to conceptualize the multimodal nature of literacy practices and contributes to a recent turn to affect in literacy research. We discuss how this work has implications not only for literacy researchers, teachers, and teacher educators, but also for architects, administrators, and researchers concerned with the physical design of literacy spaces.}
}

@article{doi:10.1509/jmkr.46.1.81,
author = {Manoj Thomas and Vicki G. Morwitz},
title = {The Ease-of-Computation Effect: The Interplay of Metacognitive Experiences and Naive Theories in Judgments of Price Differences},
journal = {Journal of Marketing Research},
volume = {46},
number = {1},
pages = {81–91},
year = {2009r},
doi = {10.1509/jmkr.46.1.81},
URL = {https://doi-org.crai.referencistas.com/10.1509/jmkr.46.1.81},
eprint = {https://doi-org.crai.referencistas.com/10.1509/jmkr.46.1.81},
abstract = {Consumers’ judgments of the magnitude of numerical differences are influenced by the ease of mental computations. The results from a set of experiments show that ease of computation can affect judgments of the magnitude of price differences, discount magnitudes, and brand choices. Participants seem to believe that it is easier to judge the size of a larger difference than that of a smaller difference. In the absence of appropriate corrective steps, this naive belief can lead to systematic biases in judgments. For example, when presented with two pairs of numbers, participants incorrectly judged the magnitude of the difference to be smaller for pairs with difficult computations (e.g., 4.97 – 3.96, an arithmetic difference of 1.01) than for pairs with easy computations (e.g., 5.00 – 4.00, an arithmetic difference of 1.00). The effect does not manifest when judgments do not entail mental computations or when participants are made aware that the ease or difficulty is caused by computational complexity. Furthermore, this effect is mitigated when participants’ prior experience is manipulated in a learning phase of the experiment. The results have implications for buyers and sellers and for understanding the role of metacognitive experiences in numerical judgments.}
}

@article{doi:10.3233/JHS-150519,
author = {Yilei Wang and Tao Li and Qianqian Liu and Jing Sun and Zhe Liu},
title = {The impact of social cloud reputation and structure on rational computation},
journal = {Journal of High Speed Networks},
volume = {21},
number = {3},
pages = {181–194},
year = {2015s},
doi = {10.3233/JHS-150519},
URL = {https://doi-org.crai.referencistas.com/10.3233/JHS-150519},
eprint = {https://doi-org.crai.referencistas.com/10.3233/JHS-150519},
abstract = {Abstract Rational computation means secure multi-party computation in the presence of rational parties, who care about maximizing their utility. Here the notion of utility denotes payoffs when parties take certain actions in the computation. Therefore rational parties may carefully choose their actions before they interact with others. Traditionally, the main tasks of rational computation is to encourage parties to exchange information with their opponents, even if they do not trust each other. Exchanging information is similar to the action of cooperation towards the view of game theory. Therefore, measures should be taken to encourage parties to cooperate with others in rational computation. In this paper, we assume that rational parties who participate in the computation protocol form a social cloud. Parties in the social cloud may interact within several rounds and in each round, some desirable properties such as reputation may be generated. Parties who have good reputation means they are likely to cooperate with others. The structure of social cloud is not static. Instead, it evolutes when parties complete one round of computation. We mainly discuss the impact of social cloud structure on rational computation. Simulation results show that when the community structure reach to a proper value, parties are more likely to cooperate in the computation protocol. In addition, rational parties can gain optimal utilities.}
}

@article{doi:10.1177/07356331241270707,
author = {Abdullahi Yusuf and Amiru Yusuf Muhammad},
title = {Exploring Clusters of Novice Programmers’ Anxiety-Induced Behaviors During Block- and Text-Based Coding: A Predictive and Moderation Analysis of Programming Quality and Error Debugging Skills},
journal = {Journal of Educational Computing Research},
volume = {62},
number = {7},
pages = {1798–1836},
year = {2024t},
doi = {10.1177/07356331241270707},
URL = {https://doi-org.crai.referencistas.com/10.1177/07356331241270707},
eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331241270707},
abstract = {The study investigates the potential of anxiety clusters in predicting programming performance in two distinct coding environments. Participants comprised 83 second-year programming students who were randomly assigned to either a block-based or a text-based group. Anxiety-induced behaviors were assessed using physiological measures (Apple Watch and Electrocardiogram machine), behavioral observation, and self-report. Utilizing the Hidden Markov Model and Optimal Matching algorithm, we found three representative clusters in each group. In the block-based group, clusters were designated as follows: “stay calm” (students allocating more of their time to a calm state), “stay hesitant” (students allocating more of their time to a hesitant state), and “to-calm” (those allocating minimal time to a hesitant and anxious state but displaying a pronounced propensity to transition to a calm state). In contrast, clusters in the text-based group were labeled as: “to-hesitant” (exhibiting a higher propensity to transition to a hesitant state), “stay hesitant” (allocating significant time to a hesitant state), and “stay anxious” (remaining persistently anxious in a majority of the coding time). Additionally, our results indicate that novice programmers are more likely to experience anxiety during text-based coding. We discussed the findings and highlighted the policy implications of the study.}
}

