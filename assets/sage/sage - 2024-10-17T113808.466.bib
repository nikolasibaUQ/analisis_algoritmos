@article{doi:10.1177/1932202X231218487,
author = {Kadir Bahar and Erdogan Kaya and Xiaolu Zhang and Eter Mjavanadze},
title = {Quo Vadis Racial Disparities? Trend Analysis of the Participation and Top Achievement in Advanced Placement Computer Science Exams},
journal = {Journal of Advanced Academics},
volume = {34},
number = {3–4},
pages = {240–270},
year = {2023a},
doi = {10.1177/1932202X231218487},
URL = {https://doi-org.crai.referencistas.com/10.1177/1932202X231218487},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1932202X231218487},
abstract = {This study explores the direction and magnitude of racial disparities on three advanced placement (AP) computer science (CS) exams, namely AP CS Principles, AP CS A, and AP CS AB, based on the test scores of more than one million students who have taken AP CS exams between 1997 and 2020. Using Mann–Kendall test and Sen’s slope procedures we found that the number of Black, Hispanic, and Native American students in AP CS exams have increased steadily and significantly over years, yet they are still far from reaching parity. Further, our findings suggest that the racial disparities among top achievers are very wide. The results provide educators and researchers support for identifying and quantifying the racial disparities in advanced academic programs and may inform the development of policies, practices, and programs to reduce racial disparities in pre-college CS education and further.}
}

@article{doi:10.1260/1759-3131.1.3-4.223,
author = {Manasa Ranjan Behera and K. Murali and V. Sundar},
title = {Identification of Suitable Grid Size for Accurate Computation of Run-up Height},
journal = {The International Journal of Ocean and Climate Systems},
volume = {1},
number = {3–4},
pages = {223–237},
year = {2010b},
doi = {10.1260/1759-3131.1.3-4.223},
URL = {https://doi-org.crai.referencistas.com/10.1260/1759-3131.1.3-4.223},
eprint = {https://doi-org.crai.referencistas.com/10.1260/1759-3131.1.3-4.223},
abstract = {A numerical investigation has been carried out to obtain a non-dimensional grid size (grid size/ tsunami base width) for the near shore discretisation of computational domains for long wave modelling. A 1D domain has been considered in which, the flow has been solved by 1D shallow water equations with vertically integrated flow variables. The sensitivity study of the grid size was carried out in the 1D channel with an open boundary at one end and shelf boundary at the other end. The grid size was varied from 10 m to 1000 m and its effect on the computation of the tsunami run-up along the shoreline has been investigated. The non-dimensional grid size for the computation of run-up was optimised by comparing the non-dimensional run-up (tsunami run-up/initial tsunami height) and a threshold value of 5.0e-4 was obtained. Further, the study was extended to real scenario by adopting various grids for the shelf region of northern Tamil Nadu coast, south east coast of India in 2D and a suitable grid size was obtained.}
}

@article{doi:10.1177/109434209200600303,
author = {Jean-Philippe Brunet and S. Lennart Johnsson},
title = {All-To-All Broadcast and Applications On the Connection Machine},
journal = {The International Journal of Supercomputing Applications},
volume = {6},
number = {3},
pages = {241–256},
year = {1992c},
doi = {10.1177/109434209200600303},
URL = {https://doi-org.crai.referencistas.com/10.1177/109434209200600303},
eprint = {https://doi-org.crai.referencistas.com/10.1177/109434209200600303},
abstract = {An all-to-all broadcast algorithm that exploits concur rent communication on all channels of the Connection Machine system CM-200 binary cube network is de scribed. Issues in integrating a physical all-to-all broad cast between processing nodes into a language envi ronment using a global address space are discussed. Timings for the physical broadcast between nodes and for the virtual broadcast are given. The peak data transfer rate for the physical broadcast on a CM-200 is 5.9 gigabytes/sec, and the peak rate for the virtual broadcast is 31 gigabytes/sec. Array reshaping is an effective performance optimization technique. An ex ample is given where reshaping improved perfor mance by a factor of 7 by reducing the amount of local data motion. We also show how to exploit symmetry for computation of an interaction matrix using the all- to-all broadcast function. Further optimizations are suggested for N-body-type calculations. Using the all- to-all broadcast function, a peak rate of 9.3 GFLOPS/ sec has been achieved for the N-body computations in 32-bit precision on a 2,048 node Connection Machine system CM-200.}
}

@article{doi:10.1177/1536867X1201100401,
author = {Rhian M. Daniel and Bianca L. De Stavola and Simon N. Cousens},
title = {Gformula: Estimating Causal Effects in the Presence of Time-Varying                     Confounding or Mediation using the G-Computation Formula},
journal = {The Stata Journal},
volume = {11},
number = {4},
pages = {479–517},
year = {2011d},
doi = {10.1177/1536867X1201100401},
URL = {https://doi-org.crai.referencistas.com/10.1177/1536867X1201100401},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1536867X1201100401},
abstract = {This article describes a new command, gformula, that is an implementation of the g-computation procedure. It is used to estimate the causal effect of time-varying exposures on an outcome in the presence of time-varying confounders that are themselves also affected by the exposures. The procedure also addresses the related problem of estimating direct and indirect effects when the causal effect of the exposures on an outcome is mediated by intermediate variables, and in particular when confounders of the mediator–outcome relationships are themselves affected by the exposures. A brief overview of the theory and a description of the command and its options are given, and illustrations using two simulated examples are provided.}
}

@article{doi:10.1177/1073858409354384,
author = {Gustavo Deco and Maurizio Corbetta},
title = {The Dynamical Balance of the Brain at Rest},
journal = {The Neuroscientist},
volume = {17},
number = {1},
pages = {107–123},
year = {2011e},
doi = {10.1177/1073858409354384},
note = {PMID:21196530},
URL = {https://doi-org.crai.referencistas.com/10.1177/1073858409354384},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1073858409354384},
abstract = {The authors review evidence that spontaneous, that is, not stimulus or task driven, activity in the brain at the level of large-scale neural systems is not noise, but orderly and organized in a series of functional networks that maintain, at all times, a high level of coherence. These networks of spontaneous activity correlation or resting state networks (RSN) are closely related to the underlying anatomical connectivity, but their topography is also gated by the history of prior task activation. Network coherence does not depend on covert cognitive activity, but its strength and integrity relates to behavioral performance. Some RSN are functionally organized as dynamically competing systems both at rest and during tasks. Computational studies show that one of such dynamics, the anticorrelation between networks, depends on noise-driven transitions between different multistable cluster synchronization states. These multistable states emerge because of transmission delays between regions that are modeled as coupled oscillators systems. Large-scale systems dynamics are useful for keeping different functional subnetworks in a state of heightened competition, which can be stabilized and fired by even small modulations of either sensory or internal signals.}
}

@article{doi:10.1177/1094342019849618,
author = {John M Dennis and Brian Dobbins and Christopher Kerr and Youngsung Kim},
title = {Optimizing the HOMME dynamical core for multicore platforms},
journal = {The International Journal of High Performance Computing Applications},
volume = {33},
number = {5},
pages = {1030–1045},
year = {2019f},
doi = {10.1177/1094342019849618},
URL = {https://doi-org.crai.referencistas.com/10.1177/1094342019849618},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342019849618},
abstract = {The approach of the next-generation computing platforms offers a tremendous opportunity to advance the state-of-the-art in global atmospheric dynamical models. We detail our incremental approach to utilize this emerging technology by enhancing concurrency within the High-Order Method Modeling Environment (HOMME) atmospheric dynamical model developed at the National Center for Atmospheric Research (NCAR). The study focused on improvements to the performance of HOMME which is a Fortran 90 code with a hybrid (MPIOpenMP) programming model. The article describes the changes made to the use of message passing interface (MPI) and OpenMP as well as single-core optimizations to achieve significant improvements in concurrency and overall code performance. For our optimization studies, we utilize the “Cori” system with an Intel Xeon Phi Knights Landing processor deployed at the National Energy Research Supercomputing Center and the “`Cheyenne” system with an Intel Xeon Broadwell processor installed at the NCAR. The results from the studies, using “workhorse” configurations performed at NCAR, show that these changes have a transformative impact on the computational performance of HOMME. Our improvements have shown that we can effectively increase potential concurrency by efficiently threading the vertical dimension. Further, we have seen a factor of two overall improvement in the computational performance of the code resulting from the single-core optimizations. Most notably from the work is that our incremental approach allows for high-impact changes without disrupting existing scientific productivity in the HOMME community.}
}

@article{doi:10.1177/1077546309341137,
author = {Venkatesh Deshmukh},
title = {Approximate Stability Analysis and Computation of Solutions of Nonlinear Delay Differential Algebraic Equations with Time Periodic Coefficients},
journal = {Journal of Vibration and Control},
volume = {16},
number = {7–8},
pages = {1235–1260},
year = {2010g},
doi = {10.1177/1077546309341137},
URL = {https://doi-org.crai.referencistas.com/10.1177/1077546309341137},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1077546309341137},
abstract = {Approximate stability analysis of nonlinear delay differential algebraic equations (DDAEs) with periodic coefficients is proposed with a geometric interpretation of evolution of the linearized system. Firstly, a numerical algorithm based on direct integration by expansion in terms of Chebyshev polynomials is derived for linear analysis. The proposed algorithm is shown to have deeper connections with and be computationally less cumbersome than the solution of the underlying semi-explicit system via a similarity transformation. The stability of time periodic DDAE systems is characterized by the spectral radius of a “monodromy matrix”, which is a finite-dimensional approximation of a compact infinite-dimensional operator. The monodromy matrix is essentially a map of the Chebyshev coefficients (or collocation vector) of the state from the delay interval to the next adjacent interval of time. The computations are entirely performed with the original system to avoid cumbersome transformations associated with the semi-explicit form of the system. Next, two computational algorithms, the first based on perturbation series and the second based on Chebyshev spectral collocation, are detailed to obtain solutions of nonlinear DDAEs with periodic coefficients for consistent initial functions.}
}

@article{doi:10.3233/ISB-00180,
author = {Marylens     Hernández Guía and Abel González Pérez and Vladimir Espinosa Angarica and Ana T. Vasconcelos and Julio Collado-Vides},
title = {Complementing Computationally Predicted Regulatory Sites in     Tractor_DB Using a Pattern Matching Approach},
journal = {In Silico Biology},
volume = {5},
number = {2},
pages = {209–219},
year = {2005h},
doi = {10.3233/ISB-00180},
URL = {https://doi-org.crai.referencistas.com/10.3233/ISB-00180},
eprint = {https://doi-org.crai.referencistas.com/10.3233/ISB-00180},
abstract = {Prokaryotic genomes annotation has focused on genes location and function. The lack of regulatory information has limited the knowledge on cellular transcriptional regulatory networks. However, as more phylogenetically close genomes are sequenced and annotated, the implementation of phylogenetic footprinting strategies for the recognition of regulators and their regulons becomes more important. In this paper we describe a comparative genomics approach to the prediction of new gamma-proteobacterial regulon members. We take advantage of the phylogenetic proximity of Escherichia coli and other 16 organisms of this subdivision and the intensive search of the space sequence provided by a pattern-matching strategy. Using this approach we complement predictions of regulatory sites made using statistical models currently stored in Tractor_DB, and increase the number of transcriptional regulators with predicted binding sites up to 86. All these computational predictions may be reached at Tractor_DB (www.bioinfo.cu/Tractor_DB, www.tractor.lncc.br, www.ccg.unam.mx/Computational_Genomics/tractorDB/). We also take a first step in this paper towards the assessment of the conservation of the architecture of the regulatory network in the gamma-proteobacteria through evaluating the conservation of the overall connectivity of the network.}
}

@article{doi:10.1243/0957650971537105,
author = {L He},
title = {Computation of unsteady flow through steam turbine blade rows at partial admission},
journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
volume = {211},
number = {3},
pages = {197–205},
year = {1997i},
doi = {10.1243/0957650971537105},
URL = {https://doi-org.crai.referencistas.com/10.1243/0957650971537105},
eprint = {https://doi-org.crai.referencistas.com/10.1243/0957650971537105},
abstract = {Abstract Partial admission in the steam turbine is associated with strong unsteady flow effects on aerodynamic performance. This paper presents a first-of-its-kind computational study of the problem. The unsteady flow field in multiple blade passages and multiple blade rows is governed by the quasi three-dimensional unsteady Navier-Stokes equations, closed by a mixing-length turbulence model. The partial admission is introduced by blocking one segmental arc (or several segmental arcs) of the inlet guide vane of the first stage. The flow equations are solved by using a time-dependent finite volume method. The calculated unsteady force on rotor blades for a turbine stage at partial admission compares well with the corresponding experimental data. The present results show that a cyclic pumping and sucking phenomenon occurs in the rotor blade row of the first stage, resulting in large unsteady loading and marked mixing loss. For a single stage at a given admission rate, a blocking arrangement with two flow segments is shown to be much more detrimental than one arc of admission, because of the extra mixing loss. The results for a two-stage case, however, suggest that the decaying rate of circumferential non-uniformities could be far more important for performance. For this reason, an enhanced mixing loss in the first stage might be beneficial to the overall efficiency of a multistage turbine.}
}

@article{doi:10.1177/2336825X1902700303,
author = {Pinja Lehtonen},
title = {How Quantum Ontology and Q Methodology Can Revitalise Agency in IR},
journal = {New Perspectives},
volume = {27},
number = {3},
pages = {37–61},
year = {2019j},
doi = {10.1177/2336825X1902700303},
URL = {https://doi-org.crai.referencistas.com/10.1177/2336825X1902700303},
eprint = {https://doi-org.crai.referencistas.com/10.1177/2336825X1902700303},
abstract = {This article brings Alexander Wendt’s (2015) ‘quantum social ontology’ into the realm of empirical International Relations (IR) research by coupling it with Q methodology. It shows how Wendt’s ontology and Q methodology share a central interest in complex agency and are inherently allied in terms of principles and purposes. The quantum view has catalysed conversation within IR and social sciences more broadly, but that debate has remained almost exclusively on a theoretical level. This article shows that there is potential for empirical research in this area regardless of whether one considers the quantum view to be an analogy or ontological reality. Q methodology’s grounding ideas align with quantum physics and the quantum social ontology, e.g. in the fashion it conceives of subjective states of mind and their measurement. Practical examples of Q methodological work are presented to illustrate the quantum concepts in a social scientific setting. The article argues for a broader study of political subjectivity within IR through a notion of personhood, which opens up vast potentialities for agency as well as for breaking free of determinism, and fixed notions of human nature as well as ostensibly fixed understandings of advantaged or disadvantaged subject positions.}
}

@article{doi:10.1177/0143624415615328,
author = {Anna Parkin and Andrew Mitchell and David Coley},
title = {A new way of thinking about environmental building standards: Developing and demonstrating a client-led zero-energy standard},
journal = {Building Services Engineering Research and Technology},
volume = {37},
number = {4},
pages = {413–430},
year = {2016k},
doi = {10.1177/0143624415615328},
URL = {https://doi-org.crai.referencistas.com/10.1177/0143624415615328},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0143624415615328},
abstract = {There are over 70 low energy and carbon standards in use around the world. None of these standards have been designed by the clients who pay for and occupy the buildings in question. In this work, the client was asked to define the building code for the construction of a new 2800 m2 building via a structured survey. The resulting zero-energy standard simply required the building to incur no energy utility bill. One year of monitoring of the completed building was used to see if the standard had been met. The result of this work is a new way of thinking about environmental building standards that solves many of the issues of obtaining and maintaining buy-in from the client. Practical application: This is the first time that the client has played a key role in the definition of a low-energy building standard. Measured energy consumption and renewable energy generation data are presented and demonstrate that the zero-energy criteria were successfully met. This work is important as it shows that the client can have a meaningful input into the design of an environmental standard. The paper should be of interest to architects, engineers, building energy researchers and those interested in methods that can be used to reduce the energy demand of buildings.}
}

@article{doi:10.1177/0049124119826159,
author = {Krisztián Pósch},
title = {Testing Complex Social Theories With Causal Mediation Analysis and G-Computation: Toward a Better Way to Do Causal Structural Equation Modeling},
journal = {Sociological Methods & Research},
volume = {50},
number = {3},
pages = {1376–1406},
year = {2021l},
doi = {10.1177/0049124119826159},
URL = {https://doi-org.crai.referencistas.com/10.1177/0049124119826159},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0049124119826159},
abstract = {Complex social scientific theories are conventionally tested using linear structural equation modeling (SEM). However, the underlying assumptions of linear SEM often prove unrealistic, making the decomposition of direct and indirect effects problematic. Recent advancements in causal mediation analysis can help to address these shortcomings, allowing for causal inference when a new set of identifying assumptions are satisfied. This article reviews how these ideas can be generalized to multiple mediators, with a focus on the posttreatment confounding and causal ordering cases. Using the potential outcome framework as a rigorous tool for causal inference, the application is the theory of procedural justice policing. Analysis of data from two randomized experiments shows that making similar parametric assumptions to SEMs and using G-computation improve the viability of effect decomposition. The article concludes with a discussion of how causal mediation analysis improves upon SEM and the potential limitation of the methods.}
}

@article{doi:10.1177/09622802231167436,
author = {Francesco Pozza and Euloge Clovis Kenne Pagui and Alessandra Salvan},
title = {Improved and computationally stable estimation of relative risk regression with one binary exposure},
journal = {Statistical Methods in Medical Research},
volume = {32},
number = {6},
pages = {1234–1246},
year = {2023m},
doi = {10.1177/09622802231167436},
note = {PMID:37032617},
URL = {https://doi-org.crai.referencistas.com/10.1177/09622802231167436},
eprint = {https://doi-org.crai.referencistas.com/10.1177/09622802231167436},
abstract = {In medical statistics, when the effect of a binary risk factor on a binary response is of interest, relative risk is often the preferred measure due to its direct interpretation. However, statistical inference on this quantity is not as straightforward as for other measures of association, especially when further explanatory variables have to be taken into account. Starting from a review of available methods for inference on relative risk, this paper deals with small and moderate sample size settings for which we show that classical approaches can be problematic. For this reason, we propose the use of improved estimation procedures, aiming at mean or median bias reduction of the maximum likelihood estimator. In particular, these methods are developed for a new alternative specification of a model recently proposed by Richardson et al, where higher computational stability of the estimation methods is achieved. A real-data example and extensive simulation studies show that the proposed methods perform remarkably better than the standard ones.}
}

@article{doi:10.1177/23998083221100550,
author = {Steven Jige Quan},
title = {Urban-GAN: An artificial intelligence-aided computation system for plural urban design},
journal = {Environment and Planning B: Urban Analytics and City Science},
volume = {49},
number = {9},
pages = {2500–2515},
year = {2022n},
doi = {10.1177/23998083221100550},
URL = {https://doi-org.crai.referencistas.com/10.1177/23998083221100550},
eprint = {https://doi-org.crai.referencistas.com/10.1177/23998083221100550},
abstract = {The current urban design computation is mostly centered on the professional designer while ignoring the plural dimension of urban design. In addition, available public participation computational tools focus mainly on information and idea sharing, leaving the public excluded in design generation because of their lack of design expertise. To address such an issue, this study develops Urban-GAN, a plural urban design computation system, to provide new technical support for design empowerment, allowing the public to generate their own designs. The sub-symbolic representation and artificial intelligence techniques of deep convolutional neural networks, case-based reasoning, and generative adversarial networks are used to acquire and embody design knowledge as the density function, and generate design schemes with this knowledge. The system consists of an urban form database and five process models through which the user with little design expertise can select urban form cases, generate designs similar to those cases, and make design decisions. The Urban-GAN is applied to hypothetical design experiments, which show that the user is able to apply the system to successfully generate distinctive designs following the urban form “styles” in Manhattan, Portland, and Shanghai. This study further extends the discussion about the plural urban design computation to general reflections on the goals and values in AI technique application in planning and design.}
}

@article{doi:10.1177/03063127231185095,
author = {Ludovico Rella},
title = {Close to the metal: Towards a material political economy of the epistemology of computation},
journal = {Social Studies of Science},
volume = {54},
number = {1},
pages = {3–29},
year = {2024o},
doi = {10.1177/03063127231185095},
note = {PMID:37427772},
URL = {https://doi-org.crai.referencistas.com/10.1177/03063127231185095},
eprint = {https://doi-org.crai.referencistas.com/10.1177/03063127231185095},
abstract = {This paper investigates the role of the materiality of computation in two domains: blockchain technologies and artificial intelligence (AI). Although historically designed as parallel computing accelerators for image rendering and videogames, graphics processing units (GPUs) have been instrumental in the explosion of both cryptoasset mining and machine learning models. The political economy associated with video games and Bitcoin and Ethereum mining provided a staggering growth in performance and energy efficiency and this, in turn, fostered a change in the epistemological understanding of AI: from rules-based or symbolic AI towards the matrix multiplications underpinning connectionism, machine learning and neural nets. Combining a material political economy of markets with a material epistemology of science, the article shows that there is no clear-cut division between software and hardware, between instructions and tools, and between frameworks of thought and the material and economic conditions of possibility of thought itself. As the microchip shortage and the growing geopolitical relevance of the hardware and semiconductor supply chain come to the fore, the paper invites social scientists to engage more closely with the materialities and hardware architectures of ‘virtual’ algorithms and software.}
}

@article{doi:10.1177/09567976211043428,
author = {David Rosenbaum and Moshe Glickman and Stephen M. Fleming and Marius Usher},
title = {The Cognition/Metacognition Trade-Off},
journal = {Psychological Science},
volume = {33},
number = {4},
pages = {613–628},
year = {2022p},
doi = {10.1177/09567976211043428},
note = {PMID:35333670},
URL = {https://doi-org.crai.referencistas.com/10.1177/09567976211043428},
eprint = {https://doi-org.crai.referencistas.com/10.1177/09567976211043428},
abstract = {Integration to boundary is an optimal decision algorithm that accumulates evidence until the posterior reaches a decision boundary, resulting in the fastest decisions for a target accuracy. Here, we demonstrated that this advantage incurs a cost in metacognitive accuracy (confidence), generating a cognition/metacognition trade-off. Using computational modeling, we found that integration to a fixed boundary results in less variability in evidence integration and thus reduces metacognitive accuracy, compared with a collapsing-boundary or a random-timer strategy. We examined how decision strategy affects metacognitive accuracy in three cross-domain experiments, in which 102 university students completed a free-response session (evidence terminated by the participant’s response) and an interrogation session (fixed number of evidence samples controlled by the experimenter). In both sessions, participants observed a sequence of evidence and reported their choice and confidence. As predicted, the interrogation protocol (preventing integration to boundary) enhanced metacognitive accuracy. We also found that in the free-response sessions, participants integrated evidence to a collapsing boundary—a strategy that achieves an efficient compromise between optimizing choice and metacognitive accuracy.}
}

@article{doi:10.1177/00222194221097710,
author = {Rajiv Satsangi and Alexandra R. Raines},
title = {Examining Virtual Manipulatives for Teaching Computations With Fractions to Children With Mathematics Difficulty},
journal = {Journal of Learning Disabilities},
volume = {56},
number = {4},
pages = {295–309},
year = {2023q},
doi = {10.1177/00222194221097710},
note = {PMID:35658741},
URL = {https://doi-org.crai.referencistas.com/10.1177/00222194221097710},
eprint = {https://doi-org.crai.referencistas.com/10.1177/00222194221097710},
abstract = {As digital technology use increases in K–12 education, greater numbers of strategies become available to support students in mathematics. One technology that provides students diverse representations of mathematical concepts is virtual manipulatives. Although instruction featuring representations with physical manipulatives possesses a large body of research, the virtual form lacks comparable study, particularly with young children experiencing mathematics difficulty or identified with a mathematics learning disability. These students often demonstrate challenges learning integral skills such as fractions that subsequently affect their academic success in future years. This study examined the use of virtual manipulatives paired with explicit instruction and a system of least prompts for teaching computations with fractions to three elementary students with mathematics difficulty. A functional relation was found using a single-subject multiple probe design between the treatment condition and students’ accuracy performance solving problems. These results and their implications for the field at-large are discussed.}
}

@article{doi:10.1177/095965180321700207,
author = {S P Tomlinson and R W Cooke and D C Cosserat},
title = {A computationally efficient technique for modelling velocity-dependent sliding friction},
journal = {Proceedings of the Institution of Mechanical Engineers, Part I: Journal of Systems and Control Engineering},
volume = {217},
number = {2},
pages = {139–146},
year = {2003r},
doi = {10.1177/095965180321700207},
URL = {https://doi-org.crai.referencistas.com/10.1177/095965180321700207},
eprint = {https://doi-org.crai.referencistas.com/10.1177/095965180321700207},
abstract = {Abstract This paper contains a summary of research undertaken at QinetiQ (Winfrith) to simulate velocity-dependent ‘stick/slip’ or ‘sliding’ friction for use in the modelling of complex engineering systems. The research arises from combined simulation and experimental investigations conducted into the performance optimization of towed array outboard handling systems on submarines and surface ships. However, the technique developed is general and suitable for any time-domain system simulation, entailing the solution of a set of non-linear differential equations. The actual friction-velocity relationship may be either equation based or determined from experimental data as a set of x-y values. The simulation technique is novel in that it avoids the traditional problem of a friction discontinuity as the surface relative velocity changes sign. It thereby overcomes the computational problems that often occur when multiple discontinuities are encountered, such as simulations stopping prematurely, failing or giving erroneous results.}
}

@article{doi:10.1177/2053951718811843,
author = {Petter Törnberg and Anton Törnberg},
title = {The limits of computation: A philosophical critique of contemporary Big Data research},
journal = {Big Data & Society},
volume = {5},
number = {2},
pages = {2053951718811843},
year = {2018s},
doi = {10.1177/2053951718811843},
URL = {https://doi-org.crai.referencistas.com/10.1177/2053951718811843},
eprint = {https://doi-org.crai.referencistas.com/10.1177/2053951718811843},
abstract = {This paper reviews the contemporary discussion on the epistemological and ontological effects of Big Data within social science, observing an increased focus on relationality and complexity, and a tendency to naturalize social phenomena. The epistemic limits of this emerging computational paradigm are outlined through a comparison with the discussions in the early days of digitalization, when digital technology was primarily seen through the lens of dematerialization, and as part of the larger processes of “postmodernity”. Since then, the online landscape has become increasingly centralized, and the “liquidity” of dematerialized technology has come to empower online platforms in shaping the conditions for human behavior. This contrast between the contemporary epistemological currents and the previous philosophical discussions brings to the fore contradictions within the study of digital social life: While qualitative change has become increasingly dominant, the focus has gone towards quantitative methods; while the platforms have become empowered to shape social behavior, the focus has gone from social context to naturalizing social patterns; while meaning is increasingly contested and fragmented, the role of hermeneutics has diminished; while platforms have become power hubs pursuing their interests through sophisticated data manipulation, the data they provide is increasingly trusted to hold the keys to understanding social life. These contradictions, we argue, are partially the result of a lack of philosophical discussion on the nature of social reality in the digital era; only from a firm metatheoretical perspective can we avoid forgetting the reality of the system under study as we are affected by the powerful social life of Big Data.}
}

@article{doi:10.1177/0040517514540767,
author = {Liwei Wu and Bohong Gu},
title = {Fatigue behaviors of four-step three-dimensional braided composite material: a meso-scale approach computation},
journal = {Textile Research Journal},
volume = {84},
number = {18},
pages = {1915–1930},
year = {2014t},
doi = {10.1177/0040517514540767},
URL = {https://doi-org.crai.referencistas.com/10.1177/0040517514540767},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0040517514540767},
abstract = {This paper reports the computational results of the bending fatigue behaviors of four-step three-dimensional rectangular braided composite materials from a meso-scale approach. A full-size meso-scale model of a four-step three-dimensional braided composite was established to numerically analyze the deformation and damage under cyclic bending loading. The stress distribution, energy absorption, hysteresis loop features and damage morphologies were obtained to explain the structural effects on the deformation and damage of the three-dimensional braided composite material subjected to three-point bending cyclic loading. The influences of the microstructure on the fatigue behaviors have been discussed for designing the three-dimensional braided composite material with high fatigue damage tolerance.}
}

