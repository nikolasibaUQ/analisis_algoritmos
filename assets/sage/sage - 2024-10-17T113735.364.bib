@article{doi:10.3233/JAD-2005-7301,
author = {Hojjat Adeli and Samanwoy Ghosh-Dastidar and Nahid Dadmehr},
title = {Alzheimer’s disease and models of computation: Imaging, classification, and neural models},
journal = {Journal of Alzheimer’s Disease},
volume = {7},
number = {3},
pages = {187–199},
year = {2005a},
doi = {10.3233/JAD-2005-7301},
note = {PMID:16006662},
URL = {https://doi-org.crai.referencistas.com/10.3233/JAD-2005-7301},
eprint = {https://doi-org.crai.referencistas.com/10.3233/JAD-2005-7301},
abstract = {Prediction or early-stage diagnosis of Alzheimer’s disease (AD) requires a comprehensive understanding of the underlying mechanisms of the disease and its progression. Researchers in this area have approached the problem from multiple directions by attempting to develop (a) neurological (neurobiological and neurochemical) models, (b) analytical models for anatomical and functional brain images, (c) analytical feature extraction models for electroencephalograms (EEGs), (d) classification models for positive identification of AD, and (e) neural models of memory and memory impairment in AD. This article presents a state-of-the-art review of research performed on computational modeling of AD and its markers. The review covers the following approaches: computer imaging, classification models, connectionist neural models, and biophysical neural models. It is concluded that a mixture of markers and a combination of novel computational techniques such as neural computing, chaos theory, and wavelets can increase the accuracy of algorithms for automated detection and diagnosis of AD.}
}

@article{doi:10.1177/0263276421990435,
author = {Ryan Bishop and Daniel Ross},
title = {Technics, Time and the Internation: Bernard Stiegler’s Thought – A Dialogue with Daniel Ross},
journal = {Theory, Culture & Society},
volume = {38},
number = {4},
pages = {111–133},
year = {2021b},
doi = {10.1177/0263276421990435},
URL = {https://doi-org.crai.referencistas.com/10.1177/0263276421990435},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0263276421990435},
abstract = {This interview with Bernard Stiegler’s long-time translator and collaborator, Daniel Ross, examines the connections between different periods of Stiegler’s work, thought, writing and activism. Moving from the three volumes of Technics and Time to the final large-scale collaborative project of The Internation, the discussion concentrates on Stiegler’s conceptualization of ‘protentionality’, hope and care for a world confronted by climate crises, entropy and computational economic reconfigurations of work, economy and imaginations for futural possibilities. The interview foreshadows the special issue on The Internation project planned by Bishop and Stiegler for TCS that will appear in the near future.}
}

@article{doi:10.3233/JID-2007-11302,
author = {Peter von Buelow},
title = {ADVANTAGES OF EVOLUTIONARY COMPUTATION USED FOR EXPLORATION IN THE     CREATIVE DESIGN PROCESS},
journal = {Journal of Integrated Design and Process Science},
volume = {11},
number = {3},
pages = {5–18},
year = {2007c},
doi = {10.3233/JID-2007-11302},
URL = {https://doi-org.crai.referencistas.com/10.3233/JID-2007-11302},
eprint = {https://doi-org.crai.referencistas.com/10.3233/JID-2007-11302},
abstract = {In early phases of design a wide exploration of the design space is crucial to the development of creative solutions. In this regard, Evolutionary Computation (EC), and in particular Genetic Algorithms, contain several qualities that can enhance exploration by opening the search process beyond the focus of finding a single “best” solution. Over the years many researchers in the area of creative thinking including Gordon, de Bono, Parnes, Osborn and others, have suggested design strategies that have interesting parallels in EC processes. For instance, a well known inhibitor of creative thinking is design fixation, where the suggestion of a particular solution makes it difficult to imagine other good solutions. Unlike many other computational search algorithms, EC methods work with populations of “fairly good” solutions. Therefore, there is less danger that creativity will be harmed by design fixation on one “best” solution. This paper shows through a specific example of a truss bridge how an EC based design exploration program can aid the designer by providing a selection of “pretty good” solutions rather than a single optimal solution. Other aspects of the EC program are also discussed including drawbacks to the method such as computational intensity as well as directions of future development.}
}

@article{doi:10.1177/00222194241248188,
author = {Lynn S. Fuchs and Douglas Fuchs and Eunsoo Cho and Marcia A. Barnes and Tuire Koponen and Daniel R. Espinas},
title = {Comorbid Word Reading and Mathematics Computation Difficulty at Start of First Grade},
journal = {Journal of Learning Disabilities},
volume = {0},
number = {0},
pages = {00222194241248188},
year = {2024d},
doi = {10.1177/00222194241248188},
note = {PMID:38686606},
URL = {https://doi-org.crai.referencistas.com/10.1177/00222194241248188},
eprint = {https://doi-org.crai.referencistas.com/10.1177/00222194241248188},
abstract = {The purpose of this analysis was to describe cognitive processes associated with comorbid difficulty between word reading (WR) and mathematics computation (MC) at the start of first grade among children selected for WR and MC delays. A sample of 234 children (mean age 6.50 years, SD = 0.31) was assessed on WR, MC, core cognitive processes (phonological processing, rapid automatized naming, verbal counting [VC]), and domain-general cognitive processes (working memory, oral language, nonverbal reasoning, attentive behavior). Structural equation modeling was used to predict a latent Comorbidity factor, which modeled shared variance between WR and MC, and to identify processes associated with that Comorbidity factor. Results identified each of the core cognitive processes, especially VC, and each of the domain-general cognitive processes, especially working memory, as explaining shared variance between WR and MC. Implications for understanding comorbid difficulty at the start of first grade and designing coordinated first-grade interventions are discussed.}
}

@article{doi:10.1177/0963721415618485,
author = {Adam E. Green},
title = {Creativity, Within Reason: Semantic Distance and Dynamic State Creativity in Relational Thinking and Reasoning},
journal = {Current Directions in Psychological Science},
volume = {25},
number = {1},
pages = {28–35},
year = {2016e},
doi = {10.1177/0963721415618485},
URL = {https://doi-org.crai.referencistas.com/10.1177/0963721415618485},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0963721415618485},
abstract = {Human reasoning and creativity represent perhaps the two highest evolutionary reaches of cognition. These two capacities are distinct from each other, but research on creativity in analogical reasoning has identified a point of convergence between them at one of the farthest forward and most recently evolved reaches of the brain. Analogy is a form of relational cognition because analogies form connections that relate otherwise separate concepts. Quantitative tools for measuring the semantic distance between concepts have advanced the measurement of creativity in relational cognition (more creative relational cognition forms connections across greater semantic distance). These tools are especially useful for the emerging neuroscience of creativity. I describe this semantic-distance approach and how it is being leveraged in my laboratory and elsewhere to investigate not only differences in creative ability between individuals but also creativity as a dynamic state that varies across time within an individual.}
}

@article{doi:10.1177/109434208800200406,
author = {J.A. Hertz},
title = {Statistical Mechanics of Neural Computation},
journal = {The International Journal of Supercomputing Applications},
volume = {2},
number = {4},
pages = {54–62},
year = {1988f},
doi = {10.1177/109434208800200406},
URL = {https://doi-org.crai.referencistas.com/10.1177/109434208800200406},
eprint = {https://doi-org.crai.referencistas.com/10.1177/109434208800200406},
abstract = {Neural computation is a style of computation that draws inspiration from the way the brain computes. It is an in trinsically collective paradigm characterized by high con nectivity among a very large number of simple pro cessors running in parallel, possibly asynchronously. Methods developed in the theory of many-particle systems can be brought to bear on important conceptual questions about the operation and programming of such computational assemblies. This paper reviews several basic problems that arise in this area: the mathematical formulation of the collective computation done by such a network and of algorithms for programming (“teaching”) them. The importance of phase transitions for understanding the generic behavior of such systems and algorithms is emphasized.}
}

@article{doi:10.1260/026635103322987968,
author = {A. M. Horr and S. R. Asadsajadi and M. Safi},
title = {Design of Large Space Structures with Imperfection Using ANN-Based Simulator},
journal = {International Journal of Space Structures},
volume = {18},
number = {4},
pages = {235–255},
year = {2003g},
doi = {10.1260/026635103322987968},
URL = {https://doi-org.crai.referencistas.com/10.1260/026635103322987968},
eprint = {https://doi-org.crai.referencistas.com/10.1260/026635103322987968},
abstract = {Based on the advanced computational plasticity and an artificial neural network (ANN) simulator, a new design strategy has been presented for large space structures with imperfections. Nonlinear system identification approach has also been greatly spread among the researchers and engineers in the past few years. The neural network simulators as a non-parametric system identification approach present a robust and efficient way to simulate the nonlinear behaviour of engineering systems. In the paper herein an artificial neural network (ANN) simulator, a general back error propagating perceptron, is use to simulate random imperfection for nonlinear dynamic analysis of large space structures. It is also desirable to search for a procedure for wind pressure calculation with accuracy and reliability. In this respect, attention is paid to the advanced computational fluid dynamics (CFD). The use of the advanced CFD analysis can help engineers to estimate the wind pressure for the design of large space structures with complex geometries. The characteristics of the new design method have been shown graphically using a full documented numerical example, which highlights the efficiency of the new simulation method. The purpose of this paper is to present a new design method, which takes into account the effects of imperfection on the resulting dynamic responses of large space structures under gravity, temperature and wind loadings.}
}

@article{doi:10.1038/jcbfm.2009.231,
author = {Clare Howarth and Claire M Peppiatt-Wildman and David Attwell},
title = {The Energy Use Associated with Neural Computation in the Cerebellum},
journal = {Journal of Cerebral Blood Flow & Metabolism},
volume = {30},
number = {2},
pages = {403–414},
year = {2010h},
doi = {10.1038/jcbfm.2009.231},
note = {PMID:19888288},
URL = {https://doi-org.crai.referencistas.com/10.1038/jcbfm.2009.231},
eprint = {https://doi-org.crai.referencistas.com/10.1038/jcbfm.2009.231},
abstract = {The brain’s energy supply determines its information processing power, and generates functional imaging signals, which are often assumed to reflect principal neuron spiking. Using measured cellular properties, we analysed how energy expenditure relates to neural computation in the cerebellar cortex. Most energy is used on information processing by non-principal neurons: Purkinje cells use only 18% of the signalling energy. Excitatory neurons use 73% and inhibitory neurons 27% of the energy. Despite markedly different computational architectures, the granular and molecular layers consume approximately the same energy. The blood vessel area supplying glucose and O2 is spatially matched to energy consumption. The energy cost of storing motor information in the cerebellum was also estimated.}
}

@article{doi:10.1057/ivs.2010.8,
author = {Erwan Le Martelot and Peter J. Bentley},
title = {Novel Visualisation and Analysis of Natural and Complex Systems Using Systemic Computation},
journal = {Information Visualization},
volume = {10},
number = {1},
pages = {1–31},
year = {2011i},
doi = {10.1057/ivs.2010.8},
URL = {https://doi-org.crai.referencistas.com/10.1057/ivs.2010.8},
eprint = {https://doi-org.crai.referencistas.com/10.1057/ivs.2010.8},
abstract = {The study, analysis and understanding of natural processes are difficult tasks considering the complex nature of such processes. In this respect, the visual analysis of such systems can be of great help in the understanding of their behaviour. The increasing power of modern computers enables novel possible uses of computer graphics for such tasks. Previous work introduced systemic computation, a new model of computation and corresponding computer architecture aiming at enabling a clear formalism of natural and complex systems and providing tools for their analysis. Here, we present an online visualisation of dynamic systems based on this novel paradigm. The observation is done at a high level of abstraction, focussing on information flow, interactions and emergent behaviour, and enabling the identification of similarities and differences between models of complex systems. This visualisation framework is then applied to two biological networks: a bistable gene network and a MAPK signalling cascade.}
}

@article{doi:10.1177/1475472X16680447,
author = {P Martínez-Lera and J Christophe and C Schram},
title = {Computation of the self-noise of a controlled-diffusion airfoil based on the acoustic analogy},
journal = {International Journal of Aeroacoustics},
volume = {16},
number = {1–2},
pages = {44–64},
year = {2017j},
doi = {10.1177/1475472X16680447},
URL = {https://doi-org.crai.referencistas.com/10.1177/1475472X16680447},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1475472X16680447},
abstract = {The self-noise of a controlled-diffusion airfoil is computed with several numerical techniques based on the acoustic analogy and involving different degrees of approximation. The flow solution is obtained through an incompressible large eddy simulation. The acoustic field as described by Lighthill’s analogy is computed with a finite element method applied to the exact airfoil geometry, and this solution is compared with results based on a half-plane Green’s function. This problem behaves as a classical trailing-edge noise problem for a wide range of frequencies; however, other mechanisms of sound production become significant at high frequencies. The results highlight the relative strengths and weaknesses of quadrupole- and dipole-based formulations of the acoustic analogy based on incompressible Computational Fluid Dynamics (CFD) results when applied to wall-bounded turbulent flows.}
}

@article{doi:10.1177/0267658308095737,
author = {Corrine McCarthy},
title = {Morphological variability in the comprehension of agreement: an argument for representation over computation},
journal = {Second Language Research},
volume = {24},
number = {4},
pages = {459–486},
year = {2008k},
doi = {10.1177/0267658308095737},
URL = {https://doi-org.crai.referencistas.com/10.1177/0267658308095737},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0267658308095737},
abstract = {Previous accounts of morphological variability disagree over whether its cause is representational or computational in nature. Under a computational account, variability is confined to production; under a representational account, variability extends to comprehension and is qualitatively similar to variability in production. This article presents experimental evidence from the comprehension and production of gender and number agreement in second language (L2) Spanish clitics and adjectives. Intermediate-level participants show variability across comprehension and production; across tasks, masculine defaults are adopted. Advanced-level participants show less variability, although evidence for masculine defaults emerges across tasks. Number agreement proved relatively unproblematic, except in the production of adjective agreement where singular defaults are systematically adopted by intermediate- and advanced-level speakers. The qualitative similarity of variability across comprehension and production supports a representational account; however, previous research disfavours an account based in syntactic deficits. This article argues for a theory of morphological variability that places the representational cause in the morphology, rather than the syntax.}
}

@article{doi:10.1177/0956797612437427,
author = {Martin M. Monti and Lawrence M. Parsons and Daniel N. Osherson},
title = {Thought Beyond Language: Neural Dissociation of Algebra and Natural Language},
journal = {Psychological Science},
volume = {23},
number = {8},
pages = {914–922},
year = {2012l},
doi = {10.1177/0956797612437427},
note = {PMID:22760883},
URL = {https://doi-org.crai.referencistas.com/10.1177/0956797612437427},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0956797612437427},
abstract = {A central question in cognitive science is whether natural language provides combinatorial operations that are essential to diverse domains of thought. In the study reported here, we addressed this issue by examining the role of linguistic mechanisms in forging the hierarchical structures of algebra. In a 3-T functional MRI experiment, we showed that processing of the syntax-like operations of algebra does not rely on the neural mechanisms of natural language. Our findings indicate that processing the syntax of language elicits the known substrate of linguistic competence, whereas algebraic operations recruit bilateral parietal brain regions previously implicated in the representation of magnitude. This double dissociation argues against the view that language provides the structure of thought across all cognitive domains.}
}

@article{doi:10.1243/13506501JET125,
author = {George K Nikas},
title = {Boussinesq-Cerruti functions and a simple technique for substantial acceleration of subsurface stress computations in elastic half-spaces},
journal = {Proceedings of the Institution of Mechanical Engineers, Part J: Journal of Engineering Tribology},
volume = {220},
number = {1},
pages = {19–28},
year = {2006m},
doi = {10.1243/13506501JET125},
URL = {https://doi-org.crai.referencistas.com/10.1243/13506501JET125},
eprint = {https://doi-org.crai.referencistas.com/10.1243/13506501JET125},
abstract = {Abstract The Boussinesq-Cerruti potential functions for the calculation of subsurface stresses and displacements in elastic half-spaces are presented in full and in clear formulation. They were expanded and optimized for fast computational analysis of subsurface stress and displacement fields, including fatigue life computations. A simple technique was developed to accelerate the computations by omitting the stress contribution of parts of the loaded boundary if the said contribution is lower than a predetermined limit. The error of this approximation was defined and formulated for the comparison of various solutions. The technique was applied in a computationally intensive problem involving a rolling-sliding-spinning elastohydrody-namic, elliptical, heavily loaded contact, and three-dimensional subsurface stress and displacement fields were calculated with hundreds of thousands of surface gridpoints and tens of thousands of subsurface gridpoints, followed by the computation of fatigue lives of the contacting solids. Results are presented for a smooth contact, but the method has been applied and is particularly useful for rough contacts as well. The results show a two to ten-fold acceleration of computations, which reduces the computational times by 50-95 per cent for a negligible-to-small loss of accuracy. In real terms, this means reduction of computer time from days to hours or from hours to minutes.}
}

@article{doi:10.1177/00375497221132566,
author = {James Nutaro and Ozgur Ozmen},
title = {Race conditions and data partitioning: risks posed by common errors to reproducible parallel simulations},
journal = {SIMULATION},
volume = {99},
number = {4},
pages = {417–427},
year = {2023n},
doi = {10.1177/00375497221132566},
URL = {https://doi-org.crai.referencistas.com/10.1177/00375497221132566},
eprint = {https://doi-org.crai.referencistas.com/10.1177/00375497221132566},
abstract = {When parallel algorithms for simulation were introduced in the 1970s, their development and use interested only experts in parallel computation. This circumstance changed as multi-core processors became commonplace, putting a parallel computer into the hands of every modeler. A natural outcome is growing interest in parallel simulation among persons not intimately familiar with parallel computing. At the same time, parallel simulation tools continue to be developed with the implicit assumption that the modeler is knowledgeable about parallel programming. The unintended consequence is a rapidly growing number of users of parallel simulation tools that are unlikely to recognize when the interaction of race conditions, partitioning strategies, and simultaneous action in their simulation models make results non-reproducible, thereby calling into question the validity of conclusions drawn from the simulation data. We illustrate the potential dangers of exposing parallel algorithms to users who are not experts in parallel computation with example models constructed using existing parallel simulation tools. By doing so, we hope to refocus tool developers on usability, even if this new focus incurs loss of some performance.}
}

@article{doi:10.1080/19462166.2011.608225,
author = {Vincenzo Pallotta and Rodolfo Delmonte},
title = {Automatic argumentative analysis for interaction mining},
journal = {Argument & Computation},
volume = {2},
number = {2–3},
pages = {77–106},
year = {2011o},
doi = {10.1080/19462166.2011.608225},
URL = {https://doi-org.crai.referencistas.com/10.1080/19462166.2011.608225},
eprint = {https://doi-org.crai.referencistas.com/10.1080/19462166.2011.608225},
abstract = {Interaction mining is about discovering and extracting insightful information from digital conversations, namely those human–human information exchanges mediated by digital network technology. We present in this article a computational model of natural arguments and its implementation for the automatic argumentative analysis of digital conversations, which allows us to produce relevant information to build interaction business analytics applications overcoming the limitations of standard text mining and information retrieval technology. Applications include advanced visualisations and abstractive summaries.}
}

@article{doi:10.1177/105971239400200402,
author = {Julie C. Rutkowska},
title = {Scaling Up Sensorimotor Systems: Constraints from Human Infancy},
journal = {Adaptive Behavior},
volume = {2},
number = {4},
pages = {349–373},
year = {1994p},
doi = {10.1177/105971239400200402},
URL = {https://doi-org.crai.referencistas.com/10.1177/105971239400200402},
eprint = {https://doi-org.crai.referencistas.com/10.1177/105971239400200402},
abstract = {Work in human infancy and behavior-based robotics that grounds intelligent abilities in sensorimotor exchanges between a system and its environment shares recurrent problems of when, whether, and how scaling up from basic to supposedly higher abilities is possible. An action-based model of the infant is introduced that converges with features of independently motivated animat models exploiting emergent functionality and challenges alternatives that invoke conceptual representations. Adaptive change routinely exhibited in infants’ everyday activities outstrips the scaling-up potential of current robotic systems and clarifies effective principles obeyed by naturally intelligent systems. A general form is outlined to subject-environment interaction that “engineers” restructuring of early abilities in the direction of greater anticipation (considered an upper boundary for the competence of concept-free human and animat systems); and an action-based account of the phenomena is provided. This emphasizes the relationship between representation and situated inference and the role of reciprocal constraints between cognitive and physical-motor mechanisms. Finally, this article questions how far typical self organizing connectionist networks take us toward understanding a system that is capable of mapping recurrent viable patterns of activity into more permanent adaptive changes.}
}

@article{doi:10.1177/0272989X221085569,
author = {Peter Shewmaker and Stavroula A. Chrysanthopoulou and Rowan Iskandar and Derek Lake and Earic Jutkowitz},
title = {Microsimulation Model Calibration with Approximate Bayesian Computation in R: A Tutorial},
journal = {Medical Decision Making},
volume = {42},
number = {5},
pages = {557–570},
year = {2022q},
doi = {10.1177/0272989X221085569},
note = {PMID:35311401},
URL = {https://doi-org.crai.referencistas.com/10.1177/0272989X221085569},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0272989X221085569},
abstract = {Mathematical health policy models, including microsimulation models (MSMs), are widely used to simulate complex processes and predict outcomes consistent with available data. Calibration is a method to estimate parameter values such that model predictions are similar to observed outcomes of interest. Bayesian calibration methods are popular among the available calibration techniques, given their strong theoretical basis and flexibility to incorporate prior beliefs and draw values from the posterior distribution of model parameters and hence the ability to characterize and evaluate parameter uncertainty in the model outcomes. Approximate Bayesian computation (ABC) is an approach to calibrate complex models in which the likelihood is intractable, focusing on measuring the difference between the simulated model predictions and outcomes of interest in observed data. Although ABC methods are increasingly being used, there is limited practical guidance in the medical decision-making literature on approaches to implement ABC to calibrate MSMs. In this tutorial, we describe the Bayesian calibration framework, introduce the ABC approach, and provide step-by-step guidance for implementing an ABC algorithm to calibrate MSMs, using 2 case examples based on a microsimulation model for dementia. We also provide the R code for applying these methods.}
}

@article{doi:10.1509/jmkr.46.1.81,
author = {Manoj Thomas and Vicki G. Morwitz},
title = {The Ease-of-Computation Effect: The Interplay of Metacognitive Experiences and Naive Theories in Judgments of Price Differences},
journal = {Journal of Marketing Research},
volume = {46},
number = {1},
pages = {81–91},
year = {2009r},
doi = {10.1509/jmkr.46.1.81},
URL = {https://doi-org.crai.referencistas.com/10.1509/jmkr.46.1.81},
eprint = {https://doi-org.crai.referencistas.com/10.1509/jmkr.46.1.81},
abstract = {Consumers’ judgments of the magnitude of numerical differences are influenced by the ease of mental computations. The results from a set of experiments show that ease of computation can affect judgments of the magnitude of price differences, discount magnitudes, and brand choices. Participants seem to believe that it is easier to judge the size of a larger difference than that of a smaller difference. In the absence of appropriate corrective steps, this naive belief can lead to systematic biases in judgments. For example, when presented with two pairs of numbers, participants incorrectly judged the magnitude of the difference to be smaller for pairs with difficult computations (e.g., 4.97 – 3.96, an arithmetic difference of 1.01) than for pairs with easy computations (e.g., 5.00 – 4.00, an arithmetic difference of 1.00). The effect does not manifest when judgments do not entail mental computations or when participants are made aware that the ease or difficulty is caused by computational complexity. Furthermore, this effect is mitigated when participants’ prior experience is manipulated in a learning phase of the experiment. The results have implications for buyers and sellers and for understanding the role of metacognitive experiences in numerical judgments.}
}

@article{doi:10.1177/0956797617708234,
author = {Charlotte Van den Driessche and Mikaël Bastian and Hugo Peyre and Coline Stordeur and Éric Acquaviva and Sara Bahadori and Richard Delorme and Jérôme Sackur},
title = {Attentional Lapses in Attention-Deficit/Hyperactivity Disorder: Blank Rather Than Wandering Thoughts},
journal = {Psychological Science},
volume = {28},
number = {10},
pages = {1375–1386},
year = {2017s},
doi = {10.1177/0956797617708234},
note = {PMID:28800281},
URL = {https://doi-org.crai.referencistas.com/10.1177/0956797617708234},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0956797617708234},
abstract = {People with attention-deficit/hyperactivity disorder (ADHD) have difficulties sustaining their attention on external tasks. Such attentional lapses have often been characterized as the simple opposite of external sustained attention, but the different types of attentional lapses, and the subjective experiences to which they correspond, remain unspecified. In this study, we showed that unmedicated children (ages 6–12) with ADHD, when probed during a standard go/no-go task, reported more mind blanking (a mental state characterized by the absence of reportable content) than did control participants. This increase in mind blanking happened at the expense of both focused and wandering thoughts. We also found that methylphenidate reverted the level of mind blanking to baseline (i.e., the level of mind blanking reported by control children without ADHD). However, this restoration led to mind wandering more than to focused attention. In a second experiment, we extended these findings to adults who had subclinical ADHD. These results suggest that executive functions impaired in ADHD are required not only to sustain external attention but also to maintain an internal train of thought.}
}

@article{doi:10.3233/IA-2011-0001,
author = {Leonardo Vanneschi and Luca Mussi and Stefano Cagnoni},
title = {Hot topics in Evolutionary Computation},
journal = {Intelligenza Artificiale},
volume = {5},
number = {1},
pages = {5–17},
year = {2011t},
doi = {10.3233/IA-2011-0001},
URL = {https://doi-org.crai.referencistas.com/10.3233/IA-2011-0001},
eprint = {https://doi-org.crai.referencistas.com/10.3233/IA-2011-0001},
abstract = {We introduce the special issue on Evolutionary Computation (EC) reporting a non-exhaustive list of topics which have recently attracted much interest from the EC community, with particular regard to the ones dealt with by the papers included in this issue: EC research, hybrid neuro-evolutionary systems and synergies between EC and complex systems. In addition, we introduce a more technological emerging topic: the parallel implementation of evolutionary and Swarm Intelligence algorithms on graphics processor units (GPUs), by which new applications of evolutionary algorithms have been made possible, even in real-time environments.}
}

