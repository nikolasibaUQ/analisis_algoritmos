@article{doi:10.1177/1094342009347892,
author = {Mehmet Belgin and Godmar Back and Calvin J. Ribbens},
title = {Operation Stacking for Ensemble Computations With Variable Convergence},
journal = {The International Journal of High Performance Computing Applications},
volume = {24},
number = {2},
pages = {194–212},
year = {2010a},
doi = {10.1177/1094342009347892},
URL = {https://doi-org.crai.referencistas.com/10.1177/1094342009347892},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342009347892},
abstract = {Sparse matrix operations achieve only small fractions of peak CPU speeds because of the use of specialized, index-based matrix representations, which degrade cache utilization by imposing irregular memory accesses and increasing the number of overall accesses. Compounding the problem, the small number of floating-point operations in a single sparse iteration leads to low floating-point pipeline utilization. Operation stacking addresses these problems for large ensemble computations that solve multiple systems of linear equations with identical sparsity structure. By combining the data of multiple problems and solving them as one, operation stacking improves locality, reduces cache misses, and increases floating-point pipeline utilization. Operation stacking also requires less memory bandwidth because it involves fewer index array accesses. In this paper we present the Operation Stacking Framework (OSF), an object-oriented framework that provides runtime and code generation support for the development of stacked iterative solvers. OSF’s runtime component provides an iteration engine that supports efficient ejection of converged problems from the stack. It separates the specific solver algorithm from the coding conventions and data representations that are necessary to implement stacking. Stacked solvers created with OSF can be used transparently without requiring significant changes to existing applications. Our results show that stacking can provide speedups up to 1.94× with an average of 1.46×, even in scenarios in which the number of iterations required to converge varies widely within a stack of problems. Our evaluation shows that these improvements correlate with better cache utilization, improved floating-point utilization, and reduced memory accesses.}
}

@article{doi:10.1177/109434209200600303,
author = {Jean-Philippe Brunet and S. Lennart Johnsson},
title = {All-To-All Broadcast and Applications On the Connection Machine},
journal = {The International Journal of Supercomputing Applications},
volume = {6},
number = {3},
pages = {241–256},
year = {1992b},
doi = {10.1177/109434209200600303},
URL = {https://doi-org.crai.referencistas.com/10.1177/109434209200600303},
eprint = {https://doi-org.crai.referencistas.com/10.1177/109434209200600303},
abstract = {An all-to-all broadcast algorithm that exploits concur rent communication on all channels of the Connection Machine system CM-200 binary cube network is de scribed. Issues in integrating a physical all-to-all broad cast between processing nodes into a language envi ronment using a global address space are discussed. Timings for the physical broadcast between nodes and for the virtual broadcast are given. The peak data transfer rate for the physical broadcast on a CM-200 is 5.9 gigabytes/sec, and the peak rate for the virtual broadcast is 31 gigabytes/sec. Array reshaping is an effective performance optimization technique. An ex ample is given where reshaping improved perfor mance by a factor of 7 by reducing the amount of local data motion. We also show how to exploit symmetry for computation of an interaction matrix using the all- to-all broadcast function. Further optimizations are suggested for N-body-type calculations. Using the all- to-all broadcast function, a peak rate of 9.3 GFLOPS/ sec has been achieved for the N-body computations in 32-bit precision on a 2,048 node Connection Machine system CM-200.}
}

@article{doi:10.1177/1478077116638945,
author = {Andre Chaszar and Sam Conrad Joyce},
title = {Generating freedom: Questions of flexibility in digital design and architectural computation},
journal = {International Journal of Architectural Computing},
volume = {14},
number = {2},
pages = {167–181},
year = {2016c},
doi = {10.1177/1478077116638945},
URL = {https://doi-org.crai.referencistas.com/10.1177/1478077116638945},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1478077116638945},
abstract = {Generative processes and generative design approaches are topics of continuing interest and debate within the realms of architectural design and related fields. While they are often held up as giving designers the opportunity (the freedom) to explore far greater numbers of options/alternatives than would otherwise be possible, questions also arise regarding the limitations of such approaches on the design spaces explored, in comparison with more conventional, human-centric design processes. This article addresses the controversy with a specific focus on parametric-associative modelling and genetic programming methods of generative design. These represent two established contenders within the pool of procedural design approaches gaining increasingly wide acceptance in architectural computational research, education and practice. The two methods are compared and contrasted to highlight important differences in freedoms and limitations they afford, with respect to each other and to ‘manual’ design. We conclude that these methods may be combined with an appropriate balance of automation and human intervention to obtain ‘optimal’ design freedom, and we suggest steps towards finding that balance.}
}

@article{doi:10.1177/1536867X1201100401,
author = {Rhian M. Daniel and Bianca L. De Stavola and Simon N. Cousens},
title = {Gformula: Estimating Causal Effects in the Presence of Time-Varying                     Confounding or Mediation using the G-Computation Formula},
journal = {The Stata Journal},
volume = {11},
number = {4},
pages = {479–517},
year = {2011d},
doi = {10.1177/1536867X1201100401},
URL = {https://doi-org.crai.referencistas.com/10.1177/1536867X1201100401},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1536867X1201100401},
abstract = {This article describes a new command, gformula, that is an implementation of the g-computation procedure. It is used to estimate the causal effect of time-varying exposures on an outcome in the presence of time-varying confounders that are themselves also affected by the exposures. The procedure also addresses the related problem of estimating direct and indirect effects when the causal effect of the exposures on an outcome is mediated by intermediate variables, and in particular when confounders of the mediator–outcome relationships are themselves affected by the exposures. A brief overview of the theory and a description of the command and its options are given, and illustrations using two simulated examples are provided.}
}

@article{doi:10.1177/1073858409354384,
author = {Gustavo Deco and Maurizio Corbetta},
title = {The Dynamical Balance of the Brain at Rest},
journal = {The Neuroscientist},
volume = {17},
number = {1},
pages = {107–123},
year = {2011e},
doi = {10.1177/1073858409354384},
note = {PMID:21196530},
URL = {https://doi-org.crai.referencistas.com/10.1177/1073858409354384},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1073858409354384},
abstract = {The authors review evidence that spontaneous, that is, not stimulus or task driven, activity in the brain at the level of large-scale neural systems is not noise, but orderly and organized in a series of functional networks that maintain, at all times, a high level of coherence. These networks of spontaneous activity correlation or resting state networks (RSN) are closely related to the underlying anatomical connectivity, but their topography is also gated by the history of prior task activation. Network coherence does not depend on covert cognitive activity, but its strength and integrity relates to behavioral performance. Some RSN are functionally organized as dynamically competing systems both at rest and during tasks. Computational studies show that one of such dynamics, the anticorrelation between networks, depends on noise-driven transitions between different multistable cluster synchronization states. These multistable states emerge because of transmission delays between regions that are modeled as coupled oscillators systems. Large-scale systems dynamics are useful for keeping different functional subnetworks in a state of heightened competition, which can be stabilized and fired by even small modulations of either sensory or internal signals.}
}

@article{doi:10.1177/1094342019849618,
author = {John M Dennis and Brian Dobbins and Christopher Kerr and Youngsung Kim},
title = {Optimizing the HOMME dynamical core for multicore platforms},
journal = {The International Journal of High Performance Computing Applications},
volume = {33},
number = {5},
pages = {1030–1045},
year = {2019f},
doi = {10.1177/1094342019849618},
URL = {https://doi-org.crai.referencistas.com/10.1177/1094342019849618},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342019849618},
abstract = {The approach of the next-generation computing platforms offers a tremendous opportunity to advance the state-of-the-art in global atmospheric dynamical models. We detail our incremental approach to utilize this emerging technology by enhancing concurrency within the High-Order Method Modeling Environment (HOMME) atmospheric dynamical model developed at the National Center for Atmospheric Research (NCAR). The study focused on improvements to the performance of HOMME which is a Fortran 90 code with a hybrid (MPIOpenMP) programming model. The article describes the changes made to the use of message passing interface (MPI) and OpenMP as well as single-core optimizations to achieve significant improvements in concurrency and overall code performance. For our optimization studies, we utilize the “Cori” system with an Intel Xeon Phi Knights Landing processor deployed at the National Energy Research Supercomputing Center and the “`Cheyenne” system with an Intel Xeon Broadwell processor installed at the NCAR. The results from the studies, using “workhorse” configurations performed at NCAR, show that these changes have a transformative impact on the computational performance of HOMME. Our improvements have shown that we can effectively increase potential concurrency by efficiently threading the vertical dimension. Further, we have seen a factor of two overall improvement in the computational performance of the code resulting from the single-core optimizations. Most notably from the work is that our incremental approach allows for high-impact changes without disrupting existing scientific productivity in the HOMME community.}
}

@article{doi:10.1177/1077546309341137,
author = {Venkatesh Deshmukh},
title = {Approximate Stability Analysis and Computation of Solutions of Nonlinear Delay Differential Algebraic Equations with Time Periodic Coefficients},
journal = {Journal of Vibration and Control},
volume = {16},
number = {7–8},
pages = {1235–1260},
year = {2010g},
doi = {10.1177/1077546309341137},
URL = {https://doi-org.crai.referencistas.com/10.1177/1077546309341137},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1077546309341137},
abstract = {Approximate stability analysis of nonlinear delay differential algebraic equations (DDAEs) with periodic coefficients is proposed with a geometric interpretation of evolution of the linearized system. Firstly, a numerical algorithm based on direct integration by expansion in terms of Chebyshev polynomials is derived for linear analysis. The proposed algorithm is shown to have deeper connections with and be computationally less cumbersome than the solution of the underlying semi-explicit system via a similarity transformation. The stability of time periodic DDAE systems is characterized by the spectral radius of a “monodromy matrix”, which is a finite-dimensional approximation of a compact infinite-dimensional operator. The monodromy matrix is essentially a map of the Chebyshev coefficients (or collocation vector) of the state from the delay interval to the next adjacent interval of time. The computations are entirely performed with the original system to avoid cumbersome transformations associated with the semi-explicit form of the system. Next, two computational algorithms, the first based on perturbation series and the second based on Chebyshev spectral collocation, are detailed to obtain solutions of nonlinear DDAEs with periodic coefficients for consistent initial functions.}
}

@article{doi:10.3233/ISB-00180,
author = {Marylens     Hernández Guía and Abel González Pérez and Vladimir Espinosa Angarica and Ana T. Vasconcelos and Julio Collado-Vides},
title = {Complementing Computationally Predicted Regulatory Sites in     Tractor_DB Using a Pattern Matching Approach},
journal = {In Silico Biology},
volume = {5},
number = {2},
pages = {209–219},
year = {2005h},
doi = {10.3233/ISB-00180},
URL = {https://doi-org.crai.referencistas.com/10.3233/ISB-00180},
eprint = {https://doi-org.crai.referencistas.com/10.3233/ISB-00180},
abstract = {Prokaryotic genomes annotation has focused on genes location and function. The lack of regulatory information has limited the knowledge on cellular transcriptional regulatory networks. However, as more phylogenetically close genomes are sequenced and annotated, the implementation of phylogenetic footprinting strategies for the recognition of regulators and their regulons becomes more important. In this paper we describe a comparative genomics approach to the prediction of new gamma-proteobacterial regulon members. We take advantage of the phylogenetic proximity of Escherichia coli and other 16 organisms of this subdivision and the intensive search of the space sequence provided by a pattern-matching strategy. Using this approach we complement predictions of regulatory sites made using statistical models currently stored in Tractor_DB, and increase the number of transcriptional regulators with predicted binding sites up to 86. All these computational predictions may be reached at Tractor_DB (www.bioinfo.cu/Tractor_DB, www.tractor.lncc.br, www.ccg.unam.mx/Computational_Genomics/tractorDB/). We also take a first step in this paper towards the assessment of the conservation of the architecture of the regulatory network in the gamma-proteobacteria through evaluating the conservation of the overall connectivity of the network.}
}

@article{doi:10.1243/0957650971537105,
author = {L He},
title = {Computation of unsteady flow through steam turbine blade rows at partial admission},
journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
volume = {211},
number = {3},
pages = {197–205},
year = {1997i},
doi = {10.1243/0957650971537105},
URL = {https://doi-org.crai.referencistas.com/10.1243/0957650971537105},
eprint = {https://doi-org.crai.referencistas.com/10.1243/0957650971537105},
abstract = {Abstract Partial admission in the steam turbine is associated with strong unsteady flow effects on aerodynamic performance. This paper presents a first-of-its-kind computational study of the problem. The unsteady flow field in multiple blade passages and multiple blade rows is governed by the quasi three-dimensional unsteady Navier-Stokes equations, closed by a mixing-length turbulence model. The partial admission is introduced by blocking one segmental arc (or several segmental arcs) of the inlet guide vane of the first stage. The flow equations are solved by using a time-dependent finite volume method. The calculated unsteady force on rotor blades for a turbine stage at partial admission compares well with the corresponding experimental data. The present results show that a cyclic pumping and sucking phenomenon occurs in the rotor blade row of the first stage, resulting in large unsteady loading and marked mixing loss. For a single stage at a given admission rate, a blocking arrangement with two flow segments is shown to be much more detrimental than one arc of admission, because of the extra mixing loss. The results for a two-stage case, however, suggest that the decaying rate of circumferential non-uniformities could be far more important for performance. For this reason, an enhanced mixing loss in the first stage might be beneficial to the overall efficiency of a multistage turbine.}
}

@article{doi:10.1038/jcbfm.2012.35,
author = {Clare Howarth and Padraig Gleeson and David Attwell},
title = {Updated Energy Budgets for Neural Computation in the Neocortex and Cerebellum},
journal = {Journal of Cerebral Blood Flow & Metabolism},
volume = {32},
number = {7},
pages = {1222–1232},
year = {2012j},
doi = {10.1038/jcbfm.2012.35},
note = {PMID:22434069},
URL = {https://doi-org.crai.referencistas.com/10.1038/jcbfm.2012.35},
eprint = {https://doi-org.crai.referencistas.com/10.1038/jcbfm.2012.35},
abstract = {The brain’s energy supply determines its information processing power, and generates functional imaging signals. The energy use on the different subcellular processes underlying neural information processing has been estimated previously for the grey matter of the cerebral and cerebellar cortex. However, these estimates need reevaluating following recent work demonstrating that action potentials in mammalian neurons are much more energy efficient than was previously thought. Using this new knowledge, this paper provides revised estimates for the energy expenditure on neural computation in a simple model for the cerebral cortex and a detailed model of the cerebellar cortex. In cerebral cortex, most signaling energy (50%) is used on postsynaptic glutamate receptors, 21% is used on action potentials, 20% on resting potentials, 5% on presynaptic transmitter release, and 4% on transmitter recycling. In the cerebellar cortex, excitatory neurons use 75% and inhibitory neurons 25% of the signaling energy, and most energy is used on information processing by non-principal neurons: Purkinje cells use only 15% of the signaling energy. The majority of cerebellar signaling energy use is on the maintenance of resting potentials (54%) and postsynaptic receptors (22%), while action potentials account for only 17% of the signaling energy use.}
}

@article{doi:10.1177/109434209200600403,
author = {S. Lennart Johnsson and Luis F. Ortiz},
title = {Local Basic Linear Algebra Subroutines (Lblas) for Distributed Memory Architectures and Languages With Array Syntax},
journal = {The International Journal of Supercomputing Applications},
volume = {6},
number = {4},
pages = {322–350},
year = {1992k},
doi = {10.1177/109434209200600403},
URL = {https://doi-org.crai.referencistas.com/10.1177/109434209200600403},
eprint = {https://doi-org.crai.referencistas.com/10.1177/109434209200600403},
abstract = {We describe a subset of the level-1, level-2, and level-3 BLAS implemented for each node of the Connection Machine system CM-200. The routines, collectively called LBLAS, have interfaces consistent with lan guages with an array syntax such as Fortran 90. One novel feature, important for distributed memory archi tectures, is the capability of performing computations on multiple instances of objects in a single call. The number of instances and their allocation across mem ory units, and the strides for the different axes within the local memories, are derived from an array descrip tor that contains type, shape, and data distribution in formation. Another novel feature of the LBLAS is a se lection of loop order for rank-1 updates and matrix- matrix multiplication based on array shapes, strides, and DRAM page faults. The peak efficiencies for the routines are in excess of 75%. Matrix-vector multiplica tion achieves a peak efficiency of 92%. The optimiza tion of loop ordering has a success rate exceeding 99.8% for matrices for which the sum of the lengths of the axes is at most 60. The success rate is even higher for all possible matrix shapes. The performance loss when a nonoptimal choice is made is less than ∼15% of peak and typically less than 1% of peak. We also show that the performance gain for high-rank updates may be as much as a factor of 6 over rank-1 updates.}
}

@article{doi:10.1177/2336825X1902700303,
author = {Pinja Lehtonen},
title = {How Quantum Ontology and Q Methodology Can Revitalise Agency in IR},
journal = {New Perspectives},
volume = {27},
number = {3},
pages = {37–61},
year = {2019l},
doi = {10.1177/2336825X1902700303},
URL = {https://doi-org.crai.referencistas.com/10.1177/2336825X1902700303},
eprint = {https://doi-org.crai.referencistas.com/10.1177/2336825X1902700303},
abstract = {This article brings Alexander Wendt’s (2015) ‘quantum social ontology’ into the realm of empirical International Relations (IR) research by coupling it with Q methodology. It shows how Wendt’s ontology and Q methodology share a central interest in complex agency and are inherently allied in terms of principles and purposes. The quantum view has catalysed conversation within IR and social sciences more broadly, but that debate has remained almost exclusively on a theoretical level. This article shows that there is potential for empirical research in this area regardless of whether one considers the quantum view to be an analogy or ontological reality. Q methodology’s grounding ideas align with quantum physics and the quantum social ontology, e.g. in the fashion it conceives of subjective states of mind and their measurement. Practical examples of Q methodological work are presented to illustrate the quantum concepts in a social scientific setting. The article argues for a broader study of political subjectivity within IR through a notion of personhood, which opens up vast potentialities for agency as well as for breaking free of determinism, and fixed notions of human nature as well as ostensibly fixed understandings of advantaged or disadvantaged subject positions.}
}

@article{doi:10.1177/1094342004038951,
author = {John Mellor-Crummey and John Garvin},
title = {Optimizing Sparse Matrix–Vector Product Computations Using Unroll                and Jam},
journal = {The International Journal of High Performance Computing Applications},
volume = {18},
number = {2},
pages = {225–236},
year = {2004m},
doi = {10.1177/1094342004038951},
URL = {https://doi-org.crai.referencistas.com/10.1177/1094342004038951},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342004038951},
abstract = {Large-scale scientific applications frequently compute sparse matrix–vector products in their computational core. For this reason, techniques for computing sparse matrix– vector products efficiently on modern architectures are important. In this paper we describe a strategy for improving the performance of sparse matrix–vector product computations using a loop transformation known as unrollandjam. We describe a novel sparse matrix representation that enables us to apply this transformation. Our approach is best suited for sparse matrices that have rows with a small number of predictable lengths. This work was motivated by sparse matrices that arise in SAGE, an application from Los Alamos National Laboratory. We evaluate the performance benefits of our approach using sparse matrices produced by SAGE for a pair of sample inputs. We show that our strategy is effective for improving sparse matrix–vector product performance using these matrices on MIPS R12000, Alpha Ev67, IBM Power 3, and Itanium 2 processors. Our measurements show that for this class of sparse matrices, our strategy improves sparse matrix–vector product performance from a low of 41% on MIPS to well over a factor of 2 on Itanium.}
}

@article{doi:10.1177/0143624415615328,
author = {Anna Parkin and Andrew Mitchell and David Coley},
title = {A new way of thinking about environmental building standards: Developing and demonstrating a client-led zero-energy standard},
journal = {Building Services Engineering Research and Technology},
volume = {37},
number = {4},
pages = {413–430},
year = {2016n},
doi = {10.1177/0143624415615328},
URL = {https://doi-org.crai.referencistas.com/10.1177/0143624415615328},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0143624415615328},
abstract = {There are over 70 low energy and carbon standards in use around the world. None of these standards have been designed by the clients who pay for and occupy the buildings in question. In this work, the client was asked to define the building code for the construction of a new 2800 m2 building via a structured survey. The resulting zero-energy standard simply required the building to incur no energy utility bill. One year of monitoring of the completed building was used to see if the standard had been met. The result of this work is a new way of thinking about environmental building standards that solves many of the issues of obtaining and maintaining buy-in from the client. Practical application: This is the first time that the client has played a key role in the definition of a low-energy building standard. Measured energy consumption and renewable energy generation data are presented and demonstrate that the zero-energy criteria were successfully met. This work is important as it shows that the client can have a meaningful input into the design of an environmental standard. The paper should be of interest to architects, engineers, building energy researchers and those interested in methods that can be used to reduce the energy demand of buildings.}
}

@article{doi:10.1177/0049124119826159,
author = {Krisztián Pósch},
title = {Testing Complex Social Theories With Causal Mediation Analysis and G-Computation: Toward a Better Way to Do Causal Structural Equation Modeling},
journal = {Sociological Methods & Research},
volume = {50},
number = {3},
pages = {1376–1406},
year = {2021o},
doi = {10.1177/0049124119826159},
URL = {https://doi-org.crai.referencistas.com/10.1177/0049124119826159},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0049124119826159},
abstract = {Complex social scientific theories are conventionally tested using linear structural equation modeling (SEM). However, the underlying assumptions of linear SEM often prove unrealistic, making the decomposition of direct and indirect effects problematic. Recent advancements in causal mediation analysis can help to address these shortcomings, allowing for causal inference when a new set of identifying assumptions are satisfied. This article reviews how these ideas can be generalized to multiple mediators, with a focus on the posttreatment confounding and causal ordering cases. Using the potential outcome framework as a rigorous tool for causal inference, the application is the theory of procedural justice policing. Analysis of data from two randomized experiments shows that making similar parametric assumptions to SEMs and using G-computation improve the viability of effect decomposition. The article concludes with a discussion of how causal mediation analysis improves upon SEM and the potential limitation of the methods.}
}

@article{doi:10.1177/09567976211043428,
author = {David Rosenbaum and Moshe Glickman and Stephen M. Fleming and Marius Usher},
title = {The Cognition/Metacognition Trade-Off},
journal = {Psychological Science},
volume = {33},
number = {4},
pages = {613–628},
year = {2022p},
doi = {10.1177/09567976211043428},
note = {PMID:35333670},
URL = {https://doi-org.crai.referencistas.com/10.1177/09567976211043428},
eprint = {https://doi-org.crai.referencistas.com/10.1177/09567976211043428},
abstract = {Integration to boundary is an optimal decision algorithm that accumulates evidence until the posterior reaches a decision boundary, resulting in the fastest decisions for a target accuracy. Here, we demonstrated that this advantage incurs a cost in metacognitive accuracy (confidence), generating a cognition/metacognition trade-off. Using computational modeling, we found that integration to a fixed boundary results in less variability in evidence integration and thus reduces metacognitive accuracy, compared with a collapsing-boundary or a random-timer strategy. We examined how decision strategy affects metacognitive accuracy in three cross-domain experiments, in which 102 university students completed a free-response session (evidence terminated by the participant’s response) and an interrogation session (fixed number of evidence samples controlled by the experimenter). In both sessions, participants observed a sequence of evidence and reported their choice and confidence. As predicted, the interrogation protocol (preventing integration to boundary) enhanced metacognitive accuracy. We also found that in the free-response sessions, participants integrated evidence to a collapsing boundary—a strategy that achieves an efficient compromise between optimizing choice and metacognitive accuracy.}
}

@article{doi:10.1177/00222194221097710,
author = {Rajiv Satsangi and Alexandra R. Raines},
title = {Examining Virtual Manipulatives for Teaching Computations With Fractions to Children With Mathematics Difficulty},
journal = {Journal of Learning Disabilities},
volume = {56},
number = {4},
pages = {295–309},
year = {2023q},
doi = {10.1177/00222194221097710},
note = {PMID:35658741},
URL = {https://doi-org.crai.referencistas.com/10.1177/00222194221097710},
eprint = {https://doi-org.crai.referencistas.com/10.1177/00222194221097710},
abstract = {As digital technology use increases in K–12 education, greater numbers of strategies become available to support students in mathematics. One technology that provides students diverse representations of mathematical concepts is virtual manipulatives. Although instruction featuring representations with physical manipulatives possesses a large body of research, the virtual form lacks comparable study, particularly with young children experiencing mathematics difficulty or identified with a mathematics learning disability. These students often demonstrate challenges learning integral skills such as fractions that subsequently affect their academic success in future years. This study examined the use of virtual manipulatives paired with explicit instruction and a system of least prompts for teaching computations with fractions to three elementary students with mathematics difficulty. A functional relation was found using a single-subject multiple probe design between the treatment condition and students’ accuracy performance solving problems. These results and their implications for the field at-large are discussed.}
}

@article{doi:10.1177/2053951718811843,
author = {Petter Törnberg and Anton Törnberg},
title = {The limits of computation: A philosophical critique of contemporary Big Data research},
journal = {Big Data & Society},
volume = {5},
number = {2},
pages = {2053951718811843},
year = {2018r},
doi = {10.1177/2053951718811843},
URL = {https://doi-org.crai.referencistas.com/10.1177/2053951718811843},
eprint = {https://doi-org.crai.referencistas.com/10.1177/2053951718811843},
abstract = {This paper reviews the contemporary discussion on the epistemological and ontological effects of Big Data within social science, observing an increased focus on relationality and complexity, and a tendency to naturalize social phenomena. The epistemic limits of this emerging computational paradigm are outlined through a comparison with the discussions in the early days of digitalization, when digital technology was primarily seen through the lens of dematerialization, and as part of the larger processes of “postmodernity”. Since then, the online landscape has become increasingly centralized, and the “liquidity” of dematerialized technology has come to empower online platforms in shaping the conditions for human behavior. This contrast between the contemporary epistemological currents and the previous philosophical discussions brings to the fore contradictions within the study of digital social life: While qualitative change has become increasingly dominant, the focus has gone towards quantitative methods; while the platforms have become empowered to shape social behavior, the focus has gone from social context to naturalizing social patterns; while meaning is increasingly contested and fragmented, the role of hermeneutics has diminished; while platforms have become power hubs pursuing their interests through sophisticated data manipulation, the data they provide is increasingly trusted to hold the keys to understanding social life. These contradictions, we argue, are partially the result of a lack of philosophical discussion on the nature of social reality in the digital era; only from a firm metatheoretical perspective can we avoid forgetting the reality of the system under study as we are affected by the powerful social life of Big Data.}
}

@article{doi:10.1177/0040517514540767,
author = {Liwei Wu and Bohong Gu},
title = {Fatigue behaviors of four-step three-dimensional braided composite material: a meso-scale approach computation},
journal = {Textile Research Journal},
volume = {84},
number = {18},
pages = {1915–1930},
year = {2014s},
doi = {10.1177/0040517514540767},
URL = {https://doi-org.crai.referencistas.com/10.1177/0040517514540767},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0040517514540767},
abstract = {This paper reports the computational results of the bending fatigue behaviors of four-step three-dimensional rectangular braided composite materials from a meso-scale approach. A full-size meso-scale model of a four-step three-dimensional braided composite was established to numerically analyze the deformation and damage under cyclic bending loading. The stress distribution, energy absorption, hysteresis loop features and damage morphologies were obtained to explain the structural effects on the deformation and damage of the three-dimensional braided composite material subjected to three-point bending cyclic loading. The influences of the microstructure on the fatigue behaviors have been discussed for designing the three-dimensional braided composite material with high fatigue damage tolerance.}
}

@article{doi:10.1177/027046769201200262,
title = {Computers and Learning: Helping Children Acquire Thinking Skills, Geoffrey Underwood & Jean Underwood. 1990. Basil Blackwell, Cambridge, MA. 209 pages. ISBN: 0-631-15807-3 (hc); 0-631-15808-1 (pb). $42.95 (hc); $14.95 (pb},
journal = {Bulletin of Science, Technology & Society},
volume = {12},
number = {2},
pages = {107–107},
year = {1992t},
doi = {10.1177/027046769201200262},
URL = {https://doi-org.crai.referencistas.com/10.1177/027046769201200262},
eprint = {https://doi-org.crai.referencistas.com/10.1177/027046769201200262}
}

