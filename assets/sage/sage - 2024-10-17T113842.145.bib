@article{doi:10.1177/016146812112300708,
author = {Naa Ammah-Tagoe and Kyra Caspary and Matthew A. Cannady and Eric Greenwald},
title = {Learning to Teach to Argue: Case Studies in Professional Learning in Evidence-Based Science Writing},
journal = {Teachers College Record},
volume = {123},
number = {7},
pages = {1–39},
year = {2021a},
doi = {10.1177/016146812112300708},
URL = {https://doi-org.crai.referencistas.com/10.1177/016146812112300708},
eprint = {https://doi-org.crai.referencistas.com/10.1177/016146812112300708},
abstract = {Background/Context The emphasis on scientific practices articulated by the National Research Council framework and the Next Generation Science Standards requires significant pedagogical shifts for U.S. science teachers. Purpose/Objective/Research Question/Focus of Study This study provides a rare window into the challenges and opportunities teachers encounter as they introduce argument writing into their science classrooms with support from the National Writing Project’s Inquiry into Science Writing project. The purpose of this study is to better understand the teacher-change process so as to inform the development of future professional development efforts. Population/Participants/Subjects Case studies were drawn from a professional development network led by the National Writing Project to support teachers in studying and improving their practice while sharing knowledge and benefiting from the expertise of others. The network included 28 middle school teachers at five writing project sites around the United States; the case studies presented in this article are based on the experiences of three of these teachers. Intervention/Program/Practice The Inquiry into Science Writing Project was a 2-year practitioner-driven professional learning experience seeking to better understand and support student practice around evidence-based science writing. During the duration of the project, teachers taught at least one lesson series culminating in written arguments by students each semester, and participated in two summer institutes, an ongoing national professional learning community, and monthly meetings of their local teacher research group. Research Design The study uses a qualitative comparative case study approach. Data Collection and Analysis The case studies draw on interviews, lesson artifacts, written teacher reflections, and samples of student work. Conclusions/Recommendations The study findings reinforce the complexity of the change process: The relationship between teachers’ knowledge, beliefs, and attitudes and their practice was not linear and unidirectional (i.e., change in attitude leads to change in practice) but rather iterative and mediated by both student work and the external supports they received. These findings confirm the need for sustained learning environments with features that promote enactment and reflection on student work to support teacher change. Further, they suggest that professional development providers should think about how to build habits of reflection into their own design processes, allowing space for feedback and learning from practitioners.}
}

@article{doi:10.1177/10943420221116056,
author = {Kazuto Ando and Rahul Bale and ChungGang Li and Satoshi Matsuoka and Keiji Onishi and Makoto Tsubokura},
title = {Digital transformation of droplet/aerosol infection risk assessment realized on “Fugaku” for the fight against COVID-19},
journal = {The International Journal of High Performance Computing Applications},
volume = {36},
number = {5–6},
pages = {568–586},
year = {2022b},
doi = {10.1177/10943420221116056},
URL = {https://doi-org.crai.referencistas.com/10.1177/10943420221116056},
eprint = {https://doi-org.crai.referencistas.com/10.1177/10943420221116056},
abstract = {The fastest supercomputer in 2020, Fugaku, has not only achieved digital transformation of epidemiology in allowing end-to-end, detailed quantitative modeling of COVID-19 transmissions for the first time but also transformed the behavior of the entire Japanese public through its detailed analysis of transmission risks in multitudes of societal situations entailing heavy risks. A novel aerosol simulation methodology was synthesized out of a combination of a new CFD methods meeting industrial demands in the solver, CUBE (Jansson et al., 2019), which not only allowed the simulations to scale massively with high resolution required for micrometer virus-containing aerosol particles but also enabled extremely rapid time-to-solution due to its ability to generate the digital twins representing multitudes of societal situations in a matter of minutes, attaining true overall application high performance; such simulations have been running for the past 1.5°years on Fugaku, cumulatively consuming top supercomputer-class resources and the communicated by the media as well as becoming the basis for official public policies.}
}

@article{doi:10.1177/0037549710373571,
author = {André Berger and Ralf Hoffmann and Ulf Lorenz and Sebastian Stiller},
title = {Online railway delay management: Hardness, simulation and computation},
journal = {SIMULATION},
volume = {87},
number = {7},
pages = {616–629},
year = {2011c},
doi = {10.1177/0037549710373571},
URL = {https://doi-org.crai.referencistas.com/10.1177/0037549710373571},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0037549710373571},
abstract = {Delays in a railway network are a common problem that railway companies face in their daily operations. When a train is delayed, it may either be beneficial to let a connecting train wait so that passengers in the delayed train do not miss their connection, or it may be beneficial to let the connecting train depart on time to avoid further delays. These decisions naturally depend on the global structure of the network, on the schedule, on the passenger routes and on the imposed delays. The railway delay management (RDM) problem (in a broad sense) is to decide which trains have to wait for connecting trains and which trains have to depart on time. The offline version (i.e. when all delays are known in advance) is already NP-hard for very special networks. In this paper we show that the online railway delay management (ORDM) problem is PSPACE-hard. This result justifies the need for a simulation approach to evaluate wait policies for ORDM. For this purpose we present TOPSU—RDM, a simulation platform for evaluating and comparing different heuristics for the ORDM problem with stochastic delays. Our novel approach is to separate the actual simulation and the program that implements the decision-making policy, thus enabling implementations of different heuristics to ‘“compete”’ on the same instances and delay distributions. We also report on computational results indicating the worthiness of developing intelligent wait policies. For RDM and other logistic planning processes, it is our goal to bridge the gap between theoretical models, which are accessible to theoretical analysis, but are often too far away from practice, and the methods which are used in practice today, whose performance is almost impossible to measure.}
}

@article{doi:10.1177/0013164404266386,
author = {Lee J. Cronbach and Richard J. Shavelson},
title = {My Current Thoughts on Coefficient Alpha and Successor Procedures},
journal = {Educational and Psychological Measurement},
volume = {64},
number = {3},
pages = {391–418},
year = {2004d},
doi = {10.1177/0013164404266386},
URL = {https://doi-org.crai.referencistas.com/10.1177/0013164404266386},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0013164404266386},
abstract = {In 1997, noting that the 50th anniversary of the publication of “Coefficient Alpha and the Internal Structure of Tests” was fast approaching, Lee Cronbach planned what have become the notes published here. His aimwas to point out theways in which his views on coefficient alpha had evolved, doubting nowthat the coefficientwas the bestway of judging the reliability of an instrument to which it was applied. Tracing in these notes, in vintage Cronbach style, his thinking before, during, and after the publication of the alpha paper, his “current thoughts” on coefficient alpha are that alpha covers only a small perspective of the range of measurement uses for which reliability information is needed and that it should be viewed within a much larger system of reliability analysis, generalizability theory.}
}

@article{doi:10.3102/00028312037003747,
author = {Sharon J. Derry and Joel R. Levin and Helen P. Osana and Melanie S. Jones and Michael Peterson},
title = {Fostering Students’ Statistical and Scientific Thinking: Lessons Learned                From an Innovative College Course},
journal = {American Educational Research Journal},
volume = {37},
number = {3},
pages = {747–773},
year = {2000e},
doi = {10.3102/00028312037003747},
URL = {https://doi-org.crai.referencistas.com/10.3102/00028312037003747},
eprint = {https://doi-org.crai.referencistas.com/10.3102/00028312037003747},
abstract = {Current research and theory indicate that college students’ scientific and statistical reasoning skills are deficient, but can be improved through instruction. Accordingly, an innovative statistics course was developed for the undergraduate education curriculum at the University of Wisconsin—Madison. The course promoted the idea “that the purpose of statistics is to organize a useful argument from quantitative evidence based on a form of principled rhetoric” (Abelson, 1995, pp. xiii). Most instruction was anchored to mentored, small-group collaborative activities that simulated complex, real-life problem solving. In conjunction with the second offering, evidence of student growth was obtained from pre- and post-course interviews designed to assess students’ ability to reason with statistical evidence from everyday sources. Both quantitative and qualitative analyses indicated that students made meaningful gains in their ability to reason statistically. Analyses also pointed to specific conceptual confusions, some related to course design. Students’ reactions Io the course were variable.}
}

@article{doi:10.1177/0263276420966386,
author = {M. Beatrice Fazi},
title = {Beyond Human: Deep Learning, Explainability and Representation},
journal = {Theory, Culture & Society},
volume = {38},
number = {7–8},
pages = {55–77},
year = {2021f},
doi = {10.1177/0263276420966386},
URL = {https://doi-org.crai.referencistas.com/10.1177/0263276420966386},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0263276420966386},
abstract = {This article addresses computational procedures that are no longer constrained by human modes of representation and considers how these procedures could be philosophically understood in terms of ‘algorithmic thought’. Research in deep learning is its case study. This artificial intelligence (AI) technique operates in computational ways that are often opaque. Such a black-box character demands rethinking the abstractive operations of deep learning. The article does so by entering debates about explainability in AI and assessing how technoscience and technoculture tackle the possibility to ‘re-present’ the algorithmic procedures of feature extraction and feature learning to the human mind. The article thus mobilises the notion of incommensurability (originally developed in the philosophy of science) to address explainability as a communicational and representational issue, which challenges phenomenological and existential modes of comparison between human and algorithmic ‘thinking’ operations.}
}

@article{doi:10.1177/09637214211046955,
author = {Evelina Fedorenko and Cory Shain},
title = {Similarity of Computations Across Domains Does Not Imply Shared Implementation: The Case of Language Comprehension},
journal = {Current Directions in Psychological Science},
volume = {30},
number = {6},
pages = {526–534},
year = {2021g},
doi = {10.1177/09637214211046955},
note = {PMID:35295820},
URL = {https://doi-org.crai.referencistas.com/10.1177/09637214211046955},
eprint = {https://doi-org.crai.referencistas.com/10.1177/09637214211046955},
abstract = {Understanding language requires applying cognitive operations (e.g., memory retrieval, prediction, structure building) that are relevant across many cognitive domains to specialized knowledge structures (e.g., a particular language’s lexicon and syntax). Are these computations carried out by domain-general circuits or by circuits that store domain-specific representations? Recent work has characterized the roles in language comprehension of the language network, which is selective for high-level language processing, and the multiple-demand (MD) network, which has been implicated in executive functions and linked to fluid intelligence and thus is a prime candidate for implementing computations that support information processing across domains. The language network responds robustly to diverse aspects of comprehension, but the MD network shows no sensitivity to linguistic variables. We therefore argue that the MD network does not play a core role in language comprehension and that past findings suggesting the contrary are likely due to methodological artifacts. Although future studies may reveal some aspects of language comprehension that require the MD network, evidence to date suggests that those will not be related to core linguistic processes such as lexical access or composition. The finding that the circuits that store linguistic knowledge carry out computations on those representations aligns with general arguments against the separation of memory and computation in the mind and brain.}
}

@article{doi:10.3141/2422-14,
author = {Qiao Ge and Biagio Ciuffo and Monica Menendez},
title = {Comprehensive Approach for the Sensitivity Analysis of High-Dimensional and Computationally Expensive Traffic Simulation Models},
journal = {Transportation Research Record},
volume = {2422},
number = {1},
pages = {121–130},
year = {2014h},
doi = {10.3141/2422-14},
URL = {https://doi-org.crai.referencistas.com/10.3141/2422-14},
eprint = {https://doi-org.crai.referencistas.com/10.3141/2422-14},
abstract = {The reliability of traffic model results is strictly connected to the quality of its calibration. A challenge arising in this context concerns the selection of the most influential input parameters. A model sensitivity analysis should be used with this aim. However, because of the limitations of time and computational resources, a proper sensitivity analysis is rarely performed in common practice. A recent study introduced a methodology based on Gaussian process metamodels for the sensitivity analysis of computationally expensive traffic simulation models. The main limitation was a dependence on model dimensionality. When the model has more than about 15 to 20 parameters, estimation of a Gaussian process metamodel (also known as a Kriging metamodel) may become problematic. In this paper, the Kriging-based approach is coupled with a recently developed approach, quasi-optimized trajectory-based elementary effects (quasi-OTEE), for the sensitivity analysis of computationally expensive models. The quasi-OTEE sensitivity analysis can be used to identify the whole subset of sensitive parameters of a high-dimensional model, and the Kriging-based sensitivity analysis can then be used to refine the analysis and to rank the different parameters of the subset in a more reliable way. Application of this new sequential sensitivity analysis method is illustrated with the Wiedemann-74 car-following model. Results show that the new method requires 40 times fewer model evaluations than a standard variance-based sensitivity analysis to identify the influential parameters and their ranks.}
}

@article{doi:10.1177/003754976400300307,
author = {R.T. Harnett and F.J. Sansom and L.M. Warshawsky},
title = {MIDAS...an Analog Approach to Digital Computation},
journal = {SIMULATION},
volume = {3},
number = {3},
pages = {17–43},
year = {1964i},
doi = {10.1177/003754976400300307},
URL = {https://doi-org.crai.referencistas.com/10.1177/003754976400300307},
eprint = {https://doi-org.crai.referencistas.com/10.1177/003754976400300307}
}

@article{doi:10.1177/10943420221121151,
author = {Marc T Henry de Frahan and Jon S Rood and Marc S Day and Hariswaran Sitaraman and Shashank Yellapantula and Bruce A Perry and Ray W Grout and Ann Almgren and Weiqun Zhang and John B Bell et al.},
title = {PeleC: An adaptive mesh refinement solver for compressible reacting flows},
journal = {The International Journal of High Performance Computing Applications},
volume = {37},
number = {2},
pages = {115–131},
year = {2023j},
doi = {10.1177/10943420221121151},
URL = {https://doi-org.crai.referencistas.com/10.1177/10943420221121151},
eprint = {https://doi-org.crai.referencistas.com/10.1177/10943420221121151},
abstract = {Reacting flow simulations for combustion applications require extensive computing capabilities. Leveraging the AMReX library, the Pele suite of combustion simulation tools targets the largest supercomputers available and future exascale machines. We introduce PeleC, the compressible solver in the Pele suite, and detail its capabilities, including complex geometry representation, chemistry integration, and discretization. We present a comparison of development efforts using both OpenACC and AMReX’s C++ performance portability framework for execution on multiple GPU architectures. We discuss relevant details that have allowed PeleC to achieve high performance and scalability. PeleC’s performance characteristics are measured through relevant simulations on multiple supercomputers. The success of PeleC’s design for exascale is exhibited through demonstration of a 160 billion cell simulation and weak scaling onto 100% of Summit, an NVIDIA-based GPU supercomputer at Oak Ridge National Laboratory. Our results provide confidence that PeleC will enable future combustion science simulations with unprecedented fidelity.}
}

@article{doi:10.1177/0961000616668572,
author = {Shuiqing Huang and Zhengbiao Han and Bo Yang and Ni Ren},
title = {Factor identification and computation in the assessment of information security risks for digital libraries},
journal = {Journal of Librarianship and Information Science},
volume = {51},
number = {1},
pages = {78–94},
year = {2019k},
doi = {10.1177/0961000616668572},
URL = {https://doi-org.crai.referencistas.com/10.1177/0961000616668572},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0961000616668572},
abstract = {This study proposes an objective methodology for identifying and computing the factors relevant to the assessment of information security risks for digital libraries that is also compliant with the ISO 27000 and the GB/T 20984 standards. By introducing a fuzzy comprehensive assessment method and an expert investigation method to the dimensions of assets and threats, this study proposes a model for computing the value of assets and the severity of threats. In the dimension of vulnerabilities, a vulnerability computation model based on the multi-channel weighted average method is proposed. By considering the digital library of a typical public library in China as the object of assessment, this study acquires assessment data by using a combination of a questionnaire survey, an on-site survey and vulnerability scanning. Research findings consisted of the following: (1) the digital library identified a total of 3111 information security risk items; (2) according to the assessment results attained using a combination of the factor identification and computational methodologies proposed here in conjunction with the multiplicative method specified in GB/T 20984, the high-risk (or higher risk) items accounted for 0.9% of all risky items, which is consistent with the status quo in information security risks faced by digital libraries. The analysis showed that the proposed methodology is more scientific than the currently prevailing direct value assignment method.}
}

@article{doi:10.1177/0278364904045478,
author = {Yan-Bin Jia},
title = {Computation on Parametric Curves with an Application in Grasping},
journal = {The International Journal of Robotics Research},
volume = {23},
number = {7–8},
pages = {827–857},
year = {2004l},
doi = {10.1177/0278364904045478},
URL = {https://doi-org.crai.referencistas.com/10.1177/0278364904045478},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0278364904045478},
abstract = {Curved shapes are frequent subjects of maneuvers by the human hand. In robotics, it is well known that antipodal grasps exist on curved objects and guarantee force closure under proper finger contact conditions. This paper presents an efficient algorithm that computes, up to numerical resolution, all pairs of antipodal points on a simple, closed, and twice continuously differentiable plane curve. Dissecting the curve into segments everywhere convex or everywhere concave, the algorithm marches simultaneously on a pair of such segments with provable convergence and interleaves marching with numerical bisection recursively. It makes use of new insights into the differential geometry at two antipodal points. We have avoided resorting to traditional nonlinear programming, which would neither be quite as efficient nor guarantee to find all antipodal points. A byproduct of our result is a procedure that constructs all common tangent lines of two curves, achieving quadratic convergence rate. Dissection and the coupling of marching with bisection constitute an algorithm design scheme potentially applicable to computational problems involving curves and curved shapes.}
}

@article{doi:10.1177/2055207619880671,
author = {Adi Kuntsman and Esperanza Miyake and Sam Martin},
title = {Re-thinking Digital Health: Data, Appisation and the (im)possibility of ‘Opting out’},
journal = {DIGITAL HEALTH},
volume = {5},
number = { },
pages = {2055207619880671},
year = {2019m},
doi = {10.1177/2055207619880671},
note = {PMID:31636917},
URL = {https://doi-org.crai.referencistas.com/10.1177/2055207619880671},
eprint = {https://doi-org.crai.referencistas.com/10.1177/2055207619880671}
}

@article{doi:10.1177/02632764231174811,
author = {Shiqiao Li and Scott Lash},
title = {Against Ontology: Chinese Thought and François Jullien: An Introduction},
journal = {Theory, Culture & Society},
volume = {40},
number = {4–5},
pages = {3–23},
year = {2023n},
doi = {10.1177/02632764231174811},
URL = {https://doi-org.crai.referencistas.com/10.1177/02632764231174811},
eprint = {https://doi-org.crai.referencistas.com/10.1177/02632764231174811},
abstract = {François Jullien wants us to see what thought and life could look like without ontology, promising intellectual riches unavailable in the heavy ontological apparatus we are deeply invested in. The strength of Jullien’s argument comes from a deep and unique alliance between philosophy and Chinese thought, a risky one – incurring predictable disgruntlement from both philosophy and sinology – but nevertheless enduring and productive. This is far from advocating one in place of another, as we are accustomed to do in critical theory in relation to divinity, grand narratives, scientism, modernity, and even stable truths. It is an endeavour to think the ‘unthought of’ of ontology through what Jullien calls a vis-à-vis suspended in productive tension, a dialogue. In philosophy, the other is subsumed in a singular dialectical relationship through oppositions. What Jullien insists on is a doubleness of co-existence in Chinese thought rather than a singularity of dialectics in ontology. If ontology grounds a philosophy that makes a world in its image, and if that image is increasingly untenable as an ecology of the planet, Jullien’s call for a deeper reflexivity in ontology is of enormous significance. This special issue brings both Jullien’s argument and Chinese thought to a forum to explicate what this could mean in multiple fields from art and architecture to anthropology and critical theory.}
}

@article{doi:10.1177/1070496512471947,
author = {Daniel A. Mazmanian and John Jurewitz and Hal T. Nelson},
title = {The Paradox of “Acting Globally While Thinking Locally”: Discordance in Climate Change Adaption Policy},
journal = {The Journal of Environment & Development},
volume = {22},
number = {2},
pages = {186–206},
year = {2013o},
doi = {10.1177/1070496512471947},
URL = {https://doi-org.crai.referencistas.com/10.1177/1070496512471947},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1070496512471947},
abstract = {The paradox motivating this article is why California has acted globally by enacting a comprehensive mitigation policy to reduce the emissions of Greenhouse gases, a true public good since the benefits will be shared across the planet, but has not mustered the will to act locally through the adoption of an equally comprehensive adaptation policy for the state to protect its own public and private assets and interests. We attempt to explain the paradox by identifying what it is that differentiates climate change adaptation from mitigation, both substantively and politically. The paradox notwithstanding, we identify several imaginable adaptation policies and strategies that would be commensurate with individual and collective self-interested behavior.}
}

@article{doi:10.1177/1081286508089845,
author = {Pablo V. Negrón-Marrero and Jeyabal Sivaloganathan},
title = {The Numerical Computation of the Critical Boundary Displacement for Radial Cavitation},
journal = {Mathematics and Mechanics of Solids},
volume = {14},
number = {8},
pages = {696–726},
year = {2009p},
doi = {10.1177/1081286508089845},
URL = {https://doi-org.crai.referencistas.com/10.1177/1081286508089845},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1081286508089845},
abstract = {We study radial solutions of the equations of isotropic elasticity in two dimensions (for a disc) and three dimensions (for a sphere). We describe a numerical scheme for computing the critical boundary displacement for cavitation based on the solution of a sequence of initial value problems for punctured domains. We give examples for specific materials and compare our numerical computations with some previous analytical results. A key observation in the formulation of the method is that the strong—ellipticity condition implies that the specification of the normal component of the Cauchy stress on an inner pre—existing but small cavity, leads to a relation for the radial strain as a function of the circumferential strain. To establish the convergence of the numerical scheme we prove a monotonicity property for the inner deformed radius for punctured balls.}
}

@article{doi:10.1177/0959651813520147,
author = {Hasan A Nozari and Hamed D Banadaki},
title = {Intelligent computationally efficient modelling of multi-input multi-output non-linear dynamical process plants: An industrial steam generator case study},
journal = {Proceedings of the Institution of Mechanical Engineers, Part I: Journal of Systems and Control Engineering},
volume = {228},
number = {5},
pages = {278–294},
year = {2014q},
doi = {10.1177/0959651813520147},
URL = {https://doi-org.crai.referencistas.com/10.1177/0959651813520147},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0959651813520147},
abstract = {Most of the process plants have intrinsic non-linear, time-varying, non-minimum phase and coupling characteristics that result in a highly time-consuming and tedious task to derive complicated non-linear dynamic equations governing on such types of processes. On the other hand, even though such equations can be analytically extracted, they have not worked out in most cases yet. To find a simple fix that can address these hurdles associated with analytical modelling, two types of non-analytical models, namely, simulator and predictor, are proposed based on a computationally efficient technique so-called locally linear neuro-fuzzy modelling. That is, a simulator model as well as a specially structured long-term predictor model that is built based on a sequential arrangement of single-stage predictor models is presented for multi-input multi-output non-linear dynamical process plants and feasibly applied to a real water-tube steam generator for the first time. An adaptive evolving algorithm named linear model tree contributes to estimate the parameters of neuro-fuzzy simulator and predictor models. Furthermore, an order selection method based on Lipschitz theory, whose merit is being totally independent from developing any model prior to commencing tedious modelling trials, is also proposed for the first time by defining a modified Lipschitz index to remedy the order determination problem within the very first steps of black-box non-linear system identification. The blend of such a fully modelling-independent order selection algorithm and a unique type of computationally inexpensive non-linear neuro-fuzzy model lessens the overall computational burden of modelling procedure that represents a great concern in a black-box system identification approach. The recorded data from a real water-tube steam generator operating at Abbot Power plant unit of Champaign, IL, were exploited to carry out experimental modelling in order to reveal the pros and cons of the presented models.}
}

@article{doi:10.1177/1745691619898795,
author = {Deming (Adam) Wang and Martin S. Hagger and Nikos L. D. Chatzisarantis},
title = {Ironic Effects of Thought Suppression: A Meta-Analysis},
journal = {Perspectives on Psychological Science},
volume = {15},
number = {3},
pages = {778–793},
year = {2020r},
doi = {10.1177/1745691619898795},
note = {PMID:32286932},
URL = {https://doi-org.crai.referencistas.com/10.1177/1745691619898795},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1745691619898795},
abstract = {The ironic effect of thought suppression refers to the phenomenon in which individuals trying to rid their mind of a target thought ironically experience greater levels of occurrence and accessibility of the thought compared with individuals who deliberately concentrate on the thought (Wegner, 1994, doi:10.1037/0033-295X.101.1.34). Ironic effects occurring after thought suppression, also known as rebound effects, were consistently detected by previous meta-analyses. However, ironic effects that occur during thought suppression, also known as immediate enhancement effects, were found to be largely absent. In this meta-analysis, we test Wegner’s original proposition that detection of immediate enhancement effects depends on the cognitive load experienced by individuals when enacting thought suppression. Given that thought suppression is an effortful cognitive process, we propose that the introduction of additional cognitive load would compete for the allocation of existing cognitive resources and impair capacity for thought suppression. Studies (k = 31) consistent with Wegner’s original thought-suppression paradigm were analyzed. Consistent with our predictions, rebound effects were observed regardless of cognitive load, whereas immediate enhancement effects were observed only in the presence of cognitive load. We discuss implications in light of ironic-process theory and suggest future thought-suppression research.}
}

@article{doi:10.1177/10943420211006169,
author = {Karl-Robert Wichmann and Martin Kronbichler and Rainald Löhner and Wolfgang A Wall},
title = {A runtime based comparison of highly tuned lattice Boltzmann and finite difference solvers},
journal = {The International Journal of High Performance Computing Applications},
volume = {35},
number = {4},
pages = {370–390},
year = {2021s},
doi = {10.1177/10943420211006169},
URL = {https://doi-org.crai.referencistas.com/10.1177/10943420211006169},
eprint = {https://doi-org.crai.referencistas.com/10.1177/10943420211006169},
abstract = {The aim of this work is a fair and unbiased comparison of a lattice Boltzmann method (LBM) against a finite difference method (FDM) for the simulation of fluid flows. Rather than reporting metrics such as floating point operation rates or memory throughput, our work considers the engineering quest of reaching a desired solution quality with the least computational effort. The specific lattice Boltzmann and finite difference methods selected here are of a very basic nature to emphasize the influence of the fundamentally different approaches. To minimize the skew in the measurements, complex boundary condition schemes and further advanced techniques are avoided and instead both methods are fully explicit, weakly compressible approaches. Due to the highly optimized nature of both codes, different sets of restrictions are imposed by either method. Using the common set of features, two relatively simple test cases in terms of a duct flow and the flow in a lid driven cavity are considered and are tuned to perform optimally with both approaches. As a third test case, a transient flow around a square cylinder is used to demonstrate the applicability to engineering oriented settings and in a temporal domain. The performance of the two methods is found to be very similar with no full advantage for any of the approaches. Overall a tendency toward better performance of the LBM at larger target errors and for indirect benchmark quantities, such as lift and drag, is observed, while the FDM excels at smaller target errors and direct comparisons of velocity and pressure profiles to analytical solutions. Other factors such as the difficulty of setting consistent boundary conditions in the LBM or the effect of stabilization in the FDM are likely to be the most important criteria when searching for a very fast flow solver for practical applications.}
}

@article{doi:10.1111/ldrp.12237,
author = {Menahem Yeari and Anat Lavie},
title = {The Role of Surface Text Processing in Centrality Deficit and Poor Text Comprehension of Adolescents with Attention Deficit Hyperactivity Disorder: A Think–Aloud Study},
journal = {Learning Disabilities Research & Practice},
volume = {36},
number = {1},
pages = {40–55},
year = {2021t},
doi = {10.1111/ldrp.12237},
URL = {https://doi-org.crai.referencistas.com/10.1111/ldrp.12237},
eprint = {https://doi-org.crai.referencistas.com/10.1111/ldrp.12237},
abstract = {The present study employed a think–aloud method to explore the origin of centrality deficit (i.e., poor recall of central ideas) in individuals with Attention Deficit Hyperactivity Disorder (ADHD). Moreover, utilizing the diverse think–aloud responses, we examined the overall quality of text processing employed by individuals with ADHD during reading, in order to shed more light on text–level deficiencies underlying their poor comprehension after reading. To address these goals, adolescents with and without ADHD were asked to state aloud whatever comes to their minds during the reading of two expository texts. After reading, the participants freely recalled text ideas and answered multiple–choice questions on the texts. Compared to controls, participants with ADHD generated fewer responses that reflect deep, efficient text processing, and reinstated fewer prior text ideas, particularly central ones, during reading. Moreover, the proportions of deep processing responses positively associated with participants’ performance on recall and comprehension tasks. These findings suggest that individuals with ADHD exhibit poor text comprehension and memory, particularly of central ideas, because they construct a low–quality, less–connected text representation during reading, and produce fewer, less–elaborated retrieval cues for subsequent tasks after reading.}
}

