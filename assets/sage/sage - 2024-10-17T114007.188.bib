@article{doi:10.1177/1525107115623505,
author = {Yoshio Akiyama and James J. Nolan and Karen G. Weiss and Stacia Gilliard-Matthews},
title = {Lifetime Likelihood Computations With NIBRS},
journal = {Justice Research and Policy},
volume = {16},
number = {2},
pages = {129–146},
year = {2015a},
doi = {10.1177/1525107115623505},
URL = {https://doi-org.crai.referencistas.com/10.1177/1525107115623505},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1525107115623505},
abstract = {This article explores the conditions and assumptions under which it is possible to use National Incident-Based Reporting System (NIBRS) in lifetime crime computations, particularly for nonfatal violent crimes. We describe methods for using NIBRS to study lifetime risk for a variety of crimes and show how researchers and policy makers can apply these methods using readily available software such as Microsoft Excel. Finally, we demonstrate in two different studies how NIBRS can be used to estimate lifetime risk at the state and national levels. In doing so, we introduce the concept of the “average person” in each age–sex–race grouping to calculate the risk of victimization for this hypothetical person only.}
}

@article{doi:10.1177/001440291408000207,
author = {Brian A. Bottge and Xin Ma and Linda Gassaway and Mark Butler and Michael D. Toland},
title = {Detecting and Correcting Fractions Computation Error Patterns},
journal = {Exceptional Children},
volume = {80},
number = {2},
pages = {237–255},
year = {2014b},
doi = {10.1177/001440291408000207},
URL = {https://doi-org.crai.referencistas.com/10.1177/001440291408000207},
eprint = {https://doi-org.crai.referencistas.com/10.1177/001440291408000207},
abstract = {This article describes a follow-up analysis of findings from a randomized study that tested the efficacy of a blended version of Enhanced Anchored Instruction (EAI) designed to improve both the computation and problem-solving performances of middle school students with disabilities. The goals of the secondary analysis were to track overall error patterns of students in computing with fractions and to compare the effects of EAI and Business As Usual (BAU) on making these errors. Results showed that students taught with EAI reduced their errors compared to students in BAU classrooms and that reducing the one common error led to improved performance. Error pattern analysis provided clues about how to modify instructional materials for improving computation with fractions.}
}

@article{doi:10.1177/0305735613517285,
author = {Lori A. Buma and Frank C. Bakker and Raôul R. D. Oudejans},
title = {Exploring the thoughts and focus of attention of elite musicians under pressure},
journal = {Psychology of Music},
volume = {43},
number = {4},
pages = {459–472},
year = {2015c},
doi = {10.1177/0305735613517285},
URL = {https://doi-org.crai.referencistas.com/10.1177/0305735613517285},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0305735613517285},
abstract = {Although musicians often have to perform under high pressure, there is little systematic research into the foci of attention needed to maintain performance in such situations. In the current study, we asked elite musicians to report what they focus on and think about during moments of high pressure, using two retrospective methods (concept mapping and verbal reports). For concept mapping, seven expert teachers from an elite academy of music generated clusters of statements about this issue. In the verbal reports, 44 elite musicians described their thoughts and focus of attention. Concept mapping resulted in six clusters, of which “focus on physical aspects,” “thoughts that give confidence,” and “music-related focus” were shown to be the main foci of attention, together representing 85.2% of all 190 statements generated in the verbal reports. Statements regarding “music-related focus” represented 49.7% of all statements. In conclusion, to maintain a high level of performance under pressure, experienced musicians frequently focus on music-related information, physical aspects, and thoughts that give confidence. Implications and suggestions for future research are discussed.}
}

@article{doi:10.1177/25152459241236149,
author = {Arthur Chatton and Julia M. Rohrer},
title = {The Causal Cookbook: Recipes for Propensity Scores, G-Computation, and Doubly Robust Standardization},
journal = {Advances in Methods and Practices in Psychological Science},
volume = {7},
number = {1},
pages = {25152459241236148},
year = {2024d},
doi = {10.1177/25152459241236149},
URL = {https://doi-org.crai.referencistas.com/10.1177/25152459241236149},
eprint = {https://doi-org.crai.referencistas.com/10.1177/25152459241236149},
abstract = {Recent developments in the causal-inference literature have renewed psychologists’ interest in how to improve causal conclusions based on observational data. A lot of the recent writing has focused on concerns of causal identification (under which conditions is it, in principle, possible to recover causal effects?); in this primer, we turn to causal estimation (how do researchers actually turn the data into an effect estimate?) and modern approaches to it that are commonly used in epidemiology. First, we explain how causal estimands can be defined rigorously with the help of the potential-outcomes framework, and we highlight four crucial assumptions necessary for causal inference to succeed (exchangeability, positivity, consistency, and noninterference). Next, we present three types of approaches to causal estimation and compare their strengths and weaknesses: propensity-score methods (in which the independent variable is modeled as a function of controls), g-computation methods (in which the dependent variable is modeled as a function of both controls and the independent variable), and doubly robust estimators (which combine models for both independent and dependent variables). A companion R Notebook is available at github.com/ArthurChatton/CausalCookbook. We hope that this nontechnical introduction not only helps psychologists and other social scientists expand their causal toolbox but also facilitates communication across disciplinary boundaries when it comes to causal inference, a research goal common to all fields of research.}
}

@article{doi:10.1111/j.1745-6916.2006.00007.x,
author = {Ap Dijksterhuis and Loran F. Nordgren},
title = {A Theory of Unconscious Thought},
journal = {Perspectives on Psychological Science},
volume = {1},
number = {2},
pages = {95–109},
year = {2006e},
doi = {10.1111/j.1745-6916.2006.00007.x},
note = {PMID:26151465},
URL = {https://doi-org.crai.referencistas.com/10.1111/j.1745-6916.2006.00007.x},
eprint = {https://doi-org.crai.referencistas.com/10.1111/j.1745-6916.2006.00007.x},
abstract = {We present a theory about human thought named the unconscious-thought theory (UTT). The theory is applicable to decision making, impression formation, attitude formation and change, problem solving, and creativity. It distinguishes between two modes of thought: unconscious and conscious. Unconscious thought and conscious thought have different characteristics, and these different characteristics make each mode preferable under different circumstances. For instance, contrary to popular belief, decisions about simple issues can be better tackled by conscious thought, whereas decisions about complex matters can be better approached with unconscious thought. The relations between the theory and decision strategies, and between the theory and intuition, are discussed. We end by discussing caveats and future directions.}
}

@article{doi:10.1068/b12979,
author = {Leila Floriani and Paola Magillo},
title = {Algorithms for Visibility Computation on Terrains: A Survey},
journal = {Environment and Planning B: Planning and Design},
volume = {30},
number = {5},
pages = {709–728},
year = {2003f},
doi = {10.1068/b12979},
URL = {https://doi-org.crai.referencistas.com/10.1068/b12979},
eprint = {https://doi-org.crai.referencistas.com/10.1068/b12979},
abstract = {Several environment applications require the computation of visibility information on a terrain. Examples are optimal placement of observation points, line-of-sight communication, and computation of hidden as well as scenic paths. Visibility computations on a terrain may involve either one or many viewpoints, and range from visibility queries (for example, testing whether a given query point is visible), to the computation of structures that encode the visible portions of the surface. In this paper, the authors consider a number of visibility problems on terrains and present an overview of algorithms to tackle such problems on triangulated irregular networks and regular square grids.}
}

@article{doi:10.1177/1045389X14546650,
author = {Ali Ghaffari and Seyed Hassan Hashemabadi and Mahshid Ashtiani},
title = {A review on the simulation and modeling of magnetorheological fluids},
journal = {Journal of Intelligent Material Systems and Structures},
volume = {26},
number = {8},
pages = {881–904},
year = {2015g},
doi = {10.1177/1045389X14546650},
URL = {https://doi-org.crai.referencistas.com/10.1177/1045389X14546650},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1045389X14546650},
abstract = {The magnetorheological fluids are classified as smart materials with controllable rheological properties. The fast growing application of magnetorheological fluids in recent years has increased the demand for simulation and modeling of these fluids. From the invention of magnetorheological fluids up to now, many experimental and also theoretical investigations have been carried out to study these types of smart materials; also many attempts have been made to formulate and simulate their behavior. The aim of this investigation is to present a review on the different models and simulation methods that were applied in the studying of magnetorheological fluids. In this study, the different simulation methods of magnetorheological fluid have been categorized into two general approaches: continuum and discrete phase approaches. The different rheological and structural models of magnetorheological fluids in continuum approach have been summarized in this study. The computational framework of discrete approach and the basic models for magnetorheological fluid in this approach are also discussed.}
}

@article{doi:10.1068/p3811rvw,
author = {Constanze Hesse and Snehlata Jaswal},
title = {Reviews: Sensorimotor Control of Grasping: Physiology and Pathophysiology, Computation, Cognition, and Pylyshyn},
journal = {Perception},
volume = {38},
number = {11},
pages = {1735–1738},
year = {2009h},
doi = {10.1068/p3811rvw},
URL = {https://doi-org.crai.referencistas.com/10.1068/p3811rvw},
eprint = {https://doi-org.crai.referencistas.com/10.1068/p3811rvw}
}

@article{doi:10.1177/1094342018816377,
author = {Niclas Jansson and Rahul Bale and Keiji Onishi and Makoto Tsubokura},
title = {CUBE: A scalable framework for large-scale industrial simulations},
journal = {The International Journal of High Performance Computing Applications},
volume = {33},
number = {4},
pages = {678–698},
year = {2019i},
doi = {10.1177/1094342018816377},
URL = {https://doi-org.crai.referencistas.com/10.1177/1094342018816377},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342018816377},
abstract = {Writing high-performance solvers for engineering applications is a delicate task. These codes are often developed on an application to application basis, highly optimized to solve a certain problem. Here, we present our work on developing a general simulation framework for efficient computation of time-resolved approximations of complex industrial flow problems—Complex Unified Building cube method (CUBE). To address the challenges of emerging, modern supercomputers, suitable data structures and communication patterns are developed and incorporated into CUBE. We use a Cartesian grid together with various immersed boundary (IB) methods to accurately capture moving, complex geometries. The asymmetric workload of the IB is balanced by a predictive dynamic load balancer, and a multithreaded halo exchange algorithm is employed to efficiently overlap communication with computations. Our work also concerns efficient methods for handling the large amount of data produced by large-scale flow simulations, such as scalable parallel I/O, data compression, and in-situ processing.}
}

@article{doi:10.3141/2563-18,
author = {Natalia Ruiz Juri and Rachel M. James and Nan Jiang and Jennifer Duthie and Abdul R. Pinjari and Chandra R. Bhat},
title = {Computation of Skims for Large-Scale Implementations of Integrated Activity-Based and Dynamic Traffic Assignment Models},
journal = {Transportation Research Record},
volume = {2563},
number = {1},
pages = {134–143},
year = {2016j},
doi = {10.3141/2563-18},
URL = {https://doi-org.crai.referencistas.com/10.3141/2563-18},
eprint = {https://doi-org.crai.referencistas.com/10.3141/2563-18},
abstract = {Integrated activity-based model (ABM) and dynamic traffic assignment (DTA) frameworks have emerged as promising tools to support transportation planning and operations, particularly in the context of novel technologies and data sources. This research proposes an approach to characterize the implementation of integrated ABM-DTA models and seeks to facilitate the interpretation and comparison of frameworks and, ultimately, the selection of appropriate tools. The importance of the dimensions considered in this characterization is illustrated through a detailed analysis of the computation of skims. Skims are the level of service (LOS) metric produced by DTA models, and the computation of skims may impact the performance and convergence of ABM-DTA applications. Numerical results from experiments on a regional ABM-DTA model in Austin, Texas, suggest that skims produced at relatively small time steps (10 to 30 min) may lead to a faster integrated model convergence. Finer time-grained skims are also observed to capture sharper temporal peaking patterns in the LOS. This work considers two skim computation methods; the analysis of the results suggests that simpler techniques are adequate, as the inherent variability of travel times from simulation overshadows any gain in precision from more complex methods. This study also uses promising techniques to visualize and analyze the model results, a challenging task in the context of highly disaggregate models and the subject of further research. The insights from this research effort can inform future research on the implementation of ABM-DTA methods and practical applications of existing frameworks.}
}

@article{doi:10.1177/009385487800500301,
author = {Simha F. Landau},
title = {Thought Content of Delinquent and Nondelinquent Young Adults: The Effect of Institutionalization},
journal = {Correctional Psychologist},
volume = {5},
number = {3},
pages = {195–210},
year = {1978k},
doi = {10.1177/009385487800500301},
URL = {https://doi-org.crai.referencistas.com/10.1177/009385487800500301},
eprint = {https://doi-org.crai.referencistas.com/10.1177/009385487800500301},
abstract = {The aim of this study was to investigate several aspects of daily thinking among delinquents and nondelinquents while adequately controlling institutionalization. For this purpose the following groups were selected: institutionalized delinquents (prison inmates), institutionalized nondelinquents (soldiers), noninstitutionalized delinquents (delinquents on probation), and noninstitutionalized nondelinquents (vocational students). The findings show that prisoners are more preoccupied with events occurring within the total institution than are soldiers. However, in both types of institution, the closer the subjects are to release, the greater their preoccupation with events outside the institution. Prisoners demonstrate a higher proportion of contents related to deviance and delinquency and a lower proportion of cognitive-instrumental contents than probationers. However, as they approach release there is a decrease in the former and an increase in the latter in their life-space. Noninstitutionalized subjects demonstrate a higher degree of activity in their daily thinking than their institutionalized counterparts. Both in prison and army in the early and late phases of institutionalization, the degree of activity demonstrated is higher than that in the middle phase. The findings are discussed in relation to the process of prisonization, and a hypothetical model incorporating several variables related to this process is suggested.}
}

@article{doi:10.3233/FI-2018-1749,
author = {Sumit Mishra and Samrat Mondal and Sriparna Saha},
title = {Towards Obtaining Upper Bound on Sensitivity Computation Process for Cluster Validity Measures},
journal = {Fundamenta Informaticae},
volume = {163},
number = {4},
pages = {351–374},
year = {2018l},
doi = {10.3233/FI-2018-1749},
URL = {https://doi-org.crai.referencistas.com/10.3233/FI-2018-1749},
eprint = {https://doi-org.crai.referencistas.com/10.3233/FI-2018-1749},
abstract = {Cluster validity indices are proposed in the literature to measure the goodness of a clustering result. The validity measure provides a value which shows how good or bad the obtained clustering result is, as compared to the actual clustering result. However, the validity measures are not arbitrarily generated. A validity measure should satisfy some of the important properties. However, there are cases when in-spite of satisfying these properties, a validity measure is not able to differentiate the two clustering results correctly. In this regard, sensitivity as a property of validity measure is introduced to capture the differences between the two clustering results. However, sensitivity computation is a computationally expensive task as it requires to explore all the possible combinations of clustering results which are very large in number and these are growing exponentially. So, it is required to compute the sensitivity efficiently. As the possible combinations of clustering results grow exponentially, so it is required to first obtain an upper bound on this possible number of combinations which will be sufficient to compute the value of the sensitivity. In this paper, we obtain an upper bound on the number of possible combinations of clustering results. For this purpose, a generic approach which is suitable for various validity measures and a specific approach which is applicable for two validity measures are proposed. It is also shown that this upper bound is sufficient to compute the sensitivity of various validity measures. This upper bound is very less as compared to the total number of possible combinations of clustering results.}
}

@article{doi:10.1106/F9W7-XGV5-9PME-D5CX,
author = {Devendra Natekar and Ganesh Subbarayan},
title = {Computationally Efficient Fracture Analysis of Electronic Packages through Decomposition},
journal = {International Journal of Damage Mechanics},
volume = {10},
number = {2},
pages = {171–186},
year = {2001m},
doi = {10.1106/F9W7-XGV5-9PME-D5CX},
URL = {https://doi-org.crai.referencistas.com/10.1106/F9W7-XGV5-9PME-D5CX},
eprint = {https://doi-org.crai.referencistas.com/10.1106/F9W7-XGV5-9PME-D5CX},
abstract = {In the present paper we describe and demonstrate a computationally efficient technique for analyzing fracture mechanics problems in mixed linear-nonlinear systems. The technique combines the methodology of Rybicki and Kanninen for calculation of the energy release rate in fracture processes with a decomposition based analysis procedure recently proposed by Subbarayan and co-workers. The methodology will enable quick design decisions during the package development stages without significant loss of accuracy. The developed procedure is demonstrated on a hypothetical 5 × 5 array package. It is shown that on this representative package nearly 30% time savings (or a 150% speed-up) can be acheived in estimating the energy release rate at an accuracy loss of only 6.2%. Prior research has shown that the analysis time is nearly independent of package size, indicating unbounded speed-ups for larger package sizes.}
}

@article{doi:10.1177/10943420211027539,
author = {Matthew R Norman and David C Bader and Christopher Eldred and Walter M Hannah and Benjamin R Hillman and Christopher R Jones and Jungmin M Lee and LR Leung and Isaac Lyngaas and Kyle G Pressel et al.},
title = {Unprecedented cloud resolution in a GPU-enabled full-physics atmospheric climate simulation on OLCF’s summit supercomputer},
journal = {The International Journal of High Performance Computing Applications},
volume = {36},
number = {1},
pages = {93–105},
year = {2022n},
doi = {10.1177/10943420211027539},
URL = {https://doi-org.crai.referencistas.com/10.1177/10943420211027539},
eprint = {https://doi-org.crai.referencistas.com/10.1177/10943420211027539},
abstract = {Clouds represent a key uncertainty in future climate projection. While explicit cloud resolution remains beyond our computational grasp for global climate, we can incorporate important cloud effects through a computational middle ground called the Multi-scale Modeling Framework (MMF), also known as Super Parameterization. This algorithmic approach embeds high-resolution Cloud Resolving Models (CRMs) to represent moist convective processes within each grid column in a Global Climate Model (GCM). The MMF code requires no parallel data transfers and provides a self-contained target for acceleration. This study investigates the performance of the Energy Exascale Earth System Model-MMF (E3SM-MMF) code on the OLCF Summit supercomputer at an unprecedented scale of simulation. Hundreds of kernels in the roughly 10K lines of code in the E3SM-MMF CRM were ported to GPUs with OpenACC directives. A high-resolution benchmark using 4600 nodes on Summit demonstrates the computational capability of the GPU-enabled E3SM-MMF code in a full physics climate simulation.}
}

@article{doi:10.1260/175682909789498279,
author = {Michael V. Ol and Mark Reeder and Daniel Fredberg and Gregory Z. McGowan and Ashok Gopalarathnam and Jack R. Edwards},
title = {Computation vs. Experiment for High-Frequency Low-Reynolds Number Airfoil Plunge},
journal = {International Journal of Micro Air Vehicles},
volume = {1},
number = {2},
pages = {99–119},
year = {2009o},
doi = {10.1260/175682909789498279},
URL = {https://doi-org.crai.referencistas.com/10.1260/175682909789498279},
eprint = {https://doi-org.crai.referencistas.com/10.1260/175682909789498279},
abstract = {We seek to extend the literature on sinusoidal pure-plunge of 2D airfoils at high reduced frequency and low Reynolds number, by including effects of camber and nonzero mean incidence angle. We compare experimental results in a water tunnel using dye injection and 2D particle image velocimetry, with a set of computations in 2D – Immersed Boundary Method and unsteady Reynolds-Averaged Navier Stokes. The Re range is from 10,000 to 60,000, based on free stream velocity and airfoil chord, chosen to cover cases where transition in attached boundary layers would be of some importance, and where transition would only occur in the wake. Generally at high reduced frequency there is no Reynolds number effect. Mean angle of attack has significance, notionally, depending on whether it is below or above static stall. Computations were found to agree well with experimentally-derived velocity contours, vorticity contours and momentum in the wake. As found previously for the NACA0012, varying Strouhal number is found to control the topology of the wake, while varying reduced amplitude and reduced frequency together, but keeping Strouhal number constant, causes wake vortical structures to scale with the reduced amplitude of plunge. Flowfield periodicity – as evinced from comparison of instantaneous and time-averaged particle image velocimetry – is generally attained after two periods of oscillation from motion onset.}
}

@article{doi:10.1177/07067437221094552,
author = {Mark Sinyor and Rabia Zaheer and Roger T. Webb and Duleeka Knipe and Emily Eyles and Julian P.T. Higgins and Luke McGuinness and Lena Schmidt and Catherine Macleod-Hall and Dana Dekel et al.},
title = {SARS-CoV-2 Infection and the Risk of Suicidal and Self-Harm Thoughts and Behaviour: A Systematic Review},
journal = {The Canadian Journal of Psychiatry},
volume = {67},
number = {11},
pages = {813–828},
year = {2022p},
doi = {10.1177/07067437221094552},
note = {PMID:35532916},
URL = {https://doi-org.crai.referencistas.com/10.1177/07067437221094552},
eprint = {https://doi-org.crai.referencistas.com/10.1177/07067437221094552},
abstract = {Objective The COVID-19 pandemic has had a complex impact on risks of suicide and non-fatal self-harm worldwide with some evidence of increased risk in specific populations including women, young people, and people from ethnic minority backgrounds. This review aims to systematically address whether SARS-CoV-2 infection and/or COVID-19 disease confer elevated risk directly. Method As part of a larger Living Systematic Review examining self-harm and suicide during the pandemic, automated daily searches using a broad list of keywords were performed on a comprehensive set of databases with data from relevant articles published between January 1, 2020 and July 18, 2021. Eligibility criteria for our present review included studies investigating suicide and/or self-harm in people infected with SARS-CoV-2 with or without manifestations of COVID-19 disease with a comparator group who did not have infection or disease. Suicidal and self-harm thoughts and behaviour (STBs) were outcomes of interest. Studies were excluded if they reported data for people who only had potential infection/disease without a confirmed exposure, clinical/molecular diagnosis or self-report of a positive SARS-CoV-2 test result. Studies of news reports, treatment studies, and ecological studies examining rates of both SARS-CoV-2 infections and suicide/self-harm rates across a region were also excluded. Results We identified 12 studies examining STBs in nine distinct samples of people with SARS-CoV-2. These studies, which investigated STBs in the general population and in subpopulations, including healthcare workers, generally found positive associations between SARS-CoV-2 infection and/or COVID-19 disease and subsequent suicidal/self-harm thoughts and suicidal/self-harm behaviour. Conclusions This review identified some evidence that infection with SARS-CoV-2 and/or COVID-19 disease may be associated with increased risks for suicidal and self-harm thoughts and behaviours but a causal link cannot be inferred. Further research with longer follow-up periods is required to confirm these findings and to establish whether these associations are causal.}
}

@article{doi:10.1177/1071181312561292,
author = {Ganyun Sun and Shengji Yao},
title = {A Framework for an Evolutionary Computation Approach to Supporting Concept Generation},
journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
volume = {56},
number = {1},
pages = {1972–1976},
year = {2012q},
doi = {10.1177/1071181312561292},
URL = {https://doi-org.crai.referencistas.com/10.1177/1071181312561292},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1071181312561292},
abstract = {This paper proposes a framework for an evolutionary computation approach to supporting concept generation in engineering design. This approach attempts to assist designers in increasing quantity and diversity of design concepts. The framework integrates the Theory of Inventive Problem Solving (TRIZ) methodology into an evolution process. First, the product resources are analyzed. The relationships between the resources are constructed using Genetic Programming. Second, contradictions between the relationships are identified based on the physical features of the resources and their relationships. Finally, principles for resolving contradictions in the TRIZ matrix guide designers to generate alternative design solutions. A case study is conducted to show how the approach works.}
}

@article{doi:10.1068/a301839,
author = {I Turton and S Openshaw},
title = {High-Performance Computing and Geography: Developments, Issues, and Case Studies},
journal = {Environment and Planning A: Economy and Space},
volume = {30},
number = {10},
pages = {1839–1856},
year = {1998r},
doi = {10.1068/a301839},
URL = {https://doi-org.crai.referencistas.com/10.1068/a301839},
eprint = {https://doi-org.crai.referencistas.com/10.1068/a301839},
abstract = {In this paper we outline some of the results that were obtained by the application of a Cray T3D parallel supercomputer to human geography problems. We emphasise the fundamental importance of high-performance computing (HPC) as a future relevant paradigm for doing geography. We offer an introduction to recent developments and illustrate how new computational intelligence technologies can start to be used to make use of opportunities created by data riches from geographic information systems, artificial intelligence tools, and HPC in geography.}
}

@article{doi:10.1243/095441002321029026,
author = {G Zuppardi and D Paterna},
title = {Influence of rarefaction on the computation of aerodynamic parameters in hypersonic flow},
journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
volume = {216},
number = {6},
pages = {277–290},
year = {2002s},
doi = {10.1243/095441002321029026},
URL = {https://doi-org.crai.referencistas.com/10.1243/095441002321029026},
eprint = {https://doi-org.crai.referencistas.com/10.1243/095441002321029026},
abstract = {Abstract The results from two well-known and widely accepted codes, the Navier—Stokes solver FLUENT and the direct simulation Monte Carlo (DSMC) solver DS2G, have been analysed in order to fix the levels of the flow field rarefaction where the codes can work properly for the computation of aerodynamic forces and heat flux on a spacecraft during the re-entry. This subject has already been widely investigated; thus the purpose of the present work is to provide a further contribution. In order to make realistic computations, a probable path of a typical capsule, returning from an interplanetary mission to Earth, has been considered in the altitude range 50—120 km. Proper use of FLUENT was fixed at the free-stream Knudsen number Kn∞ < 7×10−5. Attempts have been made to increase this limit, but with no success. More specifically, a finer mesh as well as a slip velocity and temperature jump were considered. Physical conditions like the lack of isotropy of the pressure tensor and the failure of the classical phenomenological equations, both increasing with the rarefaction, are very probably the causes of the failure of FLU EN T. The basic principle of the DSMC solver is valid at each rarefaction level; a sensitivity analysis on the characteristic dimension of the cell, on the time step and on the number of simulated molecules verified that the restrictions on DS2G are imposed only by the capability of the computer. As neither experimental data nor numerical results are available at the present test conditions, the evaluation of the results relies just on qualitative considerations about the trends of experimental data, reported in the literature, of a sphere in a hypersonic transitional regime.}
}

@article{doi:10.1177/0954410011418223,
title = {Special Issue On Evolutionary Computation In Aerospace Sciences},
journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
volume = {225},
number = {10},
pages = {1063–1064},
year = {2011t},
doi = {10.1177/0954410011418223},
URL = {https://doi-org.crai.referencistas.com/10.1177/0954410011418223},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0954410011418223}
}

