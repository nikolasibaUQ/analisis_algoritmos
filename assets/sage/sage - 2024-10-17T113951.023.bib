@article{doi:10.1177/10943420241280060,
author = {Cody J Balos and Marcus Day and Lucas Esclapez and Anne M Felden and David J Gardner and Malik Hassanaly and Daniel R Reynolds and Jon S Rood and Jean M Sexton and Nicholas T Wimer et al.},
title = {SUNDIALS time integrators for exascale applications with many independent systems of ordinary differential equations},
journal = {The International Journal of High Performance Computing Applications},
volume = {0},
number = {0},
pages = {10943420241280060},
year = {2024a},
doi = {10.1177/10943420241280060},
URL = {https://doi-org.crai.referencistas.com/10.1177/10943420241280060},
eprint = {https://doi-org.crai.referencistas.com/10.1177/10943420241280060},
abstract = {Many complex systems can be accurately modeled as a set of coupled time-dependent partial differential equations (PDEs). However, solving such equations can be prohibitively expensive, easily taxing the world’s largest supercomputers. One pragmatic strategy for attacking such problems is to split the PDEs into components that can more easily be solved in isolation. This operator splitting approach is used ubiquitously across scientific domains, and in many cases leads to a set of ordinary differential equations (ODEs) that need to be solved as part of a larger “outer-loop” time-stepping approach. The SUNDIALS library provides a plethora of robust time integration algorithms for solving ODEs, and the U.S. Department of Energy Exascale Computing Project (ECP) has supported its extension to applications on exascale-capable computing hardware. In this paper, we highlight some SUNDIALS capabilities and its deployment in combustion and cosmology application codes (Pele and Nyx, respectively) where operator splitting gives rise to numerous, small ODE systems that must be solved concurrently.}
}

@article{doi:10.1177/10812865221102549,
author = {Selçuk Başdemir and Hüsnü Dal},
title = {A one-pass predictor-corrector algorithm for the inverse Langevin function},
journal = {Mathematics and Mechanics of Solids},
volume = {28},
number = {4},
pages = {920–930},
year = {2023b},
doi = {10.1177/10812865221102549},
URL = {https://doi-org.crai.referencistas.com/10.1177/10812865221102549},
eprint = {https://doi-org.crai.referencistas.com/10.1177/10812865221102549},
abstract = {Inverse Langevin function has an extensive use in statistical mechanics, polymer chemistry, and physics. Main struggle is that the inverse Langevin function cannot be expressed in an exact analytical form. To this end, many approaches to estimate the inverse Langevin function have been proposed. A trade-off can be observed between level of accuracy and mathematical complexity in the existing approximants in the literature. In the present contribution, a simple, yet efficient one-pass predictor-corrector algorithm is proposed for the accurate prediction of the inverse Langevin function. The predictor step uses the approximants yp(x) proposed in the literature. The corrector term is based on a single iteration applied to the error function (yp) – x. The correction term is based on the yp and has the same form irrespective of the specific approximant used in the predictor step. Hence, the additional computational cost is constant irrespective of the approximant used as predictor. In order to demonstrate the accuracy and efficiency of the approach, maximum relative error and the computational cost before and after corrector step are analysed for eight different approximants. The proposed one-pass predictor-corrector approach allows the use of relatively simpler approximants of the inverse Langevin function by improving their accuracy by at least an order of magnitude.}
}

@article{doi:10.1177/02632764221141804,
author = {Ryan Bishop},
title = {Bernard Stiegler and the Internation Project: An Introduction},
journal = {Theory, Culture & Society},
volume = {39},
number = {7–8},
pages = {5–17},
year = {2022c},
doi = {10.1177/02632764221141804},
URL = {https://doi-org.crai.referencistas.com/10.1177/02632764221141804},
eprint = {https://doi-org.crai.referencistas.com/10.1177/02632764221141804},
abstract = {This article serves as the introduction to the Annual Review special section entitled ‘Bernard Stiegler and the Internation Project: Computational Practices and Circumscribed Futures’. As such, it introduces the collective undertaking of the Internation Project in relation to Stiegler’s long career as a thinker, educator and community organizer. The introduction pursues a number of themes addressed in the section’s contributions, including pharmacological logic, transindividuation, computational practices, bifurcation and negentropy (means of slowing entropic processes at individual and collective levels). All of these themes pertain to the climate crises the world collectively faces and posit means by which futures can be conceived in less detrimental and destructive economic, social, technological and intellectual ways. The Internation Collective as represented and furthered in this special section responds to the demands of climate crises through a macroeconomic model designed to combat entropy at various scales, from the bio-chemical to the biosphere.}
}

@article{doi:10.1177/1073858417750466,
author = {Luca Casartelli and Alessandra Federici and Emilia Biffi and Massimo Molteni and Luca Ronconi},
title = {Are We “Motorically” Wired to Others? High-Level Motor Computations and Their Role in Autism},
journal = {The Neuroscientist},
volume = {24},
number = {6},
pages = {568–581},
year = {2018d},
doi = {10.1177/1073858417750466},
note = {PMID:29271293},
URL = {https://doi-org.crai.referencistas.com/10.1177/1073858417750466},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1073858417750466},
abstract = {High-level motor computations reflect abstract components far apart from the mere motor performance. Neural correlates of these computations have been explored both in nonhuman and human primates, supporting the idea that our brain recruits complex nodes for motor representations. Of note, these computations have exciting implications for social cognition, and they also entail important challenges in the context of autism. Here, we focus on these challenges benefiting from recent studies addressing motor interference, motor resonance, and high-level motor planning. In addition, we suggest new ideas about how one maps and shares the (motor) space with others. Taken together, these issues inspire intriguing and fascinating questions about the social tendency of our high-level motor computations, and this tendency may indicate that we are “motorically” wired to others. Thus, after furnishing preliminary insights on putative neural nodes involved in these computations, we focus on how the hypothesized social nature of high-level motor computations may be anomalous or limited in autism, and why this represents a critical challenge for the future.}
}

@article{doi:10.1177/0954410011414320,
author = {M S R Chandra Murty and D Chakraborty},
title = {Numerical simulation of angular injection of hydrogen fuel in scramjet combustor},
journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
volume = {226},
number = {7},
pages = {861–872},
year = {2012e},
doi = {10.1177/0954410011414320},
URL = {https://doi-org.crai.referencistas.com/10.1177/0954410011414320},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0954410011414320},
abstract = {Angular injection of hydrogen fuel in a scramjet combustor is explored numerically. Three-dimensional Navier–Stokes equations with turbulence and combustion models are solved using commercial computational fluid dynamics software. Both infinitely fast kinetics and single-step finite rate H2–air kinetics are used to find out the effect of chemical kinetics in the thermochemical behaviour of the flow field. Grid independence of the results is demonstrated and gridconvergence index-based error estimate provided. k-ω turbulence model performs better, in comparison to k–ϵ and shear stress transport models, in predicting the surface pressure. Single-step finite rate chemistry (SSC) performs extremely well in predicting the flow features in the combustor. Computed temperature and species mole fraction and wall pressure distributions with SSC match better with the experimental results compared to fast chemistry calculation and detailed chemistry calculation of other workers. It has been observed that simple chemistry can describe H2–air reaction in scramjet combustor reasonably well.}
}

@article{doi:10.1177/00220027231179102,
author = {Stephanie Dornschneider-Elkink and Nick Henderson},
title = {Repression and Dissent: How Tit-for-Tat Leads to Violent and Nonviolent Resistance},
journal = {Journal of Conflict Resolution},
volume = {68},
number = {4},
pages = {756–785},
year = {2024f},
doi = {10.1177/00220027231179102},
URL = {https://doi-org.crai.referencistas.com/10.1177/00220027231179102},
eprint = {https://doi-org.crai.referencistas.com/10.1177/00220027231179102},
abstract = {Much research examines the state-dissident nexus by large-n studies and rational choice theories. This article contributes an analysis of dissident reasoning through a computational evaluation of ethnographic interviews. The analysis shows that dissident decision-making is based on tit-for-tat deliberations: Dissidents choose violent means primarily in response to violent repression, and nonviolent means in response to nonviolent repression. Ordinary citizens not participating in dissent consider positive state behavior or safety concerns instead. Consistent with arguments that state-dissident interactions are reciprocal, these findings reveal unexpected cognitive similarities between political dissent and cooperation, which is often associated with tit-for-tat deliberations. They also show the importance of state repression compared with other motivators of dissent, including perceived relative deprivation and social contagion. The findings identify heuristic patterns of reasoning which suggest that dissidents may be more open to change and, ultimately, cooperation with state authorities than what is argued by repressive states.}
}

@article{doi:10.1177/09544089221134449,
author = {Seyyed Hossein Hosseini and Abedin Zargoushi and Farhad Tablebi},
title = {Effect of side stream obstruction on the results of an industrial three-stream cold box: Numerical study},
journal = {Proceedings of the Institution of Mechanical Engineers, Part E: Journal of Process Mechanical Engineering},
volume = {237},
number = {6},
pages = {2362–2372},
year = {2023g},
doi = {10.1177/09544089221134449},
URL = {https://doi-org.crai.referencistas.com/10.1177/09544089221134449},
eprint = {https://doi-org.crai.referencistas.com/10.1177/09544089221134449},
abstract = {The cold box heat exchangers are used in petrochemical and gas refinery industries. Here, an industrial complex cold box equipped with a number of plate-fins is simulated by computational fluid dynamics. The model predicts the outlet vapor fraction, pressure drop, and outlet temperature with average absolute relative deviations of 0.17%, 3.3%, and 12.89% for all streams, respectively. The influence of obstruction in streams B and C on the computational fluid dynamics results are studied. When stream B or C is blocked, the remaining open streams experience an increase in pressure drop, temperature, and vapor fraction, which negatively affects the heat exchanger’s performance over a long time. Finally, the computational fluid dynamics results of the cold box are compared with those of commercial software Aspen-EDR. Even though Aspen-EDR predicts an acceptable mean temperature and vapor fraction (phase change) along channels, it calculates pressure drop incorrectly. So, Aspen-EDR, computationally efficient software, can be used for modeling of mean temperature and phase change of flow in an industrial multi-stream cold box.}
}

@article{doi:10.1177/016146811912101410,
author = {Daniel G. Krutka and Stefania Manca and Sarah M. Galvin and Christine Greenhow and Matthew J. Koehler and Emilia Askari},
title = {Teaching “Against” Social Media: Confronting Problems of Profit in the Curriculum},
journal = {Teachers College Record},
volume = {121},
number = {14},
pages = {1–42},
year = {2019h},
doi = {10.1177/016146811912101410},
URL = {https://doi-org.crai.referencistas.com/10.1177/016146811912101410},
eprint = {https://doi-org.crai.referencistas.com/10.1177/016146811912101410},
abstract = {Educators increasingly teach with social media in varied ways, but they may do so without considering the ways in which social media corporations profit from their uses or compromise transparency, equity, health, safety, and democracy through the design of platforms. There is a lack of scholarship that addresses the curricular topics that educators might investigate to teach about social media platforms and the potential challenges they pose for education and society. In this article, we draw on sociotechnical theories that conceive of social media as microsystems to understand the relationship between users, education, and social media companies. We identify and describe five topics concerning social media design that educators can consider and investigate with students in a variety of settings: user agreements and use of data; algorithms of oppression, echo, and extremism; distraction, user choice, and access for nonusers; harassment and cyberbullying; and gatekeeping for accurate information. In each case, we suggest curricular possibilities for teaching about social media platforms that draw from intersections of curriculum, media, and educational studies.}
}

@article{doi:10.1177/0002716215569174,
author = {Jimmy Lin},
title = {On Building Better Mousetraps and Understanding the Human Condition: Reflections on Big Data in the Social Sciences},
journal = {The ANNALS of the American Academy of Political and Social Science},
volume = {659},
number = {1},
pages = {33–47},
year = {2015i},
doi = {10.1177/0002716215569174},
URL = {https://doi-org.crai.referencistas.com/10.1177/0002716215569174},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0002716215569174},
abstract = {Over the past few years, we have seen the emergence of “big data”: disruptive technologies that have transformed commerce, science, and many aspects of society. Despite the tremendous enthusiasm for big data, there is no shortage of detractors. This article argues that many criticisms stem from a fundamental confusion over goals: whether the desired outcome of big data use is “better science” or “better engineering.” Critics point to the rejection of traditional data collection and analysis methods, confusion between correlation and causation, and an indifference to models with explanatory power. From the perspective of advancing social science, these are valid reservations. I contend, however, that if the end goal of big data use is to engineer computational artifacts that are more effective according to well-defined metrics, then whatever improves those metrics should be exploited without prejudice. Sound scientific reasoning, while helpful, is not necessary to improve engineering. Understanding the distinction between science and engineering resolves many of the apparent controversies surrounding big data and helps to clarify the criteria by which contributions should be assessed.}
}

@article{doi:10.1177/1071181312561043,
author = {Elizabeth D. Murphy and Harold A. Albert and Jennifer M. Chen and Gregory G. Anderson},
title = {The Role of Mental Computations in Current and Future En Route Air Traffic Control},
journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
volume = {56},
number = {1},
pages = {110–114},
year = {2012j},
doi = {10.1177/1071181312561043},
URL = {https://doi-org.crai.referencistas.com/10.1177/1071181312561043},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1071181312561043},
abstract = {As air traffic control (ATC) becomes increasingly automated, software designers need to know how air traffic controllers process information as they manage operations in today’s system. Extracting knowledge from today’s controller workforce and representing that knowledge in the form of mental computations are essential steps toward needs assessment and development of advanced decision-aiding tools and technologies. A recent task analysis documented information derived by controllers from their cognitive integration of displayed information. This work envisions future, more detailed analyses of the controller’s mental computations as essential to identifying needs for advanced software tools, including predictive displays.}
}

@article{doi:10.1177/10762175221149259,
author = {Mary O’Grady-Jones and Michael M. Grant},
title = {Ready Coder One: Collaborative Game Design-Based Learning on Gifted Fourth Graders’ 21st Century Skills},
journal = {Gifted Child Today},
volume = {46},
number = {2},
pages = {84–107},
year = {2023k},
doi = {10.1177/10762175221149259},
URL = {https://doi-org.crai.referencistas.com/10.1177/10762175221149259},
eprint = {https://doi-org.crai.referencistas.com/10.1177/10762175221149259},
abstract = {The purpose of this research was to describe the impact of digital game building on fourth grade gifted and talented students’ problem-solving, creativity, and collaboration skills. Increasingly, there has been a call to involve students in real-world experiences through projects that explore authentic issues using technology. Game design-based learning with its unique set of affordances may offer a path to integrating technology, computer science education, creativity, and problem-solving. Increasingly, the ability to create rather than just consume technology has gained attention linking creativity and collaboration to using coding language. In this study, data collection included student reflection journals, classroom observations, classroom video recordings, a focus group interview, and students’ games. Participants came from two GT classes (n = 45). Qualitative analysis identified five themes: overcoming challenges of group work, developing a culture of collaboration, creating narrative, and connecting science, problem-solving in Scratch’s coding environment, and reflecting on learning. Findings indicated involving gifted students in game design-based learning in science had a positive impact on student perceptions of problem-solving, creativity, and collaboration.}
}

@article{doi:10.1177/0033294117729183,
author = {Richard Perlow and Mia Jattuso},
title = {A Comparison of Computation Span and Reading Span Working Memory Measures’ Relations With Problem-Solving Criteria},
journal = {Psychological Reports},
volume = {121},
number = {3},
pages = {430–444},
year = {2018l},
doi = {10.1177/0033294117729183},
note = {PMID:29298565},
URL = {https://doi-org.crai.referencistas.com/10.1177/0033294117729183},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0033294117729183},
abstract = {Researchers have operationalized working memory in different ways and although working memory–performance relationships are well documented, there has been relatively less attention devoted to determining whether seemingly similar measures yield comparable relations with performance outcomes. Our objective is to assess whether two working memory measures deploying the same processes but different item content yield different relations with two problem-solving criteria. Participants completed a computation-based working memory measure and a reading-based measure prior to performing a computerized simulation. Results reveal differential relations with one of the two criteria and support the notion that the two working memory measures tap working memory capacity and other cognitive abilities. One implication for theory development is that researchers should consider incorporating other cognitive abilities in their working memory models and that the selection of those abilities should correspond to the criterion of interest. One practical implication is that researchers and practitioners shouldn’t automatically assume that different phonological loop-based working memory scales are interchangeable.}
}

@article{doi:10.1177/1094342020905971,
author = {Roberto Porcù and Edie Miglio and Nicola Parolini and Mattia Penati and Noemi Vergopolan},
title = {HPC simulations of brownout: A noninteracting particles dynamic model},
journal = {The International Journal of High Performance Computing Applications},
volume = {34},
number = {3},
pages = {267–281},
year = {2020m},
doi = {10.1177/1094342020905971},
URL = {https://doi-org.crai.referencistas.com/10.1177/1094342020905971},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342020905971},
abstract = {Helicopters can experience brownout when flying close to a dusty surface. The uplifting of dust in the air can remarkably restrict the pilot’s visibility area. Consequently, a brownout can disorient the pilot and lead to the helicopter collision against the ground. Given its risks, brownout has become a high-priority problem for civil and military operations. Proper helicopter design is thus critical, as it has a strong influence over the shape and density of the cloud of dust that forms when brownout occurs. A way forward to improve aircraft design against brownout is the use of particle simulations. For simulations to be accurate and comparable to the real phenomenon, billions of particles are required. However, using a large number of particles, serial simulations can be slow and too computationally expensive to be performed. In this work, we investigate an message passing interface (MPI) + graphics processing unit (multi-GPU) approach to simulate brownout. In specific, we use a semi-implicit Euler method to consider the particle dynamics in a Lagrangian way, and we adopt a precomputed aerodynamic field. Here, we do not include particle–particle collisions in the model; this allows for independent trajectories and effective model parallelization. To support our methodology, we provide a speedup analysis of the parallelization concerning the serial and pure-MPI simulations. The results show (i) very high speedups of the MPI + multi-GPU implementation with respect to the serial and pure-MPI ones, (ii) excellent weak and strong scalability properties of the implemented time-integration algorithm, and (iii) the possibility to run realistic simulations of brownout with billions of particles at a relatively small computational cost. This work paves the way toward more realistic brownout simulations, and it highlights the potential of high-performance computing for aiding and advancing aircraft design for brownout mitigation.}
}

@article{doi:10.1177/10943420221128529,
author = {Long Qu and Rached Abdelkhalak and Hatem Ltaief and Issam Said and David Keyes},
title = {Exploiting temporal data reuse and asynchrony in the reverse time migration},
journal = {The International Journal of High Performance Computing Applications},
volume = {37},
number = {2},
pages = {132–150},
year = {2023n},
doi = {10.1177/10943420221128529},
URL = {https://doi-org.crai.referencistas.com/10.1177/10943420221128529},
eprint = {https://doi-org.crai.referencistas.com/10.1177/10943420221128529},
abstract = {Reverse Time Migration (RTM) is a state-of-the-art algorithm used in seismic depth imaging in complex geological environments for the oil and gas exploration industry. It calculates high-resolution images by solving the three-dimensional acoustic wave equation using seismic datasets recorded at various receiver locations. Reverse Time Migration’s computational phases are predominantly composed of stencil computational kernels for the finite-difference time-domain scheme, applying the absorbing boundary conditions, and I/O operations needed for the imaging condition. In this paper, we integrate the asynchronous Multicore Wavefront Diamond (MWD) tiling approach into the full RTM workflow. Multicore Wavefront Diamond permits to further increase data reuse by leveraging spatial with Temporal Blocking (TB) during the stencil computations. This integration engenders new challenges with a snowball effect on the legacy synchronous RTM workflow as it requires rethinking of how the absorbing boundary conditions, the I/O operations, and the imaging condition operate. These disruptive changes are necessary to maintain the performance superiority of asynchronous stencil execution throughout the time integration, while ensuring the quality of the subsurface image does not deteriorate. We assess the overall performance of the new MWD-based RTM and compare against traditional Spatial Blocking (SB)-based RTM on various shared-memory systems using the SEG Salt3D model. The MWD-based RTM achieves up to 70% performance speedup compared to SB-based RTM. To our knowledge, this paper highlights for the first time the applicability of asynchronous executions with temporal blocking throughout the whole RTM. This may eventually create new research opportunities in improving hydrocarbon extraction for the petroleum industry.}
}

@article{doi:10.1155/2014/245924,
author = {Guillermo G. Riva and Jorge M. Finochietto},
title = {In-Network Filtering Schemes for Type-Threshold Function Computation in Wireless Sensor Networks},
journal = {International Journal of Distributed Sensor Networks},
volume = {10},
number = {8},
pages = {245924},
year = {2014o},
doi = {10.1155/2014/245924},
URL = {https://doi-org.crai.referencistas.com/10.1155/2014/245924},
eprint = {https://doi-org.crai.referencistas.com/10.1155/2014/245924},
abstract = {Data collection in wireless sensor networks (WSNs) can become extremely expensive in terms of power consumption if all measurements have to be fetched. However, since multiple applications do not require data from all nodes but to compute a function over a smaller data set, much of the available data on the network can be considered irrelevant and not worthy of spending energy. In this context, in-network filtering schemes can be used to forward only relevant data towards a sink node for processing purposes. In this work, we propose and evaluate two schemes that can drive this filtering process. Both of them are based on the integration of metaheuristics and learning algorithms inspired by nature. In particular, we consider the computation of the maximum function as case study for these schemes. We investigate the trade-off between communications costs, which are directly associated with power consumption, and error costs due to fetching not all relevant data. We show by simulation that communication costs can be significantly reduced with respect to traditional schemes while keeping the computation error bounded.}
}

@article{doi:10.1037/a0032803,
author = {Dean Keith Simonton},
title = {Creative Thoughts as Acts of Free Will: A Two-Stage Formal Integration},
journal = {Review of General Psychology},
volume = {17},
number = {4},
pages = {374–383},
year = {2013p},
doi = {10.1037/a0032803},
URL = {https://doi-org.crai.referencistas.com/10.1037/a0032803},
eprint = {https://doi-org.crai.referencistas.com/10.1037/a0032803},
abstract = {This article integrates two topics usually considered disciplines apart, namely, creativity and free will. In particular, creative thoughts are conceived as acts of free will. This integration begins by reviewing recent advances in a specific two-stage theory of creative problem solving, namely blind variation and selective retention (BVSR). After discussing the parallel two-stage theory of free will (chance then choice), both two-stage theories are then integrated into a single formal representation entailing choice initial probabilities, final utilities, and prior knowledge values. These three parameters are used to define the creativity of any given solution and the “sightedness” of any generated thought or choice. Both creativity and free will vanish as sightedness increases, but their relation to blindness is more complex, yielding a triangular joint distribution that mandates a second-stage selection or decision process. In addition, to accommodate the need to create choices actively rather than just decide among given choices, the treatment expands to encompass both thoughts and choices as combinatorial products. This extension connects the discussion of free will with both combinatorial models of creativity and the research on the factors that enable a person to engage in free combinatorial processes. The article closes with suggestions of future empirical and theoretical research with respect to psychology, philosophy, and potential future exchanges between the two disciplines.}
}

@article{doi:10.3102/00346543211070048,
author = {Amy Vetter and Beverly S. Faircloth and Kimberly K. Hewitt and Laura M. Gonzalez and Ye He and Marcia L. Rock},
title = {Equity and Social Justice in Research Practice Partnerships in the United States},
journal = {Review of Educational Research},
volume = {92},
number = {5},
pages = {829–866},
year = {2022q},
doi = {10.3102/00346543211070048},
URL = {https://doi-org.crai.referencistas.com/10.3102/00346543211070048},
eprint = {https://doi-org.crai.referencistas.com/10.3102/00346543211070048},
abstract = {Research–practice partnerships (RPPs) have grown rapidly in the last decade in the United States to challenge traditional notions of education research by emphasizing the importance of researchers and practitioners working together in a spirit of mutuality to develop research questions, collect data, implement interventions, and analyze and use findings. RPP scholarship in the United States has historically advocated for the need to pay more focused attention to issues of equity and justice. To address that need, this literature review examined how RPPs in the United States have addressed equity and justice in their work. Based on five dimensions of equity and justice that could be observed within the 149 examples of RPP work we reviewed, we identified 17 exemplar projects that explicitly and effectively forefront equity and justice in RPPs, what we call equity-focused. Implications suggest that researchers and practitioners who have initiated equity-orientated RPPs may reflect on the partnerships’ existing strengths, specifically related to the five interconnected features that characterize equity-focused RPPs, to sustain and advance equity and justice through RPPs.}
}

@article{doi:10.1177/109434209200600407,
author = {Skef Wholey and Clifford Lasser and Gyan Bhanot},
title = {Correspondence: FLO67: a Case Study in Scalable Programming},
journal = {The International Journal of Supercomputing Applications},
volume = {6},
number = {4},
pages = {383–388},
year = {1992r},
doi = {10.1177/109434209200600407},
URL = {https://doi-org.crai.referencistas.com/10.1177/109434209200600407},
eprint = {https://doi-org.crai.referencistas.com/10.1177/109434209200600407},
abstract = {We describe a scalable version of the FLO67 program written in the loop style that is characteristic of Fortran 77. This single scalable source program can be com piled and run on conventional scalar processors, vec tor computers, or massively parallel computers, with respectable performance on all platforms. Initial runs have been on the IBM RISC System/6000, the Sun SPARC, and the Connection Machine supercomputer. This article presents and discusses interesting frag ments of the program, focusing on issues of code scalability.}
}

@article{doi:10.1177/1536867X1301300102,
author = {Theresa Wimberley and Erik Parner and Henrik Støvring},
title = {Stata as a Numerical Tool for Scientific Thought Experiments: A Tutorial with Worked Examples},
journal = {The Stata Journal},
volume = {13},
number = {1},
pages = {3–20},
year = {2013s},
doi = {10.1177/1536867X1301300102},
URL = {https://doi-org.crai.referencistas.com/10.1177/1536867X1301300102},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1536867X1301300102},
abstract = {Thought experiments based on simulation can be used to explain the impact of the chosen study design, statistical analysis strategy, or the sensitivity of results to fellow researchers. In this article, we demonstrate with two examples how to implement quantitative thought experiments in Stata. The first example uses a large-sample approach to study the impact on the estimated effect size of dichotomizing an exposure variable at different values. The second example uses simulations of datasets of realistic size to illustrate the necessity of using sampling fractions as inverse probability weights in statistical analysis for protection against bias in a complex sampling design. We also give a brief outline of the general steps needed for implementing quantitative thought experiments in Stata. We demonstrate how Stata provides programming facilities for conveniently implementing such thought experiments, with the advantage of saving researchers time, speculation, and debate as well as improving communication in interdisciplinary research groups.}
}

@article{doi:10.1177/27527263231217826,
author = {Yiling Yao and Suijun Jia and Jinfa Cai},
title = {Beyond computation: Assessing in-service mathematics teachers’ conceptual understanding of fraction division through problem posing},
journal = {Asian Journal for Mathematics Education},
volume = {2},
number = {4},
pages = {413–429},
year = {2023t},
doi = {10.1177/27527263231217826},
URL = {https://doi-org.crai.referencistas.com/10.1177/27527263231217826},
eprint = {https://doi-org.crai.referencistas.com/10.1177/27527263231217826},
abstract = {Problem posing has long been recognized as a critically important teaching method and goal in the area of mathematics education. However, few studies have used problem posing to assess in-service teachers’ mathematical understanding. The present study investigated in-service teachers’ mathematical understanding of fraction division, which is often considered challenging content in elementary school, from three angles: computation, drawing, and problem posing. Two studies involving 66 and 193 primary and middle school teachers were conducted to reveal the in-service teachers’ mathematical understanding and whether drawing and problem posing affected each other. Although the in-service teachers rarely had the opportunity to pose mathematical problems in their daily teaching, they were able to pose mathematical problems in this study. In addition, problem-posing tasks were more useful in diagnosing the in-service teachers’ conceptual understanding than were computation or drawing. Thus, problem posing seems to have contributed to their conceptual understanding of fraction division on the drawing task.}
}

