@article{doi:10.1177/0734282918809793,
author = {Roberto A. Abreu-Mendoza and Yaira Chamorro and Esmeralda Matute},
title = {Psychometric Properties of the WRAT Math Computation Subtest in Mexican Adolescents},
journal = {Journal of Psychoeducational Assessment},
volume = {37},
number = {8},
pages = {957–972},
year = {2019a},
doi = {10.1177/0734282918809793},
URL = {https://doi-org.crai.referencistas.com/10.1177/0734282918809793},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0734282918809793},
abstract = {The goal of this study was to provide normative scores and examine the psychometric properties of the Math Computation subtest of the Wide Range Achievement Test–IV (WRAT-IV) for Mexican adolescents after the completion of junior high school. We group-administered this subtest to 1,318 first-year Mexican high school students. We then obtained its overall internal reliability and examined its underlying factor structure. Finally, we determined its concurrent and criterion validity by evaluating a subsample of 106 students that included adolescents with mathematical difficulty, mathematical talent, and typical performance. Results showed that the subtest has a good internal reliability and appropriate psychometric characteristics, suggesting its appropriateness for the detection of adolescents with particular difficulty or ability in mathematics. The exploratory factor analysis identified three factors: arithmetic, fractions and basic algebra, and rational numbers. There were also sex differences in the number of correct responses, but the effect size was small.}
}

@article{doi:10.1177/1059712315589355,
author = {Eduardo Alonso and Michael Fairbank and Esther Mondragón},
title = {Back to optimality: a formal framework to express the dynamics of learning optimal behavior},
journal = {Adaptive Behavior},
volume = {23},
number = {4},
pages = {206–215},
year = {2015b},
doi = {10.1177/1059712315589355},
URL = {https://doi-org.crai.referencistas.com/10.1177/1059712315589355},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1059712315589355},
abstract = {Whether animals behave optimally is an open question of great importance, both theoretically and in practice. Attempts to answer this question focus on two aspects of the optimization problem, the quantity to be optimized and the optimization process itself. In this paper, we assume the abstract concept of cost as the quantity to be minimized and propose a reinforcement learning algorithm, called Value-Gradient Learning (VGL), as a computational model of behavior optimality. We prove that, unlike standard models of Reinforcement Learning, Temporal Difference in particular, VGL is guaranteed to converge to optimality under certain conditions. The core of the proof is the mathematical equivalence of VGL and Pontryagin’s Minimum Principle, a well-known optimization technique in systems and control theory. Given the similarity between VGL’s formulation and regulatory models of behavior, we argue that our algorithm may provide psychologists with a tool to formulate such models in optimization terms.}
}

@article{doi:10.1177/1468087414566513,
author = {Joshua A Bittle and Timothy J Jacobs},
title = {A computationally efficient combustion trajectory prediction model developed for real-time diesel combustion control},
journal = {International Journal of Engine Research},
volume = {17},
number = {2},
pages = {246–258},
year = {2016c},
doi = {10.1177/1468087414566513},
URL = {https://doi-org.crai.referencistas.com/10.1177/1468087414566513},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1468087414566513},
abstract = {The heterogeneous nature of diesel combustion adds many complexities that make understanding the combustion process difficult. Many researchers have made great efforts in diagnostics, prediction, and control capabilities. In this work, a computationally efficient thermodynamic-based model (15 ms on 2010 dual core processor) has been created that predicts the combustion trajectory (path through the ϕ–T plane) with the goal of bridging the gap between typical off-line engine prediction simulations and on-line real-time engine control strategies. The ϕ–T plane is often used to help illustrate the soot and NOx formation behavior during diesel combustion. The experimental engine operating conditions shown illustrate how exhaust gas recirculation influences the combustion trajectory at different timings—that is, showing the typical soot–NOx trade-off for diesel engines and the defeat of this trade-off when low-temperature combustion is obtained. The major insight gained is that the low-temperature combustion trajectory looks similar to a conventional one with just subtle differences that keep it from moving into the soot formation region. Additionally, the traditional conceptual explanations for diesel combustion are explored relative to how they are illustrated by the combustion trajectory, especially the transition from premixed to mixing-controlled combustion. Understanding that behavior in this context aids in explaining the different observations for the low-temperature combustion modes. The fact that these observations are made using this simplified modeling approach is promising for future use of this type of thermodynamic-based models in real-time engine control.}
}

@article{doi:10.1068/p230399,
author = {Muriel Boucart and Sandrine Delord and Anne Giersch},
title = {The Computation of Contour Information in Complex Objects},
journal = {Perception},
volume = {23},
number = {4},
pages = {399–409},
year = {1994d},
doi = {10.1068/p230399},
note = {PMID:7991341},
URL = {https://doi-org.crai.referencistas.com/10.1068/p230399},
eprint = {https://doi-org.crai.referencistas.com/10.1068/p230399},
abstract = {Perceptual organisation, and especially the computation of contour information, has been the object of considerable interest in the last few years. In the first part of the paper we review recent accounts on the mechanisms involved in the processing of contour. In the second part we report an experiment designed to examine (1) how physical parameters such as spatial proximity and collinearity of elements affect the integration of global contour in objects and (2) whether the activation of stored representations of objects facilitates the computation of contour. Incomplete forms varying in the spacing and the alignment of line segments on their contour were used as stimuli in a matching task. Subjects were asked to decide which of two laterally displayed figures matched a reference form presented previously. The matching target and the distractor were physically identical but differed in their orientation. In one condition the reference object was always an outline drawing of an object. In a second condition the reference object was either a complete object or a more or less identifiable incomplete form. Little variation in performance was found for forms having continuous and discontinuous contour up to a spacing of 5 pixels (10.8 min) between elements. Response times and errors increased abruptly beyond this limit. This effect occurred in the two conditions of reference stimulus, suggesting that the computation of contour information is more affected by physical constraints at early processes than by high-level processes involving activation of stored structural representations of objects.}
}

@article{doi:10.1177/0391398818790343,
author = {Leonid Goubergrits and Ulrich Kertzscher and Michael Lommel},
title = {Past and future of blood damage modelling in a view of translational research},
journal = {The International Journal of Artificial Organs},
volume = {42},
number = {3},
pages = {125–132},
year = {2019e},
doi = {10.1177/0391398818790343},
note = {PMID:30073891},
URL = {https://doi-org.crai.referencistas.com/10.1177/0391398818790343},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0391398818790343},
abstract = {Anatomic pathologies such as stenosed or regurgitating heart valves and artificial organs such as heart assist devices or heart valve prostheses are associated with non-physiological flow. This regime is associated with regions of spatially high-velocity gradients, high-velocity and/or pressure fluctuations as well as neighbouring regions with stagnant flow associated with high residence time. These hemodynamic conditions cause destruction and/or activation of blood components and their accumulation in regions with high residence time. The development of next-generation artificial organs, which allow long-term patient care by reducing adverse events and improve quality of life, requires the development of blood damage models serving as a cost function for device optimization. We summarized the studies underlining the key findings with subsequent elaboration of the requirements for blood damage models as well as a decision tree based on the classification of existing blood damage models. The four major classes are Lagrangian or Eulerian approaches with stress- or strain-based blood damage. Key challenges were identified and future steps towards the translation of blood damage models into the device development pipeline were formulated. The integration of blood damage caused by turbulence into models as well as in vitro and in vivo validation of models remain the major challenges for future developments. Both require the development of novel experimental setups to provide reliable and well-documented experimental data.}
}

@article{doi:10.1177/2472555216682725,
author = {Albert Gough and Andrew M. Stern and John Maier and Timothy Lezon and Tong-Ying Shun and Chakra Chennubhotla and Mark E. Schurdak and Steven A. Haney and D. Lansing Taylor},
title = {Biologically Relevant Heterogeneity: Metrics and Practical Insights},
journal = {SLAS DISCOVERY: Advancing the Science of Drug Discovery},
volume = {22},
number = {3},
pages = {213–237},
year = {2017f},
doi = {10.1177/2472555216682725},
note = {PMID:28231035},
URL = {https://doi-org.crai.referencistas.com/10.1177/2472555216682725},
eprint = {https://doi-org.crai.referencistas.com/10.1177/2472555216682725},
abstract = {Heterogeneity is a fundamental property of biological systems at all scales that must be addressed in a wide range of biomedical applications, including basic biomedical research, drug discovery, diagnostics, and the implementation of precision medicine. There are a number of published approaches to characterizing heterogeneity in cells in vitro and in tissue sections. However, there are no generally accepted approaches for the detection and quantitation of heterogeneity that can be applied in a relatively high-throughput workflow. This review and perspective emphasizes the experimental methods that capture multiplexed cell-level data, as well as the need for standard metrics of the spatial, temporal, and population components of heterogeneity. A recommendation is made for the adoption of a set of three heterogeneity indices that can be implemented in any high-throughput workflow to optimize the decision-making process. In addition, a pairwise mutual information method is suggested as an approach to characterizing the spatial features of heterogeneity, especially in tissue-based imaging. Furthermore, metrics for temporal heterogeneity are in the early stages of development. Example studies indicate that the analysis of functional phenotypic heterogeneity can be exploited to guide decisions in the interpretation of biomedical experiments, drug discovery, diagnostics, and the design of optimal therapeutic strategies for individual patients.}
}

@article{doi:10.1177/10943420211027937,
author = {J. Austin Harris and Ran Chu and Sean M Couch and Anshu Dubey and Eirik Endeve and Antigoni Georgiadou and Rajeev Jain and Daniel Kasen and M P Laiu and OE B Messer et al.},
title = {Exascale models of stellar explosions: Quintessential multi-physics simulation},
journal = {The International Journal of High Performance Computing Applications},
volume = {36},
number = {1},
pages = {59–77},
year = {2022g},
doi = {10.1177/10943420211027937},
URL = {https://doi-org.crai.referencistas.com/10.1177/10943420211027937},
eprint = {https://doi-org.crai.referencistas.com/10.1177/10943420211027937},
abstract = {The ExaStar project aims to deliver an efficient, versatile, and portable software ecosystem for multi-physics astrophysics simulations run on exascale machines. The code suite is a component-based multi-physics toolkit, built on the capabilities of current simulation codes (in particular Flash-X and Castro), and based on the massively parallel adaptive mesh refinement framework AMReX. It includes modules for hydrodynamics, advanced radiation transport, thermonuclear kinetics, and nuclear microphysics. The code will reach exascale efficiency by building upon current multi- and many-core packages integrated into an orchestration system that uses a combination of configuration tools, code translators, and a domain-specific asynchronous runtime to manage performance across a range of platform architectures. The target science includes multi-physics simulations of astrophysical explosions (such as supernovae and neutron star mergers) to understand the cosmic origin of the elements and the fundamental physics of matter and neutrinos under extreme conditions.}
}

@article{doi:10.4137/EBO.S419,
author = {Glenn Hickey and Frank Dehne and Andrew Rau-Chaplin and Christian Blouin},
title = {SPR Distance Computation for Unrooted Trees},
journal = {Evolutionary Bioinformatics},
volume = {4},
number = { },
pages = {EBO.S419},
year = {2008h},
doi = {10.4137/EBO.S419},
note = {PMID:19204804},
URL = {https://doi-org.crai.referencistas.com/10.4137/EBO.S419},
eprint = {https://doi-org.crai.referencistas.com/10.4137/EBO.S419},
abstract = {The subtree prune and regraft distance (dSPR) between phylogenetic trees is important both as a general means of comparing phylogenetic tree topologies as well as a measure of lateral gene transfer (LGT). Although there has been extensive study on the computation of dSPR and similar metrics between rooted trees, much less is known about SPR distances for unrooted trees, which often arise in practice when the root is unresolved. We show that unrooted SPR distance computation is NP-Hard and verify which techniques from related work can and cannot be applied. We then present an efficient heuristic algorithm for this problem and benchmark it on a variety of synthetic datasets. Our algorithm computes the exact SPR distance between unrooted tree, and the heuristic element is only with respect to the algorithm’s computation time. Our method is a heuristic version of a fixed parameter tractability (FPT) approach and our experiments indicate that the running time behaves similar to FPT algorithms. For real data sets, our algorithm was able to quickly compute dSPR for the majority of trees that were part of a study of LGT in 144 prokaryotic genomes. Our analysis of its performance, especially with respect to searching and reduction rules, is applicable to computing many related distance measures.}
}

@article{doi:10.1177/1094342020918305,
author = {Yang Liu and Wissam Sid-Lakhdar and Elizaveta Rebrova and Pieter Ghysels and Xiaoye Sherry Li},
title = {A parallel hierarchical blocked adaptive cross approximation algorithm},
journal = {The International Journal of High Performance Computing Applications},
volume = {34},
number = {4},
pages = {394–408},
year = {2020i},
doi = {10.1177/1094342020918305},
URL = {https://doi-org.crai.referencistas.com/10.1177/1094342020918305},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342020918305},
abstract = {This article presents a low-rank decomposition algorithm based on subsampling of matrix entries. The proposed algorithm first computes rank-revealing decompositions of submatrices with a blocked adaptive cross approximation (BACA) algorithm, and then applies a hierarchical merge operation via truncated singular value decompositions (H-BACA). The proposed algorithm significantly improves the convergence of the baseline ACA algorithm and achieves reduced computational complexity compared to the traditional decompositions such as rank-revealing QR. Numerical results demonstrate the efficiency, accuracy, and parallel scalability of the proposed algorithm.}
}

@article{doi:10.1177/027836498900800605,
author = {Anthony A. Maciejewski and Charles A. Klein},
title = {The Singular Value Decomposition: Computation and Applications to Robotics},
journal = {The International Journal of Robotics Research},
volume = {8},
number = {6},
pages = {63–79},
year = {1989j},
doi = {10.1177/027836498900800605},
URL = {https://doi-org.crai.referencistas.com/10.1177/027836498900800605},
eprint = {https://doi-org.crai.referencistas.com/10.1177/027836498900800605},
abstract = {The singular value decomposition has been extensively used for the analysis of the kinematic and dynamic characteristics of robotic manipulators. Due to a reputation for being nu merically expensive to compute, however, it has not been used for real-time applications. This work illustrates a for mulation for the singular value decomposition that takes advantage of the nature of robotics matrix calculations to ob tain a computationally feasible algorithm. Several applica tions, including the control of redundant manipulators and the optimization of dexterity, are discussed. A detailed illus tration of the use of the singular value decomposition to deal with the general problem of singularities is also presented.}
}

@article{doi:10.1177/002383098302600207,
author = {Brendan A. Maher and Theo C. Manschreck and Michael A.C. Molino},
title = {Redundancy, Pause Distributions and Thought Disorder in Schizophrenia},
journal = {Language and Speech},
volume = {26},
number = {2},
pages = {191–199},
year = {1983k},
doi = {10.1177/002383098302600207},
note = {PMID:6664181},
URL = {https://doi-org.crai.referencistas.com/10.1177/002383098302600207},
eprint = {https://doi-org.crai.referencistas.com/10.1177/002383098302600207},
abstract = {Recent studies have indicated that schizophrenics with current evidence of formal thought disorder (FTD) can be distinguished from schizophrenics without such disorder on measures of ability to use available redundancies in the recall of words, redundancy and lexicon variability of spoken language, and deficient motor synchrony in the production of rhythmic movements. This investigation examined pause patterns, redundancy, word frequency, and disordered thinking in a group of conservatively diagnosed schizophrenic patients, utilizing Butterworth’s model of language production. It was hypothesized that schizophrenics with FTD would be more vulnerable than those without FTD to breakdown in control processes of the interactive type than the autonomous type because of impaired capacities for involuntary attention. The results indicate that schizophrenics with FTD show a disruption of the normal systematic relationship between word redundancy and pauses in speech. This disruption is not found in schizophrenics without FTD, whose responses resemble those of normal speakers. The results are discussed in the light of attempts to integrate findings from language and motor research in schizophrenia.}
}

@article{doi:10.1068/p120203,
author = {Michael J Morgan},
title = {Mental Rotation: A Computationally Plausible Account of Transformation through Intermediate Steps},
journal = {Perception},
volume = {12},
number = {2},
pages = {203–211},
year = {1983l},
doi = {10.1068/p120203},
note = {PMID:6689208},
URL = {https://doi-org.crai.referencistas.com/10.1068/p120203},
eprint = {https://doi-org.crai.referencistas.com/10.1068/p120203},
abstract = {A critical difficulty in theories of the ‘mental rotation’ phenomenon has been to find a computationally plausible reason why the rotation should occur in small intermediate steps. It is pointed out that this difficulty is peculiar to metrical representations: if spatial relations are presented symbolically but nonmetrically, then the iterative or recursive application of minimal transformations is memory saving. A program rotter is described to illustrate this principle.}
}

@article{doi:10.1177/17442591241252417,
author = {Y Quoc Nguyen and Trieu N Huynh and Khoa Le-Cao},
title = {Comparison of the induced flow obtained with 2D and 3D CFD simulations of a solar chimney at different width-to-gap ratios},
journal = {Journal of Building Physics},
volume = {48},
number = {1},
pages = {100–126},
year = {2024m},
doi = {10.1177/17442591241252417},
URL = {https://doi-org.crai.referencistas.com/10.1177/17442591241252417},
eprint = {https://doi-org.crai.referencistas.com/10.1177/17442591241252417},
abstract = {Solar chimneys are among the most common methods for natural ventilation of buildings. Computational Fluid Dynamics (CFD) has been widely applied in research and design of solar chimneys. Most of the previous studies are based on 2D CFD models which ignore the effects of the width (the third dimension) of the air channel. In addition, the applicability of 2D models for the solar chimneys with a low width-to-gap ratio is still questionable. This study investigates both 2D and 3D CFD models for window-sized vertical solar chimneys. The 2D model is applied to the domain comprising the central plane of the channel gap (G) and height (H) while the 3D model is applied to the domain consisting of the channel gap, height, and width (W). The flow fields, flow rates and thermal efficiencies computed with the 2D and 3D models at different heights, gaps, and widths are compared. The results show that W/G is the most crucial factor. The effects of the side walls are significant at a low W/G but gradually diminishes as W/G increases. At W/G = 15, the side wall effects are confined to a region of about 2.6% W. Particularly, for W/G>8, the differences between the 2D and 3D flow rates and thermal efficiencies are within ±5%. These findings offer a reference for researchers and engineers to select between 2D and 3D CFD models for a specific solar chimney.}
}

@article{doi:10.1177/0278364906072038,
author = {Yizhar Or and Elon Rimon},
title = {Computation and Graphical Characterization of Robust Multiple-Contact                 Postures in Two-Dimensional Gravitational Environments},
journal = {The International Journal of Robotics Research},
volume = {25},
number = {11},
pages = {1071–1086},
year = {2006n},
doi = {10.1177/0278364906072038},
URL = {https://doi-org.crai.referencistas.com/10.1177/0278364906072038},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0278364906072038},
abstract = {This paper is concerned with computation and graphical characterization of robust equilibrium postures suited to quasistatic multi-legged locomotion. Quasistatic locomotion consists of postures in which the mechanism supports itself against gravity while moving its free limbs to new positions. A posture is robust if the contacts can passively support the mechanism against gravity as well as disturbance forces generated by its moving limbs. This paper is concerned with planar mechanisms supported by frictional contacts in two-dimensional gravitational environments. The kinematic structure of the mechanism is lumped into a rigid body B having the same contacts with the environment and a variable center of mass. Inertial forces generated by moving parts of the mechanism are lumped into a neighborhood of wrenches centered at the nominal gravitational wrench. The robust equilibrium postures associated with a given set of contacts become the center-of-mass locations of B that maintain equilibrium with respect to all wrenches in the given neighborhood. The paper formulates the computation of the robust center-of-mass locations as a linear programming problem. It provides graphical characterization of the robust center-of-mass locations, and gives a geometric algorithm for computing these center-of-mass locations. The paper reports experiments validating the equilibrium criterion on a two-legged prototype. Finally, it describes initial progress toward computation of robust equilibrium postures in three dimensions.}
}

@article{doi:10.3233/ISP-220001,
author = {Guido Oud and Serge Toxopeus},
title = {A technique for efficient computation of steady yaw manoeuvres using CFD},
journal = {International Shipbuilding Progress},
volume = {69},
number = {1},
pages = {3–24},
year = {2022o},
doi = {10.3233/ISP-220001},
URL = {https://doi-org.crai.referencistas.com/10.3233/ISP-220001},
eprint = {https://doi-org.crai.referencistas.com/10.3233/ISP-220001},
abstract = {Hydrodynamic loads acting on a ship can nowadays be reliably obtained from Computational Fluid Dynamics (CFD) techniques. In particular for the determination of the hydrodynamic coefficients of a mathematical manoeuvring model, the forces and moments on a ship sailing at a drift angle or with a yaw rate can be computed efficiently with CFD. While computations with a drift angle are relatively straightforward, computations involving a yaw rate present a challenge. This challenge consists in how to deal with the grid, the setup and the ship encountering its own wake when rotating. A solution based on a single grid setup with consistent boundary conditions and utilising a body force wake damping zone to remedy this challenge is proposed in this paper, leading to an effective, fast, and accurate method to compute hydrodynamic loads of a ship in steady yaw manoeuvres.}
}

@article{doi:10.1177/00218863231175508,
author = {Ryan W. Quinn and Bret Crane and Jared D. Harris and Andrew Manikas},
title = {Designed Organizational Search: A Comparative Analysis of Alternative Procedures for Learning from Success},
journal = {The Journal of Applied Behavioral Science},
volume = {59},
number = {3},
pages = {391–425},
year = {2023p},
doi = {10.1177/00218863231175508},
URL = {https://doi-org.crai.referencistas.com/10.1177/00218863231175508},
eprint = {https://doi-org.crai.referencistas.com/10.1177/00218863231175508},
abstract = {Sampling on the dependent variable is unlikely to be an effective way to learn and develop the strategy. Even so, organizations spend millions of dollars on processes such as Appreciative Inquiry that make inferences about how to adapt their strategies, routines, and practices based upon only successful examples. Two techniques that are common to this kind of learning process are searching solely for successful solutions and reframing search problems (e.g., unconditionally positive questions). We build a computational model by formalizing appreciative inquiry and comparing it with other, similar processes to understand their relative effectiveness. We find that the organizations simulated in our computational model almost always improved performance over time, despite learning solely from successful observations. Their relative effectiveness depended on the complexity of the problems, the number of iterations of learning, and how much the learning process preserved variety in potential solutions. These findings suggest that appreciative inquiry may be most effective when people take the cost and complexity of organizational problems into account before engaging in the learning process and adapt the process accordingly. These findings contribute to research on organizational learning by explaining why learners may benefit from structuring the way they communicate as they search, why reframing performance measures may dissolve search problems, and how designed organizational search enables managers to be more deliberate about organizational learning.}
}

@article{doi:10.3233/jid-2012-0009,
author = {Reza Shojanoori and Radmila Juric and Mahi Lohi},
title = {Computationally Significant Semantics in Pervasive     Healthcare},
journal = {Journal of Integrated Design and Process Science},
volume = {16},
number = {1},
pages = {43–62},
year = {2012q},
doi = {10.3233/jid-2012-0009},
URL = {https://doi-org.crai.referencistas.com/10.3233/jid-2012-0009},
eprint = {https://doi-org.crai.referencistas.com/10.3233/jid-2012-0009},
abstract = {Pervasive computing (PerC) is leading the way in a fast-growing trend of integrating transparently physical heterogeneous computational devices into our private and professional lives. The ubiquity of these devices and advances in developing software solutions in PerC across domains, have raised hopes for the creation of true wide-spread pervasive computing environments (PCE). In this paper we explore the possibility of applying semantics of PCEs in the healthcare domain, and in Self Care Homes (SeCH) in particular, in order to define and comment on its computationally significant semantics. Our aim is to illustrate that we can manipulate the computationally significant semantics of SeCH through OWL/SWRL enabled ontologies, as candidate technologies for achieving effective and automated decision making in SeCH. The possibility of reasoning upon OWL/SWRL enabled concepts and creating computations from them, and enables the delivery of healthcare services to SeCH residents. They are automatically supported by software applications generated from the Assistive Self Care Systems (ASeCS) software architecture, which hosts our OWL/SWRL enabled ontology and its reasoning.}
}

@article{doi:10.4137/BBI.S5983,
author = {Daisuke Tominaga},
title = {Periodicity Detection Method for Small-Sample Time Series Datasets},
journal = {Bioinformatics and Biology Insights},
volume = {4},
number = { },
pages = {BBI.S5983},
year = {2010r},
doi = {10.4137/BBI.S5983},
note = {PMID:21151841},
URL = {https://doi-org.crai.referencistas.com/10.4137/BBI.S5983},
eprint = {https://doi-org.crai.referencistas.com/10.4137/BBI.S5983},
abstract = {Time series of gene expression often exhibit periodic behavior under the influence of multiple signal pathways, and are represented by a model that incorporates multiple harmonics and noise. Most of these data, which are observed using DNA microarrays, consist of few sampling points in time, but most periodicity detection methods require a relatively large number of sampling points. We have previously developed a detection algorithm based on the discrete Fourier transform and Akaike’s information criterion. Here we demonstrate the performance of the algorithm for small-sample time series data through a comparison with conventional and newly proposed periodicity detection methods based on a statistical analysis of the power of harmonics. We show that this method has higher sensitivity for data consisting of multiple harmonics, and is more robust against noise than other methods. Although “combinatorial explosion” occurs for large datasets, the computational time is not a problem for small-sample datasets. The MATLAB/GNU Octave script of the algorithm is available on the author’s web site: http://www.cbrc.jp/%7Etominaga/piccolo/.}
}

@article{doi:10.1243/03093247V074303,
author = {C E Turner and J S T Cheung},
title = {Computation of post-yield behaviour in notch-bend and tension testpieces},
journal = {Journal of Strain Analysis},
volume = {7},
number = {4},
pages = {303–312},
year = {1972s},
doi = {10.1243/03093247V074303},
URL = {https://doi-org.crai.referencistas.com/10.1243/03093247V074303},
eprint = {https://doi-org.crai.referencistas.com/10.1243/03093247V074303},
abstract = {Abstract First results of an analysis of notch-bend pieces in plane strain by two-dimensional elastic-plastic finite-element computations are given. Comparisons are given of load-displacement records for three notch depths and a general non-dimensional presentation proposed. Relations between the values of surface and cracktip crack opening displacement are shown and comparison is made with a few results in plane stress, by Dugdale model and in tension. The trend of local stress and strain patterns is shown and stress triaxiality is evaluated approximately. It is concluded that nearly quantitative agreement can be reached for the general behaviour of notched pieces by the use of fairly simple two-dimensional models but many of the results could be improved by the use of better element meshes. For the study of local crack-tip deformations more detailed models or more powerful means of characterizing the crack tip are required.}
}

@article{doi:10.1177/1094342018774126,
author = {Karl-Robert Wichmann and Martin Kronbichler and Rainald Löhner and Wolfgang A Wall},
title = {Practical applicability of optimizations and performance models to complex stencil-based loop kernels in CFD},
journal = {The International Journal of High Performance Computing Applications},
volume = {33},
number = {4},
pages = {602–618},
year = {2019t},
doi = {10.1177/1094342018774126},
URL = {https://doi-org.crai.referencistas.com/10.1177/1094342018774126},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342018774126},
abstract = {This work investigates the application and interaction of optimization techniques and performance models in a computational fluid dynamics (CFD) approach employing an OpenMP parallelized, explicit, weakly compressible, finite difference–based solver for the incompressible Navier–Stokes equations using a five-point wide stencil. The presented loop and stencil optimizations lead to a 6.8× increase in per core throughput. In order to verify optimal CPU utilization, performance models are applied to the tuned code. Three different performance models are considered: a roofline-based model, utilizing purely theoretical figures, one which is enhanced by measurements, and the execution cache memory model. It is shown that the models provide reliable estimates for simple benchmarks, such as seven-point stencils for scalar Laplacians, but the estimate quality is significantly worse for the complex and tuned stencil. While it is possible to include even more details in the model, it eventually leads to a state in which it purely reproduces the benchmarks from which it was derived. Thus, the applied general-purpose performance models are found to inaccurately predict the actual performance. They overestimate the achievable performance by more than about 97% for highly tuned code. Through further code tuning, 66% of the predicted performance could be achieved.}
}

