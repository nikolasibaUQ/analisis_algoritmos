@INPROCEEDINGS{6900484,
  author={Biswas, Subhodip and Eita, Mohammad A. and Das, Swagatam and Vasilakos, Athanasios V.},
  booktitle={2014 IEEE Congress on Evolutionary Computation (CEC)}, 
  title={Evaluating the performance of Group Counseling Optimizer on CEC 2014 problems for Computational Expensive Optimization}, 
  year={2014},
  volume={},
  number={},
  pages={1076-1083},
  abstract={Group Counseling Optimizer (GCO) is a recently proposed population-based metaheuristics that simulates the ability of human beings to solve problems through counseling within a group. It is motivated by the fact that the human thinking ability is often predicted to be the most rational. This research article examines the performance of GCO on the benchmark test suite designed for the CEC 2014 Competition for Computational Expensive Optimization. Experimental results on 24 black-box optimization problems (8 test problems with 10, 20 and 30 dimensions) have been tabulated along with the algorithm complexity metrics. Additionally we investigate the parametric behavior of GCO based on these test instances.},
  keywords={Employee welfare;Vectors;Optimization;Sociology;Statistics;Linear programming;Problem-solving},
  doi={10.1109/CEC.2014.6900484},
  ISSN={1941-0026},
  month={July},}@INPROCEEDINGS{8769513,
  author={Almutairi, Mona M. and Alhamad, Nada and Alyami, Albandari and Alshobbar, Zainab and Alfayez, Heela and Al-Akkas, Noor and Alhiyafi, Jamal A. and Olatunji, Sunday O.},
  booktitle={2019 2nd International Conference on Computer Applications & Information Security (ICCAIS)}, 
  title={Preemptive Diagnosis of Schizophrenia Disease Using Computational Intelligence Techniques}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  abstract={Schizophrenia is a severe chronic mental disorder, which affects the behavior, the perception and the thinking of the patient. The purpose of this research is to develop a predictive system to preemptively diagnose Schizophrenia Disease using computational intelligence-based techniques. The system will show the possibilities of getting the disease at an early stage, which will improve the health state of the patients. This will be done using machine learning techniques. The used dataset has 86 records, which was obtained from the Machine Learning for Signal Processing (MLSP) 2014 Schizophrenia Classification Kaggle Challenge. The used techniques in this paper are Support Vector Machine (SVM), Random Forest (RF), Artificial Neural Network (ANN), and Naive Bayesian (NB). The highest accuracy was 90.6977% reached by using SVM, RF, and NB techniques while ANN technique reached 88.3721% accuracy. The obtained accuracies are reached by using 204 features. Therefore, we conclude that using SVM, RF, and NB techniques are better in this particular problem.},
  keywords={Support vector machines;Radio frequency;Diseases;Computer science;Artificial neural networks;Information technology;Machine learning;Schizophrenia Disease;Diagnose;Machine Learning;Artificial Neural Network;Random Forest;Naive Bayesian;Support Vector Machine;Functional Network Connectivity;Source-Based Morphometry},
  doi={10.1109/CAIS.2019.8769513},
  ISSN={},
  month={May},}@INPROCEEDINGS{9659152,
  author={Li, Bin and He, Yuqing and Li, Wenfeng},
  booktitle={2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={Neural-Physical Fusion Computation for Container Terminal Handling Systems by Computational Logistics and Deep Learning}, 
  year={2021},
  volume={},
  number={},
  pages={2828-2833},
  abstract={The emerging information technology and the rising computational thinking make it possible to establish new theoretical methods and engineering practices for planning, scheduling, control and decision-making of complex logistics systems. Consequently, the complex logistics systems oriented neural-physical fusion computation (CLSO-NPFC) is proposed initially to discuss and explore the decision issues of complex logistics systems at all the strategic, tactical and operational levels. By CLSO-NPFC, the container terminal oriented logistics generalized computing mechanization, automation and intelligence are constructed uniformly and tentatively, and the typical deep learning model neural computing architecture (DLM-NCA) in CLSO-NPFC is designed for the prediction of calling liner handling volume that is measured by container units rather than twenty-feet equivalent unit. A typical regional container terminal along the coast of China is selected to implement, execute and evaluate the DLM-NCA, and the DLM-NCA shows the agile, efficient and robust performance for forecasting with the low and stable computation consumption. It demonstrates the feasibility, credibility and practicality of the abstract principles, design paradigms and computing architecture in CLSO-NPFC preliminarily.},
  keywords={Deep learning;Solid modeling;Automation;Processor scheduling;Volume measurement;Computer architecture;Containers},
  doi={10.1109/SMC52423.2021.9659152},
  ISSN={2577-1655},
  month={Oct},}@INPROCEEDINGS{234877,
  author={Palmer, J. and Steele, G.L.},
  booktitle={[Proceedings 1992] The Fourth Symposium on the Frontiers of Massively Parallel Computation}, 
  title={Connection Machine model CM-5 system overview}, 
  year={1992},
  volume={},
  number={},
  pages={474-483},
  abstract={The Connection Machine model CM-5 provides high performance and ease of use for large data-intensive applications. The CM-5 architecture is designed to scale to teraflops performance on terabyte-sized problems. SPARC-based processing nodes, each with four vector pipes, are connected by two communications networks, the Data Network and the Control Network. The system combines the best features of SIMD (single-instruction multiple-data) and MIMD (multiple-instruction multiple-data) designs, integrating them into a single 'universal' parallel architecture. The processor nodes may be divided into independent computational partitions; each partition may be independently timeshared or devoted to batch processing. Programming languages include Fortran (with Fortran 90 array constructs) and C*, a parallel dialect of C. The PRISM programming environment supports source-level debugging, tracing, and profiling through a graphical interface based on X Windows.<>},
  keywords={Process control;Control systems;Bandwidth;Communication system control;Computer architecture;Weight control;Concurrent computing;Communication networks;Computer languages;Programming environments},
  doi={10.1109/FMPC.1992.234877},
  ISSN={},
  month={Oct},}@ARTICLE{8047430,
  author={Gaither, Kelly},
  journal={IEEE Computer Graphics and Applications}, 
  title={How Visualization Can Foster Diversity and Inclusion in Next-Generation Science}, 
  year={2017},
  volume={37},
  number={5},
  pages={106-112},
  abstract={Visualization researchers, developers, practitioners, and educators routinely work across traditional discipline boundaries, oftentimes in teams of people that come from a diverse blend of backgrounds, using visualizations as a common language for collaboration. There is a looming global workforce shortage in the computational science and high-tech space, primarily due to a disconnect between population demographics and the demographics of those educated to fill these jobs. The visualization community is uniquely positioned to bring a fresh approach to making diversity and inclusion fundamental tenets that are necessary rather than desirable.},
  keywords={Visualization;Computer graphics;Globalization;Employment;Research and development;computer graphics;visualization;global workforce;cross-disciplinary research;interdisciplinary skills},
  doi={10.1109/MCG.2017.3621230},
  ISSN={1558-1756},
  month={},}@INPROCEEDINGS{6402119,
  author={Hoji, Eduardo S. and Vianna, William B. and De A. Félix, Taísa},
  booktitle={2012 15th International Conference on Interactive Collaborative Learning (ICL)}, 
  title={A computer-aided math teaching approach for students in a technical institute: The experience with the Octave in the electro-mechanical technical course}, 
  year={2012},
  volume={},
  number={},
  pages={1-5},
  abstract={This paper describes the experience to teach mathematics in the electro-mechanical technical course applying a computer-aided approach. Instead of give a bunch of equations and a calculator to the students, as it is usual in technical courses, we offer them the Octave, which is a numerical computational tool, and the mathematical concepts involved, in order to them find the solution of applied problems. Despite the deficiencies the students in technical courses have in their formation, we could notice that their vision regarding mathematics has changed after discover that there is more than calculators to help them finding the solution. The approach was initially applied with a group of 25 mature students, which passed by a flawed basic educational system and stayed away from school for a long time.},
  keywords={Educational institutions;Calculators;Software;Mathematical model;technical education;mathematics teaching;numerical computation;mature students},
  doi={10.1109/ICL.2012.6402119},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{8947750,
  author={Horner, Jack K.},
  booktitle={2018 International Conference on Computational Science and Computational Intelligence (CSCI)}, 
  title={A Mathematica-Based Automated Deduction of the Marsden-Herman Theorem in Quantum Logic}, 
  year={2018},
  volume={},
  number={},
  pages={1199-1203},
  abstract={We can think of the set of propositions describing the outcome of measurements of physical systems as a set of propositions of the form “Measurand X had (didn't have) value V at time t”. In classical physics, this system of propositions has a Boolean logic (BL). In quantum mechanics, this system is isomorphic to the algebra, C(H), of closed linear subspaces of (equivalently, the system of linear operators on (observables in)) in a Hilbert space and is isomorphic to an ortholattice (OL). A consequence of the existence of non-commuting observables in quantum mechanics is that QL does not satisfy the BL distribution law. Quantum logicians have thus paid much attention to "quasi"- distributive theorems, one of the better known of which is the Marsden-Herman theorem (MHT). Informally, the MHT states that if there is a four-element cyclic chain of commuting elements in an orthomodular lattice, the distributive law holds for those elements. Here I provide a Mathematica-based automated deduction of the MHT.},
  keywords={Lattices;Quantum mechanics;Algebra;Hilbert space;Finite element analysis;Presses;Time measurement;automated deduction;quantum logic;orthomodular lattice;Hilbert space},
  doi={10.1109/CSCI46756.2018.00230},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9668455,
  author={Mason, A. and Esper, I. and Korostynska, O. and Haiddegger, T. and Popov, A. and Christensen, L. B. and Alvseike, O.},
  booktitle={2021 IEEE 21st International Symposium on Computational Intelligence and Informatics (CINTI)}, 
  title={The Meat Factory Cell: A new way of thinking for meat producers}, 
  year={2021},
  volume={},
  number={},
  pages={000091-000096},
  abstract={This paper presents the novel Meat Factory Cell (MFC) concept which is being developed in both semi- and fully-automated forms. The MFC provides several important opportunities for the red meat sector, including enhanced robustness, scalability and flexibility. Moreover, it is mindful of the need for small-medium meat processors requiring access to automation, which has proven uneconomical until now. The industry has renewed interest in such automation initiatives, particularly considering its need to improve resilience in the face of future global pandemics. The paper describes the progress of the MFC, as well as a rudimentary framework for realising the implementation. Finally, the paper discusses some of the major hurdles faced in the future.},
  keywords={Technological innovation;Automation;Program processors;Scalability;Robustness;Production facilities;Safety;automation;meat;pork;robotics;butchering},
  doi={10.1109/CINTI53070.2021.9668455},
  ISSN={2471-9269},
  month={Nov},}@INPROCEEDINGS{6417279,
  author={Khouri, Selma and Bellatreche, Ladjel and Boukhari, Ilyes and Bouarar, Selma},
  booktitle={2012 IEEE 15th International Conference on Computational Science and Engineering}, 
  title={More Investment in Conceptual Designers: Think about it!}, 
  year={2012},
  volume={},
  number={},
  pages={88-93},
  abstract={Developing database (DB) and data warehouse (DW) applications passes through three main modeling phases imposed by the ANSI/SPARC architecture: conceptual, logical and physical. This architecture creates two different actors: (i) conceptual designers and Database Administrators (DBA). The first actor collects user requirements, chooses the relevant diagrams for designing the conceptual model (or the logical model). The DBA ensures the performance, the maintenance and the tuning of the final application. Most of the tasks performed by these two actors are complex and time consuming for a majority of companies. Recently, some academic and industry research efforts are moving towards truly zero-administration of DW by proposing tools (advisors) substituting some tasks of DBA. One of the functionalities of these tools is to propose recommendations in choosing optimization structures such as indexing, materialized views, etc. They do not guarantee robust solutions. In this paper, we propose a revolutionary economical model for DB /DW application. Instead of substituting the DBA by advisors, we propose to delegate some DBA tasks to conceptual designers like selecting optimization structures. First, we propose to connect user requirements to the conceptual model of the target application. Secondly, based on the analysis of requirements, SQL queries are identified and then used to select optimization structures. Finally, a feasibility of our approach is tested through the selection of bitmap join indexes based on user requirements.},
  keywords={Optimization;Indexes;Unified modeling language;Data models;Benchmark testing;Amplitude modulation;Requirements Engineering;ANSI/SPARC Ar-chitecture;Designer;Administrator;Data Warehouse;Optimization;Experiments},
  doi={10.1109/ICCSE.2012.22},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{5952118,
  author={Levine, Daniel S.},
  booktitle={2011 IEEE Symposium on Computational Intelligence, Cognitive Algorithms, Mind, and Brain (CCMB)}, 
  title={Modeling decisions by brains that think, feel, and vegetate}, 
  year={2011},
  volume={},
  number={},
  pages={1-8},
  abstract={This paper summarizes three interrelated neural network models of data on emotionally influenced decision making: the first on a gambling task, the second on probability judgment, and the third on probability weighting. The networks incorporate data on executive regions of the brain and organizing principles such as adaptive resonance and fuzzy traces that have been utilized to model other cognitive data.},
  keywords={Encoding;Brain models;Data models;Steady-state;Basal ganglia;Neurons},
  doi={10.1109/CCMB.2011.5952118},
  ISSN={},
  month={April},}@INPROCEEDINGS{8253363,
  author={Ahmad, Adang Suwandi},
  booktitle={2017 International Symposium on Electronics and Smart Devices (ISESD)}, 
  title={Brain inspired cognitive artificial intelligence for knowledge extraction and intelligent instrumentation system}, 
  year={2017},
  volume={},
  number={},
  pages={352-356},
  abstract={Artificial intelligence evolves with the development of computers even rely on computational development. The ways and processes of human thinking developed by Psychologists and welcomed by computational experts produce the science of Artificial Intelligence. This continues with the development of cognitive science that encourages the development of Artificial Intelligence to Cognitive Thinking Intelligence, a new pathway to the science of Artificial Intelligence that can emulate human cognitive abilities even if not 100%. Emulation of human cognitive abilities is developed based on the modeling of system interaction with the environment and information fusion, which can be used to conduct Inferencing, so when this occurs repeatedly it will produce knowledge that grows. This process is called Knowledge Growing System which is Brain Inspired Cognitive Artificial Intelligence and can be used for information extraction and when applied to instrumentation system will realize Intelligent Instrumentation System.},
  keywords={Artificial intelligence;Heart;Data mining;Software;Instruments;Electrocardiography;Smart devices;Artificial Intelligence;Cognitive Artificial Intelligece;Knowledge Extraction},
  doi={10.1109/ISESD.2017.8253363},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8511891,
  author={Li, Yue and Pan, Yiqing and Liu, Wensheng and Zhang, Xingming},
  booktitle={2018 IEEE 16th Intl Conf on Dependable, Autonomic and Secure Computing, 16th Intl Conf on Pervasive Intelligence and Computing, 4th Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)}, 
  title={An Automated Evaluation System for App Inventor Apps}, 
  year={2018},
  volume={},
  number={},
  pages={230-235},
  abstract={More and more K-12 schools are paying attention to the training of Computational Thinking. A considerable amount of K-12 use blocked-based visual programming platforms such as Scratch, App Inventor, Alice and etc. MIT App Inventor is one of the more popular mobile application visualization programming platforms. Visual programming is based on the 'You see what you get' doctrine. Its simplicity and ease of use coheres with K-12 teaching principles and allows students to access to computational thinking without the burden of learning Coding grammar. Recently App Inventor is gaining momentum and traction in China at a very high speed and is expected to grow even faster in the future. Teachers using App Inventor for teaching face the problem of having to go through a very high number of App Inventor apps without any way to catalogue them. The 2017 Google App Inventor Competition alone received over 1300 entries. This article aims to devise an automated scoring method based on TF-IDF and clustering to help teachers evaluate App Inventor apps, thus greatly reducing their workload. Evaluating the method gives us an 75.42% with space for further improvement in the future.},
  keywords={Clustering algorithms;Programming;Visualization;Mobile applications;Creativity;Education;App Inventor;automated evaluation system},
  doi={10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.00048},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{8985981,
  author={Lee, Sarah and Ivy, Jessica and Stamps, Andrew},
  booktitle={2019 Research on Equity and Sustained Participation in Engineering, Computing, and Technology (RESPECT)}, 
  title={Providing Equitable Access to Computing Education in Mississippi}, 
  year={2019},
  volume={},
  number={},
  pages={1-4},
  abstract={To maintain competitiveness as a nation, the United States must broaden participation in computing education and career pathways. Integrating computational thinking and computer science in K-12 classrooms is becoming increasingly essential to the development of a responsible and innovative workforce. Further, with increased recognition of the need for computing competency, workforce development programs that target the emerging workforce that may not be on a college pathway and those citizens who want to retool for the digital economy are also essential. This study examines work by researchers and practitioners in Mississippi to engage all public school students and the workforce with computational thinking, computer science concepts, and cybersecurity, providing pathways for learning that make computing education accessible for all citizens. The state of Mississippi has the lowest median wage in the nation, and one of the lowest rates of STEM employment. With half of the public school children identified as African American, and half of them female, there is much opportunity for broadening participation in computing. Assessments from a K-12 teacher professional development programs will be discussed, in addition to outcomes from one year of a workforce development program.},
  keywords={computer science;K-12;workforce development},
  doi={10.1109/RESPECT46404.2019.8985981},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{9570851,
  author={Su, Karen and Bonnet, Evgeniia and Mondada, Francesco},
  booktitle={2021 IEEE AFRICON}, 
  title={Developing STEM and Team-working Skills Through Collaborative Space Robotics Missions}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={Building on the success of the first North-South cross-continental collaborative educational robotics mission held in November 2015 [1], this paper presents a practical model for scaling the event up into a program for developing countries, particularly those of Africa, where the first edition took place. Past participants in Remote Rescue Thymio II (R2T2) missions, students and educators alike, have shared qualitative observations about its positive educational impact, notably in the areas of Collaboration, Communication, Critical thinking, Creative problem solving, Computational thinking, and Cross-cultural exchange. The proposed framework for delivering R2T2 missions has been adapted to Africa, being specifically designed to tackle the critical challenges of internet connectivity, teacher training and program costs. In so doing, it strives to pave the way for collaboration with partners in African educational ecosystems and North-South funding bodies that share the vision of making collaborative STEM education more accessible and attractive to teachers and schoolchildren around the world.},
  keywords={Training;Computational modeling;Space missions;Ecosystems;Collaboration;Africa;Tools;educational robotics;telerobotics;space robotics;Thymio robot;STEM education;21st century skills;North-South collaboration;local education networks},
  doi={10.1109/AFRICON51333.2021.9570851},
  ISSN={2153-0033},
  month={Sep.},}@INPROCEEDINGS{1419802,
  author={Miyake, Y.},
  booktitle={Third International Conference on Creating, Connecting and Collaborating through Computing (C5'05)}, 
  title={Co-creation system and human-computer interaction}, 
  year={2005},
  volume={},
  number={},
  pages={169-172},
  abstract={The purpose of this research is to realize a "co-creation system". Co-creation means co-emergence of real-time coordination by sharing subjective space and time between different persons. Human communication with emergent reality like this is essential to improve communicability in social systems, and we assume such communication needs two kinds of information process at the same time. One is explicit communication such as the transmission of messages and the other is implicit embodied interaction such as the sympathy and direct experience. The conventional IT system mainly covers the former process but we have been pointing out the importance of the latter process. Especially, this implicit process is related with rhythmic interaction between humans, such as entrainment of body motion. From this background, using these implicit and explicit processing complementarily, we are developing co-creative man-machine interfaces and communication media. We think this dual-processing based new technology will be effective for recovering human linkage and mutual-reliability that has been weakened in modern IT society.},
  keywords={Humans;Artificial intelligence;Competitive intelligence;Real time systems;Computational intelligence;Space technology;Couplings;Machine intelligence;Application software;Joining processes},
  doi={10.1109/C5.2005.8},
  ISSN={1556-0090},
  month={Jan},}@INPROCEEDINGS{5211102,
  author={Du, Cai-Feng},
  booktitle={2009 WASE International Conference on Information Engineering}, 
  title={High Clustering Coefficient of Computer Networks}, 
  year={2009},
  volume={1},
  number={},
  pages={371-374},
  abstract={Due to the rapid development of network technology, the structure of computer network is increasingly complicated.The traditional random network model is difficult to characterize the topology of current computer network.Complex network theory provides new view and thinking to study in this field. In this paper, we give a probabilistic model to examine the evolution of computer networks by random duplication processes of node degree as well as the preferential choice mechanisms. The model is solved exactly for large network. We demonstrate that both the degree distribution and the triangle distribution have stationary properties. When the size of the network tends to infinity,the degree distribution behaves as P(k) prop k-3 and the average clustering coefficient C is independent of the network size N.},
  keywords={Computer networks;Complex networks;H infinity control;Predictive models;Educational institutions;Mathematics;Petroleum;Network topology;Numerical simulation;Computational modeling},
  doi={10.1109/ICIE.2009.276},
  ISSN={},
  month={July},}@INPROCEEDINGS{10544496,
  author={Venkateshwarlu, M. and Javeed, Sk. and Shreya, S. and Muthukumaran, N. and Vilas, B. Pranay},
  booktitle={2024 International Conference on Inventive Computation Technologies (ICICT)}, 
  title={Handwritten Paragraph Text Recognition using OCR}, 
  year={2024},
  volume={},
  number={},
  pages={1243-1246},
  abstract={The recognition of unlimited signatures is an ongoing challenge for computer vision systems. Traditionally, in paragraph text recognition, two separate models are used for character splitting and text character recognition. In this regard, this study proposes an integrated end-to-end framework and a new hybrid thinking approach to address the existing challenge. The prototype is specifically intended to handle a splitting picture per align iteratively, with three different modules. Initially, the encoder creates maps of features derived from the whole passage picture. Then, a concept module produces typically a mask with weights, which enables focused analysis of current text line features, especially implicit line splitting Finally, the decoder module is used for performing spectrum detection in each text line feature, ending with comprehensive detection of the entire paragraph offers a new solution.},
  keywords={Computer vision;Text recognition;Computational modeling;Optical character recognition;Prototypes;Feature extraction;Decoding;Implicit line splitting;Attention Module;Encoder;Decoder Module},
  doi={10.1109/ICICT60155.2024.10544496},
  ISSN={2767-7788},
  month={April},}@ARTICLE{10320320,
  author={Maulidevi, Nur Ulfa and Aji, Byan Sakura Kireyna and Hikmawati, Erna and Surendro, Kridanto},
  journal={IEEE Access}, 
  title={Modeling Integrated Sustainability Monitoring System for Carbon Footprint in Higher Education Buildings}, 
  year={2023},
  volume={11},
  number={},
  pages={135365-135376},
  abstract={Recent research has shed light on integrating cutting-edge technologies in various organisations to support daily operations in the digital age. However, this implementation has resulted in undesirable consequences, notably contributing to global warming and climate change, primarily due to unchecked carbon emissions. Higher Educational Institutions are expected to lead by example in reducing global carbon emissions. Despite some institutions’ efforts in this regard, multiple studies suggest that progress could be expedited through digital technology, leaving room for improvement. Numerous Higher Educational Institutions still need sustainable practices within their building infrastructure. Hence, this research project was developed to create an intelligent model for monitoring the carbon footprint of buildings within Higher Educational Institutions. The study focused on the Bandung Institute of Technology facilities, which encountered similar challenges stemming from various aspects contributing to the overall carbon footprint. This research effectively managed energy consumption by implementing design thinking, encompassing electricity and water usage, and reducing emissions. Furthermore, the application of this model provided valuable insights into both current and projected energy consumption and emission production. It also assessed the sustainability status of these buildings and enhanced the efficiency of information dissemination in energy reporting processes. This, in turn, facilitated the implementation of proactive measures for emission management not only at the Bandung Institute of Technology but also in other Higher Educational Institution buildings sharing similar characteristics.},
  keywords={Carbon footprint;Climate change;Lifetime estimation;Product lifecycle management;Energy management;Design engineering;Predictive models;Education;Buildings;Sustainable development;Building materials;Construction industry;Carbon footprint;life cycle assessment;energy management;monitoring systems;prediction model;design thinking},
  doi={10.1109/ACCESS.2023.3333890},
  ISSN={2169-3536},
  month={},}@ARTICLE{9917300,
  author={García-Zamora, Diego and Labella, Álvaro and Rodríguez, Rosa M. and Martínez, Luis},
  journal={IEEE Transactions on Fuzzy Systems}, 
  title={A Linguistic Metric for Consensus Reaching Processes Based on ELICIT Comprehensive Minimum Cost Consensus Models}, 
  year={2023},
  volume={31},
  number={5},
  pages={1676-1688},
  abstract={Linguistic group decision making (LiGDM) aims at solving decision situations involving human decision makers (DMs) whose opinions are modeled by using linguistic information. To achieve agreed solutions that increase DMs' satisfaction toward the collective solution, linguistic consensus reaching processes (LiCRPs) have been developed. These LiCRPs aim at suggesting DMs to change their original opinions to increase the group consensus degree, computed by a certain consensus measure. In recent years, these LiCRPs have been a prolific research line, and consequently, numerous proposals have been introduced in the specialized literature. However, we have pointed out the nonexistence of objective metrics to compare these models and decide which one presents the best performance for each LiGDM problem. Therefore, this article aims at introducing a metric to evaluate the performance of LiCRPs that takes into account the resulting consensus degree and the cost of modifying DMs' initial opinions. Such a metric is based on a linguistic comprehensive minimum cost consensus (CMCC) model based on Extended Comparative Linguistic Expressions with Symbolic Translation information that models DMs' hesitancy and provides accurate Computing with Words processes. In addition, the linguistic CMCC optimization model is linearized to speed up the computational model and improve its accuracy.},
  keywords={Linguistics;Computational modeling;Proposals;Measurement;Numerical models;Costs;Analytical models;Computing with Words (CW);extended comparative linguistic expressions with symbolic translation (ELICIT) information;fuzzy linguistic approach;linguistic cost metric;minimum cost consensus},
  doi={10.1109/TFUZZ.2022.3213943},
  ISSN={1941-0034},
  month={May},}@INPROCEEDINGS{9832076,
  author={Zhu, Ping and Lv, Pohua and Shi, Jin and Jiang, Xuetao and Zou, Weiming and Ma, Yirong},
  booktitle={2022 IEEE 2nd International Conference on Software Engineering and Artificial Intelligence (SEAI)}, 
  title={Semantic Inheritance and Overloading}, 
  year={2022},
  volume={},
  number={},
  pages={01-09},
  abstract={Humanoid reasoning and computing are the crucial research objectives of computing thinking, so semantic understanding and explicit representation are the basis steps for computing thinking. However, at present, there is no mature and systematic theory and engineering technology to fully describe massive multi-dimensional semantic expressions of natural language. Then the semantic inheritance and overloading theory was proposed to describe the semantic superposition and synthesis processing of the input vocabularies in this paper. Taking the clause as the basic semantic unit, the semantics were integrated by the local semantics of the clause itself and the global semantics of the cross clauses. The semantic frameworks were represented by the core vocabulary sequence of the clause, and the unified semantic representation grammar of the semantic frameworks to explicitly describe the local semantics and global semantics was studied; From the perspective of big data engineering, according to the unified grammar, the semantic annotation was chose as the initial step, the semantic scene based on the pattern matching of semantic frameworks was divided, the implementation technologies of semantic inheritance and overloading theory between scenes on the base of the observed language representation phenomenon were put forward, and the core algorithms of machine humanoid resolving primary mathematics application problems were realized, the effectiveness of semantic inheritance and overloading theory was preliminarily verified. The advantage was that the implementation could make full use of the power of the cloud computing to do semantic annotation and semantic framework pattern matching in parallel, and explore the engineering way to achieve natural language semantic understanding and computing thinking.},
  keywords={Vocabulary;Systematics;Annotations;Semantics;Natural languages;Humanoid robots;Grammar;computing thinking;semantic understanding;semantic inheritance;semantic overloading;humanoid resolving},
  doi={10.1109/SEAI55746.2022.9832076},
  ISSN={},
  month={June},}@INPROCEEDINGS{8247835,
  author={Balci, Osman and Fujimoto, Richard M. and Goldsman, David and Nance, Richard E. and Zeigler, Bernard P.},
  booktitle={2017 Winter Simulation Conference (WSC)}, 
  title={The state of innovation in modeling and simulation: The last 50 years}, 
  year={2017},
  volume={},
  number={},
  pages={821-836},
  abstract={Innovation in Modeling and Simulation (M&S) refers to exploiting new ideas, exploiting new technology, and employing out-of-the-box thinking, which lead to the creation of new methodologies, techniques, concepts, frameworks, and software. This paper addresses the following questions: (a) what was the state of the art in M&S 50 years ago, what is it today, how much progress has been made? (b) how much innovation in M&S has been accomplished over the last half a century? (c) what were the obstacles to innovation in M&S? (d) what are some recommendations to promote innovation in M&S? (e) what message should be sent to the funding agencies to encourage innovation in M&S?},
  keywords={Handheld computers;Data models;Computational modeling;Computer simulation;Cloud computing;Conferences;Technological innovation},
  doi={10.1109/WSC.2017.8247835},
  ISSN={1558-4305},
  month={Dec},}@INPROCEEDINGS{7509810,
  author={Chen, Jinhua and Jiang, Qin and Wang, Yuxin and Tang, Jing},
  booktitle={2016 IEEE International Conference on Big Data Analysis (ICBDA)}, 
  title={Study of data analysis model based on big data technology}, 
  year={2016},
  volume={},
  number={},
  pages={1-6},
  abstract={The traditional data analysis are based on the cause and effect relationship, formed a sample microscopic analysis, qualitative and quantitative analysis, the thinking mode of trend extrapolation analysis. Big data has a fundamental impact on the traditional data analysis. Big data analysis based on correlation, formed global macro analysis, data and technical analysis, correlation analysis and new thinking mode of correlation analysis. Namely, from causal analysis to correlation analysis and knowledge discovery, from model fitting to data mining, from logical reasoning to association rules. Data analysis in the era of big data have taken great changes, Namely, Big data analysis, from the analysis of objects, the mode of data processing, analytical methods and tools, analytical thinking.},
  keywords={Data analysis;Big data;Statistical analysis;Data models;Correlation;Analytical models;Computational modeling;big data;data analysis;qualitative and quantitative analysis},
  doi={10.1109/ICBDA.2016.7509810},
  ISSN={},
  month={March},}@INBOOK{9986161,
  author={Lee, Clifford and Soep, Elisabeth and Kyles, Kyra and Emdin, Christopher},
  booktitle={Code for What?: Computer Science for Storytelling and Social Justice}, 
  title={2 A Framework: Critical Computational Expression}, 
  year={2022},
  volume={},
  number={},
  pages={37-52},
  abstract={To gain clarity on how we think about our world, first we have to understand how our experiences and underlying ideologies shape the way we see it. In academic discourse, conceptual frameworks provide us with the language and a lens to understand our research. They supply mental maps or visual organizers for ideas. And they orient our present and future selves: how to think, what to say, how to act, and how to make sense of whatever we encounter.},
  keywords={},
  doi={},
  ISSN={},
  publisher={MIT Press},
  isbn={9780262371841},
  url={https://ieeexplore-ieee-org.crai.referencistas.com/document/9986161},}@INPROCEEDINGS{5522827,
  author={Chakradhar, Srimat T. and Raghunathan, Anand},
  booktitle={Design Automation Conference}, 
  title={Best-effort computing: Re-thinking parallel software and hardware}, 
  year={2010},
  volume={},
  number={},
  pages={865-870},
  abstract={With the advent of mainstream parallel computing, applications can obtain better performance only by scaling to platforms with larger numbers of cores. This is widely considered to be a very challenging problem due to the difficulty of parallel programming and the bottlenecks to efficient parallel execution. Inspired by how networking and storage systems have scaled to handle very large volumes of packet traffic and persistent data, we propose a new approach to the design of scalable, parallel computing platforms. For decades, computing platforms have gone to great lengths to ensure that every computation specified by applications is faithfully executed. While this design philosophy has remained largely unchanged, applications and the basic characteristics of their workloads have changed considerably. A wide range of existing and emerging computing workloads have an inherent forgiving nature. We therefore argue that adopting a best-effort service model for various software and hardware components of the computing platform stack can lead to drastic improvements in scalability. Applications are cognizant of the best-effort model, and separate their computations into those that may be executed on a best-effort basis and those that require the traditional execution guarantees. Best-effort computations may be exploited to simply reduce the computing workload, shape it to be more suitable for parallel execution, or execute it on unreliable hardware components. Guaranteed computations are realized either through an overlay software layer on top of the best-effort substrate, or through the use of application-specific strategies. We describe a system architecture for a best-effort computing platform, provide examples of parallel software and hardware that embody the best-effort model, and show that large improvements in performance and energy efficiency are possible through the adoption of this approach.},
  keywords={Concurrent computing;Hardware;Application software;Parallel processing;Parallel programming;Telecommunication traffic;Computer applications;Scalability;Shape;Computer architecture;Best effort systems;parallel computing;multi core;performance;scalability},
  doi={10.1145/1837274.1837492},
  ISSN={0738-100X},
  month={June},}@INPROCEEDINGS{4736743,
  author={Demchenko, Yuri and de Laat, Cees and Koeroo, Oscar and Groep, David},
  booktitle={2008 IEEE Fourth International Conference on eScience}, 
  title={Re-thinking Grid Security Architecture}, 
  year={2008},
  volume={},
  number={},
  pages={79-86},
  abstract={The security models used in Grid systems today strongly bear the marks of their diverse origin. Historically retrofitted to the distributed systems they are designed to protect and control, the security model is usually limited in scope and applicability, and its implementation tailored towards a few specific deployment scenarios. A common approach towards even the "basic" elements such as authentication to resources is only now emerging, whereas for more complex issues such as community organization, integration of site access control with operating systems, cross-domain resource provisioning, or overlay community Grids ("late authentication" for pilot job frameworks or community-based virtual machines) there is no single coherent and consistent "security" view. Via this paper we aim to share some observations on current security models and solutions found in Grid architectures and deployments today and identify architectural limitations in solving complex access control and policy enforcement scenarios in distributed resource management. The paper provides a short overview of the OGSA security services and other security solutions used in Grid middleware and operations practice. However, it is becoming clear that further development in Grid requires a fresh look at the concepts, both operationally and security-wise. This paper analyses the security aspects of different types of Grids and a set of use cases that may require extended security functionality, such as dynamic security context management, and management of stateful services. Recent developments in open systems security, and revisiting basic security concepts in networking and computing including the OSI security architecture and the concepts used in the trusted computing base provide interesting examples on how some of the conceptual security problems in Grid can be addressed, and on how the shortcomings of current systems and the frequently proposed "ad-hoc" stop-gaps for what are in fact complex security manageability problems may be avoided. This paper is thus intended to initiate and stimulate the wider discussion on the concepts of Grid security, thereby setting the scene for and providing input to a Grid security taxonomy leading to a more consistent Grid security architecture.},
  keywords={Security;Authentication;Access control;Open systems;Computer networks;Grid computing;Protection;Operating systems;Virtual machining;Resource management;Grids;Open Grid Security Architecture;Trusted Computing Base;Reference Monitor;Security models;Security Context;Authentication;Authorisation session},
  doi={10.1109/eScience.2008.53},
  ISSN={},
  month={Dec},}
