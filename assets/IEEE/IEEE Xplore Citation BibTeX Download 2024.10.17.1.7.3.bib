@ARTICLE{5408527,
  author={Mackay, D. M.},
  journal={IEEE Transactions on Systems, Man, and Cybernetics}, 
  title={The Mechanics of ``Tacit Knowing''}, 
  year={1974},
  volume={SMC-4},
  number={1},
  pages={94-95},
  abstract={The term ``tacit knowing'' has been much used to denote certain aspects of human knowledge that are only partly verbalizable. The claim which it presupposes, that ``we know more than we can tell,'' has sometimes been taken to imply that mechanistic approaches to the understanding or simulation of human thought are in principle foredoomed to failure. This correspondence argues that a general-purpose goal-pursuing information system, equipped to represent the structure of its world in terms of conditional constraints on action and the planning of action, would automatically show signs of ``tacit knowledge.'' Examples are cited ofmechanistic principles (including the use of interactive elements that combine digital and analog information processing) which would lend themselves to the operation of such a system and would be able to handle information unverbalizable by the system itself. It is suggested that programs seeking to simulate only introspectively verbalizable thinking are methodologically inadequate to represent the full range of human intelligence.},
  keywords={Humans;Organizing;Information systems;Information processing;Inspection;Analog computers;Computational modeling;Artificial intelligence;Psychology;Computer science education},
  doi={10.1109/TSMC.1974.5408527},
  ISSN={2168-2909},
  month={Jan},}@ARTICLE{299539,
  author={Comerford, R.},
  journal={IEEE Spectrum}, 
  title={Mecha...what? [mechatronics]}, 
  year={1994},
  volume={31},
  number={8},
  pages={46-49},
  abstract={Pioneered in Japan embraced in Europe and the United States, the engineering discipline of mechatronics seeks to design optimum performance into subsystems of electromechanical products. Mechatronics is the synergistic combination of precision mechanical engineering, electronic control and systems thinking in the design of products and manufacturing processes. The author examines the benefits of mechatronics by discussing the example of the design of an electronic braking system for automobiles.<>},
  keywords={Mechatronics;Concurrent engineering;Computational modeling;Libraries;Design engineering;System performance;Chemical industry;Design automation;Solid modeling;Europe},
  doi={10.1109/6.299539},
  ISSN={1939-9340},
  month={Aug},}@INPROCEEDINGS{4148710,
  author={Hirankitti, Visit and Krohkaew, Jaturapith},
  booktitle={First Asia International Conference on Modelling & Simulation (AMS'07)}, 
  title={An Agent Approach for Intelligent Traffic-Light Control}, 
  year={2007},
  volume={},
  number={},
  pages={496-501},
  abstract={In this paper we have adopted an agent approach for traffic light control. According to this approach, our system consists of agents and their world. In the traffic context, the world consists of cars, road networks, traffic lights, etc. Each of these agents controls all traffic lights at a road junction by an observe-think-act cycle. That is, the agent repeatedly observes the current traffic condition at the junction, it then uses this information to reason with condition-action rules to determine how the agent should act in what traffic condition, and finally it performs those actions in order to efficiently manage the traffic flows. We have also developed a NetLogo-based traffic simulator to serve as the agents' world. Our approach is experimented with traffic control of a few connected junctions and the result obtained is promising; it can reduce the average delayed time of each car at each traffic-light near a junction rather substantially when compared with other approaches},
  keywords={Intelligent agent;Intelligent control;Traffic control;Lighting control;Communication system traffic control;Roads;Intelligent transportation systems;Delay effects;Engines;Computational modeling},
  doi={10.1109/AMS.2007.11},
  ISSN={},
  month={March},}@ARTICLE{7160906,
  author={Endert, Alex and Chang, Remco and North, Chris and Zhou, Michelle},
  journal={IEEE Computer Graphics and Applications}, 
  title={Semantic Interaction: Coupling Cognition and Computation through Usable Interactive Analytics}, 
  year={2015},
  volume={35},
  number={4},
  pages={94-99},
  abstract={The success of visual analytics is predicated on the ability of users to interactively explore information. Humans think about their data through interactive visual exploration, including testing hypotheses, exploring anomalies, and other cognitive processes of building understanding from data. The claim that these insights are generated as a result of the interaction led the attendees at the Pacific Northwest National Laboratory (PNNL) workshop on "Semantic Interaction: Coupling Cognition and Computation through Usable Interactive Analytics" to posit that user interaction must play a more central role in visual analytics systems, serving as the method for coupling cognition and computation. The claims and design principles discussed in this workshop report present research directions to advance visual analytics via a user interaction approach called semantic interaction.},
  keywords={Visual analytics;Computational modeling;Semantics;Analytical models;Data models;Cognition;computer graphics;visualization;visual analytics;information visualization;semantic interaction;sensemaking},
  doi={10.1109/MCG.2015.91},
  ISSN={1558-1756},
  month={July},}@INPROCEEDINGS{9418294,
  author={Acharya, Heta and Mehta, Rutvik and Kumar Singh, Dheeraj},
  booktitle={2021 5th International Conference on Computing Methodologies and Communication (ICCMC)}, 
  title={Alzheimer Disease Classification Using Transfer Learning}, 
  year={2021},
  volume={},
  number={},
  pages={1503-1508},
  abstract={In recent years, transfer learning has gained huge popularity in solving problems from various fields including the medical image analysis. Clinical picture examination has altered medical services in the course of recent years, permitting specialists to discover disease earlier and improve patient recovery. Imaging has assumed a significant role in the finding of Alzheimer disease (AD). AD is a progressive neurological issue that gradually destroy memory and thinking skills of human. Initially, Computed Tomography scan (CT) and then Magnetic Resonance Imaging (MRI) were utilized to discover reasons of dementia in AD patients. This research aims to classify MRI of Alzheimer disease patients into multiple class by using VGG16, ResNet -50 and AlexNet as transfer learning models along with convolution neural networks. There are some stages of AD like mild cognitive impairment, mild Alzheimer’s, moderate Alzheimer’s and severe impairment. The proposed strategies show results with an accuracy of 95.70%, this represents a substantial improvement in accuracy over previous studies, demonstrating the efficacy of the proposed method.},
  keywords={Magnetic resonance imaging;Computational modeling;Computed tomography;Transfer learning;Training data;Data models;Real-time systems;Alzheimer’s disease prediction;Alzheimer’s stage classification;brain MRI;transfer learning;convolution neural network;Alexnet;ResNet 50;and VGG16},
  doi={10.1109/ICCMC51019.2021.9418294},
  ISSN={},
  month={April},}@INPROCEEDINGS{10182534,
  author={Kanna, R. Kishore and Subha Ramya, V. and Khafel, Asraa Ahmed and Jabbar, Kadim A. and Al-Tahee, Mustafa and Khalid, Raed},
  booktitle={2023 3rd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE)}, 
  title={Cognitive Disability Prediction & Analysis using Machine Learning Application}, 
  year={2023},
  volume={},
  number={},
  pages={1359-1364},
  abstract={A person with mild cognitive disability (MCD), a kind of memory loss that affects both memory and thinking skills, may be at an increased risk of acquiring dementia brought on by Alzheimer's disease or other neurological diseases. MCD affects between 13 and 19% of those who are 60 years of age or older. People who suffer from cognitive abnormalities should seek therapy and diagnosis as soon as they can. The major effect of MCD on the target is its effect on memory. Accurate MCD diagnosis is quite challenging with the current approaches. A hybrid approach is put forward in this study to identify MCD at an early stage. EEG data from MCD individuals and healthy controls was collected for this purpose. With the use of machine learning models including Support Vector Machines (SVM), Decision Trees (DT), k-Nearest Neighbour (KNN), and the hybrid approach ACO KNN, Renyi entropy (RE) and Discrete Wavelet Transform (DWT) characteristics were retrieved (combined Ant Colony Optimisation with k-Nearest Neighbour). The performance of the system is assessed based on an accuracy comparison with machine learning models. When compared to other models, RE and ACO KNN had an accuracy of 85.0%.},
  keywords={Support vector machines;Neurological diseases;Computational modeling;Medical treatment;Machine learning;Transforms;Brain modeling;EEG;MCD;DWT;KNN;SVM;ML},
  doi={10.1109/ICACITE57410.2023.10182534},
  ISSN={},
  month={May},}@INPROCEEDINGS{7040962,
  author={Che, Shuai},
  booktitle={2014 IEEE High Performance Extreme Computing Conference (HPEC)}, 
  title={GasCL: A vertex-centric graph model for GPUs}, 
  year={2014},
  volume={},
  number={},
  pages={1-6},
  abstract={There are increasing research efforts of using GPUs for graph processing. Most prior work on accelerating GPGPU graph algorithms has been focused on algorithm and device-specific optimizations. There is little research on studying high-level programming models and associate run-time systems for graph processing on GPUs, which will be useful to solve diverse real-world problems flexibly. This paper presents a preliminary implementation of a graph framework, GasCL, supporting the well-known “think-like-a-vertex” programming model. The system is built on top of OpenCL and portable across diverse accelerators. We describe our design and use two applications as case studies. The initial performance result shows an average of 2.5× speedup on a GPU compared with a CPU.},
  keywords={Graphics processing units;Kernel;Arrays;Runtime;Computational modeling;Programming},
  doi={10.1109/HPEC.2014.7040962},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{6919083,
  author={Ismail, Sabir and Rahman, M. Shahidur},
  booktitle={2014 International Conference on Electrical Engineering and Information & Communication Technology}, 
  title={Bangla word clustering based on N-gram language model}, 
  year={2014},
  volume={},
  number={},
  pages={1-5},
  abstract={In this paper, we describe a method for producing Bangla word clusters based on semantic and contextual similarity. Word clustering is important for parts of speech (POS) tagging, word sense disambiguation, text classification, recommender system, spell checker, grammar checker, knowledge discover and for many others Natural Language Processing (NLP) applications. Computerization of Bangla language processing has been started a long ago, but still it is in neophyte stage and suffers from resource scarcity. We propose anunsupervised machine learning technique to develop Bangla word clusters based on their semantic and contextual similarity using N-gram language model. According to N-gram model, a word can be predictedbased on its previous and next words sequence. N-gram model is applied successfully for word clustering in English and some other languages. As word clustering in Bangla is a new dimension in Bangla language processing research, so we think this process is good way to start and our assumption is true as our result is quite decent. We produced 456 clusters using a locally available large Bangla corpus. Subjective score derived from the clusters reveal strong similarity of the words in the same cluster.},
  keywords={Computational modeling;Semantics;Natural language processing;Context;Mathematical model;Educational institutions;Speech processing;word cluster;information retrival;natural language processing;machine learning;n-gram model},
  doi={10.1109/ICEEICT.2014.6919083},
  ISSN={},
  month={April},}@INPROCEEDINGS{4419756,
  author={Eldabi, Tillal and Young, Terry},
  booktitle={2007 Winter Simulation Conference}, 
  title={Towards a framework for healthcare simulation}, 
  year={2007},
  volume={},
  number={},
  pages={1454-1460},
  abstract={The changing needs of healthcare provision around the world are forcing service designers and decision makers to adopt new tools in design and evaluation of processes. Apart from the pressure to deliver better services from constrained resources, the increasing use of metrics to monitor and manage care delivery, also means that service providers require a clearer idea of how a service improvement will perform prior to implementation. In turn, this opens up an opportunity for a much greater use of simulation and modeling techniques, provided they can be set within an appropriate framework. This paper discusses and describes a research project aimed at conducting pilot work for developing a framework that facilitates joined up thinking and enables integrative modeling. An approach to achieving such an end is described and progress to date is reported. Since this is an ongoing project, some of the latest results are presented at the conference.},
  keywords={Medical services;Costs;Pharmaceutical technology;Joining processes;Computational modeling;Monitoring;Drugs;Government;Technological innovation;Information systems},
  doi={10.1109/WSC.2007.4419756},
  ISSN={1558-4305},
  month={Dec},}@INPROCEEDINGS{7019954,
  author={Elmegreen, Bruce G. and Sanchez, Susan M. and Szalay, Alexander S.},
  booktitle={Proceedings of the Winter Simulation Conference 2014}, 
  title={The future of computerized decision making}, 
  year={2014},
  volume={},
  number={},
  pages={943-949},
  abstract={Computerized decision making is becoming a reality with exponentially growing data and machine capabilities. Some decision making is extremely complex, historically reserved for governing bodies or market places where the collective human experience and intelligence come to play. Other decision making can be trusted to computers that are on a path now into the future through novel software development and technological improvements in data access. In all cases, we should think about this carefully first: what data are really important for our goals and what data should be ignored or not even stored? The answer to these questions involves human intelligence and understanding before the data-to-decision process begins.},
  keywords={Computational modeling;Data models;Computers;Adaptation models;Decision making;Big data;Analytical models},
  doi={10.1109/WSC.2014.7019954},
  ISSN={1558-4305},
  month={Dec},}@INPROCEEDINGS{6011908,
  author={Tang, Ketan and Au, Oscar C. and Fang, Lu and Zhiding Yu and Yuanfang Guo},
  booktitle={2011 IEEE International Conference on Multimedia and Expo}, 
  title={How anti-aliasing filter affects image contrast: An analysis from majorization theory perspective}, 
  year={2011},
  volume={},
  number={},
  pages={1-6},
  abstract={When we design an anti-aliasing low pass filter, it is usually an IIR filter. We need to truncate the filter to an FIR filter. One may think that the more taps there are, the better the image quality is. However, we find that there exists an optimal value of tap number that will give the best visual quality. Filters with larger or smaller number of taps will degrade the image quality, due to the fact that the image contrast is reduced. In this paper we analyze this phenomenon using majorization theory and find that the image contrast can be formulated as a Schur convex function on filter coefficients. We also propose an effective method to choose the best filter so that the image contrast is maximized, so as to give best visual quality.},
  keywords={Visualization;Convex functions;Finite impulse response filter;Correlation;IIR filters;Filtering theory;Computational complexity;anti-aliasing filter;contrast;majorization theory;Schur convex},
  doi={10.1109/ICME.2011.6011908},
  ISSN={1945-788X},
  month={July},}@INPROCEEDINGS{8343242,
  author={Lee, KangYoon and Ha, Neul},
  booktitle={2018 International Conference on Information Networking (ICOIN)}, 
  title={AI platform to accelerate API economy and ecosystem}, 
  year={2018},
  volume={},
  number={},
  pages={848-852},
  abstract={Nowadays Enterprises are pursuing Digital Transformation to build a new business model and internal processes. Cloud Computing, Big Data, Mobile and Social are top subject and the direction is how to integrate their legacy process and make a convergence to new model. AI and Deep learning technology are rapidly emerged and becoming really critical of new solutions and the solution definitely uses cloud platform by API services and Pre-built assets. It drives API economy growth by global service providers' platform services. API Economy generates a lot of application diversity and platform provides the services, which can support Digital Transformation. Once they build their platform by themselves or 3rd party platform services, they should think about the expansion of their business via the collaboration of business partners. This collaboration model is ecosystem generation to build new partnership. By providing services to business partners they can expand customer who are dealing with business partners. The collaboration of platform service with business partner and the customer will be the new way of enterprise revolution as a part of the 4th industrial revolution.},
  keywords={Machine learning;Business;Cloud computing;Acceleration;Ecosystems;Computational modeling;Artificial Intelligence;Cloud Platform;API Economy;Ecosystem},
  doi={10.1109/ICOIN.2018.8343242},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{7552216,
  author={Deng, Chunyan and Zhou, Zhiguo and Li, Wenqing and Hou, Boyu},
  booktitle={2016 IEEE 40th Annual Computer Software and Applications Conference (COMPSAC)}, 
  title={A Panoramic Geology Field Trip System Using Image-Based Rendering}, 
  year={2016},
  volume={2},
  number={},
  pages={264-268},
  abstract={Geology field trip is an important teaching step for the majors of geology, natural resources prospecting engineering and geographical sciences to learn geology knowledge and grasp field working skills. There are some problems in the traditional geology field trip teaching. Inspired by the Google Maps Street View, the author proposes applying the Image-Based Rendering (IBR), a branch of Virtual Reality (VR), to the field of geology field trip teaching. In this paper, the author designs and implements a panoramic geology field trip system for College of Earth Sciences of Jilin University, in which students can preview the geology field trip location by roaming in the three-dimension (3D) virtual environment. The practice shows that not only can the system help teach 3D thinking, but also reduce the teaching cost, improve students' learning efficiency and enhance learning interest.},
  keywords={Geology;Education;Three-dimensional displays;Solid modeling;Virtual environments;Computational modeling;Earth;Image-Based Rendering;Geology Field Trip;Virtual Reality},
  doi={10.1109/COMPSAC.2016.33},
  ISSN={0730-3157},
  month={June},}@INPROCEEDINGS{5972294,
  author={Chandramohan, D. and Jayakumar, S. K. V. and Khapre, Shailesh and Kishore, M. S. Nanda},
  booktitle={2011 International Conference on Recent Trends in Information Technology (ICRTIT)}, 
  title={Dwse-simulator for distributed web service environment}, 
  year={2011},
  volume={},
  number={},
  pages={1203-1208},
  abstract={In the world of distributed web services, its environment and features are more vital to afford and satisfy the requested users query for communicating globally and virtually with the help of DWS (Distributed Web Servers), An organizational datasets are considered to be distributed as a service only its components and pair are interconnected with each other in a relatively insubstantial approach used in own home networks. If they think to share or communicate their techniques and data across globally through internet there must be a strong cohesion in those sharing which need more dominant issues throughout global network. In this paper we are proposing to develop a Distributed Web Service Evaluator-DWSE, which will help to evaluate the features of web service in DWS environment using traditional testing techniques, this kit help the clients and their dependable tools to integrate automatically in the DWS world. By fabricating the concepts encoded with SOAP (Simple Object Access Protocol), WSDL(Web Service Description Language) and XML (Extensible Mark-up Language) with a unique and specific structure for designing a light weighted application in the form of simulator test kit supervising in DWSE. The proposed test kit helps the user and providers to resolve many service conflicts in a timely and consistent service approaches.},
  keywords={Service oriented architecture;XML;Simple object access protocol;Computational modeling;Scalability;XML;SOAP;WSDL;UDDI;Web Service;Web Service Standards;Distributed simulation;Service Oriented Architecture-SOA;Distributed Web Service;XML Schema;Service Oriented Computing;HTTP;QOS},
  doi={10.1109/ICRTIT.2011.5972294},
  ISSN={},
  month={June},}@INPROCEEDINGS{7264381,
  author={Huang, Bing-Hao and Dai, Bi-Ru},
  booktitle={2015 16th IEEE International Conference on Mobile Data Management}, 
  title={A Weighted Distance Similarity Model to Improve the Accuracy of Collaborative Recommender System}, 
  year={2015},
  volume={2},
  number={},
  pages={104-109},
  abstract={Collaborative filtering is one of the most widely used methods to provide product recommendation in online stores. The key component of the method is to find similar users or items by using user-item matrix so that products can be recommended based on the similarities. However, traditional collaborative filtering approaches compute the similarity between a target user and the other user without considering a target item. More specifically, they give an equal weight to each of the items which are rated by both users. However, we think that the similarity between the target item and each of the co-rated items is a very important factor when we calculate the similarity between two users. Therefore, in this paper we propose a new similarity function that takes similarities between a target item and each of the co-rated items and the proportion of common ratings into account. Experimental results from Movie Lens dataset show that the method improves accuracy of recommender system significantly.},
  keywords={Collaboration;Recommender systems;Accuracy;Social network services;Computational modeling;Predictive models;Recommendation system;Collaborative filtering;Similarity measure},
  doi={10.1109/MDM.2015.43},
  ISSN={2375-0324},
  month={June},}@INPROCEEDINGS{8466463,
  author={Liu, Jun and Shang, Wenqian and Lin, Weiguo},
  booktitle={2018 IEEE/ACIS 17th International Conference on Computer and Information Science (ICIS)}, 
  title={Improved Stacking Model Fusion Based on Weak Classifier and Word2vec}, 
  year={2018},
  volume={},
  number={},
  pages={820-824},
  abstract={Stacking model Fusion is a combination classification method for natural language processing and text categorization. Compared to a single weak classifier, model Fusion has the advantage of combining the classification strengths of multiple classifiers, so the combination classifier is often more accurate than a single classifier, and the research of this field has been developed rapidly in recent years, and the combination classifier has been applied in various natural language processing tasks. But only by using the prediction results of the first layer weak classifier to train the second layer classifier, it has a strong limitation, only considers the training of the classification result and ignores the semantic information. We think that the method of training the weak classifier by TFIDF to the document, the expression of the document is not enough, only the information about the frequency of the document and the document is lack of the semantic information of the word2vector. In this paper, a new combination classification method is proposed, which combines the various weak classifiers trained by TFIDF and Word2vector to express the documents in many aspects, and the feature expression can fully utilize the information provided by the document. It has better classification effect than individual word2vector expression and classification and simple weak classifier combination classification.},
  keywords={Classification algorithms;Training;Stacking;Text categorization;Semantics;Computational modeling;Machine learning algorithms;model fusion;TFIDF;word2vec;combination classifier},
  doi={10.1109/ICIS.2018.8466463},
  ISSN={},
  month={June},}@INPROCEEDINGS{4561692,
  author={Demo, G. Barbara and Marcianò, Giovanni and Siega, Simonetta},
  booktitle={2008 Eighth IEEE International Conference on Advanced Learning Technologies}, 
  title={Concrete Programming: Using Small Robots in Primary Schools}, 
  year={2008},
  volume={},
  number={},
  pages={301-302},
  abstract={Small robots are very simple computers that can move autonomously. Their use in primary schools allows pupils to have concrete yet full programming experiences at the age in which Piaget situates the concrete operational stage of cognitive development. Indeed, for their first robotic activities, pupils think of paths where the robot moves forward, decides which direction to go when getting to a crossroad or repeats part of its previous trip. In planning a path for their robot, children walk it themselves thus finding out its successive tangible parts and related features. Our pupils use NQCBaby and NXCJunior programming languages for the formal specification of their robot's behaviour. These are textual languages, mother-tongue-based and Logo-like, in order to be oriented to children rather than to robots. Thus, when programming small robots, children are introduced to a deep computer competence because they deal with the basic blocks of algorithmics (sequence, selection and iteration) and learn how to specify them.},
  keywords={Concrete;Robot programming;Cognitive robotics;Educational institutions;Legged locomotion;Robot sensing systems;Path planning;Computational geometry;Virtual colonoscopy;Computer languages;Primary Schools;Mindstorms;programmable robots},
  doi={10.1109/ICALT.2008.190},
  ISSN={2161-377X},
  month={July},}@INPROCEEDINGS{8659267,
  author={Caro, Veronica and Carter, Brandon and Dagli, Sahil and Schissler, Mark and Millunchick, Joanna},
  booktitle={2018 IEEE Frontiers in Education Conference (FIE)}, 
  title={Can Virtual Reality Enhance Learning: A Case Study in Materials Science}, 
  year={2018},
  volume={},
  number={},
  pages={1-4},
  abstract={This Innovative Practice Work in Progress tests whether virtual reality (VR) can enhance students’ understanding in scientific fields, specifically Materials Science and Engineering (MSE), when compared to more traditional approaches. Of interest is how VR-based learning activities impact the performance of individuals with experience ranging from none to expert level in MSE compared to paper-based learning activities. To test this, an activity related to crystal structures, similar to what students would see in an introductory level MSE course, was administered to a group of students with varying knowledge levels in MSE. Each participant completed the same worksheet in either VR or on paper. The testing group was composed of seven students, which was too small of a sample size to draw definitive conclusions, yet significant observations could be made. On questions that required recall of prior knowledge, participants using paper-based activities generally performed better, whereas on questions requiring more spatial reasoning and critical thinking, VR participants generally performed better. Most of the participants reported enjoying the VR activities and platform, indicating high usability. These results suggest that VR may be beneficial in teaching complex spatial concepts.},
  keywords={Materials science and technology;Virtual reality;Crystals;Education;Three-dimensional displays;Solid modeling;Computational modeling;Virtual Reality;Engineering;Education;Materials;Science},
  doi={10.1109/FIE.2018.8659267},
  ISSN={2377-634X},
  month={Oct},}@INPROCEEDINGS{6628581,
  author={Santosh, K.C. and Belaïd, Abdel},
  booktitle={2013 12th International Conference on Document Analysis and Recognition}, 
  title={Document Information Extraction and Its Evaluation Based on Client's Relevance}, 
  year={2013},
  volume={},
  number={},
  pages={35-39},
  abstract={In this paper, we present a model-based document information content extraction approach and perform in-depth evaluation based on clients' relevance. Real-world users i.e., clients first provide a set of key fields from the document image which they think are important. These are used to represent a graph where nodes (i.e., fields) are labelled with dynamic semantics including other features and edges are attributed with spatial relations. Such an attributed relational graph (ARG) is then used to mine similar graphs from a document image that are used to reinforce or update the initial graph iteratively each time we extract them, in order to produce a model. Models therefore, can be employed in the absence of clients. We have validated the concept and evaluated its scientific impact on real-world industrial problem, where table extraction is found to be the best suited application.},
  keywords={Feature extraction;Computational modeling;Optical character recognition software;Measurement;Data mining;Semantics;Vectors;Document information exploitation;graph mining;table extraction},
  doi={10.1109/ICDAR.2013.16},
  ISSN={2379-2140},
  month={Aug},}@ARTICLE{364953,
  author={Krueger, M.W.},
  journal={IEEE Computer Graphics and Applications}, 
  title={Automating virtual reality}, 
  year={1995},
  volume={15},
  number={1},
  pages={9-11},
  abstract={The author discusses some applications of virtual reality and the limitations of interactivity. He considers how automatic simulated behavior minus the application judgment of direct human participation is already evolving in computer animation. In traditional animation, the animator explicitly controls every action of the characters, provides every cause, and determines every effect. While we think of virtual reality as the ultimate interactive technology, it is likely that only new tasks will be purely interactive. As applications are better understood, the ratio of interactive to automated processing will go down. We can expect anthropomorphic simulated humans to work on simulated projects in not necessarily visible virtual worlds.<>},
  keywords={Virtual reality;Humans;Computational modeling;Computer simulation;Supercomputers;Computer applications;Computer displays;Computer science;Artificial intelligence;Brain modeling},
  doi={10.1109/38.364953},
  ISSN={1558-1756},
  month={Jan},}@INPROCEEDINGS{8740471,
  author={Yang, Xiang and Wang, Xuan and Wang, Wei},
  booktitle={2018 IEEE 4th Information Technology and Mechatronics Engineering Conference (ITOEC)}, 
  title={An Improved Centroid Localization Algorithm for WSN}, 
  year={2018},
  volume={},
  number={},
  pages={1120-1123},
  abstract={Node localization is one of the key technologies in wireless sensor network (WSN). Centroid localization algorithm is totally dependent on the density of the size and distribution of anchor nodes, but they distribute randomly and the density of them is small. In view of the traditional centroid localization algorithm is vulnerable to the influence of network deployment status, and the present situation of low positioning precision, put forward an improved centroid localization algorithm. Firstly, put the thinking of weight to the process of node coordinates' calculation, introduce the concept of reference anchor nodes. Then design the weighting strategy accordingly, using the weighted results as the unknown nodes' estimated coordinates, and improve the positioning accuracy through multiple weights. Finally, use simulation experiment to compare the accuracy and stability of the improved algorithm with the classical one. The experimental results show that compared with the traditional algorithm, the improved algorithm's localization accuracy is higher.},
  keywords={Wireless sensor networks;Distance measurement;Analytical models;Information science;Stability analysis;Hardware;Computational complexity;wireless sensor network (WSN);location technique;centroid localization algorithm;weighting},
  doi={10.1109/ITOEC.2018.8740471},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{7265264,
  author={Omheni, Nizar and Kalboussi, Anis and Mazhoud, Omar and Hadjkacem, Ahmed},
  booktitle={2015 IEEE 15th International Conference on Advanced Learning Technologies}, 
  title={Modelling Learner's Personality Profile through Analysis of Annotation Digital Traces in Learning Environment}, 
  year={2015},
  volume={},
  number={},
  pages={66-67},
  abstract={The researchers in education are interested to observing and modelling the learner, and adapting their learning experience accordingly. When learners read and interact actively with their reading materials, they do unselfconscious activities like annotation practices which can be key feature to their personalities. Annotation practices require the reader to be active with the document, to think critically and to make specific annotations in the margins of the text. In this paper we propose modeling learner's personality by referring to their annotations. The experiments show the significant role of annotation activity to reflect certain learner' personality traits.},
  keywords={Feature extraction;Psychology;Computational modeling;Writing;Adaptation models;Analytical models;Standards;Learner's profile;annotations;personality traits;personality computing;learning personalization},
  doi={10.1109/ICALT.2015.76},
  ISSN={2161-377X},
  month={July},}@INPROCEEDINGS{7757019,
  author={Lucke, Ulrike and Castro, Thais},
  booktitle={2016 IEEE 16th International Conference on Advanced Learning Technologies (ICALT)}, 
  title={The Process of Inclusive Design}, 
  year={2016},
  volume={},
  number={},
  pages={446-447},
  abstract={Re-thinking the process of designing inclusive systems may help to identify the potential for re-use and interoperability of developed systems. Based on existing models of system engineering and project management, a process model for inclusive design as well as consequences for its practical application are presented.},
  keywords={Computers;Software;Computational modeling;Stakeholders;Guidelines;Design methodology;System analysis and design;accessibility;design methodology;process model},
  doi={10.1109/ICALT.2016.132},
  ISSN={2161-377X},
  month={July},}@INPROCEEDINGS{7411221,
  author={Varadharajan, Vijayaraghavan and Amid, Alon and Rai, Sudhanshu},
  booktitle={2015 International Conference on Computing and Network Communications (CoCoNet)}, 
  title={Policy based Role Centric Attribute Based Access Control model Policy RC-ABAC}, 
  year={2015},
  volume={},
  number={},
  pages={427-432},
  abstract={As network speed and storage capacities are rising faster and higher, the remote storage and `cloud' concept is gaining momentum and is becoming increasingly popular with personal users as well as business users. At the same time, privacy awareness and business confidentially are important factors in every cloud and data sharing decision. For these and other reasons, the concept of access control models was developed in order to present complete solutions for privacy and confidentiality control in modern systems. Various access controls methodologies have been suggested in the literature, each simplifying and providing flexibility to previous methods. In this paper we will focus on two main methods which are Role Based Access Control (RBAC) and Attribute Based Access Control (ABAC). We will discuss the benefits and drawbacks of each of them, and therefore the need for a novel approach that combines the benefits of these two techniques. We are proposing a different hybrid model and justify how our suggested model and present why we think our suggested model (Policy RC-ABAC) will be more beneficiary in relation to a specific set of needs focusing on flexibility and auditability.},
  keywords={Access control;Logic gates;Measurement;Computational modeling;Business;Context;Standards;access control;authorization;ABAC;RBAC},
  doi={10.1109/CoCoNet.2015.7411221},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{1630728,
  author={Simpson, C.R. and Reddy, D. and Riley, G.F.},
  booktitle={20th Workshop on Principles of Advanced and Distributed Simulation (PADS'06)}, 
  title={Empirical Models of TCP and UDP End-User Network Traffic from NETI@home Data Analysis}, 
  year={2006},
  volume={},
  number={},
  pages={166-174},
  abstract={The simulation of computer networks requires accurate models of user behavior. To this end, we present empirical models of end-user network traffic derived from the analysis of NETI@home data. There are two forms of models presented. The first models traffic for a specific TCP or UDP port. The second models all TCP or UDP traffic for an end-user. These models are meant to be network- independent and contain aspects such as bytes sent, bytes received, and user think time. The empirical models derived in this study can then be used to enable more realistic simulations of computer networks.},
  keywords={Telecommunication traffic;Traffic control;Data analysis;Computational modeling;Computer networks;Computer simulation;Internet;Analytical models;Statistics;Data engineering},
  doi={10.1109/PADS.2006.17},
  ISSN={1087-4097},
  month={May},}
