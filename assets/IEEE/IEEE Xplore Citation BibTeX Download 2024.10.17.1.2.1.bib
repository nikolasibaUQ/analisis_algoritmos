@ARTICLE{8740901,
  author={Shen, Kai-wen and Wang, Xiao-kang and Qiao, Dong and Wang, Jian-qiang},
  journal={IEEE Transactions on Fuzzy Systems}, 
  title={Extended Z-MABAC Method Based on Regret Theory and Directed Distance for Regional Circular Economy Development Program Selection With Z-Information}, 
  year={2020},
  volume={28},
  number={8},
  pages={1851-1863},
  abstract={Decision makers (DMs) have different cognitive levels in practical experience, information reserve, and thinking ability. Thus, decision information is often not completely reliable. As a tool that can effectively represent information reliability, Z-number has been studied by many scholars in recent years. Current research on Z-number assumes that differences in various parts of a Z-number can complement one another. However, in many cases, the preference of DMs for each part is difficult to determine, or DMs believe that the differences in various parts cannot be complementary. Therefore, to solve such decision problems, this paper attempts to extend the traditional MABAC method to the Z-information environment by introducing the directed distance and regret theory. The proposed method simultaneously considers the randomness and fuzziness of Z-number. An example about regional circular economy development program selection is provided to illustrate the feasibility of the proposed method. Results show that the proposed method can solve complex decision problems rationally and effectively, and it has broad application prospects.},
  keywords={Euclidean distance;Decision making;Probability distribution;Computational complexity;Reliability theory;Current measurement;Circular economy (CE);fuzzy cut-set theory;regret theory;standardized Euclidean distance;Z-number},
  doi={10.1109/TFUZZ.2019.2923948},
  ISSN={1941-0034},
  month={Aug},}@ARTICLE{5432168,
  author={Marriott, Kim and Sbarski, Peter and van Gelder, Tim and Prager, Daniel and Bulka, Andy},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Hi-Trees and Their Layout}, 
  year={2011},
  volume={17},
  number={3},
  pages={290-304},
  abstract={We introduce hi-trees, a new visual representation for hierarchical data in which, depending on the kind of parent node, the child relationship is represented using either containment or links. We give a drawing convention for hi-trees based on the standard layered drawing convention for rooted trees, then show how to extend standard bottom-up tree layout algorithms to draw hi-trees in this convention. We also explore a number of other more compact layout styles for layout of larger hi-trees and give algorithms for computing these. Finally, we describe two applications of hi-trees: argument mapping and business decision support.},
  keywords={Standards development;Data visualization;Electronic mail;Compaction;Decision making;Economic indicators;Tree layout;hi-tree;argument mapping;decision support;information visualization.},
  doi={10.1109/TVCG.2010.45},
  ISSN={1941-0506},
  month={March},}@INPROCEEDINGS{7833330,
  author={de Kereki, Inés Friss and Paulós, J. Víctor and Manataki, Areti},
  booktitle={2016 XLII Latin American Computing Conference (CLEI)}, 
  title={A bilingual MOOC that teaches youngsters how to program: Analysis and reflections on one year of experiences}, 
  year={2016},
  volume={},
  number={},
  pages={1-11},
  abstract={Critical thinking and problem solving are fundamental skills to function successfully particularly in today's world. When programming, these skills are promoted, developed and deployed. In this context, Universidad ORT Uruguay and The University of Edinburgh co-created in 2015 a MOOC (Massive Open Online Course) that teaches young teenagers how to program. The course was implemented simultaneously in 2 versions: in Spanish, called “¡A Programar!” and in English, called “Code Yourself!”, which are available on the Coursera platform. Since its launch in March 2015, more than 139,000 people from 197 countries have registered. Initially it was offered in a “fixed session”; while currently it is offered in an “auto-cohort” mode. In both cases, students' surveys indicate that the course has met or exceeded expectations (values above 93%). In this paper, we detail the characteristics of the MOOC, and we analyze and compare the modes and results.},
  keywords={Videos;Software;Programming;Java;Reflection;Problem-solving;Context;MOOC;programming;computational thinking;Scratch},
  doi={10.1109/CLEI.2016.7833330},
  ISSN={},
  month={Oct},}@ARTICLE{8915693,
  author={Estevez, Julian and Garate, Gorka and Graña, Manuel},
  journal={IEEE Access}, 
  title={Gentle Introduction to Artificial Intelligence for High-School Students Using Scratch}, 
  year={2019},
  volume={7},
  number={},
  pages={179027-179036},
  abstract={The importance of educating the next generations in the understanding of the fundamentals of the upcoming scientific and technological innovations that will force a broad social and economical paradigm change can not be overstressed. One such breakthrough technologies is Artificial Intelligence (AI), specifically machine learning algorithms. Nowadays, the public has little understanding of the workings and implications of AI techniques that are already entering their lives in many ways. We aim to achieve widespread public understanding of these issues in an experiential learning framework. Following a design based research approach, we propose to implement program coding scaffoldings to teach and experiment some basic mechanisms of AI systems. Such experiments would be shedding new light into AI potentials and limitations. In this paper we focus on innovative ways to introduce high school students to the fundamentals and operation of two of the most popular AI algorithms. We describe the elements of a workshop where we provide an academic use-create-modify scaffolding where students work on the Scratch partial coding of the algorithms so they can explore the behavior of the algorithm, gaining understanding of the underlying computational thinking of AI processes. The extent of the impact on the students of this experience is measured through questionnaires filled before and after participation in the workshop. Preliminary experiments offer encouraging results, showing that the workshop has differential impact on the way students understand AI.},
  keywords={Artificial intelligence;Conferences;Tools;Distributed Bragg reflectors;Programming profession;Education;Scratch programming;teaching AI fundamentals;public AI awareness},
  doi={10.1109/ACCESS.2019.2956136},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{1222055,
  author={Hashieda, T. and Yoshida, K.},
  booktitle={Proceedings 2003 IEEE International Symposium on Computational Intelligence in Robotics and Automation. Computational Intelligence in Robotics and Automation for the New Millennium (Cat. No.03EX694)}, 
  title={Online learning system with logical and intuitive processings using fuzzy Q-learning and neural network}, 
  year={2003},
  volume={1},
  number={},
  pages={13-18 vol.1},
  abstract={This research tries to construct a new architecture of intelligent information processing imitating that of human thought and having ability of autonomous learning. Its processing is divided into logical and intuitive processing, which are selected flexibly and properly for the changing situation. To investigate its effectiveness, it is applied to a robot playing a Tetris game, which is regarded as a symbol of human intellectual behavior. The robot consists of three elements: recognition part, thinking part, and action part. The thinking part consists of logical and intuitive processing. Logical processing has the ability of accurate and slow processing, and intuitive processing has the ability of rough and quick processing. The learning method of logical processing consists of fuzzy Q-learning with neural network and intuitive processing consists of neural network, which is trained by empirical knowledge of logical processing. The combination of logical and intuitive processing leads to the improvement of game playing results for the reason that intuitive and logical processing built along with playing time becomes more applicable. As a result, it is shown that the proposed method is a useful approach.},
  keywords={Learning systems;Fuzzy logic;Fuzzy neural networks;Fuzzy systems;Neural networks;Humans;Information processing;Intelligent robots;Artificial intelligence;Biological neural networks},
  doi={10.1109/CIRA.2003.1222055},
  ISSN={},
  month={July},}@ARTICLE{7937927,
  author={Setchi, Rossitza and Asikhia, Obokhai K.},
  journal={IEEE Transactions on Affective Computing}, 
  title={Exploring User Experience with Image Schemas, Sentiments, and Semantics}, 
  year={2019},
  volume={10},
  number={2},
  pages={182-195},
  abstract={Although the concept of user experience includes two key aspects, experience of meaning (usability) and experience of emotion (affect), the empirical work that measures both the usability and affective aspects of user experience is currently limited. This is particularly important considering that affect could significantly influence a user's perception of usability. This paper uses image schemas to quantitatively and systematically evaluate both these aspects. It proposes a method for evaluating user experience that is based on using image schemas, sentiment analysis, and computational semantics. The aim is to link the sentiments expressed by users during their interactions with a product to the specific image schemas used in the designs. The method involves semantic and sentiment analysis of the verbal responses of the users to identify (i) task-related words linked to the task for which a certain image schema has been used and (ii) affect-related words associated with the image schema employed in the interaction. The main contribution is in linking image schemas with interaction and affect. The originality of the method is twofold. First, it uses a domain-specific ontology of image schemas specifically developed for the needs of this study. Second, it employs a novel ontology-based algorithm that extracts the image schemas employed by the user to complete a specific task and identifies and links the sentiments expressed by the user with the specific image schemas used in the task. The proposed method is evaluated using a case study involving 40 participants who completed a set task with two different products. The results show that the method successfully links the users' experiences to the specific image schemas employed to complete the task. This method facilitates significant improvements in product design practices and usability studies in particular, as it allows qualitative and quantitative evaluation of designs by identifying specific image schemas and product design features that have been positively or negatively received by the users. This allows user experience to be assessed in a systematic way, which leads to a better understanding of the value associated with particular design features.},
  keywords={Usability;Product design;Semantics;Sentiment analysis;Tools;Context;Ontologies;Affect;computational semantics;image schema;interaction design;ontology;sentiment analysis;usability;user experience},
  doi={10.1109/TAFFC.2017.2705691},
  ISSN={1949-3045},
  month={April},}@INPROCEEDINGS{9893636,
  author={Margarido, Solange and Machado, Penousal and Roque, Licínio and Martins, Pedro},
  booktitle={2022 IEEE Conference on Games (CoG)}, 
  title={Let’s Make Games Together: Explainability in Mixed-initiative Co-creative Game Design}, 
  year={2022},
  volume={},
  number={},
  pages={638-645},
  abstract={There has been growing development of co-creative systems for game design, where both humans and computers work as colleagues, proactively contributing with creative input. However, the collaborative process is still not as seamless as in human-human co-creativity. A key element still underdeveloped in these approaches is the communication between the human and the machine, which can be facilitated by providing the computational agent with explanatory capabilities. Based on principles of explainability for co-creative systems from previous literature, we propose a framework of explainability specifically applied to mixed-initiative scenarios in game design. We illustrate the applications of the framework by suggesting possible solutions adapted to different use cases of existing approaches and, additionally, of our own proposed approach.},
  keywords={Computers;Visualization;Collaboration;Games;Guidelines;explainability;mixed-initiative;game design;computational creativity;co-creativity},
  doi={10.1109/CoG51982.2022.9893636},
  ISSN={2325-4289},
  month={Aug},}@INPROCEEDINGS{6602429,
  author={Sayama, Hiroki and Dionne, Shelley D.},
  booktitle={2013 IEEE Symposium on Artificial Life (ALife)}, 
  title={Using evolutionary computation as models/tools for human decision making and creativity research}, 
  year={2013},
  volume={},
  number={},
  pages={35-42},
  abstract={This paper presents a review of our recently completed interdisciplinary research project “Evolutionary Perspective on Collective Decision Making” that was conducted through close collaboration between computational, organizational and social scientists at Binghamton University. In this project, we utilized Evolutionary Computation in several non-traditional ways-(l) as a theoretical framework for reinterpreting the dynamics of collective human decision making processes, (2) as a computational simulation model of idea generation and selection, and (3) as a research tool for collecting high-resolution experimental data of actual collaborative design and decision making from human subjects.},
  keywords={Decision making;Computational modeling;Sociology;Statistics;Evolution (biology);Computer simulation;Cultural differences;Evolution of ideas;human decision making;collaborative design;creativity;agent-based simulation;human subject experiments;evolutionary computation;hyperinteractive evolutionary computation},
  doi={10.1109/ALIFE.2013.6602429},
  ISSN={2160-6382},
  month={April},}@ARTICLE{8836111,
  author={Southwick, Daniel},
  journal={IEEE Annals of the History of Computing}, 
  title={High Noon on the Creative Frontier: Configuring Human and Machine Expertise}, 
  year={2019},
  volume={41},
  number={4},
  pages={97-109},
  abstract={In 1960, CBS aired a special issue entitled “The Thinking Machine” which featured three Western playlets scripted by a computer programmed by MIT researchers. Almost 60 years later, two researchers at Autodesk used a computer program to help design a chair. In this article, I link these two seemingly discrete examples of computational creativity in order to highlight how digital fabrication technologies have served as an important test site for defining human and computational expertise. I do so by illustrating how concepts of “creativity” and “routine” were produced alongside the concepts of computational creativity during the development of digital fabrication. This dichotomy of “creative” and “routine” is not only used to determine the kinds of tasks that are appropriate for humans and computers to perform within the design and production process, but it is also used to render invisible the embodied craft knowledge required to substantiate these systems.},
  keywords={History;Fabricatoin;Digital systems;Design automation;Creativity;Computational modeling;Programming;Automation;Computer-aided design;Computer-aided manufacturing;Industrial automation},
  doi={10.1109/MAHC.2019.2941108},
  ISSN={1934-1547},
  month={Oct},}@INPROCEEDINGS{7474583,
  author={Pech, Robert and Lin, Barry and Cho, Chung-Suk and Al-Muhairi, Hassan},
  booktitle={2016 IEEE Global Engineering Education Conference (EDUCON)}, 
  title={Innovation, design and entrepreneurship for engineering students: Development and integration of innovation and entrepreneurship curriculum in an engineering degree}, 
  year={2016},
  volume={},
  number={},
  pages={389-396},
  abstract={The authors of this paper introduced junior (3rd year BSc) students for the first time to the principles and practices of innovation and entrepreneurship in engineering design at Khalifa University in the United Arab Emirates, in 2014. This included an overview of the techniques that managers in organizations and startup entrepreneurs use to initiate and manage innovation effectively. The course introduces four main phases of innovation and entrepreneurship: (i) identification of a new need or want; (ii) invention of a technological solution; (iii) testing and implementing that solution; and (iv) the mock-start of an entrepreneurial venture. These are explored at length. The course uses a hands-on approach to engage students in groups of four, to the full process of innovation and entrepreneurship from concept generation and selection, needs finding and screening leading to the development of viable financial strategies in their business plans. The innovation/entrepreneurship part of the course is taught in parallel with the design engineering part. The emphasis of the course is on the development of innovative and competitive needs-based solutions for real world engineering problems through the creation of prototypes and simulations. Students are taught "tools" to assist them: design modelling, problem-solving methods, ethical debate, reasoning and logic, and a business plan canvas to help them organize their thinking about business prospects in a realistic fashion.},
  keywords={5G mobile communication;Engineering education;Conferences;innovation;engineering design;entrepreneurship;curriculum;experiential learning},
  doi={10.1109/EDUCON.2016.7474583},
  ISSN={2165-9567},
  month={April},}@INPROCEEDINGS{8248136,
  author={Wildman, Wesley J. and Fishwick, Paul A. and Shults, F. LeRon},
  booktitle={2017 Winter Simulation Conference (WSC)}, 
  title={Teaching at the intersection of simulation and the humanities}, 
  year={2017},
  volume={},
  number={},
  pages={4162-4174},
  abstract={Human simulation (applying Modeling and Simulation (M&S) to topics in the humanities, the interpretative social sciences, and the arts) is a potent extension of social simulation. This paper offers reflections on teaching at this intersection, presenting best practices in pedagogy for undergraduate and graduate students engaged in formal studies, and for established researchers having no structured curriculum. The fact that human simulation is possible drives home the presence of formal patterns in a host of phenomena that for a long time were thought to be inimical to mathematical analysis. That implies a double pedagogical challenge: teaching humanities students to recognize formal structures in the phenomena they study (counter-intuitive for them), and teaching M&S students to collaborate with humanities people who think very differently (equally counter-intuitive). The three perspectives presented here underline the usefulness of human simulation, as well as the difficulties and benefits associated with teaching and learning human simulation.},
  keywords={Iron;Reflection;Complexity theory;History},
  doi={10.1109/WSC.2017.8248136},
  ISSN={1558-4305},
  month={Dec},}@INPROCEEDINGS{10070941,
  author={Yao, Jianguo and Zhou, Hao and Zhang, Yalin and Li, Ying and Feng, Chuang and Chen, Shi and Chen, Jiaoyan and Wang, Yongdong and Hu, Qiaojuan},
  booktitle={2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA)}, 
  title={High Performance and Power Efficient Accelerator for Cloud Inference}, 
  year={2023},
  volume={},
  number={},
  pages={1003-1016},
  abstract={Facing the growing complexity of Deep Neural Networks (DNNs), high-performance and power-efficient AI accelerators are desired to provide effective and affordable cloud inference services. We introduce our flagship product, i.e., the Cloudblazer i20 accelerator, which integrates the innovated Deep Thinking Unit (DTU 2.0). The design is driven by requests drawn from various AI inference applications and insights learned from our previous products. With careful tradeoffs in hardware-software co-design, Cloudblazer i20 delivers impressive performance and energy efficiency while maintaining acceptable hardware costs and software complexity/flexibility. To tackle computation- and data-intensive workloads, DTU 2.0 integrates powerful vector/matrix engines and a large-capacity multi-level memory hierarchy with high bandwidth. It supports comprehensive data flow and synchronization patterns to fully exploit parallelism in computation/memory access within or among concurrent tasks. Moreover, it enables sparse data compression/decompression, data broadcasting, repeated data transfer, and kernel code prefetching to optimize bandwidth utilization and reduce data access overheads. To utilize the underlying hardware and simplify the development of customized DNNs/operators, the software stack enables automatic optimizations (such as operator fusion and data flow tuning) and provides diverse programming interfaces for developers. Lastly, the energy consumption is optimized through dynamic power integrity and efficiency management, eliminating integrity risks and energy wastes. Based on the performance requirement, developers also can assign their workloads with the entire or partial hardware resources accordingly. Evaluated with 10 representative DNN models widely adopted in various domains, Cloudblazer i20 outperforms Nvidia T4 and A10 GPUs with a geometric mean of 2.22x and 1.16x in performance and 1.04x and 1.17x in energy efficiency, respectively. The improvements demonstrate the effectiveness of Cloudblazer i20’s design that emphasizes performance, efficiency, and flexibility.},
  keywords={Full stack;Bandwidth;Programming;Hardware;Energy efficiency;Synchronization;Task analysis},
  doi={10.1109/HPCA56546.2023.10070941},
  ISSN={2378-203X},
  month={Feb},}@INPROCEEDINGS{1432316,
  author={Joongsun Yoon and Sangjoo Park and Jinyoung Kim},
  booktitle={30th Annual Conference of IEEE Industrial Electronics Society, 2004. IECON 2004}, 
  title={Emotional robotics based on iT/spl I.bar/Media}, 
  year={2004},
  volume={3},
  number={},
  pages={3148-3153 Vol. 3},
  abstract={Intelligence is thought to be related to interaction rather than a deep but passive thinking. Interactive tangible media "iTMedia" is proposed to explore these issues. Personal robotics is a major area to investigate these ideas. A new design methodology for personal and emotional robotics is proposed. Sciences of the artificial and intelligence have been investigated. A short history of artificial intelligence is presented in terms of logic, heuristics, and mobility; a science of intelligence is presented in terms of imitation and understanding; intelligence issues for robotics and intelligence measures are described. A design methodology for personal robots based on science of emotion is investigated. We investigate three different aspects of design: visceral, behavioral, and reflective. We also discuss affect and emotion in robots, robots that sense emotion, robots that induce emotion in people, and implications and ethical issues of emotional robots. Personal robotics for the elderly is investigated to explore these ideas.},
  keywords={Artificial intelligence;Intelligent robots;Robot sensing systems;Humans;Design methodology;History;Logic;Mobile robots;Mechanical engineering;Intelligent systems},
  doi={10.1109/IECON.2004.1432316},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{7244619,
  author={Bachtiar, Vincent and Kerrigan, Eric C. and Moase, William H. and Manzie, Chris},
  booktitle={2015 10th Asian Control Conference (ASCC)}, 
  title={Smoothness properties of the MPC value function with respect to sampling time and prediction horizon}, 
  year={2015},
  volume={},
  number={},
  pages={1-6},
  abstract={Sampling time and the prediction horizon length are two underlying design choices for the implementation of model predictive control (MPC). Smoothness properties of the open- and closed-loop value function with respect to the two parameters are essential to characterise for the purpose of providing knowledge that is useful in the context of MPC design optimisation to maximise system performance. Specifically, these properties are continuity, differentiability and monotonicity. This paper presents both numerical and analytical results to reveal the smoothness properties of the value function. Increasing sampling rate and/or prediction horizon does not necessarily improve closed-loop performance. Furthermore, the value function in open-loop under input constraints is differentiable, which may contradict the traditional thinking and expectations.},
  keywords={Optimization;Harmonic analysis;Oscillators;Taylor series;Electronic mail;Predictive control;Context},
  doi={10.1109/ASCC.2015.7244619},
  ISSN={},
  month={May},}@ARTICLE{1637974,
  author={Arroyo, A.A.},
  journal={IEEE Instrumentation & Measurement Magazine}, 
  title={Machine intelligence grounded in reality}, 
  year={2006},
  volume={9},
  number={3},
  pages={17-23},
  abstract={The design, realization, and application of intelligent, autonomous, sensor-driven, behavior-based robotic agents are discussed. The authors have identified four philosophical goals for machine intelligence grounded in reality to flourish: integration, real-world issues, interdisciplinary teamwork, and critical thinking. Building a robot requires that the designer integrates control in electronic and mechanical systems and produces a working device, confronts the user with interactions, between different subsystems, and gives the designer the opportunity to trade off between the different subsystems in constructing an autonomous agent. The environment also encourages biomedical engineers to confront the issues involved in getting a physical agent to operate reliably in a realistic environment by giving them the opportunity to build their own animal, providing a unique perspective on the many problems that nervous systems actually solve.},
  keywords={Machine intelligence;Intelligent robots;Robot sensing systems;Intelligent agent;Intelligent sensors;Teamwork;Buildings;Control systems;Mechanical systems;Autonomous agents},
  doi={10.1109/MIM.2006.1637974},
  ISSN={1941-0123},
  month={June},}@ARTICLE{9138677,
  author={McCain, E. C. and Bastien, P. and Belmar, B. F. and Bhattacharya, B. and Cheruiyot, K. K. and Coq, M. and Dartey, R. and Deekaram, K. and Ghadai, K. and Lalima, L. D. and Nettey, J. and Owolabi, A. W. and Phillips, K. and Shiling, T. M. and Schroeder, D. T. and Slegel, C. and Steen, B. and Thorne, D. A. and Venuto, E. and Willoughby, J. D. and Yaniv, D. and Ziemis, N.},
  journal={IBM Journal of Research and Development}, 
  title={IBM Z development transformation}, 
  year={2020},
  volume={64},
  number={5/6},
  pages={14:1-14:13},
  abstract={This article discusses how the product development cycle is being transformed with “Artificial Intelligence” (AI) for the first time in zSeries history. This new era of AI, under the project name IBM Z Development Transformation (zDT), has allowed the team to grow and learn new skills in data science. This transformation forces change structurally in how data is prepared and stored. In z14, there were incremental productivity gains with enhancements to automation with eServer Automation Test Solution and a technology data analysis engine called zDataAssist. However, in z15, AI will significantly accelerate our efficiency. This article explains how Design Thinking and Agile principles were used to identify areas that are of high impact and feasible to implement: 1) what and how data is collected via System Test Event Logging and Analysis engine, Problem ticket management system (Jupitr), and Processor data analysis engine (Xrings); 2) problem identification, analysis, and management (AutoJup) along with Intelligent Recovery Verification Assistant; 3) product design documentation search engine (AskTheMachine); and 4) prototype microprocessor allocation processes Intelligent Commodity Fulfillment System using Machine Learning. This article details the approach of these areas for z15, the implementation of these solutions under the zDT project, as well as the results and future work.},
  keywords={Tools;Artificial intelligence;Automation;Engines;Databases;Hardware;Writing},
  doi={10.1147/JRD.2020.3008122},
  ISSN={0018-8646},
  month={Sep.},}@INPROCEEDINGS{9146085,
  author={Cepni, Elif},
  booktitle={2019 IEEE 18th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC)}, 
  title={The Science and Art Of Decision Making}, 
  year={2019},
  volume={},
  number={},
  pages={272-277},
  abstract={Decision makers of today routinely encounter increasingly complex and interrelated problems, preceding the necessity for a large number of significant decisions to be dynamic in nature. Frequently rather than a single decision the requirement of a number of decisions exists, conventionally being interdependent on each other in an environment of progressive change. For thousands of years people have endeavoured to document observations of the environment and surroundings, with the aspiration of comprehending situations, which in turn enable a form of anticipation or prediction of the future. Through the contributions of a range of scientists and philosophers' humanity has affected the achievement of an improved quality of life, commencement of influence on the essence of life and encouragement to attempt to gain even further knowledge through travel to other planets. Without any doubt science is exceptional and dynamic and by far the optimum means of discovering the world and all that it encompasses. What hasn't changed is the curiosity, imagination and intelligence of those doing science [1]. Despite the fact that scientific discoveries and inventions invariably enhance life to a large degree as well as being accredited with expanding the expected lifespan of humans, scientific and technological improvements may equally precipitate alienation, loss of privacy, environmental problems (chemical and electronic waste), and a greater uncertainty or a black swan event. Science is perceived to be subject about knowledge with curiosity lying at the heart of it, differing from technology in that technology is preferably explained as doing. The 19th century scientist Pierre Laplace elevated determinism to a key place in science. He linked determinism and the ability to predict to the very notion of success in science [2]. For technical decisions science is an unrivalled tool to use, however, for managerial, institutional and personal daily life decisions the same recommendation cannot be given. Numerous key systems incorporated in the life of humans exhibit diverse complexities. Markets compromised of various buyers and sellers all categorized in groups participating in mutual funds, economies with hierarchies of workers, departments, firms, and industries; multi-celled organisms consisting of proteins, membranes, organelles, cells, and organs, the internet with users, stations, servers, and websites. Each of these complex systems exhibits a distinctive property called “emergence” roughly described by a phrase “the whole is more than the sum of the actions of the parts [3]. Scientists depend on the law of rationality; however, the fact that emotion habitually dominates humans on innumerable occasions is well recognized. Perhaps a more effective method for solving the problems of humanity should include deciphering the laws of human nature. As an alternative to the law of rationality, consideration could be given to whether it is preferable for scientists use the law of bounded rationality which may entail radical paradigm shift in scientific studies. The fundamental gap between the explicit accomplishments of knowledge acquisition in the natural sciences versus the rather minimal successes in understanding the dynamics of the social realm is the inherent nonlinearity, instability, and uncertainty of behaviour consistent with social systems. However, the possibility that an alternative strategy exists to close this gap is highly feasible. This article aims at showing the justification for the discarding the rule of rationality assumption in engagement and comprehension of scientific studies, and as a substitute insert human behaviours and emotions. Our emotional self is the principal power behind our creativity and passion and constitutes humanity. Controlling the nature may be easier than controlling the human nature. Today the study of chaos, and systemic thinking (emphasis is given to complexity, networks and patterns of organization) has emerged at the forefront of natural sciences too. Disquiet exists concerning events that may lead to the destruction of our civilization even the elimination of life on Earth. In 2050 the World population will reach 9.7 billion. There is also an urgent need to introduce eco-ethical standards into science. Decision making is not merely a science; there is a requisite for creative and individuality aspects of it to be examined. In the development of technologies, the human nature, psychological and sociological impacts of these technologies must be analysed in a holistic way. The main aim of the paper is to show that decision making especially under uncertainty is partly scientific partly heuristic or artistic phenomenon. The art side of decision making shouldn't be expelled from science.},
  keywords={Uncertainty;Decision making;Standards;Mathematical model;Art;Complexity theory;complexity neuroeconomics;styling;decision-making},
  doi={10.1109/ICCICC46617.2019.9146085},
  ISSN={},
  month={July},}@INPROCEEDINGS{6826147,
  author={Giordano, Daniela and Maiorana, Francesco},
  booktitle={2014 IEEE Global Engineering Education Conference (EDUCON)}, 
  title={Use of cutting edge educational tools for an initial programming course}, 
  year={2014},
  volume={},
  number={},
  pages={556-563},
  abstract={Programming skills are an important component of an engineering curriculum, not only because they enable the customization of software tools to be used in the profession, but also (and perhaps more crucially) because of the "computational thinking" and problem solving capabilities that are ideally developed by young students who learn to program for the first time. The necessity to expand the computing curriculum across a wider range of schools and university courses for students who are not majoring in Computer Science (CS) ) it is well-documented in literature [1], as is the difficulty of teaching 21st century skills (www.p21.org. This work presents an educational approach to teaching initial programming based on the development of fundamental and transversal skills and computer science skills, including creative and computational thinking as well as problem solving and critical thinking. The approach is based on cutting-edge educational tools, namely the visual programming frameworks Scratch, AppInventor, BYOB, and the well-known C/C++ language; curriculum material is drawn from CSPrinciples pilot courses, CS unplugged, school level preparation material for the International Olympiad in Informatics, and are complemented by supplementary information. The pedagogical approaches used in the course are based on constructivist learning theory, experiential learning and guided inquiry. This paper presents a year-long teaching experience in a 10th/13th grade high school with 14 to 16-year-old students. Ways to extend the experience to a university course are also presented. An initial analysis of the course results, both qualitative (based on two student surveys) and quantitative (based on formal written examinations) is presented and discussed. Results are encouraging, showing how visual programming languages help students to improve their problems solving skills and reasoning practices. Exposing the younger generation to computational concepts is fundamental in order to improve the mastering of these concepts and increase the success rate in university studies.},
  keywords={Educational institutions;Programming profession;Visualization;Problem-solving;Pedagogy;Curriculum design;Initial programming course;Visual programming frameworks},
  doi={10.1109/EDUCON.2014.6826147},
  ISSN={2165-9567},
  month={April},}@ARTICLE{7829303,
  author={John, Lawrence and John, Georganne Brier and Parker, Matthew and Sauser, Brian J. and Wade, Jon P.},
  journal={IEEE Systems Journal}, 
  title={Self-Organizing Cooperative Dynamics in Government Extended Enterprises}, 
  year={2018},
  volume={12},
  number={3},
  pages={2905-2916},
  abstract={This paper presents the results of our research into cooperation in Government Extended Enterprises, a type of system of systems. The effort proposed and evaluated a novel theory that these decisions are the result of the interaction of four canonical forces-Sympathy, Trust, Fear, and Greed. A computational simulation involving the Stag Hunt game examined information sharing decisions in a series of key decision points in three large case studies. For the five hypotheses tested, exploratory data analysis and nonparametric statistical testing show strong support for three of the hypotheses (cooperation is positively correlated with actors' levels of Sympathy and Trust and negatively correlated with actors' levels of Fear) and moderate support for the fourth (cooperation is negatively correlated with actors' levels of Greed). Indications are that the fifth hypothesis (cooperation is correlated with history of behavior) is not needed to explain observed behavior. Multiple correspondence analysis showed significant interactions both among pairs of forces and when a force is paired with decision making strategies. These results can form the basis for: 1) analysis of additional case studies; 2) development of an agent-based simulation; and 3) creation of training programs for current and future organizational leaders.},
  keywords={Government;System of systems;Ethics;Standards organizations;Data analysis;Information management;Cooperative systems;computational modeling;complex systems engineering;extended enterprise;game theory;institutional analysis;system of systems;systems thinking},
  doi={10.1109/JSYST.2016.2647598},
  ISSN={1937-9234},
  month={Sep.},}@INPROCEEDINGS{9653337,
  author={Carlos, Enríquez Ramírez and Mariza, Raluy Herrero and Sosa, Luz María Vega},
  booktitle={2021 9th International Conference in Software Engineering Research and Innovation (CONISOFT)}, 
  title={Use of Techno-Pedagogical Tools to Incorporate Remote Collaboration in a Data Structure Course}, 
  year={2021},
  volume={},
  number={},
  pages={232-237},
  abstract={Traditionally, teaching-learning activities in typical schools in most of the states in the Mexican Republic have been face-to-face interactions between students and learning facilitators, but obviously, in 2020, everything changed, so that educational videoconferencing will gain ground in the academic world. Participation in solving individual and / or group problems was adapted to a remote scheme, stimulating the use of different skills in students to face the new normal, as is the case in the development of remote programming projects for the purpose of training novice programmers to solve problems collaboratively, going through the entire global software development process. To do so, it is necessary to incorporate skills that allow them to adapt this work philosophy, which is why in this work the use of competencies that allows to detonate social and cognitive aspects in the development of software in small teams was evaluated, as well as the frequency of use. It is also important to determine if the gender has any influence on the adoption of a certain trait. For this, groups of students were trained by developing computational problems in the field of data structure.},
  keywords={Training;Teleconferencing;Technological innovation;Collaboration;Data structures;Software;Task analysis;Collaborative programming;Skills;Computational thinking},
  doi={10.1109/CONISOFT52520.2021.00039},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9028656,
  author={Shoaib, Huma and Cardella, Monica E. and Madamanchi, Aasakiran and Umulis, David},
  booktitle={2019 IEEE Frontiers in Education Conference (FIE)}, 
  title={Computation, Gender, and Engineering Identity Among Biomedical Engineering Undergraduates}, 
  year={2019},
  volume={},
  number={},
  pages={1-5},
  abstract={This study explores interactions between computational thinking, gender and engineering identity among biomedical engineering undergraduate students. Biomedical engineering enjoys higher rates of women's enrollment than other engineering disciplines, but nevertheless has gender disparities in persistence within the field. Additionally, trends towards greater incorporation of computation into biomedical engineering have the potential to recreate the gender inequities seen in more computationally intensive engineering disciplines. Recently, `engineering identity' has emerged as a powerful analytic lens to understand persistence in engineering, particularly for underrepresented groups such as women. However, there is limited work examining how experiences using computational methods influences engineering identity formation in undergraduate biomedical engineers. Further, it remains unclear to what extent gender differentially mediates the effects of computational practice on engineering identity formation. In order to explore the intersection of these issues, we study a thermodynamics course in the biomedical engineering department of a large Midwestern public research institution in the United States. The thermodynamics course includes in-class computational modeling group activities and has an enrollment of more than 120, primarily sophomore year, undergraduate students. We use a qualitative study approach that includes gathering data through classroom observation and detailed semi-structured interviews. We analyze classroom observation data to try to understand student experiences of learning and participation during in-class computational modeling exercises. Specifically, we look for evidence of gendered differences in task sorting and engagement with the exercise. Classroom data is complemented by semi-structured interviews. Thematic analysis of semi-structured interviews gains student's perspectives on how gender has influenced their learning experience and their identity as engineers.},
  keywords={computation;engineering identity;gender},
  doi={10.1109/FIE43999.2019.9028656},
  ISSN={2377-634X},
  month={Oct},}@INPROCEEDINGS{10020957,
  author={Gosselar, Ashley},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={A Data-Driven Approach to Reparative Description at the University of Chicago}, 
  year={2022},
  volume={},
  number={},
  pages={2532-2540},
  abstract={Reparative description of collections is a growing element of diversity, equity, and inclusion efforts at cultural heritage institutions. However, the scale and complexity of the work can be overwhelming in practice. I demonstrate that computational methodologies and data analytics can be used to kickstart the planning stage for reparative description of archival finding aids. I discuss auditing and analyzing finding aids at the University of Chicago Library’s Hanna Holborn Gray Special Collections Research Center for potentially problematic language utilizing Python, Trifacta, Tableau, and Neo4j. I describe insights gained by treating finding aids as data, and I share recommendations for structuring reparative description work in a logical and attainable way.},
  keywords={Pain;Forestry;Metadata;Big Data;Propulsion;Data science;Libraries;Computational Thinking;Data Analysis;Reparative Description;Inclusive Description;Archives;Archival Description;Archival Metadata;Finding Aids},
  doi={10.1109/BigData55660.2022.10020957},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9962737,
  author={Barman, Arko and Beckman, Leslie S. and Chebaro, Yasmin},
  booktitle={2022 IEEE Frontiers in Education Conference (FIE)}, 
  title={Interdisciplinary Computing Education: An Introductory Programming and Data Science Course for Postdoctoral Researchers in the Biosciences}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={This Innovative-Practice Full Paper presents the curriculum development of an introductory course in programming and data science for postdoctoral researchers (PDRs) in the biosciences. The use of computing software has become ubiquitous and a working knowledge of data science has become increasingly essential for researchers in all domains. However, curriculum development focusing on imparting foundational programming skills and fundamentals of data science for researchers in domains other than computing has been scarce. Thus, there is an unmet need for curriculum development involving computational thinking, programming, and the fundamentals of data science for this audience. Recognizing these growing needs and demands of researchers to learn programming and data science that can then be applied to their area of research or practice, we developed an introductory course in programming and data science for PDRs in biology and medicine. The primary goal of the course was to develop computational thinking skills in PDRs who hail from backgrounds that have traditionally not focused on inculcating computational thinking. This course covered the fundamental concepts of programming using either Python or R - languages that researchers outside the computing community use in numerous ways including the statistical analysis of large datasets that are becoming increasingly common in biomedical research. Further, PDRs enrolled in the course were introduced to some of the broad categories of problems in data science - exploratory data analysis, classification, regression, and clustering - along with relevant algorithms and how they can be applied to real-world datasets in their respective domains using packages or libraries in Python or R. We also report the feedback from the enrolled PDRs, lessons learned, and recommendations for instructors interested in designing similar curricula. Our course focusing on computing and data science education for postdoctoral scholars from a non-computing background demonstrates a promising model for incorporating computing education in other areas of study that do not traditionally have a focus on computing education as well as in continuing education.},
  keywords={Statistical analysis;Education;Curriculum development;Focusing;Data science;Software;Libraries;computer science education;interdisciplinary computing education;data science education;continuing education},
  doi={10.1109/FIE56618.2022.9962737},
  ISSN={2377-634X},
  month={Oct},}@INPROCEEDINGS{5470704,
  author={},
  booktitle={2010 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum (IPDPSW)}, 
  title={Workshop on parallel and distributed Computing in Finance - PDCoF}, 
  year={2010},
  volume={},
  number={},
  pages={1-2},
  abstract={Today, principles of finance are combined with advanced mathematical structures to form useful financial products, strategies and models that are tested and implemented with the use of advanced quantitative techniques. Use of computing technology is pervasive throughout this process. Quantitative and Computational Finance is an area referred to under a variety of names, for example, ‘computational finance’, ‘financial engineering’, and ‘mathematical finance’. But in all cases there is an effort that involves ‘financial’, ‘mathematical’, ‘quantitative’ and ‘computational’ thinking to build, test and implement models that are at the center of these financial activities. In the last decade Computational Finance (CF) has influenced the market place extensively with enormous impact on wealth building, employment opportunities, and tremendous economic growth. This field forms an ever-expanding part of the financial sector, in numerous ways today. The time is, therefore, ripe now to bring together researchers in the areas of finance using complicated financial models to solve computationally intensive problems and computer scientists having the resources and solution methodologies to solve such problems. The main goal of this workshop is to provide a timely forum for these two groups to exchange and disseminate new ideas, techniques, and research in computational finance. Strong discussions will follow the presentations and the experience could lead them into the formulation, implementation of the models used by the practitioners in financial sector. This workshop will be a first such effort in bringing together researchers in the areas of finance and advanced computing (i) who develop and employ parallel and distributed computing extensively (ii) at a venue where parallel, distributed, high performance computing is the fundamental thread of discussions and arguments.},
  keywords={USA Councils;Finance;Biological system modeling;Computational modeling;Australia;Conferences;Distributed computing},
  doi={10.1109/IPDPSW.2010.5470704},
  ISSN={},
  month={April},}@INPROCEEDINGS{5510833,
  author={Scorcioni, Ruggero},
  booktitle={2010 Biomedical Sciences and Engineering Conference}, 
  title={6.8: Presentation session: Neuroanatomy, neuroregeneration, and modeling: “GPGPU implementation of a synaptically optimized, anatomically accurate spiking network simulator”}, 
  year={2010},
  volume={},
  number={},
  pages={1-2},
  abstract={Simulation of biological spiking networks is becoming more relevant in understanding neuronal processes. An increasing proportion of these simulations focuses on large scale modeling efforts. Unfortunately the size of large networks is often limited by both computational power and memory. Computational power constrains both the maximum number of differential equations and the maximum number of spikes that can be processed per unit time. Memory size limits the maximum number of neurons and synapses that can be simulated. To solve for the computational bottleneck, a neuronal simulator is implemented on a CUDA-based General Purpose Graphic Processing Unit (GPGPU). CUDA provides a C-like environment to harness the computational power of specialized video cards from NVIDIA (these cards provide a computational peak power on single precision floats of 1TFLOPS, at least an order of magnitude higher than the fastest CPU). To solve for the memory bottleneck, a just-in-time synapse storing algorithm is implemented requiring only 4 bytes per synapse. Only the synaptic weight is stored, while both post-synaptic contact and delay are recomputed at run-time. This allows a resource shift from memory to computation which fits with the peculiar GPGPU architecture, where an abundance of compute nodes access the memory via a bandwidth-limited bus. Neurons are represented by a single compartment whose activity is modeled by the Izhikevich formalism. Excitatory synapses are plastic and follow both a spike-timing-dependent plasticity rule and a short term potentiation/depression rule. We are able to simulate networks with up to a million neurons and up to 100 million synapses on a single GPGPU card. Networks of this size cannot be simulated on desktop computers. For smaller networks the speedup obtained is at least of an order of magnitude compared to traditional CPU platform. As an example of possible use of the present work we present preliminary results on the simulation of early stage of the visual pathway. In the retina, Retinal Ganglion Cells (RGC) project to the Lateral Geniculate Nucleus (LGN). LGN projects to the cortical area V1. V1 projects back to LGN. The network is represented by 100,000 neurons, 10 million synapses, and 32 different morphological classes with >350 topological projections. Input to the network is provided by current injection in the RGC layer. The RGC layer models midget and parasol ganglion cells (representing 80% of the RGC in primates). Each ganglion type is then subdivided into on- and off-center cells for a total of 4 different types of RGC. Training is performed via natural stimuli while testing is done with vertical and horizontal bars. The network average spiking frequency is within biological limits. Testing performed with both vertical and horizontal bars shows each pattern propagating along the network's anatomical projections. At each stage the pattern is progressively elaborated and modified. In conclusion we present a novel simulator that is fast, synaptically optimized and anatomically accurate. At an additional cost to an available desktop PC of few hundred dollars we think the GPGPU is an ideal platform to simulate large spiking networks.},
  keywords={Computational modeling;Biological system modeling;Neurons;Biology computing;Computer networks;Central Processing Unit;Retina;Performance evaluation;Testing;Bars},
  doi={10.1109/BSEC.2010.5510833},
  ISSN={},
  month={May},}
