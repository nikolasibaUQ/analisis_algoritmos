@INPROCEEDINGS{5588928,
  author={Shao Xiao-yan and He Lei and Peng Xuan},
  booktitle={2010 Second International Conference on Communication Systems, Networks and Applications}, 
  title={Novel algorithm of label placement in digital map based on grid}, 
  year={2010},
  volume={2},
  number={},
  pages={93-96},
  abstract={Label placement is very important when we display the electronic map. It will affect the beauty of electronic map. Furthermore, in order to display label placement in real-time, the computational complexity can not be too high. A novel algorithm of label placement is expounded based on the thinking of grid. It first divides the screen simply, then those three objects of points, arcs and polygons will be dealt with. The label configuration is reasonable, and does not result in any overlap. The experimental result indicates this algorithm is simple and effective. It can meet the demand of high quality map dynamic label placement and is successfully applied in commercial GIS.},
  keywords={Complexity theory;Roads;Industries;dynamic label placement;digital map;collision;overlap},
  doi={10.1109/ICCSNA.2010.5588928},
  ISSN={},
  month={June},}@INPROCEEDINGS{7594054,
  author={Riaz, Sadaf and Muhammad, Jan},
  booktitle={2015 International Symposium on Mathematical Sciences and Computing Research (iSMSC)}, 
  title={An evaluation of public cloud adoption for higher education: A case study from Pakistan}, 
  year={2015},
  volume={},
  number={},
  pages={208-213},
  abstract={There has been a global change of perception about right to education over the past few decades. Information and Communication Technologies (ICT) are one of such advanced means to access educational material and resources anywhere/anytime in the world. Invention of cloud computing technology has opened many options for organizations and individual to share the educational resources among students. By adopting cloud computing organizations can save various recurring costs such as ICT hardware/software, licensing, storage space, backups, maintenance and human resources. Cloud computing enables teachers and students to access and approach the academic material ubiquitously through portable appliances (smart phones, tablets, IPAD's etc.). There are numerous public cloud applications (offerings) available for accessing educational resources in urban as well as rural vicinity. The Usability evaluation of these cloud-based services in a particular academic atmosphere is significant prior to their adoption. Moreover, this enables the stakeholders to make better decisions to adopt these contemporary services. In this paper we present the usability evaluation of public cloud applications across three universities in Pakistan from stakeholders' perspective i.e. (teachers and students). Our experimental results show that there is lack of awareness on using cloud-based services for accessing educational resources.},
  keywords={Cloud computing;Google;Usability;Education;Correlation;Computational modeling;Cloud computing;Information and Communication Technology;Usability testing;Google sites;Think aloud testing;Ubiquitous access},
  doi={10.1109/ISMSC.2015.7594054},
  ISSN={},
  month={May},}@INPROCEEDINGS{10343349,
  author={Candiotto, Lucas S. and De Assis, Marcos V. O. and Schreiner, Marcos A. and Fioravanti, Maria Lydia and Barbosa, Ellen F. and Marcolino, Anderson S.},
  booktitle={2023 IEEE Frontiers in Education Conference (FIE)}, 
  title={A Lightweight Web Tool to Support Feedback in Introductory Programming Practices in Brazilian Undergraduate Disciplines}, 
  year={2023},
  volume={},
  number={},
  pages={1-9},
  abstract={High schools and undergraduate courses present several problems regarding teaching programming. In this perspective, several factors have led to the development of tools to support programming disciplines, including: (i) the persistent problems in teaching programming worldwide, (ii) the inclusion of computational thinking and programming practices in the new Basic Education Curriculum in Brazil, (iii) the need for a lightweight virtual tool that can be freely shared and used, as many available tools charge fees, and (iv) the practical application of knowledge by licentiate degree students in Computer Science, encompassing both learning theories and software development, through the evaluation of the tool. This research aims to develop and evaluate a lightweight web tool through a case study in the context of supporting feedback for programming activities in Brazilian higher education. Additionally, the study investigates whether the feedback is considered the main problem for first-year students in computer courses from the perspective of licentiate computer science students and whether their participation in the case study can improve their skills and abilities in analyzing educational tools. A total of 16 students and three professors from four disciplines participated in the case study, with an online questionnaire as the main instrument for data collection. The analysis was performed considering the collected responses' quantitative and qualitative aspects. We obtained quantitative responses through a Likert scale, while qualitative aspects were analyzed using the Discourse Unveiling Method (UDUM). Positive aspects were identified for both professors and students, providing initial evidence of the tool's potential. Moreover, the study highlighted important feedback from undergraduate Computer Science students, which provides an opportunity to apply their knowledge technically.},
  keywords={Instruments;Education;Data collection;Software;Programming profession},
  doi={10.1109/FIE58773.2023.10343349},
  ISSN={2377-634X},
  month={Oct},}@INPROCEEDINGS{10187381,
  author={Dwyer, Daniel and Omwenga, Maxwell M.},
  booktitle={2023 IEEE International Conference on Electro Information Technology (eIT)}, 
  title={Training Topology With Graph Neural Cellular Automata}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Graph neural cellular automata are a recently introduced class of computational models that extend neural cellular automata to arbitrary graphs. They are promising in various applications based on preliminary test results and the successes of related computational models, such as neural cellular automata and convolutional and graph neural networks. However, all previous graph neural cellular automaton implementations have only been able to modify data associated with the vertices and edges, not the underlying graph topology itself. Here we introduce a method of encoding graph topology information as vertex data by assigning each edge and vertex an opacity value, which is the confidence with which the model thinks that that edge or vertex should be present in the output graph. Graph neural cellular automata equipped with this encoding method, henceforth referred to as translucent graph neural cellular automata, were tested in their ability to learn to reconstruct graphs from random subgraphs of them as a proof of concept. The results suggest that translucent graph neural cellular automata are capable of this task, albeit with optimal learning rates highly dependent on the graph to be reconstructed.},
  keywords={Training;Network topology;Learning automata;Computational modeling;Automata;Encoding;Topology;Graph neural networks;machine learning;supervised learning},
  doi={10.1109/eIT57321.2023.10187381},
  ISSN={2154-0373},
  month={May},}@INPROCEEDINGS{8623310,
  author={Han, Xiao and Hu, Fang and Xiong, Gang and Liu, Xiwei and Gong, Xiaoyan and Niu, Xiaojie and Shi, Wanruo and Wang, Xinzhu},
  booktitle={2018 Chinese Automation Congress (CAC)}, 
  title={Design of AI + Curriculum for Primary and Secondary Schools in Qingdao}, 
  year={2018},
  volume={},
  number={},
  pages={4135-4140},
  abstract={We are entering a new AI era, and the development of AI technology and popularization of AI education has become the national strategy of China. The primary and secondary school students are the builder of future society, and have strong curiosity and passion to learn new things; therefore, we should make a good preparation of AI education for them and cultivate their basic AI literacy and computational thinking to adapt to the future. In this paper, from the viewpoint of thinking mode and literacy and skill training, a curriculum including basic, project, and practical types is designed. Integrating with the iSTREAM educational system proposed by Chinese Academy of Sciences, exploratory works were shown with some case studies. The applications show that AI education is helpful to enrich the study of primary and secondary school students and to prompt comprehensive quality.},
  keywords={Artificial intelligence;Education;Industries;Technological innovation;Programming profession;Task analysis;AI education;teaching and research;curriculum design;iSTREAM},
  doi={10.1109/CAC.2018.8623310},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9274155,
  author={Conte, Davi Jose and de Souza, Paulo Sergio Lopes and Martins, Guilherme and Bruschi, Sarita Mazzini},
  booktitle={2020 IEEE Frontiers in Education Conference (FIE)}, 
  title={Teaching Parallel Programming for Beginners in Computer Science}, 
  year={2020},
  volume={},
  number={},
  pages={1-9},
  abstract={This research full paper describes our experience in teaching parallel programming for students without previous knowledge of basic concepts of computing, comparing their levels of learning. The use of parallel software grew considerably in recent years due to the increasing availability of multi and many-core devices. The evolution of hardware and software resources collaborated for a remarkable computational processing power offered by parallel programs. However, parallel programming is taught usually in more advanced years of the undergraduate computer courses, due to its supposed prerequisites as sequential programming, operating systems, computer architectures and others. Postponing parallel programming teaching hinder students to apply parallelism other subjects, reducing the probability of these future professionals think on parallel solutions naturally. We executed 05 experiments teaching parallel programming subjects for 252 students. We analyzed whether students without prerequisites could learn parallel programming in the same level verified with students with prior computing knowledge. We used three different teaching methodologies: traditional, Problem Based Learning (PBL), and Team-Based Learning (TBL). The teaching and learning evaluation took into account such metrics: parallelism thinking of students, use of programming-model, correct output of the program, source-code readability and satisfaction of the students. The paper shows that it is possible to teach parallel programming to students without previous knowledge of computing, obtaining high scores and interest in such learning. Our results contribute positively to disseminate parallel programming, which is vital to extract performance from nowadays computers.},
  keywords={Education;Parallel programming;Parallel processing;Programming profession;Software;Measurement;Computer architecture;Education;Teaching;Learning;Parallel Programming;Computer Science Teaching},
  doi={10.1109/FIE44824.2020.9274155},
  ISSN={2377-634X},
  month={Oct},}@ARTICLE{10005170,
  author={Peterson, Tina L. and Ferreira, Rodrigo and Vardi, Moshe Y.},
  journal={IEEE Transactions on Technology and Society}, 
  title={Abstracted Power and Responsibility in Computer Science Ethics Education}, 
  year={2023},
  volume={4},
  number={1},
  pages={96-102},
  abstract={As computing becomes more powerful and extends the reach of those who wield it, the imperative grows for computing professionals to make ethical decisions regarding the use of that power. We propose the concept of abstracted power to help computer science students understand how technology may distance them perceptually from consequences of their actions. Specifically, we identify technological intermediation and computational thinking as two factors in computer science that contribute to this distancing. To counter the abstraction of power, we argue for increased emotional engagement in computer science ethics education, to encourage students to feel as well as think regarding the potential impacts of their power on others. We suggest four concrete pedagogical approaches to enable this emotional engagement in computer science ethics curriculum, and we share highlights of student reactions to the material.},
  keywords={Ethics;Computer science education;Writing;Cognition;Codes;Philosophical considerations;Social implications of technology;Curriculum development;Social factors;Power;abstraction;responsibility;social impact;emotional engagement;ethics},
  doi={10.1109/TTS.2022.3233776},
  ISSN={2637-6415},
  month={March},}@INPROCEEDINGS{6113219,
  author={Nagar, Yiftach},
  booktitle={2011 IEEE Third International Conference on Privacy, Security, Risk and Trust and 2011 IEEE Third International Conference on Social Computing}, 
  title={Beyond the Human-Computation Metaphor}, 
  year={2011},
  volume={},
  number={},
  pages={800-805},
  abstract={Two assumptions have become dominant in the field of social computing and crowd sourcing - the computational view, and the assumption of a human-only crowd. In this paper, I address those assumptions. I trace their origin in the human-computation metaphor, and argue that while this metaphor is instrumental in facilitating novel developments, it also constrains the thinking of designers. I discuss some of the limitations this metaphor might impose, and offer that additional perspectives, such as an organizational design perspective and the distributed cognition perspective can help us think of novel possibilities of organizing work with crowd sourcing. I call for extending the conversation among computer-scientists and organizational researchers, and propose that the metaphor of 'information processing' might serve as a 'boundary-object' around which the dialogue among these communities can thrive.},
  keywords={Humans;Computers;Cognition;Games;Organizing;Encyclopedias;human-computation;crowdsourcing;collective-intelligence;human-computer interaction;computer-supported-collaborative-work},
  doi={10.1109/PASSAT/SocialCom.2011.205},
  ISSN={},
  month={Oct},}@ARTICLE{7302530,
  author={Ning, Huansheng and Liu, Hong and Ma, Jianhua and Yang, Laurence T. and Wan, Yueliang and Ye, Xiaozhen and Huang, Runhe},
  journal={IEEE Access}, 
  title={From Internet to Smart World}, 
  year={2015},
  volume={3},
  number={},
  pages={1994-1999},
  abstract={The development of informationization and intelligentization prompts Internet developing toward a new era. A deep fusion among cyber space, physical space, social space, and thinking space brings a quaternionic cyber-physical-social-thinking hyperspace, based on which an embryo of smart world is being established through heterogeneous spaces. The smart world is expected to be an attractive perspective involving ubiquitous sensing, computing, and communication to achieve comprehensive interconnections of physical perception, cyber interaction, social correlation, and cognitive thinking. In this paper, evolution of the smart world is briefly introduced, and physical-based coordination, social-inspired interactivity, brain-abstracted cooperativity, and cyber-enabled homogeneity are, respectively, discussed as the main characteristics of the smart world.},
  keywords={Cyber-physical systems;Internet of things;Social factors;Smart world;Internet of Things;cyber-physicalsocial system;hyperspace;Smart world;Internet of Things;cyber-physical-social system;hyperspace},
  doi={10.1109/ACCESS.2015.2493890},
  ISSN={2169-3536},
  month={},}@ARTICLE{9425540,
  author={Metcalfe, Jason S. and Perelman, Brandon S. and Boothe, David L. and Mcdowell, Kaleb},
  journal={IEEE Access}, 
  title={Systemic Oversimplification Limits the Potential for Human-AI Partnership}, 
  year={2021},
  volume={9},
  number={},
  pages={70242-70260},
  abstract={The modern world is evolving rapidly, especially with respect to the development and proliferation of increasingly intelligent, artificial intelligence (AI) and AI-related technologies. Nevertheless, in many ways, what this class of technologies has offered as return on investment remains less impressive than what has been promised. In the present paper, we argue that the continued failure to realize the potential in modern AI and AI-related technologies is largely attributable to the oversimplified, yet pervasive ways that our global society treats the relationship between these technologies and humans. Oversimplified concepts, once conveyed, tend to perpetuate myths that in turn limit the impact of such technologies in human society. To counter these oversimplifications, we offer a theoretical construct, which we call the landscape of human-AI partnership. This construct characterizes individual capability for real-world task performance as a dynamic function of information certainty, available time to respond, and task complexity. With this, our goal is to encourage more nuanced discourse about novel ways to solve challenges to modern and future sociotechnical societies, but without defaulting to notions that remain rooted in today's technologies-as-tools ways of thinking. The core of our argument is that society at large must recognize that intelligent technologies are evolving well beyond being mere tools for human use and are instead becoming capable of operating as interdependent teammates. This means that how we think about interactions between humans and AI needs to go beyond a “Human-or-AI” conversation about task assignments to more contextualized “Human-and-AI” way of thinking about how best to capitalize on the strengths hidden within emergent capabilities of unique human-AI partnerships that have yet to be fully realized.},
  keywords={Artificial intelligence;Task analysis;Complexity theory;Tools;Robot sensing systems;Ecosystems;Aging;Human-AI partnership;human-autonomy teaming;sociotechnical systems;AI ecosystems;function allocation;task complexity;capability;use cases;implementation},
  doi={10.1109/ACCESS.2021.3078298},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{7546203,
  author={Manekar, Amit Kumar and Pradeepini, G.},
  booktitle={2015 International Conference on Computational Intelligence and Communication Networks (CICN)}, 
  title={Cloud Based Big Data Analytics a Review}, 
  year={2015},
  volume={},
  number={},
  pages={785-788},
  abstract={Today's computing world is facing tsunami and driving without riding on this tsunami towards next generation computing is no choice. So many IT companies decided to grow up with this tsunami like technology. One of these is cloud computing and another is Big data. Currently more than 5 billion mobile users and nearly same facebook and other social media user generate this tsunami of data. On another side to deliver this services of big data a model called as cloud computing is spreading everywhere as next generations IT Service model. Both technologies continue to evolve. Ultimately as a cloud computing development matures, every top mind of organizations will think for development of efficient and agile cloud environment. At the other side every cloud provider offers the services to the huge number data processing companies that generate data process data and make decision on cloud infrastructure. Ultimately its today's need to think on futures efficient cloud based Big data analytics In this review paper we are focusing on, how we can club Big data and cloud Computing in one frame of development.},
  keywords={Cloud computing;Big data;Distributed databases;Security;Next generation networking;Tsunami;Market research;Big Data;Cloud Computing;Data Management;Distributed Computing},
  doi={10.1109/CICN.2015.160},
  ISSN={2472-7555},
  month={Dec},}@INPROCEEDINGS{8311704,
  author={Subekti, Mohammad and Junaidi and Warnars, Harco Leslie Hendric Spits and Heryadi, Yaya},
  booktitle={2017 IEEE International Conference on Cybernetics and Computational Intelligence (CyberneticsCom)}, 
  title={The 3 steps of best data warehouse model design with leaning implementation for sales transaction in franchise restaurant}, 
  year={2017},
  volume={},
  number={},
  pages={170-174},
  abstract={Doing Data Warehouse (DW) to your business or system is not only think about the trend only, but how to understand the DW knowledge itself and how to implement it. DW as a real technology of Artificial Intelligent (AI) which show up as how to think like a human, inevitably help the human particularly in making quick decision reports. Doing DW to your systems should apply to the mother knowledge of AI which we recognized as Software Engineering (SE) where we should apply best software application in more importantly we should satisfy the user. In order to make a good DW model design as user expectation then we should apply 3 steps such as collect all the needed reporting, order all the reports start from the most needed reports and mapping all the ordered reports into fact constellation schema. In this paper the 3 steps to model DW schema is applied in sale transaction in franchise restaurant as an example data. Building franchise restaurant must supported by information technology such as Data Warehouse (DW) in order to manage and control the business process, the branch, the sale, the staff and so on.},
  keywords={Data warehouses;Databases;Data models;Computer science;Finance;Software;Star Schema;Snowflake;Fact Constellation;Data Warehouse;OLAP},
  doi={10.1109/CYBERNETICSCOM.2017.8311704},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{713830,
  author={Perlovsky, L.I.},
  booktitle={Proceedings of the 1998 IEEE International Symposium on Intelligent Control (ISIC) held jointly with IEEE International Symposium on Computational Intelligence in Robotics and Automation (CIRA) Intell}, 
  title={Cyberaesthetics: aesthetics, symbols and control}, 
  year={1998},
  volume={},
  number={},
  pages={857-862},
  abstract={The notions of concepts and emotions are mathematically described, and an "elementary" perception-cognition process is discussed inherently involving both aspects of the psyche. The mathematical nature of perception and cognition turn out to be very similar. The developed mathematical technique is practically useful for engineering designs and also corresponds to our intuition about our psyche. The paper establishes relationships between the developed mathematical techniques, classical semiotic concepts of sign and symbol, and more general notions of symbol in culture and psychology. An important and psychologically loaded notion of symbol refers to psychic processes connecting conscious and unconscious representations (concepts and archetypes). I analyze the elementary thinking process and discuss which aspects of this process are available to consciousness. This leads to a suggestion that the mathematical nature of the symbol process is similar to the perception-cognition process. Thus, thinking-process and symbol-process are same: a vortex of emotions, models-archetypes, and sensory data. This vortex brings forth into consciousness the previously unconscious contents of mind, which improve our understanding of the world.},
  keywords={Psychology;Artificial intelligence;Intelligent systems;Cognition;Design engineering;Joining processes;Cultural differences;Adaptive systems;Potential well;Intelligent networks},
  doi={10.1109/ISIC.1998.713830},
  ISSN={2158-9860},
  month={Sep.},}@INPROCEEDINGS{6643413,
  author={Jiang, Zuowen and Zhao, Yiming and Qin, Zhiping and Lin, Xueming},
  booktitle={2013 International Conference on Computational and Information Sciences}, 
  title={Coding Standard Based Object Oriented Programming Course Teaching Reform}, 
  year={2013},
  volume={},
  number={},
  pages={1890-1892},
  abstract={Object Oriented Programming is an important subject for computer major students, this course not only delivers the expertise on how to write program in an OOP language, but also guides students to think in object oriented paradigm. Teachers always found themselves in confusion on which language should be used as the teaching tool. In this paper we present some reform measures took based on our own teaching experiences, by introducing the thinking in object oriented paradigm. We use C#, C++ and Java in parallel to deliver the course. By emphasizing the programming standard in coding, the students acquired the knowledge to distinguish the 3 languages in practice as well as the OOP itself.},
  keywords={Educational institutions;Programming profession;Computers;Standards;Object oriented programming;Object Oriented Programming;coding standard},
  doi={10.1109/ICCIS.2013.494},
  ISSN={},
  month={June},}@INPROCEEDINGS{9988214,
  author={Ramesh, T. and Madhubala, E and Rani, N Sandhya and Roshini, K T},
  booktitle={2022 2nd International Conference on Technological Advancements in Computational Sciences (ICTACS)}, 
  title={A Novel Approach for Alzheimer Detection Using SMLT}, 
  year={2022},
  volume={},
  number={},
  pages={380-385},
  abstract={The absence of the brain chemical acetylcholine is the characteristic of Alzheimer's disease. In the early stages, this condition is thought to cause minimal memory loss, but as the disease progresses, it causes some loss of ability during discussion. It is a brain illness in which memory is progressively lost along with the capacity to think and carry out some daily duties. Some areas of the brain are also involved, including those that regulate the ability to think, remember, and use language. Alzheimer's treatment choices vary depending on whether or not the Alzheimer's is present. An Alzheimer's disease biopsy is required prior to final brain surgery in order to confirm the diagnosis. There are an estimated 6 million persons in the United States who suffer from Alzheimer's disease, and the majority of them are over the age of 65. Memory and other essential internal processes are lost as brain cell connections and cells themselves degrade and die. The most common signs and symptoms are disorientation and loss of memory. There is no cure, however medications and surgical procedures can alleviate symptoms for a while. For example, there are various hazard features to an Alzheimer's complaint, as well as a requirement for time to gather correct and accepted procedures in order to form an early assessment and resolve the complaint as quickly as possible. Data mining is a common practice in the healthcare industry for reusing large amounts of data. To better understand Alzheimer's disease, researchers use data mining and machine learning approaches to evaluate large volumes of medical information. Machine learning is being used to develop a model for diagnosing Alzheimer's disease at this time. In order to estimate the rise, various algorithms are examined and a current model is employed.},
  keywords={Training;Surgery;Machine learning;Medical services;Predictive models;Prediction algorithms;Data models;Alzheimer's disease Prediction;Machine Learning;SMLT},
  doi={10.1109/ICTACS56270.2022.9988214},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{6381625,
  author={Mandaleeka, Narayana GPL},
  booktitle={2012 IEEE International Conference on Computational Intelligence and Cybernetics (CyberneticsCom)}, 
  title={Organization-wide innovation management, a cybernetics approach}, 
  year={2012},
  volume={},
  number={},
  pages={98-102},
  abstract={Firms are recognizing the need to distinguish between R&D and Innovation. Innovation is becoming a distinct management function in the lines of marketing, HR, etc. Businesses need to be ambidextrous in their thinking taking into account capabilities of both right (creative) and left (analytical) side of the brain as challenges faced today requires integral solutions. The need is to work in cross functional teams, develop deep understanding of customer role, collaborate and more importantly address customer needs that may even not known to them; rather than just apply branding or fancy design styling. Fundamentals to integral approach are collaboration, embracing multidimensionality, systems thinking and applying cybernetics principles. The integral approach should involve users, designers, customers and suppliers. One need to understand the user based on fieldwork study, reach insights into user role and discover unarticulated needs. We normally measure innovation in terms of its technical strength and its utility value. In many cases this may not be enough. Innovation is a renewal process in that it prepares one for the future. It involves investing in the development of competence of the people, developing capabilities in them, nurturing customer relationships, and organizing knowledge bases. The point where technical innovation gets done is at the core of all these activities. The Balanced Score Card (BSC) has aroused considerable interest in the last two decades in business performance measurement. The reason could be that the managers are finding something more than the short term revenue reports that are prevalent. In the similar way the innovation can be looked not just at the point of idea creation but should be viewed as a staged process in the organization for the innovation to see the light of the market in a concerted way. This paper explains a cybernetics approach of modeling the innovation activities along the lines of Balanced Score Card (BSC).The principles of cybernetics may considered to be useful in `how we design” innovations as much as in “what innovations we design”.},
  keywords={Technological innovation;Companies;Cybernetics;Innovation management;Couplings;Innovation;key processes;cybernetics;balanced score card (BSC)},
  doi={10.1109/CyberneticsCom.2012.6381625},
  ISSN={},
  month={July},}@INPROCEEDINGS{638185,
  author={Eisner, H.},
  booktitle={1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation}, 
  title={Some lateral conjectures in science and psychology}, 
  year={1997},
  volume={2},
  number={},
  pages={1451-1456 vol.2},
  abstract={This paper uses aspects of lateral thinking traceable to de Bono (1970) to offer several conjectures relating to selected domains of science and psychology. In one instance, the former domain is that of thermodynamics and information theory and the latter involves mental health. The lateral conjecture takes the form of drawing an analogy between notions of entropy and the ability of the human to construct choices. In another case, a law of thermodynamics is viewed in terms of organizational behavior. In a third instance, a Prigogine idea (Prigogine, 1980) is briefly explored from the perspective of a lateral conjecture. These and other notions are introduced to encourage other researchers interested in the heuristics of lateral thinking within the context of interdisciplinary research.},
  keywords={Psychology;Cybernetics;Polarization;Thermodynamics;Fractionation;Information theory;Entropy;Humans;Communication system control;Control systems},
  doi={10.1109/ICSMC.1997.638185},
  ISSN={1062-922X},
  month={Oct},}@ARTICLE{9319727,
  author={Chen, Xing and Liu, Guizhong},
  journal={IEEE Internet of Things Journal}, 
  title={Energy-Efficient Task Offloading and Resource Allocation via Deep Reinforcement Learning for Augmented Reality in Mobile Edge Networks}, 
  year={2021},
  volume={8},
  number={13},
  pages={10843-10856},
  abstract={The augmented reality (AR) applications have been widely used in the field of Internet of Things (IoT) because of good immersion experience for users, but their ultralow delay demand and high energy consumption bring a huge challenge to the current communication system and terminal power. The emergence of mobile-edge computing (MEC) provides a good thinking to solve this challenge. In this article, we study an energy-efficient task offloading and resource allocation scheme for AR in both the single-MEC and multi-MEC systems. First, a more specific and detailed AR application model is established as a directed acyclic graph according to its internal functionality. Second, based on this AR model, a joint optimization problem of task offloading and resource allocation is formulated to minimize the energy consumption of each user subject to the latency requirement and the limited resources. The problem is a mixed multiuser competition and cooperation problem, which involves the task offloading decision, uplink/downlink transmission resources allocation, and computing resources allocation of users and MEC server. Since it is an NP-hard problem and the communication environment is dynamic, it is difficult for genetic algorithms or heuristic algorithms to solve. Therefore, we propose an intelligent and efficient resource allocation and task offloading algorithm based on the deep reinforcement learning framework of multiagent deep deterministic policy gradient (MADDPG) in a dynamic communication environment. Finally, simulation results show that the proposed algorithm can greatly reduce the energy consumption of each user terminal.},
  keywords={Task analysis;Servers;Optimization;Resource management;Energy consumption;Computational modeling;Heuristic algorithms;Augmented reality (AR);deep reinforcement learning;Internet of Things (IoT);mobile-edge computing (MEC);multiagent deep deterministic policy gradient (MADDPG);resource allocation;task offloading},
  doi={10.1109/JIOT.2021.3050804},
  ISSN={2327-4662},
  month={July},}@ARTICLE{6185551,
  author={Avraham, Guy and Nisky, Ilana and Fernandes, Hugo L. and Acuna, Daniel E. and Kording, Konrad P. and Loeb, Gerald E. and Karniel, Amir},
  journal={IEEE Transactions on Haptics}, 
  title={Toward Perceiving Robots as Humans: Three Handshake Models Face the Turing-Like Handshake Test}, 
  year={2012},
  volume={5},
  number={3},
  pages={196-207},
  abstract={In the Turing test a computer model is deemed to “think intelligently” if it can generate answers that are indistinguishable from those of a human. We developed an analogous Turing-like handshake test to determine if a machine can produce similarly indistinguishable movements. The test is administered through a telerobotic system in which an interrogator holds a robotic stylus and interacts with another party - artificial or human with varying levels of noise. The interrogator is asked which party seems to be more human. Here, we compare the human-likeness levels of three different models for handshake: (1) Tit-for-Tat model, (2) λ model, and (3) Machine Learning model. The Tit-for-Tat and the Machine Learning models generated handshakes that were perceived as the most human-like among the three models that were tested. Combining the best aspects of each of the three models into a single robotic handshake algorithm might allow us to advance our understanding of the way the nervous system controls sensorimotor interactions and further improve the human-likeness of robotic handshakes.},
  keywords={Humans;Muscles;Force;Computational modeling;Haptic interfaces;Robot sensing systems;Noise;Handshake;sensorimotor control;psychophysics;teleoperation;turing test.},
  doi={10.1109/TOH.2012.16},
  ISSN={2329-4051},
  month={Third},}@INPROCEEDINGS{8252094,
  author={Ahmad, Adang Suwandi and Sumari, Arwin Datumaya Wahyudi},
  booktitle={2017 Computing Conference}, 
  title={Cognitive artificial intelligence: Brain-inspired intelligent computation in artificial intelligence}, 
  year={2017},
  volume={},
  number={},
  pages={135-141},
  abstract={Computation occurred within human brain is very much awesome and is not possible to be emulated 100% exactly in Artificial Intelligence (AI) method-based machines. What scientists did and have been done so far up to now are to try to model it as close as to what exactly occurs within the brain. Human brain has an awesome mechanism in performing computation with the end result is new knowledge and human uses the knowledge to actuate his organs. In this paper we will show a new approach for emulating the computation occured within human brain to obtain new knowledge based on the inputs sensed by the system's sensory system taken from the environment. When this process is carried out recursively, the system's knowledge becomes newer and newer, and it is called as knowledge growing. This approach is designed for an agent that has ability to think and act rationally like human. Our cognitive modelling approach is resulted in a model of human information processing and a technique to obtain the most maximum performance should be taken by the cognitive agent. This method is called as A3S (Arwin-Adang-Aciek-Sembiring), the agent is called as Knowledge-Growing System (KGS) and this brain-inspired method opens a new perspective in AI that we call as Cognitive Artificial Intelligence (CAI).},
  keywords={Brain modeling;Information processing;Artificial intelligence;Mathematical model;Psychology;Computational modeling;A3S;Cognitive Artificial Intelligence;intelligent computation;knowledge extraction;Knowledge-Growing System},
  doi={10.1109/SAI.2017.8252094},
  ISSN={},
  month={July},}@INPROCEEDINGS{8819498,
  author={Haque, Md Ariful and Shetty, Sachin and Krishnappa, Bheshaj},
  booktitle={2019 IEEE 5th Intl Conference on Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High Performance and Smart Computing, (HPSC) and IEEE Intl Conference on Intelligent Data and Security (IDS)}, 
  title={ICS-CRAT: A Cyber Resilience Assessment Tool for Industrial Control Systems}, 
  year={2019},
  volume={},
  number={},
  pages={273-281},
  abstract={In this work, we use a subjective approach to compute cyber resilience metrics for industrial control systems. We utilize the extended form of the R4 resilience framework and span the metrics over physical, technical, and organizational domains of resilience. We develop a qualitative cyber resilience assessment tool using the framework and a subjective questionnaire method. We make sure the questionnaires are realistic, balanced, and pertinent to ICS by involving subject matter experts into the process and following security guidelines and standards practices. We provide detail mathematical explanation of the resilience computation procedure. We discuss several usages of the qualitative tool by generating simulation results. We provide a system architecture of the simulation engine and the validation of the tool. We think the qualitative simulation tool would give useful insights for industrial control systems' overall resilience assessment and security analysis.},
  keywords={Resilience;Measurement;Integrated circuits;Tools;Security;Robustness;Computational modeling;Cyber Resilience;Industrial Control Systems;Qualitative Tool;Resilience Metrics;Critical Service Functionality},
  doi={10.1109/BigDataSecurity-HPSC-IDS.2019.00058},
  ISSN={},
  month={May},}@INPROCEEDINGS{9322470,
  author={Baccour, Emna and Erbad, Aiman and Mohamed, Amr and Hamdi, Mounir and Guizani, Mohsen},
  booktitle={GLOBECOM 2020 - 2020 IEEE Global Communications Conference}, 
  title={DistPrivacy: Privacy-Aware Distributed Deep Neural Networks in IoT surveillance systems}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  abstract={With the emergence of smart cities, Internet of Things (IoT) devices as well as deep learning technologies have witnessed an increasing adoption. To support the requirements of such paradigm in terms of memory and computation, joint and real-time deep co-inference framework with IoT synergy was introduced. However, the distribution of Deep Neural Networks (DNN) has drawn attention to the privacy protection of sensitive data. In this context, various threats have been presented, including black-box attacks, where a malicious participant can accurately recover an arbitrary input fed into his device. In this paper, we introduce a methodology aiming to secure the sensitive data through re-thinking the distribution strategy, without adding any computation overhead. First, we examine the characteristics of the model structure that make it susceptible to privacy threats. We found that the more we divide the model feature maps into a high number of devices, the better we hide proprieties of the original image. We formulate such a methodology, namely DistPrivacy, as an optimization problem, where we establish a trade-off between the latency of co-inference, the privacy level of the data, and the limited-resources of IoT participants. Due to the NP-hardness of the problem, we introduce an online heuristic that supports heterogeneous IoT devices as well as multiple DNNs and datasets, making the pervasive system a general-purpose platform for privacy-aware and low decision-latency applications.},
  keywords={Privacy;Image segmentation;Task analysis;Computational modeling;Automobiles;Surveillance;Servers;IoT devices;distributed DNN;privacy;sensitive data;black-box;resource constraints},
  doi={10.1109/GLOBECOM42002.2020.9322470},
  ISSN={2576-6813},
  month={Dec},}@INPROCEEDINGS{8790188,
  author={Heintz, Benjamin and Hong, Rankyung and Singh, Shivangi and Khandelwal, Gaurav and Tesdahl, Corey and Chandra, Abhishek},
  booktitle={2019 IEEE International Conference on Cloud Engineering (IC2E)}, 
  title={MESH: A Flexible Distributed Hypergraph Processing System}, 
  year={2019},
  volume={},
  number={},
  pages={12-22},
  abstract={With the rapid growth of large online social networks, the ability to analyze large-scale social structure and behavior has become critically important, and this has led to the development of several scalable graph processing systems. In reality, however, social interaction takes place not only between pairs of individuals as in the graph model, but rather in the context of multi-user groups. Research has shown that such group dynamics can be better modeled through a more general hypergraph model, resulting in the need to build scalable hypergraph processing systems. In this paper, we present MESH, a flexible distributed framework for scalable hypergraph processing. MESH provides an easy-to-use and expressive application programming interface that naturally extends the "think like a vertex" model common to many popular graph processing systems. Our framework provides a flexible implementation based on an underlying graph processing system, and enables different design choices for the key implementation issues of partitioning a hypergraph representation. We implement MESH on top of the popular GraphX graph processing framework in Apache Spark. Using a variety of real datasets and experiments conducted on a local 8-node cluster as well as a 65-node Amazon AWS testbed, we demonstrate that MESH provides flexibility based on data and application characteristics, as well as scalability with cluster size. We further show that it is competitive in performance to HyperX, another hypergraph processing system based on Spark, while providing a much simpler implementation (requiring about 5X fewer lines of code), thus showing that simplicity and flexibility need not come at the cost of performance.},
  keywords={Computational modeling;Manganese;Cluster computing;Scalability;Partitioning algorithms;Sparks;Social networking (online);Hypergraph Processing System;Distributed Processing System},
  doi={10.1109/IC2E.2019.00-11},
  ISSN={},
  month={June},}@INPROCEEDINGS{7827611,
  author={Chang, Wanli and Roy, Debayan and Zhang, Licong and Chakraborty, Samarjit},
  booktitle={2016 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)}, 
  title={Model-based design of resource-efficient automotive control software}, 
  year={2016},
  volume={},
  number={},
  pages={1-8},
  abstract={Automotive platforms today run hundreds of millions of lines of software code implementing a large number of different control applications spanning across safety-critical functionality to driver assistance and comfort-related functions. While such control software today is largely designed following model-based approaches, the underlying models do not take into account the details of the implementation platforms, on which the software would eventually run. Following the state-of-the-art in control theory, the focus in such design is restricted to ensuring the stability of the designed controllers and meeting control performance objectives, such as settling time or peak overshoot. However, automotive platforms are highly cost-sensitive and the issue of designing “resource-efficient” controllers has largely been ignored so far and is addressed using very ad hoc techniques. In this paper, we will illustrate how, following traditional embedded systems design oriented thinking, computation, communication and memory issues can be incorporated in the controller design stage, thereby resulting in control software not only satisfying the usual control performance metrics but also making efficient utilization of the resources on distributed automotive architectures.},
  keywords={Automotive engineering;Actuators;Computer architecture;Measurement;Computational modeling;Embedded systems},
  doi={10.1145/2966986.2980075},
  ISSN={1558-2434},
  month={Nov},}@INPROCEEDINGS{1582903,
  author={Sprinkle, J. and Ames, A.D. and Pinto, A. and Haiyang Zheng and Sastry, S.S.},
  booktitle={Proceedings of the 44th IEEE Conference on Decision and Control}, 
  title={On the Partitioning of Syntax and Semantics For Hybrid Systems Tools}, 
  year={2005},
  volume={},
  number={},
  pages={4694-4699},
  abstract={Interchange formats are notoriously difficult to finish. That is, once one is developed, it is highly nontrivial to prove (or disprove) generality, and difficult at best to gain acceptance from all major players in the application domain. This paper addresses such a problem for hybrid systems, but not from the perspective of a tool interchange format, but rather that of tool availability in a toolbox. Through the paper we explain why we think this is a good approach for hybrid systems, and we also analyze the domain of hybrid systems to discern the semantic partitions that can be formed to yield a classification of tools based on their semantics. These discoveries give us the foundation upon which to build semantic capabilities, and to guarantee operational interaction between tools based on matched operational semantics.},
  keywords={Computational modeling;Embedded computing;Control system synthesis;Process control;Control systems;Hardware;Resistors;Capacitors;Inductors;Design engineering},
  doi={10.1109/CDC.2005.1582903},
  ISSN={0191-2216},
  month={Dec},}
