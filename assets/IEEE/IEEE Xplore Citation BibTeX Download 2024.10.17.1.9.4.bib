@INPROCEEDINGS{10575471,
  author={Harshjeet and Gogoi, Chandan and Snehalatha, N. and Amudha, S.},
  booktitle={2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC)}, 
  title={Enhancing Visual Creativity with Neural Style Transfer using Celery Backend Architecture}, 
  year={2024},
  volume={},
  number={},
  pages={966-974},
  abstract={Neural Style Transfer (NST) is an influential technique in the field of deep learning that combines the content of one image with the style of another, resulting in visually impressive compositions. By utilizing Convolutional Neural Networks (CNNs) such as VGG19, a well-known pretrained model that excels at extracting advanced characteristics from images, NST algorithms may efficiently isolate and merge content and style representations. This project aims to create a user-friendly Neural Style Transfer (NST) application. The frontend of the program will be constructed using React, enabling users to easily upload their content and style images. Next, the user can initiate the process of neural style transfer by clicking on the style transfer button. The computational process of style transfer is managed by a backend system that utilizes Celery, a distributed task queue, and Redis, an in-memory data structure store. This architectural design allows for effective asynchronous execution of style transfer activities, assuring the ability to handle large workloads while maintaining scalability and responsiveness. Our NST application combines advanced deep learning algorithms with modern web development technology to make artistic creativity accessible to everyone. Users may effortlessly produce compelling visual compositions with just a few clicks. Our website provides a smooth and entertaining experience for both beginners and expert users, whether they want to turn regular photos into works of art resembling famous styles or explore innovative combinations. The proposed NST application combines state-of-the-art algorithms with a user-friendly interface, making powerful AI-driven creativity accessible to everyone.},
  keywords={Deep learning;Visualization;Art;Scalability;Data structures;Convolutional neural networks;Task analysis;Neural Style Transfer;CNN;VGG19;Celery;Redis;Web Application;Python;Flask},
  doi={10.1109/ICAAIC60222.2024.10575471},
  ISSN={},
  month={June},}@ARTICLE{10415237,
  author={Vallarino, Mario and Iacono, Saverio and Bellanti, Edoardo and Vercelli, Gianni V.},
  journal={IEEE Transactions on Learning Technologies}, 
  title={A Flipped Remote Lab: Using a Peer-Assessment Tool for Learning 3-D Modeling}, 
  year={2024},
  volume={17},
  number={},
  pages={1140-1154},
  abstract={This article introduces a novel approach to remote laboratory instruction, specifically designed for teaching three-dimensional modeling using Blender software. The lab uses virtual machines to provide students with the necessary computational power to carry out the course activities, along with the correct version of the software. The flipped remote lab approach combines the elements of flipped classroom and peer assessment, making it suitable for face-to-face, totally online, or hybrid classes. Prior to each of the two lectures, students begin to practice by replicating the instructor's demonstrations in a set of concise tutorials. Upon completion of the assigned tasks, students carry out self-assessments of their own modeling, in addition to assessing two models created by their peers. A rubric comprising three questions facilitates the assessment process and allows providing feedback on each response. During the subsequent lecture, students work together with the instructor to address challenges encountered in their modeling, exploring also the advanced aspects of software usage that time constraints preclude in a traditional setting. The analysis of the flipped remote lab results reveals that student responses in peer-assessment activities are relevant to the posed questions. Moreover, the students who realized the models demonstrated a comparable level of rigor in self-assessment as their mates who reviewed their works. While students express a high degree of appreciation for the laboratory activities, a notable concern is the highlighted heavy workload. Increasing the allocated time for task completion can help mitigate the workload impact. The article concludes with insights gained from the implementation of the flipped remote lab approach.},
  keywords={Three-dimensional displays;Laboratories;Solid modeling;Software;Education;Virtual machining;Task analysis;Blender software;flipped classroom;peer assessment;remote lab;three-dimensional (3-D) modeling;virtual machines},
  doi={10.1109/TLT.2024.3358800},
  ISSN={1939-1382},
  month={},}@INPROCEEDINGS{10617440,
  author={Bhise, Ramkishan Baburao and Puyed, Ranjeet and Rane, Smita Kiran and Bhise, Swati Ramkishan},
  booktitle={2024 4th International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE)}, 
  title={Reexamining Disposition in AI Models Impact in Building Fiction based Characters}, 
  year={2024},
  volume={},
  number={},
  pages={1327-1332},
  abstract={As artificial intelligence (AI) continues to revolutionize various facets of human existence, its influence in creative domains, particularly literature, has increasingly come to be pronounced. The following paper will handle the ways that character development is changing in modern fiction. It investigates the ways in which AI technologies are outlining the way the creation, representation, and perception of characters in fiction are being developed. That means AI is advancing in character creation in ways that open up a whole new set of doors for writers: the use of powerful tools to generate, field, analyze, and refine their characters in ways previously unthinkable. Using the algorithms of natural language processing (NLP), these days, AI is used to help write dialogue, create characters with their traits, and sometimes even develop the entire character’s arc. This union of human creativity and computational powers doesn’t just make the process of writing faster but, rather, opens doors for new possibilities in innovative storytelling. AI-driven analytics help authors to pick up the insight into reader preferences and trends that tend to help them develop character traits and the storyline, most appealing to the readers. The application of machine-learning algorithms to vast repositories of literature seems to derive character dynamics and tropes that are so archetyped, but their renderings are much more nuanced and impactful. At the same time, this AI character raises very basic questions about authorship, creativity, and the nature of the story. As more and more writers adopt AI for character generation, they do raise some question about authenticity and originality.},
  keywords={Ethics;Sensitivity;Heuristic algorithms;Machine learning;Writing;Rendering (computer graphics);Market research;Artificial Intelligence;Characterization;Modern Fiction;Technology;Creativity;Narrative;AI-Generated Characters;Ethical Considerations;Storytelling;Literature},
  doi={10.1109/ICACITE60783.2024.10617440},
  ISSN={},
  month={May},}@ARTICLE{10227838,
  author={Nowak, Stan and Aseniero, Bon Adriel and Bartram, Lyn and Grossman, Tovi and Fitzmaurice, George and Matejka, Justin},
  journal={IEEE Computer Graphics and Applications}, 
  title={Identifying Visualization Opportunities to Help Architects Manage the Complexity of Building Codes}, 
  year={2023},
  volume={43},
  number={6},
  pages={75-86},
  abstract={We report a study investigating the viability of using interactive visualizations to aid architectural design with building codes. While visualizations have been used to support general architectural design exploration, existing computational solutions treat building codes as separate from, rather than part of, the design process, creating challenges for architects. Through a series of participatory design studies with professional architects, we found that interactive visualizations have promising potential to aid design exploration and sensemaking in early stages of architectural design by providing feedback about potential allowances and consequences of design decisions. However, implementing a visualization system necessitates addressing the complexity and ambiguity inherent in building codes. To tackle these challenges, we propose various user-driven knowledge management mechanisms for integrating, negotiating, interpreting, and documenting building code rules.},
  keywords={Codes;Buildings;Visualization;Interviews;Surveys;Automation;Data visualization},
  doi={10.1109/MCG.2023.3307971},
  ISSN={1558-1756},
  month={Nov},}@ARTICLE{10246990,
  author={Zhao, Le and Yao, Hongtai and Fan, Yajun and Ma, Haihua and Li, Zhihui and Tian, Meng},
  journal={IEEE Transactions on Aerospace and Electronic Systems}, 
  title={SceneNet: A Multifeature Joint Embedding Network With Complexity Assessment for Power Line Scene Classification}, 
  year={2023},
  volume={59},
  number={6},
  pages={9094-9116},
  abstract={Power line extraction is not only crucial for unmanned aerial vehicles (UAVs) obstacle avoidance, but also a fundamental step for fault diagnosis of power lines. Therefore, achieving robust and accurate extraction of power lines in aerial images is essential to enable intelligent UAVs inspection. Unfortunately, power line extraction is an extremely challenging task, and all the current methods attempt to utilize a single model to solve the problem of power line extraction in complex and variable scenes. This results in insufficient generalization ability and suboptimal computational efficiency. In this work, we propose a power line scene classification network based on complexity assessment, named SceneNet, which can provide a solution for tackling power line extraction challenges. First, we propose a human–machine hybrid reasoning model to obtain the ground truth of image complexity reasonably and build the first benchmark dataset that can be used for automatic classification research of power line scenes. Second, we propose an improved StyleGAN3 model and loop transfer learning strategy for data augmentation. Most importantly, the SceneNet comprises a multifeature joint embedding module and a feature encoding–decoding module. On one hand, it achieves the multilevel fusion of artificial features and high-dimensional semantic features. On the other hand, we use a self-attention mechanism to enable full use of the contextual association between each block of the fusion feature map. The SceneNet has successfully achieved the mapping and pattern recognition between the abstract concept and the concrete features. Experimental results demonstrate that the SceneNet is obviously superior to the existing 12 state-of-the-art models, and it provides guidance and delineation of applicable scenes for power line extraction methods.},
  keywords={Feature extraction;Complexity theory;Data mining;Inspection;Data models;Adaptation models;Standards;Human–machine hybrid reasoning;image complexity (IC) assessment;multifeature joint embedding;scene classification;UAVs inspection},
  doi={10.1109/TAES.2023.3313993},
  ISSN={1557-9603},
  month={Dec},}@INPROCEEDINGS{10323987,
  author={Reed, Andy and Dooley, Laurence S. and Mostefaoui, Soraya Kouadri},
  booktitle={2023 International Symposium on Networks, Computers and Communications (ISNCC)}, 
  title={Packet Filtering and Sampling for Efficient Slow Denial of Service Detection in Resource Scarce IoT Networks}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={There has recently been considerable interest in automatic detection strategies for recognising application layer security threats such as Hypertext Transfer Protocol (HTTP) Slow Denial of-Service (Slow DoS) attacks in Internet of Things (IoT) networks. Most existing approaches however, fail to take cognisance of the substantial resource constraints imposed upon IoT environments, which limits the applicability and deployment of many Slow DoS detection mechanisms. This paper addresses this significant security threat for resource scarce IoT nodes and networks in proposing an accurate and computationally efficient approach to packet-based intrusion detection of HTTP Slow DoS activity. The paper both critically analyses and measures the impact of applying network attribute filtering and packet sampling to reduce the computational overheads on the resource constrained IoT Slow DoS detection node. The unique solution proposed uses a dataset synthesised from a live IoT environment comprising both legitimate and malicious network events in the form of legitimate HTTP traffic and Slow DoS attacks. Experimental results corroborate that combining filtering at the Border Router of only in-bound packets containing no TCP payload with a systematic packet sampling scheme at a sampling ratio of up to 1:64, the processing overheads on the detection node are significantly reduced. The novel contribution presented is a resource efficient solution, garnered by employing systematic sampling to seamlessly and accurately support selective attribute-based intrusion detection of HTTP Slow DoS attacks in IoT networks.},
  keywords={Systematics;Protocols;Intrusion detection;TCPIP;Information filters;Real-time systems;Energy efficiency;Internet of Things;Intrusion Detection;Slow DoS;Sampling;Attribute Filtering;Systematic Sampling},
  doi={10.1109/ISNCC58260.2023.10323987},
  ISSN={2768-0940},
  month={Oct},}@INPROCEEDINGS{7398384,
  author={Seaman, Bill},
  booktitle={2015 International Conference on Cyberworlds (CW)}, 
  title={Neosentient Architecture Generator}, 
  year={2015},
  volume={},
  number={},
  pages={8-13},
  abstract={This paper outlines an approach to a series of "open" purpose "architecture" generators and related systems. The word "architecture" is here defined in the broadest of linguistic manners. This paper outlines the long-term goals for the facilitation of the functional components of this 'system of systems', although the system will be designed to be extensible as a large-scale open source project. Three major components make up the system: 1) one or more databases, 2) a means to generate intelligent queries, and 3) a virtual media space to display media elements and processes as derived from the queries/interaction. I will begin by discussing precursors and historical problems related to computational creativity and symbiotic creative systems. Given the non-linear nature of the subject matter, the following text will progress in a modular manner, where much of the paper could we read in differing orders. Thus, it will not unfold in the linear manner of other more traditional IEEE papers, and should be explored with this intentionality by the reader. The project takes a multi-perspective approach to the subject matter in which no overarching hierarchy can be given, suggesting the need for such a form. Thus I will draw on the IEEE formatting related to "Component Heads" -- "components of your paper and are not topically subordinate to each other." This structure also mirrors the functionality of the system itself which seeks to explore dynamic heterarchical combinatorics as its overarching methodology as an intelligent system. Thus the reader may need to work harder at making the creative jumps that enable this analogue "system" to function by reading each of these different component heads in relation to each other and articulating their own connections between the components. This is a Koestler-like bisociational approach[1]. Thus the paper itself becomes an analogue combinatoric "architecture" used to discuss this project.},
  keywords={Computer architecture;Media;Artificial intelligence;Architecture;Context;Generators;Sensors;Generative Architecture;Generative Art;Recombinant Poetics;Recombinant Informatics;Generative Systems;Cybernetics;Systems Approach},
  doi={10.1109/CW.2015.55},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{4274657,
  author={Cui, Wanan and Song, Bin and Yue, Chaoyuan and Qin, Jiajun},
  booktitle={2006 IEEE International Conference on Systems, Man and Cybernetics}, 
  title={Evaluation of Criticality Analysis Measurements in Resource-Constrained Projects}, 
  year={2006},
  volume={6},
  number={},
  pages={4709-4716},
  abstract={Criticality analysis of activity and path is an important problem in resource-constrained projects. This paper proposes the method of minimal feasible sets (MFS) for the identification of critical activities and critical sequences. And then, we compare MFS with seven related previous methods. Computational results manifest that MFS does not perform inferior to other methods at least and excel them in most situations in establishing resource links and computing floats.},
  keywords={Processor scheduling;Project management;Chaos;Algorithm design and analysis;Geoscience;Delay effects;Cybernetics;Scheduling algorithm;Mathematical model;Risk management},
  doi={10.1109/ICSMC.2006.385048},
  ISSN={1062-922X},
  month={Oct},}@ARTICLE{9756272,
  author={Cao, Longbing},
  journal={IEEE Intelligent Systems}, 
  title={A New Age of AI: Features and Futures}, 
  year={2022},
  volume={37},
  number={1},
  pages={25-37},
  abstract={By reviewing the 70 years of AI, this article summarizes and discusses the paradigm transformations from the age of AI before the year 2000 to the new age of AI from the year 2000 onward. It reviews the AI thinking and features of various AI generations and paradigms during these two ages of AI and their transformations. The paper further summarizes several AI Formulas from the AI vision, system, goal, task, and process perspectives. Several important areas are highlighted in developing AI Futures: shrinking the gaps between human, natural and social AI, and developing human-like/level AI, meta AI, reflective AI, metasynthetic AI, data-driven AI, beyond ‘IID AI,’ actionable AI, and sustainable AI. In the new age of AI, we encourage your deep thinking of AI futures.},
  keywords={Machine vision;Artificial intelligence;Technology forecasting;Intelligent systems},
  doi={10.1109/MIS.2022.3150944},
  ISSN={1941-1294},
  month={Jan},}@ARTICLE{9324949,
  author={Garvey, Shunryu Colin},
  journal={IEEE Annals of the History of Computing}, 
  title={The “General Problem Solver” Does Not Exist: Mortimer Taube and the Art of AI Criticism}, 
  year={2021},
  volume={43},
  number={1},
  pages={60-73},
  abstract={This article reconfigures the history of artificial intelligence (AI) and its accompanying tradition of criticism by excavating the work of Mortimer Taube, a pioneer in information and library sciences, whose magnum opus, Computers and Common Sense: The Myth of Thinking Machines (1961), has been mostly forgotten. To convey the essence of his distinctive critique, the article focuses on Taube's attack on the general problem solver (GPS), the second major AI program. After examining his analysis of the social construction of this and other “thinking machines,” it concludes that, despite technical changes in AI, much of Taube's criticism remains relevant today. Moreover, his status as an “information processing” insider who criticized AI on behalf of the public good challenges the boundaries and focus of most critiques of AI from the past half-century. In sum, Taube's work offers an alternative model from which contemporary AI workers and critics can learn much.},
  keywords={Artificial intelligence;History;Computers;Information retrieval;Artificial Intelligence (AI);Information Processing;Cognitive Simulation;Computer Systems;Social Criticism},
  doi={10.1109/MAHC.2021.3051686},
  ISSN={1934-1547},
  month={Jan},}@INPROCEEDINGS{9795727,
  author={Alvarado-Landeo, Ismael and Surichaqui-Montalvo, Erick and Velasquez-Colorado, Kener and Huamanchahua, Deyby},
  booktitle={2022 IEEE International IOT, Electronics and Mechatronics Conference (IEMTRONICS)}, 
  title={Artificial Intelligence Applied in Human Medicine with the Implementation of Prostheses}, 
  year={2022},
  volume={},
  number={},
  pages={1-7},
  abstract={The use of artificial intelligence (AI) in medicine is already a reality. Everywhere there is talk of the advantages that AI can mean for the future in our daily lives, as well as its possible applications. The future of "standard" medical practice could appear here ahead of schedule, where a patient could go to a computer before seeing a doctor. Through advances in AI, it becomes more possible for the days of misdiagnosis and treatment of the symptoms of the disease, rather than its root cause, to be left behind. Think about how many years of blood pressure measurements you have or how much storage you would need to remove so that you can fit a complete 3D image of an organ on your laptop. The idea of artificial intelligence in medicine may make you think of robots roaming the halls of a hospital in the distant future, but AI is already here.},
  keywords={Schedules;Three-dimensional displays;Prototypes;Artificial intelligence;Prognostics and health management;Robots;Medical diagnostic imaging;Artificial intelligence;prosthesis implantation;prosthetic rehabilitation;nanomaterials;modeling},
  doi={10.1109/IEMTRONICS55184.2022.9795727},
  ISSN={},
  month={June},}@INPROCEEDINGS{713806,
  author={Rocha, L.M.},
  booktitle={Proceedings of the 1998 IEEE International Symposium on Intelligent Control (ISIC) held jointly with IEEE International Symposium on Computational Intelligence in Robotics and Automation (CIRA) Intell}, 
  title={Syntactic autonomy}, 
  year={1998},
  volume={},
  number={},
  pages={706-711},
  abstract={The study of adapting and evolving autonomous agents should be based on a complex systems-theoretic framework which requires both self-organizing and symbolic dimensions. An inclusive framework based on the notions of semiotics and situated action is advanced to build models capable of representing, as well as evolving in their environments. Such undertaking is pursued by discussing the ways in which symbol and self-organization are irreducibly intertwined in evolutionary systems. This way, we re-think the notion of autonomy of evolving systems, and show that evolutionary systems are characterized by a particular type of syntactic autonomy. Developments in emergent computation in cellular automata are discussed as examples of the emergence of syntactic autonomy in computational environments. New results emphasizing this syntactic autonomy in cellular automata are presented.},
  keywords={Physics computing;Neural networks;State-space methods;Pattern recognition;Computer networks;Laboratories;Uniform resource locators;Autonomous agents;Genetic algorithms;Interference},
  doi={10.1109/ISIC.1998.713806},
  ISSN={2158-9860},
  month={Sep.},}@INPROCEEDINGS{4630414,
  author={Seising, Rudolf},
  booktitle={2008 IEEE International Conference on Fuzzy Systems (IEEE World Congress on Computational Intelligence)}, 
  title={Is there a concept of fuzziness in the epistemological systems of Heinrich Hertz and Ludwig Wittgenstein?}, 
  year={2008},
  volume={},
  number={},
  pages={498-505},
  abstract={"A picture is a model of reality", "A picture is a fact", and "We picture facts to ourselves" asserted Ludwig Wittgenstein in his Tractatus logico-philosophicus, thereby confirming the influence on his thinking - which he himself acknowledged - of Heinrich Hertz's Prinzipien der Mechanik (principles of mechanics). In this contribution the "picture" concept, which has a long tradition in philosophy, serves as the starting point of an interpretation of the relationship between real systems and theoretical structures of modern science. In addition, the approach dubbed as the "structuralist" approach of scientific theories in the 20th century will be extended and enhanced by the concept of "fuzzy sets" and "fuzzy relations". This fuzzy structuralist view on scientific theories enables us to combine philosophy of science with the methodologies of Computing with words (CW) and the computational theory of perceptions (CTP). Finally we give future prospects of this work.},
  keywords={Fuzzy systems;Conferences},
  doi={10.1109/FUZZY.2008.4630414},
  ISSN={1098-7584},
  month={June},}@INPROCEEDINGS{9041785,
  author={Jordan Rojas Dallaqua, Fernanda Beatriz and Fazenda, Álvaro Luiz and Faria, Fabio Augusto},
  booktitle={2019 15th International Conference on eScience (eScience)}, 
  title={ForestEyes Project: Can Citizen Scientists Help Rainforests?}, 
  year={2019},
  volume={},
  number={},
  pages={18-27},
  abstract={Scientific projects involving volunteers for analyzing, collecting data, and using their computational resources, known as Citizen Science (CS), have become popular due to advances in information and communication technology (ICT). Many CS projects have been proposed to involve citizens in different knowledge domain such as astronomy, chemistry, mathematics, and physics. This work presents a CS project called ForestEyes, which proposes to track deforestation in rainforests by asking volunteers to analyze and classify remote sensing images. These manually classified data are used as input for training a pattern classifier that will be used to label new remote sensing images. ForestEyes project was created on the Zooniverse.org CS platform, and to attest the quality of the volunteers' answers, were performed early campaigns with remote sensing images from Brazilian Legal Amazon (BLA). The results were processed and compared to an oracle classification (PRODES - Amazon Deforestation Monitoring Project). Two and a half weeks after launch, more than 35,000 answers from 383 volunteers (117 anonymous and 266 registered users) were received, completing all 2050 tasks. The ForestEyes campaigns' results have shown that volunteers achieved excellent effectiveness results in remote sensing image classification task. Furthermore, these results show that CS might be a powerful tool to quickly obtain a large amount of high-quality labeled data.},
  keywords={Citizen Science;Deforestation area detection;Rainforest;Tropical forest;Volunteered Thinking},
  doi={10.1109/eScience.2019.00010},
  ISSN={},
  month={Sep.},}@ARTICLE{10669020,
  author={Ye, Jinfei and Lu, Yang and Wang, Kuokuo and Jiao, Qingchun and Chen, Xuwen and Wang, Lijun},
  journal={IEEE Access}, 
  title={Design and Stage Analysis of a Non-Invasive Monitoring System for Determining Formaldehyde Release From Wood-Based Panels by the Chamber Method}, 
  year={2024},
  volume={12},
  number={},
  pages={137952-137969},
  abstract={With the extensive application of wood-based panels in the home decor industry, determining formaldehyde release form wood-based panels (FRWBP) is crucial for ensuring environmental air quality and human health. This article addresses the issues of insufficient monitoring of the testing stages for FRWBP and the difficulty in tracing the operation records of operator during determining FRWBP by the chamber method. It proposes a system for monitoring and analyzing the testing stages of FRWBP based on electrical monitoring. The system employs non-invasive sensing technology that does not interfere with normal testing process to collect real-time electrical usage data from the devices. The data is uploaded to an industrial computer for storage and processing. Statistical features of the electrical time series data are extracted using a sliding window approach and optimized with Principal Component Analysis (PCA). Subsequently, the binary tree support vector machine algorithm (BTSVM) optimized by k-means clustering algorithm is used to identify the detection states of the wood-based panels. Experimental results show that the system can achieve traceability of the formaldehyde detection stage records of artificial boards, and the designed algorithm model can also effectively identify the detection status of wood-based panels. The system and methods proposed in this paper have significant prospects for implementation in monitoring and traceability management within the testing industry.},
  keywords={Environmental monitoring;Temperature measurement;Support vector machines;Sensor phenomena and characterization;Noninvasive treatment;Formaldehyde;Principal component analysis;Air quality;FRWBP;chamber method;non-invasive;PCA;k-means;BTSVM},
  doi={10.1109/ACCESS.2024.3455375},
  ISSN={2169-3536},
  month={},}@ARTICLE{10433494,
  author={Okuno, Akifumi and Morishita, Yuya and Mototake, Yoh-Ichi},
  journal={IEEE Access}, 
  title={Autoregressive With Slack Time Series Model for Forecasting a Partially-Observed Dynamical Time Series}, 
  year={2024},
  volume={12},
  number={},
  pages={24621-24630},
  abstract={This study delves into the domain of dynamical systems, specifically the forecasting of dynamical time series defined through an evolution function. Traditional approaches in this area predict the future behavior of dynamical systems by inferring the evolution function. However, these methods may confront obstacles due to the presence of missing variables, which are usually attributed to challenges in measurement and a partial understanding of the system of interest. To overcome this obstacle, we introduce the autoregressive with slack time series (ARS) model, that simultaneously estimates the evolution function and imputes missing variables as a slack time series. Assuming time-invariance and linearity in the (underlying) entire dynamical time series, our experiments demonstrate the ARS model’s capability to forecast future time series. From a theoretical perspective, we prove that a 2-dimensional time-invariant and linear system can be reconstructed by utilizing observations from a single, partially observed dimension of the system.},
  keywords={Time series analysis;Mathematical models;Dynamical systems;Predictive models;Forecasting;Delays;Dynamics;Autoregressive processes;Autoregressive model;completely missing variables;dynamical system;slack time series},
  doi={10.1109/ACCESS.2024.3365724},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{7840980,
  author={Bunn, Jenny},
  booktitle={2016 IEEE International Conference on Big Data (Big Data)}, 
  title={Mind the explanatory gap: Quality from quantity}, 
  year={2016},
  volume={},
  number={},
  pages={3240-3244},
  abstract={This paper makes a contribution to the development of computational archival science by thinking about and linking computational and archival thinking. It suggests that archival thinking and the archival problem space encompasses questions about the nature of consciousness, highlighting how these questions seem to be apparent within the fundamental archival principles of respect des fonds, provenance and original order. It seeks to shift attention back to provenance, not just in the sense of where a given object has come from, but also in the sense of the grounds on which it becomes an object and it suggests that the application of computational methods and tools for archival purposes and the integration of computational with archival thinking may offer a way to maintain awareness of this more philosophical dimension in practice.},
  keywords={Information processing;Organizations;Computers;Substrates;Context;Big data;consciousness;provenance;ontology},
  doi={10.1109/BigData.2016.7840980},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8954359,
  author={Aoki, Yasuhiro and Goforth, Hunter and Srivatsan, Rangaprasad Arun and Lucey, Simon},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={PointNetLK: Robust & Efficient Point Cloud Registration Using PointNet}, 
  year={2019},
  volume={},
  number={},
  pages={7156-7165},
  abstract={PointNet has revolutionized how we think about representing point clouds. For classification and segmentation tasks, the approach and its subsequent variants/extensions are considered state-of-the-art. To date, the successful application of PointNet to point cloud registration has remained elusive. In this paper we argue that PointNet itself can be thought of as a learnable "imaging" function. As a consequence, classical vision algorithms for image alignment can be brought to bear on the problem -- namely the Lucas & Kanade (LK) algorithm. Our central innovations stem from: (i) how to modify the LK algorithm to accommodate the PointNet imaging function, and (ii) unrolling PointNet and the LK algorithm into a single trainable recurrent deep neural network. We describe the architecture, and compare its performance against state-of-the-art in several common registration scenarios. The architecture offers some remarkable properties including: generalization across shape categories and computational efficiency -- opening up new paths of exploration for the application of deep learning to point cloud registration. Code and videos are available at https://github.com/hmgoforth/PointNetLK.},
  keywords={Point cloud compression;Deep learning;Training;Technological innovation;Shape;Imaging;Computer architecture;Deep Learning;3D from Multiview and Sensors;Motion and Tracking},
  doi={10.1109/CVPR.2019.00733},
  ISSN={2575-7075},
  month={June},}@ARTICLE{5675671,
  author={Mininno, Ernesto and Neri, Ferrante and Cupertino, Francesco and Naso, David},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={Compact Differential Evolution}, 
  year={2011},
  volume={15},
  number={1},
  pages={32-54},
  abstract={This paper proposes the compact differential evolution (cDE) algorithm. cDE, like other compact evolutionary algorithms, does not process a population of solutions but its statistic description which evolves similarly to all the evolutionary algorithms. In addition, cDE employs the mutation and crossover typical of differential evolution (DE) thus reproducing its search logic. Unlike other compact evolutionary algorithms, in cDE, the survivor selection scheme of DE can be straightforwardly encoded. One important feature of the proposed cDE algorithm is the capability of efficiently performing an optimization process despite a limited memory requirement. This fact makes the cDE algorithm suitable for hardware contexts characterized by small computational power such as micro-controllers and commercial robots. In addition, due to its nature cDE uses an implicit randomization of the offspring generation which corrects and improves the DE search logic. An extensive numerical setup has been implemented in order to prove the viability of cDE and test its performance with respect to other modern compact evolutionary algorithms and state-of-the-art population-based DE algorithms. Test results show that cDE outperforms on a regular basis its corresponding population-based DE variant. Experiments have been repeated for four different mutation schemes. In addition cDE outperforms other modern compact algorithms and displays a competitive performance with respect to state-of-the-art population-based algorithms employing a DE logic. Finally, the cDE is applied to a challenging experimental case study regarding the on-line training of a nonlinear neural-network-based controller for a precise positioning system subject to changes of payload. The main peculiarity of this control application is that the control software is not implemented into a computer connected to the control system but directly on the micro-controller. Both numerical results on the test functions and experimental results on the real-world problem are very promising and allow us to think that cDE and future developments can be an efficient option for optimization in hardware environments characterized by limited memory.},
  keywords={Algorithm design and analysis;Optimization;Evolutionary computation;Training;Robots;Memory management;Hardware;Adaptive systems;compact genetic algorithms;differential evolution (DE);estimation distribution algorithms},
  doi={10.1109/TEVC.2010.2058120},
  ISSN={1941-0026},
  month={Feb},}@INPROCEEDINGS{7102580,
  author={Harman, Mark and Jia, Yue and Zhang, Yuanyuan},
  booktitle={2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)}, 
  title={Achievements, Open Problems and Challenges for Search Based Software Testing}, 
  year={2015},
  volume={},
  number={},
  pages={1-12},
  abstract={Search Based Software Testing (SBST) formulates testing as an optimisation problem, which can be attacked using computational search techniques from the field of Search Based Software Engineering (SBSE). We present an analysis of the SBST research agenda, focusing on the open problems and challenges of testing non-functional properties, in particular a topic we call 'Search Based Energy Testing' (SBET), Multi-objective SBST and SBST for Test Strategy Identification. We conclude with a vision of FIFIVERIFY tools, which would automatically find faults, fix them and verify the fixes. We explain why we think such FIFIVERIFY tools constitute an exciting challenge for the SBSE community that already could be within its reach.},
  keywords={Software testing;Search problems;Optimization;Software engineering;Energy consumption;Software},
  doi={10.1109/ICST.2015.7102580},
  ISSN={2159-4848},
  month={April},}@ARTICLE{1386655,
  author={Choy, R. and Edelman, A.},
  journal={Proceedings of the IEEE}, 
  title={Parallel MATLAB: Doing it Right}, 
  year={2005},
  volume={93},
  number={2},
  pages={331-341},
  abstract={MATLAB is one of the most widely used mathematical computing environments in technical computing. It is an interactive environment that provides high-performance computational routines and an easy-to-use, C-like scripting language. It started out as an interactive interface to EISPACK and LINPACK and has remained a serial program. In 1995, C. Moler of Mathworks argued that there was no market at the time for a parallel MATLAB. But times have changed and we are seeing increasing interest in developing a parallel MATLAB, from both academic and commercial sectors. In a recent survey, 27 parallel MATLAB projects have been identified. We expand upon that survey and discuss the approaches the projects have taken to parallelize MATLAB. Also, we describe innovative features in some of the parallel MATLAB projects. Then we will conclude with an idea of a "right" parallel MATLAB. Finally we will give an example of what we think is a "right" parallel MATLAB: MATLAB*P.},
  keywords={MATLAB;Concurrent computing;Mathematical model;Libraries;Distributed computing;Application software;Parallel processing;Eigenvalues and eigenfunctions;Linear systems;Mathematics;MATLAB;MATLAB;parallel;Star-P},
  doi={10.1109/JPROC.2004.840490},
  ISSN={1558-2256},
  month={Feb},}@ARTICLE{1039192,
  author={Ovaska, S.J. and VanLandingham, H.F. and Kamiya, A.},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)}, 
  title={Fusion of soft computing and hard computing in industrial applications: an overview}, 
  year={2002},
  volume={32},
  number={2},
  pages={72-79},
  abstract={Soft computing (SC) is an emerging collection of methodologies which aims to exploit tolerance for imprecision, uncertainty, and partial truth to achieve robustness, tractability, and low total cost. It differs from conventional hard computing (HC) in the sense that, unlike hard computing, it is strongly based on intuition or subjectivity. Therefore, soft computing provides an attractive opportunity to represent the ambiguity in human thinking with real life uncertainty. Fuzzy logic (FL), neural networks (NN), and genetic algorithms (GA) are the core methodologies of soft computing. However, FL, NN, and GA should not be viewed as competing with each other, but synergistic and complementary instead. Considering the number of available journal and conference papers on various combinations of these three methods, it is easy to conclude that the fusion of individual soft computing methodologies has already been advantageous in numerous applications. On the other hand, hard computing solutions are usually more straightforward to analyze; their behavior and stability are more predictable; and, the computational burden of algorithms is typically either low or moderate. These characteristics. are particularly important in real-time applications. Thus, it is natural to see SC and HC as potentially complementary methodologies. Novel combinations of different methods are needed when developing high-performance, cost-effective, and safe products for the demanding global market. We present an overview of applications in which the fusion of soft computing and hard computing has provided innovative solutions for challenging real-world problems. A carefully selected list of references is considered with evaluative discussions and conclusions.},
  keywords={Computer applications;Computer industry;Fuzzy logic;Neural networks;Uncertainty;Robustness;Costs;Humans;Genetic algorithms;Computer networks},
  doi={10.1109/TSMCC.2002.801354},
  ISSN={1558-2442},
  month={May},}@ARTICLE{8648434,
  author={Lipka, Melanie and Sippel, Erik and Vossiek, Martin},
  journal={IEEE Access}, 
  title={An Extended Kalman Filter for Direct, Real-Time, Phase-Based High Precision Indoor Localization}, 
  year={2019},
  volume={7},
  number={},
  pages={25288-25297},
  abstract={Radio-based indoor localization is currently a very vibrant scientific research field with many potential use cases. It offers high value for customers, for example, in the fields of robotics, logistics, and automation, or in context-aware IT services. Especially for autonomous systems, dynamic human-machine interaction, or augmented reality applications, precise localization coupled with a high update rate is a key. In this paper, we present a completely novel localization concept whereby received radio signal phase values that are fed into an extended Kalman filter (EKF) without any preprocessing are evaluated. Standard preprocessing steps, such as angle-of-arrival estimation, beamforming, and time-of-flight or time-difference-of-arrival estimations are not required with this approach. The innovative localization concept benefits from the high sensitivity of radio signals' phase to distance changes and the fast and straightforward recursive computation offered by the EKF. It completely forgoes the computational burden of other phase-based high-precision localization techniques, such as synthetic aperture methods. To verify the proposed method, we use an exemplary setup employing a 24 GHz frequency-modulated continuous-wave (CW) single-input multiple-output secondary radar with 250 MHz bandwidth. A high-precision six-axis robotic arm serves as a 3D positioning reference. The test setup emulates a realistic industrial indoor environment with significant multipath reflections. Despite the challenging conditions and the rather low bandwidth, the results show an outstanding localization 3D RMSE of around 1.7 cm. The proposed method can easily be applied to nearly any type of radio signal with CW carrier and is an attractive alternative to common multilateration and multiangulation localization approaches. We think it is a quantum leap in wireless locating, as it has the potential for precise, simple, and low-cost wireless localization even with standard narrowband communication signals.},
  keywords={Kalman filters;Covariance matrices;Transmitters;Receiving antennas;Radar;Wireless sensor networks;Wireless communication;FMCW;radar;localization;extended Kalman filter;near field;indoor},
  doi={10.1109/ACCESS.2019.2900799},
  ISSN={2169-3536},
  month={},}@ARTICLE{9866607,
  author={Fan, Ying and Fan, Chen and He, Xiaofeng and Hu, Xiaoping and Zhou, Wenzhou and Wu, Xuesong and Shang, Hang},
  journal={IEEE Sensors Journal}, 
  title={Bionic Polarized Skylight Orientation Method Based on the Model Consistency of Polarization Patterns in Cloudy Weather}, 
  year={2022},
  volume={22},
  number={20},
  pages={19455-19465},
  abstract={Skylight polarized patterns provide fresh thinking for the human to use the information sources in nature for orientation, but this orientation method is susceptible to weather conditions. Especially, in the case of cloudy weather, the cloud directly leads to the decline of orientation precision. In this article, a robust bionic polarized skylight orientation method based on the model consistency of polarization patterns (MCOPPs) is proposed for cloudy weather. First, we made full use of the DOP information to preprocess the polarization information as an aid to the subsequent polarization light orientation. Then, based on the vertical feature of the E-vector of the polarized light and the scattering plane, we developed the MCOPP error criterion and proposed a two-step optimizing framework for the effective selection of the interior points. Finally, the outdoor static rotating experiment and the dynamic vehicle experiment are conducted for evaluating our algorithm. The experimental results demonstrate that MCOPP can realize an effective and robust polarized orientation under cloudy conditions, and the accuracy of orientation in the outdoor static experiment is within 0.5° and in the dynamic car experiment is within 0.7°. Compared with the other four classic methods, the RMSE of the orientation error can be decreased by 50.6% in the outdoor static experiment and by 46.0% in the dynamic vehicle experiment. Especially, the proposed MCOPP has lower computational complexity than the existing robust polarized skylight method.},
  keywords={Meteorology;Atmospheric modeling;Cloud computing;Rayleigh scattering;Clouds;Scattering;Heuristic algorithms;Bionic polarized skylight orientation;cloudy weather;pixelated polarized vision sensor;progressive sample consensus (PROSAC)},
  doi={10.1109/JSEN.2022.3199855},
  ISSN={1558-1748},
  month={Oct},}@INPROCEEDINGS{8906929,
  author={Vordonis, Dimitris and Paliouras, Vassilis},
  booktitle={2019 IEEE Nordic Circuits and Systems Conference (NORCAS): NORCHIP and International Symposium of System-on-Chip (SoC)}, 
  title={Sphere Decoder for Massive MIMO Systems}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  abstract={The increasing demand for higher data rates and for more connected devices has led to Massive MIMO (MMIMO) Technology. The large number of antennas makes the Maximum-Likelihood (ML) detector infeasible to be implemented due to high complexity, despite its optimal performance. Sphere Decoder (SD) has a bit error rate (BER) performance similar to ML detector, therefore making it more efficient (0.5-1.25 dB gain) than Linear Detectors (LD), proposed in the literature. However, the low complexity of LD and the non-deterministic behavior of SD are the main reasons that prohibit the use of sphere decoding methods in MMIMO systems. The results of this paper disrupt conventional thinking and show that there may be a future for SD in certain MMIMO system. The number of visited nodes during detection and the Initial Radius (IR) method are crucial for the computational complexity of SD. In this paper, an effective IR method, decreasing significantly the complexity and the number of visited nodes is proposed. Furthermore an optimization at tree searching further reduces the number of visited nodes, where in combination with an implementation featured with one-node-per-cycle architecture minimize the latency and make the SD attainable to large-scale systems for $E_{b}/N_{0}\geq 4 \ \mathrm{dB}$. Hardware aspects are investigated for both a Virtex-7 FPGA and a 28 nm ASIC technology.},
  keywords={massive MIMO;sphere decoding;initial radius;MIMO detection;eigenvalue problem;norm distance},
  doi={10.1109/NORCHIP.2019.8906929},
  ISSN={},
  month={Oct},}
