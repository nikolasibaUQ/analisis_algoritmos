@INPROCEEDINGS{10589958,
  author={Sun, Yue and Xiao, Wen},
  booktitle={2024 6th International Conference on Computer Science and Technologies in Education (CSTE)}, 
  title={Analysis of Blended Learning Behaviour Based on K-Means Clustering Algorithm}, 
  year={2024},
  volume={},
  number={},
  pages={315-318},
  abstract={As blended learning models gain popularity, analyzing the online learning behaviors of college students has emerged as a crucial approach to enhancing teaching effectiveness. The objective of this study is to conduct a comprehensive analysis of the learning behavior data from 44 students specializing in computer-related subjects on the online platform of the “Introduction to Computational Thinking” course, utilizing clustering algorithms. Subsequently, we utilized the Pearson correlation coefficient method to delve into the intricate relationship between diverse behavioral traits and academic performance. Our research has revealed that cluster analysis is adept at discerning behavioral disparities among students. Specifically, we identified four types of learning groups with different learning characteristics. Furthermore, a notable correlation was observed between homework scores and key evaluation metrics, including academic performance. Based on these findings, this study proposes targeted learning strategies and teaching suggestions, aiming to help educators better guide students in learning and improve the effectiveness of blended learning.},
  keywords={Measurement;Analytical models;Correlation;Federated learning;Education;Clustering algorithms;Solids;blended learning;learning behavior analysis;clustering analysis;correlation analysis},
  doi={10.1109/CSTE62025.2024.00065},
  ISSN={},
  month={April},}@INPROCEEDINGS{9925899,
  author={Pretelli, Adele and Klopfenstein, Lorenz Cuno and Francesco, Gian Marco Di and Paolini, Brendan Dominic and Bogliolo, Alessandro},
  booktitle={2022 IEEE 2nd IoT Vertical and Topical Summit for Tourism (IoTT)}, 
  title={All Aboard: One Year of Accessible and Inclusive Remote School Trips}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={The rapid spread of the CoViD-19 pandemic in early 2020 and the adoption of social distancing and "stay-at-home" orders gave rise to the sudden need for schools to be able to provide effective remote teaching. In Italy and elsewhere, the introduction of distance learning technologies has significantly impacted the education of students and limited their social interaction and activities, among which are school trips and other scholastic experiences. To support the constrained educational possibilities of Italian schools, we developed "ActiveViewer", a technological platform that enables massive and interactive remote events, and performed a year-long large-scale experiment, providing a series of massive remote school trips to several Italian cities. We designed a new school trip format, named "CodyTrip" that merges coding and computational thinking with elements of traditional school outings, and provides an engaging experience for participants. In this paper, we present the format and five online trips performed during the school year 2020/2021. The results based on user feedback support the adoption of the tool not only as a stopgap measure for school outings throughout the pandemic but as an accessible and inclusive tool to widen the possibilities of cultural tourism.},
  keywords={COVID-19;Pandemics;Education;Urban areas;Sociology;Human factors;Particle measurements;CoViD-19;distance learning;educational technology;virtual tourism},
  doi={10.1109/IoTT56174.2022.9925899},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{7797363,
  author={Bortoletto, Antonio C. and Minami, Mario and Kurashima, Celso S.},
  booktitle={2016 IEEE International Symposium on Consumer Electronics (ISCE)}, 
  title={DSP altered feedback system for anti-stuttering applications}, 
  year={2016},
  volume={},
  number={},
  pages={5-6},
  abstract={An altered feedback system aims to help people that suffer of stuttering and speech fluency disorder. The basic therapy approach is to modify the original voice and to present it back to his/her ear. By hearing the altered signal, he/she thinks someone else is talking the same words together, which results in a natural fluent speech. Our system modifies the voice pitch frequency and reduces background noise. The output signal produces a comfortable and natural altered feedback voice. The hardware implementation with a low-cost DSP processor presents a satisfactory real-time computational performance and low energy consumption.},
  keywords={Digital signal processing;Speech;Speech processing;Power demand;Signal processing algorithms;Adaptive filters;Real-time systems;speech/voice processing;digital signal processing},
  doi={10.1109/ISCE.2016.7797363},
  ISSN={2159-1423},
  month={Sep.},}@INPROCEEDINGS{488476,
  author={Huffman, J.},
  booktitle={Proceedings of WESCON '93}, 
  title={The Zen of fuzzy logic and neural networks}, 
  year={1993},
  volume={},
  number={},
  pages={451-454},
  abstract={The modern technologies of fuzzy logic and neural networks are based on operations on truth values or degrees of class membership. They operate more closely to the manner that we currently believe humans use to think and control. Once you understand the underlying functionality you can find ways to apply the technologies to otherwise difficult to solve problems. If we take human thought and action as one baseline for intelligence, then neural nets, fuzzy logic and their hybrids can be seen as more intelligent platforms. These more intelligent tools allow users to re-frame the problem space to a larger systems view. An implementor no longer has to be concerned with the underlying physics of a system, or with control theory details. The problem space is expanded to a higher level while the lower level part is passed off to the core computational method. This paper proposes the effects of a whole new approach to problem solving enabled by neural/fuzzy technologies.},
  keywords={Fuzzy logic;Neural networks;Space technology;Problem-solving;Humans;Computer networks;Marketing and sales;Springs;Marine vehicles;Physics},
  doi={10.1109/WESCON.1993.488476},
  ISSN={1095-791X},
  month={Sep.},}@ARTICLE{9380389,
  author={Tzimpragos, Georgios and Volk, Jennifer and Vasudevan, Dilip and Tsiskaridze, Nestan and Michelogiannakis, George and Madhavan, Advait and Shalf, John and Sherwood, Timothy},
  journal={IEEE Micro}, 
  title={Temporal Computing With Superconductors}, 
  year={2021},
  volume={41},
  number={3},
  pages={71-79},
  abstract={Creating computing systems able to address our ever-increasing needs, especially as we reach the end of CMOS transistor scaling, will require truly novel methods of computing. However, the traditional logic abstractions and the digital design patterns we understand so well have coevolved with the hardware technology that has embodied them. As we look past CMOS, there is no reason to think that those same abstractions best serve to encapsulate the computational potential inherent to emerging devices. We posit that a new and radically more efficient foundation for computing lies at the intersection of superconductor electronics and delay-coded computation. Building on recent work in race logic, we show that superconducting circuits can naturally compute directly over temporal relationships between pulse arrivals; that the computational relationships between those pulse arrivals can be formalized through a functional extension to a temporal predicate logic used in the verification community; and that the resulting architectures can operate asynchronously and describe real and useful computations. We verify our hypothesis through a combination of detailed analog circuit models and layout designs, a formal analysis of our abstractions, and an evaluation of several superconducting temporal accelerators.},
  keywords={Superconducting logic circuits;Logic gates;Delays;Superconductivity;Semantics;Hardware;Josephson junctions},
  doi={10.1109/MM.2021.3066377},
  ISSN={1937-4143},
  month={May},}@INPROCEEDINGS{8658572,
  author={Lobo de Aguiar Gomes, Ludymila and Reginaldo Hughes Carvalho, José and Lauschner, Tanara and Nakamura, Fabilo G. and de Freitas, Rosiane},
  booktitle={2018 IEEE Frontiers in Education Conference (FIE)}, 
  title={Encouraging Women to Pursue a Computer Science Career in the Context of a Third World Country}, 
  year={2018},
  volume={},
  number={},
  pages={1-5},
  abstract={This innovative practice full paper presents a set of engaging actions aimed to encourage women to pursue a Computer Science career in a city of a third world country (Manaus, Brazil). Despite worldwide efforts to promote gender equality, typically, women account for less than 30% of the workforce in technological areas. In third world countries, the situation is much more unbalanced. Poor educational and economic conditions, allied with a chauvinism culture contaminated by sexism and stereotypes, are strong forces that repel the young girls from IT areas. As a result, the percentage of women in local Computer Science majors is lower than expected. The authors detail a program to involve girls from all school levels in computer science career, which is indeed the adaptation of a national program, combined with indigenous elements. The mentioned adaptation was a key success factor to catch the attention of students and local educators. Some activities that are included in this program are lectures at scientific, technological and gender discussion events, realization of dynamics in schools for the dissemination of computational thinking in children and young students, training students to take part in programming contests and develop knowledge into real computational applications. These actions resulted in highlights achieved in programming contests and prizes obtained through application development, and have provided a more conducive academic environment to discuss issues related to the female gender in science and technology fields. Besides the fundamentals of the program, the authors present the results of the last three initiatives, which happened in conjunction with local events, and the promising opportunities perceived in Computer Science major of a local university.},
  keywords={Engineering profession;Conferences;Programming profession;STEM;gender equality;diversity politics, STEM education;women inclusion and leadership},
  doi={10.1109/FIE.2018.8658572},
  ISSN={2377-634X},
  month={Oct},}@INPROCEEDINGS{1383168,
  author={Lumetta, S.S. and Krishnamurthy, A. and Culler, D.E.},
  booktitle={Supercomputing '95:Proceedings of the 1995 ACM/IEEE Conference on Supercomputing}, 
  title={Towards Modeling the Performance of a Fast Connected Components Algorithm on Parallel Machines}, 
  year={1995},
  volume={},
  number={},
  pages={32-32},
  abstract={We present and analyze a portable, high-performance algorithm for finding connected components on modern distributed memory multiprocessors. The algorithm is a hybrid of the classic DFS on the subgraph local to each processor and a variant of the Shiloach-Vishkin PRAM algorithm on the global collection of subgraphs. We implement the algorithm in Split-C and measure performance on the the Cray T3D, the Meiko CS-2, and the Thinking Machines CM-5 using a class of graphs derived from cluster dynamics methods in computational physics. On a 256 processor Cray T3D, the implementation outperforms all previous solutions by an order of magnitude. A characterization of graph parameters allows us to select graphs that highlight key performance features. We study the effects of these parameters and machine characteristics on the balance of time between the local and global phases of the algorithm and find that edge density, surface-to-volume ratio, and relative communication cost dominate performance. By understanding the effect of machine characteristics on performance, the study sheds light on the impact of improvements in computational and/or communication performance on this challenging problem.},
  keywords={Parallel machines;Clustering algorithms;Phase change random access memory;Concurrent computing;Computer science;Performance analysis;Algorithm design and analysis;Physics computing;Costs;Computer vision;performance modeling (modelling);connected components;distributed memory;parallel machines;hybrid algorithm},
  doi={10.1145/224170.224275},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9885427,
  author={Joseph, Ushus Maria and Jacob, Mendus},
  booktitle={2022 International Conference on Computing, Communication, Security and Intelligent Systems (IC3SIS)}, 
  title={Developing a Real time model to Detect SMS Phishing Attacks in Edges using BERT}, 
  year={2022},
  volume={},
  number={},
  pages={1-7},
  abstract={Phishing is the enchanting utilization of automated trades to cheat and exploit clients. Phishing attacks exertion to get intriguing, private information, for instance, usernames, passwords, charge card information, and affiliation confirmations, absolutely. By acting like a real individual or foundation through telephone or email, electronic aggressors utilize social expecting to push occurrences toward performing unequivocal activities like tapping on a perilous collusion or affiliation or wilfully uncovering private data. Nowadays, aggressors use different correspondence mediums to talk with the adversities, for instance, email, message (SMS), telephone, and others. No matter what the quick movement of Internet show based illuminating affiliations, SMS genuinely remains an obvious correspondence relationship in our lives as of in the relatively recent past. For example, a few affiliations consider that messages are more convincing than messages. This is thinking about the way that 82% of SMSs are explored inside 5 min., yet clients simply open one of each four messages they get. The significance of generally suggests irritating or unconstrained messages got by phone clients through Short Messaging Service (SMS). The SMS phishing is another strategy where the phisher works the SMS as a medium to visit with individuals being suggested and this system is seen as smishing (SMS+phishing). In any case, SMS is one of the potential instruments to really chat with others through phones without the web. As Transfer Learning from colossal degree set up models ends up being more inevitable in Natural Language Processing (NLP), working these huge models in on-the-edge as well as under obliged computational arrangement or confirmation monetary plans stays testing. Phones are famous with engineers since they're expected fast responses pondering insignificant huge information. For seeing phishing attacks in low computational contraptions we can use a quantized model. This work focuses on seeing SMS phishing attacks continuously with the help of BERT in edges. Not the slightest bit like reliable language depiction models, BERT expected to pre-train basic bidirectional depictions from unlabeled text by customarily shaping on both left and right setting in all layers.},
  keywords={},
  doi={10.1109/IC3SIS54991.2022.9885427},
  ISSN={},
  month={June},}@INPROCEEDINGS{10578908,
  author={Possaghi, Isabella and Papavlasopoulou, Sofia},
  booktitle={2024 IEEE Global Engineering Education Conference (EDUCON)}, 
  title={Challenges in Digital STEM Education Delivery: A Case Study from the Teachers' Perspective}, 
  year={2024},
  volume={},
  number={},
  pages={1-10},
  abstract={In the last decade, educational reforms introduced technologies and digital tools to a higher degree, serving as powerful catalysts in educational innovation for creating, managing, and delivering content in STEM education with a focus on computational and engineering subjects. However, in the small reality of the classroom, some discrepancies arise between policy objectives and everyday practice. The research community emphasises the importance of providing technology infusions based on educators' pedagogical beliefs as well as needs rooted in the authentic scholarly environment. Considering this perspective when orchestrating in-classroom technology can support educators in their daily experiences and enhance their digital competencies. To contribute to this aim, we propose a case study carried out via interviews with fellow primary and secondary teachers, tackling challenges in STEM (Science, Technology, Engineering, and Mathematics), including Computer Science (CS) and Computational thinking (CT) education delivery. Along with the teachers' perspectives, our study aims to obtain a detailed account of the classroom's internal dynamics as perceived by the teachers. The most prominent findings concern the disparity among educators' proficiency in technology endorsement, impacting the consistency in content delivery, the complexity of the assessment of students' learning process if it is digital-based, and the inclusivity and trustworthiness of online content. Finally, we suggest guidance for a further mindful in-school uptake of technology to support teachers in orchestrating content delivery with educational technologies and enhance their confidence in this aspect.},
  keywords={Computer science;Technological innovation;Catalysts;Educational technology;Complexity theory;Stakeholders;Interviews;Digital Literacy;STEM;Digital Education;Learning;Educational Technology;Teachers},
  doi={10.1109/EDUCON60312.2024.10578908},
  ISSN={2165-9567},
  month={May},}@ARTICLE{8932345,
  author={Oliveira, Tiago and Stringhini, Denise and Santos Craibas, José Jailson},
  journal={IEEE Latin America Transactions}, 
  title={A Practical and Systemic Curricular Approach to Teach Computer Systems}, 
  year={2019},
  volume={17},
  number={08},
  pages={1349-1362},
  abstract={The design of computational systems is an important matter that must be especially present in Computer Engineering curriculum. Traditionally, Computer Engineering curriculum address this theme in a compartmentalized way in specific curricular units. This organization creates a fragmented view of the complex computational systems development, understood here as the specification, design, implementation and integration of the lines of computer architecture and organization, systems engineering, compilers, operating systems and computer networks. To avoid this fragmented view, this article proposes a systemic and curricular approach, integrating hardware and software, which allows students to design a complex computer system over three years of their academic trajectories. This approach was validated in a Computer Engineering course at the Federal University of São Paulo, bringing positive results in relation to students' motivation and enthusiasm, as well as encouraging creative and innovative thinking.},
  keywords={Software;Hardware;Field programmable gate arrays;Unified modeling language;IEEE transactions;Organizations;Education;teaching of computer systems;curricular and systemic approach;hardware and software designs},
  doi={10.1109/TLA.2019.8932345},
  ISSN={1548-0992},
  month={August},}@INPROCEEDINGS{8965997,
  author={Jia, Jizheng and Zhao, Qiyang},
  booktitle={2019 12th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)}, 
  title={Siamese Score: Detecting Mode Collapse for GANs}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  abstract={Despite large strides in terms of generative adversarial networks (GANs) for image generation, evaluating and comparing GANs remains an open question. Several measures have been introduced, however, there is no consensus in terms of the best score. In this paper, we delve into the widely-used metric Inception Score (based on KL divergence), revealing that it fails to detect intra-class mode collapse. Meanwhile, Wasserstein distance has received much attention in comparing distributions in recent years but suffers heavy computational burden in high dimensional space. Our idea is that we can find specific embedding space where Euclidean distance could mimic Wasserstein distance to solve the heavy computational problem. This space can be found using a Siamese network, which could be trained quickly because of shared weights. We also apply several proposed new techniques to get better image embedding. To evaluate our proposed metric (Siamese Score), we simulate mode collapse using K-means clustering performed on real data set. To further validate it, we perform an empirical study on several GAN models and use the generated images to do the task. Experiments show that Siamese Score can detect mode collapse and is time-efficient compared with Inception Score and we think our score can be complementary to Inception Score.},
  keywords={},
  doi={10.1109/CISP-BMEI48845.2019.8965997},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{6972195,
  author={Fekete, Krisztián and Pelle, Ádám and Csorba, Kristóf},
  booktitle={2014 IEEE 36th International Telecommunications Energy Conference (INTELEC)}, 
  title={Energy efficient code optimization in mobile environment}, 
  year={2014},
  volume={},
  number={},
  pages={1-6},
  abstract={Nowadays the mobile devices energy consumption has become a very serious issue. Due to the fast growing mobile industry the current devices usually contain wireless (Wi-Fi, 3G, 4G and Bluetooth), GPS and other heavy network sensitive technologies. The cell battery manufacturers usually cannot keep the pace with this fast altering environment and demands, hence the devices get inappropriate battery. Recently many researchers deal with this topic trying to find out a reasonable solution for energy optimization without compromising the functionality of the devices. A possible approach to deal with the problem today and achieve the desired result is the code side optimization. The market based mobile application distribution model makes this challenge harder. Sometimes the code quality of the uploaded applications are poor and the market owners are not able to force the developers to write clear and energy efficient code, they can only give them recommendations. The question is coming up: how could we still encourage the individual developers and companies to write optimized and a bit more standardized code? The answer could be delivering them tools to facilitate the refactoring and code quality managing processes. One way we can deal with energy problem is to reorganize the heavy computational tasks out of the device to the cloud or to just another machine. This technique is called “offloading”. In this paper we are going to introduce a new code generation extension for software engineers which is aiming to automate the offload to web services. Those services can then be easily deployed to any desired location to ease the mobile devices computational task. We think this tool would be really helpful to the community and the industrial users also.},
  keywords={Optimization;Mobile communication;Visualization;Measurement;Software;Mobile handsets;Generators},
  doi={10.1109/INTLEC.2014.6972195},
  ISSN={2158-5210},
  month={Sep.},}@ARTICLE{9409047,
  author={Yahia, Nesrine Ben and Hlel, Jihen and Colomo-Palacios, Ricardo},
  journal={IEEE Access}, 
  title={From Big Data to Deep Data to Support People Analytics for Employee Attrition Prediction}, 
  year={2021},
  volume={9},
  number={},
  pages={60447-60458},
  abstract={In the era of data science and big data analytics, people analytics help organizations and their human resources (HR) managers to reduce attrition by changing the way of attracting and retaining talent. In this context, employee attrition presents a critical problem and a big risk for organizations as it affects not only their productivity but also their planning continuity. In this context, the salient contributions of this research are as follows. Firstly, we propose a people analytics approach to predict employee attrition that shifts from a big data to a deep data context by focusing on data quality instead of its quantity. In fact, this deep data-driven approach is based on a mixed method to construct a relevant employee attrition model in order to identify key employee features influencing his/her attrition. In this method, we started thinking `big' by collecting most of the common features from the literature (an exploratory research) then we tried thinking `deep' by filtering and selecting the most important features using survey and feature selection algorithms (a quantitative method). Secondly, this attrition prediction approach is based on machine, deep and ensemble learning models and is experimented on a large-sized and a medium-sized simulated human resources datasets and then a real small-sized dataset from a total of 450 responses. Our approach achieves higher accuracy (0.96, 0.98 and 0.99 respectively) for the three datasets when compared previous solutions. Finally, while rewards and payments are generally considered as the most important keys to retention, our findings indicate that `business travel', which is less common in the literature, is the leading motivator for employees and must be considered within HR policies to retention.},
  keywords={Big Data;Organizations;Radio frequency;Predictive models;Support vector machines;Data models;Analytical models;Deep people analytics;employee attrition;retention;prediction;interpretation;policies recommendation},
  doi={10.1109/ACCESS.2021.3074559},
  ISSN={2169-3536},
  month={},}@ARTICLE{9940938,
  author={Wang, Ziyuan and Wang, Li},
  journal={IEEE Access}, 
  title={An Attention Approach for Dimensionality Reduction That Can Correct Feature Collection Skewness}, 
  year={2022},
  volume={10},
  number={},
  pages={117273-117280},
  abstract={It has been demonstrated that adding an attention mechanism to a convolutional neural network enhances network performance. However, in practice, the skewness error during the extraction process affects the feature information owing to the implementation of the global average pooling operation, which eventually lowers the performance of the network design. We think that when the weight is activated, the spatial information is effectively captured while the extraction range of the channel information is narrowed. We also think that because the weight is activated using multiple local extraction substitutions, the influence of skewness error can be effectively reduced. As a result, we suggest dimensionality reduction attention(RA) in this study as a new type of attention mechanism. Dimensionality reduction attention is a method of combining spatial information and channel information into a new feature distribution by aggregating dimensionality reduction of feature information, in contrast to other attentions that act on distinct feature tensors in space and channel. Under the premise of capturing long-distance context information and guaranteeing feature coordinates, it may realize the locality of spatial information through this operation and identify the local information of each channel. To completely display the important feature information, the produced feature maps are then encoded into two complimentary attention maps along the spatial and channel directions, respectively. Our dimension reduction focus is a straightforward and all-encompassing module. We run tests on various datasets using various deep architectures and various class designs. The outcomes of the trial demonstrate that our attention-based approach has clear benefits.},
  keywords={Feature extraction;Dimensionality reduction;Tensors;Convolutional neural networks;Convolution;Three-dimensional displays;Convolutional neural network;attention mechanism;feature extraction network},
  doi={10.1109/ACCESS.2022.3220245},
  ISSN={2169-3536},
  month={},}@ARTICLE{8762113,
  author={Dey, Niladri Sekhar and Gunasekhar, T.},
  journal={IEEE Access}, 
  title={A Comprehensive Survey of Load Balancing Strategies Using Hadoop Queue Scheduling and Virtual Machine Migration}, 
  year={2019},
  volume={7},
  number={},
  pages={92259-92284},
  abstract={The recent growth in the demand for scalable applications from the consumers of the services has motivated the application development community to build and deploy the applications on cloud in the form of services. The deployed applications have significant dependency on the infrastructure available with the application providers. Bounded by the limitations of available resource pools on-premises, many application development companies have migrated the applications to third party cloud environments called data centers. The data center owners or the cloud service providers are entitled to ensure high performance and high availability of the applications and at the same time the desired scalability for the applications. Also, the cloud service providers are also challenging in terms of cost reduction and energy consumption reductions for better manageability of the data center without degrading the performance of the deployed applications. It is to be noted that the performance of the application does not only depend on the responsiveness of the applications rather also must be measured in terms of service level agreements. The violation of the service level agreements or SLA can easily disprove the purpose of application deployments on cloud-based data centers. Thus, the data center owners apply multiple load balancing strategies for maintaining the desired outcomes from the application owners at the minimized cost of data center maintainability. Hence, the demand of the research is to thoroughly study and identify the scopes for improvements in the parallel research outcomes. As the number of applications ranging from small data-centric applications coming with the demand of frequent updates with higher computational capabilities to the big data-centric application as big data analytics applications coming with efficient algorithms for data and computation load managements, the data center owners are forced to think for efficient algorithms for load managements. The algorithms presented by various research attempts have engrossed on application specific demands for load balancing using virtual machine migrations and the solution as the proposed algorithms have become application problem specific. Henceforth, the further demand of the research is a guideline for selecting the appropriate load balancing algorithm via virtual machine migration for characteristics-based specific applications. Hence, this paper presents a comprehensive survey on existing virtual machine migration and selection processes to understand the specific application-oriented capabilities of these strategies with the advantages and bottlenecks. Also, with the understanding of the existing measures for load balancing, it is also important to furnish the further improvement strategies, which can be made possible with a detailed understanding of the parallel research outcomes. Henceforth, this paper also equips the study with guidelines for improvements and for further study. Nonetheless, the study cannot be completed without the mathematical analysis for better understanding and experimental analysis on different standards of datasets for better conclusive decisions. Hence, this paper also presents the discussion on mathematical models and experimental result analysis for the conclusive decision on the improvement factors and the usability of the migration methods for various purposes. Finally, this paper is a comprehensive survey on the background of the research, recent research outcomes using mathematical modeling and experimental studies on various available datasets, and finally identify the scopes of improvements considering various aspects such as execution time, mean time before a VM migration, mean time before a host shutdown, number of node shutdowns, SLA performance degradation, VM migrations, and energy consumption.},
  keywords={Task analysis;Load management;Cloud computing;Data centers;Resource management;Virtual machining;Indexes;Data center;load balancing;task scheduler;FIFO;FAIR;capacity;hybrid;LATE;SAMR;context-aware;threshold;IQR;LR;MAD;LRR;THR;VM consolidation;VM migration;MC;MMT;RS;MU;PlanetLab;metric;VM migration analysis;energy consumption analysis;SLA analysis},
  doi={10.1109/ACCESS.2019.2927076},
  ISSN={2169-3536},
  month={},}@ARTICLE{10325513,
  author={Simonetti, Marco and Perri, Damiano and Gervasi, Osvaldo},
  journal={IEEE Access}, 
  title={Variational Methods in Optical Quantum Machine Learning}, 
  year={2023},
  volume={11},
  number={},
  pages={131394-131408},
  abstract={The computing world is rapidly evolving and advancing, with new ground-breaking technologies emerging. Quantum Computing and Quantum Machine Learning have opened up new possibilities, providing unprecedented computational power and problem-solving capabilities while offering a deeper understanding of complex systems. Our research proposes new variational methods based on a deep learning system based on an optical quantum neural network applied to Machine Learning models for point classification. As a case study, we considered the binary classification of points belonging to a certain geometric pattern (the Two-Moons Classification problem) on a plane. We think it is reasonable to expect benefits from using hybrid deep learning systems (classical + quantum), not just in terms of accelerating computation but also in understanding the underlying phenomena and mechanisms. This will result in the development of new machine-learning paradigms and a significant advancement in the field of quantum computation. The selected dataset is a set of 2D points creating two interleaved semicircles and is based on a 2D binary classification generator, which aids in evaluating the performance of particular methods. The two coordinates of each unique point,  $x_{1}$  and  $x_{2}$ , serve as the features since they present two disparate data sets in a two-dimensional representation space. The goal was to create a quantum deep neural network that could recognise and categorise points accurately with the fewest trainable parameters possible.},
  keywords={Quantum computing;Quantum mechanics;Logic gates;Computers;Nonlinear optics;Optical network units;Deep learning;Feedforward neural networks;Neural networks;Quantum computing;variational methods;deep learning;quantum feed-forward neural networks;optical quantum computing},
  doi={10.1109/ACCESS.2023.3335625},
  ISSN={2169-3536},
  month={},}@ARTICLE{8732419,
  author={Rappaport, Theodore S. and Xing, Yunchou and Kanhere, Ojas and Ju, Shihao and Madanayake, Arjuna and Mandal, Soumyajit and Alkhateeb, Ahmed and Trichopoulos, Georgios C.},
  journal={IEEE Access}, 
  title={Wireless Communications and Applications Above 100 GHz: Opportunities and Challenges for 6G and Beyond}, 
  year={2019},
  volume={7},
  number={},
  pages={78729-78757},
  abstract={Frequencies from 100 GHz to 3 THz are promising bands for the next generation of wireless communication systems because of the wide swaths of unused and unexplored spectrum. These frequencies also offer the potential for revolutionary applications that will be made possible by new thinking, and advances in devices, circuits, software, signal processing, and systems. This paper describes many of the technical challenges and opportunities for wireless communication and sensing applications above 100 GHz, and presents a number of promising discoveries, novel approaches, and recent results that will aid in the development and implementation of the sixth generation (6G) of wireless networks, and beyond. This paper shows recent regulatory and standard body rulings that are anticipating wireless products and services above 100 GHz and illustrates the viability of wireless cognition, hyper-accurate position location, sensing, and imaging. This paper also presents approaches and results that show how long distance mobile communications will be supported to above 800 GHz since the antenna gains are able to overcome air-induced attenuation, and present methods that reduce the computational complexity and simplify the signal processing used in adaptive antenna arrays, by exploiting the Special Theory of Relativity to create a cone of silence in over-sampled antenna arrays that improve performance for digital phased array antennas. Also, new results that give insights into power efficient beam steering algorithms, and new propagation and partition loss models above 100 GHz are given, and promising imaging, array processing, and position location results are presented. The implementation of spatial consistency at THz frequencies, an important component of channel modeling that considers minute changes and correlations over space, is also discussed. This paper offers the first in-depth look at the vast applications of THz wireless products and applications and provides approaches for how to reduce power and increase performance across several problem domains, giving early evidence that THz techniques are compelling and available for future wireless communications.},
  keywords={Wireless communication;Wireless sensor networks;Antenna arrays;Bandwidth;Communication system security;Cognition;Imaging;mmWave;millimeter wave;5G;D-band;6G;channel sounder;propagation measurements;Terahertz (THz);array processing;imaging;scattering theory;cone of silence;digital phased arrays;digital beamformer;signal processing for THz;position location;channel modeling;THz applications;wireless cognition;network offloading},
  doi={10.1109/ACCESS.2019.2921522},
  ISSN={2169-3536},
  month={},}@ARTICLE{539739,
  author={Nieuwejaar, N. and Kotz, D. and Purakayastha, A. and Sclatter Ellis, C. and Best, M.L.},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={File-access characteristics of parallel scientific workloads}, 
  year={1996},
  volume={7},
  number={10},
  pages={1075-1089},
  abstract={Phenomenal improvements in the computational performance of multiprocessors have not been matched by comparable gains in I/O system performance. This imbalance has resulted in I/O becoming a significant bottleneck for many scientific applications. One key to overcoming this bottleneck is improving the performance of multiprocessor file systems. The design of a high-performance multiprocessor file system requires a comprehensive understanding of the expected workload. Unfortunately, until recently, no general workload studies of multiprocessor file systems have been conducted. The goal of the CHARISMA project was to remedy this problem by characterizing the behavior of several production workloads, on different machines, at the level of individual reads and writes. The first set of results from the CHARISMA project describe the workloads observed on an Intel iPSC/860 and a Thinking Machines CM-5. This paper is intended to compare and contrast these two workloads for an understanding of their essential similarities and differences, isolating common trends and platform-dependent variances. Using this comparison, we are able to gain more insight into the general principles that should guide multiprocessor file-system design.},
  keywords={File systems;Application software;Concurrent computing;Computer Society;Performance gain;Production;Student members;System performance;Scientific computing;Supercomputers},
  doi={10.1109/71.539739},
  ISSN={1558-2183},
  month={Oct},}@ARTICLE{6949509,
  author={Cummings, Mary Missy},
  journal={IEEE Intelligent Systems}, 
  title={Man versus Machine or Man + Machine?}, 
  year={2014},
  volume={29},
  number={5},
  pages={62-69},
  abstract={Allocating roles and functions between the human and computer is critical in defining efficient and effective system architectures. However, past methodologies for balancing the roles and functionalities between humans and computers in complex systems have little connection to different types of required cognition, behaviors, or tasks, or don't address the role of uncertainty in the environment. To augment these previous role allocation approaches, this article presents a modification to the skill, rule, and knowledge-based behavior taxonomy that includes expertise and uncertainty. Skill-based behaviors are the best candidates for automation, assuming significant sensor performance assumptions can be met, but rule and knowledge-based reasoning are better suited for human-computer collaboration. Such systems should be designed so that humans harness the raw computational and search power of computers for state-space reduction, but also allow them the latitude to apply their expertise in uncertain situations through inductive reasoning for potentially creative, out-of-the-box thinking.},
  keywords={Human computer interaction;Automation;Human factors;Man machine systems;Resource management;Information processing;automation;computer-supported collaborative work;systems analysis and design;interactive systems;intelligent systems},
  doi={10.1109/MIS.2014.87},
  ISSN={1941-1294},
  month={Sep.},}@ARTICLE{642945,
  author={Ramaswamy, S. and Sapatnekar, S. and Banerjee, P.},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={A framework for exploiting task and data parallelism on distributed memory multicomputers}, 
  year={1997},
  volume={8},
  number={11},
  pages={1098-1116},
  abstract={Distributed Memory Multicomputers (DMMs), such as the IBM SP-2, the Intel Paragon, and the Thinking Machines CM-5, offer significant advantages over shared memory multiprocessors in terms of cost and scalability. Unfortunately, the utilization of all the available computational power in these machines involves a tremendous programming effort on the part of users, which creates a need for sophisticated compiler and run-time support for distributed memory machines. In this paper, we explore a new compiler optimization for regular scientific applications-the simultaneous exploitation of task and data parallelism. Our optimization is implemented as part of the PARADIGM HPF compiler framework we have developed. The intuitive idea behind the optimization is the use of task parallelism to control the degree of data parallelism of individual tasks. The reason this provides increased performance is that data parallelism provides diminishing returns as the number of processors used is increased. By controlling the number of processors used for each data parallel task in an application and by concurrently executing these tasks, we make program execution more efficient and, therefore, faster.},
  keywords={Parallel processing;Data structures;Random access memory;Optimizing compilers;Costs;Scalability;Runtime;Concurrent computing;Distributed computing;Program processors},
  doi={10.1109/71.642945},
  ISSN={1558-2183},
  month={Nov},}@ARTICLE{7562319,
  author={Yang, Xin-She and Deb, Suash and Fong, Simon and He, Xingshi and Zhao, Yu-Xin},
  journal={Computer}, 
  title={From Swarm Intelligence to Metaheuristics: Nature-Inspired Optimization Algorithms}, 
  year={2016},
  volume={49},
  number={9},
  pages={52-59},
  abstract={Nature has provided rich models for computational problem solving, including optimizations based on the swarm intelligence exhibited by fireflies, bats, and ants. These models can stimulate computer scientists to think nontraditionally in creating tools to address application design challenges.},
  keywords={Particle swarm optimization;Metaheuristics;Molecular computing;swarm intelligence;metaheuristics;nature-inspired computing;emerging computing paradigms},
  doi={10.1109/MC.2016.292},
  ISSN={1558-0814},
  month={Sep.},}@ARTICLE{10194240,
  author={Wang, Xingxia and Yang, Jing and Wang, Yutong and Miao, Qinghai and Wang, Fei-Yue and Zhao, Aijun and Deng, Jian-Ling and Li, Lingxi and Na, Xiaoxiang and Vlacic, Ljubo},
  journal={IEEE/CAA Journal of Automatica Sinica}, 
  title={Steps Toward Industry 5.0: Building “6S” Parallel Industries With Cyber-Physical-Social Intelligence}, 
  year={2023},
  volume={10},
  number={8},
  pages={1692-1703},
  abstract={Very recently, intensive discussions and studies on Industry 5.0 have sprung up and caused the attention of researchers, entrepreneurs, and policymakers from various sectors around the world. However, there is no consensus on why and what is Industry 5.0 yet. In this paper, we define Industry 5.0 from its philosophical and historical origin and evolution, emphasize its new thinking on virtual-real duality and human-machine interaction, and introduce its new theory and technology based on parallel intelligence (PI), artificial societies, computational experiments, and parallel execution (the ACP method), and cyber-physical-social systems (CPSS). Case studies and applications of Industry 5.0 over the last decade have been briefly summarized and analyzed with suggestions for its future development. We believe that Industry 5.0 of virtual-real interactive parallel industries has great potentials and is critical for building smart societies. Steps are outlined to ensure a roadmap that would lead to a smooth transition from CPS-based Industry 4.0 to CPSS-based Industry 5.0 for a better world which is Safe in physical spaces, Secure in cyberspaces, Sustainable in ecology, Sensitive in indi-vidual privacy and rights, Service for all, and Smartness of all.},
  keywords={Industries;Privacy;Service robots;Robot kinematics;Buildings;Smart contracts;Transportation;ACP;artificial intelligence;CPS;CPSS;Industry 4.0;Industry 5.0;parallel industries;parallel intelligence},
  doi={10.1109/JAS.2023.123753},
  ISSN={2329-9274},
  month={August},}@ARTICLE{5009467,
  author={Gertner, Izidor and Shamash, Moshe},
  journal={IEEE Transactions on Computers}, 
  title={VLSI Architectures for Multidimensional Fourier Transform Processing}, 
  year={1987},
  volume={C-36},
  number={11},
  pages={1265-1274},
  abstract={It is often desirable in modern signal processing applications to perform two-dimensional or three-dimensional Fourier transforms. Until the advent of VLSI it was not possible to think about one chip implementation of such processes. In this paper several methods for implementing the multidimensional Fourier transform together with the VLSI computational model are reviewed and discussed. We show that the lower bound for the computation of the multidimensional transform is O(n2 log2 n). Existing nonoptimal architectures suitable for implementing the 2-D transform, the RAM array transposer, mesh connected systolic array, and the linear systolic matrix vector multiplier are discussed for area time tradeoff. For achieving a higher degree of concurrency we suggest the use of rotators for permutation of data. With ``hybrid designs'' comprised of a rotator and one-dimensional arrays which compute the one-dimensional Fourier transform we propose two methods for implementation of multidimensional Fourier transform. One design uses the perfect shuffle for rotations and achieves an AT2p of O(n2 log2 n· log2 N). An optimal architecture for calculation of multidimensional Fourier transform is proposed in this paper. It is based on arrays of processors computing one-dimensional Fourier transforms and a rotation network or rotation array. This architecture realizes the AT2p lower bound for the multidimensional FT processing.},
  keywords={Computer architecture;Arrays;Transforms;Discrete Fourier transforms;Random access memory;Fourier transforms;Microprocessors;Cube connected cycles;hybrid designs;mesh connected systolic array;multidimensional Fourier transform;necklaces;optimal architecture;perfect shuffle;RAM array transposer;rotation network;VLSI complexity},
  doi={10.1109/TC.1987.5009467},
  ISSN={1557-9956},
  month={Nov},}@ARTICLE{4135849,
  author={Snyder, Richard V.},
  journal={IEEE Microwave Magazine}, 
  title={Practical aspects of microwave filter development}, 
  year={2007},
  volume={8},
  number={2},
  pages={42-54},
  abstract={Design of practical filters involves consideration of a large variety of disciplines and factors. These include the electrical, physical, and economic properties of resonant and coupled elements, the materials and processes used for fabrication (properties and cost factors), and the labor cost associated with assembly and adjustment. Modeling is a vital asset in the design process, but the real properties of filter elements must be incorporated into the modeling process, using an evolutional method in which the model is adjusted to compensate for the unavoidable nonideal nature of the elements, stray couplings, and the like. This is similar to the older, laboratory "cut and try" method, but far less costly and time-consuming. This implies that modeling should accept available measured data as input. Designers should always think about minimizing production labor, considering manufacturing tolerances possibly as a trade-off against tuning time and recognizing that availability of skilled labor is more than simply a cost but rather a constraint on delivery rate. Continued development of accurate and complete models of parts, enclosures, and interconnects, in conjunction with ever-better computational capabilities, could (if properly used) enable rapid and accurate designs of filters with very predictable and producible results},
  keywords={Microwave filters;Costs;Economic forecasting;Resonance;Fabrication;Assembly;Process design;Laboratories;Production;Manufacturing},
  doi={10.1109/MMW.2007.335528},
  ISSN={1557-9581},
  month={April},}@ARTICLE{7971992,
  author={Burleson, Winslow S. and Harlow, Danielle B. and Nilsen, Katherine J. and Perlin, Ken and Freed, Natalie and Jensen, Camilla Nørgaard and Lahey, Byron and Lu, Patrick and Muldner, Kasia},
  journal={IEEE Transactions on Learning Technologies}, 
  title={Active Learning Environments with Robotic Tangibles: Children's Physical and Virtual Spatial Programming Experiences}, 
  year={2018},
  volume={11},
  number={1},
  pages={96-106},
  abstract={As computational thinking becomes increasingly important for children to learn, we must develop interfaces that leverage the ways that young children learn to provide opportunities for them to develop these skills. Active Learning Environments with Robotic Tangibles (ALERT) and Robopad, an analogous on-screen virtual spatial programming environment for educational Human Robot Interaction (HRI), have been developed. Evaluations of these in the context of free play and open-ended learning activities show that both systems afford opportunities for young children to engage in spatial programming, creating improvisational and sequential programs that mediate interactions between the environment, robots, and humans in responsive and creative ways. These systems demonstrate innovative opportunities for advancing mixed reality spatial programming activities as a form of HRI that fosters engaging seamless cyberlearning experiences, across formal and informal environments.},
  keywords={Robots;Computers;Electronic mail;Programming profession;Education;Context;Computers and education;human-computer interaction;robotics},
  doi={10.1109/TLT.2017.2724031},
  ISSN={1939-1382},
  month={Jan},}
