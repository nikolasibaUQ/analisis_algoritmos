@article{doi:10.1002/per.2270,
 abstract = {We present a new method for personality assessment at a distance to uncover personality structure in historical texts. We focus on how two 19th century authors understood and described human personality; we apply a new bottom–up computational approach to extract personality dimensions used by Jane Austen and Charles Dickens to describe fictional characters in 21 novels. We matched personality descriptions using three person–description dictionaries marker scales as reference points for interpretation. Factor structures did not show strong convergence with the contemporary Big Five model. Jane Austen described characters in terms of social and emotional richness with greater nuances but using a less extensive vocabulary. Charles Dickens, in contrast, used a rich and diverse personality vocabulary, but those descriptions centred around more restricted dimensions of power and dominance. Although we could identify conceptually similar factors across the two authors, analyses of the overlapping vocabulary between the two authors suggested only moderate convergence. We discuss the utility and potential of automated text analysis and the lexical hypothesis to (i) provide insights into implicit personality models in historical texts and (ii) bridge the divide between idiographic and nomothetic perspectives. © 2020 European Association of Personality Psychology},
 author = {Ronald Fischer and Johannes Alfons Karl and Markus Luczak–Roesch and Velichko H. Fetvadjiev and Adam Grener},
 doi = {10.1002/per.2270},
 eprint = {https://doi-org.crai.referencistas.com/10.1002/per.2270},
 journal = {European Journal of Personality},
 number = {5},
 pages = {917–943},
 title = {Tracing Personality Structure in Narratives: A Computational Bottom–Up Approach to Unpack Writers, Characters, and Personality in Historical Context},
 url = {https://doi-org.crai.referencistas.com/10.1002/per.2270},
 volume = {34},
 year = {2020e}
}

@article{doi:10.1037/1089-2680.10.2.113,
 abstract = {Studies of scientific and technological thinking can be organized via a taxonomy of methodological approaches that reveal areas for further study. Dunbar provides such a taxonomy, using biological methods like in vivo and in vitro as examples. In vitro corresponds to laboratory simulations of scientific thinking; In Vivo corresponds to field studies of actual scientists and inventors. Dunbar expands this taxonomy to incorporate historical studies of scientists and inventors, computer simulation and the possibility of neurological studies. These methods can and should be combined: Computational simulations are frequently based on historical case-studies, for example. In this article, a wide range of studies are classified according to this taxonomy, and their main points summarized. Consideration is also given to studies of scientific and technological thinking in collaborative dyads and teams. The article concludes with suggestions for future research.},
 author = {Michael E. Gorman},
 doi = {10.1037/1089-2680.10.2.113},
 eprint = {https://doi-org.crai.referencistas.com/10.1037/1089-2680.10.2.113},
 journal = {Review of General Psychology},
 number = {2},
 pages = {113–129},
 title = {Scientific and Technological Thinking},
 url = {https://doi-org.crai.referencistas.com/10.1037/1089-2680.10.2.113},
 volume = {10},
 year = {2006d}
}

@article{doi:10.1037/a0013134,
 abstract = {Based on Unconscious Thought Theory (UTT) and a series of experimental and correlational studies, Dijksterhuis and his colleagues conclude that when making complex choices/decisions, conscious thought—deliberation while attention is directed at the problem—leads to poorer choices/decisions than “unconscious thought”—deliberation in the absence of conscious attention directed at the problem. UTT comprises six principles said to apply to decision making, impression formation, attitude formation and change, problem solving, and creativity. Because the implications of UTT for psychological research and theory are considerable, the authors critically examined these six principles (and the studies used to support them) in light of the extant scholarship on unconscious processes, memory, attention, and social cognition. Our examination reveals that UTT is a theory of the unconscious that fails to take into account important work in cognitive psychology, particularly in the judgment and decision making area. Moreover, established literatures in social psychology that contradict fundamental tenets of UTT and its empirical basis are ignored. The authors conclude that theoretical and experimental deficiencies undermine the claims of the superiority of unconscious thinking as portrayed by UTT.},
 author = {Claudia González-Vallejo and G. Daniel Lassiter and Francis S. Bellezza and Matthew J. Lindberg},
 doi = {10.1037/a0013134},
 eprint = {https://doi-org.crai.referencistas.com/10.1037/a0013134},
 journal = {Review of General Psychology},
 number = {3},
 pages = {282–296},
 title = {“Save Angels Perhaps”: A Critical Examination of Unconscious Thought Theory and the Deliberation-Without-Attention Effect},
 url = {https://doi-org.crai.referencistas.com/10.1037/a0013134},
 volume = {12},
 year = {2008h}
}

@article{doi:10.1037/a0032803,
 abstract = {This article integrates two topics usually considered disciplines apart, namely, creativity and free will. In particular, creative thoughts are conceived as acts of free will. This integration begins by reviewing recent advances in a specific two-stage theory of creative problem solving, namely blind variation and selective retention (BVSR). After discussing the parallel two-stage theory of free will (chance then choice), both two-stage theories are then integrated into a single formal representation entailing choice initial probabilities, final utilities, and prior knowledge values. These three parameters are used to define the creativity of any given solution and the “sightedness” of any generated thought or choice. Both creativity and free will vanish as sightedness increases, but their relation to blindness is more complex, yielding a triangular joint distribution that mandates a second-stage selection or decision process. In addition, to accommodate the need to create choices actively rather than just decide among given choices, the treatment expands to encompass both thoughts and choices as combinatorial products. This extension connects the discussion of free will with both combinatorial models of creativity and the research on the factors that enable a person to engage in free combinatorial processes. The article closes with suggestions of future empirical and theoretical research with respect to psychology, philosophy, and potential future exchanges between the two disciplines.},
 author = {Dean Keith Simonton},
 doi = {10.1037/a0032803},
 eprint = {https://doi-org.crai.referencistas.com/10.1037/a0032803},
 journal = {Review of General Psychology},
 number = {4},
 pages = {374–383},
 title = {Creative Thoughts as Acts of Free Will: A Two-Stage Formal Integration},
 url = {https://doi-org.crai.referencistas.com/10.1037/a0032803},
 volume = {17},
 year = {2013p}
}

@article{doi:10.1037/a0032947,
 abstract = {This article describes PSI theory, which is a formalized computational architecture of human psychological processes. In contrast to other existing theories, PSI theory not only models cognitive, but also motivational and emotional processes and their interactions. The article starts with a brief overview of the theory showing the connections between its different parts. We then discuss the theory’s components in greater detail. Key constructs and processes are the five basic human needs, the satisfaction of needs using the cognitive system, including perception, schemas in memory, planning, and action. Furthermore, emotions are defined and the role of emotions in cognitive and motivational processes is elaborated, referring to a specific example. The neural basis of the PSI theory is also highlighted referring to the “quad structure,” to specific brain areas, and to thinking as scanning in a neural network. Finally, some evidence for the validity of the theory is provided.},
 author = {Dietrich Dörner and C. Dominik Güss},
 doi = {10.1037/a0032947},
 eprint = {https://doi-org.crai.referencistas.com/10.1037/a0032947},
 journal = {Review of General Psychology},
 number = {3},
 pages = {297–317},
 title = {PSI: A Computational Architecture of Cognition, Motivation, and Emotion},
 url = {https://doi-org.crai.referencistas.com/10.1037/a0032947},
 volume = {17},
 year = {2013f}
}

@article{doi:10.1037/gpr0000050,
 abstract = {The prefix meta is popular in psychology as well as in other sciences oriented on the investigation of psychology. In the first case the usage is quite simple: every term is “the sum” of meta and usual psychological concept. In the second case, the situation is much more complicated. Sometimes, it is related to transition on a higher level of abstraction, from first-order to second-order investigations. Why is such a transition necessary and is it useful for scientific progress? The comparative analysis of the meta-approach to different sciences shows both specific peculiarities (reasons for application of the prefix, methods of manipulation and investigation, main objects of analysis, goals raised, and status of metaresearcher) and common features and regularities that may be discovered only in an interdisciplinary context. In this special section, we suggest a new way of application of meta-approach to psychology—in terms of metathinking. Our main idea is as follows: The wide circulation of meta-approach to psychology with necessity leads to serious changes in thinking of both researchers in the field of psychology and in sciences oriented on investigation of psychology.},
 author = {Ilya E. Garber and Steven E. Wallis},
 doi = {10.1037/gpr0000050},
 eprint = {https://doi-org.crai.referencistas.com/10.1037/gpr0000050},
 journal = {Review of General Psychology},
 number = {3},
 pages = {329–333},
 title = {Transformation of Psychology: From Thinking/Thought to Metathinking/Metathought},
 url = {https://doi-org.crai.referencistas.com/10.1037/gpr0000050},
 volume = {19},
 year = {2015g}
}

@article{doi:10.1038/jcbfm.2009.231,
 abstract = {The brain’s energy supply determines its information processing power, and generates functional imaging signals, which are often assumed to reflect principal neuron spiking. Using measured cellular properties, we analysed how energy expenditure relates to neural computation in the cerebellar cortex. Most energy is used on information processing by non-principal neurons: Purkinje cells use only 18% of the signalling energy. Excitatory neurons use 73% and inhibitory neurons 27% of the energy. Despite markedly different computational architectures, the granular and molecular layers consume approximately the same energy. The blood vessel area supplying glucose and O2 is spatially matched to energy consumption. The energy cost of storing motor information in the cerebellum was also estimated.},
 author = {Clare Howarth and Claire M Peppiatt-Wildman and David Attwell},
 doi = {10.1038/jcbfm.2009.231},
 eprint = {https://doi-org.crai.referencistas.com/10.1038/jcbfm.2009.231},
 journal = {Journal of Cerebral Blood Flow & Metabolism},
 note = {PMID:19888288},
 number = {2},
 pages = {403–414},
 title = {The Energy Use Associated with Neural Computation in the Cerebellum},
 url = {https://doi-org.crai.referencistas.com/10.1038/jcbfm.2009.231},
 volume = {30},
 year = {2010h}
}

@article{doi:10.1038/jcbfm.2010.107,
 abstract = {We investigate metabolic interactions between astrocytes and GABAergic neurons at steady states corresponding to different activity levels using a six-compartment model and a new methodology based on Bayesian statistics. Many questions about the energetics of inhibition are still waiting for definite answers, including the role of glutamine and lactate effluxed by astrocytes as precursors for γ-aminobutyric acid (GABA), and whether metabolic coupling applies to the inhibitory neurotransmitter GABA. Our identification and quantification of metabolic pathways describing the interaction between GABAergic neurons and astrocytes in connection with the release of GABA makes a contribution to this important problem. Lactate released by astrocytes and its neuronal uptake is found to be coupled with neuronal activity, unlike glucose consumption, suggesting that in astrocytes, the metabolism of GABA does not require increased glycolytic activity. Negligible glutamine trafficking between the two cell types at steady state questions glutamine as a precursor of GABA, not excluding glutamine cycling as a transient dynamic phenomenon, or a prominent role of GABA reuptake. Redox balance is proposed as an explanation for elevated oxidative phosphorylation and adenosine triphosphate hydrolysis in astrocytes, decoupled from energy requirements.},
 author = {Rossana Occhipinti and Erkki Somersalo and Daniela Calvetti},
 doi = {10.1038/jcbfm.2010.107},
 eprint = {https://doi-org.crai.referencistas.com/10.1038/jcbfm.2010.107},
 journal = {Journal of Cerebral Blood Flow & Metabolism},
 note = {PMID:20664615},
 number = {11},
 pages = {1834–1846},
 title = {Energetics of Inhibition: Insights with a Computational Model of the Human GABAergic Neuron–Astrocyte Cellular Complex},
 url = {https://doi-org.crai.referencistas.com/10.1038/jcbfm.2010.107},
 volume = {30},
 year = {2010o}
}

@article{doi:10.1038/jcbfm.2012.35,
 abstract = {The brain’s energy supply determines its information processing power, and generates functional imaging signals. The energy use on the different subcellular processes underlying neural information processing has been estimated previously for the grey matter of the cerebral and cerebellar cortex. However, these estimates need reevaluating following recent work demonstrating that action potentials in mammalian neurons are much more energy efficient than was previously thought. Using this new knowledge, this paper provides revised estimates for the energy expenditure on neural computation in a simple model for the cerebral cortex and a detailed model of the cerebellar cortex. In cerebral cortex, most signaling energy (50%) is used on postsynaptic glutamate receptors, 21% is used on action potentials, 20% on resting potentials, 5% on presynaptic transmitter release, and 4% on transmitter recycling. In the cerebellar cortex, excitatory neurons use 75% and inhibitory neurons 25% of the signaling energy, and most energy is used on information processing by non-principal neurons: Purkinje cells use only 15% of the signaling energy. The majority of cerebellar signaling energy use is on the maintenance of resting potentials (54%) and postsynaptic receptors (22%), while action potentials account for only 17% of the signaling energy use.},
 author = {Clare Howarth and Padraig Gleeson and David Attwell},
 doi = {10.1038/jcbfm.2012.35},
 eprint = {https://doi-org.crai.referencistas.com/10.1038/jcbfm.2012.35},
 journal = {Journal of Cerebral Blood Flow & Metabolism},
 note = {PMID:22434069},
 number = {7},
 pages = {1222–1232},
 title = {Updated Energy Budgets for Neural Computation in the Neocortex and Cerebellum},
 url = {https://doi-org.crai.referencistas.com/10.1038/jcbfm.2012.35},
 volume = {32},
 year = {2012j}
}

@article{doi:10.1057/ivs.2009.27,
 abstract = {At the core of successful visual analytics systems are computational techniques that transform data into concise, human comprehensible visual representations. The general process often requires multiple transformation steps before a final visual representation is generated. This article characterizes the complex raw data to be analyzed and then describes two different sets of transformations and representations. The first set transforms the raw data into more concise representations that improve the performance of sophisticated computational methods. The second transforms internal representations into visual representations that provide the most benefit to an interactive user. The end result is a computing system that enhances an end user’s analytic process with effective visual representations and interactive techniques. While progress has been made on improving data transformations and representations, there is substantial room for improvement.},
 author = {David J. Kasik and David Ebert and Guy Lebanon and Haesun Park and William M. Pottenger},
 doi = {10.1057/ivs.2009.27},
 eprint = {https://doi-org.crai.referencistas.com/10.1057/ivs.2009.27},
 journal = {Information Visualization},
 number = {4},
 pages = {275–285},
 title = {Data Transformations and Representations for Computation and Visualization},
 url = {https://doi-org.crai.referencistas.com/10.1057/ivs.2009.27},
 volume = {8},
 year = {2009j}
}

@article{doi:10.1057/ivs.2010.8,
 abstract = {The study, analysis and understanding of natural processes are difficult tasks considering the complex nature of such processes. In this respect, the visual analysis of such systems can be of great help in the understanding of their behaviour. The increasing power of modern computers enables novel possible uses of computer graphics for such tasks. Previous work introduced systemic computation, a new model of computation and corresponding computer architecture aiming at enabling a clear formalism of natural and complex systems and providing tools for their analysis. Here, we present an online visualisation of dynamic systems based on this novel paradigm. The observation is done at a high level of abstraction, focussing on information flow, interactions and emergent behaviour, and enabling the identification of similarities and differences between models of complex systems. This visualisation framework is then applied to two biological networks: a bistable gene network and a MAPK signalling cascade.},
 author = {Erwan Le Martelot and Peter J. Bentley},
 doi = {10.1057/ivs.2010.8},
 eprint = {https://doi-org.crai.referencistas.com/10.1057/ivs.2010.8},
 journal = {Information Visualization},
 number = {1},
 pages = {1–31},
 title = {Novel Visualisation and Analysis of Natural and Complex Systems Using Systemic Computation},
 url = {https://doi-org.crai.referencistas.com/10.1057/ivs.2010.8},
 volume = {10},
 year = {2011i}
}

@article{doi:10.1057/jit.2011.2,
 abstract = {Research on information technology has been focused primarily on the worlds of IT and management systems for business and government to the relative neglect of research on the digital and institutional infrastructures that underpin the research enterprise itself. When digital research is studied, the emphasis has been on the diffusion of technological innovations, rather than the social and political dynamics shaping the design and role of technologies in research. However, what researchers know, and with whom they collaborate, could be transformed through the strategic use of advances designed to support research, defined here as ‘research-centred computational networks’. This article presents a framework for conceptualizing the social and technological choices shaping the next generation of research in ways that could open – democratize – key aspects of the research process that move well beyond academic publication. The framework highlights the limited scope of innovation to date, and identifies a variety of factors that maintain and enhance institutional control over the research process, at the risk of losing the creative and productive bottom-up participation by networked researchers and citizen researchers among the public at large. Conceptualizing, prioritizing and advancing study of next generation research is one of the most significant but difficult challenges facing scholars of information technology.},
 author = {William H Dutton},
 doi = {10.1057/jit.2011.2},
 eprint = {https://doi-org.crai.referencistas.com/10.1057/jit.2011.2},
 journal = {Journal of Information Technology},
 number = {2},
 pages = {109–119},
 title = {The Politics of Next Generation Research Democratizing Research-Centred Computational Networks},
 url = {https://doi-org.crai.referencistas.com/10.1057/jit.2011.2},
 volume = {26},
 year = {2011g}
}

@article{doi:10.1057/palgrave.ivs.9500141,
 abstract = {We have built an AJAX-enabled browser-based testbed for evaluating the performance of computational linguistics algorithms. Our testbed consists of a visualization system and analysis portal. Our focus is on algorithms that classify and cluster documents by assigning weights to words and scoring each document against high-dimensional reference concept vectors. The testbed visualization and algorithm analysis techniques include Confusion Matrices, ROC Curves, Document Visualizations showing word importance, and Interactive Reports. A unique aspect of our testbed is document visualizations built using Scalable Vector Graphics that show why documents are assigned to particular concepts and categories.},
 author = {Stephen G. Eick and Justin Mauger and Alan Ratner},
 doi = {10.1057/palgrave.ivs.9500141},
 eprint = {https://doi-org.crai.referencistas.com/10.1057/palgrave.ivs.9500141},
 journal = {Information Visualization},
 number = {1},
 pages = {64–74},
 title = {A Visualization Testbed for Analyzing the Performance of Computational Linguistics Algorithms},
 url = {https://doi-org.crai.referencistas.com/10.1057/palgrave.ivs.9500141},
 volume = {6},
 year = {2007e}
}

@article{doi:10.1068/a151219,
 abstract = {A unified approach to deriving models of urban location, destination, mode, and route choice is illustrated, and an algorithm based on Evans’s approach and the Lagrange multiplier procedure is proposed. By examining derivatives of the Lagrangian function, we show that the Newton—Raphson technique can be implemented for finding the optimal Lagrange multipliers for these models. Procedures for identifying values of generalized cost-function coefficients are studied.},
 author = {D E Boyce and K S Chon and Y J Lee and K T Lin and L J LeBlanc},
 doi = {10.1068/a151219},
 eprint = {https://doi-org.crai.referencistas.com/10.1068/a151219},
 journal = {Environment and Planning A: Economy and Space},
 number = {9},
 pages = {1219–1230},
 title = {Implementation and Computational Issues for Combined Models of Location, Destination, Mode, and Route Choice},
 url = {https://doi-org.crai.referencistas.com/10.1068/a151219},
 volume = {15},
 year = {1983d}
}

@article{doi:10.1068/a220169,
 abstract = {In this paper the characteristics and performance are described of a computational process model (CPM) of human way-finding that is based on psychological models of cognition and experimental data on human way-finding. The model comprises two modules, one for representing objectively a suburban environment and the other for representing the cognitive processes involved in navigation. The CPM is employed to simulate and investigate how spatial knowledge that has been acquired and stored is retrieved and processed in order to plan a path to a given goal. The results of the experiments carried out with NAVIGATOR are compared with the results of experiments involving young adults in real environments. We analyze the performance of NAVIGATOR in terms of its ability to find the goal, the amount of time taken to reach a given goal, the differences in performance during way-finding, and the errors in navigation, as a function of several parameters. The parameters are intended to represent key psychological components of human perceptual and cognitive systems and are derived from experimental studies of human way-finding behavior. The main value of this model lies in its use as a device for investigating and understanding human spatial performance, in terms of accurate performance and errors that arise in tasks of way-finding, and the variations among individuals in terms of performance. A set of computational experiments are used to examine the differences in search strategies between ‘individuals’. The effects of various parameters on way-finding performance are analyzed. Errors in way-finding are classified and are compared with Norman’s and Reason’s classification of errors in human performance. The CPM appears to provide a fruitful model for investigating human way-finding behavior.},
 author = {S Gopal and T R Smith},
 doi = {10.1068/a220169},
 eprint = {https://doi-org.crai.referencistas.com/10.1068/a220169},
 journal = {Environment and Planning A: Economy and Space},
 number = {2},
 pages = {169–191},
 title = {Human Way-Finding in an Urban Environment: A Performance Analysis of a Computational Process Model},
 url = {https://doi-org.crai.referencistas.com/10.1068/a220169},
 volume = {22},
 year = {1990h}
}

@article{doi:10.1068/a301839,
 abstract = {In this paper we outline some of the results that were obtained by the application of a Cray T3D parallel supercomputer to human geography problems. We emphasise the fundamental importance of high-performance computing (HPC) as a future relevant paradigm for doing geography. We offer an introduction to recent developments and illustrate how new computational intelligence technologies can start to be used to make use of opportunities created by data riches from geographic information systems, artificial intelligence tools, and HPC in geography.},
 author = {I Turton and S Openshaw},
 doi = {10.1068/a301839},
 eprint = {https://doi-org.crai.referencistas.com/10.1068/a301839},
 journal = {Environment and Planning A: Economy and Space},
 number = {10},
 pages = {1839–1856},
 title = {High-Performance Computing and Geography: Developments, Issues, and Case Studies},
 url = {https://doi-org.crai.referencistas.com/10.1068/a301839},
 volume = {30},
 year = {1998r}
}

@article{doi:10.1068/b1296,
 abstract = {This paper presents a methodology for the creation of homogeneous demographic regions with geographical information systems (GIS) and computational intelligence. The proposed method is unsupervised fuzzy classification performed by neural networks using the fuzzy Kohonen algorithm. GIS technology offers a powerful set of tools for the input, management, and output of data, whereas computational intelligence is used for the analysis and the classification of the data. The proposed methodology is applied to the municipality of Athens, in Greece. Finally the advantages and disadvantages of the approach are discussed.},
 author = {Thomas Hatzichristos},
 doi = {10.1068/b1296},
 eprint = {https://doi-org.crai.referencistas.com/10.1068/b1296},
 journal = {Environment and Planning B: Planning and Design},
 number = {1},
 pages = {39–49},
 title = {Delineation of Demographic Regions with GIS and Computational Intelligence},
 url = {https://doi-org.crai.referencistas.com/10.1068/b1296},
 volume = {31},
 year = {2004g}
}

@article{doi:10.1068/b12979,
 abstract = {Several environment applications require the computation of visibility information on a terrain. Examples are optimal placement of observation points, line-of-sight communication, and computation of hidden as well as scenic paths. Visibility computations on a terrain may involve either one or many viewpoints, and range from visibility queries (for example, testing whether a given query point is visible), to the computation of structures that encode the visible portions of the surface. In this paper, the authors consider a number of visibility problems on terrains and present an overview of algorithms to tackle such problems on triangulated irregular networks and regular square grids.},
 author = {Leila Floriani and Paola Magillo},
 doi = {10.1068/b12979},
 eprint = {https://doi-org.crai.referencistas.com/10.1068/b12979},
 journal = {Environment and Planning B: Planning and Design},
 number = {5},
 pages = {709–728},
 title = {Algorithms for Visibility Computation on Terrains: A Survey},
 url = {https://doi-org.crai.referencistas.com/10.1068/b12979},
 volume = {30},
 year = {2003f}
}

@article{doi:10.1068/b230239,
 author = {U Flemming and P Fisher and B Ilbery and S M Romaya and J Kneale and M Gandy and P Malpass},
 doi = {10.1068/b230239},
 eprint = {https://doi-org.crai.referencistas.com/10.1068/b230239},
 journal = {Environment and Planning B: Planning and Design},
 number = {2},
 pages = {239–252},
 title = {Review: Review Essay: Good, Not So Good, and Everything in Between: Current Work on Computational Creativity Modeling Creativity and Knowledge-Based Creative Design, Spatial Analysis and GIS, National and Regional Tourism Planning: Methodologies and Case Studies, Urban Environmental Management: Environmental Change and Urban Design, Writing the City: Eden, Babylon and the New Jerusalem, the Urban Experience: A People—Environment Perspective, Malign Neglect: Homelessness in an American City},
 url = {https://doi-org.crai.referencistas.com/10.1068/b230239},
 volume = {23},
 year = {1996e}
}

@article{doi:10.1068/b3309,
 abstract = {Within the context of the growing polarisation and fragmentation of the urban landscape, this paper presents a computational typology applicable to the study of minority communities, both ethnic and religious, which is useful in understanding their spatial distribution and juxtaposition at neighbourhood levels. The typology has been applied to multicultural London with the use of the 2001 Census, in which there were questions on ethnicity and religion. The landscape of religion is found to be more highly segregated in contrast to the landscape of ethnicity. Furthermore, on the basis of a preliminary analysis of indicator variables, minorities seem on aggregate to be in an improved situation given a level of residential segregation, with the exception of residents of segregated Asian-Bangladeshi areas for ethnicity and residents of segregated Muslim areas for religion. This questions the generally held view that segregation in a multicultural society is undesirable per se and suggests that a ‘one size fits all’ government policy towards residential segregation is insufficiently perceptive. The typology introduced here should facilitate a more critically informed approach to multiculturalism and the contemporary city.},
 author = {Allan J Brimicombe},
 doi = {10.1068/b3309},
 eprint = {https://doi-org.crai.referencistas.com/10.1068/b3309},
 journal = {Environment and Planning B: Planning and Design},
 number = {5},
 pages = {884–904},
 title = {Ethnicity, Religion, and Residential Segregation in London: Evidence from a Computational Typology of Minority Communities},
 url = {https://doi-org.crai.referencistas.com/10.1068/b3309},
 volume = {34},
 year = {2007b}
}

@article{doi:10.1068/b37151,
 abstract = {In this paper we assess the efficacy of a dynamic adaptive planning (DAP) approach for guiding the long-term development of infrastructure. The efficacy of the approach is tested on the specific case of airport strategic planning. Utilizing a fast and simple model of an airport, and a composition of small models that can generate a wide spectrum of alternative futures, the performance of a dynamic adaptive plan is compared with the performance of a static, rigid implementation plan across a wide spectrum of conceivable futures. These computational experiments reveal that the static rigid plan outperforms the dynamic adaptive plan in only a small part of the spectrum. Moreover, given the wide array of possible futures, the dynamic adaptive plan has a narrower spread of outcomes than the static rigid plan, implying that the dynamic adaptive plan exposes planners to less uncertainty about its future performance despite the wide variety of uncertainties that are present. These computational results confirm theoretical hypotheses in the literature that DAP approaches are more efficacious for planning under uncertainty.},
 author = {Jan H Kwakkel and Warren E Walker and Vincent A W J Marchau},
 doi = {10.1068/b37151},
 eprint = {https://doi-org.crai.referencistas.com/10.1068/b37151},
 journal = {Environment and Planning B: Planning and Design},
 number = {3},
 pages = {533–550},
 title = {Assessing the Efficacy of Dynamic Adaptive Planning of Infrastructure: Results from Computational Experiments},
 url = {https://doi-org.crai.referencistas.com/10.1068/b37151},
 volume = {39},
 year = {2012m}
}

@article{doi:10.1068/b38036,
 abstract = {In this paper we deal with different metrics using Lp norms in the 1-facility location problem and their properties. We propose to revisit the problem of optimal center location by discussing the properties of three well-known centers in 2-dimensional space: The 1-median for L1, the 1-center (Chebyshev center) for L∞ and the gravity center for L2, respectively, the median, the mean, and the center of extreme values in one dimension. The contribution of the research concerns methods to map influence and sensitivity that provide valuable and complementary information on space for decision making in territorial planning. We also discuss the center properties according to the primary objectives of equity, equality, and efficacy in the access to a facility. In a spatial-thinking approach, we present some methodological propositions to obtain robust and durable centers in geographical space, that rely on the adaptation of the general frame of the Lp norm to the planning objectives.},
 author = {Didier Josselin and Marc Ciligot-Travain},
 doi = {10.1068/b38036},
 eprint = {https://doi-org.crai.referencistas.com/10.1068/b38036},
 journal = {Environment and Planning B: Planning and Design},
 number = {5},
 pages = {923–941},
 title = {Revisiting the Optimal Center Location. A Spatial Thinking Based on Robustness, Sensitivity, and Influence Analysis},
 url = {https://doi-org.crai.referencistas.com/10.1068/b38036},
 volume = {40},
 year = {2013i}
}

@article{doi:10.1068/p020415,
 abstract = {A computational approach to visual perception is outlined. The investigation centres on two problems. One concerns the shape, and the other the edges, of dot figures. In determining edges it is not possible to delimit arbitrarily the region on which each point computes. So in this context a global computation is defined. This leads to a novel way of investigating clustering. When the edges of a figure have been sequenced to yield the boundary it is then possible to determine its shape. The study of shape has been confined to convex figures. In these an attempt is made to extract global units which correspond to corner-like regions. Finally, it is suggested that a theory which evolves in the manner described has a more unitary aspect than other approaches.},
 author = {B Rosenberg and D J Langridge},
 doi = {10.1068/p020415},
 eprint = {https://doi-org.crai.referencistas.com/10.1068/p020415},
 journal = {Perception},
 note = {PMID:4803950},
 number = {4},
 pages = {415–424},
 title = {A Computational View of Perception},
 url = {https://doi-org.crai.referencistas.com/10.1068/p020415},
 volume = {2},
 year = {1973q}
}

@article{doi:10.1068/p090125,
 abstract = {Piaget has distinguished a number of distinct stages in the development of the concept of an enduring external object during infancy. I present a theory of a class of behaviours at one of these stages embodied in a working computer program. The behaviour of this program matches a class of perceptual behaviours of infants between about twelve and twenty weeks of age in a number of experimental situations studied by Bower. The theory argues that these behaviours are a result of the interaction between the perceptual and conceptual levels of the system, and the way in which conflicts between competing descriptions of an object are resolved. I locate the cause of several features of the behaviours in the procedures for managing the changing representation of the world, and the system’s way of treating transitions between the states of an object (for example, moving to stationary). The basic conceptual primitives of the analysis are objects and events, not motion and place, as argued by Bower, or the infant’s previous activity, as argued by Piaget. I argue that adequate explanations of experimental findings such as these require the construction of fairly detailed computational models.},
 author = {Slava Prazdny},
 doi = {10.1068/p090125},
 eprint = {https://doi-org.crai.referencistas.com/10.1068/p090125},
 journal = {Perception},
 note = {PMID:7375323},
 number = {2},
 pages = {125–150},
 title = {A Computational Study of a Period of Infant Object-Concept Development},
 url = {https://doi-org.crai.referencistas.com/10.1068/p090125},
 volume = {9},
 year = {1980q}
}

@article{doi:10.1068/p120089,
 author = {R Watt and G Pick and R Harper},
 doi = {10.1068/p120089},
 eprint = {https://doi-org.crai.referencistas.com/10.1068/p120089},
 journal = {Perception},
 number = {1},
 pages = {89–94},
 title = {Reviews: Vision: A Computational Investigation into the Human Representation and Processing of Visual Information, Hearing Research and Theory, Volume 1, the Perception of Odors},
 url = {https://doi-org.crai.referencistas.com/10.1068/p120089},
 volume = {12},
 year = {1983s}
}

@article{doi:10.1068/p120203,
 abstract = {A critical difficulty in theories of the ‘mental rotation’ phenomenon has been to find a computationally plausible reason why the rotation should occur in small intermediate steps. It is pointed out that this difficulty is peculiar to metrical representations: if spatial relations are presented symbolically but nonmetrically, then the iterative or recursive application of minimal transformations is memory saving. A program rotter is described to illustrate this principle.},
 author = {Michael J Morgan},
 doi = {10.1068/p120203},
 eprint = {https://doi-org.crai.referencistas.com/10.1068/p120203},
 journal = {Perception},
 note = {PMID:6689208},
 number = {2},
 pages = {203–211},
 title = {Mental Rotation: A Computationally Plausible Account of Transformation through Intermediate Steps},
 url = {https://doi-org.crai.referencistas.com/10.1068/p120203},
 volume = {12},
 year = {1983k}
}

@article{doi:10.1068/p130083,
 author = {J Ackroyd and K Rayner and P T Smith},
 doi = {10.1068/p130083},
 eprint = {https://doi-org.crai.referencistas.com/10.1068/p130083},
 journal = {Perception},
 number = {1},
 pages = {83–88},
 title = {Reviews: The Psychology of Human—Computer Interaction, Eye Movements: Cognition and Visual Perception, Eye Movements and Psychological Functions: International Views, Thinking, Problem Solving, Cognition},
 url = {https://doi-org.crai.referencistas.com/10.1068/p130083},
 volume = {13},
 year = {1984a}
}

@article{doi:10.1068/p180691,
 author = {F Kingdom and D Green and J Brown},
 doi = {10.1068/p180691},
 eprint = {https://doi-org.crai.referencistas.com/10.1068/p180691},
 journal = {Perception},
 number = {5},
 pages = {691–694},
 title = {Reviews: Visual Processing: Computational, Psychophysical and Cognitive Research. Essays in Cognitive Psychology, The Nature of Creativity: Contemporary Psychological Approaches, Remembering Reconsidered. Ecological and Traditional Approaches to the Study of Memory},
 url = {https://doi-org.crai.referencistas.com/10.1068/p180691},
 volume = {18},
 year = {1989k}
}

@article{doi:10.1068/p220597,
 abstract = {A conceptual model of the human haptic system in relation to object identification is presented. The model encompasses major architectural elements including representations of haptically accessible object properties and exploratory procedures (EPs)—dedicated movement patterns that are specialized to extract particular properties. These architectural units are related in processing-specific ways. Properties are associated with exploratory procedures in keeping with the extent to which a given procedure delivers information about a given property. The EPs are associated with one another in keeping with their compatibility, as determined by parameters of motor execution and interactions with the object and the workspace. The resulting architecture is treated as a system of constraints which guide the exploration of an object during the course of identification. The selection of the next step in a sequence of exploration requires that constraints be optimally satisfied. A network approach to constraint satisfaction is implemented and shown to account for a number of previous empirical results concerning the time course of exploration, object classification speed, and incidental learning about object properties. This system has potential applications for robotic haptic exploration.},
 author = {Roberta L Klatzky and Susan J Lederman},
 doi = {10.1068/p220597},
 eprint = {https://doi-org.crai.referencistas.com/10.1068/p220597},
 journal = {Perception},
 note = {PMID:8414884},
 number = {5},
 pages = {597–621},
 title = {Toward a Computational Model of Constraint-Driven Exploration and Haptic Object Identification},
 url = {https://doi-org.crai.referencistas.com/10.1068/p220597},
 volume = {22},
 year = {1993i}
}

@article{doi:10.1068/p230367,
 author = {I M Glyn and G Brelstaff},
 doi = {10.1068/p230367},
 eprint = {https://doi-org.crai.referencistas.com/10.1068/p230367},
 journal = {Perception},
 number = {3},
 pages = {367–370},
 title = {Reviews: The Astonishing Hypothesis: The Scientific Search for the Soul, Computational Models of Visual Processing},
 url = {https://doi-org.crai.referencistas.com/10.1068/p230367},
 volume = {23},
 year = {1994f}
}

@article{doi:10.1068/p230399,
 abstract = {Perceptual organisation, and especially the computation of contour information, has been the object of considerable interest in the last few years. In the first part of the paper we review recent accounts on the mechanisms involved in the processing of contour. In the second part we report an experiment designed to examine (1) how physical parameters such as spatial proximity and collinearity of elements affect the integration of global contour in objects and (2) whether the activation of stored representations of objects facilitates the computation of contour. Incomplete forms varying in the spacing and the alignment of line segments on their contour were used as stimuli in a matching task. Subjects were asked to decide which of two laterally displayed figures matched a reference form presented previously. The matching target and the distractor were physically identical but differed in their orientation. In one condition the reference object was always an outline drawing of an object. In a second condition the reference object was either a complete object or a more or less identifiable incomplete form. Little variation in performance was found for forms having continuous and discontinuous contour up to a spacing of 5 pixels (10.8 min) between elements. Response times and errors increased abruptly beyond this limit. This effect occurred in the two conditions of reference stimulus, suggesting that the computation of contour information is more affected by physical constraints at early processes than by high-level processes involving activation of stored structural representations of objects.},
 author = {Muriel Boucart and Sandrine Delord and Anne Giersch},
 doi = {10.1068/p230399},
 eprint = {https://doi-org.crai.referencistas.com/10.1068/p230399},
 journal = {Perception},
 note = {PMID:7991341},
 number = {4},
 pages = {399–409},
 title = {The Computation of Contour Information in Complex Objects},
 url = {https://doi-org.crai.referencistas.com/10.1068/p230399},
 volume = {23},
 year = {1994d}
}

@article{doi:10.1068/p270785,
 abstract = {To indicate motion in a static drawing, artists often include lines trailing a moving object. The use of these motion lines is notable because they do not seem to be related to anything in the optic array. The dynamic behavior of a neural-network model for contour detection is analyzed and it is shown that it generates trails of oriented responses behind moving stimuli. The properties of the oriented response trails are shown to correspond to motion lines. The model generates trails of different orientations depending on the speed and length of the movement, and thereby predicts different uses of two types of motion lines. The model further predicts that motion lines should bias real motion in some situations. An experiment relating motion lines to ambiguous motion percepts demonstrates that motion lines contribute to motion percepts.},
 author = {Hyungjun Kim and Gregory Francis},
 doi = {10.1068/p270785},
 eprint = {https://doi-org.crai.referencistas.com/10.1068/p270785},
 journal = {Perception},
 note = {PMID:10209641},
 number = {7},
 pages = {785–797},
 title = {A Computational and Perceptual Account of Motion Lines},
 url = {https://doi-org.crai.referencistas.com/10.1068/p270785},
 volume = {27},
 year = {1998i}
}

@article{doi:10.1068/p2808rvw,
 author = {E Forde and Adriane E Seiffert},
 doi = {10.1068/p2808rvw},
 eprint = {https://doi-org.crai.referencistas.com/10.1068/p2808rvw},
 journal = {Perception},
 number = {8},
 pages = {1051–1054},
 title = {Reviews: Handbook of Clinical and Experimental Neuropsychology, High-Level Motion Processing: Computational, Neurobiological, and Psychophysical Perspectives},
 url = {https://doi-org.crai.referencistas.com/10.1068/p2808rvw},
 volume = {28},
 year = {1999h}
}

@article{doi:10.1068/p281089,
 abstract = {Given a constant stream of perceptual stimuli, how can the underlying invariances associated with a given input be learned? One approach consists of using generic truths about the spatiotemporal structure of the physical world as constraints on the types of quantities learned. The learning methodology employed here embodies one such truth: that perceptually salient properties (such as stereo disparity) tend to vary smoothly over time. Unfortunately, the units of an artificial neural network tend to encode superficial image properties, such as individual grey-level pixel values, which vary rapidly over time. However, if the states of units are constrained to vary slowly, then the network is forced to learn a smoothly varying function of the training data. We implemented this temporal-smoothness constraint in a backpropagation network which learned stereo disparity from random-dot stereograms. Temporal smoothness was formalised with the use of regularisation theory by modifying the standard cost function minimised during training of a network. Temporal smoothness was found to be similar to other techniques for improving generalisation, such as early stopping and weight decay. However, in contrast to these, the theoretical underpinnings of temporal smoothing are intimately related to fundamental characteristics of the physical world. Results are discussed in terms of regularisation theory and the physically realistic assumptions upon which temporal smoothing is based.},
 author = {James V Stone and Nicol Harper},
 doi = {10.1068/p281089},
 eprint = {https://doi-org.crai.referencistas.com/10.1068/p281089},
 journal = {Perception},
 note = {PMID:10694959},
 number = {9},
 pages = {1089–1104},
 title = {Temporal Constraints on Visual Learning: A Computational Model},
 url = {https://doi-org.crai.referencistas.com/10.1068/p281089},
 volume = {28},
 year = {1999p}
}

@article{doi:10.1068/p3103rvw,
 author = {Marina Bloj and Andrew E Welchman},
 doi = {10.1068/p3103rvw},
 eprint = {https://doi-org.crai.referencistas.com/10.1068/p3103rvw},
 journal = {Perception},
 number = {3},
 pages = {387–390},
 title = {Reviews: Color Perception: Philosophical, Psychological, Artistic, and Computational Perspectives, Visual Perception: An Introduction},
 url = {https://doi-org.crai.referencistas.com/10.1068/p3103rvw},
 volume = {31},
 year = {2002d}
}

@article{doi:10.1068/p3811rvw,
 author = {Constanze Hesse and Snehlata Jaswal},
 doi = {10.1068/p3811rvw},
 eprint = {https://doi-org.crai.referencistas.com/10.1068/p3811rvw},
 journal = {Perception},
 number = {11},
 pages = {1735–1738},
 title = {Reviews: Sensorimotor Control of Grasping: Physiology and Pathophysiology, Computation, Cognition, and Pylyshyn},
 url = {https://doi-org.crai.referencistas.com/10.1068/p3811rvw},
 volume = {38},
 year = {2009h}
}

@article{doi:10.1068/p4005rvw,
 author = {Joanna Hall and Moritz Buerck},
 doi = {10.1068/p4005rvw},
 eprint = {https://doi-org.crai.referencistas.com/10.1068/p4005rvw},
 journal = {Perception},
 number = {5},
 pages = {631–633},
 title = {Review: Seeing: The Computational Approach to Biological Vision: See What I’m Saying: The Extraordinary Powers of Our Five Senses},
 url = {https://doi-org.crai.referencistas.com/10.1068/p4005rvw},
 volume = {40},
 year = {2011g}
}

@article{doi:10.1068/p6057,
 abstract = {Might it be possible to harness the visual system to carry out artificial computations, somewhat akin to how DNA has been harnessed to carry out computation? I provide the beginnings of a research programme attempting to do this. In particular, new techniques are described for building ‘visual circuits’ (or ‘visual software’) using wire, NOT, OR, and AND gates in a visual modality such that our visual system acts as ‘visual hardware’ computing the circuit, and generating a resultant perception which is the output.},
 author = {Mark Changizi},
 doi = {10.1068/p6057},
 eprint = {https://doi-org.crai.referencistas.com/10.1068/p6057},
 journal = {Perception},
 note = {PMID:18773734},
 number = {7},
 pages = {1131–1134},
 title = {Harnessing Vision for Computation},
 url = {https://doi-org.crai.referencistas.com/10.1068/p6057},
 volume = {37},
 year = {2008d}
}

@article{doi:10.1068/p7275,
 abstract = {Marr proposed that human vision constructs “a true description of what is there”. He argued that to understand human vision one must discover the features of the world it recovers and the constraints it uses in the process. Bayesian decision theory (BDT) is used in modern vision research as a probabilistic framework for understanding human vision along the lines laid out by Marr. Marr’s contribution to vision research is substantial and justly influential. We propose, however, that evolution by natural selection does not, in general, favor perceptions that are true descriptions of the objective world. Instead, research with evolutionary games shows that perceptual systems tuned solely to fitness routinely outcompete those tuned to truth. Fitness functions depend not just on the true state of the world, but also on the organism, its state, and the type of action. Thus, fitness and truth are distinct. Natural selection depends only on expected fitness. It shapes perceptual systems to guide fitter behavior, not to estimate truth. To study perception in an evolutionary context, we introduce the framework of Computational Evolutionary Perception (CEP). We show that CEP subsumes BDT, and reinterprets BDT as evaluating expected fitness rather than estimating truth.},
 author = {Donald D Hoffman and Manish Singh},
 doi = {10.1068/p7275},
 eprint = {https://doi-org.crai.referencistas.com/10.1068/p7275},
 journal = {Perception},
 note = {PMID:23409373},
 number = {9},
 pages = {1073–1091},
 title = {Computational Evolutionary Perception},
 url = {https://doi-org.crai.referencistas.com/10.1068/p7275},
 volume = {41},
 year = {2012j}
}

@article{doi:10.1068/p7327,
 abstract = {David Marr’s book Vision attempted to formulate a thoroughgoing formal theory of perception. Marr borrowed much of the “computational” level from James Gibson: a proper understanding of the goal of vision, the natural constraints, and the available information are prerequisite to describing the processes and mechanisms by which the goal is achieved. Yet, as a research program leading to a computational model of human vision, Marr’s program did not succeed. This article asks why, using the perception of 3D shape as a morality tale. Marr presumed that the goal of vision is to recover a general-purpose Euclidean description of the world, which can be deployed for any task or action. On this formulation, vision is underdetermined by information, which in turn necessitates auxiliary assumptions to solve the problem. But Marr’s assumptions did not actually reflect natural constraints, and consequently the solutions were not robust. We now know that humans do not in fact recover Euclidean structure—rather, they reliably perceive qualitative shape (hills, dales, courses, ridges), which is specified by the second-order differential structure of images. By recasting the goals of vision in terms of our perceptual competencies, and doing the hard work of analyzing the information available under ecological constraints, we can reformulate the problem so that perception is determined by information and prior knowledge is unnecessary.},
 author = {William H Warren},
 doi = {10.1068/p7327},
 eprint = {https://doi-org.crai.referencistas.com/10.1068/p7327},
 journal = {Perception},
 note = {PMID:23409371},
 number = {9},
 pages = {1053–1060},
 title = {Does This Computational Theory Solve the Right Problem? Marr, Gibson, and the Goal of Vision},
 url = {https://doi-org.crai.referencistas.com/10.1068/p7327},
 volume = {41},
 year = {2012s}
}

@article{doi:10.1080/01650250143000409,
 author = {Horst Krist},
 doi = {10.1080/01650250143000409},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/01650250143000409},
 journal = {International Journal of Behavioral Development},
 number = {5},
 pages = {475–476},
 title = {Book Review: Emerging minds: The process of change in children’s thinking},
 url = {https://doi-org.crai.referencistas.com/10.1080/01650250143000409},
 volume = {26},
 year = {2002j}
}

@article{doi:10.1080/02103702.1992.10822322,
 abstract = {Piaget’s (1953, 1955) increasingly controversial claim that infant knowledge depends upon action need not be rejected, provided the mechanisms underlying infant ability are conceptualized appropriately in computational terms. Computational concepts solve many problems caused by Piaget’s notions of perception, behaviour, schemes, reciprocal-assimilation and action. Artificial intelligence work on vision offers a way of explaining early perceptual abilities that is precise, internally coherent and able to encompass recent findings of innate organization. Concepts from the procedural programming languages offer a way of accounting for both internal and overt aspects of behaviour, and for the functional coordination of perception and behaviour that characterizes infant anion. This perspective challenges Piaget’s view that development necessitates a radical reconstruction of action-based knowledge. Conceptualized computationally, perceptual-behavioural action can be seen to involve representation in a nontrivial sense. Restructuring of action mechanisms can account for some central phenomena of infant development.},
 author = {Julie C. Rutkowska},
 doi = {10.1080/02103702.1992.10822322},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/02103702.1992.10822322},
 journal = {Journal for the Study of Education and Development},
 number = {57},
 pages = {23–48},
 title = {A computational alternative to the Piagetian infant},
 url = {https://doi-org.crai.referencistas.com/10.1080/02103702.1992.10822322},
 volume = {15},
 year = {1992p}
}

@article{doi:10.1080/02103702.2018.1450475,
 abstract = {Edith Ackermann was one of the most important scholars in the interdisciplinary field of Child-Computer Interaction. Trained as a developmental psychologist and having worked with Jean Piaget in Switzerland, Ackermann went to MIT in 1985 to join an intrepid group of researchers led by Seymour Papert who were trying to understand how extant theories of development and learning would fare in a world in which children would be surrounded by computational artefacts. For the ensuing three decades, Ackermann would use her unique interdisciplinary expertise in the service of creating new theories of development for this new world, generating cutting-edge research, inspiring a generation of students, and producing seminal papers. This article, making use of literature as well as interviews with colleagues of Edith, discusses Ackermann’s contribution to the field, her life trajectory, impact, and ideas. Edith passed away on 24 December 2016.},
 author = {Paulo Blikstein},
 doi = {10.1080/02103702.2018.1450475},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/02103702.2018.1450475},
 journal = {Journal for the Study of Education and Development},
 number = {2},
 pages = {248–286},
 title = {Thinking with your fingers and touching with your mind: the cognitive dance of Edith Ackermann / Pensando con los dedos y tocando con la mente: la danza cognitiva de Edith Ackermann},
 url = {https://doi-org.crai.referencistas.com/10.1080/02103702.2018.1450475},
 volume = {41},
 year = {2018a}
}

@article{doi:10.1080/02103702.2019.1604022,
 abstract = {The purpose of the present study was to examine the structure and development of algebraic thinking across multiple dimensions. An algebraic thinking test was administered to 803 students aged 10–13 years old. One hundred and one students of different performance outcomes in the algebraic thinking test participated in semi-structured clinical interviews in order to develop further insights into students’ strategies and difficulties. The results confirmed the multifaceted nature of algebraic thinking and showed that algebraic thinking consists of three aspects: reasoning about covariation, generalization of arithmetic properties and abilities directly related to algebraic syntax. Each one of these aspects is composed of specific algebraic thinking abilities, such as modelling relations using algebraic symbols. Four groups of students of different algebraic thinking profiles were identified: ‘pre-algebraic’, ‘protoalgebraic-procedural’, ‘relational-symbolic algebraic thinking’ and ‘structural-global algebraic thinking’. The results also supported the presence of a consistent trend in the difficulty level across the algebraic thinking abilities, which suggests a specific developmental trend from a more procedural perspective of algebraic thinking to a more structural one.},
 author = {Marilena-Barbara Chrysostomou and Constantinos Christou},
 doi = {10.1080/02103702.2019.1604022},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/02103702.2019.1604022},
 journal = {Journal for the Study of Education and Development},
 number = {3},
 pages = {721–781},
 title = {Analysing the notion of algebraic thinking based on empirical evidence / Un análisis del concepto de pensamiento algebraico basado en evidencia empírica},
 url = {https://doi-org.crai.referencistas.com/10.1080/02103702.2019.1604022},
 volume = {42},
 year = {2019d}
}

@article{doi:10.1080/02103702.2020.1778280,
 abstract = {In this paper, we discuss the theoretical foundation and implementation of two alternative instructional courses that aimed to support the development of elementary school students’ early algebraic thinking. Both courses approached three basic algebra content strands: generalized arithmetic, functional thinking and modelling languages. The courses differed according to the characteristics of the tasks that were used. The first course involved ‘pure mathematical guided investigations’ emphasizing scaffolding steps in pure mathematics contexts. The second course focused on ‘applied open explorations’ underlining more open questions in applied everyday contexts. The courses were compared in respect to students’ learning outcomes. The findings, yielded from the analysis of pre-test and post-test data, indicated that the second course had better learning outcomes compared to the first.},
 author = {Maria Chimoni and Demetra Pitta-Pantazi and Constantinos Christou},
 doi = {10.1080/02103702.2020.1778280},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/02103702.2020.1778280},
 journal = {Journal for the Study of Education and Development},
 number = {3},
 pages = {503–552},
 title = {The impact of two different types of instructional tasks on students’ development of early algebraic thinking (El impacto de dos tipos diferentes de tareas instruccionales en el desarrollo del pensamiento algebraico temprano de los estudiantes)},
 url = {https://doi-org.crai.referencistas.com/10.1080/02103702.2020.1778280},
 volume = {44},
 year = {2021d}
}

@article{doi:10.1080/02109395.2015.1122436,
 abstract = {Discussions on counterfactual thinking (CT) have been focused on whether it is a language skill or it emerges spontaneously before language acquisition. This paper surveys the most compelling arguments regarding these frameworks: (1) the approach ‘CT as a language skill’; (2) those who claim that pretending shows that children have CT; (3) those who consider pretend play is a rehearsal for cognitive dispositions. I shall point out that the three approaches on CT are incomplete: (1) neglects pretend play (which prelinguistic children perform) as an instantiation of the CT; (2) puts too much emphasis on the linguistic dimension of pretense; (3) is highly demanding on the cognitive architecture that we ought to have by nature — the so-called children as scientists.},
 author = {Alberto Murcia},
 doi = {10.1080/02109395.2015.1122436},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/02109395.2015.1122436},
 journal = {Studies in Psychology},
 number = {1},
 pages = {35–58},
 title = {Counterfactual thinking as an issue between language and pretend play / El pensamiento contrafactual como un problema entre el lenguaje y el juego de simulación},
 url = {https://doi-org.crai.referencistas.com/10.1080/02109395.2015.1122436},
 volume = {37},
 year = {2016m}
}

@article{doi:10.1080/02109395.2020.1748841,
 abstract = {The article presents the results of a literature review that covered various study types and contributions made on cognition and numerical thinking in Colombia. After conducting a search in different sources, 84 relevant documents were selected and analysed in detail. Three categories were created: (i) subjects and lines of research; (ii) teacher training; and (iii) contributions to teaching and learning processes in the classroom. The results of the study indicate that in Colombia there is a prevalence of studies that focus on proposals addressing needs in mathematics classes. There is also a significant amount of studies on cognitive, semiotic and sociocultural perspectives. A gap between these two types of studies is also presented. Some considerations for future research on the topic are outlined at the end of the article.},
 author = {Gilberto Obando-Zapata and Teresa Pontón-Ladino and Sandra-Evely Parada-Rico and Jhony A. Villa-Ochoa},
 doi = {10.1080/02109395.2020.1748841},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/02109395.2020.1748841},
 journal = {Studies in Psychology},
 number = {2},
 pages = {319–347},
 title = {Research into cognition and numerical thinking in Colombia (Investigación sobre cognición y pensamiento numérico en Colombia)},
 url = {https://doi-org.crai.referencistas.com/10.1080/02109395.2020.1748841},
 volume = {41},
 year = {2020i}
}

@article{doi:10.1080/02670836.2018.1489939,
 abstract = {This paper presents a new computational framework to describe the evolution of grain structure during metal additive manufacturing and to simulate an inelastic deformation of the additively manufactured material, taking into account the grain structure explicitly. A combined effect of grain structure and loading conditions on the evolution of the stress–strain state in additively manufactured specimens is investigated. The results of the research highlight the need to account for the realistic microstructure, to properly describe the mechanical behaviour of additively manufactured specimens and parts. This is part of a thematic issue on Small Scale Mechanics - EUROMAT.},
 author = {O. Zinovieva and A. Zinoviev and V. Ploshikhin and V. Romanova and R. Balokhonov},
 doi = {10.1080/02670836.2018.1489939},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/02670836.2018.1489939},
 journal = {Materials Science and Technology},
 number = {13},
 pages = {1591–1605},
 title = {Strategy of computational predictions for mechanical behaviour of additively manufactured materials},
 url = {https://doi-org.crai.referencistas.com/10.1080/02670836.2018.1489939},
 volume = {34},
 year = {2018t}
}

@article{doi:10.1080/02670836.2022.2065754,
 abstract = {In this work, theoretical and experimental techniques were employed to analyze the feasibility of ‘green’ Ag- and Bi-based QDs as sensitisers in QDSSC. In the computational approach, the geometrical structures and optoelectronic properties of anatase-TiO2 (101) surface sensitised with Ag2S, Bi2S3 and Ag2S–Bi2S3 nanoclusters were investigated by means of density functional theory. In the experimental approach, the performances of the QDSSCs were validated. The properties of the QD-sensitised TiO2 electrode were characterised with FESEM, TEM, EDS, and UV–visible spectroscopy. It was observed that Ag2S–Bi2S3 QDSSC treated with post heat treatment exhibited better efficiency. UV–VIS spectroscopy result showed that the same sample had better absorption below the 570 nm wavelength. Efficiency obtained was at the low side due to charge recombination.},
 author = {J. K. H. Lai and T. T. Nguyen and N. N. Ha and N. H. Lan and H. K. Jun},
 doi = {10.1080/02670836.2022.2065754},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/02670836.2022.2065754},
 journal = {Materials Science and Technology},
 number = {12},
 pages = {842–852},
 title = {Computational and performance studies of Ag2S–Bi2S3 quantum dot-sensitised solar cells},
 url = {https://doi-org.crai.referencistas.com/10.1080/02670836.2022.2065754},
 volume = {38},
 year = {2022n}
}

@article{doi:10.1080/02724988743000097,
 author = {Alan Garnham},
 doi = {10.1080/02724988743000097},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/02724988743000097},
 journal = {The Quarterly Journal of Experimental Psychology Section A},
 number = {1},
 pages = {171–173},
 title = {Book Review: Natural Languqge Parsing: Psychological, Computational, and Theoretical Perspectives},
 url = {https://doi-org.crai.referencistas.com/10.1080/02724988743000097},
 volume = {39},
 year = {1987i}
}

@article{doi:10.1080/03080188.2020.1794381,
 abstract = {The computational metaphor of organisms as phenotypic automata controlled by a genetic programme has been replaced by the cognitive metaphor of organisms as intelligent agents making decisions about how to use their genomic and environmental resources. This metaphor facilitates novel ways of thinking about organisms that defy the assumptions of the old machine metaphor. But like all metaphors, the cognitive metaphor discloses important similarities at the expense of eclipsing significant dissimilarities. I argue that although cognitivism carries with it the crucial insight that living organisms are agents rather than mere automata, this metaphor distorts the nature of biological agency by over-intellectualizing, which risks eliminating the distinction between life and mind. I trace the Cartesian root of these metaphors and argue that it generates a dilemma between mechanism and intellectualism. To capture the middle path that most organisms occupy between these two extremes, I propose an ecological account of natural agency.},
 author = {Fermín C. Fulda},
 doi = {10.1080/03080188.2020.1794381},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/03080188.2020.1794381},
 journal = {Interdisciplinary Science Reviews},
 number = {3},
 pages = {315–330},
 title = {Biopsychism: life between computation and cognition},
 url = {https://doi-org.crai.referencistas.com/10.1080/03080188.2020.1794381},
 volume = {45},
 year = {2020f}
}

@article{doi:10.1080/03080188.2020.1840223,
 abstract = {Computational brain models use machine learning, algorithms and statistical models to harness big data for delivering disease-specific diagnosis or prognosis for individuals. While intended to support clinical decision-making, their translation into clinical practice remains challenging despite efforts to improve implementation through training clinicians and clinical staff in their use and benefits. Drawing on the specific case of neurology, we argue that existing implementation efforts are insufficient for the responsible translation of computational models. Our research based on a collective seven-year engagement with the Human Brain Project, participant observation at workshops and conferences, and expert interviews, suggests that relationships of trust between clinicians and researchers (modellers, data scientists) are essential to the meaningful translation of computational models. In particular, efforts to increase model transparency, strengthen upstream collaboration, and integrate clinicians’ perspectives and tacit knowledge have the potential to reinforce trust building and increase translation of technologies that are beneficial to patients.},
 author = {S. Datta Burton and T. Mahfoud and C. Aicardi and N. Rose},
 doi = {10.1080/03080188.2020.1840223},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/03080188.2020.1840223},
 journal = {Interdisciplinary Science Reviews},
 number = {1–2},
 pages = {138–157},
 title = {Clinical translation of computational brain models: understanding the salience of trust in clinician–researcher relationships},
 url = {https://doi-org.crai.referencistas.com/10.1080/03080188.2020.1840223},
 volume = {46},
 year = {2021d}
}

@article{doi:10.1080/03080188.2020.1865659,
 abstract = {The Faculty of Digital and Computational Studies (DCS) at Bowdoin College proposes a critical, analytical framework, referred to as the ‘4As,’ as an interdisciplinary means to interpret, evaluate, and create the data, operations, and devices of computing across all domains of knowledge production. Following other disciplines that have developed in symbiotic relationships to one another, DCS puts computation in conversation with fields from across the arts, humanities, physical, and social sciences. Our foundational premise is the bidirectional influence between these disciplines and digital artifacts and computation. The 4As (artifact, architecture, abstraction, and agency) benefit from both the scepticism of the liberal arts in the face of ubiquitous digital processes and the analytical opening for examining questions pertaining to creative and imaginative alternatives to the digital and computational status quo. We provide an ultra-contemporary case study to demonstrate the framework in use.},
 author = {Crystal Hall and Eric Chown and Fernando Nascimento},
 doi = {10.1080/03080188.2020.1865659},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/03080188.2020.1865659},
 journal = {Interdisciplinary Science Reviews},
 number = {4},
 pages = {458–476},
 title = {A critical, analytical framework for the digital machine},
 url = {https://doi-org.crai.referencistas.com/10.1080/03080188.2020.1865659},
 volume = {46},
 year = {2021e}
}

@article{doi:10.1080/03080188.2021.1872144,
 abstract = {This paper considers debates that have taken place in ethnomusicology as a result of engagement with the classification of data. Landmark projects over the past century introduced various classification systems and initiated important debates within the field. From the author’s perspective, classification of data is understood as a necessary precursor to computation in these projects. Classificatory thinking is used here as a theme to explore debates that have arisen when abstractions of musical practice have been suggested for use with ethnomusicology. The paper proposes that a recent approach to research practice for embedding computation adds to ongoing interdisciplinary work, demonstrating novel ways of contextualizing archival materials with ethnography alongside computation. The approach attempted to strike a balance for engaging large amounts of data in ethnomusicology. The paper argues that some resulting tensions arising from classification lead to insights, which cannot be drawn by ethnomusicological methods alone.},
 author = {Patrick Egan (Pádraig Mac Aodhgáin)},
 doi = {10.1080/03080188.2021.1872144},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/03080188.2021.1872144},
 journal = {Interdisciplinary Science Reviews},
 number = {4},
 pages = {477–500},
 title = {Insider or outsider? Exploring some digital challenges in ethnomusicology},
 url = {https://doi-org.crai.referencistas.com/10.1080/03080188.2021.1872144},
 volume = {46},
 year = {2021c}
}

@article{doi:10.1080/03080188.2023.2215024,
 abstract = {In the context of global research culture, practitioner-researchers in theatre and performance have consistently struggled to best account for the embodied and emergent qualities of their subjects. As methodologies for ‘practice as research’ (PaR) in theatre and performance have developed since the 1990s, artistic researchers have often continued to define themselves against scientific conceptions of thinking, knowledge, and research to highlight the specific efficacies of artistic practice. I argue that this strategy genuinely hinders researchers, and that interdisciplinary approaches that move across the sciences, humanities, and arts are the key to robust accounts of theatre and performance. By revisiting a seminal PaR performance project, Joanne ‘Bob’ Whalley and Lee Miller’s Partly Cloudy, Chance of Rain (2002), I suggest how interdisciplinary approaches such as enaction from the cognitive sciences should be integrated into PaR methodologies to better address the complexity and richness of embodiment and emergence in theatre and performance.},
 author = {Maiya Murphy},
 doi = {10.1080/03080188.2023.2215024},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/03080188.2023.2215024},
 journal = {Interdisciplinary Science Reviews},
 number = {4},
 pages = {628–650},
 title = {Thinking again: Enaction as a resource for ‘practice as research’ in theatre and performance},
 url = {https://doi-org.crai.referencistas.com/10.1080/03080188.2023.2215024},
 volume = {48},
 year = {2023o}
}

@article{doi:10.1080/09506608.2020.1868889,
 abstract = {In the current review, an exceptional view on the multi-scale integrated computational modelling and data-driven methods in the Additive manufacturing (AM) of metallic materials in the framework of integrated computational materials engineering (ICME) is discussed. In the first part of the review, process simulation (P-S linkage), structure modelling (S-P linkage), property simulation (S-P linkage), and integrated modelling (PSP and PSPP linkages) are elaborated considering different physical phenomena (multi-physics) in AM and at micro/meso/macro scales (multi-scale modelling). The second part provides an extensive discussion of a data-driven framework, which involves extracting existing data from databases and texts, data pre-processing, high throughput screening, and, therefore, database construction. A data-driven workflow that integrates statistical methods, including ML, artificial intelligence (AI), and neural network (NN) models, has great potential for completing PSPP linkages. This review paper provides an insight for both academic and industrial researchers, working on the AM of metallic materials.},
 author = {Seyed Mahdi Hashemi and Soroush Parvizi and Haniyeh Baghbanijavid and Alvin T. L. Tan and Mohammadreza Nematollahi and Ali Ramazani and Nicholas X. Fang and Mohammad Elahinia},
 doi = {10.1080/09506608.2020.1868889},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/09506608.2020.1868889},
 journal = {International Materials Reviews},
 number = {1},
 pages = {1–46},
 title = {Computational modelling of process–structure–property–performance relationships in metal additive manufacturing: a review},
 url = {https://doi-org.crai.referencistas.com/10.1080/09506608.2020.1868889},
 volume = {67},
 year = {2022e}
}

@article{doi:10.1080/09506608.2023.2169501,
 abstract = {This review article provides a critical assessment of the progress made in computational modelling of metal-based additive manufacturing (AM) with emphasis on its ability to predict physical phenomena, concepts of microstructural evolution, residual stresses, role of multiple thermal cycles, and formation of multi-dimensional defects along with the achieved degree of experimental validation. The uniqueness of this article stems from the inclusion of comprehensive information on computational progress in the field of fusion-based, sintering-based, and mechanical deformation-based AM. A computational model’s role in determining the process framework for the desired outcome of the set properties of the AM components is recognised while presenting the process-microstructure maps, thereby appraising computational ability towards the qualification of products. The inclusion of a detailed discussion on the bi-directional coupling of machine learning and physics-based computational models provides a futuristic roadmap for the digital twin of metal-based AM.},
 author = {Shashank Sharma and Sameehan S. Joshi and Mangesh V. Pantawane and Madhavan Radhakrishnan and Sangram Mazumder and Narendra B. Dahotre},
 doi = {10.1080/09506608.2023.2169501},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/09506608.2023.2169501},
 journal = {International Materials Reviews},
 number = {7},
 pages = {943–1009},
 title = {Multiphysics multi-scale computational framework for linking process–structure–property relationships in metal additive manufacturing: a critical review},
 url = {https://doi-org.crai.referencistas.com/10.1080/09506608.2023.2169501},
 volume = {68},
 year = {2023o}
}

@article{doi:10.1080/10915810701876737,
 author = {John A. Budny},
 doi = {10.1080/10915810701876737},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/10915810701876737},
 journal = {International Journal of Toxicology},
 number = {1},
 pages = {167–168},
 title = {Computational Toxicology—Risk Assessment for Pharmaceutical and Environmental Chemicals Edited by Sean Ekins Publisher: Wiley-Interscience, John Wiley & Sons, Inc. 2007. 814 pages ISBN: 978-0-470-04962-4. 814 pages. Price: $140.00.},
 url = {https://doi-org.crai.referencistas.com/10.1080/10915810701876737},
 volume = {27},
 year = {2008c}
}

@article{doi:10.1080/11356405.2017.1305075,
 abstract = {The pedagogical integration of computing is interesting in educational contexts based on the contributions of Seymour Papert and Wing’s concept of computational thinking. Integrating arts in education can lead to the design of activities using Scratch combined with devices. The main goal is to evaluate the integration of computational thinking in art education making use of technological resources, sensor cards and minicomputers, with a student-centred pedagogical approach. This research assesses the results of a control group of 35 students and an experimental group of 109 students in four different schools, using Mann-Whitney’s U-test for independent samples assessing ‘Active Learning’, ‘computational concepts’ and ‘fun’ scales. Applying data triangulation, and consistent with design-based research, the results of interviews and focus groups reinforced the results obtained in the aforementioned test, providing validity to the study. There are advantages regarding student interest, motivation and commitment related to programming technologies in art and education, particularly pedagogical sessions with music. Handling devices, sensors and Raspberry Pi provides participating students with a factor of commitment and enthusiasm, with significant improvements. Working with coding and devices brings an additional advantage in the development of computational thinking and digital competence. The results show an increase in creativity and artistic competence related to the ability to create music from the activities and technological resources described in the technological intervention.},
 author = {José-Manuel Sáez-López and María-Luisa Sevillano-García},
 doi = {10.1080/11356405.2017.1305075},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/11356405.2017.1305075},
 journal = {Culture and Education},
 number = {2},
 pages = {350–384},
 title = {Sensors, programming and devices in Art Education sessions. One case in the context of primary education / Sensores, programación y dispositivos en sesiones de Educación Artística. Un caso en el contexto de Educación Primaria},
 url = {https://doi-org.crai.referencistas.com/10.1080/11356405.2017.1305075},
 volume = {29},
 year = {2017k}
}

@article{doi:10.1080/14640748208400855,
 abstract = {There are two conflicting views about the nature of thought: it is invariably rational or invariably irrational. Bartlett argued that thinking is a high level skill, and this idea suggests an obvious third possibility: thought is sometimes rational and sometimes irrational. This view is defended in the present paper, which argues that the doctrine of logical infallibility is either falsified by the results of some experiments on syllogistic reasoning or else empirically vacuous. There is no need to postulate a mental logic of the sort that Piaget and others have proposed. The rapid implicit inferences of daily life depend on the ability to interpret sentences by constructing mental models of the states of affairs that they describe. Deliberate deductions depend on the further ability to search for alternative models that violate putative conclusions. All that you need to know to assess validity is the fundamental semantic principle of deduction: an inference is valid if, and only if, its conclusion is true in every situation in which its premises are true and there is no way of interpreting the premises so as to render the conclusion false. This principle guides the construction of all logics though it is not explicitly stated in any of them. The paper concludes by examining the ways in which people differ in their ability to reason, the practical need to improve this ability, and some of the pedagogical implications of the present studies.},
 author = {P. N. Johnson-Laird},
 doi = {10.1080/14640748208400855},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/14640748208400855},
 journal = {The Quarterly Journal of Experimental Psychology Section A},
 number = {1},
 pages = {1–29},
 title = {Ninth Bartlett Memorial Lecture. Thinking as a Skill},
 url = {https://doi-org.crai.referencistas.com/10.1080/14640748208400855},
 volume = {34},
 year = {1982g}
}

@article{doi:10.1080/14640748408401509,
 abstract = {This exciting and original book describes the work of David Marr and his colleagues at MIT on the computational theory of vision, particularly the early stages of shape analysis. Although much of the theorising is frankly speculative and not certain to survive without major modification, it sets new standards of rigour in its formal approach and is certain to have a stimulating effect upon the field. Sadly, Marr’s death from leukaemia at the age of 35 means that he will not himself be able to take part in the further development of his ideas, but the book itself will exert an important influence for some time to come. The first point to make, for those who may already have attempted Marr’s lengthy and difficult research papers, is that the book is clearly and entertainingly written: It can be tackled by the non-specialist who wishes to see why Marr’s work has caused such interest in the vision community. Despite the very difficult circumstances in which the book must have been written, it is carefully put together and gives a fascinating insight into the way in which the author’s ideas developed.},
 author = {M. J. Morgan},
 doi = {10.1080/14640748408401509},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/14640748408401509},
 journal = {The Quarterly Journal of Experimental Psychology Section A},
 number = {1},
 pages = {157–165},
 title = {Computational Theories of Vision},
 url = {https://doi-org.crai.referencistas.com/10.1080/14640748408401509},
 volume = {36},
 year = {1984o}
}

@article{doi:10.1080/14640749008401229b,
 author = {George Mather},
 doi = {10.1080/14640749008401229b},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/14640749008401229b},
 journal = {The Quarterly Journal of Experimental Psychology Section A},
 number = {2},
 pages = {427–429},
 title = {Book Review: Visual Processing: Computational, Psychophysical and Cognitive Research},
 url = {https://doi-org.crai.referencistas.com/10.1080/14640749008401229b},
 volume = {42},
 year = {1990l}
}

@article{doi:10.1080/14640749408401123a,
 author = {Geoffrey Hall},
 doi = {10.1080/14640749408401123a},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/14640749408401123a},
 journal = {The Quarterly Journal of Experimental Psychology Section A},
 number = {2},
 pages = {518–520},
 title = {Book Review: Learning and Computational Neuroscience: Foundations of Adaptive Networks},
 url = {https://doi-org.crai.referencistas.com/10.1080/14640749408401123a},
 volume = {47},
 year = {1994h}
}

@article{doi:10.1080/17470210903156586,
 abstract = {The masked onset priming effect (MOPE) refers to the empirical finding that target naming is faster when the target (SIB) is preceded by a briefly presented masked prime that starts with the same letter/phoneme (suf) than when it does not (mof; Kinoshita, 2000, Experiment 1). The dual-route cascaded (DRC) computational model of reading (Coltheart, Rastle, Perry, Langdon, & Ziegler, 2001) has offered an explanation for how the MOPE might occur in humans. However, there has been some empirical discrepancy regarding whether for nonword items the effect is limited to the first-letter/phoneme overlap between primes and targets or whether orthographic/phonological priming effects occur beyond the first letter/phoneme. Experiment 1 tested these two possibilities. The human results, which were successfully simulated by the DRC model, showed priming beyond the first letter/phoneme. Nevertheless, two recent versions of the DRC model made different predictions regarding the nature of these priming effects. Experiment 2 examined whether it is facilitatory, inhibitory, or both, in order to adjudicate between the two versions of the model. The human results showed that primes exert both facilitatory and inhibitory effects.},
 author = {Petroula Mousikou and Max Coltheart and Matthew Finkbeiner and Steven Saunders},
 doi = {10.1080/17470210903156586},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/17470210903156586},
 journal = {Quarterly Journal of Experimental Psychology},
 note = {PMID:19742386},
 number = {5},
 pages = {984–1003},
 title = {Can the dual-route cascaded computational model of reading offer a valid account of the masked onset priming effect?},
 url = {https://doi-org.crai.referencistas.com/10.1080/17470210903156586},
 volume = {63},
 year = {2010o}
}

@article{doi:10.1080/17470215208416607,
 author = {F. C. B.},
 doi = {10.1080/17470215208416607},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/17470215208416607},
 journal = {Quarterly Journal of Experimental Psychology},
 number = {2},
 pages = {87–90},
 title = {Book Review: Thinking: An Introduction to its Experimental Psychology},
 url = {https://doi-org.crai.referencistas.com/10.1080/17470215208416607},
 volume = {4},
 year = {1952f}
}

@article{doi:10.1080/17470218.2014.911925,
 abstract = {This study investigated the computational cost associated with grammatical planning in sentence production. We measured people’s pupillary responses as they produced spoken descriptions of depicted events. We manipulated the syntactic structure of the target by training subjects to use different types of sentences following a colour cue. The results showed higher increase in pupil size for the production of passive and object dislocated sentences than for active canonical subject–verb–object sentences, indicating that more cognitive effort is associated with more complex noncanonical thematic order. We also manipulated the time at which the cue that triggered structure-building processes was presented. Differential increase in pupil diameter for more complex sentences was shown to rise earlier as the colour cue was presented earlier, suggesting that the observed pupillary changes are due to differential demands in relatively independent structure-building processes during grammatical planning. Task-evoked pupillary responses provide a reliable measure to study the cognitive processes involved in sentence production.},
 author = {Yamila Sevilla and Mora Maldonado and Diego E. Shalóm},
 doi = {10.1080/17470218.2014.911925},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/17470218.2014.911925},
 journal = {Quarterly Journal of Experimental Psychology},
 note = {PMID:24712982},
 number = {6},
 pages = {1041–1052},
 title = {Pupillary Dynamics Reveal Computational Cost in Sentence Planning},
 url = {https://doi-org.crai.referencistas.com/10.1080/17470218.2014.911925},
 volume = {67},
 year = {2014q}
}

@article{doi:10.1080/17470218.2014.943673,
 abstract = {This article reviews some of the recent work on the remarkable cognitive capacities of food-caching corvids. The focus will be on their ability to think about other minds and other times, and tool-using tests of physical problem solving. Research on developmental cognition suggests that young children do not pass similar tests until they are at least four years of age in the case of the social cognition experiments, and eight years of age in the case of the tasks that tap into physical cognition. This developmental trajectory seems surprising. Intuitively, one might have thought that the social and planning tasks required more complex forms of cognitive process, namely Mental Time Travel and Theory of Mind. Perhaps the fact that children pass these tasks earlier than the physical problem-solving tasks is a reflection of cultural influences. Future research will hope to identify these cognitive milestones by starting to develop tasks that might go some way towards understanding the mechanisms underlying these abilities in both children and corvids, to explore similarities and differences in their ways of thinking.},
 author = {Nicola S. Clayton},
 doi = {10.1080/17470218.2014.943673},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/17470218.2014.943673},
 journal = {Quarterly Journal of Experimental Psychology},
 note = {PMID:25207993},
 number = {2},
 pages = {209–241},
 title = {EPS Mid-Career Award 2013: Ways of thinking: From crows to children and back again},
 url = {https://doi-org.crai.referencistas.com/10.1080/17470218.2014.943673},
 volume = {68},
 year = {2015f}
}

@article{doi:10.1080/17470218.2015.1134602,
 abstract = {Swets et al. (2008. Underspecification of syntactic ambiguities: Evidence from self-paced reading. Memory and Cognition, 36(1), 201–216) presented evidence that the so-called ambiguity advantage [Traxler et al. (1998). Adjunct attachment is not a form of lexical ambiguity resolution. Journal of Memory and Language, 39(4), 558–592], which has been explained in terms of the Unrestricted Race Model, can equally well be explained by assuming underspecification in ambiguous conditions driven by task-demands. Specifically, if comprehension questions require that ambiguities be resolved, the parser tends to make an attachment: when questions are about superficial aspects of the target sentence, readers tend to pursue an underspecification strategy. It is reasonable to assume that individual differences in strategy will play a significant role in the application of such strategies, so that studying average behaviour may not be informative. In order to study the predictions of the good-enough processing theory, we implemented two versions of underspecification: the partial specification model (PSM), which is an implementation of the Swets et al. proposal, and a more parsimonious version, the non-specification model (NSM). We evaluate the relative fit of these two kinds of underspecification to Swets et al.’s data; as a baseline, we also fitted three models that assume no underspecification. We find that a model without underspecification provides a somewhat better fit than both underspecification models, while the NSM model provides a better fit than the PSM. We interpret the results as lack of unambiguous evidence in favour of underspecification; however, given that there is considerable existing evidence for good-enough processing in the literature, it is reasonable to assume that some underspecification might occur. Under this assumption, the results can be interpreted as tentative evidence for NSM over PSM. More generally, our work provides a method for choosing between models of real-time processes in sentence comprehension that make qualitative predictions about the relationship between several dependent variables. We believe that sentence processing research will greatly benefit from a wider use of such methods.},
 author = {Pavel Logačev and Shravan Vasishth},
 doi = {10.1080/17470218.2015.1134602},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/17470218.2015.1134602},
 journal = {Quarterly Journal of Experimental Psychology},
 note = {PMID:26960441},
 number = {5},
 pages = {996–1012},
 title = {Understanding underspecification: A comparison of two computational implementations},
 url = {https://doi-org.crai.referencistas.com/10.1080/17470218.2015.1134602},
 volume = {69},
 year = {2016k}
}

@article{doi:10.1080/19462166.2010.485698,
 abstract = {This paper reports our research concerning dialogue strategies suitable for adoption by a human–computer debating system. We propose a set of strategic heuristics for a computer to adopt to enable it to function as a dialogue participant. In particular, we consider means of assessing the proposed strategy. A system involving two agents in dialogue with each other and a human–agent debate system are constructed and subsequently used to facilitate the evaluations. The evaluations suggest that the proposed strategy can enable the computer to act as an effective dialogue participant. It is anticipated that this work will contribute towards the development of computerised dialogue systems and help to illuminate research issues concerning strategies in dialectical systems.},
 author = {Tangming Yuan and David Moore and Alec Grierson},
 doi = {10.1080/19462166.2010.485698},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/19462166.2010.485698},
 journal = {Argument & Computation},
 number = {3},
 pages = {215–248},
 title = {Assessing debate strategies via computational agents},
 url = {https://doi-org.crai.referencistas.com/10.1080/19462166.2010.485698},
 volume = {1},
 year = {2010s}
}

@article{doi:10.1080/19462166.2011.608225,
 abstract = {Interaction mining is about discovering and extracting insightful information from digital conversations, namely those human–human information exchanges mediated by digital network technology. We present in this article a computational model of natural arguments and its implementation for the automatic argumentative analysis of digital conversations, which allows us to produce relevant information to build interaction business analytics applications overcoming the limitations of standard text mining and information retrieval technology. Applications include advanced visualisations and abstractive summaries.},
 author = {Vincenzo Pallotta and Rodolfo Delmonte},
 doi = {10.1080/19462166.2011.608225},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/19462166.2011.608225},
 journal = {Argument & Computation},
 number = {2–3},
 pages = {77–106},
 title = {Automatic argumentative analysis for interaction mining},
 url = {https://doi-org.crai.referencistas.com/10.1080/19462166.2011.608225},
 volume = {2},
 year = {2011o}
}

@article{doi:10.1080/19462166.2012.663409,
 abstract = {John Pollock (1940–2009) was an influential American philosopher who made important contributions to various fields, including epistemology and cognitive science. In the last 25 years of his life, he also contributed to the computational study of defeasible reasoning and practical cognition in artificial intelligence. He developed one of the first formal systems for argumentation-based inference and he put many issues on the research agenda that are still relevant for the argumentation community today. This paper presents an appreciation of Pollock’s work on defeasible reasoning and its relevance for the computational study of argument. In our opinion, Pollock deserves to be remembered as one of the founding fathers of the field of computational argument, while, moreover, his work contains important lessons for current research in this field, reminding us of the richness of its object of study.},
 author = {Henry Prakken and John Horty},
 doi = {10.1080/19462166.2012.663409},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/19462166.2012.663409},
 journal = {Argument & Computation},
 number = {1},
 pages = {1–19},
 title = {An appreciation of John Pollock’s work on the computational study of argument},
 url = {https://doi-org.crai.referencistas.com/10.1080/19462166.2012.663409},
 volume = {3},
 year = {2012q}
}

@article{doi:10.1080/25726641.2019.1708657,
 abstract = {Comminution and classification are the two major unit operations involved in the processing of pure minerals from its ore rocks. In the current paper, an assessment is made on different numerical models used for the prediction of fluid and solid flow properties in tumbling mill, hydrocyclone and dense medium cyclone (DMC). A detailed discussion on the selection of suitable turbulence and multiphase models for the accurate prediction of flow field in hydrocyclone is made by comparing the predictions among and against experimental data. The additional requirements for accurate performance predictions at high feed solid content is elaborated. The drawbacks of DPM model and the usage of CFD-DEM coupling technique to predict the coal partition curve in DMC’s has been elucidated. The discrepancies between DEM, CFD, one way and two way coupled CFD-DEM predicted mean flow field and particle dynamics against experimental measurements in tumbling mills also made in detail.},
 author = {Narasimha Mangadoddy and Teja Reddy Vakamalla and Mayank Kumar and Aubrey Mainza},
 doi = {10.1080/25726641.2019.1708657},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/25726641.2019.1708657},
 journal = {Mineral Processing and Extractive Metallurgy},
 number = {2},
 pages = {145–156},
 title = {Computational modelling of particle-fluid dynamics in comminution and classification: a review},
 url = {https://doi-org.crai.referencistas.com/10.1080/25726641.2019.1708657},
 volume = {129},
 year = {2020k}
}

@article{doi:10.1080/25726668.2018.1436957,
 abstract = {The ball mill is usually the largest energy consumer at a mine site and significantly affects operational expenditures. Given a target particle size, Bond Mill Work Index estimates are used to predict a ball mill’s throughput. In order to maximize ball mill throughput and optimize energy utilization, it is important to get these estimates right. At the Tropicana Gold Mine, Work Index estimates, derived from X-Ray Fluorescence and Hyperspectral scanning of Grade Control samples, are used to construct spatial GeoMetallurgical models (GeoMet). Inaccuracies in block estimates exist due to limited calibration between grade control derived and laboratory Work Index values. To improve the calibration, an updating algorithm has been tested at the Tropicana Gold Mine. The aim of the study was to demonstrate a new process for updating block estimates using actual mill performance data. Deviations between predicted and actual mill performance are monitored and used to locally improve the Work Index estimates in the GeoMet model. The updating algorithm improves the spatial Work Index estimates, resulting in a real-time reconciliation of already extracted blocks and a recalibration of future scheduled blocks. The case study shows that historic and future production estimates improve on average by about 72 and 26%.},
 author = {T. Wambeke and D. Elder and A. Miller and J. Benndorf and R. Peattie},
 doi = {10.1080/25726668.2018.1436957},
 eprint = {https://doi-org.crai.referencistas.com/10.1080/25726668.2018.1436957},
 journal = {Mining Technology},
 number = {3},
 pages = {115–130},
 title = {Real-time reconciliation of a geometallurgical model based on ball mill performance measurements – a pilot study at the Tropicana gold mine},
 url = {https://doi-org.crai.referencistas.com/10.1080/25726668.2018.1436957},
 volume = {127},
 year = {2018s}
}

@article{doi:10.1093/jicru_1.3.65,
 author = {W. G. Alberts and D. T. Bartlett and J.-L. Chartier and C. R. Hirning and J. C. McDonald and H. Schraube and R. B. Schwartz},
 doi = {10.1093/jicru_1.3.65},
 eprint = {https://doi-org.crai.referencistas.com/10.1093/jicru_1.3.65},
 journal = {Journal of the ICRU},
 number = {3},
 pages = {65–71},
 title = {Appendix 1: Role of Computational Methods in the Determination of Operational Dose Equivalent Quantities for Neutrons},
 url = {https://doi-org.crai.referencistas.com/10.1093/jicru_1.3.65},
 volume = {1},
 year = {2001a}
}

@article{doi:10.1106/F9W7-XGV5-9PME-D5CX,
 abstract = {In the present paper we describe and demonstrate a computationally efficient technique for analyzing fracture mechanics problems in mixed linear-nonlinear systems. The technique combines the methodology of Rybicki and Kanninen for calculation of the energy release rate in fracture processes with a decomposition based analysis procedure recently proposed by Subbarayan and co-workers. The methodology will enable quick design decisions during the package development stages without significant loss of accuracy. The developed procedure is demonstrated on a hypothetical 5 × 5 array package. It is shown that on this representative package nearly 30% time savings (or a 150% speed-up) can be acheived in estimating the energy release rate at an accuracy loss of only 6.2%. Prior research has shown that the analysis time is nearly independent of package size, indicating unbounded speed-ups for larger package sizes.},
 author = {Devendra Natekar and Ganesh Subbarayan},
 doi = {10.1106/F9W7-XGV5-9PME-D5CX},
 eprint = {https://doi-org.crai.referencistas.com/10.1106/F9W7-XGV5-9PME-D5CX},
 journal = {International Journal of Damage Mechanics},
 number = {2},
 pages = {171–186},
 title = {Computationally Efficient Fracture Analysis of Electronic Packages through Decomposition},
 url = {https://doi-org.crai.referencistas.com/10.1106/F9W7-XGV5-9PME-D5CX},
 volume = {10},
 year = {2001m}
}

@article{doi:10.1111/1467-8721.00131,
 abstract = {The fundamental nature of learning is a central problem in psychology. Traditionally, psychologists have assumed that learning must involve the formation of associations. Early last century, the pioneering work of Pavlov on conditioned learning in animals seemed to put this assumption beyond doubt. More recently, many psychologists came to believe that a different kind of process must underlie complex learning, such as language learning in humans, and that this process must be described as computational rather than associative. Whether complex human learning is associative or computational continues to be a subject of intense research. The articles in this Special Section turn this debate on its head by asking whether simple animal learning is associative or computational. Surprisingly, the question is still very much open, and excitingly, it appears quite tractable.},
 author = {Alan M. Leslie},
 doi = {10.1111/1467-8721.00131},
 eprint = {https://doi-org.crai.referencistas.com/10.1111/1467-8721.00131},
 journal = {Current Directions in Psychological Science},
 number = {4},
 pages = {124–127},
 title = {Learning: Association or Computation? Introduction to a Special Section},
 url = {https://doi-org.crai.referencistas.com/10.1111/1467-8721.00131},
 volume = {10},
 year = {2001k}
}

@article{doi:10.1111/1467-8721.00132,
 abstract = {Causal learning enables humans and other animals not only to predict important events or outcomes, but also to control their occurrence in the service of needs and desires. Computational theories assume that causal judgments are based on an estimate of the contingency between a causal cue and an outcome. However, human causal learning exhibits many of the characteristics of the associative learning processes thought to underlie animal conditioning. One problem for associative theory arises from the finding that judgments of the causal power of a cue can be revalued retrospectively after learning episodes when that cue is not present. However, if retrieved representations of cues can support learning, retrospective revaluation is anticipated by modified versions of standard associative theories.},
 author = {Anthony Dickinson},
 doi = {10.1111/1467-8721.00132},
 eprint = {https://doi-org.crai.referencistas.com/10.1111/1467-8721.00132},
 journal = {Current Directions in Psychological Science},
 number = {4},
 pages = {127–132},
 title = {Causal Learning: Association Versus Computation},
 url = {https://doi-org.crai.referencistas.com/10.1111/1467-8721.00132},
 volume = {10},
 year = {2001a}
}

@article{doi:10.1111/1467-8721.00133,
 abstract = {A Turing test is proposed to evaluate current computational and associative models of learning, and to guide theoretical developments. This test requires a specification of the procedures to which the model applies, a sampling of procedures and response measures, and an objective way to determine the difficulty of discriminating the responses of the model from the responses of the animal. Scalar timing theory is used as an example of a well-developed computational theory of timing that involves addition, multiplication, division, and sampling. The behavioral theory of timing is used as an example of a well-developed associative theory of timing that involves state transitions and strengthening of connections. A Turing test provides a way to evaluate such theories.},
 author = {Russell M. Church},
 doi = {10.1111/1467-8721.00133},
 eprint = {https://doi-org.crai.referencistas.com/10.1111/1467-8721.00133},
 journal = {Current Directions in Psychological Science},
 number = {4},
 pages = {132–136},
 title = {A Turing Test for Computational and Associative Theories of Learning},
 url = {https://doi-org.crai.referencistas.com/10.1111/1467-8721.00133},
 volume = {10},
 year = {2001c}
}

@article{doi:10.1111/1467-9280.00129,
 abstract = {On May 23, 1997, the National Institute on Drug Abuse and the American Psychological Society co-sponsored a conference titled “Cognitive Sciences Research: More Than Thinking About Drug Abuse.” The conference highlighted important lines of research, both within and outside of drug abuse, that may elucidate the relationships between substance abuse and cognitive processes. This Special Section of Psychological Science presents a compilation of articles from that conference by scientists who are working in the forefront of this exciting new research area. The research questions posed by these articles take the following forms: What are the cognitive and developmental effects (i.e., the consequences) of substance abuse? What are the antecedents or precursors of drug use that render persons vulnerable to taking drugs? How do the effects of drugs, in turn, become antecedents for changes in perception, behavior, and cognition that further enhance vulnerability to drugs?},
 author = {Jaylan S. Turkkan and David Shurtleff},
 doi = {10.1111/1467-9280.00129},
 eprint = {https://doi-org.crai.referencistas.com/10.1111/1467-9280.00129},
 journal = {Psychological Science},
 number = {3},
 pages = {179–180},
 title = {Cognitive Sciences Research: More Than Thinking About Drug Abuse},
 url = {https://doi-org.crai.referencistas.com/10.1111/1467-9280.00129},
 volume = {10},
 year = {1999r}
}

@article{doi:10.1111/1467-954X.12002,
 abstract = {This article explores the role of images in the workings of contemporary power. It examines one of the central ways in which sociology has approached images as representations and proposes an alternative understanding of images through the concepts of interactivity, intensity and the virtual. Focusing on the examples of three interactive mirrors, one a piece of artwork, another designed to be located in a designer shop and the other a medical mirror for tracking ‘vital signs’, it suggests that the mirrors emphasize the screen and, in so doing, disrupt a notion of images of representations. Images are instead brought to life; intensively experienced rather than extensively read. The article engages, first, with the increasing prevalence of screens and, second, with the moves in sociology towards theorizing the value of the concept of the virtual. Arguing that images are felt and lived out, the article seeks to contribute to how sociology has dealt with, and might further develop, the concept of the virtual as a productive way of understanding the relationships between images, screens, power and life.},
 author = {Rebecca Coleman},
 doi = {10.1111/1467-954X.12002},
 eprint = {https://doi-org.crai.referencistas.com/10.1111/1467-954X.12002},
 journal = {The Sociological Review},
 number = {1},
 pages = {1–20},
 title = {Sociology and the Virtual: Interactive Mirrors, Representational Thinking and Intensive Power},
 url = {https://doi-org.crai.referencistas.com/10.1111/1467-954X.12002},
 volume = {61},
 year = {2013b}
}

@article{doi:10.1111/j.1467-9280.1994.tb00625.x,
 abstract = {Traditional theories of cognitive development predict that children progress from intuitive to computational thinking, whereas fuzzy-trace theory makes the opposite prediction To evaluate these alternatives, framing problems were administered to preschoolers, second graders, and fifth graders Consistent with fuzzy-trace theory, results indicated (a) that younger children focused on quantitative differences between outcomes and did not exhibit framing effects (risk avoidance for gains, risk seeking for losses) and (b) that older children assimilated these quantitative differences and displayed framing effects},
 author = {Valerie F Reyna and Susan C Ellis},
 doi = {10.1111/j.1467-9280.1994.tb00625.x},
 eprint = {https://doi-org.crai.referencistas.com/10.1111/j.1467-9280.1994.tb00625.x},
 journal = {Psychological Science},
 number = {5},
 pages = {275–279},
 title = {Fuzzy-Trace Theory and Framing Effects in Children’s Risky Decision Making},
 url = {https://doi-org.crai.referencistas.com/10.1111/j.1467-9280.1994.tb00625.x},
 volume = {5},
 year = {1994r}
}

@article{doi:10.1111/j.1540-5826.2006.00220.x,
 abstract = {The purpose of this study was to examine the relations of various cognitive abilities and aspects of math performance with computational estimation skill among third graders. Students (n= 315) were assessed on language, nonverbal reasoning, concept formation, processing speed, long–term memory, working memory, inattentive behavior, basic reading skill, arithmetic number combination skill, double–digit computation skill, and computational estimation ability. One–way analysis of variance indicated significant differences in estimation skill among students of low, average, and high math computation performance. The unique predictors of estimation skill were arithmetic number combination skill, nonverbal reasoning, concept formation, working memory, and inattentive behavior.},
 author = {Pamela M. Seethaler and Lynn S. Fuchs},
 doi = {10.1111/j.1540-5826.2006.00220.x},
 eprint = {https://doi-org.crai.referencistas.com/10.1111/j.1540-5826.2006.00220.x},
 journal = {Learning Disabilities Research & Practice},
 number = {4},
 pages = {233–243},
 title = {The Cognitive Correlates of Computational Estimation Skill among Third–Grade Students},
 url = {https://doi-org.crai.referencistas.com/10.1111/j.1540-5826.2006.00220.x},
 volume = {21},
 year = {2006p}
}

@article{doi:10.1111/j.1745-6916.2006.00007.x,
 abstract = {We present a theory about human thought named the unconscious-thought theory (UTT). The theory is applicable to decision making, impression formation, attitude formation and change, problem solving, and creativity. It distinguishes between two modes of thought: unconscious and conscious. Unconscious thought and conscious thought have different characteristics, and these different characteristics make each mode preferable under different circumstances. For instance, contrary to popular belief, decisions about simple issues can be better tackled by conscious thought, whereas decisions about complex matters can be better approached with unconscious thought. The relations between the theory and decision strategies, and between the theory and intuition, are discussed. We end by discussing caveats and future directions.},
 author = {Ap Dijksterhuis and Loran F. Nordgren},
 doi = {10.1111/j.1745-6916.2006.00007.x},
 eprint = {https://doi-org.crai.referencistas.com/10.1111/j.1745-6916.2006.00007.x},
 journal = {Perspectives on Psychological Science},
 note = {PMID:26151465},
 number = {2},
 pages = {95–109},
 title = {A Theory of Unconscious Thought},
 url = {https://doi-org.crai.referencistas.com/10.1111/j.1745-6916.2006.00007.x},
 volume = {1},
 year = {2006d}
}

@article{doi:10.1111/ldrp.12237,
 abstract = {The present study employed a think–aloud method to explore the origin of centrality deficit (i.e., poor recall of central ideas) in individuals with Attention Deficit Hyperactivity Disorder (ADHD). Moreover, utilizing the diverse think–aloud responses, we examined the overall quality of text processing employed by individuals with ADHD during reading, in order to shed more light on text–level deficiencies underlying their poor comprehension after reading. To address these goals, adolescents with and without ADHD were asked to state aloud whatever comes to their minds during the reading of two expository texts. After reading, the participants freely recalled text ideas and answered multiple–choice questions on the texts. Compared to controls, participants with ADHD generated fewer responses that reflect deep, efficient text processing, and reinstated fewer prior text ideas, particularly central ones, during reading. Moreover, the proportions of deep processing responses positively associated with participants’ performance on recall and comprehension tasks. These findings suggest that individuals with ADHD exhibit poor text comprehension and memory, particularly of central ideas, because they construct a low–quality, less–connected text representation during reading, and produce fewer, less–elaborated retrieval cues for subsequent tasks after reading.},
 author = {Menahem Yeari and Anat Lavie},
 doi = {10.1111/ldrp.12237},
 eprint = {https://doi-org.crai.referencistas.com/10.1111/ldrp.12237},
 journal = {Learning Disabilities Research & Practice},
 number = {1},
 pages = {40–55},
 title = {The Role of Surface Text Processing in Centrality Deficit and Poor Text Comprehension of Adolescents with Attention Deficit Hyperactivity Disorder: A Think–Aloud Study},
 url = {https://doi-org.crai.referencistas.com/10.1111/ldrp.12237},
 volume = {36},
 year = {2021t}
}

@article{doi:10.1111/poms.13971,
 abstract = {Discrete games provide the means to analyze market dynamics with limited data. However, computing such games with many players—especially in a complete information setting—is computationally infeasible because the strategy space increases exponentially with the number of players. This study presents a novel and practical method to compute and estimate discrete games. To do so, the study introduces two methodological innovations. First, we develop an efficient simulator that requires fewer random draws to evaluate the likelihood of discrete games with multiple equilibria. The augmented simulator avoids random draws that are not compatible with the observed equilibrium outcome and, thus, efficiently uses all draws to evaluate the likelihood. Second, we utilize general‐purpose computing on graphics‐processing unit (GPGPU), using multiple processing cores in a graphics‐processing unit, to increase computational speed. The two features allow us to estimate the model significantly faster compared to traditional methods. The study’s empirical application examines the effect of Apple’s company‐owned stores on the retail market structure. The results show that agglomeration effects exist between Apple and upscale firms. The presence of an Apple store attracts high‐income customers, promoting the entry of upscale firms and the exit of discount firms.},
 author = {Doug J. Chung and Kyoungwon Seo and Reo Song},
 doi = {10.1111/poms.13971},
 eprint = {https://doi-org.crai.referencistas.com/10.1111/poms.13971},
 journal = {Production and Operations Management},
 number = {7},
 pages = {2245–2263},
 title = {Efficient computation of discrete games: Estimating the effect of Apple on market structure},
 url = {https://doi-org.crai.referencistas.com/10.1111/poms.13971},
 volume = {32},
 year = {2023f}
}

@article{doi:10.1155/2013/125291,
 abstract = {A multicriteria decision-making model was proposed in order to acquire the optimum one among different product design schemes. VIKOR method was introduced to compute the ranking value of each scheme. A multiobjective optimization model for criteria weight was established. In this model, projection pursuit method was employed to identify a criteria weight set which could keep classification information of original schemes to the greatest extent, while PROMETHEE II was adopted to keep sorting information. Dominance based multiobjective simulated annealing algorithm (D-MOSA) was introduced to solve the optimization model. Finally, an example was taken to demonstrate the feasibility and efficiency of this model.},
 author = {Yi-Xiong Feng and Yi-Cong Gao and Xuan Song and Jian-Rong Tan},
 doi = {10.1155/2013/125291},
 eprint = {https://doi-org.crai.referencistas.com/10.1155/2013/125291},
 journal = {Advances in Mechanical Engineering},
 number = { },
 pages = {125291},
 title = {Equilibrium Design Based on Design Thinking Solving: An Integrated Multicriteria Decision-Making Methodology},
 url = {https://doi-org.crai.referencistas.com/10.1155/2013/125291},
 volume = {5},
 year = {2013d}
}

@article{doi:10.1155/2013/847612,
 abstract = {Condition-based monitoring (CBM) has advanced to the stage where industry is now demanding machinery that possesses self-diagnosis ability. This need has spurred the CBM research to be applicable in more expanded areas over the past decades. There are two critical issues in implementing CBM in harsh environments using embedded systems: computational efficiency and adaptability. In this paper, a computationally efficient and adaptive approach including simple principal component analysis (SPCA) for feature dimensionality reduction and K-means clustering for classification is proposed for online embedded machinery diagnosis. Compared with the standard principal component analysis (PCA) and kernel principal component analysis (KPCA), SPCA is adaptive in nature and has lower algorithm complexity when dealing with a large amount of data. The effectiveness of the proposed approach is firstly validated using a standard rolling element bearing test dataset on a personal computer. It is then deployed on an embedded real-time controller and used to monitor a rotating shaft. It was found that the proposed approach scaled well, whereas the standard PCA-based approach broke down when data quantity increased to a certain level. Furthermore, the proposed approach achieved 90% accuracy when diagnosing an induced fault compared to 59% accuracy obtained using the standard PCA-based approach.},
 author = {Chuan Jiang and Samuel H. Huang},
 doi = {10.1155/2013/847612},
 eprint = {https://doi-org.crai.referencistas.com/10.1155/2013/847612},
 journal = {Advances in Mechanical Engineering},
 number = { },
 pages = {847612},
 title = {A Computationally Efficient and Adaptive Approach for Online Embedded Machinery Diagnosis in Harsh Environments},
 url = {https://doi-org.crai.referencistas.com/10.1155/2013/847612},
 volume = {5},
 year = {2013i}
}

@article{doi:10.1155/2013/857941,
 abstract = {We analyze and compare three different computational methods for a solar heating system with seasonal water tank heat storage (SHS-SWTHS). These methods are accurate numerical method, temperature stratification method, and uniform temperature method. The accurate numerical method can accurately predict the performance of the system, but it takes about 4 to 5 weeks, which is too long and hard for the performance analysis of this system. The temperature stratification method obtains relatively accurate computation results and takes a relatively short computation time, which is about 2 to 3 hours. Therefore, this method is most suitable for the performance analysis of this system. The deviation of the computational results of the uniform temperature method is great, and the time consumed is similar to that of the temperature stratification method. Therefore, this method is not recommended herein. Based on the above analyses, the temperature stratification method is applied to analyze the influence of the embedded depth of water tank, the thickness of thermal insulation material, and the collection area on the performance of this system. The results will provide a design basis for the related demonstration projects.},
 author = {Dongliang Sun and Jinliang Xu and Peng Ding},
 doi = {10.1155/2013/857941},
 eprint = {https://doi-org.crai.referencistas.com/10.1155/2013/857941},
 journal = {Advances in Mechanical Engineering},
 number = { },
 pages = {857941},
 title = {Performance Analysis and Application of Three Different Computational Methods for Solar Heating System with Seasonal Water Tank Heat Storage},
 url = {https://doi-org.crai.referencistas.com/10.1155/2013/857941},
 volume = {5},
 year = {2013r}
}

@article{doi:10.1155/2013/983962,
 abstract = {We consider a system of coupled partial differential equations describing transient fluid flow and heat transfer with variable flow properties. Classical Lie point symmetry analysis of this system resulted in admitted large Lie algebras for some special cases of the arbitrary constants and the source term. Symmetry reductions are performed and as such the system of partial differential equations is reduced to the system of ordinary differential equations. Some reduced ordinary differential equation could be solved exactly with restrictions on the parameters appearing in it. In addition, shooting quadrature is employed to numerically tackle the nonlinear model boundary value problem and pertinent results are presented graphically and discussed quantitatively.},
 author = {R. J. Moitsheki and O. D. Makinde},
 doi = {10.1155/2013/983962},
 eprint = {https://doi-org.crai.referencistas.com/10.1155/2013/983962},
 journal = {Advances in Mechanical Engineering},
 number = { },
 pages = {983962},
 title = {Computational Modelling and Similarity Reduction of Equations for Transient Fluid Flow and Heat Transfer with Variable Properties},
 url = {https://doi-org.crai.referencistas.com/10.1155/2013/983962},
 volume = {5},
 year = {2013m}
}

@article{doi:10.1155/2014/245924,
 abstract = {Data collection in wireless sensor networks (WSNs) can become extremely expensive in terms of power consumption if all measurements have to be fetched. However, since multiple applications do not require data from all nodes but to compute a function over a smaller data set, much of the available data on the network can be considered irrelevant and not worthy of spending energy. In this context, in-network filtering schemes can be used to forward only relevant data towards a sink node for processing purposes. In this work, we propose and evaluate two schemes that can drive this filtering process. Both of them are based on the integration of metaheuristics and learning algorithms inspired by nature. In particular, we consider the computation of the maximum function as case study for these schemes. We investigate the trade-off between communications costs, which are directly associated with power consumption, and error costs due to fetching not all relevant data. We show by simulation that communication costs can be significantly reduced with respect to traditional schemes while keeping the computation error bounded.},
 author = {Guillermo G. Riva and Jorge M. Finochietto},
 doi = {10.1155/2014/245924},
 eprint = {https://doi-org.crai.referencistas.com/10.1155/2014/245924},
 journal = {International Journal of Distributed Sensor Networks},
 number = {8},
 pages = {245924},
 title = {In-Network Filtering Schemes for Type-Threshold Function Computation in Wireless Sensor Networks},
 url = {https://doi-org.crai.referencistas.com/10.1155/2014/245924},
 volume = {10},
 year = {2014p}
}

@article{doi:10.1155/2014/815378,
 abstract = {This paper describes a hardware architecture for real-time image component labeling and the computation of image component feature descriptors. These descriptors are object related properties used to describe each image component. Embedded machine vision systems demand a robust performance and power efficiency as well as minimum area utilization, depending on the deployed application. In the proposed architecture, the hardware modules for component labeling and feature calculation run in parallel. A CMOS image sensor (MT9V032), operating at a maximum clock frequency of 27 MHz, was used to capture the images. The architecture was synthesized and implemented on a Xilinx Spartan-6 FPGA. The developed architecture is capable of processing 390 video frames per second of size 640 × 480 pixels. Dynamic power consumption is 13 mW at 86 frames per second.},
 author = {Abdul Waheed Malik and Benny Thörnberg and Muhammad Imran and Najeem Lawal},
 doi = {10.1155/2014/815378},
 eprint = {https://doi-org.crai.referencistas.com/10.1155/2014/815378},
 journal = {International Journal of Distributed Sensor Networks},
 number = {1},
 pages = {815378},
 title = {Hardware Architecture for Real-Time Computation of Image Component Feature Descriptors on a FPGA},
 url = {https://doi-org.crai.referencistas.com/10.1155/2014/815378},
 volume = {10},
 year = {2014k}
}

@article{doi:10.1155/2015/475150,
 abstract = {Scenarios in which two nodes who distrust each other in wireless sensor networks (WSNs) would like to know the distance between them are considered. The scenario is designed to protect the private information of WSNs, in this case each node’s location, from the other nodes and from a passive attacker. The goal of the present work is to provide two novel and secure two-party distance computation protocols based on a semihonest model, the first with aid of a third party and the second based on randomization technique. Both of these protocols can extend the calculated value into a real number field. The output of the distance computation and the intermediate values in the proposed protocols are also private and not accessible to a third party or any other attackers. When executing these two protocols, security is guaranteed, and the performances of communication and computation of them are found to be satisfactory when compared to those of other similar protocols.},
 author = {Haiping Huang and Tianhe Gong and Ping Chen and Guoxia Qiu and Ruchuan Wang},
 doi = {10.1155/2015/475150},
 eprint = {https://doi-org.crai.referencistas.com/10.1155/2015/475150},
 journal = {International Journal of Distributed Sensor Networks},
 number = {7},
 pages = {475150},
 title = {Secure Two-Party Distance Computation Protocols with a Semihonest Third Party and Randomization for Privacy Protection in Wireless Sensor Networks},
 url = {https://doi-org.crai.referencistas.com/10.1155/2015/475150},
 volume = {11},
 year = {2015g}
}

@article{doi:10.1155/2022/7874826,
 abstract = {This study was aimed at evaluating the artificial neural network (ANN), genetic algorithm (GA), adaptive neurofuzzy interference (ANFIS), and the response surface methodology (RSM) approaches for modeling and optimizing the simultaneous adsorptive removal of chemical oxygen demand (COD) and total organic carbon (TOC) in produced water (PW) using tea waste biochar (TWBC). Comparative analysis of RSM, ANN, and ANFIS models showed mean square error (MSE) as 5.29809, 1.49937, and 0.24164 for adsorption of COD and MSE of 0.11726, 0.10241, and 0.08747 for prediction of TOC adsorption, respectively. The study showed that ANFIS outperformed the ANN and RSM in terms of fast convergence, minimum MSE, and sum of square error for prediction of adsorption data. The adsorption parameters were optimized using ANFIS-surface plots, ANN-GA hybrid, RSM-GA hybrid, and RSM optimization tool in design expert (DE) software. Maximum COD (88.9%) and TOC (98.8%) removal were predicted at pH of 7, a dosage of 300 mg/L, and contact time of 60 mins using ANFIS-surface plots. The optimization approaches showed the performance in the following order: ANFIS-surface plots>ANN-GA>RSM-GA>RSM.},
 author = {Areej Alhothali and Hifsa Khurshid and Muhammad Raza Ul Mustafa and Kawthar Mostafa Moria and Umer Rashid and Omaimah Omar Bamasag and George Kyzas},
 doi = {10.1155/2022/7874826},
 eprint = {https://doi-org.crai.referencistas.com/10.1155/2022/7874826},
 journal = {Adsorption Science & Technology},
 number = { },
 pages = {7874826},
 title = {Evaluation of Contemporary Computational Techniques to Optimize Adsorption Process for Simultaneous Removal of COD and TOC in Wastewater},
 url = {https://doi-org.crai.referencistas.com/10.1155/2022/7874826},
 volume = {2022},
 year = {2022b}
}

@article{doi:10.1163/22125868-12340075,
 abstract = {The 21st century era of rapidly changing technology entails cognizance of the changing nature of knowledge, learning and environments. New models of knowledge building and knowledge co-creation are emerging. Personalized learning takes on new dimensions with mobile devices and new tools for sharing and meta-thinking. Evidences from research in learning sciences and neurosciences point to the importance of understanding human cognition and behaviors in optimizing the use of technology for learning. Future learning entails a powerful use of the cognitive propensity to learn by imitation and modeling as well as the novelty of inquiry and creation. Didactics are replaced by conversational learning with social media as powerful platforms. Apart from the analytics and logic, future learning incorporates big picture thinking, multiple perspective thinking and connective thinking to flourish problem-solving and creativity. The address will conclude with some implications for design and practice.},
 author = {Oon Seng Tan},
 doi = {10.1163/22125868-12340075},
 eprint = {https://doi-org.crai.referencistas.com/10.1163/22125868-12340075},
 journal = {International Journal of Chinese Education},
 number = {1},
 pages = {81–104},
 title = {Technology, Future Learning and Flourishing Thinking},
 url = {https://doi-org.crai.referencistas.com/10.1163/22125868-12340075},
 volume = {6},
 year = {2017n}
}

@article{doi:10.1177/0002716215569174,
 abstract = {Over the past few years, we have seen the emergence of “big data”: disruptive technologies that have transformed commerce, science, and many aspects of society. Despite the tremendous enthusiasm for big data, there is no shortage of detractors. This article argues that many criticisms stem from a fundamental confusion over goals: whether the desired outcome of big data use is “better science” or “better engineering.” Critics point to the rejection of traditional data collection and analysis methods, confusion between correlation and causation, and an indifference to models with explanatory power. From the perspective of advancing social science, these are valid reservations. I contend, however, that if the end goal of big data use is to engineer computational artifacts that are more effective according to well-defined metrics, then whatever improves those metrics should be exploited without prejudice. Sound scientific reasoning, while helpful, is not necessary to improve engineering. Understanding the distinction between science and engineering resolves many of the apparent controversies surrounding big data and helps to clarify the criteria by which contributions should be assessed.},
 author = {Jimmy Lin},
 doi = {10.1177/0002716215569174},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0002716215569174},
 journal = {The ANNALS of the American Academy of Political and Social Science},
 number = {1},
 pages = {33–47},
 title = {On Building Better Mousetraps and Understanding the Human Condition: Reflections on Big Data in the Social Sciences},
 url = {https://doi-org.crai.referencistas.com/10.1177/0002716215569174},
 volume = {659},
 year = {2015m}
}

@article{doi:10.1177/0002716215569220,
 abstract = {There is considerable controversy surrounding the study of presidential debates, particularly efforts to connect their content and impact. Research has long debated whether the citizenry reacts to what candidates say, how they say it, or simply how they appear. This study uses detailed coding of the first 2012 debate between Barack Obama and Mitt Romney to test the relative influence of the candidates’ verbal persuasiveness and nonverbal features on viewers’ “second screen” behavior—their use of computers, tablets, and mobile phones to enhance or extend the televised viewing experience. To examine these relationships, we merged two datasets: (1) a shot-by-shot content analysis coded for functional, tonal, and visual elements of both candidates’ communication behavior during the debate; and (2) corresponding real-time measures, synched and lagged, of the volume and sentiment of Twitter expression about Obama and Romney. We find the candidates’ facial expressions and physical gestures to be more consistent and robust predictors of the volume and valence of Twitter expression than candidates’ persuasive strategies, verbal utterances, and voice tone during the debate.},
 author = {Dhavan V. Shah and Alex Hanna and Erik P. Bucy and Chris Wells and Vidal Quevedo},
 doi = {10.1177/0002716215569220},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0002716215569220},
 journal = {The ANNALS of the American Academy of Political and Social Science},
 number = {1},
 pages = {225–245},
 title = {The Power of Television Images in a Social Media Age: Linking Biobehavioral and Computational Approaches via the Second Screen},
 url = {https://doi-org.crai.referencistas.com/10.1177/0002716215569220},
 volume = {659},
 year = {2015r}
}

@article{doi:10.1177/0002716215569446,
 abstract = {Methods for analyzing neural and computational social science data are usually used by different types of scientists and generally seen as distinct, but they strongly complement one another. Computational social science methodologies can strengthen and contextualize individual-level analysis, specifically our understanding of the brain. Neuroscience can help to unpack the mechanisms that lead from micro- through meso- to macro-level observations. Integrating levels of analysis is essential to unified progress in social research. We present two example areas that illustrate this integration. First, combining egocentric social network data with neural variables from the “egos” provides insight about why and for whom certain types of antismoking messages may be more or less effective. Second, combining tools from natural language processing with neuroimaging reveals mechanisms involved in successful message propagation, and suggests links from microscopic to macroscopic scales.},
 author = {Matthew Brook O’Donnell and Emily B. Falk},
 doi = {10.1177/0002716215569446},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0002716215569446},
 journal = {The ANNALS of the American Academy of Political and Social Science},
 number = {1},
 pages = {274–289},
 title = {Big Data under the Microscope and Brains in Social Context: Integrating Methods from Computational Social Science and Neuroscience},
 url = {https://doi-org.crai.referencistas.com/10.1177/0002716215569446},
 volume = {659},
 year = {2015r}
}

@article{doi:10.1177/0002716215570576,
 abstract = {To deal with ever-larger datasets, media scholars are increasingly using computational analytic methods. This article focuses on how the traditional (manual) approach to conducting a content analysis—a primary method in the study of media messages—is being reconfigured, assesses what is gained and lost in turning to computational solutions, and builds on a “hybrid” approach to content analysis. We argue that computational methods are most fruitful when variables are readily identifiable in texts and when source material is easily parsed. Manual methods, though, are most appropriate for complex variables and when source material is not well digitized. These modes can be effectively combined throughout the process of content analysis to facilitate expansive and powerful analyses that are reliable and meaningful.},
 author = {Rodrigo Zamith and Seth C. Lewis},
 doi = {10.1177/0002716215570576},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0002716215570576},
 journal = {The ANNALS of the American Academy of Political and Social Science},
 number = {1},
 pages = {307–318},
 title = {Content Analysis and the Algorithmic Coder: What Computational Social Science Means for Traditional Modes of Media Analysis},
 url = {https://doi-org.crai.referencistas.com/10.1177/0002716215570576},
 volume = {659},
 year = {2015r}
}

@article{doi:10.1177/00030651030510030901,
 author = {Arnold Goldberg},
 doi = {10.1177/00030651030510030901},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00030651030510030901},
 journal = {Journal of the American Psychoanalytic Association},
 number = {3},
 pages = {995–1003},
 title = {Thinking About the Mind},
 url = {https://doi-org.crai.referencistas.com/10.1177/00030651030510030901},
 volume = {51},
 year = {2003j}
}

@article{doi:10.1177/00030651070550020501,
 abstract = {The relationship between psychoanalysis and attachment theory is complex indeed. A brief review of the psychoanalytic literature as it concerns attachment theory and research, and of the attachment literature as it pertains to psychoanalytic ideas, demonstrates an increasing interest in attachment theory within psychoanalysis. Some of the difficulties that attachment theory faces in relation to psychoanalytic ideas are traced to its links to the now dated cognitive science of the 1960s and 1970s. Today, however, a second-generation cognitive neuroscience seeks neurobiologically plausible accounts in which links with brain and body are seen as shaping mind and consciousness, which increasingly are seen as “embodied,” as emerging from or serving the needs of a physical being located in a specific time, place, and social context. This idea has also been at the core of much psychoanalytic thinking, which has historically affirmed the rootedness of symbolic thought in sensory, emotional, and enacted experience with objects. Now neurobiological advances supporting the concept of embodied cognition offer an opportunity to forge powerful links between the hitherto separate domains of attachment theory and psychoanalysis. Speculations about the nature of language are presented that emphasize the origin of internal working models (and of representations in general) in early sensorimotor and emotional experiences with a caregiver. It is argued that language and symbolic thought may be phylogenetically and ontogenetically embodied, built on a foundation of gestures and actions, and are thus profoundly influenced by the experience of early physical interaction with the primary object. Finally, the clinical and research implications of these ideas are discussed.},
 author = {Peter Fonagy and Mary Target},
 doi = {10.1177/00030651070550020501},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00030651070550020501},
 journal = {Journal of the American Psychoanalytic Association},
 note = {PMID:17601099},
 number = {2},
 pages = {411–456},
 title = {The Rooting of the Mind in the Body: New Links Between Attachment Theory and Psychoanalytic Thought},
 url = {https://doi-org.crai.referencistas.com/10.1177/00030651070550020501},
 volume = {55},
 year = {2007h}
}

@article{doi:10.1177/000306515800600404,
 author = {Victor H. Rosen},
 doi = {10.1177/000306515800600404},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/000306515800600404},
 journal = {Journal of the American Psychoanalytic Association},
 note = {PMID:13587360},
 number = {4},
 pages = {653–671},
 title = {Abstract Thinking and Object Relations: With Special Reference to the Use of Abstraction as a Regressive Defense in Highly Gifted Individuals1},
 url = {https://doi-org.crai.referencistas.com/10.1177/000306515800600404},
 volume = {6},
 year = {1958r}
}

@article{doi:10.1177/0003489419842217,
 abstract = {Objectives: Competent velopharyngeal (VP) function is the basis for normal speech. Understanding how VP structure influences the airflow during speech details is essential to the surgical improvement of pharyngoplasty. In this study, we aimed to illuminate the airflow features corresponding to various VP closure states using computed dynamic simulations. Methods: Three-dimensional models of the upper airways were established based on computed tomography of 8 volunteers. The velopharyngeal port was simulated by a cylinder. Computational fluid dynamics simulations were applied to illustrate the correlation between the VP port size and the airflow parameters, including the flow velocity, pressure in the velopharyngeal port, as well as the pressure in oral and nasal cavity. Results: The airflow dynamics at the velopharynx were maintained in the same velopharyngeal pattern as the area of the velopharyngeal port increased from 0 to 25 mm2. A total of 5 airflow patterns with distinct features were captured, corresponding to adequate closure, adequate/borderline closure (Class I and II), borderline/inadequate closure, and inadequate closure. The maximal orifice area that could be tolerated for adequate VP closure was determined to be 2.01 mm2. Conclusion: Different VP functions are of characteristic airflow dynamic features. Computational fluid dynamic simulation is of application potential in individualized VP surgery planning.},
 author = {Hanyao Huang and Xu Cheng and Yang Wang and Dantong Huang and Yuhao Wei and Heng Yin and Bing Shi and Jingtao Li},
 doi = {10.1177/0003489419842217},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0003489419842217},
 journal = {Annals of Otology, Rhinology & Laryngology},
 note = {PMID:30957524},
 number = {8},
 pages = {742–748},
 title = {Analysis of Velopharyngeal Functions Using Computational Fluid Dynamics Simulations},
 url = {https://doi-org.crai.referencistas.com/10.1177/0003489419842217},
 volume = {128},
 year = {2019j}
}

@article{doi:10.1177/0008068319750110,
 abstract = {A computational procedure is presented for the approximation of the density of a linear combination of univariate -generalized normal random variables. (The -generalized normal random variable generalizes the ordinary normal one by replacing the power two in the exponent of the density by an arbitrary positive number.) The procedure applies a truncated form of the Fourier Inversion Theorem to the power series expansion of the characteristic function of a -generalized normal random variable. Because of the unimodal nature of -generalized normal characteristic functions for ⩽ 2 and the oscillatory nature for > 2, much of the computational procedure divides into two corresponding parts. Complete error analysis and accuracy control in all computations are also presented.},
 author = {Ru-Ying Lee and I. R. Goodman},
 doi = {10.1177/0008068319750110},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0008068319750110},
 journal = {Calcutta Statistical Association Bulletin},
 number = {1–4},
 pages = {101–116},
 title = {Computation of Density for A Linear Combination of –Generalized Normal Random Variables},
 url = {https://doi-org.crai.referencistas.com/10.1177/0008068319750110},
 volume = {24},
 year = {1975l}
}

@article{doi:10.1177/0011000005283558,
 abstract = {Effect sizes are critical to result interpretation and synthesis across studies. Although statistical significance testing has historically dominated the determination of result importance, modern views emphasize the role of effect sizes and confidence intervals. This article accessibly discusses how to calculate and interpret the effect sizes that counseling psychologists use most frequently. To provide context, the author presents a brief history of statistical significance tests. Second, the author discusses the difference between statistical, practical, and clinical significance. Third, the author reviews and graphically demonstrates two common types of effect sizes, commenting on multivariate and corrected effect sizes. Fourth, the author emphasizes meta-analytic thinking and the potential role of confidence intervals around effect sizes. Finally, the author gives a hypothetical example of how to report and potentially interpret some effect sizes.},
 author = {Robin K. Henson},
 doi = {10.1177/0011000005283558},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0011000005283558},
 journal = {The Counseling Psychologist},
 number = {5},
 pages = {601–629},
 title = {Effect-Size Measures and Meta-Analytic Thinking in Counseling Psychology Research},
 url = {https://doi-org.crai.referencistas.com/10.1177/0011000005283558},
 volume = {34},
 year = {2006i}
}

@article{doi:10.1177/001112876200800302,
 abstract = {This article presents the highlights of the author’s survey of current thought in the United States on the use of prediction statistics in parole selection and the extent to which prediction tables are used or have been used by paroling authorities in each of the fifty states. Also presented are the views of leading criminologists, prison administrators, and parole board members about prediction devices and their place in parole selection.},
 author = {Victor H. Evjen},
 doi = {10.1177/001112876200800302},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/001112876200800302},
 journal = {Crime & Delinquency},
 number = {3},
 pages = {215–238},
 title = {Current Thinking on Parole Prediction Tables},
 url = {https://doi-org.crai.referencistas.com/10.1177/001112876200800302},
 volume = {8},
 year = {1962f}
}

@article{doi:10.1177/00131245241229666,
 abstract = {With the introduction and implementation of core literacy, scientific thinking (ST) has become an essential goal and key dimension of science teaching. At present, there is no agreement on how to cultivate students’ ST. This study took 238 sixth grade students from a public primary school in urban China as research sample, built a theoretical model of scientific thinking development based on the theory of the Bronfenbrenner’ ecological systems theory, and used multiple data to explore and analyze the impact path of ST development of primary school students using the fuzzy sets of qualitative comparative analysis method (fsQCA). The development of urban primary school students’ ST is the result of multiple factors at the level of individual drive, family environment, school teaching, and social resources. The result of data analysis showed that the influence path of ST development of urban primary school students driven by multiple factors includes three paths: parent participation leading, scientific practice leading, and home-school-community integration. We have interpreted the influence mechanism of each path in detail and put forward the enlightenment for science education policy and practice.},
 author = {Lin Lin and Danhua Zhou and Xinyi Hu and Jingying Wang and Yu Wang},
 doi = {10.1177/00131245241229666},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00131245241229666},
 journal = {Education and Urban Society},
 number = {7},
 pages = {906–927},
 title = {Multiple Factors Drive the Development of Scientific Thinking in Urban Primary School Students of China: FsQCA Analysis Based on the Ecological Systems Theory},
 url = {https://doi-org.crai.referencistas.com/10.1177/00131245241229666},
 volume = {56},
 year = {2024g}
}

@article{doi:10.1177/0013164404266386,
 abstract = {In 1997, noting that the 50th anniversary of the publication of “Coefficient Alpha and the Internal Structure of Tests” was fast approaching, Lee Cronbach planned what have become the notes published here. His aimwas to point out theways in which his views on coefficient alpha had evolved, doubting nowthat the coefficientwas the bestway of judging the reliability of an instrument to which it was applied. Tracing in these notes, in vintage Cronbach style, his thinking before, during, and after the publication of the alpha paper, his “current thoughts” on coefficient alpha are that alpha covers only a small perspective of the range of measurement uses for which reliability information is needed and that it should be viewed within a much larger system of reliability analysis, generalizability theory.},
 author = {Lee J. Cronbach and Richard J. Shavelson},
 doi = {10.1177/0013164404266386},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0013164404266386},
 journal = {Educational and Psychological Measurement},
 number = {3},
 pages = {391–418},
 title = {My Current Thoughts on Coefficient Alpha and Successor Procedures},
 url = {https://doi-org.crai.referencistas.com/10.1177/0013164404266386},
 volume = {64},
 year = {2004d}
}

@article{doi:10.1177/00131644211023569,
 abstract = {Setting cutoff scores is one of the most common practices when using scales to aid in classification purposes. This process is usually done univariately where each optimal cutoff value is decided sequentially, subscale by subscale. While it is widely known that this process necessarily reduces the probability of “passing” such a test, what is not properly recognized is that such a test loses power to meaningfully discriminate between target groups with each new subscale that is introduced. We quantify and describe this property via an analytical exposition highlighting the counterintuitive geometry implied by marginal threshold-setting in multiple dimensions. Recommendations are presented that encourage applied researchers to think jointly, rather than marginally, when setting cutoff scores to ensure an informative test.},
 author = {Edward Kroc and Oscar L. Olvera Astivia},
 doi = {10.1177/00131644211023569},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00131644211023569},
 journal = {Educational and Psychological Measurement},
 note = {PMID:35444337},
 number = {3},
 pages = {517–538},
 title = {The Importance of Thinking Multivariately When Setting Subscale Cutoff Scores},
 url = {https://doi-org.crai.referencistas.com/10.1177/00131644211023569},
 volume = {82},
 year = {2022m}
}

@article{doi:10.1177/001316446802800432,
 author = {William M. Stallings},
 doi = {10.1177/001316446802800432},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/001316446802800432},
 journal = {Educational and Psychological Measurement},
 number = {4},
 pages = {1250–1254},
 title = {Book Reviews : James L. Bruning and B. L. Kintz. Computational Handbook of Statistics. Glenview, Ill.: Scott, Foresman, 1968. Pp. 269. $4.95 and $3.25 (paperback},
 url = {https://doi-org.crai.referencistas.com/10.1177/001316446802800432},
 volume = {28},
 year = {1968o}
}

@article{doi:10.1177/001316447303300422,
 author = {Dennis Hunt and Bikkar S. Randhawa},
 doi = {10.1177/001316447303300422},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/001316447303300422},
 journal = {Educational and Psychological Measurement},
 number = {4},
 pages = {921–928},
 title = {Relationship Between and Among Cognitive Variables and Achievement in Computational Science},
 url = {https://doi-org.crai.referencistas.com/10.1177/001316447303300422},
 volume = {33},
 year = {1973k}
}

@article{doi:10.1177/001316447803800318,
 abstract = {A FORTRAN G/H computer program was derived for an IBM series 360/370 computer system (or compatible systems such as Amdol or Honeywell) that provides at least 1536-K of core storage. This large amount of core storage is usually provided via a VS-1 or VS-2 virtual memory system. This program provides a factor analytic solution for large 3-dimensional data matrices. The computational procedures employed are based upon those presented in Method III of Tucker (1966).},
 author = {Thomas J. Zenisek},
 doi = {10.1177/001316447803800318},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/001316447803800318},
 journal = {Educational and Psychological Measurement},
 number = {3},
 pages = {787–792},
 title = {Three-Mode Factor Analysis Via a Modification of Tucker’s Computational Method—III},
 url = {https://doi-org.crai.referencistas.com/10.1177/001316447803800318},
 volume = {38},
 year = {1978r}
}

@article{doi:10.1177/0013916502238866,
 abstract = {Experts seem to find routes in complex environments by finding a connection from the source to a “skeleton” of major paths, then moving within the skeleton to the neighborhood of the destination, making a final connection to the destination. The authors present a computational hypothesis that describes the skeleton as emerging from the interaction of three factors: (a) The topological map is represented as a bipartite graph of places and paths, where a path is a one-dimensional ordered set of places; (b) a traveler incrementally accumulates topological relationships, including the relation of a place to a path serving as a dividing boundary separating two regions; and (c) the wayfinding algorithm prefers paths rich in boundary relations so they are likely to acquire more boundary relations. This positive-feedback loop leads to an oligarchy of paths rich in boundary relations. The authors present preliminary computational and empirical tests for this hypothesis, and provide initial results.},
 author = {Benjamin Kuipers and Dan G. Tecuci and Brian J. Stankiewicz},
 doi = {10.1177/0013916502238866},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0013916502238866},
 journal = {Environment and Behavior},
 number = {1},
 pages = {81–106},
 title = {The Skeleton In The Cognitive Map: A Computational and Empirical Exploration},
 url = {https://doi-org.crai.referencistas.com/10.1177/0013916502238866},
 volume = {35},
 year = {2003i}
}

@article{doi:10.1177/001440291007600403,
 abstract = {Middle school students with learning disabilities in math (MLD) used two versions of Enhanced Anchored Instruction (EAI). In one condition, students learned how to compute with fractions on an as-needed basis while they worked to solve the EAI problems. In the other condition, teachers used a computer-based instructional module in place of one of the EAI problems to deliver formal fraction instruction. The results indicated that students in both instructional formats improved their fraction computational skills and that formal instruction provided an added benefit. Both instructional conditions improved students’ problem-solving skills by about the same amount. The findings suggest that combining formal fraction instruction with EAI is a viable way to improve the problem-solving and computational skills of students with MLD.},
 author = {Brian A. Bottge and Enrique Rueda and Timothy S. Grant and Ana C. Stephens and Perry T. Laroque},
 doi = {10.1177/001440291007600403},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/001440291007600403},
 journal = {Exceptional Children},
 number = {4},
 pages = {417–437},
 title = {Anchoring Problem-Solving and Computation Instruction in Context-Rich Learning Environments},
 url = {https://doi-org.crai.referencistas.com/10.1177/001440291007600403},
 volume = {76},
 year = {2010b}
}

@article{doi:10.1177/001440291408000207,
 abstract = {This article describes a follow-up analysis of findings from a randomized study that tested the efficacy of a blended version of Enhanced Anchored Instruction (EAI) designed to improve both the computation and problem-solving performances of middle school students with disabilities. The goals of the secondary analysis were to track overall error patterns of students in computing with fractions and to compare the effects of EAI and Business As Usual (BAU) on making these errors. Results showed that students taught with EAI reduced their errors compared to students in BAU classrooms and that reducing the one common error led to improved performance. Error pattern analysis provided clues about how to modify instructional materials for improving computation with fractions.},
 author = {Brian A. Bottge and Xin Ma and Linda Gassaway and Mark Butler and Michael D. Toland},
 doi = {10.1177/001440291408000207},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/001440291408000207},
 journal = {Exceptional Children},
 number = {2},
 pages = {237–255},
 title = {Detecting and Correcting Fractions Computation Error Patterns},
 url = {https://doi-org.crai.referencistas.com/10.1177/001440291408000207},
 volume = {80},
 year = {2014b}
}

@article{doi:10.1177/001440296403100103,
 abstract = {Forty-two junior high level students were instructed in use of the soroban, a type of abacus. Prior to instruction, the students were tested with an easy test and a difficult test to determine their skill in computation of whole and decimal numbers. After four months and again after eight months of instruction and practice with the soroban, the students were tested with equivalent tests. The results demonstrated that the soroban is a practical and efficient approach for overcoming computational problems encountered by the blind.},
 author = {Carson Y. Nolan and June E. Morris},
 doi = {10.1177/001440296403100103},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/001440296403100103},
 journal = {Exceptional Children},
 note = {PMID:14276689},
 number = {1},
 pages = {15–18},
 title = {The Japanese Abacus as a Computational Aid for Blind Children},
 url = {https://doi-org.crai.referencistas.com/10.1177/001440296403100103},
 volume = {31},
 year = {1964m}
}

@article{doi:10.1177/00169862211061874,
 abstract = {In this study, we applied different text-mining methods to the originality scoring of the Unusual Uses Test (UUT) and Just Suppose Test (JST) from the Torrance Tests of Creative Thinking (TTCT)–Verbal. Responses from 102 and 123 participants who completed Form A and Form B, respectively, were scored using three different text-mining methods. The validity of these scoring methods was tested against TTCT’s manual-based scoring and a subjective snapshot scoring method. Results indicated that text-mining systems are applicable to both UUT and JST items across both forms and students’ performance on those items can predict total originality and creativity scores across all six tasks in the TTCT-Verbal. Comparatively, the text-mining methods worked better for UUT than JST. Of the three text-mining models we tested, the Global Vectors for Word Representation (GLoVe) model produced the most reliable and valid scores. These findings indicate that creativity assessment can be done quickly and at a lower cost using text-mining approaches.},
 author = {Selcuk Acar and Kelly Berthiaume and Katalin Grajzel and Denis Dumas and Charles “Tedd” Flemister and Peter Organisciak},
 doi = {10.1177/00169862211061874},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00169862211061874},
 journal = {Gifted Child Quarterly},
 number = {1},
 pages = {3–17},
 title = {Applying Automated Originality Scoring to the Verbal Form of Torrance Tests of Creative Thinking},
 url = {https://doi-org.crai.referencistas.com/10.1177/00169862211061874},
 volume = {67},
 year = {2023a}
}

@article{doi:10.1177/00169862231222886,
 abstract = {The Torrance Test of Creative Thinking (TTCT) is the most widely used norm-referenced creativity test used in gifted identification. Although commonly used for identifying talent, little is known about how creativity tests, like the TTCT-Figural, contribute to the probability of being identified as gifted especially with underrepresented populations. Using nominated students (n = 1,191) from a diverse midsized urban school district, this study examined the differential predictive validity among student demographics (i.e., race/ethnicity, free/reduced price lunch status, English learning status, sex) and the TTCT-Figural to the probability of being identified as gifted. Results of a multilevel hierarchical generalized linear regression indicated underrepresented groups showed no difference in the probability of being identified after controlling for cognitive ability and academic achievement; the same was true when the TTCT-Figural was included within the model. The inclusion of the TTCT-Figural does contribute to the probability of identification; however, the disproportionality of underrepresented student groups remains in this school district. Gifted administrators looking to enhance equity may not find the solution with the mere inclusion of a creativity assessment. Implications for practice and future directions are discussed.},
 author = {Lindsay Ellis Lee and Anne N. Rinn and Karen E. Rambo-Hernandez},
 doi = {10.1177/00169862231222886},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00169862231222886},
 journal = {Gifted Child Quarterly},
 number = {2},
 pages = {119–136},
 title = {What Happens After Nomination? Evaluating the Probability of Gifted Identification With the Torrance Test of Creative Thinking},
 url = {https://doi-org.crai.referencistas.com/10.1177/00169862231222886},
 volume = {68},
 year = {2024k}
}

@article{doi:10.1177/0018720819829949,
 abstract = {Objective: We developed a computational model of the effects of sleep deprivation on the vigilance decrement by employing the methods of system dynamics modeling. Background: Situations that require sustained attention for a prolonged duration can cause a decline in cognitive performance, the so-called vigilance decrement. One factor that should influence the vigilance decrement is fatigue in the form of sleep deprivation. Method: We employed the methods of system dynamics modeling (numerical-integration techniques for modeling complex feedback systems) to create a computational model of the vigilance decrement. We then simulated the computational effects of sleep deprivation on the behavior of that model, using empirical data obtained from the literature for calibrating such effects. Results: Sleep deprivation of 2 hr over a 14-day period should produce an additional decline of 9% in detection performance over that found with the typical vigilance decrement, whereas 4 hr of sleep deprivation over 14 days should produce an additional decline of 14% in detection performance. Conclusion: With respect to dual-process theory, it is through its deleterious effects on analytical cognition that sleep deprivation should impact the vigilance decrement. Application: Such computational modeling may be advantageous for human-machine teaming by theoretically allowing a future autonomous software agent to anticipate the decline of human performance and compensate accordingly.},
 author = {Robert Earl Patterson and Darrell Lochtefeld and Kathleen G. Larson and Amanda Christensen-Salem},
 doi = {10.1177/0018720819829949},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0018720819829949},
 journal = {Human Factors},
 note = {PMID:30908091},
 number = {7},
 pages = {1099–1111},
 title = {Computational Modeling of the Effects of Sleep Deprivation on the Vigilance Decrement},
 url = {https://doi-org.crai.referencistas.com/10.1177/0018720819829949},
 volume = {61},
 year = {2019n}
}

@article{doi:10.1177/0018720819875347,
 abstract = {Objective This paper aims to describe and test novel computational driver models, predicting drivers’ brake reaction times (BRTs) to different levels of lead vehicle braking, during driving with cruise control (CC) and during silent failures of adaptive cruise control (ACC). Background Validated computational models predicting BRTs to silent failures of automation are lacking but are important for assessing the safety benefits of automated driving. Method Two alternative models of driver response to silent ACC failures are proposed: a looming prediction model, assuming that drivers embody a generative model of ACC, and a lower gain model, assuming that drivers’ arousal decreases due to monitoring of the automated system. Predictions of BRTs issued by the models were tested using a driving simulator study. Results The driving simulator study confirmed the predictions of the models: (a) BRTs were significantly shorter with an increase in kinematic criticality, both during driving with CC and during driving with ACC; (b) BRTs were significantly delayed when driving with ACC compared with driving with CC. However, the predicted BRTs were longer than the ones observed, entailing a fitting of the models to the data from the study. Conclusion Both the looming prediction model and the lower gain model predict well the BRTs for the ACC driving condition. However, the looming prediction model has the advantage of being able to predict average BRTs using the exact same parameters as the model fitted to the CC driving data. Application Knowledge resulting from this research can be helpful for assessing the safety benefits of automated driving.},
 author = {Giulio Bianchi Piccinini and Esko Lehtonen and Fabio Forcolin and Johan Engström and Deike Albers and Gustav Markkula and Johan Lodin and Jesper Sandin},
 doi = {10.1177/0018720819875347},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0018720819875347},
 journal = {Human Factors},
 note = {PMID:31590570},
 number = {7},
 pages = {1212–1229},
 title = {How Do Drivers Respond to Silent Automation Failures? Driving Simulator Study and Comparison of Computational Driver Braking Models},
 url = {https://doi-org.crai.referencistas.com/10.1177/0018720819875347},
 volume = {62},
 year = {2020a}
}

@article{doi:10.1177/00187208221143028,
 abstract = {Objective This study develops a computational model to predict drivers’ response time and understand the underlying cognitive mechanism for freeway exiting takeovers in conditionally automated vehicles (AVs). Background Previous research has modeled drivers’ takeover response time in emergency scenarios that demand a quick response. However, existing models may not be applicable for scheduled, non-time-critical takeovers as drivers take longer to resume control when there is no time pressure. A model of driver response time in non-time-critical takeovers is lacking. Method A computational cognitive model of driver takeover response time is developed based on Queuing Network-Model Human Processor (QN-MHP) architecture. The model quantifies gaze redirection in response to takeover request (ToR), task prioritization, driver situation awareness, and driver trust to address the complexities of drivers’ takeover strategies when sufficient time budget exists. Results Experimental data of a preliminary driving simulator study were used to validate the model. The model accounted for 97% of the experimental takeover response time for freeway exiting. Conclusion The current model can successfully predict drivers’ response time for scheduled, non-time-critical freeway exiting takeovers in conditionally AVs. Application This model can be applied to the human-machine interface design with respect to ToR lead time for enhancing safe freeway exiting takeovers in conditionally AVs. It also provides a foundation for future modeling work towards an integrated driver model of freeway exiting takeover performance.},
 author = {Xiaomei Tan and Yiqi Zhang},
 doi = {10.1177/00187208221143028},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00187208221143028},
 journal = {Human Factors},
 note = {PMID:36473708},
 number = {5},
 pages = {1583–1599},
 title = {A Computational Cognitive Model of Driver Response Time for Scheduled Freeway Exiting Takeovers in Conditionally Automated Vehicles},
 url = {https://doi-org.crai.referencistas.com/10.1177/00187208221143028},
 volume = {66},
 year = {2024s}
}

@article{doi:10.1177/0018726704045835,
 abstract = {Seven years have passed since Nigel Nicholson published his manifesto for evolutionary psychology (EP) in Human Relations. Given EP’s continued popularity, this article undertakes a timely reappraisal of its assumptions and practical implications. In particular, it assesses EP’s claim to unify the social and natural sciences by establishing a foundation for psychology in the evolutionary biological sciences. I demonstrate that EP is found wanting in both these areas: it cannot satisfy the rigorous demands of experimental evolutionary biology and does not deal well with some of the key problems faced by mainstream psychologists. As a result, EP’s claims as they pertain to management and organizations are speculative and highly normative, despite vigorous protestations to the contrary.},
 author = {Graham Sewell},
 doi = {10.1177/0018726704045835},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0018726704045835},
 journal = {Human Relations},
 number = {8},
 pages = {923–955},
 title = {Yabba-Dabba-Doo! Evolutionary Psychology and the Rise of Flintstone Psychological Thinking in Organization and Management Studies},
 url = {https://doi-org.crai.referencistas.com/10.1177/0018726704045835},
 volume = {57},
 year = {2004q}
}

@article{doi:10.1177/002029400704000209,
 doi = {10.1177/002029400704000209},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/002029400704000209},
 journal = {Measurement and Control},
 number = {2},
 pages = {60–61},
 title = {Book Review: Intelligent Control Systems Using Computational Intelligence Techniques, Carbon Based Magnetism},
 url = {https://doi-org.crai.referencistas.com/10.1177/002029400704000209},
 volume = {40},
 year = {2007t}
}

@article{doi:10.1177/0020294019858108,
 abstract = {In our systematic mapping study, we examined 289 published works to determine which intelligent computing methods (e.g. Artificial Neural Networks, Machine Learning, and Fuzzy Logic) used by air-conditioning systems can provide energy savings and improve thermal comfort. Our goal was to identify which methods have been used most in research on the topic, which methods of data collection have been employed, and which areas of research have been empirical in nature. We observed the rules for literature reviews in identifying published works on databases (e.g. the Institute of Electrical and Electronics Engineers database, the Association for Computing Machinery Digital Library, SpringerLink, ScienceDirect, and Wiley Online Library) and classified identified works by topic. After excluding works according to the predefined criteria, we reviewed selected works according to the research parameters motivating our study. Results reveal that energy savings is the most frequently examined topic and that intelligent computing methods can be used to provide better indoor environments for occupants, with energy savings of up to 50%. The most common intelligent method used has been artificial neural networks, while sensors have been the tools most used to collect data, followed by searches of databases of experiments, simulations, and surveys accessed to validate the accuracy of findings.},
 author = {Mustafa Çakır and Akhan Akbulut and Yusuf Hatay Önen},
 doi = {10.1177/0020294019858108},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0020294019858108},
 journal = {Measurement and Control},
 number = {7–8},
 pages = {1084–1094},
 title = {Analysis of the use of computational intelligence techniques for air-conditioning systems: A systematic mapping study},
 url = {https://doi-org.crai.referencistas.com/10.1177/0020294019858108},
 volume = {52},
 year = {2019c}
}

@article{doi:10.1177/00207020241276532,
 abstract = {Think tanks can leverage global influence to manifest the soft power of their home countries. International social media is a useful channel for Chinese think tanks to spread their messages abroad and enhance their influence. In this study, we examine the think tank index report released by the University of Pennsylvania’s Think Tanks and Civil Societies Program (TTCSP). Through comparative analysis and content analysis, this study analyzes the differences between Chinese and American think tanks’ usage of and presence on Twitter (now called X). Compared with American think tanks, Chinese think tanks have much room for improvement. We recommend that Chinese think tanks should: construct network infrastructure to support the development of international social media in China; increase their participation on diverse international social media platforms; actively participate in globalized issues and promote Chinese issues; cultivate professionals with the skills required to manage international social media accounts for think tanks; and incorporate metrics on social media into their think tank evaluation mechanisms.},
 author = {Xiujuan Chen and Ye Han and Zhiqiang Zhang},
 doi = {10.1177/00207020241276532},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00207020241276532},
 journal = {International Journal},
 number = {3},
 pages = {344–368},
 title = {Chinese and American Think Tanks: Comparing International Social Media Communication Characteristics},
 url = {https://doi-org.crai.referencistas.com/10.1177/00207020241276532},
 volume = {79},
 year = {2024e}
}

@article{doi:10.1177/0020720918788722,
 author = {EA Zamora-Cárdenas and A Pizano-Martínez and JM Lozano-García and VJ Gutiérrez-Martínez and R Cisneros-Magaña},
 doi = {10.1177/0020720918788722},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0020720918788722},
 journal = {International Journal of Electrical Engineering & Education},
 number = {2},
 pages = {105–123},
 title = {Computational development of a practical educational tool for state estimation of power systems using the MATLAB optimization toolbox},
 url = {https://doi-org.crai.referencistas.com/10.1177/0020720918788722},
 volume = {56},
 year = {2019s}
}

@article{doi:10.1177/0020720918800438,
 author = {Teng-Hui Tseng and Yaming Tai and Shin-Ping Tsai and Yu-Liang Ting},
 doi = {10.1177/0020720918800438},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0020720918800438},
 journal = {International Journal of Electrical Engineering & Education},
 number = {0},
 pages = {0020720918800438},
 title = {Students’ self-authoring mobile App for integrative learning of STEM},
 url = {https://doi-org.crai.referencistas.com/10.1177/0020720918800438},
 volume = {0},
 year = {2018r}
}

@article{doi:10.1177/0020720919847542,
 author = {Chandershekhar Sharma and SC Jain and Arvind K Sharma},
 doi = {10.1177/0020720919847542},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0020720919847542},
 journal = {International Journal of Electrical Engineering & Education},
 number = {1_suppl},
 pages = {1434–1453},
 title = {RETRACTED: A quantitative risk analysis methodology for the security of web application database against SQL injection (SQLi) attacks utilizing fuzzy logic system as computational technique},
 url = {https://doi-org.crai.referencistas.com/10.1177/0020720919847542},
 volume = {60},
 year = {2023m}
}

@article{doi:10.1177/002072097801500321,
 author = {A. G. J. Macfarlane},
 doi = {10.1177/002072097801500321},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/002072097801500321},
 journal = {International Journal of Electrical Engineering & Education},
 number = {3},
 pages = {284–285},
 title = {Book Review: Matrix Computation for Engineers and Scientists},
 url = {https://doi-org.crai.referencistas.com/10.1177/002072097801500321},
 volume = {15},
 year = {1978j}
}

@article{doi:10.1177/002072099603300214,
 author = {W. F. Low},
 doi = {10.1177/002072099603300214},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/002072099603300214},
 journal = {International Journal of Electrical Engineering & Education},
 number = {2},
 pages = {186–186},
 title = {Book Review: Computational Magnetics},
 url = {https://doi-org.crai.referencistas.com/10.1177/002072099603300214},
 volume = {33},
 year = {1996m}
}

@article{doi:10.1177/00208523211007533,
 abstract = {The purpose of this article is twofold: to theoretically assess ideational and organizational explanatory factors in the adoption of artificial intelligence policies; and to examine the extent to which the European Union has managed to facilitate a coordinated artificial intelligence policy in the Nordic countries. The study utilizes a mixed-methods approach based on systematic web searching, systematic policy document analysis and key informant semi-structured interviews. The study finds that the European Union has utilized framing-based strategies to set an agenda for a coordinated European artificial intelligence policy. Moreover, the strategy has affected member-state artificial intelligence policies to the extent that key tenets of European Union artificial intelligence discourse have penetrated Nordic public documents. However, the extent to which the Nordic countries incorporate European Union artificial intelligence policy discourse diverges at the national level. Differentiated national organizational capacities among Nordic countries make the adoption of artificial intelligence policies divergent. This observation is theoretically accounted for through a conversation between organizational theory of public governance and discursive institutionalism. The study argues that the framing of European Union artificial intelligence policies is filtered through organizational structures among states. Points for practitioners The study illuminates how policymakers in the Nordic countries are affected by the European Union when crafting their own artificial intelligence policies. The European Commission profoundly influences the policymaking of member states and affiliated states through the policy strategy of policy framing. The Commission uses this soft measure to nudge member states to comply with the European Union policy framework. Second, the study shows how ‘organizations matter’: variation in national organizational capacities in the Nordic states contributes to variation in national policy adoption. Even though Nordic countries adopt European Union-level policy frames, their implementation is shaped by varying organizational capacities available at the national level.},
 author = {Frans af Malmborg and Jarle Trondal},
 doi = {10.1177/00208523211007533},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00208523211007533},
 journal = {International Review of Administrative Sciences},
 number = {1},
 pages = {39–58},
 title = {Discursive framing and organizational venues: mechanisms of artificial intelligence policy adoption},
 url = {https://doi-org.crai.referencistas.com/10.1177/00208523211007533},
 volume = {89},
 year = {2023q}
}

@article{doi:10.1177/002182869702800201,
 author = {José Chabás and Bernard R. Goldstein},
 doi = {10.1177/002182869702800201},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/002182869702800201},
 journal = {Journal for the History of Astronomy},
 number = {2},
 pages = {93–105},
 title = {Computational Astronomy: Five Centuries of Finding True Syzygy},
 url = {https://doi-org.crai.referencistas.com/10.1177/002182869702800201},
 volume = {28},
 year = {1997d}
}

@article{doi:10.1177/00218863231175508,
 abstract = {Sampling on the dependent variable is unlikely to be an effective way to learn and develop the strategy. Even so, organizations spend millions of dollars on processes such as Appreciative Inquiry that make inferences about how to adapt their strategies, routines, and practices based upon only successful examples. Two techniques that are common to this kind of learning process are searching solely for successful solutions and reframing search problems (e.g., unconditionally positive questions). We build a computational model by formalizing appreciative inquiry and comparing it with other, similar processes to understand their relative effectiveness. We find that the organizations simulated in our computational model almost always improved performance over time, despite learning solely from successful observations. Their relative effectiveness depended on the complexity of the problems, the number of iterations of learning, and how much the learning process preserved variety in potential solutions. These findings suggest that appreciative inquiry may be most effective when people take the cost and complexity of organizational problems into account before engaging in the learning process and adapt the process accordingly. These findings contribute to research on organizational learning by explaining why learners may benefit from structuring the way they communicate as they search, why reframing performance measures may dissolve search problems, and how designed organizational search enables managers to be more deliberate about organizational learning.},
 author = {Ryan W. Quinn and Bret Crane and Jared D. Harris and Andrew Manikas},
 doi = {10.1177/00218863231175508},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00218863231175508},
 journal = {The Journal of Applied Behavioral Science},
 number = {3},
 pages = {391–425},
 title = {Designed Organizational Search: A Comparative Analysis of Alternative Procedures for Learning from Success},
 url = {https://doi-org.crai.referencistas.com/10.1177/00218863231175508},
 volume = {59},
 year = {2023o}
}

@article{doi:10.1177/002193479903000204,
 author = {Hae-Seong Park and Scott Bauer},
 doi = {10.1177/002193479903000204},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/002193479903000204},
 journal = {Journal of Black Studies},
 number = {2},
 pages = {204–215},
 title = {Computational Mathematical Abilities of African American Girls},
 url = {https://doi-org.crai.referencistas.com/10.1177/002193479903000204},
 volume = {30},
 year = {1999n}
}

@article{doi:10.1177/0021998304038651,
 abstract = {This paper presents experimentally validated three-dimensional transient simulations of the thermal phenomena of the tape winding process, as well as a method to determine separately the heat transfer between the hot gas originating from a torch and the composite material. The computational model predicts the temperature of the incoming tape and the substrate during the winding process. Each numerical simulation is based on an explicit time integration scheme and covers the duration of the process. The simulation within each time step employs a steady-state model. This method takes into account the cyclic nonuniform heating of the material and the effect of the growing mass. The comparison of the simulation results with the experimental data shows good agreements. The experiments were performed with preconsolidated glass fiber-reinforced polypropylene tapes. The measurements were performed with infrared pyrometry. This technique can handle moving points during the entire process, and is nonintrusive.},
 author = {Yves M.P. Toso and Paolo Ermanni and Dimos Poulikakos},
 doi = {10.1177/0021998304038651},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0021998304038651},
 journal = {Journal of Composite Materials},
 number = {2},
 pages = {107–135},
 title = {Thermal Phenomena in Fiber-reinforced Thermoplastic Tape Winding Process: Computational Simulations and Experimental Validations},
 url = {https://doi-org.crai.referencistas.com/10.1177/0021998304038651},
 volume = {38},
 year = {2004q}
}

@article{doi:10.1177/0021998308094543,
 abstract = {A numerical approach using a finite element method (FEM) was performed in order to determine the dielectric constant (ε’) of BaTiO 3—epoxy composites. In order to diminish computational resources and analyse simple models, composite topology was represented by periodic structures based on FCC configurations, but introducing novel packaging protocols, defining the way composites are filled as particle concentration is increased. The dielectric response of these anisotropic and periodic structures was mathematically represented through a quasi-static approximation using the Laplace equation. The amount of inclusions was varied in order to represent diluted and concentrated systems and structures were assessed for the whole feasible range of volume fractions. The numerical results were compared with experimental data concluding that only packaging protocols that consider higher particle—particle interaction are suitable to represent the dielectric behavior of concentrated-composite materials.},
 author = {L. Ramajo and M. Reboredo and D. Santiago and M. Castro and D. Ramajo},
 doi = {10.1177/0021998308094543},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0021998308094543},
 journal = {Journal of Composite Materials},
 number = {19},
 pages = {2027–2037},
 title = {Computational Approach of Dielectric Permitivities in BaTiO3—Epoxy Composites},
 url = {https://doi-org.crai.referencistas.com/10.1177/0021998308094543},
 volume = {42},
 year = {2008m}
}

@article{doi:10.1177/0021998310369581,
 abstract = {In this article, mode I interlaminar fracture toughness (GIC) of Vectran-stitched laminated composite is determined experimentally and computa- tionally. Critical strain energy release rates are measured by performing double can- tilever beam test on composites stitched with Vectran as stitch fiber, and are found to increase with increasing stitch thread thickness and stitch density. It is also revealed that the relationship between GIC and stitch density or stitch thread volume fraction appears to be linear. Interlaminar tension test is conducted to identify important fracture behavior of a single Vectran stitch fiber thread. The finite-element (FE) model of the stitched composite incorporates the novel four-step stitch fracture pro- cess, namely, interfacial debonding, slack absorption, fiber breakage, and pullout friction. The FE predictions of load-displacement curves and critical mode I strain energy release rates show good agreement with the experimental results. The differ- ences in interdependent stitch mechanisms between moderately stitched and densely stitched composites are discussed.},
 author = {K.T. Tan and N. Watanabe and M. Sano and Y. Iwahori and H. Hoshi},
 doi = {10.1177/0021998310369581},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0021998310369581},
 journal = {Journal of Composite Materials},
 number = {26},
 pages = {3203–3229},
 title = {Interlaminar Fracture Toughness of Vectran-stitched Composites - Experimental and Computational Analysis},
 url = {https://doi-org.crai.referencistas.com/10.1177/0021998310369581},
 volume = {44},
 year = {2010t}
}

@article{doi:10.1177/0021998311410473,
 abstract = {A computational framework for the simulation of progressive failure in composite laminates is presented. The phantom-node method (a variation to the XFEM) is used for a mesh-independent representation of matrix cracks as straight discontinuities in the displacement field. Furthermore, interface elements are used for delamination and a continuum damage model for fiber failure. The framework is validated against experimental observations for open-hole tests and compact tension tests. It is shown that different failure mechanisms are captured well, which allows for the prediction of size effects.},
 author = {F. P. van der Meer and L. J. Sluys and S. R. Hallett and M. R. Wisnom},
 doi = {10.1177/0021998311410473},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0021998311410473},
 journal = {Journal of Composite Materials},
 number = {5},
 pages = {603–623},
 title = {Computational modeling of complex failure mechanisms in laminates},
 url = {https://doi-org.crai.referencistas.com/10.1177/0021998311410473},
 volume = {46},
 year = {2012o}
}

@article{doi:10.1177/0021998315594482,
 abstract = {This paper investigates the effects of clustering on the effective transverse thermal conductivity of unidirectional cement composites filled with natural hemp fibers. A typical clustering pattern with four hemp fibers embedded into cement matrix is designed as the representative two-dimensional unit cell, which is taken from the periodic cement composite under consideration, and a clustering degree parameter is introduced to adjust the distance between clustered fibers. For this heterogeneous two-component composite model, distributions of the heat flux component are obtained using finite element simulation for various clustering cases involving different global fiber volume concentrations, clustering degree parameters, and thermal conductivity of both fiber and matrix, to evaluate the effective thermal conductivity of the composite. To further reveal the effects caused by clustered fibers, a random cluster pattern of hemp fibers in the unit cell is considered for comparison with the present regular clustering pattern. Further, a simple theoretical model with specified flexible factor f is developed by matching the theoretical and numerical predictions.},
 author = {Hui Wang and Yong-Peng Lei and Jian-Shan Wang and Qing-Hua Qin and Yi Xiao},
 doi = {10.1177/0021998315594482},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0021998315594482},
 journal = {Journal of Composite Materials},
 number = {11},
 pages = {1509–1521},
 title = {Theoretical and computational modeling of clustering effect on effective thermal conductivity of cement composites filled with natural hemp fibers},
 url = {https://doi-org.crai.referencistas.com/10.1177/0021998315594482},
 volume = {50},
 year = {2016s}
}

@article{doi:10.1177/0021998315604209,
 abstract = {A method for computational description of morphology of dispersive components’ spatial structures in composites has been developed. The method is based on the calculation and comparison average values of the elongation coefficient (Ke), sinuosity coefficient (Ks), and fullness coefficient (Kf) for structures of the dispersed phase. Stationary systems such as composite oligodiene epoxide vulcanizate (PDI 3AK) as matrix with micro-dispersive aluminium powder and composite methyl methacrylate-styrene (PMMAS) as matrix with magnetite Fe3O4 nanoparticles were investigated by SEM and AFM, respectively. Time behavior of micro-dispersive carbon black in dynamic system comprised of polydivinylisoprene oligomer (PDI) as the matrix was studied by optical microscopy. The obtained results by method for computational description of morphology had showed universality of the method and enabled ascertaining quantitative correlations describing shapes of certain particles of the filler and morphology of spatial structure under formation. Dependence of form coefficients on the time was determined. This dependence enables studying kinetics of structure formation.},
 author = {Konstantin O Ukhin and Anton I Nechaev and Viktor A Valtsifer and Vladimir N Strelnikov},
 doi = {10.1177/0021998315604209},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0021998315604209},
 journal = {Journal of Composite Materials},
 number = {17},
 pages = {2433–2442},
 title = {Computational description of morphology of dispersive components’ spatial structures in polymer composites},
 url = {https://doi-org.crai.referencistas.com/10.1177/0021998315604209},
 volume = {50},
 year = {2016q}
}

@article{doi:10.1177/0021998320987893,
 abstract = {The carbon nanotubes/nanofibers reinforced composites (CNRC) show great mechanical properties. There are several methods to simulate the mechanical properties of composites. Among the modeling techniques, embedded region (ER) shows the possibility for direct multi-scale simulation. A comparative study among beam element embedded model, solid element embedded model, as well as common solid element model is carried out. Programs developed in Matlab are utilized to generate geometric configurations, and finite element models are obtained from MSC.Patran with a script written in the Patran command language (PCL). Besides, a set of parametric studies are performed to investigate the influence of the aspect ratios of nanofibers and load cases on the mechanical properties of CNRC. The result shows that the ER technique is reliable to represent composites though neglecting the localized stress concentration, and beam element embedded models are trustworthy only for nanofibers with a large aspect ratio.},
 author = {Pei-Liang Bian and Hai Qing},
 doi = {10.1177/0021998320987893},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0021998320987893},
 journal = {Journal of Composite Materials},
 number = {17},
 pages = {2315–2327},
 title = {Computational modeling of carbon nanofibers reinforced composites: A comparative study},
 url = {https://doi-org.crai.referencistas.com/10.1177/0021998320987893},
 volume = {55},
 year = {2021b}
}

@article{doi:10.1177/00219983231194249,
 abstract = {The three-dimensional (3D) printing of polymer matrix composites has attracted considerable attention. However, the addition of filler materials to the extruded polymer matrix results in inner voids within the beads along with larger voids, called intervoids, which are generated between the beads during the 3D printing process. These voids degrade the thermal transport properties of 3D-printed polyether ether ketone (PEEK) and PEEK/carbon-fiber composites. The effects of these pores (i.e., their number, size, and location) on the thermal conductivity of 3D-printed PEEK and a PEEK/carbon-fiber composite were investigated using a multiscale finite element approach. The presence of the carbon fibers mitigated the effect of the pores on the thermal conductivity of the PEEK/carbon-fiber composite, particularly along the longitudinal direction. Moreover, the inner voids had a dominant effect on the PEEK and PEEK/carbon-fiber composite. Finally, the orientation of the intervoids had no effect on the thermal conductivity of the printed materials.},
 author = {Abdulrahman A Alghamdi},
 doi = {10.1177/00219983231194249},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00219983231194249},
 journal = {Journal of Composite Materials},
 number = {24},
 pages = {3807–3820},
 title = {Computational assessment of effect of porosity on thermal conductivity of 3D-printed PEEK/carbon-fiber composite},
 url = {https://doi-org.crai.referencistas.com/10.1177/00219983231194249},
 volume = {57},
 year = {2023a}
}

@article{doi:10.1177/0022002714540473,
 abstract = {Why do some individuals pick up arms as opposed to others who live under the same conditions? Environmental and group theories fail to differentiate between these individuals. In response, we apply the cognitive mapping approach and model violence as decisions based on chains of beliefs about various types of factors, including state aggression, access to violent groups, religion, and personal characteristics. Based on a double-paired comparison, data are constructed from ethnographic interviews with Muslim and non-Muslim individuals engaging in violent and nonviolent activity in authoritarian and democratic states—Egypt and Germany. The analysis develops a computational model formalizing the cognitive maps into Bayesian networks. In 477,604 runs, the model (1) identifies the beliefs connected to decisions, (2) traces inference chains antecedent to decisions, and (3) explores counterfactuals. This suggests that both violent and nonviolent activities are responses to state aggression, and not to Islam, group access, or personal characteristics.},
 author = {Stephanie Dornschneider and Nick Henderson},
 doi = {10.1177/0022002714540473},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0022002714540473},
 journal = {Journal of Conflict Resolution},
 number = {2},
 pages = {368–399},
 title = {A Computational Model of Cognitive Maps: Analyzing Violent and Nonviolent Activity in Egypt and Germany},
 url = {https://doi-org.crai.referencistas.com/10.1177/0022002714540473},
 volume = {60},
 year = {2016d}
}

@article{doi:10.1177/00220027231179102,
 abstract = {Much research examines the state-dissident nexus by large-n studies and rational choice theories. This article contributes an analysis of dissident reasoning through a computational evaluation of ethnographic interviews. The analysis shows that dissident decision-making is based on tit-for-tat deliberations: Dissidents choose violent means primarily in response to violent repression, and nonviolent means in response to nonviolent repression. Ordinary citizens not participating in dissent consider positive state behavior or safety concerns instead. Consistent with arguments that state-dissident interactions are reciprocal, these findings reveal unexpected cognitive similarities between political dissent and cooperation, which is often associated with tit-for-tat deliberations. They also show the importance of state repression compared with other motivators of dissent, including perceived relative deprivation and social contagion. The findings identify heuristic patterns of reasoning which suggest that dissidents may be more open to change and, ultimately, cooperation with state authorities than what is argued by repressive states.},
 author = {Stephanie Dornschneider-Elkink and Nick Henderson},
 doi = {10.1177/00220027231179102},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00220027231179102},
 journal = {Journal of Conflict Resolution},
 number = {4},
 pages = {756–785},
 title = {Repression and Dissent: How Tit-for-Tat Leads to Violent and Nonviolent Resistance},
 url = {https://doi-org.crai.referencistas.com/10.1177/00220027231179102},
 volume = {68},
 year = {2024e}
}

@article{doi:10.1177/00220345241265048,
 abstract = {Observation is at the center of all biological sciences. Advances in imaging technologies are therefore essential to derive novel biological insights to better understand the complex workings of living systems. Recent high-throughput sequencing and imaging techniques are allowing researchers to simultaneously address complex molecular variations spatially and temporarily in tissues and organs. The availability of increasingly large dataset sizes has allowed for the evolution of robust deep learning models, designed to interrogate biomedical imaging data. These models are emerging as transformative tools in diagnostic medicine. Combined, these advances allow for dynamic, quantitative, and predictive observations of entire organisms and tissues. Here, we address 3 main tasks of bioimage analysis, image restoration, segmentation, and tracking and discuss new computational tools allowing for 3-dimensional spatial genomics maps. Finally, we demonstrate how these advances have been applied in studies of craniofacial development and oral disease pathogenesis.},
 author = {E. James and A.J. Caetano and P.T. Sharpe},
 doi = {10.1177/00220345241265048},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00220345241265048},
 journal = {Journal of Dental Research},
 note = {PMID:39272216},
 number = {0},
 pages = {00220345241265048},
 title = {Computational Methods for Image Analysis in Craniofacial Development and Disease},
 url = {https://doi-org.crai.referencistas.com/10.1177/00220345241265048},
 volume = {0},
 year = {2024g}
}

@article{doi:10.1177/0022057409189001-217,
 author = {Jennifer Rabold},
 doi = {10.1177/0022057409189001-217},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0022057409189001-217},
 journal = {Journal of Education},
 number = {1–2},
 pages = {209–211},
 title = {Promoting Conceptual Mathematical Thinking through Books},
 url = {https://doi-org.crai.referencistas.com/10.1177/0022057409189001-217},
 volume = {189},
 year = {2009m}
}

@article{doi:10.1177/002221940003300605,
 abstract = {Based on their performance on a standardized achievement test, second-grade children (N = 49) were classified as having mathematics difficulties with normal reading achievement (MD only), both mathematics and reading difficulties (MD/RD), reading difficulties with normal mathematics achievement (RD only) and normal mathematics and reading achievement (NA). Each child was given a series of tasks so that we might assess their thinking across four areas of mathematics: number facts, story problems, place value, and written calculation. Children with MD/RD performed significantly worse than NA children in most areas of mathematical thinking, whereas children with MD only performed worse than NA children only on complex story problems. The MD-only group outperformed the MD/RD group on story problems and written calculation. No significant differences were found between the RD-only and NA groups on any of the tasks. The results suggested that among children with mathematics difficulties, the MD/RD subgroup is distinct from the MD-only subgroup, with the former being characterized by pervasive deficiencies in mathematical thinking and the latter by more specific deficits in problem solving.},
 author = {Nancy C. Jordan and Laurie B. Hanich},
 doi = {10.1177/002221940003300605},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/002221940003300605},
 journal = {Journal of Learning Disabilities},
 note = {PMID:15495398},
 number = {6},
 pages = {567–578},
 title = {Mathematical Thinking in Second-Grade Children with Different Forms of LD},
 url = {https://doi-org.crai.referencistas.com/10.1177/002221940003300605},
 volume = {33},
 year = {2000h}
}

@article{doi:10.1177/0022219414554228,
 abstract = {This study examined the effect of schema-based instruction (SBI) on the proportional problem-solving performance of students with mathematics difficulties only (MD) and students with mathematics and reading difficulties (MDRD). Specifically, we examined the responsiveness of 260 seventh grade students identified as MD or MDRD to a 6-week treatment (SBI) on measures of proportional problem solving. Results indicated that students in the SBI condition significantly outperformed students in the control condition on a measure of proportional problem solving administered at posttest (g = 0.40) and again 6 weeks later (g = 0.42). The interaction between treatment group and students’ difficulty status was not significant, which indicates that SBI was equally effective for both students with MD and those with MDRD. Further analyses revealed that SBI was particularly effective at improving students’ performance on items related to percents. Finally, students with MD significantly outperformed students with MDRD on all measures of proportional problem solving. These findings suggest that interventions designed to include effective instructional features (e.g., SBI) promote student understanding of mathematical ideas.},
 author = {Asha K. Jitendra and Danielle N. Dupuis and Jon R. Star and Michael C. Rodriguez},
 doi = {10.1177/0022219414554228},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0022219414554228},
 journal = {Journal of Learning Disabilities},
 note = {PMID:25312518},
 number = {4},
 pages = {354–367},
 title = {The Effects of Schema-Based Instruction on the Proportional Thinking of Students With Mathematics Difficulties With and Without Reading Difficulties},
 url = {https://doi-org.crai.referencistas.com/10.1177/0022219414554228},
 volume = {49},
 year = {2016j}
}

@article{doi:10.1177/00222194221097710,
 abstract = {As digital technology use increases in K–12 education, greater numbers of strategies become available to support students in mathematics. One technology that provides students diverse representations of mathematical concepts is virtual manipulatives. Although instruction featuring representations with physical manipulatives possesses a large body of research, the virtual form lacks comparable study, particularly with young children experiencing mathematics difficulty or identified with a mathematics learning disability. These students often demonstrate challenges learning integral skills such as fractions that subsequently affect their academic success in future years. This study examined the use of virtual manipulatives paired with explicit instruction and a system of least prompts for teaching computations with fractions to three elementary students with mathematics difficulty. A functional relation was found using a single-subject multiple probe design between the treatment condition and students’ accuracy performance solving problems. These results and their implications for the field at-large are discussed.},
 author = {Rajiv Satsangi and Alexandra R. Raines},
 doi = {10.1177/00222194221097710},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00222194221097710},
 journal = {Journal of Learning Disabilities},
 note = {PMID:35658741},
 number = {4},
 pages = {295–309},
 title = {Examining Virtual Manipulatives for Teaching Computations With Fractions to Children With Mathematics Difficulty},
 url = {https://doi-org.crai.referencistas.com/10.1177/00222194221097710},
 volume = {56},
 year = {2023q}
}

@article{doi:10.1177/00222194241248188,
 abstract = {The purpose of this analysis was to describe cognitive processes associated with comorbid difficulty between word reading (WR) and mathematics computation (MC) at the start of first grade among children selected for WR and MC delays. A sample of 234 children (mean age 6.50 years, SD = 0.31) was assessed on WR, MC, core cognitive processes (phonological processing, rapid automatized naming, verbal counting [VC]), and domain-general cognitive processes (working memory, oral language, nonverbal reasoning, attentive behavior). Structural equation modeling was used to predict a latent Comorbidity factor, which modeled shared variance between WR and MC, and to identify processes associated with that Comorbidity factor. Results identified each of the core cognitive processes, especially VC, and each of the domain-general cognitive processes, especially working memory, as explaining shared variance between WR and MC. Implications for understanding comorbid difficulty at the start of first grade and designing coordinated first-grade interventions are discussed.},
 author = {Lynn S. Fuchs and Douglas Fuchs and Eunsoo Cho and Marcia A. Barnes and Tuire Koponen and Daniel R. Espinas},
 doi = {10.1177/00222194241248188},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00222194241248188},
 journal = {Journal of Learning Disabilities},
 note = {PMID:38686606},
 number = {0},
 pages = {00222194241248188},
 title = {Comorbid Word Reading and Mathematics Computation Difficulty at Start of First Grade},
 url = {https://doi-org.crai.referencistas.com/10.1177/00222194241248188},
 volume = {0},
 year = {2024d}
}

@article{doi:10.1177/00222194241263646,
 abstract = {Establishing validated science programs for students with or at risk for learning disabilities requires testing treatment effects and exploring differential response patterns. This study explored whether students’ initial mathematics and reading skills influenced their treatment response to a whole-class, second-grade science program called Scientific Explorers (Sci2). The original Sci2 study employed a cluster randomized controlled design and included 294 students from 18 second-grade classrooms. Differential effects of the program by initial mathematics and reading skill levels were not observed for an interactive science assessment and a distal science outcome measure. However, based on initial reading skill levels, moderation results were found on a science vocabulary measure, suggesting the effects of Sci2 were greatest for students with higher initial reading skills. Similar results were found using initial mathematics skill levels as a predictor of differential response such that students with higher mathematics skills reaped stronger treatment effects on the vocabulary measure. Further, we found initial mathematics skills also influenced outcomes on the proximal science content assessment, where students with higher initial mathematics skills led to higher outcomes. Overall, findings suggest Sci2 produced robust effects for all students (g = 0.24–1.23), regardless of initial skill proficiencies. Implications for exploring differential response in science intervention research are discussed.},
 author = {Christian T. Doabler and Megan Rojo and Jenna A. Gersib and Anna-Maria Fall and Maria A. Longhi and Gail E. Lovette and Greg Roberts and Jasmine Uy and Katharina Johnson and Shadi Ghafghazi et al.},
 doi = {10.1177/00222194241263646},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00222194241263646},
 journal = {Journal of Learning Disabilities},
 note = {PMID:39056893},
 number = {0},
 pages = {00222194241263646},
 title = {Do Mathematics and Reading Skills Impact Student Science Outcomes?},
 url = {https://doi-org.crai.referencistas.com/10.1177/00222194241263646},
 volume = {0},
 year = {2024g}
}

@article{doi:10.1177/002221949703000102,
 abstract = {U.S. education suffers from shortcomings that put even children possessing adequate intellectual abilities at risk for low mathematics achievement. Consequently, identifying and understanding children whose academic failure is influenced by a genuine learning disability requires a complex “developmental” research agenda. This perspective suggests the use of sensitive research methods—clinical interviews, ethnographies—to examine the development of children’s construction of knowledge in the context of schooling. Researchers should consider such factors as the adequacy of classroom instruction, the availability in children of informal knowledge, the role of motivation, the effects of specific interventions, the role and operation of different cognitive processes in constructing mathematical understanding, children’s difficulties across different areas of mathematics, and the development of children’s thinking throughout the school years.},
 author = {Herbert P. Ginsburg},
 doi = {10.1177/002221949703000102},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/002221949703000102},
 journal = {Journal of Learning Disabilities},
 note = {PMID:9009878},
 number = {1},
 pages = {20–33},
 title = {Mathematics Learning Disabilities: A View From Developmental Psychology},
 url = {https://doi-org.crai.referencistas.com/10.1177/002221949703000102},
 volume = {30},
 year = {1997f}
}

@article{doi:10.1177/00222429231221698,
 abstract = {McShane et al.’s (2024) wide-ranging critique of null hypothesis significance testing provides a number of specific suggestions for improved practice in empirical research. This commentary amplifies several of these from the perspective of computational statistics—particularly nonparametrics, resampling/bootstrapping, and Bayesian methods—applied to common research problems. Throughout, the author emphasizes estimation (as opposed to testing) and uncertainty quantification through a comprehensive process of “curating” a variety of graphical and tabular evidence. Specifically, researchers should be encouraged to estimate the quantities that matter, with as few assumptions as possible, in multiple ways, then try to visualize it all, documenting their pathway from data to results for others to follow.},
 author = {Fred Feinberg},
 doi = {10.1177/00222429231221698},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00222429231221698},
 journal = {Journal of Marketing},
 number = {3},
 pages = {20–28},
 title = {p-Values as QWERTY: Curating Evidence in the Computational Era},
 url = {https://doi-org.crai.referencistas.com/10.1177/00222429231221698},
 volume = {88},
 year = {2024b}
}

@article{doi:10.1177/0022487109339906,
 abstract = {Cognitively Guided Instruction (CGI) researchers have found that while teachers readily ask initial questions to elicit students’ mathematical thinking, they struggle with how to follow up on student ideas. This study examines the classrooms of three teachers who had engaged in algebraic reasoning CGI professional development. We detail teachers’ questions and how they relate to students’ making explicit their complete and correct explanations. We found that after the initial “How did you get that?” question, a great deal of variability existed among teachers’ questions and students’ responses.},
 author = {Megan L. Franke and Noreen M. Webb and Angela G. Chan and Marsha Ing and Deanna Freund and Dan Battey},
 doi = {10.1177/0022487109339906},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0022487109339906},
 journal = {Journal of Teacher Education},
 number = {4},
 pages = {380–392},
 title = {Teacher Questioning to Elicit Students’ Mathematical Thinking in Elementary School Classrooms},
 url = {https://doi-org.crai.referencistas.com/10.1177/0022487109339906},
 volume = {60},
 year = {2009e}
}

@article{doi:10.1177/0022487117732317,
 abstract = {This article examines teacher preparation and teacher change in engineering and computer science education. We examined culturally responsive teaching self-efficacy (CRTSE), culturally responsive teaching outcome expectancy (CRTOE) beliefs, and attitudes toward computational thinking (CT) as teachers participated in one of three treatment groups: robotics only, game design only, or blended robotics/game design. Descriptive data revealed that CRTSE gain scores were higher in the robotics only and blended contexts than in the game design only context. However, CRTOE beliefs were consistent across all treatment groups. In regard to CT attitudes, teachers’ gain scores were higher in the game design only and blended contexts than in the robotics only context. In addition, there were differences by treatment group related to STEM (science, technology, engineering, and mathematics) practices, while cultural artifacts were evident in each learning environment. The results of this study reveal some variability by treatment type and inform future research on equitable practices in engineering and computer science education.},
 author = {Jacqueline Leonard and Monica Mitchell and Joy Barnes-Johnson and Adrienne Unertl and Jill Outka-Hill and Roland Robinson and Carla Hester-Croff},
 doi = {10.1177/0022487117732317},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0022487117732317},
 journal = {Journal of Teacher Education},
 number = {4},
 pages = {386–407},
 title = {Preparing Teachers to Engage Rural Students in Computational Thinking Through Robotics, Game Design, and Culturally Responsive Teaching},
 url = {https://doi-org.crai.referencistas.com/10.1177/0022487117732317},
 volume = {69},
 year = {2018k}
}

@article{doi:10.1177/0022487118786735,
 author = {Gail Richmond and Robert E. Floden and Corey Drake},
 doi = {10.1177/0022487118786735},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0022487118786735},
 journal = {Journal of Teacher Education},
 number = {4},
 pages = {327–329},
 title = {Research That Illuminates Enduring Dilemmas in Teacher Education},
 url = {https://doi-org.crai.referencistas.com/10.1177/0022487118786735},
 volume = {69},
 year = {2018o}
}

@article{doi:10.1177/0022487120949842,
 abstract = {Given the strong influence of teachers educators’ pedagogical modeling on new teachers’ capacity to use technology to support student learning, this study sought to answer two interrelated questions: (a) How are teacher educators and teacher education programs currently working to prepare teachers to integrate technology? and (b) How are teacher educators implementing the TPACK (complex integration of technological [T], pedagogical [P], and content [C] knowledge [K]) model? The evidence to answer these questions was derived from an analysis of quantitative and qualitative survey responses from 843 teacher educators from approximately half (n = 541) of the accredited teacher education programs in the country. The results showed that teacher educators are increasingly integrating technology across the curriculum, that there is a fairly low level of TPACK adoption, and that conceptions of TPACK vary greatly. The study helps to better understand these teacher educator practices in relationship to the literature on preparing teachers to use technology to support student learning.},
 author = {Rick Voithofer and Michael J. Nelson},
 doi = {10.1177/0022487120949842},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0022487120949842},
 journal = {Journal of Teacher Education},
 number = {3},
 pages = {314–328},
 title = {Teacher Educator Technology Integration Preparation Practices Around TPACK in the United States},
 url = {https://doi-org.crai.referencistas.com/10.1177/0022487120949842},
 volume = {72},
 year = {2021r}
}

@article{doi:10.1177/00224871231223460,
 abstract = {Recent calls encourage teacher education programs to examine how they address equity within educational technology coursework. This study therefore conceptualizes equity specifically related to using digital tools with multilingual learners. Drawing on a technological pedagogical content knowledge (TPACK) framework informed by prior research on (language) teacher education and computer-assisted language learning, this study examines how preservice teachers described their knowledge base specifically related to using digital tools equitably with multilingual learners. Qualitative analysis of 17 preservice teachers’ course discussions, assignments, and interviews revealed that preservice teachers purposefully reflected on and selected digital tools to use that would support students’ language development and leverage students’ interests. They also reflected on structural inequities and advocacy related to technological implementation. This study encourages teacher educators to support preservice teachers in developing technologically savvy practices that are also linguistically responsive and humanizing by centering equity in TPACK.},
 author = {Carmen Durham},
 doi = {10.1177/00224871231223460},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00224871231223460},
 journal = {Journal of Teacher Education},
 number = {3},
 pages = {347–360},
 title = {Centering Equity for Multilingual Learners in Preservice Teachers’ Technological Pedagogical Content Knowledge (TPACK)},
 url = {https://doi-org.crai.referencistas.com/10.1177/00224871231223460},
 volume = {75},
 year = {2024g}
}

@article{doi:10.1177/002248716201300402,
 author = {Irvin J. Lehmann},
 doi = {10.1177/002248716201300402},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/002248716201300402},
 journal = {Journal of Teacher Education},
 number = {4},
 pages = {376–385},
 title = {Critical Thinking Ability, Attitudes, and Values Among College Students},
 url = {https://doi-org.crai.referencistas.com/10.1177/002248716201300402},
 volume = {13},
 year = {1962g}
}

@article{doi:10.1177/002248718904000301,
 abstract = {The rising emphasis in schools on the explicit teaching of higher-level cogni tive skills has important implications for preservice teacher education programs. Action on this topic by teacher educators is important now for establishing a strong theoretical model for cognitive education in preservice programs, facul ty development plans for teacher educa tion faculty, new kinds of collaboration between SCDEs and the schools, coordi nation with arts and sciences faculty, re vision of existing courses in teacher edu cation, and a research agenda to study the relationship between instruction in higher-level thinking skills for future teachers and their implementation of those ideas in the classroom. Models for restructuring the teacher education program to include higher-order think ing skills are presented by Martin.},
 author = {David S. Martin},
 doi = {10.1177/002248718904000301},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/002248718904000301},
 journal = {Journal of Teacher Education},
 number = {3},
 pages = {2–8},
 title = {Restructuring Teacher Education Programs for Higher-Order Thinking Skills},
 url = {https://doi-org.crai.referencistas.com/10.1177/002248718904000301},
 volume = {40},
 year = {1989l}
}

@article{doi:10.1177/0023677220937718,
 abstract = {Several studies based on in vivo or in vitro models have found promising results for the noble gas argon in neuroprotection against ischaemic pathologies. The development of argon as a medicinal product includes the requirement for toxicity testing through non-clinical studies. The long exposure period of animals (rats) during several days results in technical and logistic challenges related to the gas administration. In particular, a minimum of 10 air changes per hour (ACH) to maintain animal welfare results in extremely large volumes of experimental gas required if the gas is not recirculated. The difficulty with handling the many cylinders prompted the development of such a recirculation-based design. To distribute the recirculating gas to individually ventilated cages and monitor them properly was deemed more difficult than constructing a single large enclosure that will hold several open cages. To address these concerns, a computational fluid dynamics (CFD) analysis of the preliminary design was performed. A purpose-made exposure chamber was designed based on the CFD simulations. Comparisons of the simulation results to measurements of gas concentration at two cage positions while filling show that the CFD results compare well to these limited experiments. Thus, we believe that the CFD results are representative of the gas distribution throughout the enclosure. The CFD shows that the design provides better gas distribution (i.e. a higher effective air change rate) than predicted by 10 ACH.},
 author = {Ira Katz and Kateryna Voronetska and Mickaël Libardi and Matthieu Chalopin and Patricia Privat and David J Esdaile and Guillaume Mougin and Géraldine Farjot and Aude Milet},
 doi = {10.1177/0023677220937718},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0023677220937718},
 journal = {Laboratory Animals},
 note = {PMID:32722999},
 number = {2},
 pages = {150–157},
 title = {Computational fluid dynamics applied to the ventilation of small-animal laboratory cages},
 url = {https://doi-org.crai.referencistas.com/10.1177/0023677220937718},
 volume = {55},
 year = {2021n}
}

@article{doi:10.1177/0023830912460513,
 abstract = {It has been hypothesized that known words in the lexicon strengthen newly formed representations of novel words, resulting in words with dense neighborhoods being learned more quickly than words with sparse neighborhoods. Tests of this hypothesis in a connectionist network showed that words with dense neighborhoods were learned better than words with sparse neighborhoods when the network was exposed to the words all at once (Experiment 1), or gradually over time, like human word-learners (Experiment 2). This pattern was also observed despite variation in the availability of processing resources in the networks (Experiment 3). A learning advantage for words with sparse neighborhoods was observed only when the network was initially exposed to words with sparse neighborhoods and exposed to dense neighborhoods later in training (Experiment 4). The benefits of computational experiments for increasing our understanding of language processes and for the treatment of language processing disorders are discussed.},
 author = {Michael S Vitevitch and Holly L Storkel},
 doi = {10.1177/0023830912460513},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0023830912460513},
 journal = {Language and Speech},
 note = {PMID:24597275},
 number = {4},
 pages = {493–527},
 title = {Examining the Acquisition of Phonological Word Forms with Computational Experiments},
 url = {https://doi-org.crai.referencistas.com/10.1177/0023830912460513},
 volume = {56},
 year = {2013p}
}

@article{doi:10.1177/00238309221111752,
 abstract = {We present an implementation of DIANA, a computational model of spoken word recognition, to model responses collected in the Massive Auditory Lexical Decision (MALD) project. DIANA is an end-to-end model, including an activation and decision component that takes the acoustic signal as input, activates internal word representations, and outputs lexicality judgments and estimated response latencies. Simulation 1 presents the process of creating acoustic models required by DIANA to analyze novel speech input. Simulation 2 investigates DIANA’s performance in determining whether the input signal is a word present in the lexicon or a pseudoword. In Simulation 3, we generate estimates of response latency and correlate them with general tendencies in participant responses in MALD data. We find that DIANA performs fairly well in free word recognition and lexical decision. However, the current approach for estimating response latency provides estimates opposite to those found in behavioral data. We discuss these findings and offer suggestions as to what a contemporary model of spoken word recognition should be able to do.},
 author = {Filip Nenadić and Benjamin V. Tucker and Louis ten Bosch},
 doi = {10.1177/00238309221111752},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00238309221111752},
 journal = {Language and Speech},
 note = {PMID:36000386},
 number = {3},
 pages = {564–605},
 title = {Computational Modeling of an Auditory Lexical Decision Experiment Using DIANA},
 url = {https://doi-org.crai.referencistas.com/10.1177/00238309221111752},
 volume = {66},
 year = {2023h}
}

@article{doi:10.1177/002383098302600207,
 abstract = {Recent studies have indicated that schizophrenics with current evidence of formal thought disorder (FTD) can be distinguished from schizophrenics without such disorder on measures of ability to use available redundancies in the recall of words, redundancy and lexicon variability of spoken language, and deficient motor synchrony in the production of rhythmic movements. This investigation examined pause patterns, redundancy, word frequency, and disordered thinking in a group of conservatively diagnosed schizophrenic patients, utilizing Butterworth’s model of language production. It was hypothesized that schizophrenics with FTD would be more vulnerable than those without FTD to breakdown in control processes of the interactive type than the autonomous type because of impaired capacities for involuntary attention. The results indicate that schizophrenics with FTD show a disruption of the normal systematic relationship between word redundancy and pauses in speech. This disruption is not found in schizophrenics without FTD, whose responses resemble those of normal speakers. The results are discussed in the light of attempts to integrate findings from language and motor research in schizophrenia.},
 author = {Brendan A. Maher and Theo C. Manschreck and Michael A.C. Molino},
 doi = {10.1177/002383098302600207},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/002383098302600207},
 journal = {Language and Speech},
 note = {PMID:6664181},
 number = {2},
 pages = {191–199},
 title = {Redundancy, Pause Distributions and Thought Disorder in Schizophrenia},
 url = {https://doi-org.crai.referencistas.com/10.1177/002383098302600207},
 volume = {26},
 year = {1983j}
}

@article{doi:10.1177/002383099203500404,
 abstract = {This paper deals with the treatment of stress in Greek inflectional morphology. First, a morphological processor is presented which is built on the basis of a linguistic analysis of Greek inflected forms. This is followed by a discussion of how stress applies to words and how stress shift phenomena are taken into account by the morphological processor. In Greek, word stress distribution is important because it represents a major difficulty in every attempt to create a morphological processor which can be used by speech recognition systems, machine readable dictionaries, and machine translation projects involving Greek as source or target language.},
 author = {Loudovikos Touratzidis and Angela Ralli},
 doi = {10.1177/002383099203500404},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/002383099203500404},
 journal = {Language and Speech},
 number = {4},
 pages = {435–453},
 title = {A Computational Treatment of Stress in Greek Inflected Forms},
 url = {https://doi-org.crai.referencistas.com/10.1177/002383099203500404},
 volume = {35},
 year = {1992s}
}

@article{doi:10.1177/003172171309500111,
 abstract = {Learning programming introduces students to solving problems, designing applications, and making connections online.},
 author = {Yasmin B. Kafai and Quinn Burke},
 doi = {10.1177/003172171309500111},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/003172171309500111},
 journal = {Phi Delta Kappan},
 number = {1},
 pages = {61–65},
 title = {Computer Programming Goes Back to School},
 url = {https://doi-org.crai.referencistas.com/10.1177/003172171309500111},
 volume = {95},
 year = {2013f}
}

@article{doi:10.1177/0033294117729183,
 abstract = {Researchers have operationalized working memory in different ways and although working memory–performance relationships are well documented, there has been relatively less attention devoted to determining whether seemingly similar measures yield comparable relations with performance outcomes. Our objective is to assess whether two working memory measures deploying the same processes but different item content yield different relations with two problem-solving criteria. Participants completed a computation-based working memory measure and a reading-based measure prior to performing a computerized simulation. Results reveal differential relations with one of the two criteria and support the notion that the two working memory measures tap working memory capacity and other cognitive abilities. One implication for theory development is that researchers should consider incorporating other cognitive abilities in their working memory models and that the selection of those abilities should correspond to the criterion of interest. One practical implication is that researchers and practitioners shouldn’t automatically assume that different phonological loop-based working memory scales are interchangeable.},
 author = {Richard Perlow and Mia Jattuso},
 doi = {10.1177/0033294117729183},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0033294117729183},
 journal = {Psychological Reports},
 note = {PMID:29298565},
 number = {3},
 pages = {430–444},
 title = {A Comparison of Computation Span and Reading Span Working Memory Measures’ Relations With Problem-Solving Criteria},
 url = {https://doi-org.crai.referencistas.com/10.1177/0033294117729183},
 volume = {121},
 year = {2018l}
}

@article{doi:10.1177/0033294118806473,
 abstract = {To explore hypotheses based on Stanovich’s proposal that analytic processing comprises a reflective-level, an algorithmic level, and specific mindware, 342 participants completed measures of thinking dispositions, general ability (GA), numeracy, and probabilistic and nonprobabilistic reasoning. In a control condition, numeracy predicted probabilistic reasoning at high levels of both thinking dispositions and GA, and GA predicted nonprobabilistic reasoning at high levels of thinking dispositions. In a logic instruction condition, numeracy predicted probabilistic reasoning when GA was high, and GA affected nonprobabilistic reasoning directly. Thinking dispositions moderated neither relationship. Instead, instructions facilitated reasoning for low thinking disposition/high-ability participants, suggesting that logic instructions cued low thinking disposition individuals to engage in higher order reflective processing. The evidence is consistent with the proposals that reflective processes are essential to the allocation of algorithmic resources, and algorithmic resources are necessary for effective mindware implementation.},
 author = {Paul A. Klaczynski and Wejdan Felmban},
 doi = {10.1177/0033294118806473},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0033294118806473},
 journal = {Psychological Reports},
 note = {PMID:30550725},
 number = {2},
 pages = {341–370},
 title = {Effects of Thinking Dispositions, General Ability, Numeracy, and Instructional Set on Judgments and Decision-Making},
 url = {https://doi-org.crai.referencistas.com/10.1177/0033294118806473},
 volume = {123},
 year = {2020l}
}

@article{doi:10.1177/0034523717723384,
 author = {Elizabeth de Freitas and Ezekiel Dixon-Román},
 doi = {10.1177/0034523717723384},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0034523717723384},
 journal = {Research in Education},
 number = {1},
 pages = {3–13},
 title = {The computational turn in education research: Critical and creative perspectives on the digital data deluge},
 url = {https://doi-org.crai.referencistas.com/10.1177/0034523717723384},
 volume = {98},
 year = {2017f}
}

@article{doi:10.1177/0034523717723385,
 abstract = {Contemporary education policy involves the integration of novel forms of data and the creation of new data platforms, in addition to the infusion of business principles into school governance networks, and intensification of socio-technical relations. In this paper, we examine how ‘computational rationality’ may be understood as intensifying of an instrumental set of logics in educational governance and decision making, and/or as opening up new explorations around the uncertainty and incompleteness of policy. We suggest that policy rationalities focused on prediction, transparency and data provide the conditions of possibility for Artificial Intelligence to be integrated into, and intensify aspects of, what we term ‘computational education policy’.},
 author = {Kalervo N Gulson and P Taylor Webb},
 doi = {10.1177/0034523717723385},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0034523717723385},
 journal = {Research in Education},
 number = {1},
 pages = {14–26},
 title = {Mapping an emergent field of ‘computational education policy’: Policy rationalities, prediction and data in the age of Artificial Intelligence},
 url = {https://doi-org.crai.referencistas.com/10.1177/0034523717723385},
 volume = {98},
 year = {2017f}
}

@article{doi:10.1177/0036850419850431,
 abstract = {Pregnancy can be accompanied by serious health risks to mother and child, such as pre-eclampsia, premature birth and postpartum haemorrhage. Understanding of the normal physiology of uterine function is essential to an improved management of such risks. Here we focus on the physiology of the smooth muscle fibres which make up the bulk of the uterine wall and which generate the forceful contractions that accompany parturition. We survey computational methods that integrate mathematical modelling with data analysis and thereby aid the discovery of new therapeutic targets that, according to clinical needs, can be manipulated to either stop contractions or cause the uterine wall muscle to become active.},
 author = {Joseph R Dunford and E Josiah Lutton and Jolene Atia and Andrew M Blanks and Hugo A van den Berg},
 doi = {10.1177/0036850419850431},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0036850419850431},
 journal = {Science Progress},
 note = {PMID:31829844},
 number = {2},
 pages = {103–126},
 title = {Computational physiology of uterine smooth muscle},
 url = {https://doi-org.crai.referencistas.com/10.1177/0036850419850431},
 volume = {102},
 year = {2019g}
}

@article{doi:10.1177/0036850419873799c,
 author = {Walid El-Sharoud},
 doi = {10.1177/0036850419873799c},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0036850419873799c},
 journal = {Science Progress},
 number = {3},
 pages = {279–279},
 title = {Book Review: Hector J. Levesque, Thinking as computation},
 url = {https://doi-org.crai.referencistas.com/10.1177/0036850419873799c},
 volume = {102},
 year = {2019g}
}

@article{doi:10.1177/0036850419874231,
 abstract = {Three arrangement types of fibrous media have been established including “Mesh” (layered distribution), “Para” (unidirectional distribution), and “Nurbs” (random distribution), with fiber diameters ranging from 5 to 7 µm and solid volume fractions ranging from 14.58% to 21.95%. To describe the filtration performance, particles trajectories, and the influence of structural parameters of fibrous media, the computation fluid dynamics is adopted. The filtration efficiency and the pressure drop of fibrous media are calculated in computation fluid dynamics and semi-analytical model. It is found that the arrangement types have significant effects on the filtration efficiency and show the limitation of the semi-analytical model. With the Stokes number larger than 1, the numerical results and semi-analytical model have great consistency in the filtration efficiency for the “Mesh” and “Para” types than “Nurbs” type. Meanwhile, the pressure drop between the numerical results and the semi-analytical model is always consistent.},
 author = {Liang Zhang and JiaWei Zhou and Bo Zhang and Wei Gong},
 doi = {10.1177/0036850419874231},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0036850419874231},
 journal = {Science Progress},
 note = {PMID:31829857},
 number = {1},
 pages = {0036850419874231},
 title = {Semi-analytical and computational investigation of different fibrous structures affecting the performance of fibrous media},
 url = {https://doi-org.crai.referencistas.com/10.1177/0036850419874231},
 volume = {103},
 year = {2020t}
}

@article{doi:10.1177/0036850419901235,
 abstract = {Hydrodynamic effects of mussel farms have attracted increased research attentions in recent years. The understanding of the hydrodynamic impacts is essential for predicting the sustainability of mussel farms. A large mussel farm includes thousands of mussel droppers, and the combined drag on the mussel droppers is sufficient to possibly affect the longevity of the entire long-lines. This article intends to study the drag and wake of an individual long-line mussel dropper using computational fluid dynamics approaches. Two equivalent rough cylinders, namely, Curved-Model and Sharp-Model, have been utilized to simulate the mussel dropper, and each rough cylinder is assigned with surface roughness. The porosity is not considered in this article due to its complexity from inhalant and exhalant of mussels. Two-dimensional laminar simulations are conducted at Reynolds number from 10 to 200, and three-dimensional large eddy simulations are conducted at subcritical Reynolds number ranging from 3900 to . The results show that larger drag coefficients and Strouhal numbers are attributed to surface roughness and sharp crowns on the rough cylinder. The obtained drag coefficient ranges from 1.1 to 1.2 with respect to the diameter of the mussel dropper and the peak value of the tidal velocities. Wakes behind rough cylinders fluctuate more actively compared to those of smooth cylinders. This research work provides new insight for further investigations on hydrodynamic interactions between fluid and mussel droppers.},
 author = {Zhijing Xu and Hongde Qin and Peng Li and Rujun Liu},
 doi = {10.1177/0036850419901235},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0036850419901235},
 journal = {Science Progress},
 note = {PMID:32024433},
 number = {1},
 pages = {0036850419901235},
 title = {Computational fluid dynamics approaches to drag and wake of a long-line mussel dropper under tidal current},
 url = {https://doi-org.crai.referencistas.com/10.1177/0036850419901235},
 volume = {103},
 year = {2020t}
}

@article{doi:10.1177/00368504211039363,
 abstract = {The blood pump is an implantable device with strict performance requirements. Any effective structural improvement will help to improve the treatment of patients. However, the research of blood pump structure improvement is a complex optimization problem with multiple parameters and objectives. This study takes the splitter blade as the object of structural improvement. Computational fluid mechanics and neural networks are combined in research and optimization. And hydraulic experiments and micro particle image velocimetry technology were used. In the optimization study, the number of blades, axial length and circumferential offset are optimization parameters, and hydraulic performance and hemolytic prediction index are optimization targets. The study analyzes the influence of each parameter on performance and completes the optimization of the parameters. In the results, the optimal parameters of number of blades, axial length ratio, and circumferential offset are 2.6° and 0.41°, respectively. Under optimized parameters, hydraulic performance can be significantly improved. And the results of hemolysis prediction and micro particle image velocimetry experiments reflect that there is no increase in the risk of hemolytic damage. The results of this study provide a method and ideas for improving the structure of the axial spiral blade blood pump. The established optimization method can be effectively applied to the design and research of axial spiral blade blood pumps with complex, high precision, and multiple parameters and targets.},
 author = {Zheqin Yu and Jianping Tan and Shuai Wang and Bin Guo},
 doi = {10.1177/00368504211039363},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00368504211039363},
 journal = {Science Progress},
 note = {PMID:34463585},
 number = {3},
 pages = {00368504211039363},
 title = {Multiple parameters and target optimization of splitter blades for axial spiral blade blood pump using computational fluid mechanics, neural networks, and particle image velocimetry experiment},
 url = {https://doi-org.crai.referencistas.com/10.1177/00368504211039363},
 volume = {104},
 year = {2021q}
}

@article{doi:10.1177/003754970007400104,
 abstract = {A framework is presented for step-by-step imple mentation of weighted-residual methods (MWR) for simulations that require the solution of boundary-value problems. A set of MATLAB-based functions of the computationally common MWR solution steps has been developed and is used in the application of eigenfunction expansion, collo cation, and Galerkin-projection discretizations of time-dependent, distributed-parameter system models. Four industrially relevant examples taken from electronic materials and chemical processing applications are used to demonstrate the simula tion approach developed. This library of functions and several demonstration scripts can be obtained from the MWRtools project Website located at http://www.ench.umd.edu/software/MWRtools.},
 author = {Raymond A. Adomaitis and Yi-hung Lin and Hsiao-Yung Chang},
 doi = {10.1177/003754970007400104},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/003754970007400104},
 journal = {SIMULATION},
 number = {1},
 pages = {28–38},
 title = {A Computational Framework for Boundary-Value Problem Based Simulations},
 url = {https://doi-org.crai.referencistas.com/10.1177/003754970007400104},
 volume = {74},
 year = {2000a}
}

@article{doi:10.1177/0037549704046740,
 abstract = {The author exemplifies the framework of PSI (Pervasive System for Indoor-GIS) for exploring the spatial model of dynamic human behavior and developing various services in an ubiquitous computational environment. This does not mean merely constructing a software framework; rather, it means attempting to establish an inclusive service framework for ubiquitous computing. The most advantageous aspect of this component-oriented framework is that it contributes toward developing various service applications for ubiquitous computation as the occasion demands. That is, service applications are expected to do modeling, simulation, monitoring, Web services, and other applications. The author also discusses the relative issues on making a coordinating bridge between behavioral sciences and multiagent systems.},
 author = {Kazuhiko Shibuya},
 doi = {10.1177/0037549704046740},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0037549704046740},
 journal = {SIMULATION},
 number = {7–8},
 pages = {367–380},
 title = {A Framework of Multi-Agent-Based Modeling, Simulation, and Computational                 Assistance in an Ubiquitous Environment},
 url = {https://doi-org.crai.referencistas.com/10.1177/0037549704046740},
 volume = {80},
 year = {2004r}
}

@article{doi:10.1177/0037549706065481,
 abstract = {Efficient computer simulation of complex physical phenomena has long been challenging due to their multiphysics and multiscale nature. In contrast to traditional time-stepped execution methods, the authors describe an approach using optimistic parallel discrete event simulation (PDES) and reverse computation techniques to execute plasma physics codes. They show that reverse computation-based optimistic parallel execution can significantly reduce the execution time of an example plasma simulation without requiring a significant amount of additional memory compared to conservative execution techniques. The authors describe an application-level reverse computation technique that is efficient and suitable for complex scientific simulations.},
 author = {Yarong Tang and Kalyan S. Perumalla and Richard M. Fujimoto and Homa Karimabadi and Jonathan Driscoll and Yuri Omelchenko},
 doi = {10.1177/0037549706065481},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0037549706065481},
 journal = {SIMULATION},
 number = {1},
 pages = {61–73},
 title = {Optimistic Simulations of Physical Systems Using Reverse Computation},
 url = {https://doi-org.crai.referencistas.com/10.1177/0037549706065481},
 volume = {82},
 year = {2006q}
}

@article{doi:10.1177/0037549706069103,
 abstract = {The addition of two emerging technologies (evolutionary computation and ecoinformatics) to computational ecology can advance our ability to build better ecological models and thus deepen our understanding of the mechanistic complexity of ecological systems. This article describes one feasible approach toward this goal–the combining of inductive and deductive modeling techniques with the optimizing power of simple algorithms of Darwinian evolution that include information-theoretic model selection methods. Specifically, the author shows a way to extend classic genetic algorithms beyond typical parameter fitting of a single, previously chosen model to a more flexible technique that can work with a suite of possible models. Inclusion of the Akaike information-theoretic model selection method within an evolutionary algorithm makes it possible to accomplish simultaneous parameter fitting and parsimonious model selection.Experiments with synthetic data show the feasibility of this approach, and experiments with time-series field data of the zebra mussel invasion of Lake Champlain (United States) result in a model of the invasion dynamics that is consistent with the known hydrodynamic features of the lake and the motile life history stage of this invasive species.The author also describes a way to extend this approach with a modified genetic programming algorithm.},
 author = {James P. Hoffmann},
 doi = {10.1177/0037549706069103},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0037549706069103},
 journal = {SIMULATION},
 number = {7},
 pages = {439–450},
 title = {Simultaneous Inductive and Deductive Modeling of Ecological Systems via                 Evolutionary Computation and Information Theory},
 url = {https://doi-org.crai.referencistas.com/10.1177/0037549706069103},
 volume = {82},
 year = {2006f}
}

@article{doi:10.1177/0037549708099886,
 abstract = {The aim of the present work is to predict the fatigue life of a friction stir welded (FSW) joint in the 2024-T351 aluminum alloy using the finite element method in the framework of Fracture Analysis Code for Two Dimensions (FRANC2D/L). The simulation is conducted using linear elastic fracture mechanics based on Paris’ model, and the maximum tensile stress and displacement correlation methods are applied to calculate the crack direction and stress intensity factor, respectively. Several strategies are applied in order to predict the crack propagations through various welded zones regarding the corresponding parameters and Paris constants for each zone. The entire crack growth process is investigated step by step through all of the FSW zones, and the fatigue lifetimes of the FSW joint under various loading conditions are predicted by implementing the same procedure. The numerical results are validated with experimental and analytical work.},
 author = {Amirreza F. Golestaneh and Aidy Ali and S. Voon Wong and Faizal Mustapha and M. Zadeh},
 doi = {10.1177/0037549708099886},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0037549708099886},
 journal = {SIMULATION},
 number = {1},
 pages = {45–59},
 title = {Computational Investigation of Crack Behavior in Friction Stir Welding},
 url = {https://doi-org.crai.referencistas.com/10.1177/0037549708099886},
 volume = {85},
 year = {2009g}
}

@article{doi:10.1177/0037549710373571,
 abstract = {Delays in a railway network are a common problem that railway companies face in their daily operations. When a train is delayed, it may either be beneficial to let a connecting train wait so that passengers in the delayed train do not miss their connection, or it may be beneficial to let the connecting train depart on time to avoid further delays. These decisions naturally depend on the global structure of the network, on the schedule, on the passenger routes and on the imposed delays. The railway delay management (RDM) problem (in a broad sense) is to decide which trains have to wait for connecting trains and which trains have to depart on time. The offline version (i.e. when all delays are known in advance) is already NP-hard for very special networks. In this paper we show that the online railway delay management (ORDM) problem is PSPACE-hard. This result justifies the need for a simulation approach to evaluate wait policies for ORDM. For this purpose we present TOPSU—RDM, a simulation platform for evaluating and comparing different heuristics for the ORDM problem with stochastic delays. Our novel approach is to separate the actual simulation and the program that implements the decision-making policy, thus enabling implementations of different heuristics to ‘“compete”’ on the same instances and delay distributions. We also report on computational results indicating the worthiness of developing intelligent wait policies. For RDM and other logistic planning processes, it is our goal to bridge the gap between theoretical models, which are accessible to theoretical analysis, but are often too far away from practice, and the methods which are used in practice today, whose performance is almost impossible to measure.},
 author = {André Berger and Ralf Hoffmann and Ulf Lorenz and Sebastian Stiller},
 doi = {10.1177/0037549710373571},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0037549710373571},
 journal = {SIMULATION},
 number = {7},
 pages = {616–629},
 title = {Online railway delay management: Hardness, simulation and computation},
 url = {https://doi-org.crai.referencistas.com/10.1177/0037549710373571},
 volume = {87},
 year = {2011c}
}

@article{doi:10.1177/0037549713505761,
 abstract = {Decision-making on discrete event systems with alternative structural configurations is a field with application to the efficient design and operation of many systems, ranging from manufacturing facilities to communication networks. The solution of this problem may be afforded by its transformation into an optimization problem. A variety of statements for this optimization problem can be presented by using different formalisms able to describe the model of the system. These different statements allow developing diverse optimization algorithms for solving the problem, which may be very demanding for a computer. In this paper, several approaches are presented in order to reduce the computing requirements needed by the mentioned algorithms, some of them are implemented in one processor and others are based on distributed computing. In particular, this paper presents a new distributed methodology, which associates sets of alternative structural configurations of the system to different alternative aggregation Petri net (AAPNs), regarding the number of available processors. Under certain conditions, this methodology alleviates the computational requirements for every processor and speeds up the optimization process. A case-study is presented and different techniques are applied to solve it, for illustrating diverse distributed and non-distributed methodologies, regarding the available processors, as well as for comparing their relative performance.},
 author = {Juan-Ignacio Latorre and Emilio Jiménez},
 doi = {10.1177/0037549713505761},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0037549713505761},
 journal = {SIMULATION},
 number = {11},
 pages = {1310–1334},
 title = {Simulation-based optimization of discrete event systems with alternative structural configurations using distributed computation and the Petri net paradigm},
 url = {https://doi-org.crai.referencistas.com/10.1177/0037549713505761},
 volume = {89},
 year = {2013l}
}

@article{doi:10.1177/0037549716640877,
 abstract = {Computational disease spread models can be broadly classified into differential equation-based models (EBMs) and agent-based models (ABMs). We examine these models in the context of illuminating their hidden assumptions and the impact these may have on the model outcomes. Drawing relevant conclusions about the usability of a model requires reliable information regarding its modeling strategy and its associated assumptions. Hence, we aim to provide clear guidelines on the development of these models and delineate important modeling choices that cause the differences between the model outputs. In this study, we present a quantitative analysis of how the choice of model trajectories and temporal resolution (continuous versus discrete-event models), coupling between agents (instantaneous versus delayed interactions), and progress of patients from one stage of the disease to the next affect the overall outcomes of modeling disease spread. Our study reveals that the magnitude and velocity of the simulated epidemic depends critically on the selection of modeling principles, various assumptions of disease process, and the choice of time advance. In order to inform public health officials and improve reproducibility, these initial decisions of modelers should be carefully considered and recorded when building and documenting an ABM.},
 author = {Özgür Özmen and James J Nutaro and Laura L Pullum and Arvind Ramanathan},
 doi = {10.1177/0037549716640877},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0037549716640877},
 journal = {SIMULATION},
 number = {5},
 pages = {459–472},
 title = {Analyzing the impact of modeling choices and assumptions in compartmental epidemiological models},
 url = {https://doi-org.crai.referencistas.com/10.1177/0037549716640877},
 volume = {92},
 year = {2016q}
}

@article{doi:10.1177/0037549716674806,
 abstract = {Due to its unrivaled ability to predict the dynamical evolution of interacting atoms, molecular dynamics (MD) is a widely used computational method in theoretical chemistry, physics, biology, and engineering. Despite its success, MD is only capable of modeling timescales within several orders of magnitude of thermal vibrations, leaving out many important phenomena that occur at slower rates. The temperature-accelerated dynamics (TAD) method overcomes this limitation by thermally accelerating the state-to-state evolution captured by MD. Due to the algorithmically complex nature of the serial TAD procedure, implementations have yet to improve performance by parallelizing the concurrent exploration of multiple states. Here we utilize a discrete-event-based application simulator to introduce and explore a new speculatively parallel TAD (SpecTAD) method. We investigate the SpecTAD algorithm, without a full-scale implementation, by constructing an application simulator proxy (SpecTADSim). Following this method, we discover that a non-trivial relationship exists between the optimal SpecTAD parameter set and the number of CPU cores available at run-time. Furthermore, we find that a majority of the available SpecTAD boost can be achieved within an existing TAD application using relatively simple algorithm modifications.},
 author = {Richard J Zamora and Arthur F Voter and Danny Perez and Nandakishore Santhi and Susan M Mniszewski and Sunil Thulasidasan and Stephan J Eidenbenz},
 doi = {10.1177/0037549716674806},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0037549716674806},
 journal = {SIMULATION},
 number = {12},
 pages = {1065–1086},
 title = {Discrete event performance prediction of speculatively parallel temperature-accelerated dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1177/0037549716674806},
 volume = {92},
 year = {2016t}
}

@article{doi:10.1177/0037549716675956,
 abstract = {The development of a real-time driving simulator involves highly complex integrated and interdependent subsystems that require a large amount of computational time. When advanced hardware is unavailable for economic reasons, achieving real-time simulation is challenging, and thus delays are inevitable. Moreover, computational delays in the response of driving simulator subsystems reduce the fidelity of the simulation. In this paper, we propose a technique to decrease computational delays in a driving simulator. We used approximation techniques, sensitivity analysis, decomposition, and sampling techniques to develop a surrogate-based vehicle dynamic model (SBVDM). This global surrogate model can be used in place of the conventional vehicle dynamic model to reduce the computational burden while maintaining an acceptable accuracy. Our results showed that the surrogate model can significantly reduce computing costs compared to the computationally expensive conventional model. In addition, the response time of the SBVDM is nearly five times faster than the original simulation codes. Also, as a method to reduce hardware cost, the SBVDM was used and the results showed that most of the responses were accurate and acceptable in relation to longitudinal and lateral dynamics. Based on the results, the authors suggested that the proposed framework could be useful for developing low-cost vehicle simulation systems that require fast computational output.},
 author = {Nariman Fouladinejad and Nima Fouladinejad and Mohamad Kasim Abdul Jalil and Jamaludin Mohd Taib},
 doi = {10.1177/0037549716675956},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0037549716675956},
 journal = {SIMULATION},
 number = {12},
 pages = {1087–1102},
 title = {Development of a surrogate-based vehicle dynamic model to reduce computational delays in a driving simulator},
 url = {https://doi-org.crai.referencistas.com/10.1177/0037549716675956},
 volume = {92},
 year = {2016c}
}

@article{doi:10.1177/0037549717726595,
 abstract = {Domain-specific simulators often have an edge on general-purpose simulators in terms of performance. Their intricate knowledge of the domain allows them to aggressively optimize and take shortcuts. In contrast, simulators for more general formalisms, such as Discrete Event System Specification (DEVS), need to support a wider set of models. Their inability to use domain information prevents DEVS simulators from achieving as high performance as their domain-specific variants. To solve this problem, we introduce a way to enhance the simulation performance of DEVS models through the use of computational resource usage models, often termed “activity” models. These models augment general-purpose DEVS models with domain-specific information, which can be used by the simulator. We apply this information in the context of data structure optimization, load balancing, and model allocation. Activity-awareness is a non-invasive extension to the DEVS formalism, meaning that activity-augmented models remain perfectly valid for use in activity-unaware simulators. Similarly, models without activity can still be simulated by an activity-aware simulator. Our approach is validated by making PythonPDEVS, a Parallel DEVS simulator, activity-aware and evaluating the performance impact on a set of benchmark models.},
 author = {Yentl Van Tendeloo and Hans Vangheluwe},
 doi = {10.1177/0037549717726595},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0037549717726595},
 journal = {SIMULATION},
 number = {12},
 pages = {1045–1061},
 title = {Increasing the performance of a Discrete Event System Specification simulator by means of computational resource usage “activity” models},
 url = {https://doi-org.crai.referencistas.com/10.1177/0037549717726595},
 volume = {93},
 year = {2017r}
}

@article{doi:10.1177/0037549720975075,
 abstract = {There is increasing interest in the use of mechanism-based multi-scale computational models (such as agent-based models (ABMs)) to generate simulated clinical populations in order to discover and evaluate potential diagnostic and therapeutic modalities. The description of the environment in which a biomedical simulation operates (model context) and parameterization of internal model rules (model content) requires the optimization of a large number of free parameters. In this work, we utilize a nested active learning (AL) workflow to efficiently parameterize and contextualize an ABM of systemic inflammation used to examine sepsis. Contextual parameter space was examined using four parameters external to the model’s rule set. The model’s internal parameterization, which represents gene expression and associated cellular behaviors, was explored through the augmentation or inhibition of signaling pathways for 12 signaling mediators associated with inflammation and wound healing. We have implemented a nested AL approach in which the clinically relevant (CR) model environment space for a given internal model parameterization is mapped using a small Artificial Neural Network (ANN). The outer AL level workflow is a larger ANN that uses AL to efficiently regress the volume and centroid location of the CR space given by a single internal parameterization. We have reduced the number of simulations required to efficiently map the CR parameter space of this model by approximately 99%. In addition, we have shown that more complex models with a larger number of variables may expect further improvements in efficiency.},
 author = {Chase Cockrell and Jonathan Ozik and Nick Collier and Gary An},
 doi = {10.1177/0037549720975075},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0037549720975075},
 journal = {SIMULATION},
 note = {PMID:34744189},
 number = {4},
 pages = {287–296},
 title = {Nested active learning for efficient model contextualization and parameterization: pathway to generating simulated populations using multi-scale computational models},
 url = {https://doi-org.crai.referencistas.com/10.1177/0037549720975075},
 volume = {97},
 year = {2021d}
}

@article{doi:10.1177/00375497211039460,
 abstract = {Although in the ionizing radiation field many concepts and processes are currently recognized as radiobiological, there are also probabilistic ones, and a probabilistic treatment makes a better understanding about them. The purpose of this study is to develop a new radiobiological simulator that calculates the tumor control probability (TCP) for a tumor heterogeneously irradiated from a fractioned treatment. The three possible types of cells and the results of interactions of ionizing radiation with each cell of a determined volume are analyzed. For an irradiated region with a dose per fraction d, the simulator determines the radiation biological effects using the cell kill (K) and cell sub-lethal damage, volume, cell density, cell repair of damaged cells during the interfractions, and number of fractions. K is determined from its probabilistic complement, the cell survival (S), described with the linear-quadratic (LQ) S(d) model as K = 1 −LQ S(d). TCP is calculated from computational simulations as in the ratio of simulations with K = 100% and their total. This application opens new avenues for theoretical and experimental investigations concerning simulations of radiation treatments, and methodologies for therapy optimizations. Our simulator represents a novel methodology as TCP is calculated without analytical formulas, but based on its own probabilistic definition.},
 author = {Terman Frometa-Castillo and Anil Pyakuryal and Ganesh Narayanasamy and Amadeo Wals-Zurita and Raul Piseaux-Aillon and Asghar Mesbahi},
 doi = {10.1177/00375497211039460},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00375497211039460},
 journal = {SIMULATION},
 number = {4},
 pages = {285–294},
 title = {Computational simulator that calculates tumor control probability in a tumor heterogeneously irradiated for fractionated radiation oncology treatments},
 url = {https://doi-org.crai.referencistas.com/10.1177/00375497211039460},
 volume = {98},
 year = {2022e}
}

@article{doi:10.1177/00375497211061260,
 abstract = {While computational fluid dynamics (CFD) can solve a wide variety of fluid flow problems, accurate CFD simulations require significant computational resources and time. We propose a general method for super-resolution of low-fidelity flow simulations using deep learning. The approach is based on a conditional generative adversarial network (GAN) with inexpensive, low-fidelity solutions as inputs and high-fidelity simulations as outputs. The details, including the flexible structure, unique loss functions, and handling strategies, are thoroughly discussed, and the methodology is demonstrated using numerical simulations of incompressible flows. The distinction between low- and high-fidelity solutions is made in terms of discretization and physical modeling errors. Numerical experiments demonstrate that the approach is capable of accurately forecasting high-fidelity simulations.},
 author = {Mahdi Pourbagian and Ali Ashrafizadeh},
 doi = {10.1177/00375497211061260},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00375497211061260},
 journal = {SIMULATION},
 number = {8},
 pages = {645–663},
 title = {Super-resolution of low-fidelity flow solutions via generative adversarial networks},
 url = {https://doi-org.crai.referencistas.com/10.1177/00375497211061260},
 volume = {98},
 year = {2022o}
}

@article{doi:10.1177/00375497221098417,
 abstract = {In this paper, we propose to use a computational method of chaos control to simulate complex experimental spectra. This computational chaos control technique is based on the Ott–Grebogi–York (OGY) method. We chose the logistic map as the base mathematical model for the development of our work. For the numeric part, we created arbitrary precision algorithms to generate the solutions. This way, we completely eliminated any degradation of chaos from our results. These algorithms were also necessary for the proper perturbation process that the computational chaos control method requires. We control the chaos of the logistic map in two cases of Period 1 and one case of Period 2 to demonstrate that our control method works. The behavior of a complex experimental spectrum was taken and numerically simulated. The simulated spectrum was obtained by controlling the chaos of the logistic map in a variable way with the methods proposed in this work. Our results show that it is possible to simulate very complicated experimental spectra by computationally controlling the chaos of an equation unrelated to the experimental system.},
 author = {Jesús Manuel Rodríguez-Núñez and Aned de León and Martín E Molinar-Tabares and Mario Flores-Acosta and SJ Castillo},
 doi = {10.1177/00375497221098417},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00375497221098417},
 journal = {SIMULATION},
 number = {9},
 pages = {835–846},
 title = {Computational chaos control based on small perturbations for complex spectra simulation},
 url = {https://doi-org.crai.referencistas.com/10.1177/00375497221098417},
 volume = {98},
 year = {2022t}
}

@article{doi:10.1177/00375497221132566,
 abstract = {When parallel algorithms for simulation were introduced in the 1970s, their development and use interested only experts in parallel computation. This circumstance changed as multi-core processors became commonplace, putting a parallel computer into the hands of every modeler. A natural outcome is growing interest in parallel simulation among persons not intimately familiar with parallel computing. At the same time, parallel simulation tools continue to be developed with the implicit assumption that the modeler is knowledgeable about parallel programming. The unintended consequence is a rapidly growing number of users of parallel simulation tools that are unlikely to recognize when the interaction of race conditions, partitioning strategies, and simultaneous action in their simulation models make results non-reproducible, thereby calling into question the validity of conclusions drawn from the simulation data. We illustrate the potential dangers of exposing parallel algorithms to users who are not experts in parallel computation with example models constructed using existing parallel simulation tools. By doing so, we hope to refocus tool developers on usability, even if this new focus incurs loss of some performance.},
 author = {James Nutaro and Ozgur Ozmen},
 doi = {10.1177/00375497221132566},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00375497221132566},
 journal = {SIMULATION},
 number = {4},
 pages = {417–427},
 title = {Race conditions and data partitioning: risks posed by common errors to reproducible parallel simulations},
 url = {https://doi-org.crai.referencistas.com/10.1177/00375497221132566},
 volume = {99},
 year = {2023n}
}

@article{doi:10.1177/003754976400300307,
 author = {R.T. Harnett and F.J. Sansom and L.M. Warshawsky},
 doi = {10.1177/003754976400300307},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/003754976400300307},
 journal = {SIMULATION},
 number = {3},
 pages = {17–43},
 title = {MIDAS...an Analog Approach to Digital Computation},
 url = {https://doi-org.crai.referencistas.com/10.1177/003754976400300307},
 volume = {3},
 year = {1964i}
}

@article{doi:10.1177/003754976500500410,
 author = {T.D. Truitt},
 doi = {10.1177/003754976500500410},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/003754976500500410},
 journal = {SIMULATION},
 number = {4},
 pages = {248–257},
 title = {A discussion of the EAI approach to hybrid computation},
 url = {https://doi-org.crai.referencistas.com/10.1177/003754976500500410},
 volume = {5},
 year = {1965r}
}

@article{doi:10.1177/003754976600700616,
 abstract = {This paper is concerned with the general problem of de termining the time history of an unknown input signal, given only the recorded response of a known physical sys tem excited by the unknown input. The particular case considered is determination of true thrust from the re corded test stand response, with the physical system re sponse noise superimposed upon the thrust data. Passive or numerical filtering can smooth the trace, but these methods will attenuate the signal within a particular fre quency band, and thus may distort the data content of the recorded response. The paper describes analog computer techniques that can be used to reconstruct data distorted by such physical system response. The data may be recovered through an “active filtering” process, even when it contains compo nents of a higher frequency than the response noise being removed. Several examples are shown, including one case where the original signal was recovered intact even though the input signal excited a system resonance by passing through the natural frequency of the simulated test stand. The method described is not restricted to linear systems, but can take into account such nonlinearities as changing rocket booster mass, and nonlinear damping and spring force versus deflection relationships. The major require ment is to obtain sufficient information to permit deriva tion of a realistic mathematical model of the physical sys tem used for test and measurement purposes.},
 author = {Maughan S. Mason and George W. Clary},
 doi = {10.1177/003754976600700616},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/003754976600700616},
 journal = {SIMULATION},
 number = {6},
 pages = {318–323},
 title = {A computational compensation for measuring system dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1177/003754976600700616},
 volume = {7},
 year = {1966l}
}

@article{doi:10.1177/003754976801100208,
 author = {Stephen J. Kahne},
 doi = {10.1177/003754976801100208},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/003754976801100208},
 journal = {SIMULATION},
 number = {2},
 pages = {81–86},
 title = {Computational questions in optimal control, quasilinearization and hybrid computers},
 url = {https://doi-org.crai.referencistas.com/10.1177/003754976801100208},
 volume = {11},
 year = {1968l}
}

@article{doi:10.1177/003754979506500104,
 abstract = {A parallel distributed processing approach to the computation of localized fractal dimension values in imagery is pre sented. This approach is a further development of the covering method which requires only nearest neighbor commu nication. A major benefit of our approach is the ability to readily incorporate any boundary information that may be available. Man y fractal textures or surfaces are fractal only in distribution. With this in mind, we show that compari son of the fractal dimension distribrctions via Kullback-Leibler can give an improved texture discrimination capability over comparison of computed fractal dimension. Results are presented for a set of textures.},
 author = {George W. Rogers and Jeffrey L. Solka and Carey E. Priebe},
 doi = {10.1177/003754979506500104},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/003754979506500104},
 journal = {SIMULATION},
 number = {1},
 pages = {26–36},
 title = {A PDP Approach to Localized Fractal Dimension Computation with Segmentation Boundaries},
 url = {https://doi-org.crai.referencistas.com/10.1177/003754979506500104},
 volume = {65},
 year = {1995o}
}

@article{doi:10.1177/00380385211015537,
 author = {Alexander Halavais},
 doi = {10.1177/00380385211015537},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00380385211015537},
 journal = {Sociology},
 number = {6},
 pages = {1246–1247},
 title = {Book Review: Simon Lindgren, Data Theory: Interpretive Sociology and Computational Methods},
 url = {https://doi-org.crai.referencistas.com/10.1177/00380385211015537},
 volume = {55},
 year = {2021j}
}

@article{doi:10.1177/0038040720953218,
 abstract = {Research shows charter schools are more segregated by race and class than are traditional public schools. I investigate an underexamined mechanism for this segregation: Charter schools project identities corresponding to parents’ race- and class-specific parenting styles and educational values. I use computational text analysis to detect the emphasis on inquiry-based learning in the websites of all charter schools operating in 2015–16. I then estimate mixed linear regression models to test the relationships between ideological emphasis and school- and district-level poverty and ethnicity. I thereby transcend methodological problems in scholarship on charter school identities by collecting contemporary, populationwide data and by blending text analysis with hypothesis testing. Findings suggest charter school identities are both race and class specific, outlining a new mechanism by which school choice may consolidate parents by race and class—and paving the way for behavioral and longitudinal studies. This project contributes to literatures on school choice and educational stratification.},
 author = {Jaren R. Haber},
 doi = {10.1177/0038040720953218},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0038040720953218},
 journal = {Sociology of Education},
 number = {1},
 pages = {43–64},
 title = {Sorting Schools: A Computational Analysis of Charter School Identities and Stratification},
 url = {https://doi-org.crai.referencistas.com/10.1177/0038040720953218},
 volume = {94},
 year = {2021f}
}

@article{doi:10.1177/0040059915594790,
 author = {Maya Israel and Quentin M. Wherfel and Jamie Pearson and Saadeddine Shehab and Tanya Tapia},
 doi = {10.1177/0040059915594790},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0040059915594790},
 journal = {TEACHING Exceptional Children},
 number = {1},
 pages = {45–53},
 title = {Empowering K–12 Students With Disabilities to Learn Computational Thinking and Computer Programming},
 url = {https://doi-org.crai.referencistas.com/10.1177/0040059915594790},
 volume = {48},
 year = {2015g}
}

@article{doi:10.1177/0040059915597689,
 author = {Kristin Sayeski},
 doi = {10.1177/0040059915597689},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0040059915597689},
 journal = {TEACHING Exceptional Children},
 number = {1},
 pages = {7–8},
 title = {New Year, Expanded Opportunities},
 url = {https://doi-org.crai.referencistas.com/10.1177/0040059915597689},
 volume = {48},
 year = {2015r}
}

@article{doi:10.1177/0040059916670629,
 author = {Lauren W. Collins and Lori Fulton},
 doi = {10.1177/0040059916670629},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0040059916670629},
 journal = {TEACHING Exceptional Children},
 number = {3},
 pages = {194–203},
 title = {Promising Practices for Supporting Students with Disabilities through Writing in Science},
 url = {https://doi-org.crai.referencistas.com/10.1177/0040059916670629},
 volume = {49},
 year = {2017b}
}

@article{doi:10.1177/0040059916673310,
 author = {Kristin E. Harbour and Karen S. Karp and Amy S. Lingo},
 doi = {10.1177/0040059916673310},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0040059916673310},
 journal = {TEACHING Exceptional Children},
 number = {2},
 pages = {126–133},
 title = {Inquiry to Action: Diagnosing and Addressing Students’ Relational Thinking About the Equal Sign},
 url = {https://doi-org.crai.referencistas.com/10.1177/0040059916673310},
 volume = {49},
 year = {2016i}
}

@article{doi:10.1177/0040517507089754,
 abstract = {In this paper we report on computational fluid dynamics (CFD) simulation of airflow inside the nozzles used in Nozzle-Ring spinning. Using the CFD, air velocities at different locations of the nozzle were obtained and then drag forces acting on hair and yarn were computed. Z-twisted carded cotton yarns were produced at ring spinning machine with and without placing nozzle. Three nozzles were used, each having air inlets at different axial angle. Using the results of simulation, the role of air drag forces and angle of impact of air current on reduction in yarn hairiness could be explained. Nozzle-Ring yarns had a lower number of S3-hairs than the regular ring yarn. The drag forces played a dominant role in reducing the hairs. The angle of air inlets controlled the impact angle of air on the hair. At very high impact angle, curving of protruding hair was bound to occur during its folding, signifying the difficulty in wrapping the hair over the yarn and, hence, a lower reduction in hairiness.},
 author = {R.S. Rengasamy and Asis Patanaik and R.D. Anandjiwala},
 doi = {10.1177/0040517507089754},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0040517507089754},
 journal = {Textile Research Journal},
 number = {5},
 pages = {412–420},
 title = {Simulation of Airflow in Nozzle-ring Spinning using Computational Fluid Dynamics: Study on Reduction in Yarn Hairiness and the Role of Air Drag Forces and Angle of Impact of Air Current},
 url = {https://doi-org.crai.referencistas.com/10.1177/0040517507089754},
 volume = {78},
 year = {2008q}
}

@article{doi:10.1177/0040517508093440,
 abstract = {The process of coating textiles with a blade coater has been investigated with CFD (Computational Fluid Dynamics) software. The simulations have uncovered a pressure drop in the area between the edge of the knife and the textile. This drop can be explained by the varying height of the flow channel which is made of the knife and the textile in conjunction with the no slip condition that is applied to the walls. In the simulations this drop became arbitrarily deep with increasing viscosity leading to negative absolute pressure levels. This may be a hint that the no slip condition is not valid in this scenario that involves shear rates of the order of 103/s and dynamic viscosities of around 10 to 100Pas. With a proposed slip condition for the textile the pressure drop became less pronounced. At the same time the flow rate through the gap between knife and textile was reduced leading to a thinner coating. This can be a hint that a slip between coating material and textile can cause distortions in the coating up to a complete disruption which is reported frequently by the industry. This could eventually be avoided by reducing the varying height of the flow channel. Alternatively a vertical lift of the textile due to the negative pressure was taken into account. This lift appeared only with low tensions of the textile of about 1N/m and showed no influence on coating thickness and pressure drop.},
 author = {Markus Schmidt and Uwe Schloßer and Eckhard Schollmeyer},
 doi = {10.1177/0040517508093440},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0040517508093440},
 journal = {Textile Research Journal},
 number = {7},
 pages = {579–584},
 title = {Computational Fluid Dynamics Investigation of the Static Pressure at the Blade in a Blade Coating Process},
 url = {https://doi-org.crai.referencistas.com/10.1177/0040517508093440},
 volume = {79},
 year = {2009p}
}

@article{doi:10.1177/0040517513481869,
 abstract = {The paper presents computational fluid dynamics-based numerical simulation of the through-thickness air permeability of woven structures, applying the theory of jet systems. The flow through the interstices between the warp and weft threads is modeled as an “in-corridor”-ordered jet system, formed by nine jets, issuing from nine pores of the woven structure. Fifteen cases were simulated and three different turbulence models were applied in the simulation: k-ɛ, k-ω and Reynolds stress model. The five simulated woven structures were manufactured and their air permeability was measured experimentally. The performed validation of the numerical results with the experimental values of the air permeability showed very good correlation with the experimental results. The analysis and the verification showed that the method can be applied for further investigation not only of the woven fabrics’ air permeability, but also for investigation of the flow after a textile barrier of a woven type.},
 author = {Radostina A Angelova and Peter Stankov and Iskra Simova and Miroslav Kyosov},
 doi = {10.1177/0040517513481869},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0040517513481869},
 journal = {Textile Research Journal},
 number = {18},
 pages = {1887–1895},
 title = {Computational modeling and experimental validation of the air permeability of woven structures on the basis of simulation of jet systems},
 url = {https://doi-org.crai.referencistas.com/10.1177/0040517513481869},
 volume = {83},
 year = {2013b}
}

@article{doi:10.1177/0040517514540767,
 abstract = {This paper reports the computational results of the bending fatigue behaviors of four-step three-dimensional rectangular braided composite materials from a meso-scale approach. A full-size meso-scale model of a four-step three-dimensional braided composite was established to numerically analyze the deformation and damage under cyclic bending loading. The stress distribution, energy absorption, hysteresis loop features and damage morphologies were obtained to explain the structural effects on the deformation and damage of the three-dimensional braided composite material subjected to three-point bending cyclic loading. The influences of the microstructure on the fatigue behaviors have been discussed for designing the three-dimensional braided composite material with high fatigue damage tolerance.},
 author = {Liwei Wu and Bohong Gu},
 doi = {10.1177/0040517514540767},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0040517514540767},
 journal = {Textile Research Journal},
 number = {18},
 pages = {1915–1930},
 title = {Fatigue behaviors of four-step three-dimensional braided composite material: a meso-scale approach computation},
 url = {https://doi-org.crai.referencistas.com/10.1177/0040517514540767},
 volume = {84},
 year = {2014s}
}

@article{doi:10.1177/0040517515586162,
 abstract = {A three-dimensional computational model is established to simulate the air flow patterns in the rotor spinning unit of a rotor spinning machine. The effects of rotor speed, rotor diameter and rotor slide wall angle on air flow characteristics and hence yarn properties are investigated. The airstream accelerates from the transfer channel inlet to the outlet. There are velocity differences in both the cross-section and along the transfer channel, causing hooked fibers to straighten. The airstream swirls around the rotor at a high speed. However, vortices that can cause fiber curving and buckling are formed inside the rotor. The effect of rotor speed is significant. There are more vortices near the wall at a lower rotor speed, while too large a rotor speed can lead to an excessive centrifugal force, thus increasing yarn breakages. The rotor diameter affects the flow characteristics in a way similar to that of rotor speed. As a smaller slide wall angle generates higher velocities in the transfer channel and more stable velocities in the rotor groove, a small angle is preferable. Computational modeling has provided a useful insight into the rotor spinning flow pattern, thus it can be used to optimize the rotor design to produce better rotor spun yarn.},
 author = {HT Lin and YC Zeng and J Wang},
 doi = {10.1177/0040517515586162},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0040517515586162},
 journal = {Textile Research Journal},
 number = {2},
 pages = {115–126},
 title = {Computational simulation of air flow in the rotor spinning unit},
 url = {https://doi-org.crai.referencistas.com/10.1177/0040517515586162},
 volume = {86},
 year = {2016k}
}

@article{doi:10.1177/0040517517698985,
 abstract = {The conventional rotor spinning unit generates flow vortices in the transfer channel upstream region which affect the fiber configuration and consequently yarn properties. Geometry and spinning parameters such as transfer channel length, inlet width, rotor outlet pressure, opening roller speed, and diameter were found to be key parameters influencing airflow characteristics. To reduce the flow vortices in the upper stream region, modifications of the transfer channel were proposed, and their airflow fields were analyzed using computational fluid dynamics. Three designs were studied: a round transfer channel inlet, a bypass channel for extra air supply, and one with both the bypass and the round inlet. Analysis of airflow revealed that the design with both round transfer channel inlet and a bypass proved to be very effective in properly directing the flow and minimizing vortices. The design was also characterized by smoother velocity streamlines and maximum mass flow across the transfer channel. A conventional rotor spinning unit was modified in which a round transfer channel inlet corner and a bypass channel were utilized to conduct the experimental tests. Three sets of yarn samples were produced using the conventional and modified rotor spinning units under different rotor speed conditions. Yarn properties were tested. Properties such as tenacity, CVm%, and thin and thick places of the spun yarns produced by the new design improved compared to that of the conventional yarn.},
 author = {Huiting Lin and Josep M Bergadà and Yongchun Zeng and Nicholus T Akankwasa and Yuze Zhang and Jun Wang},
 doi = {10.1177/0040517517698985},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0040517517698985},
 journal = {Textile Research Journal},
 number = {11},
 pages = {1244–1262},
 title = {Rotor spinning transfer channel design optimization via computational fluid dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1177/0040517517698985},
 volume = {88},
 year = {2018f}
}

@article{doi:10.1177/0040517517705627,
 abstract = {A finite element model is created from realistic microstructural data in order to predict the mechanical properties of two-dimensional tri-axially braided textile composites (2DTBCs), specifically with a focus on the prediction of their compressive strength. The virtually manufactured model is based on information about the realistic geometry of the textile architecture and nonlinear material properties of fiber tows and matrix. A parametric representative unit cell model is developed in a fully three-dimensional setting for the construction of representative textile architectures. The computational model is utilized to predict the compressive strength of 2DTBCs.},
 author = {Christian Heinrich and Wooseok Ji and Anthony M Waas},
 doi = {10.1177/0040517517705627},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0040517517705627},
 journal = {Textile Research Journal},
 number = {14},
 pages = {1593–1604},
 title = {Computational model for virtually characterizing two-dimensional tri-axially braided composites using realistic microstructural details},
 url = {https://doi-org.crai.referencistas.com/10.1177/0040517517705627},
 volume = {88},
 year = {2018i}
}

@article{doi:10.1177/0042085911429974,
 abstract = {The purpose of this article is to describe several conceptual areas that warrant attention by scholars and practitioners interested in improving access and opportunity to science, technology, engineering, and mathematics (STEM) learning in urban cities. Thinking conceptually about the urban context has been a part of intellectual traditions in the social sciences for decades. Like in other fields of study, for example, economics, sociology, and political science, the treatment of urban communities as unique geospatial organizations in terms of sites of intervention and policy reform has a history in STEM education, but the aligned research and development strategy is best characterized as nascent.},
 author = {William F. Tate and Brittni D. Jones and Elizabeth Thorne-Wallington and Mark C. Hogrebe},
 doi = {10.1177/0042085911429974},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0042085911429974},
 journal = {Urban Education},
 number = {2},
 pages = {399–433},
 title = {Science and the City: Thinking Geospatially about Opportunity to Learn},
 url = {https://doi-org.crai.referencistas.com/10.1177/0042085911429974},
 volume = {47},
 year = {2012r}
}

@article{doi:10.1177/0042085913490554,
 abstract = {Following the belief that diversity breeds innovation in scientific endeavors, there is a national push for more diversity in the science, technology, engineering, and mathematics (STEM) workforce in order to maintain national economic competitiveness. Currently, STEM-related employment is only 28% non-White; however, greater efforts to recruit and retain underrepresented minorities should increase this figure. Amidst the attention given to supporting “leaky pipelines,” less emphasis has been placed on mitigating challenges associated with bringing diverse cultures together. This article presents a framework for supporting underrepresented minorities in building STEM-relevant skills and enhancing their ability to collaborate with peers different from themselves.},
 author = {Shaundra Bryant Daily and Wanda Eugene},
 doi = {10.1177/0042085913490554},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0042085913490554},
 journal = {Urban Education},
 number = {5},
 pages = {682–704},
 title = {Preparing the Future STEM Workforce for Diverse Environments},
 url = {https://doi-org.crai.referencistas.com/10.1177/0042085913490554},
 volume = {48},
 year = {2013i}
}

@article{doi:10.1177/0042085913490555,
 abstract = {Indigenous people are significantly underrepresented in the fields of science, technology, engineering and math (STEM). The solution to this problem requires a more robust lens than representation or access alone. Specifically, it will require careful consideration of the ecological contexts of Indigenous school age youth, of which more than 70% live in urban communities (National Urban Indian Family Coalition, 2008). This article reports emergent design principles derived from a community-based design research project. These emergent principles focus on the conceptualization and uses of technology in science learning environments designed for urban Indigenous youth. In order to strengthen learning environments for urban Indigenous youth, it is necessary, we argue, that scholars and educators take seriously the ways in which culture mediates relationships with, conceptions of, and innovations in technology and technologically related disciplines. Recognizing these relationships will inform the subsequent implications for learning environments.},
 author = {Megan Bang and Ananda Marin and Lori Faber and Eli S. Suzukovich},
 doi = {10.1177/0042085913490555},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0042085913490555},
 journal = {Urban Education},
 number = {5},
 pages = {705–733},
 title = {Repatriating Indigenous Technologies in an Urban Indian Community},
 url = {https://doi-org.crai.referencistas.com/10.1177/0042085913490555},
 volume = {48},
 year = {2013b}
}

@article{doi:10.1177/0042085913491219,
 abstract = {This article investigates the motivations of African American and Latino girls (N = 41) who navigate urban Southwest school districts during the day, but voluntarily attend a 2-year, culturally responsive multimedia program after school and into the summer. Understanding that girls from economically disadvantaged settings are indeed motivated to become technological innovators but often do not have access to the necessary resources to follow their interest, our program—entitled COMPUGIRLS—assumes a culturally responsive computing approach. This research examines particular features of the program (e.g., asset building, reflections, and connectedness) that attracted and retained the Latina (74%) and African American (19%) adolescent (ages 13-18) participants as well as to what extent the culturally relevant aspects of the curriculum assist with program retention and/or affect the students’ vision of themselves as a future technologist. An evaluative approach gathered 2 years of data from the participants. Field notes from observations and interviews were transcribed and reviewed to extract themes and areas of convergence. As a standpoint theory project, the authors center the girls’ voices as the primary data sources. Two primary themes emerged from the data to explain girls’ sustained motivation. The first was the challenge of learning and mastering the technology. For many, this also included disproving the stereotypes of their abilities by age, gender, and race. The second theme was being able to manipulate technology and learning experiences as a means of self-expression and research, particularly if the results could be used to inform their community and peers. The authors posit that much of the program impact was because of the culturally responsive practices (asset building, reflection, and connectedness) embedded within the curriculum. Implications for urban educators and program developers are considered.},
 author = {Kimberly A. Scott and Mary Aleta White},
 doi = {10.1177/0042085913491219},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0042085913491219},
 journal = {Urban Education},
 number = {5},
 pages = {657–681},
 title = {COMPUGIRLS’ Standpoint: Culturally Responsive Computing and Its Effect on Girls of Color},
 url = {https://doi-org.crai.referencistas.com/10.1177/0042085913491219},
 volume = {48},
 year = {2013o}
}

@article{doi:10.1177/0042085917714512,
 abstract = {This article advocates for the intersectional rights of teachers and students of computer science (CS) and special education (SPE) in urban education. Using an intersectional nepantla lens, we propose that CS education be accessible to all SPE teachers and students with dis/abilities. We argue for a focus on social-emotional intersectional rights as crucial foundations for an equitable approach to teaching and learning in CS SPE. We end with implications for educational stakeholders and teacher education programs that open pathways for socioemotional and intersectional rights of underrepresented teachers and students of CS and SPE.},
 author = {Cueponcaxochitl D. Moreno Sandoval and David I. Hernández Saca and Adai A. Tefera},
 doi = {10.1177/0042085917714512},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0042085917714512},
 journal = {Urban Education},
 number = {5},
 pages = {675–704},
 title = {Intersectional Rights of Teachers and Students in Computer Science and Special Education: Implications for Urban Schooling},
 url = {https://doi-org.crai.referencistas.com/10.1177/0042085917714512},
 volume = {56},
 year = {2021l}
}

@article{doi:10.1177/0042098016664305,
 abstract = {As visions of smart urbanism gain traction around the world, it is crucial that we question the benefits that an increasingly technologised urbanity promise. It is not about the technology, but bettering peoples’ lives, insist smart city advocates. In this paper, I question the progressive potential of the smart city drawing on the case of Singapore’s Smart Nation initiative. Using the case studies of the smart home and ‘learning to code’ movement, I highlight the limits of such ‘smart’ interventions as they are stunted by the neoliberal-developmental logics of the state, thereby facilitating authoritarian consolidation in Singapore. As such, this paper distinguishes itself from previous works on the neoliberal smart city by situated smart urbanism within the socio-political dynamics of neoliberalism-as-developmental strategy. For smart urbanism to better peoples’ everyday lives, technological ‘solutionism’ needs to be replaced with more human-centric framings and understandings of urban challenges.},
 author = {Ezra Ho},
 doi = {10.1177/0042098016664305},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0042098016664305},
 journal = {Urban Studies},
 number = {13},
 pages = {3101–3118},
 title = {Smart subjects for a Smart Nation? Governing (smart)mentalities in Singapore},
 url = {https://doi-org.crai.referencistas.com/10.1177/0042098016664305},
 volume = {54},
 year = {2017i}
}

@article{doi:10.1177/00420980211039407,
 abstract = {The Covid-19 pandemic has accelerated digitalisation efforts in many countries as they try to contain the virus. With the physical handling of cash posing as a potential virus transmission risk, digital payments have become important in the urgent transition to a cashless society and a key feature of smart city projects. Critical analyses have typically framed smart cities as neoliberalist developmental projects that see the partnering of the state with private corporations. However, it is unclear how the smart city emerges under the technocratic inclinations of the developmental state. Focusing on the digital payments project under Singapore’s Smart Nation initiative, this paper unpacks the discursive practices employed in mobilising citizen support for electronic payments through a critical analysis of publicly available government materials and recent initiatives. The discourse surrounding digital payments is bound up in narratives surrounding economic competitiveness, technological progress and public health and safety, and strongly rooted in a technocratic governance ethos that limits genuine citizen participation in shaping smart payments technologies. This paper argues that such discursive framings represent a missed opportunity to build a smart city that is truly citizen-centric. This reorientation requires more bottom-up and grassroots-based modes of governance that reformulate smart citizenship into one that pays greater attention to the affective and social contexts behind digital technologies.},
 author = {Gordon Kuo Siong Tan},
 doi = {10.1177/00420980211039407},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00420980211039407},
 journal = {Urban Studies},
 number = {12},
 pages = {2582–2598},
 title = {Citizens go digital: A discursive examination of digital payments in Singapore’s Smart Nation project},
 url = {https://doi-org.crai.referencistas.com/10.1177/00420980211039407},
 volume = {59},
 year = {2022s}
}

@article{doi:10.1177/00420980221097590,
 abstract = {The critical research agenda on smart cities has tended to assume a largely top-down orientation in which powerful actors like the state and corporations enact programmes to embed Information & Communication Technologies (ICT) in the urban landscape. Because of the way research has framed this relation of power, the dominant response has been to seek social justice by either contesting these top-down exercises of (digital) power or by reconceptualising the smart city ‘from below’. In this paper, we join a growing chorus of voices recognising the importance of interstitial actors that influence the ways in which the smart city manifests. We draw on a five-year ongoing study in Calgary, Alberta, to examine two actor groups that are, properly, neither top-down nor bottom-up, but play an important role in envisioning, implementing and contesting how ‘smartness’ is framed. The first set of actors, situated between the top and bottom of the smart city hierarchy, are most prominently community associations, non-profit organisations and ad-hoc task groups. The second group is comprised of groups with different digital practices, whose spectre of marginalisation influences how digital systems are articulated and pursued. These actors strategically move between different interstices in order to enact particular kinds of political influence, and often influence smart cities by virtue of their absence, profoundly impacting urban political geographies of smartness.},
 author = {Ryan Burns and Preston Welker},
 doi = {10.1177/00420980221097590},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00420980221097590},
 journal = {Urban Studies},
 note = {PMID:36741348},
 number = {2},
 pages = {308–324},
 title = {Interstitiality in the smart city: More than top-down and bottom-up smartness},
 url = {https://doi-org.crai.referencistas.com/10.1177/00420980221097590},
 volume = {60},
 year = {2023e}
}

@article{doi:10.1177/0047239517748936,
 abstract = {The purpose of this literature review is to understand the current state of research on tools that collect data for the purpose of formative assessment. We were interested in identifying the types of data collected by these tools, how these data were processed, and how the processed data were presented to the instructor or student for the purpose of formative assessment. We identified two categories of data: machine-graded and activity stream data. The data were processed using three methods: unprocessed activity streams, descriptive data analysis, and data mining. Processed data were presented to students through reports and real-time feedback and to instructors through reports and visual dashboards.},
 author = {Rob Nyland},
 doi = {10.1177/0047239517748936},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0047239517748936},
 journal = {Journal of Educational Technology Systems},
 number = {4},
 pages = {505–526},
 title = {A Review of Tools and Techniques for Data-Enabled Formative Assessment},
 url = {https://doi-org.crai.referencistas.com/10.1177/0047239517748936},
 volume = {46},
 year = {2018i}
}

@article{doi:10.1177/0047239520926971,
 abstract = {Courses on computer programming are included in the curricula of almost all engineering disciplines. We surveyed the research literature and identified the techniques that are commonly used by instructors for teaching these courses. We observed that visual programming and game-based learning can enhance computational thinking and problem-solving skills in students and may be used to introduce them to programming. Robot programming may be used to attract students to programming, but the success of this technique is subjected to the availability of robots. Pair and collaborative programming allows students to learn from one another and write efficient programs. Assessment systems help instructors in evaluating programs written by students and provide them with timely feedback. Furthermore, an analysis of citations showed that Scratch is the most researched tool for teaching programming. We discuss how these techniques may be used to teach introductory courses, advanced courses, and massive open online courses on programming.},
 author = {Kanika and Shampa Chakraverty and Pinaki Chakraborty},
 doi = {10.1177/0047239520926971},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0047239520926971},
 journal = {Journal of Educational Technology Systems},
 number = {2},
 pages = {170–198},
 title = {Tools and Techniques for Teaching Computer Programming: A Review},
 url = {https://doi-org.crai.referencistas.com/10.1177/0047239520926971},
 volume = {49},
 year = {2020k}
}

@article{doi:10.1177/00472395211018801,
 abstract = {The computer science (CS) unplugged approach intends to teach CS concepts and computational thinking skills without employing any digital tools. The current study conducted a systematic literature review to analyze research studies that conducted investigations related to implementations of CS unplugged activities. A systematic review procedure was developed and applied to detect and subsequently review relevant research studies published from 2010 to 2019. It was found that 55 research studies (17 articles + 38 conference proceedings) satisfied the inclusion criteria for the analysis. These research studies were then examined with regard to their demographic characteristics, research methodologies, research results, and main findings. It was found that the unplugged approach was realized and utilized differently among researchers. The majority of the studies used the CS unplugged term when referring to “paper–pencil activities,” “problem solving,” “storytelling,” “games,” “tangible programming,” and even “robotics.”},
 author = {Ali Battal and Gülgün Afacan Adanır and Yasemin Gülbahar},
 doi = {10.1177/00472395211018801},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00472395211018801},
 journal = {Journal of Educational Technology Systems},
 number = {1},
 pages = {24–47},
 title = {Computer Science Unplugged: A Systematic Literature Review},
 url = {https://doi-org.crai.referencistas.com/10.1177/00472395211018801},
 volume = {50},
 year = {2021a}
}

@article{doi:10.1177/00472816211072533,
 abstract = {This article follows up on the conversation about new streams of approaches in technical communication and user experience (UX) design, i.e., design thinking, content strategy, and artificial intelligence (AI), which afford implications for professional practice. By extending such implications to technical communication pedagogy, we aim to demonstrate the importance of paying attention to these streams in our programmatic development and provide strategies for doing so.},
 author = {Jason Tham and Tharon Howard and Gustav Verhulsdonck},
 doi = {10.1177/00472816211072533},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00472816211072533},
 journal = {Journal of Technical Writing and Communication},
 number = {4},
 pages = {428–459},
 title = {Extending Design Thinking, Content Strategy, and Artificial Intelligence into Technical Communication and User Experience Design Programs: Further Pedagogical Implications},
 url = {https://doi-org.crai.referencistas.com/10.1177/00472816211072533},
 volume = {52},
 year = {2022o}
}

@article{doi:10.1177/0048393120917641,
 abstract = {This article argues that agent-based modeling (ABM) is the methodological implication of Lawson’s championed ontological turn in economics. We single out three major properties of agent-based computational economics (ACE), namely, autonomous agents, social interactions, and the micro-macro links, which have been well accepted by the ACE community. We then argue that ACE does make a full commitment to the ontology of economics as proposed by Lawson, based on his prompted critical realism. Nevertheless, the article also points out the current limitations or constraints of ACE. Efforts to overcome them are deemed to be crucial before ACE can make itself more promising to the current ontological turn in economics.},
 author = {Shu-Heng Chen},
 doi = {10.1177/0048393120917641},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0048393120917641},
 journal = {Philosophy of the Social Sciences},
 number = {3},
 pages = {238–259},
 title = {On the Ontological Turn in Economics: The Promises of Agent-Based Computational Economics},
 url = {https://doi-org.crai.referencistas.com/10.1177/0048393120917641},
 volume = {50},
 year = {2020e}
}

@article{doi:10.1177/0049124115610347,
 abstract = {Model uncertainty is pervasive in social science. A key question is how robust empirical results are to sensible changes in model specification. We present a new approach and applied statistical software for computational multimodel analysis. Our approach proceeds in two steps: First, we estimate the modeling distribution of estimates across all combinations of possible controls as well as specified functional form issues, variable definitions, standard error calculations, and estimation commands. This allows analysts to present their core, preferred estimate in the context of a distribution of plausible estimates. Second, we develop a model influence analysis showing how each model ingredient affects the coefficient of interest. This shows which model assumptions, if any, are critical to obtaining an empirical result. We demonstrate the architecture and interpretation of multimodel analysis using data on the union wage premium, gender dynamics in mortgage lending, and tax flight migration among U.S. states. These illustrate how initial results can be strongly robust to alternative model specifications or remarkably dependent on a knife-edge specification.},
 author = {Cristobal Young and Katherine Holsteen},
 doi = {10.1177/0049124115610347},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0049124115610347},
 journal = {Sociological Methods & Research},
 number = {1},
 pages = {3–40},
 title = {Model Uncertainty and Robustness: A Computational Framework for Multimodel Analysis},
 url = {https://doi-org.crai.referencistas.com/10.1177/0049124115610347},
 volume = {46},
 year = {2017r}
}

@article{doi:10.1177/0049124117729703,
 abstract = {This article proposes a three-step methodological framework called computational grounded theory, which combines expert human knowledge and hermeneutic skills with the processing power and pattern recognition of computers, producing a more methodologically rigorous but interpretive approach to content analysis. The first, pattern detection step, involves inductive computational exploration of text, using techniques such as unsupervised machine learning and word scores to help researchers to see novel patterns in their data. The second, pattern refinement step, returns to an interpretive engagement with the data through qualitative deep reading or further exploration of the data. The third, pattern confirmation step, assesses the inductively identified patterns using further computational and natural language processing techniques. The result is an efficient, rigorous, and fully reproducible computational grounded theory. This framework can be applied to any qualitative text as data, including transcribed speeches, interviews, open-ended survey data, or ethnographic field notes, and can address many potential research questions.},
 author = {Laura K. Nelson},
 doi = {10.1177/0049124117729703},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0049124117729703},
 journal = {Sociological Methods & Research},
 number = {1},
 pages = {3–42},
 title = {Computational Grounded Theory: A Methodological Framework},
 url = {https://doi-org.crai.referencistas.com/10.1177/0049124117729703},
 volume = {49},
 year = {2020s}
}

@article{doi:10.1177/0049124119826159,
 abstract = {Complex social scientific theories are conventionally tested using linear structural equation modeling (SEM). However, the underlying assumptions of linear SEM often prove unrealistic, making the decomposition of direct and indirect effects problematic. Recent advancements in causal mediation analysis can help to address these shortcomings, allowing for causal inference when a new set of identifying assumptions are satisfied. This article reviews how these ideas can be generalized to multiple mediators, with a focus on the posttreatment confounding and causal ordering cases. Using the potential outcome framework as a rigorous tool for causal inference, the application is the theory of procedural justice policing. Analysis of data from two randomized experiments shows that making similar parametric assumptions to SEMs and using G-computation improve the viability of effect decomposition. The article concludes with a discussion of how causal mediation analysis improves upon SEM and the potential limitation of the methods.},
 author = {Krisztián Pósch},
 doi = {10.1177/0049124119826159},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0049124119826159},
 journal = {Sociological Methods & Research},
 number = {3},
 pages = {1376–1406},
 title = {Testing Complex Social Theories With Causal Mediation Analysis and G-Computation: Toward a Better Way to Do Causal Structural Equation Modeling},
 url = {https://doi-org.crai.referencistas.com/10.1177/0049124119826159},
 volume = {50},
 year = {2021o}
}

@article{doi:10.1177/00491241221122596,
 abstract = {This article presents a computational approach to examining immigrant incorporation through shifts in the social “mainstream.” Analyzing a historical corpus of American etiquette books, texts from 1922–2017 describing social norms, we identify mainstream shifts related to long-standing groups which once were and may currently still be seen as immigrant outsiders in the United States: Catholic, Chinese, Irish, Italian, Jewish, Mexican, and Muslim groups. The analysis takes a computational grounded theory approach, combining qualitative readings and computational text analyses. Using word embeddings, we operationalize the chosen groups as focal group concepts. We extract sections of text that are salient to the focal group concepts to create group-specific text corpora. Two computational approaches make it possible to examine mainstream shifts in these corpora. First, we use sentiment analysis to observe the positive sentiment in each corpus and its change over time. Second, we observe changes in each corpus’s position on a semantic dimension represented by the poles of “strange” and “normal.” The results indicate mainstream shifts through increases in positive sentiment and movement from strange to normal over time for most of the group-specific corpora. These research techniques can be adapted to other studies of social sentiment and symbolic inclusion.},
 author = {Andrea Voyer and Zachary D. Kline and Madison Danton and Tatiana Volkova},
 doi = {10.1177/00491241221122596},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00491241221122596},
 journal = {Sociological Methods & Research},
 number = {4},
 pages = {1540–1579},
 title = {From Strange to Normal: Computational Approaches to Examining Immigrant Incorporation Through Shifts in the Mainstream},
 url = {https://doi-org.crai.referencistas.com/10.1177/00491241221122596},
 volume = {51},
 year = {2022q}
}

@article{doi:10.1177/00491241221122616,
 abstract = {This paper considers the adoption of computational techniques within research designs modeled after the extended case method. Echoing calls to augment the power of contemporary researchers through the adoption of computational text analysis methods, we offer a framework for thinking about how such techniques can be integrated into quasi-ethnographic workflows to address broad, structural sociological claims. We focus, in particular, on how this adoption of novel forms of evidence impacts corpus design and interpretation (which we tie to matters of casing), theoretical elaboration (which we associate to moving empirical claims across scales and empirical domains), and verification (which we see as a process of reflexive scaffolding of theoretical claims). We provide an example of the use of this framework through a study of the marketization of social scientific knowledge in the United Kingdom.},
 author = {Juan Pablo Pardo-Guerra and Prithviraj Pahwa},
 doi = {10.1177/00491241221122616},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00491241221122616},
 journal = {Sociological Methods & Research},
 number = {4},
 pages = {1826–1867},
 title = {The Extended Computational Case Method: A Framework for Research Design},
 url = {https://doi-org.crai.referencistas.com/10.1177/00491241221122616},
 volume = {51},
 year = {2022k}
}

@article{doi:10.1177/0049124196025001002,
 abstract = {This article examines in detail some of the principal strategies used by expert systems to represent and reason about knowledge. It identifies strengths and weaknesses of these different strategies, with particular reference to sociological knowledge as illustrated by the work of Erving Goffman. The article ends with a summary of the current status of expert systems in sociology and prospects for using expert systems to help create a systematic framework for computational sociology.},
 author = {EDWARD E. BRENT},
 doi = {10.1177/0049124196025001002},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0049124196025001002},
 journal = {Sociological Methods & Research},
 number = {1},
 pages = {31–59},
 title = {Expert Systems and their Role in Computational Sociology},
 url = {https://doi-org.crai.referencistas.com/10.1177/0049124196025001002},
 volume = {25},
 year = {1996b}
}

@article{doi:10.1177/0075424206297857,
 abstract = {This article illustrates the utility of a variety of quantitative techniques by applying them to phonetic data from the traditional English dialects. The techniques yield measures of variation in phonetic usage among English localities, identify dialect regions as clusters of localities with relatively similar patterns of usage, distinguish regions of relative uniformity from transitional zones with substantially greater variation, and identify regionally coherent groups of features that can be used to distinguish some dialect regions. Complementing each other, the techniques provide a reasonably objective method for classifying at least some traditional English dialect regions on the basis of characteristic features. The results largely corroborate standard presentations in the literature but differ in the placement of regional boundaries and identification of regional features, as well as in placing those systemic elements in a broader context of largely continuous and often random variation.},
 author = {Robert G. Shackleton},
 doi = {10.1177/0075424206297857},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0075424206297857},
 journal = {Journal of English Linguistics},
 number = {1},
 pages = {30–102},
 title = {Phonetic Variation in the Traditional English Dialects: A Computational Analysis},
 url = {https://doi-org.crai.referencistas.com/10.1177/0075424206297857},
 volume = {35},
 year = {2007q}
}

@article{doi:10.1177/0081175018777988,
 abstract = {False positive findings are a growing problem in many research literatures. We argue that excessive false positives often stem from model uncertainty. There are many plausible ways of specifying a regression model, but researchers typically report only a few preferred estimates. This raises the concern that such research reveals only a small fraction of the possible results and may easily lead to nonrobust, false positive conclusions. It is often unclear how much the results are driven by model specification and how much the results would change if a different plausible model were used. Computational model robustness analysis addresses this challenge by estimating all possible models from a theoretically informed model space. We use large-scale random noise simulations to show (1) the problem of excess false positive errors under model uncertainty and (2) that computational robustness analysis can identify and eliminate false positives caused by model uncertainty. We also draw on a series of empirical applications to further illustrate issues of model uncertainty and estimate instability. Computational robustness analysis offers a method for relaxing modeling assumptions and improving the transparency of applied research.},
 author = {John Muñoz and Cristobal Young},
 doi = {10.1177/0081175018777988},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0081175018777988},
 journal = {Sociological Methodology},
 number = {1},
 pages = {1–33},
 title = {We Ran 9 Billion Regressions: Eliminating False Positives through Computational Model Robustness},
 url = {https://doi-org.crai.referencistas.com/10.1177/0081175018777988},
 volume = {48},
 year = {2018h}
}

@article{doi:10.1177/0081175019867855,
 author = {Jan Goldenstein and Philipp Poschmann},
 doi = {10.1177/0081175019867855},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0081175019867855},
 journal = {Sociological Methodology},
 number = {1},
 pages = {144–151},
 title = {A Quest for Transparent and Reproducible Text-Mining Methodologies in Computational Social Science},
 url = {https://doi-org.crai.referencistas.com/10.1177/0081175019867855},
 volume = {49},
 year = {2019g}
}

@article{doi:10.1177/0092055X211033632,
 abstract = {Despite the centrality of data analysis to the discipline, sociology departments are currently falling short of teaching both undergraduate and graduate students crucial computing and statistical software skills. We argue that sociology instructors must intentionally and explicitly teach computing skills alongside statistical concepts to prepare their students for participation in a data-driven world. We illuminate foundational concepts for computing in the social sciences and provide easy-to-integrate recommendations for building competency with these concepts in the form of a workshop designed to introduce sociology undergraduate and graduate students to the logic of statistical software. We use our workshop to show that students appreciate and gain confidence from being taught how to think about computing.},
 author = {Amy L. Johnson and Rebecca D. Gleit},
 doi = {10.1177/0092055X211033632},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0092055X211033632},
 journal = {Teaching Sociology},
 number = {1},
 pages = {49–61},
 title = {Teaching for a Data-Driven Future: Intentionally Building Foundational Computing Skills},
 url = {https://doi-org.crai.referencistas.com/10.1177/0092055X211033632},
 volume = {50},
 year = {2022h}
}

@article{doi:10.1177/0092055X231175176,
 abstract = {The increasing ubiquity of gamification in everyday life normalizes it as a motivational tool. While much scholarship supports gamification, labor sociologists have long problematized the phenomenon. In this mixed-methods action research study, we explore the results of gamifying a lesson on gamification in a sociology of work course. We designed two gamified activities with varying degrees of consent that followed a lesson on gamification and consent. Students rated how problematic a series of gamified work scenarios were before and after the intervention. Our quantitative data did not show a significant increase in students’ ability to identify consent after the intervention, but we did discover that students took either an employee or employer’s perspective in their rating justifications. Furthermore, these findings were gendered. This article highlights the need for a more critical take on gamification in the classroom. We conclude by suggesting ways practitioners can teach about gamification in other contexts.},
 author = {Brandon Folse and Frederick J. Poole},
 doi = {10.1177/0092055X231175176},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0092055X231175176},
 journal = {Teaching Sociology},
 number = {3},
 pages = {206–218},
 title = {Gamifying Gamification in the Sociology Classroom},
 url = {https://doi-org.crai.referencistas.com/10.1177/0092055X231175176},
 volume = {52},
 year = {2024f}
}

@article{doi:10.1177/00936502211023333,
 abstract = {Using the case of the 2019 European election, the study compares the visual self-depiction of female and male political candidates from all European Union’s 28 member states on social networking sites and their depiction in the news coverage. It thereby investigates to what degree the news coverage and politicians’ self-depiction employs visual gender stereotypes. Moreover, the study presents results on differences in the depiction of male and female candidates across party lines. With the help of computational vision, we demonstrate that, while differences between progressive and conservative candidates are scarce, there are clear differences in the depiction of female and male politicians. These differences resemble emotional gender stereotypes, especially since women are more often depicted as happy. Overall, the study demonstrates that female political communication is still distinct from male political communication for both their self-representation as well as the media’s portrayal of political candidates.},
 author = {Marc Jungblut and Mario Haim},
 doi = {10.1177/00936502211023333},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/00936502211023333},
 journal = {Communication Research},
 number = {5},
 pages = {561–583},
 title = {Visual Gender Stereotyping in Campaign Communication: Evidence on Female and Male Candidate Imagery in 28 Countries},
 url = {https://doi-org.crai.referencistas.com/10.1177/00936502211023333},
 volume = {50},
 year = {2023f}
}

@article{doi:10.1177/009385487800500301,
 abstract = {The aim of this study was to investigate several aspects of daily thinking among delinquents and nondelinquents while adequately controlling institutionalization. For this purpose the following groups were selected: institutionalized delinquents (prison inmates), institutionalized nondelinquents (soldiers), noninstitutionalized delinquents (delinquents on probation), and noninstitutionalized nondelinquents (vocational students). The findings show that prisoners are more preoccupied with events occurring within the total institution than are soldiers. However, in both types of institution, the closer the subjects are to release, the greater their preoccupation with events outside the institution. Prisoners demonstrate a higher proportion of contents related to deviance and delinquency and a lower proportion of cognitive-instrumental contents than probationers. However, as they approach release there is a decrease in the former and an increase in the latter in their life-space. Noninstitutionalized subjects demonstrate a higher degree of activity in their daily thinking than their institutionalized counterparts. Both in prison and army in the early and late phases of institutionalization, the degree of activity demonstrated is higher than that in the middle phase. The findings are discussed in relation to the process of prisonization, and a hypothetical model incorporating several variables related to this process is suggested.},
 author = {Simha F. Landau},
 doi = {10.1177/009385487800500301},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/009385487800500301},
 journal = {Correctional Psychologist},
 number = {3},
 pages = {195–210},
 title = {Thought Content of Delinquent and Nondelinquent Young Adults: The Effect of Institutionalization},
 url = {https://doi-org.crai.referencistas.com/10.1177/009385487800500301},
 volume = {5},
 year = {1978k}
}

@article{doi:10.1177/009430610803700611,
 author = {Patrick Doreian},
 doi = {10.1177/009430610803700611},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/009430610803700611},
 journal = {Contemporary Sociology},
 number = {6},
 pages = {542–545},
 title = {Clashing Paradigms and Mathematics in the Social Sciences},
 url = {https://doi-org.crai.referencistas.com/10.1177/009430610803700611},
 volume = {37},
 year = {2008f}
}

@article{doi:10.1177/0141076820936491,
 author = {Ulrich Tröhler},
 doi = {10.1177/0141076820936491},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0141076820936491},
 journal = {Journal of the Royal Society of Medicine},
 note = {PMID:32663423},
 number = {7},
 pages = {274–277},
 title = {Probabilistic thinking and evaluation of therapies: an introductory overview},
 url = {https://doi-org.crai.referencistas.com/10.1177/0141076820936491},
 volume = {113},
 year = {2020r}
}

@article{doi:10.1177/0142331216673423,
 abstract = {A novel algorithm, called the edge determination algorithm, for exact computation of the frequency response of a linear interval system is proposed. The algorithm formulates candidate curves for the frequency response boundaries as cubic Bezier curves. The edge determination algorithm operates on the cubic Bezier control points of these curves to obtain those, or their parts, that are on the frequency response boundaries. It presents the frequency response boundaries as an array whose entries are the cubic Bezier control points of the curves on the boundaries. Examples for two different cases are presented to illustrate the mechanics and validity of the algorithm.},
 author = {G Dındış and A Karamancıoğlu},
 doi = {10.1177/0142331216673423},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0142331216673423},
 journal = {Transactions of the Institute of Measurement and Control},
 number = {3},
 pages = {987–994},
 title = {An edge determination algorithm for exact computation of the frequency response of linear interval systems},
 url = {https://doi-org.crai.referencistas.com/10.1177/0142331216673423},
 volume = {40},
 year = {2018g}
}

@article{doi:10.1177/014233128901100501,
 abstract = {A novel Structure is described and analysed which has a number of advantages for the practical implementation of recursive digital filters in control applications. It is based upon the use of an established alternative to the usual z-1delay operator, but this has been developed into a standard form called a δ filter which is both robust and versatile. The advantages are shown to be in the simplicity of implementation, including the determination of word-lengths for the coefficients and variables. First- and second-order filter structures are given in detail, and simulations presented to demonstrate the operation of a few typical filters. Significant reductions in computation requirements are possible using the δ filter.},
 author = {R.M. Goodall},
 doi = {10.1177/014233128901100501},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/014233128901100501},
 journal = {Transactions of the Institute of Measurement and Control},
 number = {5},
 pages = {218–224},
 title = {Minimisation of computation for digital controllers},
 url = {https://doi-org.crai.referencistas.com/10.1177/014233128901100501},
 volume = {11},
 year = {1989i}
}

@article{doi:10.1177/014233129501700301,
 abstract = {This paper presents a method for the numerical determination of the z-transform and the modified z-transform of a proper rational function expressed in the Laplace transform complex variables. The method is easy to implement on a computer using any high-level language and does not necessarily require the poles of the s-domain function to be found. The method is equally applicable to single-loop and multi-loop systems.},
 author = {Ado Dan-Isa and D. P. Atherton},
 doi = {10.1177/014233129501700301},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/014233129501700301},
 journal = {Transactions of the Institute of Measurement and Control},
 number = {3},
 pages = {107–111},
 title = {A Computational Method for Obtaining the z-Transform from the s-Transform},
 url = {https://doi-org.crai.referencistas.com/10.1177/014233129501700301},
 volume = {17},
 year = {1995b}
}

@article{doi:10.1177/014362440002100405,
 abstract = {Computational fluid dynamics has a wide range of application in the study of room air distribution. The application is providing valuable guidance for those interested in such areas as comfort, productivity and sick building syndrome. This paper gives a comparative review of some of the work undertaken in the field and highlights some of the modelling assumptions noted within the literature. It is apparent from the review that the use of the CFD methodology is helping to develop our understanding of internal ventilation flows, yet it is also apparent that many investigations are currently employing modelling assumptions that will hinder the development of generic guidance from such investigations.},
 author = {M.B. Russell and P.N. Surendran},
 doi = {10.1177/014362440002100405},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/014362440002100405},
 journal = {Building Services Engineering Research and Technology},
 number = {4},
 pages = {241–247},
 title = {Use of computational fluid dynamics to aid studies of room air distribution: A review of some recent work},
 url = {https://doi-org.crai.referencistas.com/10.1177/014362440002100405},
 volume = {21},
 year = {2000m}
}

@article{doi:10.1177/0143624411404646,
 abstract = {This article presents a numerical study of ventilation performance of balconies using computational fluid dynamics. The pressure coefficients distributed on the opposite walls of a five-storey building model, both with and without balconies, were studied under variation of the wind direction, balcony dimensions and building height. The effect of balconies on the capacity for wind-induced natural ventilation is discussed. The numerical results show good agreement with the experiments of Chand et al. They also indicate that a balcony enhances the cross-ventilation of intermediate floors but weakens that of the ground and top floors, by significantly changing the pressure distribution on the windward wall. Finally, the ventilation performance of a balcony is not greatly affected by variations in its size but is slightly weakened as the height of the building increases. Practical application: This study provides information and guidance in how best to incorporate the use of a balcony in building development at the design stage.},
 author = {ZT Ai and CM Mak and JL Niu and ZR Li},
 doi = {10.1177/0143624411404646},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0143624411404646},
 journal = {Building Services Engineering Research and Technology},
 number = {3},
 pages = {229–243},
 title = {The assessment of the performance of balconies using computational fluid dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1177/0143624411404646},
 volume = {32},
 year = {2011a}
}

@article{doi:10.1177/0143624411407950,
 abstract = {In this study, wind-driven natural ventilation in buildings is investigated by means of computational fluid dynamics. The airflow in and around three pilot buildings, which correspond to the most common natural-ventilation designs, i.e. cross-, windward- and leeward- single-sided ventilation, is simulated by applying both the Standard and a modified k–ε turbulence model. The latter represents a Prandtl number-modified version of the Standard k–ε model, based on the flow-variable (velocity and turbulence variables) distributions and on the atmospheric boundary layer (ABL) assumptions. Numerical results of streamwise and vertical velocity components, as well as of pressure coefficients at the facades, obtained by both turbulence models are compared with available wind tunnel experimental data found in literature. It is concluded that both models applied are in acceptable agreement with measurements, especially for the mean streamwise velocity component, while the proposed modified model is more accurate as far as flow within the windward and the internal parts (i.e. within the building) of the domain is concerned. Practical application: This study focuses on the development of a modified k–ε turbulence model for the prediction of wind-driven natural ventilation. The analysis described represents a methodology to produce ‘closure’ parameters, such as Prandtl numbers, compatible with the incoming wind characteristics (ABL). It was found that for all ventilation cases studied, i.e. cross- and single-sided ventilation, the proposed modified model is more accurate compared with the Standard k–ε model, and it accounts for the dumping effect near the walls and for kinetic energy reduction in the impinging region adequately. It is satisfying that the modified k–ε model leads to acceptable engineering results within relatively practical computer resources.},
 author = {GM Stavrakakis and NM Tomazinakis and NC Markatos},
 doi = {10.1177/0143624411407950},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0143624411407950},
 journal = {Building Services Engineering Research and Technology},
 number = {3},
 pages = {241–261},
 title = {Modified ‘closure’ constants of the Standard k–ε turbulence model for the prediction of wind-induced natural ventilation},
 url = {https://doi-org.crai.referencistas.com/10.1177/0143624411407950},
 volume = {33},
 year = {2012r}
}

@article{doi:10.1177/0143624415615328,
 abstract = {There are over 70 low energy and carbon standards in use around the world. None of these standards have been designed by the clients who pay for and occupy the buildings in question. In this work, the client was asked to define the building code for the construction of a new 2800 m2 building via a structured survey. The resulting zero-energy standard simply required the building to incur no energy utility bill. One year of monitoring of the completed building was used to see if the standard had been met. The result of this work is a new way of thinking about environmental building standards that solves many of the issues of obtaining and maintaining buy-in from the client. Practical application: This is the first time that the client has played a key role in the definition of a low-energy building standard. Measured energy consumption and renewable energy generation data are presented and demonstrate that the zero-energy criteria were successfully met. This work is important as it shows that the client can have a meaningful input into the design of an environmental standard. The paper should be of interest to architects, engineers, building energy researchers and those interested in methods that can be used to reduce the energy demand of buildings.},
 author = {Anna Parkin and Andrew Mitchell and David Coley},
 doi = {10.1177/0143624415615328},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0143624415615328},
 journal = {Building Services Engineering Research and Technology},
 number = {4},
 pages = {413–430},
 title = {A new way of thinking about environmental building standards: Developing and demonstrating a client-led zero-energy standard},
 url = {https://doi-org.crai.referencistas.com/10.1177/0143624415615328},
 volume = {37},
 year = {2016j}
}

@article{doi:10.1177/0143624416670695,
 abstract = {Owing to the limited installation space and duct size, coupled fittings are common in the duct systems of buildings. The coupling effect leads to changes in drag and fan energy consumption. This study investigates duct drag and flow field characteristics under coupling conditions. Experiments and numerical simulations with the Reynolds stress model are conducted. Flow field changes, flow field deformation, and drag changes in the duct are analyzed. Regardless of the coupling form, the velocity near the inner arc is fast, whereas that near the outer arc is slow. Under three different coupling connection conditions (S-shaped, L-shaped, and U-shaped), the outlet velocity gradient of the U-shaped coupling connection is the least obvious. After the fluid flows through the bend, a significant centerline velocity reduction can be observed, even greater than that in the bend. The lowest centerline velocity lies within the range of 2.5 D to 4.5 D after the bend. Coupling connection has an insignificant effect on upstream duct resistance. The resistance of single bend is less than that of the downstream bend for the coupled bend and greater than that of the upstream bend under coupling conditions. Practical application: Coupling effect is common in practical application of ventilation engineering. This effect leads to the change of fluid resistance loss of ducts and pipes. However, few researchers focus on this effect. This study finds that regardless of the coupling form, the velocity near the inner arc is fast, whereas that near the outer arc is slow. It means the guide vane should be set near inner arc. L-shaped coupling connection has the largest downstream piping resistance. The resistance of the downstream piping under S-shaped coupling is the least, thus L-shaped coupling connection should be avoided as far as possible in practical application.},
 author = {Ran Gao and Shikuo Chen and Angui Li and Zhigang Yang and Beihua Cong},
 doi = {10.1177/0143624416670695},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0143624416670695},
 journal = {Building Services Engineering Research and Technology},
 number = {2},
 pages = {163–175},
 title = {Computational fluid dynamics study on the drag and flow field differences between the single and coupled bends},
 url = {https://doi-org.crai.referencistas.com/10.1177/0143624416670695},
 volume = {38},
 year = {2017g}
}

@article{doi:10.1177/0143624418759783,
 abstract = {People spend most of their time indoors; therefore, maintaining a good indoor air quality and meeting the requirements of comfort and energy efficiency are essential. One of the most widespread strategies to achieve this objective is improving ventilation efficiency; therefore, the main aim of this study was to show an optimization of the ventilation efficiency, in a specific room, considering 47 variations (case studies) in the furniture arrangement. For this purpose, a numerical analysis using computational fluid dynamics techniques, validated by the tracer gas decay technique, was used to assess the distribution of the age of air within the space. The concept of “age of air” was implemented in the computational fluid dynamics code through user-defined functions, using the steady-state method based on the resolution of a transport equation for an additional scalar. Variations up to 5.75% in the ventilation efficiency between the cases studied have been achieved. It is concluded that an improvement up to 1.65% can be obtained when the elements of the study are introduced in a way that facilitates the air movement towards the exhaust; therefore, improvement of the ventilation efficiency through specific furniture distributions is possible, although not significant, according to the outcomes.},
 author = {Susana Hormigos-Jimenez and Miguel Ángel Padilla-Marcos and Alberto Meiss and Roberto Alonso Gonzalez-Lezcano and Jesús Feijó-Muñoz},
 doi = {10.1177/0143624418759783},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0143624418759783},
 journal = {Building Services Engineering Research and Technology},
 number = {5},
 pages = {557–571},
 title = {Computational fluid dynamics evaluation of the furniture arrangement for ventilation efficiency},
 url = {https://doi-org.crai.referencistas.com/10.1177/0143624418759783},
 volume = {39},
 year = {2018l}
}

@article{doi:10.1177/01436244211020465,
 abstract = {The paper presents a review of existing literature in the field of coupling Computational Fluid Dynamics (CFD) with Building Energy Simulations (BES) to better predict indoor environmental conditions and building energy implications. CFD is capable of providing a detailed analysis of airflow profile and temperature gradients in the space as well as better prediction of heat transfer involving convection and radiation. Whereas BES can provide dynamically changing boundary conditions to CFD to facilitate a precise transient analysis. Combining the two simulations provides a powerful framework to accurately predict building performance parameters. The review examines the variables exchanged between the two simulations and establishes that the Convective Heat Transfer Coefficient (CHTC) as the most important exchanged variable that can significantly improve the accuracy of energy simulations. Issues regarding the application of co-simulation mechanism are then discussed in terms of simulation discontinuities, along with strategies adopted by researchers to overcome the same. In the later sections, the review evaluates the applicability of co-simulation from the perspective of year-long building energy simulations and presents an overview of methods used in research to implement the same. Finally, the conclusions are discussed and the scope for future research in the field is presented. Practical implication: The review presents a critical analysis of essentially all major coupling strategies that can be used to perform a BES-CFD coupled analysis along with their strengths, limitations and possible application scenarios. Additionally, the problems associated with establishing the co-simulation are examined and various adopted solutions are presented along with methods implemented towards extending the practical applicability of such an analysis to encapsulate year-long simulations.},
 author = {Manan Singh and Ryan Sharston},
 doi = {10.1177/01436244211020465},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/01436244211020465},
 journal = {Building Services Engineering Research and Technology},
 number = {1},
 pages = {113–138},
 title = {A literature review of building energy simulation and computational fluid dynamics co-simulation strategies and its implications on the accuracy of energy predictions},
 url = {https://doi-org.crai.referencistas.com/10.1177/01436244211020465},
 volume = {43},
 year = {2022n}
}

@article{doi:10.1177/01436244241268071,
 abstract = {The wind flow field constitutes an important input parameter for computational fluid dynamics (CFD) simulations that are used in architectural design for the design and analysis of natural ventilation strategies.  Despite indications that the wind flow field may vary between places, CFD simulations that do not adequately account for this potential variation are still commonplace. This may significantly compromise the integrity and accuracy of the CFD simulations. This study was a two-pronged investigation with the ultimate objective of contributing towards efforts aimed at improving the accuracy of the CFD simulations. Firstly, a framework for integrated meso-scale and micro-scale CFD simulations was developed. Secondly, the newly developed framework was then implemented by deploying it to study the variation of the wind flow field between a reference meteorological station, and a selected local building site. The findings confirmed the indications that the wind flow field might vary spatially. The integrated multi-scale framework that was developed as part of the study was not only able to capture the wind flow field variation, but it also provided a way to quantify it, ultimately, leading to the generation of a more accurate wind flow field characterization representative of the local conditions. Through its ability to integrate multiple spatial scales that typically influence one another, this framework can help to enhance the accuracy and integrity of the CFD simulations that are used for natural ventilation design. Practical application: The findings from this study can be particularly useful when undertaking spatially integrated CFD simulations to design and analyse natural ventilation strategies in the building design process.},
 author = {Amos Kalua and James Jones and Francine Battaglia and Elizabeth Grant},
 doi = {10.1177/01436244241268071},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/01436244241268071},
 journal = {Building Services Engineering Research and Technology},
 number = {0},
 pages = {01436244241268071},
 title = {Framework for integrated multi-scale computational fluid dynamics simulations in natural ventilation design},
 url = {https://doi-org.crai.referencistas.com/10.1177/01436244241268071},
 volume = {0},
 year = {2024l}
}

@article{doi:10.1177/0144598719839760,
 author = {Ghulamullah Maitlo and Imran Nazir Unar and Rasool Bux Mahar and Khan Mohammad Brohi},
 doi = {10.1177/0144598719839760},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0144598719839760},
 journal = {Energy Exploration & Exploitation},
 number = {3},
 pages = {1073–1097},
 title = {Numerical simulation of lignocellulosic biomass gasification in concentric tube entrained flow gasifier through computational fluid dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1177/0144598719839760},
 volume = {37},
 year = {2019k}
}

@article{doi:10.1177/0144739415581077,
 abstract = {Focus groups were convened in 2005 and 2013 as the first step in successive MPA curriculum review and revision processes. Although the criteria for selecting participants and the questions posed to the groups were similar, the focus groups generated dramatically different perspectives regarding the challenges confronting administrators. An analysis indicates that the differences in focus group perspectives are a reflection of the evolving economic, political and fiscal environments. It also reinforces the need to adopt instructional strategies that cultivate the students’ creative, critical thinking and life-long learning skills. Since anchored learning maximizes the probability of achieving these outcomes, this article provides three examples of implementation.},
 author = {Robert A Peters},
 doi = {10.1177/0144739415581077},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0144739415581077},
 journal = {Teaching Public Administration},
 number = {3},
 pages = {221–240},
 title = {Anchored learning and the development of creative, critical thinking and life-long learning skills},
 url = {https://doi-org.crai.referencistas.com/10.1177/0144739415581077},
 volume = {33},
 year = {2015k}
}

@article{doi:10.1177/01447394211004990,
 abstract = {Public administration is struggling to contend with a substantial shift in practice fueled by the accelerating adoption of information technology. New skills, competencies and pedagogies are required by the field to help overcome the data-skills gap. As a means to address these deficiencies, we introduce the Data Science Literacy Framework, a heuristic for incorporating data science principles into public administration programs. The framework suggests that data literacy is the dominant principle underlying a shift in professional practice, accentuated by an understanding of computational science, statistical methodology, and data-adjacent domain knowledge. A combination of new and existing skills meshed into public administration curriculums help implement these principles and advance public administration education.},
 author = {Michael Overton and Stephen Kleinschmit},
 doi = {10.1177/01447394211004990},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/01447394211004990},
 journal = {Teaching Public Administration},
 number = {3},
 pages = {354–365},
 title = {Data science literacy: Toward a philosophy of accessible and adaptable data science skill development in public administration programs},
 url = {https://doi-org.crai.referencistas.com/10.1177/01447394211004990},
 volume = {40},
 year = {2022p}
}

@article{doi:10.1177/01447394221084488,
 abstract = {Mass adoption of advanced information technologies is fueling a need for public servants with the skills to manage data-driven public agencies. Public employees typically acquire data skills through graduate research methods courses, which focus primarily on research design and statistical analysis. What data skills are currently taught, and what content should Master of Public Administration (MPA) programs include in their research method courses? We categorized research method course content in 52 syllabi from 31 MPA programs to understand how data skills are taught in public administration. We find that most graduate programs rely on research methods more suited for academic and policy research while lacking the data skills needed to modernize public agencies. Informed by these results, this work presents the Data Science Literacy Framework as a guide for assessing and planning curriculum within MPA programs.},
 author = {Michael Overton and Stephen Kleinschmit},
 doi = {10.1177/01447394221084488},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/01447394221084488},
 journal = {Teaching Public Administration},
 number = {2},
 pages = {149–169},
 title = {Transforming research methods education through data science literacy},
 url = {https://doi-org.crai.referencistas.com/10.1177/01447394221084488},
 volume = {41},
 year = {2023o}
}

@article{doi:10.1177/0145445514559928,
 abstract = {This study investigated a method for conducting experimental analyses of academic responding. In the experimental analyses, academic responding (math computation), rather than problem behavior, was reinforced across conditions. Two separate experimental analyses (one with fluent math computation problems and one with non-fluent math computation problems) were conducted with three elementary school children using identical contingencies while math computation rate was measured. Results indicate that the experimental analysis with non-fluent problems produced undifferentiated responding across participants; however, differentiated responding was achieved for all participants in the experimental analysis with fluent problems. A subsequent comparison of the single-most effective condition from the experimental analyses replicated the findings with novel computation problems. Results are discussed in terms of the critical role of stimulus control in identifying controlling consequences for academic deficits, and recommendations for future research refining and extending experimental analysis to academic responding are made.},
 author = {Kristi L. Hofstadter-Duke and Edward J. Daly},
 doi = {10.1177/0145445514559928},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0145445514559928},
 journal = {Behavior Modification},
 note = {PMID:25480794},
 number = {2},
 pages = {342–364},
 title = {Identifying Controlling Variables for Math Computation Fluency Through Experimental Analysis: The Interaction of Stimulus Control and Reinforcing Consequences},
 url = {https://doi-org.crai.referencistas.com/10.1177/0145445514559928},
 volume = {39},
 year = {2015e}
}

@article{doi:10.1177/0146645316661077,
 author = {W.E. Bolch and D. Jokisch and M. Zankl and K.F. Eckerman and T. Fell and R. Manger and A. Endo and J. Hunt and K.P. Kim and N. Petoussi-Henss},
 doi = {10.1177/0146645316661077},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0146645316661077},
 journal = {Annals of the ICRP},
 note = {PMID:29749258},
 number = {2},
 pages = {5–73},
 title = {ICRP Publication 133: The ICRP computational framework for internal dose assessment for reference adults: specific absorbed fractions},
 url = {https://doi-org.crai.referencistas.com/10.1177/0146645316661077},
 volume = {45},
 year = {2016d}
}

@article{doi:10.1177/0146645319893605,
 author = {C.H. Kim and Y.S. Yeom and N. Petoussi-Henss and M. Zankl and W.E. Bolch and C. Lee and C. Choi and T.T. Nguyen and K. Eckerman and H.S. Kim et al.},
 doi = {10.1177/0146645319893605},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0146645319893605},
 journal = {Annals of the ICRP},
 note = {PMID:33231095},
 number = {3},
 pages = {13–201},
 title = {ICRP Publication 145: Adult Mesh-Type Reference Computational Phantoms},
 url = {https://doi-org.crai.referencistas.com/10.1177/0146645319893605},
 volume = {49},
 year = {2020m}
}

@article{doi:10.1177/0146645320915031,
 author = {W.E. Bolch and K. Eckerman and A. Endo and J.G.S. Hunt and D.W. Jokisch and C.H. Kim and K-P. Kim and C. Lee and J. Li and N. Petoussi-Henss et al.},
 doi = {10.1177/0146645320915031},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0146645320915031},
 journal = {Annals of the ICRP},
 note = {PMID:33000625},
 number = {1},
 pages = {5–297},
 title = {ICRP Publication 143: Paediatric Reference Computational Phantoms},
 url = {https://doi-org.crai.referencistas.com/10.1177/0146645320915031},
 volume = {49},
 year = {2020a}
}

@article{doi:10.1177/014833311006000112,
 author = {J. Robert Baker},
 doi = {10.1177/014833311006000112},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/014833311006000112},
 journal = {Christianity & Literature},
 number = {1},
 pages = {129–149},
 title = {Review Essay: The Rising of the “Body, Glorified”; John D. Sykes, Jr. Flannery O’Connor, Walker Percy, and the Aesthetic of Revelation.; The Body in Flannery O’Connor’s Fiction: Computational Technique and Linguistic Voice.; The Abbess of Andalusia: Flannery O’Connor’s Spiritual Journey},
 url = {https://doi-org.crai.referencistas.com/10.1177/014833311006000112},
 volume = {60},
 year = {2010a}
}

@article{doi:10.1177/0149206308321550,
 abstract = {Previous research on dynamic processes during newcomer socialization has involved longitudinal field designs and state-of-the-art analyses. However, findings from this research either contradict key conceptual issues described by prominent socialization theories or are nondiagnostic regarding these issues. In this study computational modeling was used to (1) represent the presumed underlying processes in these theories, (2) demonstrate that current research does not test these theoretical processes, and (3) suggest the designs needed to support or refute components of dynamic theories.},
 author = {Jeffrey B. Vancouver and Kevin B. Tamanini and Ryan J. Yoder},
 doi = {10.1177/0149206308321550},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0149206308321550},
 journal = {Journal of Management},
 number = {3},
 pages = {764–793},
 title = {Using Dynamic Computational Models to Reconnect Theory and Research: Socialization by the Proactive Newcomer as Example},
 url = {https://doi-org.crai.referencistas.com/10.1177/0149206308321550},
 volume = {36},
 year = {2010q}
}

@article{doi:10.1177/014920639502100510,
 abstract = {The entropy measure of firm diversification is refined to improve its precision. The concept of diversification is defined and the measure is modified to reflect its dimensions—the extent of diversification across segments (distribution) and the number of segments in which a firm operates. The refined measure makes it possible to better understand equivalences among firms with different diversification profiles.},
 author = {Sankaran P. Raghunathan},
 doi = {10.1177/014920639502100510},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/014920639502100510},
 journal = {Journal of Management},
 number = {5},
 pages = {989–1002},
 title = {A Refinement of the Entropy Measure of Firm Diversification: Toward Definitional and Computational Accuracy},
 url = {https://doi-org.crai.referencistas.com/10.1177/014920639502100510},
 volume = {21},
 year = {1995p}
}

@article{doi:10.1177/016146810210400807,
 abstract = {Although there has been a great deal of recognition in the business world that information and knowledge management can be vital tools in organizations, it is only recently that educational administrators and teachers have begun to look at how they might use information systems to assist in creating effective learning environments. In the business research environment, the evolution from data to information and from information to knowledge plays a leading role in shaping how organizations develop strategies and plans for the future. Using examples from schools, this paper illustrates how knowledge management can enable schools to examine the plethora of data they collect and how an ecological framework can be used to transform these data into meaningful information.},
 author = {Lisa A. Petrides and Susan Zahra Guiney},
 doi = {10.1177/016146810210400807},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/016146810210400807},
 journal = {Teachers College Record},
 number = {8},
 pages = {1702–1717},
 title = {Knowledge Management for School Leaders: An Ecological Framework for Thinking Schools},
 url = {https://doi-org.crai.referencistas.com/10.1177/016146810210400807},
 volume = {104},
 year = {2002j}
}

@article{doi:10.1177/016146810710900607,
 abstract = {Background The Thinking Styles Inventory—developed by Sternberg and Wagner based on Sternberg’s (1988, 1997) earlier theory of mental self-government—was selected for the research in order to assess thinking styles of student teachers. Another reason is that the theoretical constructs, as well as the inventory generated from the theory, have proved to be valuable to assess thinking styles of people in several studies. For example, previous research reported that students differed in their thinking styles depending on their personal characteristics (e.g., Sternberg & Grigorenko, 1995; Zhang, 1999, 2002b, 2001e). However, research found no distinct patterns between thinking styles and personal characteristics of students (e.g., Grigorenko & Sternberg, 1997; Zhang, 2002a). Little research has focused on the study of non-Western students’ thinking styles. Thus, it is reasonable to expect that the thinking styles could also be identified among student teachers in Turkey since the styles are differentiated within a different country and culture. Objectives of Study (1) What is the validity and reliability of the Thinking Styles Inventory (TSI) in assessing thinking styles (TS) among Turkish student teachers? (2) Is the structure of a factor analysis of the TSI consistent with the five dimensions postulated by the theory of mental self-government in a Turkish sample? (3) Are TS of student teachers differentiated based on such socialization variables as gender, age, educational level, type of university attended, and field of study followed? Research Design The study reported here used quantitative methods with a survey sampling design. Subjects The research subjects were comprised of 402 student teachers enrolled in English, mathematics, and science teaching programs at Yildiz Technical University in Istanbul, Turkey. Results The results of factor analysis for construct validity of the inventory addressed 13 sub-scales under the five dimensional constructs with 104 items. Moreover, the total internal reliability of scale was a .92 reliability coefficient. Findings demonstrated that the 13 subscales had internal consistency reliabilities; Cronbach’s alphas of the 13 subscales for these subjects ranged from .61 to .91, as well as item-scale correlation that ranged from .37 to .88. Test re-test reliability for external reliability of subscales was between .63 to .78. When the ANOVA findings of the research is evaluated as a whole, it might be said that the student teachers’ particular thinking styles were differentiated by the socialized variables of gender, age, type of university attended, as well as field of study followed. Conclusions/Recommendations In order to measure thinking styles of students based on a model of broad intellectual styles, the inventory might be used as an efficient instrument. Moreover, the results of this study demonstrated a diversity of thinking styles among the participants. However, further research is needed to clarify the nature of thinking styles as assessed by the inventory at different educational levels and culture to facilitate a better understanding of the thinking styles of students.},
 author = {Seval Fer},
 doi = {10.1177/016146810710900607},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/016146810710900607},
 journal = {Teachers College Record},
 number = {6},
 pages = {1488–1516},
 title = {What Are the Thinking Styles of Turkish Student Teachers?},
 url = {https://doi-org.crai.referencistas.com/10.1177/016146810710900607},
 volume = {109},
 year = {2007g}
}

@article{doi:10.1177/016146811912101410,
 abstract = {Educators increasingly teach with social media in varied ways, but they may do so without considering the ways in which social media corporations profit from their uses or compromise transparency, equity, health, safety, and democracy through the design of platforms. There is a lack of scholarship that addresses the curricular topics that educators might investigate to teach about social media platforms and the potential challenges they pose for education and society. In this article, we draw on sociotechnical theories that conceive of social media as microsystems to understand the relationship between users, education, and social media companies. We identify and describe five topics concerning social media design that educators can consider and investigate with students in a variety of settings: user agreements and use of data; algorithms of oppression, echo, and extremism; distraction, user choice, and access for nonusers; harassment and cyberbullying; and gatekeeping for accurate information. In each case, we suggest curricular possibilities for teaching about social media platforms that draw from intersections of curriculum, media, and educational studies.},
 author = {Daniel G. Krutka and Stefania Manca and Sarah M. Galvin and Christine Greenhow and Matthew J. Koehler and Emilia Askari},
 doi = {10.1177/016146811912101410},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/016146811912101410},
 journal = {Teachers College Record},
 number = {14},
 pages = {1–42},
 title = {Teaching “Against” Social Media: Confronting Problems of Profit in the Curriculum},
 url = {https://doi-org.crai.referencistas.com/10.1177/016146811912101410},
 volume = {121},
 year = {2019h}
}

@article{doi:10.1177/016146812012200403,
 abstract = {Background/Context This paper is a part of the special issue “Reimagining Research and Practice at the Crossroads of Philosophy, Teaching, and Teacher Education.” We center what follows on a practice used in undergraduate methods courses that we have termed Interruptions. Interruptions are a form of inter-class visitation in which faculty plan together, visit one another’s classes, and publicly interrupt the teaching of the other with a variety of both pre-planned and spontaneous questions relating to the day’s lesson. Research Design We weave together a conceptual analysis and qualitative research, drawing from a larger qualitative study conducted in one early childhood and one elementary undergraduate math methods course in Spring 2016. For the study, we co-planned eight lessons together (four in each course), and observed one another teach each of those lessons, while taking notes and purposely interrupting instruction. We collected survey data from students at the end of each classroom observation and interviewed two students from each class at the end of the term. We also kept reflective journals of our work. In this paper we deploy a narrative format to document teacher inquiry drawing upon our reflective journals and classroom observation to describe the development, enactment, and our response to Interruptions. Outcomes Our use of Interruptions pushed us to examine our own philosophical beliefs and how they were, or were not, enacted in our teaching practice. We highlight that the connections that emerged between philosophy and teacher education provided us with the necessary time to care for our ethical selves both in and out of the classroom. Specifically, we share how this exercise allowed us each to become more deliberately reflective about the work that we do and why and how we do it. Conclusions/Recommendations In addition to giving us time to slow down our teaching in order to think carefully about our choices while in the midst of teaching, we found that we also considered instructional implications long after the Interruptions were complete. Interruptions helped us think more deliberately about the ethical choices we made as educators and in the service of our students. Interruptions proved to have deep and long-lasting effects on our practice as teacher educators. Other practitioners who ground themselves in both philosophy and methods may benefit from similarly systematic approaches for examining their own practice with an eye towards improvements in teaching and understanding of the self.},
 author = {Cara Furman and Shannon Larsen},
 doi = {10.1177/016146812012200403},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/016146812012200403},
 journal = {Teachers College Record},
 number = {4},
 pages = {1–26},
 title = {Interruptions: Thinking-in-Action in Teacher Education},
 url = {https://doi-org.crai.referencistas.com/10.1177/016146812012200403},
 volume = {122},
 year = {2020e}
}

@article{doi:10.1177/016146812112300708,
 abstract = {Background/Context The emphasis on scientific practices articulated by the National Research Council framework and the Next Generation Science Standards requires significant pedagogical shifts for U.S. science teachers. Purpose/Objective/Research Question/Focus of Study This study provides a rare window into the challenges and opportunities teachers encounter as they introduce argument writing into their science classrooms with support from the National Writing Project’s Inquiry into Science Writing project. The purpose of this study is to better understand the teacher-change process so as to inform the development of future professional development efforts. Population/Participants/Subjects Case studies were drawn from a professional development network led by the National Writing Project to support teachers in studying and improving their practice while sharing knowledge and benefiting from the expertise of others. The network included 28 middle school teachers at five writing project sites around the United States; the case studies presented in this article are based on the experiences of three of these teachers. Intervention/Program/Practice The Inquiry into Science Writing Project was a 2-year practitioner-driven professional learning experience seeking to better understand and support student practice around evidence-based science writing. During the duration of the project, teachers taught at least one lesson series culminating in written arguments by students each semester, and participated in two summer institutes, an ongoing national professional learning community, and monthly meetings of their local teacher research group. Research Design The study uses a qualitative comparative case study approach. Data Collection and Analysis The case studies draw on interviews, lesson artifacts, written teacher reflections, and samples of student work. Conclusions/Recommendations The study findings reinforce the complexity of the change process: The relationship between teachers’ knowledge, beliefs, and attitudes and their practice was not linear and unidirectional (i.e., change in attitude leads to change in practice) but rather iterative and mediated by both student work and the external supports they received. These findings confirm the need for sustained learning environments with features that promote enactment and reflection on student work to support teacher change. Further, they suggest that professional development providers should think about how to build habits of reflection into their own design processes, allowing space for feedback and learning from practitioners.},
 author = {Naa Ammah-Tagoe and Kyra Caspary and Matthew A. Cannady and Eric Greenwald},
 doi = {10.1177/016146812112300708},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/016146812112300708},
 journal = {Teachers College Record},
 number = {7},
 pages = {1–39},
 title = {Learning to Teach to Argue: Case Studies in Professional Learning in Evidence-Based Science Writing},
 url = {https://doi-org.crai.referencistas.com/10.1177/016146812112300708},
 volume = {123},
 year = {2021a}
}

@article{doi:10.1177/01614681221103929,
 author = {Sylvia Celedón-Pattichis and Carlos A. LópezLeiva and Marios S. Pattichis and Marta Civil},
 doi = {10.1177/01614681221103929},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/01614681221103929},
 journal = {Teachers College Record},
 number = {5},
 pages = {3–12},
 title = {Teaching and Learning Mathematics and Computing in Multilingual Contexts},
 url = {https://doi-org.crai.referencistas.com/10.1177/01614681221103929},
 volume = {124},
 year = {2022a}
}

@article{doi:10.1177/01614681221103932,
 abstract = {Background: There has been a dearth of research on intersectional identities in STEM, including the fields of computing and engineering. In computing education research, much work has been done on broadening participation, but there has been little investigation into how the field of computer science (CS) presents opportunities for students with strong intersectional identities. This study explores the strengths and connections among the unique identities and the symbiotic relationships that elementary Latina students hold in CS identity attainment. Purpose: The aim of this article is to better understand how predominantly low-income, multilingual Latina students experience identity development through the lens of diverse group membership. We examine how young Latinas, through their participation in a yearlong culturally and linguistically responsive CS curriculum, leverage their intersecting identities to rewrite the formula of what a computer scientist is and can be, leaving space to include and invite other strong identities as well. Research Design: An explanatory sequential mixed-methods design was used that analyzed data from predominantly low-income, multilingual Latinas in upper elementary grades, including pre- and post-CS identity surveys (N = 50) delivered before and after implementation of the curriculum, and eight individual semi-structured student interviews. Findings: We found that Latina students developed significantly stronger identification with the field of CS from the beginning to the end of the school year with regard to their experiences with CS, perception of themselves as computer scientists, family support for CS and school, and friend support for CS and school. Interviews revealed that perception of their CS ability greatly influenced identification with CS and that girls’ self-perceptions stemmed from their school, cultural, and home learning environments. Conclusion: Our results highlight the wealth of resources that Latinas bring to the classroom through their home- and community-based assets, which are characterized by intersecting group membership. Students did not report on the intersection between language and CS identity development, which warrants further investigation.},
 author = {Sharin Rawhiya Jacob and Jonathan Montoya and Mark Warschauer},
 doi = {10.1177/01614681221103932},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/01614681221103932},
 journal = {Teachers College Record},
 number = {5},
 pages = {166–185},
 title = {Exploring the Intersectional Development of Computer Science Identities in Young Latinas},
 url = {https://doi-org.crai.referencistas.com/10.1177/01614681221103932},
 volume = {124},
 year = {2022p}
}

@article{doi:10.1177/01614681221104043,
 abstract = {Background: Multilingual learners have been overlooked and understudied in computer science education research. As the CS for All movement grows, it is essential to design integrated, justice-oriented curricula that help young multilingual learners begin to develop computational thinking skills and discourses. Purpose: We present a conceptual framework and accompanying design principles for justice-centered computational thinking activities that are language-rich, with the aim of supporting learners’ agency and building their capacity over time to use computing for good in their communities. Setting: Our work takes place in a research–practice partnership centered in an elementary school in California with a significant multilingual Latinx population. Research Design: We have engaged in two cycles of design-based research with preservice and in-service teachers at an elementary school. Through analysis of one case study during the second and most recent cycle, we examined the potential of teachers using our design principles for supporting multilingual learners’ language development through engagement in computational thinking. Conclusions: Our findings suggest that multilingual learners will engage in productive discourse when computational thinking lessons are designed to (1) be meaningfully contextualized, (2) position students as agentic learners, and (3) promote coherence over time. However, more research is needed to understand how teachers use these principles over time, and what additional supports are needed to ensure coordination between stakeholders to develop and effectively implement coherent learning progressions.},
 author = {Rose K. Pozos and Samuel Severance and Jill Denner and Kip Tellez},
 doi = {10.1177/01614681221104043},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/01614681221104043},
 journal = {Teachers College Record},
 number = {5},
 pages = {127–145},
 title = {Exploring Design Principles in Computational Thinking Instruction for Multilingual Learners},
 url = {https://doi-org.crai.referencistas.com/10.1177/01614681221104043},
 volume = {124},
 year = {2022m}
}

@article{doi:10.1177/01614681221104141,
 abstract = {Background/Context: Bi/multilingual students’ STEM learning is better supported when educators leverage their language and cultural practices as resources, but STEM subject divisions have been historically constructed based on oppressive, dominant values and exclude the ways of knowing of nondominant groups. Truly promoting equity requires expanding and transforming STEM disciplines. Purpose/Objective/Research Question/Focus of Study: This article contributes to efforts to illuminate emergent bi/multilingual students’ ways of knowing, languaging, and doing in STEM. We follow the development of syncretic literacies in relation to translanguaging practices, asking, How do knowledges and practices from different communities get combined and reorganized by students and teachers in service of new modeling practices? Setting and Participants: We focus on a seventh-grade science classroom, deliberately designed to support syncretic literacies and translanguaging practices, where computer science concepts were infused into the curriculum through modeling activities. The majority of the students in the bilingual program had arrived in the United States at most three years before enrolling, from the Caribbean and Central and South America. Research Design: We analyze one lesson that was part of a larger research–practice partnership focused on teaching computer science through leveraging translanguaging practices and syncretic literacies. The lesson was a modeling and computing activity codesigned by the teacher and two researchers about post–Hurricane María outmigration from Puerto Rico. Analysis used microethnographic methods to trace how students assembled translanguaging, social, and schooled practices to make sense of and construct models. Findings/Results: Findings show how students assembled representational forms from a variety of practices as part of accomplishing and negotiating both designed and emergent goals. These included sensemaking, constructing, explaining, justifying, and interpreting both the physical and computational models of migration. Conclusions/Recommendations: Implications support the development of theory and pedagogy that intentionally make space for students to engage in meaning-making through translanguaging and syncretic practices in order to provide new possibilities for lifting up STEM learning that may include, but is not constrained by, disciplinary learning. Additional implications for teacher education and student assessment practices call for reconceptualizing schooling beyond day-to-day curriculum as part of making an ontological shift away from prioritizing math, science, and CS disciplinary and language objectives as defined by and for schooling, and toward celebrating, supporting, and centering students’ diverse, syncretic knowledges and knowledge use.},
 author = {Sarah C. Radke and Sara E. Vogel and Jasmine Y. Ma and Christopher Hoadley and Laura Ascenzi-Moreno},
 doi = {10.1177/01614681221104141},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/01614681221104141},
 journal = {Teachers College Record},
 number = {5},
 pages = {206–228},
 title = {Emergent Bilingual Middle Schoolers’ Syncretic Reasoning in Statistical Modeling},
 url = {https://doi-org.crai.referencistas.com/10.1177/01614681221104141},
 volume = {124},
 year = {2022m}
}

@article{doi:10.1177/016146818608700404,
 author = {Deanna Kuhn},
 doi = {10.1177/016146818608700404},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/016146818608700404},
 journal = {Teachers College Record},
 number = {4},
 pages = {495–512},
 title = {Education for Thinking},
 url = {https://doi-org.crai.referencistas.com/10.1177/016146818608700404},
 volume = {87},
 year = {1986i}
}

@article{doi:10.1177/0162243918798899,
 abstract = {Science and technology studies (STS) and the emerging field of data science share surprising elective affinities. At the growing intersections of these fields, there will be many opportunities and not a few thorny difficulties for STS scholars. First, I discuss how both fields frame the rollout of data science as a simultaneously social and technical endeavor, even if in distinct ways and for diverging purposes. Second, I discuss the logic of domains in contemporary computer, information, and data science circles. While STS is often agnostic about the borders between the sciences or with industry and state—occasionally taking those boundaries as an object of study—data science takes those boundaries as its target to overcome. These two elective affinities present analytic and practical challenges for STS but also opportunities for engagement. Overall, in addition to these typifications, I urge STS scholars to strategically position themselves to investigate and contribute to the breadth of transformations that seek to touch virtually every science and newly bind spheres of academy, industry, and state.},
 author = {David Ribes},
 doi = {10.1177/0162243918798899},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0162243918798899},
 journal = {Science, Technology, & Human Values},
 number = {3},
 pages = {514–539},
 title = {STS, Meet Data Science, Once Again},
 url = {https://doi-org.crai.referencistas.com/10.1177/0162243918798899},
 volume = {44},
 year = {2019n}
}

@article{doi:10.1177/01622439221140003,
 abstract = {This paper draws on the notion of the asset to better understand the role of innovative research technologies in researchers’ practices and decisions. Faced with both the need to accumulate academic capital to make a living in academia and with many uncertainties about the future, researchers must find ways to anticipate future academic revenues. We illustrate that innovative research technologies provide a suitable means for doing so: First, because they promise productivity through generating interesting data and hence publications. Second, because they allow a signaling of innovativeness in contexts where research is evaluated, even across disciplinary boundaries. As such, enrolling innovative research technologies as assets allows researchers to bridge partly conflicting valuations of productivity and innovativeness they are confronted with. However, the employment of innovative technologies in anticipation of future academic revenues is not always aligned with what researchers value epistemically. Nevertheless, considerations about potential future academic revenues derived from innovative research technologies sometimes seem to override particular epistemic valuations. Illustrating these dynamics, we show that processes of assetization in academia can have significant epistemic consequences which are important to unpack.},
 author = {Ruth Falkenberg and Maximilian Fochler},
 doi = {10.1177/01622439221140003},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/01622439221140003},
 journal = {Science, Technology, & Human Values},
 number = {1},
 pages = {105–130},
 title = {Innovation in Technology Instead of Thinking? Assetization and Its Epistemic Consequences in Academia},
 url = {https://doi-org.crai.referencistas.com/10.1177/01622439221140003},
 volume = {49},
 year = {2024h}
}

@article{doi:10.1177/0162353211416437,
 abstract = {In this response to Ambrose, VanTassel-Baska, Coleman, and Cross’s (2010) thought-provoking article on the nature and state of the field of gifted education, the author first discusses the role of disciplinary knowledge in his field. He argues that gifted education, as a normative and practical endeavor (i.e., a profession), is different from academic disciplines in research agendas and that technical rationality is not sufficient for identifying its “best practice.” The author then suggests a “flat” structure to facilitate close collaboration between theorists, researchers, and practitioners in tackling pressing problems and fashioning innovative practices. To facilitate discussion of explorations in our practice, he delineates three basic service models or paradigms in gifted education as follows: the gifted child paradigm, the talent development paradigm, and the differentiation paradigm. He proposes five criteria for assessing their strengths and potential weaknesses. Finally, he suggests that the best way of providing evidence-based best practice is through use-inspired, design studies, which not only address the question of whether a practical model works but also specify goals, assumptions, resources, processes, and constraints involved so that “how it works” is made transparent.},
 author = {David Yun Dai},
 doi = {10.1177/0162353211416437},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0162353211416437},
 journal = {Journal for the Education of the Gifted},
 number = {5},
 pages = {705–730},
 title = {Hopeless Anarchy or Saving Pluralism? Reflections on Our Field in Response to Ambrose, VanTassel-Baska, Coleman, and Cross},
 url = {https://doi-org.crai.referencistas.com/10.1177/0162353211416437},
 volume = {34},
 year = {2011c}
}

@article{doi:10.1177/016235328000400104,
 abstract = {The present research assessed the structural validity of The Ross Test of Higher Cognitive Processes, a recently developed instrument designed to assess the higher-level thinking skills of analysis, synthesis and evaluation as outlined in Bloom’s Taxonomy. Confirmatory factor analysis, analysis of variance and trend analysis were used to test the correspondence between Bloom’s Taxonomy and the Ross Test, and to study the developmental nature of critical thinking. Subjects were 154 gifted third through sixth graders. Confirmatory factor analysis of individual test items and of the eight Ross subtests provided empirical evidence for the structural validity of this instrument, and normative data on gifted students’ performance on the test was presented. This investigation of the Ross Test yielded findings relevant to the use and interpretation of this measure of critical thinking in the gifted and regular classroom.},
 author = {Carolyn M. Callahan and Mary L. Corvo},
 doi = {10.1177/016235328000400104},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/016235328000400104},
 journal = {Journal for the Education of the Gifted},
 number = {1},
 pages = {17–26},
 title = {Validating the Ross Test for Identification and Evaluation of Critical Thinking Skills in Programs for the Gifted},
 url = {https://doi-org.crai.referencistas.com/10.1177/016235328000400104},
 volume = {4},
 year = {1980b}
}

@article{doi:10.1177/0162643420978564,
 abstract = {The ideas of computational thinking (CT) and computer science (CS) are increasingly being integrated into K-12 education. Yet, insufficient attention exists regarding access and exposure of CT and CS for students with disabilities. In this Technology in Action, the authors sought to present an argument—as well as actual activities—for teachers to start to expose and engage students with disabilities in CT and CS. Through the presentation of case studies as well as other non-case situated activities, practical ideas, and steps for integrating CT and CS in mathematics teaching and learning for students with disabilities are presented.},
 author = {Emily C. Bouck and Aman Yadav},
 doi = {10.1177/0162643420978564},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0162643420978564},
 journal = {Journal of Special Education Technology},
 number = {1},
 pages = {151–160},
 title = {Providing Access and Opportunity for Computational Thinking and Computer Science to Support Mathematics for Students With Disabilities},
 url = {https://doi-org.crai.referencistas.com/10.1177/0162643420978564},
 volume = {37},
 year = {2022a}
}

@article{doi:10.1177/01626434221116077,
 doi = {10.1177/01626434221116077},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/01626434221116077},
 journal = {Journal of Special Education Technology},
 number = {3},
 pages = {417–417},
 title = {Corrigendum to “Providing Access and Opportunity for Computational Thinking and Computer Science to Support Mathematics for Students With Disabilities”},
 url = {https://doi-org.crai.referencistas.com/10.1177/01626434221116077},
 volume = {38},
 year = {2023t}
}

@article{doi:10.1177/01626434231198226,
 abstract = {Digital technology in primary education can both be distracting and increase attentiveness. Many students with Special Educational Needs (SEN) have difficulties with skills that address attention, and teachers are expected to provide support. Such skills are referred to as Executive Function (EF) in neuroscience, relating to self-regulation, attention shifting, and inhibition of behavior. This systematic literature review outlines research on primary education during 2000–2022 that relates students’ EF and digital technology through empirical data and suggested SEN-inclusive educational interventions. 288 full-text journal articles were assessed, and 26 were included for analysis. Findings include common game-based solutions for EF and SEN support, enabling explicit goals, short teaching activities, and recorded outcomes. Other examples include EF skills training and classroom management with digital monitoring devices. A substantially increased research interest during 2021–2022 was observed. Aspects needing further research are discussed, such as more special education views with cost-effective behavioral approaches.},
 author = {Fabian Gunnars},
 doi = {10.1177/01626434231198226},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/01626434231198226},
 journal = {Journal of Special Education Technology},
 number = {2},
 pages = {264–276},
 title = {A Systematic Review of Special Educational Interventions for Student Attention: Executive Function and Digital Technology in Primary School},
 url = {https://doi-org.crai.referencistas.com/10.1177/01626434231198226},
 volume = {39},
 year = {2024k}
}

@article{doi:10.1177/01626434241257228,
 abstract = {Becoming proficient in scientific discourse such as argumentation or developing explanations can be challenging for students, including neurodivergent students (e.g., autistic students, students with learning disabilities, students with ADHD). Students need to practice developing arguments and explanations as well as sharing their conclusions with others; however, these opportunities do not regularly occur. The use of multimedia technology shows promise in being an engaging and effective way to support student learning and discussion of science topics and phenomena. This study investigated the use of a multimedia presentation called Dialogic Instruction for Argumentative Learning in Science (DIALS) in supporting upper-elementary neurodivergent students to provide complete responses to science questions. DIALS provides a structured inquiry investigation to the student in which they learn about a phenomenon through completing scaffolded animated experiments, explicitly learning how to develop an argument and respond to others conclusions, and learning relevant content information to support their argument and explanation development. An adapted alternating treatments design approach was used in this study. The phenomena explored were counterbalanced between three options: DIALS, PowerPoint comparison lessons, or control probe slides. Results from this study show promise in the use of DIALS in enhancing student complete responding to questions and providing quality claims for why they think phenomena occur. Implications for the findings are discussed.},
 author = {Victoria J. VanUitert and Emily Millirons and Olivia F. Coleman and Michael J. Kennedy},
 doi = {10.1177/01626434241257228},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/01626434241257228},
 journal = {Journal of Special Education Technology},
 number = {3},
 pages = {403–418},
 title = {Learning by Talking: Using Multimedia to Enhance Science Explanation Development of Neurodivergent Students},
 url = {https://doi-org.crai.referencistas.com/10.1177/01626434241257228},
 volume = {39},
 year = {2024r}
}

@article{doi:10.1177/0163443720939449,
 abstract = {Computational photography is currently altering the representational and social functions of photographic imaging. A range of heavily automated computational processing techniques produce images that remediate digital photography to circumvent physical limitations associated with the size of smartphones, emulating the aesthetics associated with larger format digital cameras and professional photographic workflows and practices. These processes include automated compositing where images seen by users are constituted of up to 15 individual frames, the simulation of a shallow depth-of-field, automated facial retouching and even providing automated assistance to suggest alternative frames within the image stream to serve as the base image. This article explores these emerging techniques and accompanying claims that such processes are radically transforming photographic practice. While the extent and modes of automation and algorithmic processing depart from prior practices, contextualising them within the histories of photographic compositing and the algorithmic malleability of digital photography suggests the intensification of existing trends rather than an epistemic break. Furthermore, exploring the representational politics of automated facial retouching and the datafication of images situates these changes within the broader social context of dataveillance and platform capitalism.},
 author = {Sy Taffel},
 doi = {10.1177/0163443720939449},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0163443720939449},
 journal = {Media, Culture & Society},
 number = {2},
 pages = {237–255},
 title = {Google’s lens: computational photography and platform capitalism},
 url = {https://doi-org.crai.referencistas.com/10.1177/0163443720939449},
 volume = {43},
 year = {2021r}
}

@article{doi:10.1177/0165551515614537,
 abstract = {This research study investigates US middle school students’ collaborative information-seeking, sense-making and knowledge-building practices in a guided discovery-based programme of game design learning in which students and their teachers participate in a formal, in-school class daily, for credit and a grade for an entire year. The learning is supported by information affordances including a wiki learning management system (LMS) housing the curriculum, organized design activities, social media features, tutorials and informational assignments. Students engage in a Constructionist blended learning setting in their classroom, and work collaboratively in teams on game design. The study draws on qualitative video data from six team cases using a coding scheme of categories for the concepts of task, collaborative information seeking (CIS) Modality, and inquiry resolution outcomes. The study also considers linkages between processes and learning outcomes. Variation in engagement across the categories among students was charted, and certain patterns emerged. Findings indicate that some categories of task appear related to some categories of students’ chosen CIS Modality for solving problems. Further, CIS processes in support of tasks appear related to inquiry incident resolution (resolved/unresolved). For student completion of advanced programming tasks in particular, we observe more frequent uses of the wiki-based LMS resources, and greater levels of challenge in fulfilling tasks. Results support existing work on these theoretical constructs in the information sciences, and lead to questions on how naturalistic emergence of CIS practices result from greater task knowledge, and whether learned CIS practices (as tasks in and of themselves) can yield project task knowledge gains. Findings of the study and the ongoing questions the work invites hold instructional design implications, and show how social constructivist educational contexts involving collaborative and information seeking and knowledge building among youth game designers can contribute to scholarly understanding of these processes more broadly in related project-based work contexts occurring among both youth and adults.},
 author = {Rebecca B. Reynolds},
 doi = {10.1177/0165551515614537},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0165551515614537},
 journal = {Journal of Information Science},
 number = {1},
 pages = {35–58},
 title = {Relationships among tasks, collaborative inquiry processes, inquiry resolutions, and knowledge outcomes in adolescents during guided discovery-based game design in school},
 url = {https://doi-org.crai.referencistas.com/10.1177/0165551515614537},
 volume = {42},
 year = {2016n}
}

@article{doi:10.1177/0165551515615842,
 abstract = {Computational cognitive models of web-navigation developed so far have largely been tested only on mock-up websites. In this paper, for the first time, we compare and contrast the performance of two models, CoLiDeS and CoLiDeS+, on two real websites from the domains of technology and health, under two conditions of task difficulty, simple and difficult. We found that CoLiDeS+ predicted more hyperlinks on the correct path and had a higher path completion ratio than CoLiDeS. CoLiDeS+ found the target page more often than CoLiDeS, took more steps to reach the target page and was more ‘disoriented’ than CoLiDeS for difficult tasks. Difficult tasks in general for both models had less task success and lower path completion ratio, predicted less hyperlinks on the correct path, visited pages with lower mean LSA and took more steps to complete compared with simple tasks. Overall, inclusion of context from previously visited pages and implementation of backtracking strategies (which are both part of CoLiDeS+) led to better modelling performance. Suggestions to further improve the performance of these computational cognitive models on real websites are discussed.},
 author = {Saraschandra Karanam and Herre van Oostendorp and Wai Tat Fu},
 doi = {10.1177/0165551515615842},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0165551515615842},
 journal = {Journal of Information Science},
 number = {1},
 pages = {94–113},
 title = {Performance of computational cognitive models of web-navigation on real websites},
 url = {https://doi-org.crai.referencistas.com/10.1177/0165551515615842},
 volume = {42},
 year = {2016k}
}

@article{doi:10.1177/0165551519861599,
 abstract = {In online social networks, spam profiles represent one of the most serious security threats over the Internet; if they do not stop producing bad advertisements, they can be exploited by criminals for various purposes. This article addresses the nature and the characteristics of spam profiles in a social network like Twitter to improve spam detection, based on a number of publicly available language-independent features. In order to investigate the effectiveness of these features in spam detection, four datasets are extracted for four different language contexts (i.e. Arabic, English, Korean and Spanish), and a fifth is formed by combining them all. We conduct our experiments using a set of five well-known classification algorithms in spam detection field, k-Nearest Neighbours (k-NN), Random Forest (RF), Naive Bayes (NB), Decision Tree (DT) (J48) and Multilayer Perceptron (MLP) classifiers, along with five filter-based feature selection methods, namely, Information Gain, Chi-square, ReliefF, Correlation and Significance. The results show oscillating performance of each classifier across all datasets, but improved classification results with feature selection. In addition, detailed analysis and comparisons are carried out on two different levels: in the first level, we compare the selected features’ importance among the feature selection methods, whereas in the second level, we observe the relations and the importance of the selected features across all datasets. The findings of this article lead to a better understanding of social spam and improving detection methods by considering the various important features resulting from the different lingual contexts.},
 author = {Ala’ M Al-Zoubi and Ja’far Alqatawna and Hossam Faris and Mohammad A Hassonah},
 doi = {10.1177/0165551519861599},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0165551519861599},
 journal = {Journal of Information Science},
 number = {1},
 pages = {58–81},
 title = {Spam profiles detection on social networks using computational intelligence methods: The effect of the lingual context},
 url = {https://doi-org.crai.referencistas.com/10.1177/0165551519861599},
 volume = {47},
 year = {2021b}
}

@article{doi:10.1177/01655515211060530,
 abstract = {In the domain of Galleries, Libraries, Archives and Museums (GLAM) institutions, creative and innovative tools and methodologies for content delivery and user engagement have recently gained international attention. New methods have been proposed to publish digital collections as datasets amenable to computational use. Standardised benchmarks can be useful to broaden the scope of machine-actionable collections and to promote cultural and linguistic diversity. In this article, we propose a methodology to select datasets for computationally driven research applied to Spanish text corpora. This work seeks to encourage Spanish and Latin American institutions to publish machine-actionable collections based on best practices and avoiding common mistakes.},
 author = {Gustavo Candela and María-Dolores Sáez and Pilar Escobar and Manuel Marco-Such},
 doi = {10.1177/01655515211060530},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/01655515211060530},
 journal = {Journal of Information Science},
 number = {6},
 pages = {1451–1461},
 title = {A benchmark of Spanish language datasets for computationally driven research},
 url = {https://doi-org.crai.referencistas.com/10.1177/01655515211060530},
 volume = {49},
 year = {2023c}
}

@article{doi:10.1177/01925121231199904,
 abstract = {Advances in data accessibility and analytical methods opened new frontiers for comparative studies of European legislative activities. However, these advances still need to be fully harnessed by legislative scholars for multiple reasons. We provide an overview of extant research agendas to identify these reasons and explore the opportunities for tapping the potential of big data and quantitative text analysis. We present significant data collection efforts, such as ParlSpeech, the Comparative Agendas Project and CLARIN, and highlight their respective value for, primarily, large-N comparative research focusing on European Union member states and the European Union itself. Our review highlights the most consequential gaps in the literature and shortcomings of available data and analysis. These include the lack of extensive historical and geographical coverage, missing harmonisation and cross-linking between separate efforts, no unified speech and document (bill, law) databases, and the unavailability of good-quality full-text variables.},
 author = {Miklós Sebők and Sven-Oliver Proksch and Christian Rauh and Péter Visnovitz and Gergő Balázs and Jan Schwalbach},
 doi = {10.1177/01925121231199904},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/01925121231199904},
 journal = {International Political Science Review},
 number = {0},
 pages = {01925121231199904},
 title = {Comparative European legislative research in the age of large-scale computational text analysis: A review article},
 url = {https://doi-org.crai.referencistas.com/10.1177/01925121231199904},
 volume = {0},
 year = {2023q}
}

@article{doi:10.1177/019263659608058304,
 author = {Peter Smagorinsky},
 doi = {10.1177/019263659608058304},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/019263659608058304},
 journal = {NASSP Bulletin},
 number = {583},
 pages = {11–17},
 title = {Multiple Intelligences, Multiple Means of Composing: An Alternative Way of Thinking About Learning},
 url = {https://doi-org.crai.referencistas.com/10.1177/019263659608058304},
 volume = {80},
 year = {1996p}
}

@article{doi:10.1177/0194599811401202,
 abstract = {Objective. Sinus ventilation is often associated with sinusitis, a common condition causing significant pain and reduced quality of life. Clinical implications of the diverse anatomy of ostia connecting sinus to nose and the efficacy of surgical intervention in chronic sinusitis are poorly understood. This study aimed to measure sinus ventilation and explore variables in physical and mathematical models. Study Design. γ-Scintigraphy of krypton 81m (81mKr) was carried out in a stylized physical model of a human maxillary sinus. Computational simulations matched this model for validation and extrapolated to combinations of variables not possible experimentally for evaluation of transport mechanisms. Setting. Research laboratory in Department of Aeronautics. Imperial College London, and Department of Nuclear Medicine, Hammersmith Hospital, London. Methods. 81mKr distribution was measured with both single- and double-ostia sinuses. Computational simulations matched and extended the physical measurements and enabled separate identification and evaluation of transport mechanisms. Results. The presence of an additional ostium resulted in a 50-fold increase in the effective volume flow rate of gas replacement in the sinus. In the case of a single ostium, doubling the ostial diameter doubled the effective volume flow rate of gas exchange. Conclusion. γ-Scintigraphy of 81mKr enables quantitative assessment of effective volume flow rate in physical model sinuses. These flow rates obtained experimentally for single- and double-ostium sinuses match the computational predictions of matching geometries. The increased ventilation seen with an additional ostium or increased ostial diameters may not be clinically beneficial, because it could reduce nitric oxide concentration in the sinus.},
 author = {Catherine E. Rennie and Christina M. Hood and Esther J. S. M. Blenke and Robert S. Schroter and Denis J. Doorly and Hazel Jones and David Towey and Neil S. Tolley},
 doi = {10.1177/0194599811401202},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0194599811401202},
 journal = {Otolaryngology–Head and Neck Surgery},
 note = {PMID:21493301},
 number = {1},
 pages = {165–170},
 title = {Physical and Computational Modeling of Ventilation of the Maxillary Sinus},
 url = {https://doi-org.crai.referencistas.com/10.1177/0194599811401202},
 volume = {145},
 year = {2011r}
}

@article{doi:10.1177/02103702241253407,
 abstract = {Fostering young children’s computational thinking (CT) has garnered global interest as it aligns with the cultivation of twenty-first-century skills. Previous studies have focused on physical, virtual and hybrid kits with virtual programming blocks, but rarely explored the use of hybrid kits that combine virtual sprites and physical programming environments. We conducted a quasi-experimental study to investigate the effect of using a hybrid programming kit on young children’s CT. Furthermore, we explored the characteristics of children’s programming engagement and the instructional strategies employed by teachers through video analysis and interviews. The results showed that: (1) children’s CT in the experimental group significantly improved, compared to that of peers in the control group; (2) children’s programming behaviour demonstrated a change from ‘action preceding thought’ to ‘thought preceding action’ and from ‘relying on trial and error’ to ‘active debugging’ with the support of teachers; (3) teachers used multiple strategies to support young children’s programming. These findings further indicate the importance of introducing programming in early childhood education and emphasize the critical role that teachers play in supporting young children’s learning of programming.},
 author = {Yue Zeng and Weipeng Yang and Alfredo Bautista},
 doi = {10.1177/02103702241253407},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/02103702241253407},
 journal = {Journal for the Study of Education and Development},
 number = {2},
 pages = {408–441},
 title = {Developing young children’s computational thinking through programming with a hybrid kit / Desarrollo del pensamiento computacional infantil a través de la programación con un kit híbrido},
 url = {https://doi-org.crai.referencistas.com/10.1177/02103702241253407},
 volume = {47},
 year = {2024s}
}

@article{doi:10.1177/02109395241241379,
 abstract = {With the disruptive emergence of Artificial Intelligences in the production of knowledge, new questions arise about the role that these technologies should occupy within the different spheres through which the past is represented. This article examines ChatGPT’s potential as a pedagogical tool for the development of historical thinking. For this purpose, we present the analysis of an interactive process around the historical event usually referred to as the Spanish Reconquista (711–1492). Through argumentative interventions, a dialogue is constructed in which the information provided by the AI is questioned, proving its ability to rectify and incorporate elements of historical thinking into its discourse. An ethical and multidisciplinary perspective is needed, which safeguards the responsible and beneficial use of these technologies in the construction and transmission of historical knowledge.},
 author = {Mario Carretero and Elisa Gartner},
 doi = {10.1177/02109395241241379},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/02109395241241379},
 journal = {Studies in Psychology},
 number = {1},
 pages = {80–102},
 title = {Artificial Intelligence and historical thinking: a dialogic exploration of ChatGPT / Inteligencia Artificial y pensamiento histórico: una exploración dialógica del ChatGPT},
 url = {https://doi-org.crai.referencistas.com/10.1177/02109395241241379},
 volume = {45},
 year = {2024c}
}

@article{doi:10.1177/0255761419861442,
 abstract = {This study explored how Australian music technology courses teach employability skills. A curriculum mapping of 63 undergraduate courses was conducted with course learning outcomes aligned against two benchmarks. The first benchmark was the Ten Skills for the Future Workforce which identifies key employability skills graduates will require in the coming decade. The second benchmark was the Australian Qualifications Framework Specification for the Bachelor Degree which defines the generic skills graduates must obtain through Australian Bachelor Degrees. This curriculum mapping reveals that Australian music technology courses teach Novel and Adaptive Thinking, Computational Thinking, New Media Literacy, and Design Mindsets universally. However, this curriculum mapping also reveals a deficit in employability skills related to Cross-Cultural Competency, Transdisciplinarity, Virtual Collaboration, and Collaboration more generally. The implications of this mapping is that Australian music technology educators seem to be prioritizing specific technical and creative skills over higher-order applications of skills and knowledge which are contextualized in their broader social and cultural contexts. Finally, this article shows how curriculum mapping can be implemented to embed employability skills progressively across a program sequence using a case study from the School of Music, University of Queensland.},
 author = {Eve Klein and James Lewandowski-Cox},
 doi = {10.1177/0255761419861442},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0255761419861442},
 journal = {International Journal of Music Education},
 number = {4},
 pages = {636–653},
 title = {Music technology and Future Work Skills 2020: An employability mapping of Australian undergraduate music technology curriculum},
 url = {https://doi-org.crai.referencistas.com/10.1177/0255761419861442},
 volume = {37},
 year = {2019m}
}

@article{doi:10.1177/02557614231194073,
 abstract = {This scoping review addresses internationally published empirical studies on the subject of technology-enhanced creativity. The study aims to identify the types of technological tools used to enhance students’ creativity and examine how technological tools can support students’ creativity in K-12 music education. This review selected and analyzed 17 studies published from 1987 to 2022 in peer-reviewed journals using a rigorous five-stage scoping framework. Data extraction and analysis were conducted in Covidence. The results revealed eight types of technological tools used to enhance creativity in the music classroom, in which sequencer software and GarageBand were the most commonly used type of technological tools and applications respectively. Technology’s support for creativity was also discussed from the perspectives of Lubart’s four roles of computers: (a) computer as nanny, (b) computer as pen-pal, (c) computer as coach, and (d) computer as colleague. The results showed a dearth of research on how technology can become students’ partners to help them generate creative ideas. Based on the findings, this review concluded with implications and recommendations for future research.},
 author = {Chi Kai Lam},
 doi = {10.1177/02557614231194073},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/02557614231194073},
 journal = {International Journal of Music Education},
 number = {0},
 pages = {02557614231194073},
 title = {Technology-enhanced creativity in K-12 music education: A scoping review},
 url = {https://doi-org.crai.referencistas.com/10.1177/02557614231194073},
 volume = {0},
 year = {2023j}
}

@article{doi:10.1177/026119290403201s117,
 abstract = {This workshop addressed current issues with regard to establishing a framework for the validation of quantitative structure–activity relationships (QSARs) and other computational prediction models. QSARs and related models attempt to associate physicochemical and structural properties of compounds to their biological activity. As such, they may permit the prediction of biological activity from chemical structure alone. As yet, no formal validation criteria have been agreed internationally for QSARs and related techniques. However, some general and preliminary criteria were discussed and agreed upon at a European Chemical Industry Council/International Council of Chemical Associations (CEFIC/ICCA) workshop on the regulatory acceptance of QSARs, held in Setubal, Portugal, in March 2002. These criteria were presented at a Fourth World Congress workshop, along with a proposal for the practical validation of computer models, such as QSARs.},
 author = {Andrew P. Worth and Mark T.D. Cronin},
 doi = {10.1177/026119290403201s117},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/026119290403201s117},
 journal = {Alternatives to Laboratory Animals},
 note = {PMID:23581163},
 number = {1_suppl},
 pages = {703–706},
 title = {Report of the Workshop on the Validation of QSARs and Other Computational Prediction Models},
 url = {https://doi-org.crai.referencistas.com/10.1177/026119290403201s117},
 volume = {32},
 year = {2004r}
}

@article{doi:10.1177/026119290903700508,
 abstract = {A number of toxic effects are brought about by the covalent interaction between the toxicant and biological macromolecules. In chemico assays are available that attempt to identify reactive compounds. These approaches have been developed independently for pharmaceuticals and for other non-pharmaceutical compounds. The assays vary widely in terms of the macromolecule (typically a peptide) and the analytical technique utilised. For both sets of methods, there are great opportunities to capture in chemico information by using in silico methods to provide computational tools for screening purposes. In order to use these in chemico and in silico methods, integrated testing strategies are required for individual toxicity endpoints. The potential for the use of these approaches is described, and a number of recommendations to improve this extremely useful technique, in terms of implementing the Three Rs in toxicity testing, are presented.},
 author = {Mark T.D. Cronin and Fania Bajot and Steven J. Enoch and Judith C. Madden and David W. Roberts and Johannes Schwöbel},
 doi = {10.1177/026119290903700508},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/026119290903700508},
 journal = {Alternatives to Laboratory Animals},
 note = {PMID:20017580},
 number = {5},
 pages = {513–521},
 title = {The In Chemico–In Silico Interface: Challenges for Integrating Experimental and Computational Chemistry to Identify Toxicity},
 url = {https://doi-org.crai.referencistas.com/10.1177/026119290903700508},
 volume = {37},
 year = {2009e}
}

@article{doi:10.1177/026119290903700509,
 abstract = {For over a decade, the United States Food and Drug Administration (US FDA) has been engaged in the applied research, development, and evaluation of computational toxicology methods used to support the safety evaluation of a diverse set of regulated products. The basis for evaluating computational toxicology methods is multi-factorial, including the potential for increased efficiency, reduction in the numbers of animals used, lower costs, and the need to explore emerging technologies that support the goals of the US FDA’s Critical Path Initiative (e.g. to make decision support information available early in the drug review process). The US FDA’s efforts have been facilitated by agency-approved data-sharing agreements between government and commercial software developers. This commentary review describes former and current scientific initiatives at the agency, in the area of computational toxicology methods. In particular, toxicology-based QSAR models, ToxML databases and knowledgebases will be addressed. Notably, many of the computational toxicology tools available are commercial products — however, several are emerging as non-commercial products, which are freely-available to the public, and which will facilitate the understanding of how these programs work and avoid the “black box” paradigm. Through productive collaborations, the US FDA Center for Drug Evaluation and Research, and the Center for Food Safety and Applied Nutrition, have worked together to evaluate, develop and apply these methods to chemical toxicity endpoints of regulatory interest.},
 author = {Chihae Yang and Luis G. Valerio and Kirk B. Arvidson},
 doi = {10.1177/026119290903700509},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/026119290903700509},
 journal = {Alternatives to Laboratory Animals},
 note = {PMID:20017581},
 number = {5},
 pages = {523–531},
 title = {Computational Toxicology Approaches at the US Food and Drug Administration},
 url = {https://doi-org.crai.referencistas.com/10.1177/026119290903700509},
 volume = {37},
 year = {2009s}
}

@article{doi:10.1177/026119299802600208,
 author = {Andrew P. Worth and Martin D. Barratt and J. Brian Houston},
 doi = {10.1177/026119299802600208},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/026119299802600208},
 journal = {Alternatives to Laboratory Animals},
 note = {PMID:26043401},
 number = {2},
 pages = {241–247},
 title = {The Validation of Computational Prediction Techniques},
 url = {https://doi-org.crai.referencistas.com/10.1177/026119299802600208},
 volume = {26},
 year = {1998s}
}

@article{doi:10.1177/0261429419854223,
 abstract = {Since programming processes involve different thinking skills and different fields of knowledge, it is especially important for children to acquire 21st-century skills. Even though the programming education activities are being intensively applied, it can be said that there is a gap in quantitative researches supporting the effort to reveal the direct or indirect effectiveness of the learning–teaching processes for the programming education. This study, which was done to fill this gap, aims to examine the degree to which students learn programming concepts (PC) and to identify effective variables in that process with a developed curriculum for gifted students studying in the second–third–fourth grade in primary schools. For this purpose, a 15-week application was carried out and each student developed an individual project. In the study, a criterion list, observation forms and peer evaluations were used based on PC to examine projects and learning process. The scores obtained from these tools were used to examine the application of each participant, to comment on the effective variables and the adequacy of the teaching process. The evidence from this study intimates that female participants obtained higher scores than male ones in programming education. Those scores are higher in 9 and 10 age group of students than others. Those who haven’t had Internet access, who have never used computer or have had access to Internet as well as who haven’t had any computer courses had lower scores than others. The upshot of this is that previous computer technology experiences of students may have affected the scores obtained programming education process.},
 author = {Hatice Yildiz Durak and Tolga Guyer},
 doi = {10.1177/0261429419854223},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0261429419854223},
 journal = {Gifted Education International},
 number = {3},
 pages = {237–258},
 title = {Programming with Scratch in primary school, indicators related to effectiveness of education process and analysis of these indicators in terms of various variables},
 url = {https://doi-org.crai.referencistas.com/10.1177/0261429419854223},
 volume = {35},
 year = {2019f}
}

@article{doi:10.1177/0261927X20969752,
 abstract = {Politics is an area that is traditionally believed to be gender divided. According to intergroup communication theory, this gender-salient context might cause differences in political communications between genders. Moreover, the internet and social media, which creates a computer-mediated interactive context, might also impact the traditional gender discrepancies in political discourse. This study used Twitter trace-data and computational text analysis to examine such suppositions. By analyzing over one million tweets, we found that compared to men, women generally had a stronger sense of group awareness and cohesion and showed a desire to promote their tweets while avoiding addressing other users in political discussions. Women also focused on family- and home-related issues more than men did. These findings suggest that Twitter is not an ideal public sphere where differences and inequalities are eliminated, but it might be a counter-public sphere that promotes the voices and increases the publicity of marginalized groups.},
 author = {Lingshu Hu and Michael Wayne Kearney},
 doi = {10.1177/0261927X20969752},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0261927X20969752},
 journal = {Journal of Language and Social Psychology},
 number = {4},
 pages = {482–503},
 title = {Gendered Tweets: Computational Text Analysis of Gender Differences in Political Discussion on Twitter},
 url = {https://doi-org.crai.referencistas.com/10.1177/0261927X20969752},
 volume = {40},
 year = {2021h}
}

@article{doi:10.1177/0263276412444473,
 abstract = {This article will take up Deleuze and Guattari’s allusive yet insightful writings on ‘the secret’ by considering the secret across three intermingling registers or modulations: as content (secret), as form (secrecy), and as expression (secretion). Setting the secret in relation to evolving modes of technological mediation and sociality as respectively pocket, pooling, and plasma, the article works through a trio of examples in order to understand the contemporary movements of secrets: the memories of secrets evoked in an intimately interactive music video by the band Arcade Fire (as an example of ‘pocket’); the movements of secrecy turned fabulative in the scopic-doublings of airport full-body scanners (as ‘pooling’); and, finally, the collective secretions that come to saturate and stretch around the globe as expressed by liquidity-seeking financial innovations (providing an angle onto ‘plasma’). These three instantiations of contemporary secrecy are framed by a discussion of Julian Assange of WikiLeaks and Mark Zuckerberg of Facebook – truly a couple for our age: each intent, in their own way, upon bringing an end to secrets. Throughout, we try to maintain close attention to the emerging rhythms and dissonances that engage secrecy in a dance between the half-voluntary and the half-enforced.},
 author = {Gregory J. Seigworth and Matthew Tiessen},
 doi = {10.1177/0263276412444473},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0263276412444473},
 journal = {Theory, Culture & Society},
 number = {6},
 pages = {47–77},
 title = {Mobile Affects, Open Secrets, and Global Illiquidity: Pockets, Pools, and Plasma},
 url = {https://doi-org.crai.referencistas.com/10.1177/0263276412444473},
 volume = {29},
 year = {2012n}
}

@article{doi:10.1177/0263276415590237,
 abstract = {In this wide-ranging conversation, Berry and Galloway explore the implications of undertaking media theoretical work for critiquing the digital in a time when networks proliferate and, as Galloway claims, we need to ‘forget Deleuze’. Through the lens of Galloway’s new book, Laruelle: Against the Digital, the potential of a ‘non-philosophy’ for media is probed. From the import of the allegorical method from excommunication to the question of networks, they discuss Galloway’s recent work and reflect on the implications of computation for media theory, thinking about media objects, and critical theory.},
 author = {David M. Berry and Alexander R. Galloway},
 doi = {10.1177/0263276415590237},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0263276415590237},
 journal = {Theory, Culture & Society},
 number = {4},
 pages = {151–172},
 title = {A Network is a Network is a Network: Reflections on the Computational and the Societies of Control},
 url = {https://doi-org.crai.referencistas.com/10.1177/0263276415590237},
 volume = {33},
 year = {2016d}
}

@article{doi:10.1177/0263276418818884,
 abstract = {In our contemporary moment, when machine learning algorithms are reshaping many aspects of society, the work of N. Katherine Hayles stands as a powerful corpus for understanding what is at stake in a new regime of computation. A renowned literary theorist whose work bridges the humanities and sciences among her many works, Hayles has detailed ways to think about embodiment in an age of virtuality (How We Became Posthuman, 1999), how code as performative practice is located (My Mother Was a Computer, 2005), and the reciprocal relations among human bodies and technics (How We Think, 2012). This special issue follows the 2017 publication of her book Unthought: The Power of the Cognitive Nonconscious, in which Hayles traces the nonconscious cognition of biological life-forms and computational media. The articles in the special issue respond in different ways to Hayles’ oeuvre, mapping the specific contours of computational regimes and developing some of the ‘inflection points’ she advocates in the deep engagement with technical systems.},
 author = {Louise Amoore},
 doi = {10.1177/0263276418818884},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0263276418818884},
 journal = {Theory, Culture & Society},
 number = {2},
 pages = {3–16},
 title = {Introduction: Thinking with Algorithms: Cognition and Computation in the Work of N. Katherine Hayles},
 url = {https://doi-org.crai.referencistas.com/10.1177/0263276418818884},
 volume = {36},
 year = {2019d}
}

@article{doi:10.1177/0263276418818889,
 abstract = {As machines have become increasingly smart and have entangled human thinking with artificial intelligences, it seems no longer possible to distinguish among levels of decision-making that occur in the newly formed space between critical reasoning, logical inference and sheer calculation. Since the 1980s, computational systems of information processing have evolved to include not only deductive methods of decision, whereby results are already implicated in their premises, but have crucially shifted towards an adaptive practice of learning from data, an inductive method of retrieving information from the environment and establishing general premises. This shift in logical methods of decision-making does not simply concern technical apparatuses, but is a symptom of a transformation in logical thinking activated with and through machines. This article discusses the pioneering work of Katherine Hayles, whose study of the cybernetic and computational infrastructures of our culture particularly clarifies this epistemological transformation of thinking in relation to machines.},
 author = {Luciana Parisi},
 doi = {10.1177/0263276418818889},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0263276418818889},
 journal = {Theory, Culture & Society},
 number = {2},
 pages = {89–121},
 title = {Critical Computation: Digital Automata and General Artificial Thinking},
 url = {https://doi-org.crai.referencistas.com/10.1177/0263276418818889},
 volume = {36},
 year = {2019k}
}

@article{doi:10.1177/0263276419843582,
 abstract = {This article takes inspiration from Kittler’s claim that philosophy has neglected the means used for its production. Kittler’s argument for media ontology will be compared to the post-Kantian project of re-inventing philosophy through the medium of thought (in particular Deleuze’s Spiritual Automaton). The article discusses these views in the context of the automation of logical thinking where procedures, tasks, and functions are part of the instrumental processing of new ends evolving a new mode of reasoning. In particular, the article suggests that in constructivist logic and information theory, the temporal gap between truth and proof, between input and output, can be taken to argue that the means of thought expose the indetermination or the incomputability of proof. The automation of reasoning in logical processing coincides not with mindless correlations of data, replacing axioms with data, truths with self-validating proofs. Instead, the problem of the indeterminacy of proof within automated logic re-habilitates techne or instrumentality, and the relation between means and ends away from classical idealism and analytic realism. By following John Dewey’s argument for instrumentality, it will be argued that the task of thinking today needs to re-invent a logic of techne away from the teleological view of ends or the crisis of finality. If the post-Kantian preoccupations about the task of thinking already announced that the medium of thought could offer possibilities for a non-human philosophy (or a philosophy beyond truth), this article envisions a machine philosophy originating from within computational media.},
 author = {Luciana Parisi},
 doi = {10.1177/0263276419843582},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0263276419843582},
 journal = {Theory, Culture & Society},
 number = {6},
 pages = {95–124},
 title = {Media Ontology and Transcendental Instrumentality},
 url = {https://doi-org.crai.referencistas.com/10.1177/0263276419843582},
 volume = {36},
 year = {2019q}
}

@article{doi:10.1177/0263276420966386,
 abstract = {This article addresses computational procedures that are no longer constrained by human modes of representation and considers how these procedures could be philosophically understood in terms of ‘algorithmic thought’. Research in deep learning is its case study. This artificial intelligence (AI) technique operates in computational ways that are often opaque. Such a black-box character demands rethinking the abstractive operations of deep learning. The article does so by entering debates about explainability in AI and assessing how technoscience and technoculture tackle the possibility to ‘re-present’ the algorithmic procedures of feature extraction and feature learning to the human mind. The article thus mobilises the notion of incommensurability (originally developed in the philosophy of science) to address explainability as a communicational and representational issue, which challenges phenomenological and existential modes of comparison between human and algorithmic ‘thinking’ operations.},
 author = {M. Beatrice Fazi},
 doi = {10.1177/0263276420966386},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0263276420966386},
 journal = {Theory, Culture & Society},
 number = {7–8},
 pages = {55–77},
 title = {Beyond Human: Deep Learning, Explainability and Representation},
 url = {https://doi-org.crai.referencistas.com/10.1177/0263276420966386},
 volume = {38},
 year = {2021f}
}

@article{doi:10.1177/02632764211048548,
 abstract = {What is algorithmic thought? It is not possible to address this question without first reflecting on how the Universal Turing Machine transformed symbolic logic and brought to a halt the universality of mathematical formalism and the biocentric speciation of thought. The article draws on Sylvia Wynter’s discussion of the sociogenic principle to argue that both neurocognitive and formal models of automated cognition constitute the epistemological explanations of the origin of the human and of human sapience. Wynter’s argument will be related to Gilbert Simondon’s reflections on ‘technical mentality’ to consider how socio-techno-genic assemblages can challenge the biocentricism and the formalism of modern epistemology. This article turns to ludic logic as one possible example of techno-semiotic languages as a speculative overturning of sociogenic programming. Algorithmic rules become technique-signs coinciding not with classic formalism but with interactive localities without re-originating the universality of colonial and patriarchal cosmogony.},
 author = {Luciana Parisi},
 doi = {10.1177/02632764211048548},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/02632764211048548},
 journal = {Theory, Culture & Society},
 number = {7–8},
 pages = {33–53},
 title = {Interactive Computation and Artificial Epistemologies},
 url = {https://doi-org.crai.referencistas.com/10.1177/02632764211048548},
 volume = {38},
 year = {2021l}
}

@article{doi:10.1177/0263276421990435,
 abstract = {This interview with Bernard Stiegler’s long-time translator and collaborator, Daniel Ross, examines the connections between different periods of Stiegler’s work, thought, writing and activism. Moving from the three volumes of Technics and Time to the final large-scale collaborative project of The Internation, the discussion concentrates on Stiegler’s conceptualization of ‘protentionality’, hope and care for a world confronted by climate crises, entropy and computational economic reconfigurations of work, economy and imaginations for futural possibilities. The interview foreshadows the special issue on The Internation project planned by Bishop and Stiegler for TCS that will appear in the near future.},
 author = {Ryan Bishop and Daniel Ross},
 doi = {10.1177/0263276421990435},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0263276421990435},
 journal = {Theory, Culture & Society},
 number = {4},
 pages = {111–133},
 title = {Technics, Time and the Internation: Bernard Stiegler’s Thought – A Dialogue with Daniel Ross},
 url = {https://doi-org.crai.referencistas.com/10.1177/0263276421990435},
 volume = {38},
 year = {2021b}
}

@article{doi:10.1177/02632764221141804,
 abstract = {This article serves as the introduction to the Annual Review special section entitled ‘Bernard Stiegler and the Internation Project: Computational Practices and Circumscribed Futures’. As such, it introduces the collective undertaking of the Internation Project in relation to Stiegler’s long career as a thinker, educator and community organizer. The introduction pursues a number of themes addressed in the section’s contributions, including pharmacological logic, transindividuation, computational practices, bifurcation and negentropy (means of slowing entropic processes at individual and collective levels). All of these themes pertain to the climate crises the world collectively faces and posit means by which futures can be conceived in less detrimental and destructive economic, social, technological and intellectual ways. The Internation Collective as represented and furthered in this special section responds to the demands of climate crises through a macroeconomic model designed to combat entropy at various scales, from the bio-chemical to the biosphere.},
 author = {Ryan Bishop},
 doi = {10.1177/02632764221141804},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/02632764221141804},
 journal = {Theory, Culture & Society},
 number = {7–8},
 pages = {5–17},
 title = {Bernard Stiegler and the Internation Project: An Introduction},
 url = {https://doi-org.crai.referencistas.com/10.1177/02632764221141804},
 volume = {39},
 year = {2022c}
}

@article{doi:10.1177/02632764231174811,
 abstract = {François Jullien wants us to see what thought and life could look like without ontology, promising intellectual riches unavailable in the heavy ontological apparatus we are deeply invested in. The strength of Jullien’s argument comes from a deep and unique alliance between philosophy and Chinese thought, a risky one – incurring predictable disgruntlement from both philosophy and sinology – but nevertheless enduring and productive. This is far from advocating one in place of another, as we are accustomed to do in critical theory in relation to divinity, grand narratives, scientism, modernity, and even stable truths. It is an endeavour to think the ‘unthought of’ of ontology through what Jullien calls a vis-à-vis suspended in productive tension, a dialogue. In philosophy, the other is subsumed in a singular dialectical relationship through oppositions. What Jullien insists on is a doubleness of co-existence in Chinese thought rather than a singularity of dialectics in ontology. If ontology grounds a philosophy that makes a world in its image, and if that image is increasingly untenable as an ecology of the planet, Jullien’s call for a deeper reflexivity in ontology is of enormous significance. This special issue brings both Jullien’s argument and Chinese thought to a forum to explicate what this could mean in multiple fields from art and architecture to anthropology and critical theory.},
 author = {Shiqiao Li and Scott Lash},
 doi = {10.1177/02632764231174811},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/02632764231174811},
 journal = {Theory, Culture & Society},
 number = {4–5},
 pages = {3–23},
 title = {Against Ontology: Chinese Thought and François Jullien: An Introduction},
 url = {https://doi-org.crai.referencistas.com/10.1177/02632764231174811},
 volume = {40},
 year = {2023n}
}

@article{doi:10.1177/02632764231196829,
 abstract = {Digital technologies are rapidly transforming economies and societies. Scholars have approached this rise of digital capitalism from various angles. However, relatively little attention has been paid to digital capitalism’s cultural underpinnings and the beliefs of those who develop most digital technologies. In this paper, we argue that a solutionist order of worth – in which value derives from solving social problems through technology – has become central to an emerging spirit of digital capitalism. We use supervised learning to trace the relative importance of different orders of worth in three novel text corpora. We find that solutionism is indeed central to the normative beliefs of digital elites and the broader digital milieu, but not to capitalist discourse at large. We illustrate the importance of these findings by discussing how the spirit of digital capitalism motivates, legitimates, and orients the actions of digital capitalists.},
 author = {Oliver Nachtwey and Timo Seidl},
 doi = {10.1177/02632764231196829},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/02632764231196829},
 journal = {Theory, Culture & Society},
 number = {2},
 pages = {91–112},
 title = {The Solutionist Ethic and the Spirit of Digital Capitalism},
 url = {https://doi-org.crai.referencistas.com/10.1177/02632764231196829},
 volume = {41},
 year = {2024l}
}

@article{doi:10.1177/02646196241253534,
 abstract = {This article describes a study of educational outcomes for 0- to 8-year-old children with blindness and low vision (BLV) who are learning sonification concepts. Children with BLV experience barriers to accessing education and careers in Science, Technology, Engineering and Mathematics (STEM), fields which traditionally rely heavily on visual representation of information. There is growing awareness of the potential of sonification, a technology to represent data and information in non-speech audio, to improve education access. While early learning of assistive technology skills is deemed essential for equity of access to education across the curriculum, children are generally not introduced to the concept of sonification at school until at academic level in secondary or tertiary education. Little is known about how young children with BLV engage with this promising technology. First, ‘CosmoBally on Sonoplanet’ is introduced, an accessible, educational game application for iPads and Android tablets. Then findings are shared from an anonymous online survey that collected formal responses from users of this app, using a combination of Likert-type scale and open-ended questions. The majority of the 17 respondents were (specialist) educators, and five of the respondents identified as having BLV. The survey investigated respondents’ perceptions of the capabilities of young children with BLV in using basic sonification in ‘CosmoBally on Sonoplanet’ to identify shapes – including a circle – to orientate in a digital grid and to create drawings on a touch screen. Results suggest that young children with BLV can learn sonification skills and additionally may build relevant non-sonification skills during this learning process. This article aims to provide a first insight into best practice around early learning of sonification as a potential tool for increased access and inclusion of children with BLV to STEM subjects in school.},
 author = {Phia Damsma},
 doi = {10.1177/02646196241253534},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/02646196241253534},
 journal = {British Journal of Visual Impairment},
 number = {0},
 pages = {02646196241253534},
 title = {Hearing a circle: An exploratory study of accessible sonification for young children with blindness and low vision},
 url = {https://doi-org.crai.referencistas.com/10.1177/02646196241253534},
 volume = {0},
 year = {2024d}
}

@article{doi:10.1177/0265813516655408,
 author = {Michael Batty},
 doi = {10.1177/0265813516655408},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0265813516655408},
 journal = {Environment and Planning B: Planning and Design},
 number = {4},
 pages = {605–609},
 title = {20 years of quantitative geographical thinking},
 url = {https://doi-org.crai.referencistas.com/10.1177/0265813516655408},
 volume = {43},
 year = {2016d}
}

@article{doi:10.1177/026635119100600204,
 abstract = {In this work alternative methodologies are proposed for following post-buckling paths without the need for a complete factorization of the stiffness matrix or any factorization at all. This is achieved with two different approaches. The first employs nonlinear versions of preconditioned conjugate-like methods and the so called conjugate and secant-Newton methods have been tested. The second approach is based on the classical Newton-Raphson or modified Newton-Raphson methods, while for the linearized problem in each iteration the preconditioned Lanczos method is employed. Both methodologies combine the convergence properties of Newton-like methods and the low storage requirements of pure iterative methods by varying the storage demands for the preconditioning matrix according to the available computer storage facilities.},
 author = {Manolis Papadrakakis},
 doi = {10.1177/026635119100600204},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/026635119100600204},
 journal = {International Journal of Space Structures},
 number = {2},
 pages = {115–131},
 title = {Computational Strategies for Tracing Post-Limit-Point Paths with Pure Iterative Methods},
 url = {https://doi-org.crai.referencistas.com/10.1177/026635119100600204},
 volume = {6},
 year = {1991p}
}

@article{doi:10.1177/0266382120968057,
 abstract = {With the development of the 4th Industrial Revolution (4th IR), its emerging technologies and skills; there is a mismatch between 4th IR, and the skills needed by information professionals to survive. This paper bridges the gap based on the skills needed to survive and provide possible solutions to challenges faced by information professionals, which will in turn help to reduce the number of unemployed, semi-employed, non-employed, and provide economic empowerment among information professionals in this new revolution. Information professionals should adopt the missing middle model/techniques in organization which asserts that robots, by and large, will not be taking our jobs; instead, human Machine collaboration will reconfigure some of our work, making and make human skills more unique and important than ever.},
 author = {Lateef Ayinde and Hal Kirkwood},
 doi = {10.1177/0266382120968057},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0266382120968057},
 journal = {Business Information Review},
 number = {4},
 pages = {142–153},
 title = {Rethinking the roles and skills of information professionals in the 4th Industrial Revolution},
 url = {https://doi-org.crai.referencistas.com/10.1177/0266382120968057},
 volume = {37},
 year = {2020a}
}

@article{doi:10.1177/02663821221110042,
 abstract = {Libraries as social spaces are bound to evolve based on a society’s level of civilisation and information media. This paper argues that the emergence of smart libraries have changed the paradigms of library by acknowledging the potential benefits and transformation smart library brings to library operations and services. It notes that analytical and computational thinking, data literacy, information literacy, social intelligence, programs and project management, cross-cultural competency, transliteracy, transdisciplinary, design thinking and mindset, virtual collaboration and cognitive load management are skills to be possessed by smart librarians. It highlights cloud computing, big data, 3D printing, IoT, Artificial Intelligence, RFID, drones etc., as the emerging technologies used for smart libraries and further discusses smart services, smart people, smart places and smart governance as the dimensions of smart library. Revelations are further made that smart libraries aid space saving, expansion of library working hours and services and promotes access to information, while remarks are made that lack of technological know-how, technophobia, data privacy and security, etc., are the challenges of smart library. It concludes that the emergence of smart library have facilitated the redefinition of library services and operations and recommends amongst others that librarians should continuously update their skills so that they can meet up with competitions that may arise from the challenges of globalisation of the information landscape.},
 author = {Kingsley N Igwe and Abdulakeem S Sulyman},
 doi = {10.1177/02663821221110042},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/02663821221110042},
 journal = {Business Information Review},
 number = {4},
 pages = {147–152},
 title = {Smart libraries: Changing the paradigms of library services},
 url = {https://doi-org.crai.referencistas.com/10.1177/02663821221110042},
 volume = {39},
 year = {2022j}
}

@article{doi:10.1177/02663821241289820,
 abstract = {This paper presents a nascent conceptual framework (The LISI4C framework) for designing a Library and Information Science (LIS) curriculum tailored to meet the demands of Industry 4.0 (4IR) in particularly South Africa, with global applicability. The framework is developed through an extensive literature review, drawing from a diverse range of sources. It integrates technical proficiencies, soft skills, adaptability, a global perspective, stakeholder collaboration, experiential learning, predictive skills, and a commitment to continuous curriculum updates to create a holistic educational model. The conceptual framework places technical skills at its core, and expertise in emerging technologies. Soft skills such as critical thinking, creativity, and collaboration, are integrated to foster a well-rounded skill set. A transdisciplinary approach encourages exploration beyond traditional LIS boundaries. The LISI4C framework also underscores the importance of sense-making, adaptability, and a global perspective in preparing graduates for the dynamic 4IR landscape. This paper contributes to the LIS education field by offering a forward-looking LISI4C framework that aligns LIS curriculum with the requirements of 4IR. The framework draws upon key elements, including technical proficiencies, soft skills, adaptability, a global perspective, stakeholder collaboration, experiential learning, predictive skills, and continuous curriculum updates, to create a holistic and forward-looking educational model.},
 author = {Yanga Livi and Oghenere Gabriel Salubi},
 doi = {10.1177/02663821241289820},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/02663821241289820},
 journal = {Business Information Review},
 number = {0},
 pages = {02663821241289820},
 title = {Navigating industry 4.0: Crafting a responsive curriculum for library and information science},
 url = {https://doi-org.crai.referencistas.com/10.1177/02663821241289820},
 volume = {0},
 year = {2024l}
}

@article{doi:10.1177/02666669211049135,
 abstract = {New information and computer technologies transform the social interaction and impose new demands for skills and thinking upon media specialists. The aim of this study is to determine the most effective set of information technologies, which can help media specialists develop competencies and thus stay competitive in the labor market. The research methodology is based on the overview of case studies concerning issues such as technology trends, human capital, and talent competitiveness. The qualitative analysis was performed in three phases – overviewing case studies, distinguishing trends and problem-solving. Analyzing data on skill supply and demand, the key skills needed to succeed in the workplace were identified. The results of the three-phase research revealed that the most important competencies needed to be in demand today are technology literacy, stress tolerance, and big data skills. The major finding of this study is that a media specialist needs to focus on learning throughout his life and gain hard and soft skills in the process.},
 author = {Nidal Al Said and Butheyna Zuheir Al-Rawashdeh},
 doi = {10.1177/02666669211049135},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/02666669211049135},
 journal = {Information Development},
 number = {3},
 pages = {380–390},
 title = {Information and computer technologies in media specialist preparation},
 url = {https://doi-org.crai.referencistas.com/10.1177/02666669211049135},
 volume = {38},
 year = {2022a}
}

@article{doi:10.1177/0267323117753751b,
 doi = {10.1177/0267323117753751b},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0267323117753751b},
 journal = {European Journal of Communication},
 number = {1},
 pages = {109–110},
 title = {David M Berry and Anders Fagerjord, Digital Humanities},
 url = {https://doi-org.crai.referencistas.com/10.1177/0267323117753751b},
 volume = {33},
 year = {2018y}
}

@article{doi:10.1177/0267323118799184d,
 doi = {10.1177/0267323118799184d},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0267323118799184d},
 journal = {European Journal of Communication},
 number = {5},
 pages = {578–579},
 title = {David E Berry and Anders Fagerjord, Digital Humanities},
 url = {https://doi-org.crai.referencistas.com/10.1177/0267323118799184d},
 volume = {33},
 year = {2018x}
}

@article{doi:10.1177/02673231211028359,
 abstract = {As digital platforms have come to play a central role in the news and information ecosystem, a new realm of watchdog journalism has emerged – the platform beat. Journalists on the platform beat report on the operation, use and misuse of social media platforms and search engines. The platform beat can serve as an important mechanism for increasing the accountability of digital platforms, in ways that can affect public trust in the platforms, but that can also, hopefully, lead to the development of stronger, more reliable, and ultimately more trustworthy, platforms. However, there are a number of tensions, vulnerabilities and potential conflicts of interest that characterize the platform beat. This article explores these complex dynamics of the platform beat in an effort assess the capacity of those on the platform beat to enhance the accountability and trustworthiness of digital platforms.},
 author = {Philip M Napoli},
 doi = {10.1177/02673231211028359},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/02673231211028359},
 journal = {European Journal of Communication},
 number = {4},
 pages = {376–390},
 title = {The platform beat: Algorithmic watchdogs in the disinformation age},
 url = {https://doi-org.crai.referencistas.com/10.1177/02673231211028359},
 volume = {36},
 year = {2021k}
}

@article{doi:10.1177/0267658308095737,
 abstract = {Previous accounts of morphological variability disagree over whether its cause is representational or computational in nature. Under a computational account, variability is confined to production; under a representational account, variability extends to comprehension and is qualitatively similar to variability in production. This article presents experimental evidence from the comprehension and production of gender and number agreement in second language (L2) Spanish clitics and adjectives. Intermediate-level participants show variability across comprehension and production; across tasks, masculine defaults are adopted. Advanced-level participants show less variability, although evidence for masculine defaults emerges across tasks. Number agreement proved relatively unproblematic, except in the production of adjective agreement where singular defaults are systematically adopted by intermediate- and advanced-level speakers. The qualitative similarity of variability across comprehension and production supports a representational account; however, previous research disfavours an account based in syntactic deficits. This article argues for a theory of morphological variability that places the representational cause in the morphology, rather than the syntax.},
 author = {Corrine McCarthy},
 doi = {10.1177/0267658308095737},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0267658308095737},
 journal = {Second Language Research},
 number = {4},
 pages = {459–486},
 title = {Morphological variability in the comprehension of agreement: an argument for representation over computation},
 url = {https://doi-org.crai.referencistas.com/10.1177/0267658308095737},
 volume = {24},
 year = {2008k}
}

@article{doi:10.1177/0267658321993873,
 abstract = {The Interface Hypothesis proposes that second language (L2) learners, even at highly proficient levels, often fail to integrate information at the external interfaces where grammar interacts with other cognitive systems. While much early L2 work has focused on the syntax–discourse interface or scalar implicatures at the semantics–pragmatics interface, the present article adds to this line of research by exploring another understudied phenomenon at the semantics–pragmatics interface, namely, presuppositions. Furthermore, this study explores both inference computation and suspension via a covered-box picture-selection task. Specifically, this study investigates the interpretation of the presupposition trigger stop and stop under negation. The results from 38 native English speakers and 41 first language (L1) Mandarin Chinese learners of English indicated similar response patterns between native and L2 groups in computing presuppositions but not in suspending presuppositions. That is, L2 learners were less likely to suspend presuppositions than native speakers. This study contributes to a more precise understanding of L2 acquisition at the external interface level, as well as computation and suspension of pragmatic inferences.},
 author = {Shuo Feng},
 doi = {10.1177/0267658321993873},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0267658321993873},
 journal = {Second Language Research},
 number = {4},
 pages = {737–763},
 title = {The computation and suspension of presuppositions by L1-Mandarin Chinese L2-English speakers},
 url = {https://doi-org.crai.referencistas.com/10.1177/0267658321993873},
 volume = {38},
 year = {2022e}
}

@article{doi:10.1177/02676583231160329,
 abstract = {This study investigates pronoun interpretation by second language (L2) learners of English, focusing on whether first language (L1) transfer and/or processing difficulty affect L2 learners’ pronoun resolution. It is hypothesized that L2 learners’ non-target performance in L2-pronoun interpretation is attributable to two sources. The first is the computational complexity required for pronoun resolution, as argued in L1 acquisition by Grodzinsky and Reinhart and L2 acquisition by Slabakova et al. The second is how pronoun interpretation operates in L1. The hypothesis is tested by comparing Korean and Spanish L2-English learners’ interpretation of English pronouns using a Truth Value Judgment Task. Both groups had difficulty rejecting pronouns with local-referential antecedents when their proficiency levels were low. Additionally, Korean speakers showed more non-target responses than Spanish speakers due to their knowledge of pronoun interpretation in Korean. These results indicate that both L1 transfer and processing difficulty may be sources of L2 learners’ non-target pronoun interpretation, supporting the hypothesis of the study.},
 author = {Eun Hee Kim},
 doi = {10.1177/02676583231160329},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/02676583231160329},
 journal = {Second Language Research},
 number = {3},
 pages = {505–531},
 title = {L1-transfer effects and the role of computational complexity in L2 pronoun interpretation},
 url = {https://doi-org.crai.referencistas.com/10.1177/02676583231160329},
 volume = {40},
 year = {2024i}
}

@article{doi:10.1177/026765839200800104,
 abstract = {This article describes a computational system for the linguistic analysis of language acquisition data (COALA). The system is a combined AI and database tool which allows the user to form highly complex queries about morphosyntactic and semantic structures contained in large sets of data. COALA identifies those sentences that meet the linguistic criteria defined by the user. It allows the user to freely define such linguistic contexts and to step through the sentences identified by the system. COALA then rapidly displays those sentences in their original discourse context. Additionally, COALA can perform statistical analyses in response to structural linguistic queries. This article contains (1) a discussion of the computational approach taken in the design of COALA, (2) a description of the functionality of the system, and (3) a reflection on the validity of the analytical categories contained therein.},
 author = {Manfred Pienemann},
 doi = {10.1177/026765839200800104},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/026765839200800104},
 journal = {Interlanguage studies bulletin (Utrecht)},
 number = {1},
 pages = {59–92},
 title = {COALA-A computational system for interlanguage analysis},
 url = {https://doi-org.crai.referencistas.com/10.1177/026765839200800104},
 volume = {8},
 year = {1992o}
}

@article{doi:10.1177/0267659107083657,
 abstract = {Peripheral access cardiopulmonary bypass (CPB) is initiated with percutaneous cannulae (CTRL) and venous drainage is often impeded due to smaller vessel and cannula size. A new cannula (Smartcanula ®, SC) was developed which can change shape in situ and, therefore, may improve venous drainage. Its performance was evaluated using a 2-D computational fluid dynamics (CFD) model. The Navier-Stokes equations could be simplified due to the fact that we use a steady state and a 2-dimensional system while the equation of continuity (ρ constant) was also simplified. We compared the results of the SC to the CTRL using CFDRC® (Version 6.6, CFDRC research corporation, Huntsville, USA) at two preloads (300 and 700 Pa). The SC’s mass flow rate outperformed the CTRL by 12.1% and 12.2% at a pressures of 300 and 700 Pa, respectively. At 700 Pa, a pressure gradient of 50% was measured for the CTRL and 11% for the SC. The mean velocity at the 700 Pa for the CTRL was 1.0 m.s-1 at exit while the SC showed an exit velocity of 1.3 m.s-1. Shear rates inside the cannulae were similar between the two cannulae. In conclusion, the prototype shows greater mass flow rates compared to the classic cannula; thus, it is more efficient. This is also advocated by a better pressure gradient and higher average velocities. By reducing cannula-tip surface area or increasing hole surface area, greater flow rates are achieved.Perfusion (2007) 22, 257—265.},
 author = {D. Jegger and S. Sundaram and K. Shah and I. Mallabiabarrena and G. Mucciolo and L.K. von Segesser},
 doi = {10.1177/0267659107083657},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0267659107083657},
 journal = {Perfusion},
 note = {PMID:18181514},
 number = {4},
 pages = {257–265},
 title = {Using computational fluid dynamics to evaluate a novel venous cannula (Smart canula ®) for use in cardiopulmonary bypass operating procedures},
 url = {https://doi-org.crai.referencistas.com/10.1177/0267659107083657},
 volume = {22},
 year = {2007j}
}

@article{doi:10.1177/0267659116656775,
 abstract = {A computational fluid dynamics model of a bicuspid aortic valve has been developed using idealised three-dimensional geometry. The aim was to compare how the orifice area and leaflet orientation affect the hemodynamics of a pure bicuspid valve. By applying physiologic material properties and boundary conditions, blood flow shear stresses were predicted during peak systole. A reduced orifice area altered blood velocity, the pressure drop across the valve and the wall shear stress through the valve. Bicuspid models predicted impaired blood flow similar to a stenotic valve, but the flow patterns were specific to leaflet orientation. Flow patterns developed in bicuspid aortic valves, such as helical flow, were sensitive to cusp orientation. In conclusion, the reduced opening area of a bicuspid aortic valve amplifies any impaired hemodynamics, but cusp orientation determines subsequent flow patterns which may determine the specific regions downstream from the valve most at risk of clinical complications.},
 author = {Sen Mei and Francisco S.N. de Souza Júnior and May Y.S. Kuan and Naomi C. Green and Daniel M. Espino},
 doi = {10.1177/0267659116656775},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0267659116656775},
 journal = {Perfusion},
 note = {PMID:27484972},
 number = {8},
 pages = {683–690},
 title = {Hemodynamics through the congenitally bicuspid aortic valve: a computational fluid dynamics comparison of opening orifice area and leaflet orientation},
 url = {https://doi-org.crai.referencistas.com/10.1177/0267659116656775},
 volume = {31},
 year = {2016i}
}

@article{doi:10.1177/0267659120944105,
 abstract = {Introduction: Extracorporeal membrane oxygenation circuit performance can be compromised by oxygenator thrombosis. Stagnant blood flow in the oxygenator can increase the risk of thrombus formation. To minimize thrombogenic potential, computational fluid dynamics is frequently applied for identification of stagnant flow conditions. We investigate the use of computed tomography angiography to identify flow patterns associated with thrombus formation. Methods: A computed tomography angiography was performed on a Quadrox D oxygenator, and video densitometric parameters associated with flow stagnation were measured from the acquired videos. Computational fluid dynamics analysis of the same oxygenator was performed to establish computational fluid dynamics–based flow characteristics. Forty-one Quadrox D oxygenators were sectioned following completion of clinical use. Section images were analyzed with software to determine oxygenator clot burden. Linear regression was used to correlate clot burden to computed tomography angiography and computational fluid dynamics–based flow characteristics. Results: Clot burden from the explanted oxygenators demonstrated a well-defined pattern, with the largest clot burden at the corner opposite the blood inlet and outlet. The regression model predicted clot burden by region of interest as a function of time to first opacification on computed tomography angiography (R2 = 0.55). The explanted oxygenator clot burden map agreed well with the computed tomography angiography predicted clot burden map. The computational fluid dynamics parameter of residence time, when summed in the Z-direction, was partially predictive of clot burden (R2 = 0.35). Conclusion: In the studied oxygenator, clot burden follows a pattern consistent with clinical observations. Computed tomography angiography–based flow analysis provides a useful adjunct to computational fluid dynamics–based flow analysis in understanding oxygenator thrombus formation.},
 author = {Robert G Conway and Jiafeng Zhang and Jean Jeudy and Charles Evans and Tieluo Li and Zhongjun Jon Wu and Bartley P Griffith},
 doi = {10.1177/0267659120944105},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0267659120944105},
 journal = {Perfusion},
 note = {PMID:32723149},
 number = {3},
 pages = {285–292},
 title = {Computed tomography angiography as an adjunct to computational fluid dynamics for prediction of oxygenator thrombus formation},
 url = {https://doi-org.crai.referencistas.com/10.1177/0267659120944105},
 volume = {36},
 year = {2021b}
}

@article{doi:10.1177/02676591241239277,
 abstract = {Cardiovascular diseases persist as a leading cause of mortality and morbidity, despite significant advances in diagnostic and surgical approaches. Computational Fluid Dynamics (CFD) represents a branch of fluid mechanics widely used in industrial engineering but is increasingly applied to the cardiovascular system. This review delves into the transformative potential for simulating cardiac surgery procedures and perfusion systems, providing an in-depth examination of the state-of-the-art in cardiovascular CFD modeling. The study first describes the rationale for CFD modeling and later focuses on the latest advances in heart valve surgery, transcatheter heart valve replacement, aortic aneurysms, and extracorporeal membrane oxygenation. The review underscores the role of CFD in better understanding physiopathology and its clinical relevance, as well as the profound impact of hemodynamic stimuli on patient outcomes. By integrating computational methods with advanced imaging techniques, CFD establishes a quantitative framework for understanding the intricacies of the cardiac field, providing valuable insights into disease progression and treatment strategies. As technology advances, the evolving synergy between computational simulations and clinical interventions is poised to revolutionize cardiovascular care. This collaboration sets the stage for more personalized and effective therapeutic strategies. With its potential to enhance our understanding of cardiac pathologies, CFD stands as a promising tool for improving patient outcomes in the dynamic landscape of cardiovascular medicine.},
 author = {Chiara Catalano and Fabrizio Crascì and Silvia Puleo and Roberta Scuoppo and Salvatore Pasta and Giuseppe M Raffa},
 doi = {10.1177/02676591241239277},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/02676591241239277},
 journal = {Perfusion},
 note = {PMID:38850015},
 number = {0},
 pages = {02676591241239277},
 title = {Computational fluid dynamics in cardiac surgery and perfusion: A review},
 url = {https://doi-org.crai.referencistas.com/10.1177/02676591241239277},
 volume = {0},
 year = {2024c}
}

@article{doi:10.1177/0268355512474250,
 abstract = {Relatively little attention has been paid to the venous system and valves from a cardiovascular engineering perspective up to now. Given the involvement of venous valve haemodynamics in the development of deep vein thrombosis this is an area that needs more detailed investigation and close collaboration between clinicians and cardiovascular engineers. The purpose of this review article is to provide an indication of the physiological conditions that need to be included in any computational model of the venous system, based on recommendations from clinicians, and to summarize published computational models of the venous system by trying to explore their limitations and application range. A MEDLINE search was carried out on the relevant literature from 1940 until today. Several models have been developed with a specific purpose in mind to coincide with the aim of each individual study. The model complexity and laws used in each model vary significantly. There are more simplistic computational models based on electric circuit analogies, termed as lumped parameter models, which can be used to provide boundary conditions to one-dimensional (1D) and three-dimensional (3D) domain models, followed by 1D continuous models based on analytical equations, which allow the description of pressure wave and can be non-linear in nature. Finally, there are the more advanced 3D models, which are based on the principles of haemodynamics, and consider the compliance of the venous system and the effect that venous valves have on the cardiovascular system. In conclusion, it appears that computer modelling of the venous system can contribute greatly to our understanding of venous physiology and allow us to evaluate the haemodynamic interactions that occur in the venous system under different physiological conditions.},
 author = {C Zervides and A D Giannoukas},
 doi = {10.1177/0268355512474250},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0268355512474250},
 journal = {Phlebology},
 note = {PMID:23479775},
 number = {4},
 pages = {209–218},
 title = {Computational phlebology: reviewing computer models of the venous system},
 url = {https://doi-org.crai.referencistas.com/10.1177/0268355512474250},
 volume = {28},
 year = {2013t}
}

@article{doi:10.1177/0268355521996087,
 abstract = {Objectives To determine the site and nature of altered hemodynamics in pathological internal jugular veins. Method With the use of computational fluid mechanics software we simulated blood flow in 3 D models of the internal jugular veins that exhibited different morphologies, including nozzle-like strictures in their upper parts and valves in the lower parts. Results In a majority of models with nozzle-like strictures, especially those positioned asymmetrically, abnormal flow pattern was revealed, with significant flow separation and regions with reversed flow. Abnormal valves had no significant impact on flow in a case of already altered flow evoked by stricture in upper part of the vein. Conclusions In our jugular model, cranially-located stenoses, which in clinical practice are primarily caused by external compression, cause more significant outflow impact respect to endoluminal defects and pathological valves located more caudally.},
 author = {Marian Simka and Paweł Latacz},
 doi = {10.1177/0268355521996087},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0268355521996087},
 journal = {Phlebology},
 note = {PMID:33611976},
 number = {7},
 pages = {541–548},
 title = {Numerical modeling of blood flow in the internal jugular vein with the use of computational fluid mechanics software},
 url = {https://doi-org.crai.referencistas.com/10.1177/0268355521996087},
 volume = {36},
 year = {2021p}
}

@article{doi:10.1177/02683962241283051,
 abstract = {The growing availability of expansive social media trace data (SMTD) offers researchers promising opportunities to create rich depictions of societal and social phenomena. Despite this potential, research analysing such data often struggles to construct novel theoretical insight. This paper argues that holistically incorporating temporality enhances data collection and data analysis, subsequently facilitating process theory construction from SMTD. Recommendations to integrate temporality are outlined in the proposed Temporal Dynamics Framework and Methodology (TDFM). We apply the TDFM to investigate the temporal dynamics of mental health discourse on Twitter (now X) across different phases of the COVID-19 pandemic, theoretically framed in the context of innate psychological needs satisfaction. The findings reveal dynamic shifts in social media use, indicating that different phases of the pandemic triggered changes in the needs motivating, and being motivated by, social media use. This illustrative case reflectively evaluates the TDFM’s usefulness in contextualising SMTD collection, analytical strategies, and process theory construction by incorporating a dynamic perspective on time.},
 author = {Shohil Kishore and David Sundaram and Michael David Myers},
 doi = {10.1177/02683962241283051},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/02683962241283051},
 journal = {Journal of Information Technology},
 number = {0},
 pages = {02683962241283051},
 title = {A temporal dynamics framework and methodology for computationally intensive social media research},
 url = {https://doi-org.crai.referencistas.com/10.1177/02683962241283051},
 volume = {0},
 year = {2024k}
}

@article{doi:10.1177/02683962241289597,
 abstract = {This study advances the field of Computationally Intensive Theory Development (CTD) by examining the capabilities of Explainable Artificial Intelligence (XAI), in particular SHapley Additive exPlanations (SHAP), for theory development, while providing guidelines for this process. We evaluate SHAP’s methodological abilities and develop a structured approach for using SHAP to harness insights from black-box predictive models. For this purpose, we leverage a dual-methodological approach. First, to assess SHAP’s capabilities in uncovering patterns that shape a phenomenon, we conduct a Monte-Carlo simulation study. Second, to illustrate and guide the theory development process with SHAP for CTD, we apply SHAP in a use-case using real-world data. Based on these analyses, we propose a stepwise uniform and replicable approach giving guidance that can benefit rigorous theory development and increase the traceability of the theorizing process. With our structured approach we contribute to the use of XAI approaches in research and, by uncovering patterns in black-box prediction models, add to the ongoing search for next-generation theorizing methods in the field of Information Systems (IS).},
 author = {Dominik Stoffels and Stefan Faltermaier and Kim Simon Strunk and Marina Fiedler},
 doi = {10.1177/02683962241289597},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/02683962241289597},
 journal = {Journal of Information Technology},
 number = {ja},
 pages = {02683962241289597},
 title = {Guiding Computationally Intensive Theory Development with Explainable Artificial Intelligence: The Case of SHAP},
 url = {https://doi-org.crai.referencistas.com/10.1177/02683962241289597},
 volume = {0},
 year = {2024p}
}

@article{doi:10.1177/027046768800800417,
 doi = {10.1177/027046768800800417},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/027046768800800417},
 journal = {Bulletin of Science, Technology & Society},
 number = {4},
 pages = {428–428},
 title = {Scientific Discovery: Computational Explorations of the Creative Process, Gary L. Bradshaw, Pat Langley, Herbert A. Simon and Jan M. Zytkow. 1987. The MIT Press, Cambridge, MA. 340 pages. Name and Subject Index. ISBN: 0-262-12116-6. $9.95 paperback},
 url = {https://doi-org.crai.referencistas.com/10.1177/027046768800800417},
 volume = {8},
 year = {1988t}
}

@article{doi:10.1177/027046769201200261,
 doi = {10.1177/027046769201200261},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/027046769201200261},
 journal = {Bulletin of Science, Technology & Society},
 number = {2},
 pages = {107–107},
 title = {EDUCATION AND STS/S Assessing Higher Order Thinking in Mathematics, Gerald Kulm. 1990. American Assoc. for Advncement of Science, Waldorf, MD. 216 pages. ISBN: 0-87168-356-3. $24.95},
 url = {https://doi-org.crai.referencistas.com/10.1177/027046769201200261},
 volume = {12},
 year = {1992t}
}

@article{doi:10.1177/027046769201200262,
 doi = {10.1177/027046769201200262},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/027046769201200262},
 journal = {Bulletin of Science, Technology & Society},
 number = {2},
 pages = {107–107},
 title = {Computers and Learning: Helping Children Acquire Thinking Skills, Geoffrey Underwood & Jean Underwood. 1990. Basil Blackwell, Cambridge, MA. 209 pages. ISBN: 0-631-15807-3 (hc); 0-631-15808-1 (pb). $42.95 (hc); $14.95 (pb},
 url = {https://doi-org.crai.referencistas.com/10.1177/027046769201200262},
 volume = {12},
 year = {1992t}
}

@article{doi:10.1177/027046769901900105,
 abstract = {Current conceptions of the integration of computers into society often depend on the view that the human mind, as well as the computer, is a computational system. This view is widely taken to have broad implications for educational policy. We present a critique of the premise and some of the conclusions of the above argument. It is here shown that the thesis that the human mind is a computational system is, in principle, not scientifically supportable. It is also shown that, even if the computational theory of mind were held for non-scientific reasons, the educational implications often derived from it do not follow.},
 author = {James E. Martin},
 doi = {10.1177/027046769901900105},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/027046769901900105},
 journal = {Bulletin of Science, Technology & Society},
 number = {1},
 pages = {25–31},
 title = {Noncomputational Versus Computational Conceptions of Reason: Contrasting Educational Implications},
 url = {https://doi-org.crai.referencistas.com/10.1177/027046769901900105},
 volume = {19},
 year = {1999j}
}

@article{doi:10.1177/02711214241288209,
 abstract = {Centering equity in our work eliminates disparities and promotes the learning and development of ALL children. Although there has been increasing focus on supporting science, technology, engineering, and mathematics (STEM) in early childhood, many children with disabilities still face challenges in accessing early STEM opportunities. In this article, we suggest a strengths-based, system-level, and theory-driven approach to encourage the field to shift away from the practice of “fixing” children when providing STEM learning and teaching for young children with disabilities. At the conclusion of this article, we also offer critical questions for readers to ponder and reflect on their work.},
 author = {Hsiu-Wen Yang and Megan Vinh and Elica Sharifnia and Jessica Amsbary and Doug H. Clements and Chih-Ing Lim and Julie Sarama},
 doi = {10.1177/02711214241288209},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/02711214241288209},
 journal = {Topics in Early Childhood Special Education},
 number = {0},
 pages = {02711214241288209},
 title = {(Re)Conceptualizing STEM Learning and Teaching for Young Children With Disabilities},
 url = {https://doi-org.crai.referencistas.com/10.1177/02711214241288209},
 volume = {0},
 year = {2024t}
}

@article{doi:10.1177/0271678X19854640,
 abstract = {Despite the plethora of published studies on intracranial aneurysms (IAs) hemodynamic using computational fluid dynamics (CFD), limited progress has been made towards understanding the complex physics and biology underlying IA pathophysiology. Guided by 1733 published papers, we review and discuss the contemporary IA hemodynamics paradigm established through two decades of IA CFD simulations. We have traced the historical origins of simplified CFD models which impede the progress of comprehending IA pathology. We also delve into the debate concerning the Newtonian fluid assumption used to represent blood flow computationally. We evidently demonstrate that the Newtonian assumption, used in almost 90% of studies, might be insufficient to describe IA hemodynamics. In addition, some fundamental properties of the Navier–Stokes equation are revisited in supplementary material to highlight some widely spread misconceptions regarding wall shear stress (WSS) and its derivatives. Conclusively, our study draws a roadmap for next-generation IA CFD models to help researchers investigate the pathophysiology of IAs.},
 author = {Khalid M Saqr and Sherif Rashad and Simon Tupin and Kuniyasu Niizuma and Tamer Hassan and Teiji Tominaga and Makoto Ohta},
 doi = {10.1177/0271678X19854640},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0271678X19854640},
 journal = {Journal of Cerebral Blood Flow & Metabolism},
 note = {PMID:31213162},
 number = {5},
 pages = {1021–1039},
 title = {What does computational fluid dynamics tell us about intracranial aneurysms? A meta-analysis and critical review},
 url = {https://doi-org.crai.referencistas.com/10.1177/0271678X19854640},
 volume = {40},
 year = {2020t}
}

@article{doi:10.1177/0272989X13514774,
 abstract = {Expected value of information methods evaluate the potential health benefits that can be obtained from conducting new research to reduce uncertainty in the parameters of a cost-effectiveness analysis model, hence reducing decision uncertainty. Expected value of partial perfect information (EVPPI) provides an upper limit to the health gains that can be obtained from conducting a new study on a subset of parameters in the cost-effectiveness analysis and can therefore be used as a sensitivity analysis to identify parameters that most contribute to decision uncertainty and to help guide decisions around which types of study are of most value to prioritize for funding. A common general approach is to use nested Monte Carlo simulation to obtain an estimate of EVPPI. This approach is computationally intensive, can lead to significant sampling bias if an inadequate number of inner samples are obtained, and incorrect results can be obtained if correlations between parameters are not dealt with appropriately. In this article, we set out a range of methods for estimating EVPPI that avoid the need for nested simulation: reparameterization of the net benefit function, Taylor series approximations, and restricted cubic spline estimation of conditional expectations. For each method, we set out the generalized functional form that net benefit must take for the method to be valid. By specifying this functional form, our methods are able to focus on components of the model in which approximation is required, avoiding the complexities involved in developing statistical approximations for the model as a whole. Our methods also allow for any correlations that might exist between model parameters. We illustrate the methods using an example of fluid resuscitation in African children with severe malaria.},
 author = {Jason Madan and Anthony E. Ades and Malcolm Price and Kathryn Maitland and Julie Jemutai and Paul Revill and Nicky J. Welton},
 doi = {10.1177/0272989X13514774},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0272989X13514774},
 journal = {Medical Decision Making},
 note = {PMID:24449434},
 number = {3},
 pages = {327–342},
 title = {Strategies for Efficient Computation of the Expected Value of Partial Perfect Information},
 url = {https://doi-org.crai.referencistas.com/10.1177/0272989X13514774},
 volume = {34},
 year = {2014n}
}

@article{doi:10.1177/0272989X211053794,
 abstract = {Background Expert elicitation (EE) has been used across disciplines to estimate input parameters for computational modeling research when information is sparse or conflictual. Objectives We conducted a systematic review to compare EE methods used to generate model input parameters in health research. Data Sources PubMed and Web of Science. Study Eligibility Modeling studies that reported the use of EE as the source for model input probabilities were included if they were published in English before June 2021 and reported health outcomes. Data Abstraction and Synthesis Studies were classified as “formal” EE methods if they explicitly reported details of their elicitation process. Those that stated use of expert opinion but provided limited information were classified as “indeterminate” methods. In both groups, we abstracted citation details, study design, modeling methodology, a description of elicited parameters, and elicitation methods. Comparisons were made between elicitation methods. Study Appraisal Studies that conducted a formal EE were appraised on the reporting quality of the EE. Quality appraisal was not conducted for studies of indeterminate methods. Results The search identified 1520 articles, of which 152 were included. Of the included studies, 40 were classified as formal EE and 112 as indeterminate methods. Most studies were cost-effectiveness analyses (77.6%). Forty-seven indeterminate method studies provided no information on methods for generating estimates. Among formal EEs, the average reporting quality score was 9 out of 16. Limitations Elicitations on nonhealth topics and those reported in the gray literature were not included. Conclusions We found poor reporting of EE methods used in modeling studies, making it difficult to discern meaningful differences in approaches. Improved quality standards for EEs would improve the validity and replicability of computational models. Highlights We find extensive use of expert elicitation for the development of model input parameters, but most studies do not provide adequate details of their elicitation methods. Lack of reporting hinders greater discussion of the merits and challenges of using expert elicitation for model input parameter development. There is a need to establish expert elicitation best practices and reporting guidelines.},
 author = {Christopher J. Cadham and Marie Knoll and Luz María Sánchez-Romero and K. Michael Cummings and Clifford E. Douglas and Alex Liber and David Mendez and Rafael Meza and Ritesh Mistry and Aylin Sertkaya et al.},
 doi = {10.1177/0272989X211053794},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0272989X211053794},
 journal = {Medical Decision Making},
 note = {PMID:34694168},
 number = {5},
 pages = {684–703},
 title = {The Use of Expert Elicitation among Computational Modeling Studies in Health Research: A Systematic Review},
 url = {https://doi-org.crai.referencistas.com/10.1177/0272989X211053794},
 volume = {42},
 year = {2022c}
}

@article{doi:10.1177/0272989X221085569,
 abstract = {Mathematical health policy models, including microsimulation models (MSMs), are widely used to simulate complex processes and predict outcomes consistent with available data. Calibration is a method to estimate parameter values such that model predictions are similar to observed outcomes of interest. Bayesian calibration methods are popular among the available calibration techniques, given their strong theoretical basis and flexibility to incorporate prior beliefs and draw values from the posterior distribution of model parameters and hence the ability to characterize and evaluate parameter uncertainty in the model outcomes. Approximate Bayesian computation (ABC) is an approach to calibrate complex models in which the likelihood is intractable, focusing on measuring the difference between the simulated model predictions and outcomes of interest in observed data. Although ABC methods are increasingly being used, there is limited practical guidance in the medical decision-making literature on approaches to implement ABC to calibrate MSMs. In this tutorial, we describe the Bayesian calibration framework, introduce the ABC approach, and provide step-by-step guidance for implementing an ABC algorithm to calibrate MSMs, using 2 case examples based on a microsimulation model for dementia. We also provide the R code for applying these methods.},
 author = {Peter Shewmaker and Stavroula A. Chrysanthopoulou and Rowan Iskandar and Derek Lake and Earic Jutkowitz},
 doi = {10.1177/0272989X221085569},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0272989X221085569},
 journal = {Medical Decision Making},
 note = {PMID:35311401},
 number = {5},
 pages = {557–570},
 title = {Microsimulation Model Calibration with Approximate Bayesian Computation in R: A Tutorial},
 url = {https://doi-org.crai.referencistas.com/10.1177/0272989X221085569},
 volume = {42},
 year = {2022q}
}

@article{doi:10.1177/0278364904045478,
 abstract = {Curved shapes are frequent subjects of maneuvers by the human hand. In robotics, it is well known that antipodal grasps exist on curved objects and guarantee force closure under proper finger contact conditions. This paper presents an efficient algorithm that computes, up to numerical resolution, all pairs of antipodal points on a simple, closed, and twice continuously differentiable plane curve. Dissecting the curve into segments everywhere convex or everywhere concave, the algorithm marches simultaneously on a pair of such segments with provable convergence and interleaves marching with numerical bisection recursively. It makes use of new insights into the differential geometry at two antipodal points. We have avoided resorting to traditional nonlinear programming, which would neither be quite as efficient nor guarantee to find all antipodal points. A byproduct of our result is a procedure that constructs all common tangent lines of two curves, achieving quadratic convergence rate. Dissection and the coupling of marching with bisection constitute an algorithm design scheme potentially applicable to computational problems involving curves and curved shapes.},
 author = {Yan-Bin Jia},
 doi = {10.1177/0278364904045478},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0278364904045478},
 journal = {The International Journal of Robotics Research},
 number = {7–8},
 pages = {827–857},
 title = {Computation on Parametric Curves with an Application in Grasping},
 url = {https://doi-org.crai.referencistas.com/10.1177/0278364904045478},
 volume = {23},
 year = {2004l}
}

@article{doi:10.1177/0278364906072038,
 abstract = {This paper is concerned with computation and graphical characterization of robust equilibrium postures suited to quasistatic multi-legged locomotion. Quasistatic locomotion consists of postures in which the mechanism supports itself against gravity while moving its free limbs to new positions. A posture is robust if the contacts can passively support the mechanism against gravity as well as disturbance forces generated by its moving limbs. This paper is concerned with planar mechanisms supported by frictional contacts in two-dimensional gravitational environments. The kinematic structure of the mechanism is lumped into a rigid body B having the same contacts with the environment and a variable center of mass. Inertial forces generated by moving parts of the mechanism are lumped into a neighborhood of wrenches centered at the nominal gravitational wrench. The robust equilibrium postures associated with a given set of contacts become the center-of-mass locations of B that maintain equilibrium with respect to all wrenches in the given neighborhood. The paper formulates the computation of the robust center-of-mass locations as a linear programming problem. It provides graphical characterization of the robust center-of-mass locations, and gives a geometric algorithm for computing these center-of-mass locations. The paper reports experiments validating the equilibrium criterion on a two-legged prototype. Finally, it describes initial progress toward computation of robust equilibrium postures in three dimensions.},
 author = {Yizhar Or and Elon Rimon},
 doi = {10.1177/0278364906072038},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0278364906072038},
 journal = {The International Journal of Robotics Research},
 number = {11},
 pages = {1071–1086},
 title = {Computation and Graphical Characterization of Robust Multiple-Contact                 Postures in Two-Dimensional Gravitational Environments},
 url = {https://doi-org.crai.referencistas.com/10.1177/0278364906072038},
 volume = {25},
 year = {2006m}
}

@article{doi:10.1177/0278364916688255,
 abstract = {Advancements in robotic technology are making it increasingly possible to integrate robots into the human workspace in order to improve productivity and decrease worker strain resulting from the performance of repetitive, arduous physical tasks. While new computational methods have significantly enhanced the ability of people and robots to work flexibly together, there has been little study of the ways in which human factors influence the design of these computational techniques. In particular, collaboration with robots presents unique challenges related to the preservation of human situational awareness and the optimization of workload allocation for human teammates while respecting their workflow preferences. We conducted a series of human subject experiments to investigate these human factors, and provide design guidelines for the development of intelligent collaborative robots based on our results.},
 author = {Matthew Gombolay and Anna Bair and Cindy Huang and Julie Shah},
 doi = {10.1177/0278364916688255},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0278364916688255},
 journal = {The International Journal of Robotics Research},
 number = {5–7},
 pages = {597–617},
 title = {Computational design of mixed-initiative human–robot teaming that considers human factors: situational awareness, workload, and workflow preferences},
 url = {https://doi-org.crai.referencistas.com/10.1177/0278364916688255},
 volume = {36},
 year = {2017f}
}

@article{doi:10.1177/027836498900800605,
 abstract = {The singular value decomposition has been extensively used for the analysis of the kinematic and dynamic characteristics of robotic manipulators. Due to a reputation for being nu merically expensive to compute, however, it has not been used for real-time applications. This work illustrates a for mulation for the singular value decomposition that takes advantage of the nature of robotics matrix calculations to ob tain a computationally feasible algorithm. Several applica tions, including the control of redundant manipulators and the optimization of dexterity, are discussed. A detailed illus tration of the use of the singular value decomposition to deal with the general problem of singularities is also presented.},
 author = {Anthony A. Maciejewski and Charles A. Klein},
 doi = {10.1177/027836498900800605},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/027836498900800605},
 journal = {The International Journal of Robotics Research},
 number = {6},
 pages = {63–79},
 title = {The Singular Value Decomposition: Computation and Applications to Robotics},
 url = {https://doi-org.crai.referencistas.com/10.1177/027836498900800605},
 volume = {8},
 year = {1989i}
}

@article{doi:10.1177/027836499101000401,
 abstract = {In this article we consider the problem of task-directed information gathering. We first develop a decision-theo retic model of task-directed sensing in which sensors are modeled as noise-contaminated, uncertain measurement systems, and sensing tasks are inodeled by a transforma tion describing the type of information required by the task, a utility function describing sensitivity to error, and a cost function describing time or resource constraints on the system. This description allows us to develop a standard condi tional Bayes decision-making model where the value of information, or payoff, of an estimate is defined as the average utility (the expected value of some function of decision or estimation error) relative to the current proba bility distribution and the best estimate is that which max imizes payoff. The optimal sensor viewing strategy is that which maximizes the net payoff (decision value minus observation costs) of the final estimate. The advantage of this solution is generality—it does not assume a particular sensing modality or sensing task. However, solutions to this updating problem do not exist in closed form. This motivates the development of an approximation to the optimal solution based on a grid-based implementation of Bayes’ theorem. We describe this algorithm, analyze its error properties. and indicate how it can be made robust to errors in the description of sensors and discrepancies between geomet ric models and sensed objects. We also present the results of this fusion technique applied to several different infor mation gathering tasks in simulated situations and in a distributed sensing system we have constructed.},
 author = {Greg Hager and Max Mintz},
 doi = {10.1177/027836499101000401},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/027836499101000401},
 journal = {The International Journal of Robotics Research},
 number = {4},
 pages = {285–313},
 title = {Computational Methods for Task-directed Sensor Data Fusion and Sensor Planning},
 url = {https://doi-org.crai.referencistas.com/10.1177/027836499101000401},
 volume = {10},
 year = {1991h}
}

@article{doi:10.1177/027836499501400105,
 abstract = {Robotic tasks (e.g., kinematics and dynamics) are computation ally expensive. The majority of these tasks must be computed in real-time to meet the high sampling rate modes of oper ations. Recently parallel processing has been used to speed up these computations. In this work, we propose a graph- based algorithm to map computational tasks onto multiple instruction-multiple data (MIMD) type of architectures. The algorithm automatically generates the task graph of a given task. Then an annealing procedure is used to allocate the gen erated subtasks to different processors, taking into account the network topology and the communication constraints. Moreover, the proposed technique is simple, flexible, and computationally viable. The efficiency of the algorithm is demonstrated by a case study with good results.},
 author = {Tarek M. Nabhan and Albert Y. Zomaya},
 doi = {10.1177/027836499501400105},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/027836499501400105},
 journal = {The International Journal of Robotics Research},
 number = {1},
 pages = {76–86},
 title = {Application of Parallel Processing to Robotic Computational Tasks},
 url = {https://doi-org.crai.referencistas.com/10.1177/027836499501400105},
 volume = {14},
 year = {1995p}
}

@article{doi:10.1177/0301006617714547,
 author = {Heath E. Matheson},
 doi = {10.1177/0301006617714547},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0301006617714547},
 journal = {Perception},
 note = {PMID:29034808},
 number = {11},
 pages = {1329–1331},
 title = {Butz, M. V., & Kutter, E. F. How the Mind Comes into Being: Introducing Cognitive Science from a Functional and Computational Perspective},
 url = {https://doi-org.crai.referencistas.com/10.1177/0301006617714547},
 volume = {46},
 year = {2017k}
}

@article{doi:10.1177/03010066231178489,
 abstract = {Trustworthy-looking faces are also perceived as more attractive, but are there other meaningful cues that contribute to perceived trustworthiness? Using data-driven models, we identify these cues after removing attractiveness cues. In Experiment 1, we show that both judgments of trustworthiness and attractiveness of faces manipulated by a model of perceived trustworthiness change in the same direction. To control for the effect of attractiveness, we build two new models of perceived trustworthiness: a subtraction model, which forces the perceived attractiveness and trustworthiness to be negatively correlated (Experiment 2), and an orthogonal model, which reduces their correlation (Experiment 3). In both experiments, faces manipulated to appear more trustworthy were indeed perceived to be more trustworthy, but not more attractive. Importantly, in both experiments, these faces were also perceived as more approachable and with more positive expressions, as indicated by both judgments and machine learning algorithms. The current studies show that the visual cues used for trustworthiness and attractiveness judgments can be separated, and that apparent approachability and facial emotion are driving trustworthiness judgments and possibly general valence evaluation.},
 author = {DongWon Oh and Nicole Wedel and Brandon Labbree and Alexander Todorov},
 doi = {10.1177/03010066231178489},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/03010066231178489},
 journal = {Perception},
 note = {PMID:37321648},
 number = {8},
 pages = {590–607},
 title = {Trustworthiness judgments without the halo effect: A data-driven computational modeling approach},
 url = {https://doi-org.crai.referencistas.com/10.1177/03010066231178489},
 volume = {52},
 year = {2023o}
}

@article{doi:10.1177/03010066231204815,
 author = {Isabelle Viaud-Delmon},
 doi = {10.1177/03010066231204815},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/03010066231204815},
 journal = {Perception},
 number = {1},
 pages = {70–72},
 title = {Book Review: Sensing in Social Interaction, the Taste for Cheese in Gourmet Shop (Learning in Doing: Social, Cognitive, and Computational Perspectives – Series) by Lorenza Mondada},
 url = {https://doi-org.crai.referencistas.com/10.1177/03010066231204815},
 volume = {53},
 year = {2024r}
}

@article{doi:10.1177/0305735613517285,
 abstract = {Although musicians often have to perform under high pressure, there is little systematic research into the foci of attention needed to maintain performance in such situations. In the current study, we asked elite musicians to report what they focus on and think about during moments of high pressure, using two retrospective methods (concept mapping and verbal reports). For concept mapping, seven expert teachers from an elite academy of music generated clusters of statements about this issue. In the verbal reports, 44 elite musicians described their thoughts and focus of attention. Concept mapping resulted in six clusters, of which “focus on physical aspects,” “thoughts that give confidence,” and “music-related focus” were shown to be the main foci of attention, together representing 85.2% of all 190 statements generated in the verbal reports. Statements regarding “music-related focus” represented 49.7% of all statements. In conclusion, to maintain a high level of performance under pressure, experienced musicians frequently focus on music-related information, physical aspects, and thoughts that give confidence. Implications and suggestions for future research are discussed.},
 author = {Lori A. Buma and Frank C. Bakker and Raôul R. D. Oudejans},
 doi = {10.1177/0305735613517285},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0305735613517285},
 journal = {Psychology of Music},
 number = {4},
 pages = {459–472},
 title = {Exploring the thoughts and focus of attention of elite musicians under pressure},
 url = {https://doi-org.crai.referencistas.com/10.1177/0305735613517285},
 volume = {43},
 year = {2015c}
}

@article{doi:10.1177/0305829815576819,
 abstract = {This article suggests to quantitative methodologists that the tools that they use (and often others they do not) are more broadly applicable than is often assumed; to reflexivist researchers that there are many more tools available to their research than are often seen as appropriate; and to the IR discipline writ large that most of the disciplinary thinking about the relationships between research, ontology, epistemology, methodology and methods is unnecessarily narrow. Our core goal is to reveal the problematically inaccurate nature of both the qualitative/quantitative and the positivist/post-positivist divides, as well as of traditional methods training. We suggest that the ability to pair, and the utility of pairing, quantitative (traditionally neopositivist) methods with critical (traditionally non-neopositivist) theorising makes this intervention. To this end, the article begins with discussions of the relationships between epistemology and method in IR research. We continue on to frame a disunity of social science in the quantitative/qualitative divide, which lays the groundwork for a section rethinking traditional understandings of how methods, methodology, and epistemology relate. We then make the case for the utility of methods traditionally classified as ‘quantitative’ for critical research in IR. The article concludes by discussing the transformative implications of this understanding for critical theorising, and for theorising knowledge within disciplinary IR.},
 author = {J. Samuel Barkin and Laura Sjoberg},
 doi = {10.1177/0305829815576819},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0305829815576819},
 journal = {Millennium},
 number = {3},
 pages = {852–871},
 title = {Calculating Critique: Thinking Outside the Methods Matching Game},
 url = {https://doi-org.crai.referencistas.com/10.1177/0305829815576819},
 volume = {43},
 year = {2015a}
}

@article{doi:10.1177/0306312719867768,
 abstract = {In 2015, the World Economic Forum announced that the world was on the threshold of a ‘fourth industrial revolution’ driven by a fusion of cutting-edge technologies with unprecedented disruptive power. The next year, in 2016, the fourth industrial revolution appeared as the theme of the Forum’s annual meeting, and as the topic of a book by its founder and executive chairman, Klaus Schwab. Ever since, the Forum has made this impending revolution its top priority, maintaining that it will inevitably change everything we once know about the world and how to live in it, thus creating what I conceptualize as ‘future essentialism’. Within a short space of time, the vision of the fourth industrial revolution was institutionalized and publicly performed in various national settings around the world as a sociotechnical imaginary of a promising and desirable future soon to come. Through readings of original material published by the Forum, and through a case study of the reception of the fourth industrial revolution in Denmark, this article highlights and analyses three discursive strategies – ‘dialectics of pessimism and optimism’, ‘epochalism’ and ‘inevitability’ – in the transformation of a corporate, highly elitist vision of the future into policymaking and public reason on a national level.},
 author = {Kasper Schiølin},
 doi = {10.1177/0306312719867768},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0306312719867768},
 journal = {Social Studies of Science},
 note = {PMID:31464575},
 number = {4},
 pages = {542–566},
 title = {Revolutionary dreams: Future essentialism and the sociotechnical imaginary of the fourth industrial revolution in Denmark},
 url = {https://doi-org.crai.referencistas.com/10.1177/0306312719867768},
 volume = {50},
 year = {2020p}
}

@article{doi:10.1177/03063127231185095,
 abstract = {This paper investigates the role of the materiality of computation in two domains: blockchain technologies and artificial intelligence (AI). Although historically designed as parallel computing accelerators for image rendering and videogames, graphics processing units (GPUs) have been instrumental in the explosion of both cryptoasset mining and machine learning models. The political economy associated with video games and Bitcoin and Ethereum mining provided a staggering growth in performance and energy efficiency and this, in turn, fostered a change in the epistemological understanding of AI: from rules-based or symbolic AI towards the matrix multiplications underpinning connectionism, machine learning and neural nets. Combining a material political economy of markets with a material epistemology of science, the article shows that there is no clear-cut division between software and hardware, between instructions and tools, and between frameworks of thought and the material and economic conditions of possibility of thought itself. As the microchip shortage and the growing geopolitical relevance of the hardware and semiconductor supply chain come to the fore, the paper invites social scientists to engage more closely with the materialities and hardware architectures of ‘virtual’ algorithms and software.},
 author = {Ludovico Rella},
 doi = {10.1177/03063127231185095},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/03063127231185095},
 journal = {Social Studies of Science},
 note = {PMID:37427772},
 number = {1},
 pages = {3–29},
 title = {Close to the metal: Towards a material political economy of the epistemology of computation},
 url = {https://doi-org.crai.referencistas.com/10.1177/03063127231185095},
 volume = {54},
 year = {2024o}
}

@article{doi:10.1177/0306419015591325,
 abstract = {The teaching of computational fluid dynamics at the undergraduate level usually focuses on giving students an understanding of the numerical methods and details involved, supported by what are little more than code fragments, followed by learning an abstract form of computational fluid dynamics skills and processes, without any real interaction with the complex core computer coding behind what is often just ‘easy-to-use’ or ‘push button’ commercial interfaces. Quite often, as the student progresses in his/her use of computational fluid dynamics, especially in the research area, it becomes clear that an ‘off-the-shelf’ commercial computational fluid dynamics package is not able to satisfy all requirements to simulate a given problem fully, nor to obtain accurate results. The purpose of this paper is to outline what must be taught to add computer coding to what usually is a well-protected, though capable of being compiled and linked, core computer code so that the complexity of interacting is lessened and better understood.},
 author = {Desmond Adair and Martin Jaeger},
 doi = {10.1177/0306419015591325},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0306419015591325},
 journal = {International Journal of Mechanical Engineering Education},
 number = {3},
 pages = {153–167},
 title = {Incorporating computational fluid dynamics code development into an undergraduate engineering course},
 url = {https://doi-org.crai.referencistas.com/10.1177/0306419015591325},
 volume = {43},
 year = {2015a}
}

@article{doi:10.1177/0306419015604432,
 abstract = {This paper describes the use of computational fluid dynamics in teaching graduate students who were in a four‐year B. Tech program. Many of these students did not have a good background in mathematics, fluid dynamics, heat transfer, and programming; however, most of them were good at computer‐aided design in ProE and were very interested in learning computational fluid dynamics as a design tool in industries. Solidworks flow simulator was chosen as the computational fluid dynamics software to teach students the entire computational fluid dynamics process in a single integrated software environment. Based on projects, computational fluid dynamics numerical methods and fundamentals of heat transfer and fluid flow were introduced to help students understand the computational fluid dynamics process, interpret, and validate simulation results. The computational fluid dynamics simulation of an orifice meter is given as the basic example for the students. Orifice meters are the most common equipment used for measuring fluid flow because of their simple mechanical structure, versatility, and low cost. In this paper, computational fluid dynamics simulation has been used to predict the orifice flow with better accuracy. Computational fluid dynamics simulations have been performed using solidworks flow simulator and validated with the data available in published literature. A new system has been proposed to accurately measure the flow using orifice metering systems.},
 author = {KS Jithish and PV Ajay Kumar},
 doi = {10.1177/0306419015604432},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0306419015604432},
 journal = {International Journal of Mechanical Engineering Education},
 number = {4},
 pages = {233–246},
 title = {Analysis of turbulent flow through an orifice meter using experimental and computational fluid dynamics simulation approach—A case study},
 url = {https://doi-org.crai.referencistas.com/10.1177/0306419015604432},
 volume = {43},
 year = {2015k}
}

@article{doi:10.1177/0306419016674133,
 abstract = {This paper reports the recent development and implementation of three teaching modules in order to teach and enhance the students’ critical thinking skills in a level IV undergraduate/postgraduate course ‘Computational Fluid Dynamics (CFD) for Engineering Applications’. These teaching modules include a lecture module, an online test module and a CFD project module. The lecture module introduces the importance of critical thinking skills by an example case, critical thinking definition and processes, and the application of critical thinking skills in formulation of CFD problems. In the online test module, seven online tests have been developed to enhance the students’ understanding of the contents of lectures and practical sessions. Meanwhile, students apply their critical thinking skills to work out some of the tests. In the project module, a student-driven CFD project is designed to help students to apply CFD techniques and critical thinking skills in engineering problems. In the project, students choose their own project topic and problems. They use CFD skills learned in the course and critical thinking skills to critically analyse their problems, identify the important parameters and review results. They apply the critical writing skills to finalise a project report. To the best knowledge of the author, this systematical integration of teaching and enhancement of critical thinking skills in computational fluid dynamics course is innovative. Feedback from students is quite positive shown by an anonymous survey in 2014.},
 author = {Zhao F Tian},
 doi = {10.1177/0306419016674133},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0306419016674133},
 journal = {International Journal of Mechanical Engineering Education},
 number = {1},
 pages = {76–88},
 title = {Teaching and enhancement of critical thinking skills for undergraduate students in a computational fluid dynamics course},
 url = {https://doi-org.crai.referencistas.com/10.1177/0306419016674133},
 volume = {45},
 year = {2017p}
}

@article{doi:10.1177/0306419019838880,
 abstract = {The University of Florida Department of Mechanical and Aerospace Engineering recently created a new senior technical elective in the field of computational fluid dynamics. The main objectives of the class are learning the process of computational fluid dynamics, skepticism, a course project that uses a popular commercial solver, and a course project that involves programming a simplified computational fluid dynamics code. The course covers introductory material, history, grid generation, numerics, equations of motion, boundary conditions, solvers, turbulence models, visualization, and a number of special topics. Skepticism is enforced throughout the course and forces students to justify the validity of their approach and question numerically generated results. Students in the class undertake a course project to predict a fundamental flow-field and compare predictions with excellent measurements from the open literature. They must also create a simplified computational fluid dynamics code to predict turbulent boundary layer flow. Students have integrated these lessons within student groups across the University of Florida. The emphasis of the course is on skepticism and increasing integration with the curriculum and student group activities. We present the class philosophy for teaching undergraduate computational fluid dynamics and the outcomes of the newly developed course.},
 author = {Steven A E Miller},
 doi = {10.1177/0306419019838880},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0306419019838880},
 journal = {International Journal of Mechanical Engineering Education},
 number = {4},
 pages = {315–334},
 title = {A contemporary course on the introduction to computational fluid dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1177/0306419019838880},
 volume = {48},
 year = {2020n}
}

@article{doi:10.1177/0306419019876866,
 abstract = {Computational fluid dynamics is taught in many universities and is a trending elective option among engineering students. Although analyzing computational fluid dynamics simulations is exciting enough, the theory is equation intensive, sometimes very abstract and also difficult to visualize for the novice. A creative thinking based approach termed synectics, which involves analogies, was therefore applied in classroom teaching to increase student comfort with the equations. For this purpose six analogies encompassing basic computational fluid dynamics concepts were developed along with pictorial representations, and are presented in this work. These analogies were integrated into classroom teaching via synectics procedures. Student feedback was positive and reflected higher engagement with the course compared to when the metaphoric activity was not implemented. This work attempts to demonstrate the feasibility and value of applying creative techniques, even when teaching a highly structured and equation oriented course such as computational fluid dynamics.},
 author = {PC Sande and S Sharma},
 doi = {10.1177/0306419019876866},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0306419019876866},
 journal = {International Journal of Mechanical Engineering Education},
 number = {2},
 pages = {171–191},
 title = {Synectics model applied in basic theory of computational fluid dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1177/0306419019876866},
 volume = {49},
 year = {2021p}
}

@article{doi:10.1177/0306419020915725,
 abstract = {The goal of the present work was to explore student perspectives about a course module designed with cognitivist and constructivist learning theories, the cognitive apprenticeship instructional model, and a T-shaped design, specifically the first module of a Computational Fluid Mechanics curricular unit within a Master of Computational Mechanics program. This module was redesigned accordingly after the professor participated in a faculty development course, Engineering Education Theory and Practice, at the Faculty of Engineering, University of Porto. Student perspectives on the efficacy of the redesigned course were solicited, along with those from students who had previously taken the course. Students who participated in the revised course module reported higher levels of satisfaction than students from the previous iteration of the course and were more likely to recognize the value of specific instructional activities. This paper describes the revision of the course module and a comparison of the student feedback before and after those revisions.},
 author = {Sónia IS Pinto and Susan M Zvacek},
 doi = {10.1177/0306419020915725},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0306419020915725},
 journal = {International Journal of Mechanical Engineering Education},
 number = {1},
 pages = {51–77},
 title = {Cognitive apprenticeship and T-shaped instructional design in computational fluid mechanics: Student perspectives on learning},
 url = {https://doi-org.crai.referencistas.com/10.1177/0306419020915725},
 volume = {50},
 year = {2022n}
}

@article{doi:10.1177/0306419020950250,
 abstract = {A curriculum enhancement project of embedding MATLAB and Simulink to a mechanical engineering (ME) vibrations and controls course is presented in this paper. MATLAB/Simulink is a popular software tool for vibration analysts and control designers, which is consistently regarded as one of the most in-demand technical skills that employers are looking for. In the past, the ME students at Mississippi State University (MSU) did not have the training opportunity to use MATLAB/Simulink for design and analysis of vibration and control systems. With the support of a teaching grants, the author created an experimental lab section to ask students to design and build vibration and control devices, and integrated these device into his vibrations and controls course. In this study, the author develops a computer lab section based on the implementation of MATLAB/Simulink, which complements with the experimental lab section to provide the students with a full lab experience. The experimental-computational lab allows the students to not only observe and characterize the dynamic response of vibration and control systems through experimental operations and measurements, but also validate experimental results and confirm experimental phenomena through computational analysis. As well as exploring dynamic behaviors of the systems in a variety of conditions through numerical simulations with different settings. An example of student project is presented to show an experimental-computational study conducted by a student team using MATLAB/Simulink software tool and self-developed data acquisition systems based on a base excitation model demonstrated in class. A questionnaire was conducted at the end of that class and results confirms that the implementation of MATLAB/Simulink into the course effectively develops the ME students’ programming skills and strengthens their capacity in modeling, simulating, and analyzing vibration, control, and other dynamic systems. The developed computational lab and the current experimental lab complementarily promote student understanding of principles and concepts conveyed in classroom lectures.},
 author = {Yucheng Liu and Silas Whitaker and Connor Hayes and Jared Logsdon and Logan McAfee and Riley Parker},
 doi = {10.1177/0306419020950250},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0306419020950250},
 journal = {International Journal of Mechanical Engineering Education},
 number = {1},
 pages = {158–175},
 title = {Establishment of an experimental-computational framework for promoting Project-based learning for vibrations and controls education},
 url = {https://doi-org.crai.referencistas.com/10.1177/0306419020950250},
 volume = {50},
 year = {2022i}
}

@article{doi:10.1177/03064190211026207,
 abstract = {Active student engagement, teaching via experience in real-life settings and learning by doing, are pedagogical strategies appropriate to improve student-reasoning skills. By building models, performing investigations, examining and explaining experimental results, using theoretical and computational thinking, constructing representations, undergraduates can acquire a deeper understanding of fundamental disciplinary concepts while reinforcing transversal abilities. In this framework, Engineering courses should be designed with the final objective to develop practical skills, focusing on hands-on activities. This contribution presents two different inquiry-based learning environments recently experienced at the University of Palermo in the context of bioelectronic and biomedical Engineering. The first study describes a laboratory activity about digital ophthalmologic signal classification; the second laboratory focuses on the analysis of the prefrontal cortex activation during a memory task using functional Near InfraRed Spectroscopy (fNIRS). We introduce and discuss the learning workshops, with the research objective of improving current instruction and training in Engineering courses. Indeed, this contribution aims to suggest a conceptual framework in the form of a structured elective suite of modules tailored to meet the needs of Engineering graduates. The outcomes of both studies seem to highlight that self-directed learning activities could enhance students’ enthusiasm to learn and engagement in engineering investigations, contributing to improve the achievements of students and acquire a more effective learning approach.},
 author = {Dominique Persano Adorno},
 doi = {10.1177/03064190211026207},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/03064190211026207},
 journal = {International Journal of Mechanical Engineering Education},
 number = {3},
 pages = {629–647},
 title = {Inquiry-based environments for bio-signal processing training in engineering education},
 url = {https://doi-org.crai.referencistas.com/10.1177/03064190211026207},
 volume = {50},
 year = {2022a}
}

@article{doi:10.1177/03064190231224334,
 abstract = {The great demands for computational fluid dynamics (CFD) practitioners in industry have motivated universities to integrate CFD courses into undergraduate curricula. This article introduces a learning-centred CFD course that aims to train critical users of commercial CFD codes. The main features of the course include a project-based approach to learning and assessment-guided learning activities, both scaffolded by technologies. The implementation of the course is informed by contemporary pedagogical theories that encourage students to have the ownership of their own learning and constructively build their knowledge in an inclusive and supportive learning environment. The students upon completing the course have developed a conceptual understanding of the CFD principles and the importance of performing CFD simulations in a critical way, making them ready for more advanced CFD learning and practices. Course evaluation based on feedback from various sources demonstrates that the learning-centred approach is the key to the success of the course.},
 author = {Xiangdong Li and Sherman CP Cheung},
 doi = {10.1177/03064190231224334},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/03064190231224334},
 journal = {International Journal of Mechanical Engineering Education},
 number = {0},
 pages = {03064190231224334},
 title = {A learning-centred computational fluid dynamics course for undergraduate engineering students},
 url = {https://doi-org.crai.referencistas.com/10.1177/03064190231224334},
 volume = {0},
 year = {2024h}
}

@article{doi:10.1177/0306624X17694405,
 abstract = {Is the relationship between criminal thinking and recidivism the same for criminal justice–involved individuals from varying demographic backgrounds? Relying on two independent samples of offenders and two measures of criminal thinking, the current studies examined whether four demographic factors—gender, race, age, and education—moderated the relationship between criminal thinking and recidivism. Study 1 consisted of 226 drug-involved probationers enrolled in a randomized clinical trial. Study 2 consisted of 346 jail inmates from a longitudinal study. Logistic regression models suggested that the strength of the relationship between criminal thinking and subsequent recidivism did not vary based on participant demographics, regardless of justice system setting or measure of criminal thinking. Criminal thinking predicts recidivism similarly for people who are male, female, Black, White, older, younger, and more or less educated.},
 author = {Johanna B. Folk and Jeffrey B. Stuewig and Brandy L. Blasko and Michael Caudy and Andres G. Martinez and Stephanie Maass and Faye S. Taxman and June P. Tangney},
 doi = {10.1177/0306624X17694405},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0306624X17694405},
 journal = {International Journal of Offender Therapy and Comparative Criminology},
 note = {PMID:29237316},
 number = {7},
 pages = {2045–2062},
 title = {Do Demographic Factors Moderate How Well Criminal Thinking Predicts Recidivism?},
 url = {https://doi-org.crai.referencistas.com/10.1177/0306624X17694405},
 volume = {62},
 year = {2018f}
}

@article{doi:10.1177/03080188241257167,
 abstract = {Narrative can be conceptualised as a complex system, with pragmatic benefits for our description of interpretative and affective processes involved in reading. In this article, I show how systemic thinking has shaped many literary theories, and I offer some suggestions about the modelling of narrative as a complex system. Considering the relations between text, reader, and reading situation as constitutive of narrative requires a way of looking at stories keeping in mind that the reader’s experiential background, their cognitive-affective states, and the situational context all play a crucial role in the emergence of what we call a narrative. Here I suggest that Transformer-based language models can be considered artificial systems that simulate a (disembodied) reading process and make it possible to model narrative as a complex system, including the encoded experiential background of the (simulated) reader.},
 author = {Federico Pianzola},
 doi = {10.1177/03080188241257167},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/03080188241257167},
 journal = {Interdisciplinary Science Reviews},
 number = {2},
 pages = {222–236},
 title = {Dynamical systems, literary theory, and the computational modelling of narrative},
 url = {https://doi-org.crai.referencistas.com/10.1177/03080188241257167},
 volume = {49},
 year = {2024q}
}

@article{doi:10.1177/0309133318804977,
 abstract = {Following the Deepwater Horizon oil spill of 2010, a substantial body of research has focused on the development of computational tools and analytical frameworks for modeling oil spill events. Much of this work is dedicated to deepening our understanding of the interactions between oil, fragile ecosystems, and the environment, as well as the impacts of oil on human settlements which are vulnerable to spill events. These advances in oil spill modeling and associated analytics have not only increased the efficiency of spill interdiction and mitigation efforts, they have also helped to nurture proactive, versus reactive, response strategies and plans for local and regional stakeholders. The purpose of this paper is to provide a progress report on the wide range of computational tools, analytical frameworks, and emerging technologies which are necessary inputs for a complete oil spill modeling package. Specifically, we explore the use of relatively mature tools, such as dedicated spill modeling packages, geographic information systems (GIS), and remote sensing, as well emerging technologies such as aerial and aquatic drones and other in-situ sensing technologies. The integration of these technologies and the advantages associated with using a geographic lens for oil spill modeling are discussed.},
 author = {Jake R. Nelson and Tony H. Grubesic},
 doi = {10.1177/0309133318804977},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0309133318804977},
 journal = {Progress in Physical Geography: Earth and Environment},
 number = {1},
 pages = {129–143},
 title = {Oil spill modeling: computational tools, analytical frameworks, and emerging technologies},
 url = {https://doi-org.crai.referencistas.com/10.1177/0309133318804977},
 volume = {43},
 year = {2019o}
}

@article{doi:10.1177/0309324715625824,
 author = {Erwin Hack and George Lampeas},
 doi = {10.1177/0309324715625824},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0309324715625824},
 journal = {The Journal of Strain Analysis for Engineering Design},
 number = {1},
 pages = {3–4},
 title = {Advances in Validation of Computational Mechanics Models},
 url = {https://doi-org.crai.referencistas.com/10.1177/0309324715625824},
 volume = {51},
 year = {2016f}
}

@article{doi:10.1177/0309524X15624346,
 abstract = {The accuracy of power production predictions made using a linear model, Wind Atlas Analysis and Application Program, and the computational fluid dynamics model, Meteodyn WT, was assessed for a Seongsan wind farm on Jeju Island, South Korea. The actual data on power production from 10 Vestas V80 2 MW wind turbines were collected through a supervisory control and data acquisition system. Wind data were collected over a period of 1 year from a 70-m-height met mast which was about 1.6 km away from the wind farm. The two applications were used for estimating the annual energy production for both an entire wind farm and an individual wind turbine and accomplished this by assessing five different wind farm sizes ranging from 5 km × 5 km to 20 km × 20 km. The results showed that Meteodyn WT software performed better than the Wind Atlas Analysis and Application Program in predicting power production for all the five domain sizes studied. For the entire wind farm, the relative error in annual energy production prediction was from 2.5% to 3.5% when running Meteodyn WT, while ranging from 3.8% to 11.5% when running WindPRO. While a little overestimation in the capacity factor occurred using the WindPRO application, a little underestimation was found using Meteodyn WT.},
 author = {Beomcheol Ju and Jihyun Jeong and Kyungnam Ko},
 doi = {10.1177/0309524X15624346},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0309524X15624346},
 journal = {Wind Engineering},
 number = {1},
 pages = {59–68},
 title = {Assessment of Wind Atlas Analysis and Application Program and computational fluid dynamics estimates for power production on a Jeju Island wind farm},
 url = {https://doi-org.crai.referencistas.com/10.1177/0309524X15624346},
 volume = {40},
 year = {2016h}
}

@article{doi:10.1177/0309524X17709724,
 abstract = {In this article, the implementation of the pitch-control option is described within the OpenFOAM computational fluid dynamics library supported by the preliminary results for a few representative test cases. The proposed pitch-control logic represents a simplified version of the typically model available for the load simulation based on blade element momentum theory codes. The control variable for the blade pitch regulation is the rotor rotational speed, for which an additional torque-speed controller determining the equilibrium between the aerodynamic and the electrical torque is required. The simulations are performed on the 80-m-diameter turbine used as a reference design within the SmartBlades project. The unsteady test cases are based on the assumption of rigid, not pre-bend blade, and refer to an emergency stop and an extreme operating gust. Results verified the correctness of the implemented control logic and its sensitivity to the control parameters, that is, the key factors for a successful regulation of power production and load mitigation in wind turbine design. The implemented simulation methodology constitutes the first step toward the reproduction of very complex operating conditions for wind turbines by means of accurate computations.},
 author = {Elia Daniele},
 doi = {10.1177/0309524X17709724},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0309524X17709724},
 journal = {Wind Engineering},
 number = {4},
 pages = {213–225},
 title = {Wind turbine control in computational fluid dynamics with OpenFOAM},
 url = {https://doi-org.crai.referencistas.com/10.1177/0309524X17709724},
 volume = {41},
 year = {2017f}
}

@article{doi:10.1177/0309524X17709732,
 abstract = {This article aims to present a two-dimensional parametric analysis of a modified Savonius wind turbine using computational fluid dynamics. The effects of three independent parameters of the rotor, namely, shape factor, overlap ratio, and tip speed ratio on turbine performance were studied and then optimized for maximum coefficient of performance using response surface methodology. The rotor performance was analyzed over specific domains of the parameters under study, and three-variable Box-Behnken design was used for design of experiment. The specific parametric combinations as per design of experiment were simulated using ANSYS Fluent®, and the response variable, coefficient of performance (Cp), was calculated. The sliding mesh model was utilized, and the flow was simulated using Shear Stress Transport (SST) k − ω model. The model was validated using past experimental results and found to predict parametric effects accurately. Minitab® and ReliaSoft DOE++® were used to develop regression equation and find the optimum combination of parameters for coefficient of performance over the specified parametric domains using response surface methodology.},
 author = {Haris Moazam Sheikh and Zeeshan Shabbir and Hassan Ahmed and Muhammad Hamza Waseem and Muhammad Zubair Sheikh},
 doi = {10.1177/0309524X17709732},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0309524X17709732},
 journal = {Wind Engineering},
 number = {5},
 pages = {285–296},
 title = {Computational fluid dynamics analysis of a modified Savonius rotor and optimization using response surface methodology},
 url = {https://doi-org.crai.referencistas.com/10.1177/0309524X17709732},
 volume = {41},
 year = {2017n}
}

@article{doi:10.1177/0309524X18780384,
 abstract = {This study is dedicated to drawing a comparison between two- and three-dimensional approach capabilities for the simulation of two similar rotors placed in three inline (or tandem) arrangements. This arrangement is generally recognized as the worst-case scenario for the downwind rotor considering the vortices and disorders produced by the upwind rotor. The rotor in question with the diameter of 2.5 m is made up of three NACA0015 blades with the chord length and span size equal to 0.4 and 3 m, respectively. Based on the authors’ previous works, the shear stress transport model was selected for this comparative study. According to the results, there is an appreciable deviation in the aerodynamic performance of the upwind rotor predicted by the two-dimensional and three-dimensional simulation techniques. There is no tangible difference between the two-dimensional and three-dimensional results in terms of the averaged power output for the downwind rotor. However, the study of flow field employing different means like vortex structures, axial velocity, and even torque variation indicates that the two-dimensional approach is unable to achieve realistic and reliable output data. The introduced “pillar effect” regarding the dimensional limitations of the two-dimensional approach, which affects the vorticity shape and its dissipation, is plausible evidence for this discrepancy.},
 author = {Saeed Nazari and Mahdi Zamani and Sajad A Moshizi},
 doi = {10.1177/0309524X18780384},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0309524X18780384},
 journal = {Wind Engineering},
 number = {6},
 pages = {647–664},
 title = {Comparison between two-dimensional and three-dimensional computational fluid dynamics techniques for two straight-bladed vertical-axis wind turbines in inline arrangement},
 url = {https://doi-org.crai.referencistas.com/10.1177/0309524X18780384},
 volume = {42},
 year = {2018o}
}

@article{doi:10.1177/0309524X211060552,
 abstract = {This paper details the development of a generalized computational approach that enables prediction of cavity-internal sound pressure distribution due to flow-generated noise at high frequencies. The outcomes of this research is of particular interest for development of an acoustics-based structural health monitoring system for wind turbine blades. The methodology builds from existing reduced-order aeroacoustic modeling techniques and ray tracing based geometrical acoustics and is demonstrated on the model NREL 5 MW wind turbine blade as a case study. The computational predictions demonstrated that damage could be successfully detected in the first half of the blade cavity near the root and that the change in frequency content may be indicative of the type of damage that has occurred. This study provides a foundation to analyze specific blades and likely damage cases for determining key factors of system design such as number and placement of sensors as well as for hardware selection.},
 author = {Caleb Traylor and Murat Inalpolat},
 doi = {10.1177/0309524X211060552},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0309524X211060552},
 journal = {Wind Engineering},
 number = {3},
 pages = {914–937},
 title = {A generalized computational approach to predict high-frequency acoustic pressure response of cavity structures for structural health monitoring of wind turbine blades},
 url = {https://doi-org.crai.referencistas.com/10.1177/0309524X211060552},
 volume = {46},
 year = {2022r}
}

@article{doi:10.1177/0309524X231169298,
 abstract = {A CFD-based assessment and validation of the NREL Phase-VI Sequence-S rotor at seven wind speeds presented here. The ability of a three-dimensional, unstructured, unsteady RANS solver in successfully predicting the wind flow interactions with a rotating, twisted and tapered rotor is described. The uRANS equations were coupled with SST κ-ω turbulence model and a correlation-based Gamma-Theta transition model. The simulations were performed in ANSYS CFX using both Single Reference Frame (SRF) and Multiple Reference Frame (MRF) modelling approaches. A good agreement with measurements is observed at six of seven wind speeds when comparing the integral quantities, the spanwise and chordwise distributions. The only exception is the 10 m/s wind speed case, attributed to the onset of a massive leading edge stall around the mid-span region. It is successfully demonstrated here how uRANS-based CFD computations can be effectively employed in the study of wind turbine rotor aerodynamics.},
 author = {Mujahid Shaik and Balaji Subramanian},
 doi = {10.1177/0309524X231169298},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0309524X231169298},
 journal = {Wind Engineering},
 number = {5},
 pages = {973–994},
 title = {Computational investigation of NREL Phase-VI rotor: Validation of test sequence-S measurements},
 url = {https://doi-org.crai.referencistas.com/10.1177/0309524X231169298},
 volume = {47},
 year = {2023p}
}

@article{doi:10.1177/0309524X241229169,
 abstract = {A computational investigation of New MEXICO test cases operating under axial flow conditions is reported. Three wind speed cases (10, 15, 24 m/s) corresponding to three different tip speed ratios (10, 6.67, 4.17) when the turbine operates at 425.1 rpm were considered. ANSYS CFX 2021R1 was employed to perform simulations using Single Reference Frame (SRF) and Multiple Reference Frame (MRF) approaches. The flow field is computed by solving unsteady Reynolds Averaged Navier-Stokes (uRANS) equations coupled with SST k-ω turbulence model and Gamma-Theta transition model. Validation involved comparing CFD-predicted integral quantities, static pressure distributions, and loads with corresponding experimental values demonstrating reasonably good agreement at all three wind speeds. Overall, SRF exhibited slightly better wake predictions (hypothetical), while MRF predictions were closer to measurements for integral quantities, static pressure and loads. This study demonstrates the utility of uRANS-based 3D CFD computations in wind turbine aerodynamics studies.},
 author = {Mujahid Shaik and Balaji Subramanian},
 doi = {10.1177/0309524X241229169},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0309524X241229169},
 journal = {Wind Engineering},
 number = {4},
 pages = {651–683},
 title = {Computational investigation and validation of new MEXICO experiment},
 url = {https://doi-org.crai.referencistas.com/10.1177/0309524X241229169},
 volume = {48},
 year = {2024q}
}

@article{doi:10.1177/0361198105192300121,
 abstract = {This paper addresses a dynamic vehicle routing problem related to courier mail services with explicit consideration of multiple objectives. This problem is modeled as a dynamic traveling repairman problem with time windows, and with three objectives considered: maximization of the number of serviced customers, minimization of customer waiting, and minimization of total travel time. A solution (simulation) framework, extending techniques developed for single-objective dynamic routing problems in the literature, is proposed to tackle this multiobjective problem. An efficient chained local search heuristic is embedded in this solution framework to solve the underlying static problem, the traveling salesman problem with time windows. Comprehensive simulation experiments conducted on data sets derived from benchmark problems show that the proposed algorithmic approach is able to provide high-quality solutions with limited computing effort. These experiments also indicate that the multiobjective model proposed in this study can offer much more attractive solutions to decision makers than single-objective models.},
 author = {Hao Tang and Mingwei Hu},
 doi = {10.1177/0361198105192300121},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0361198105192300121},
 journal = {Transportation Research Record},
 number = {1},
 pages = {199–207},
 title = {Dynamic Vehicle Routing Problem with Multiple Objectives: Solution Framework and Computational Experiments},
 url = {https://doi-org.crai.referencistas.com/10.1177/0361198105192300121},
 volume = {1923},
 year = {2005q}
}

@article{doi:10.1177/0361198119841554,
 abstract = {Driver distraction is one of the major causes for fatal car accidents. In a distracting activity, manual distraction is a triggered response of other types of distraction, such as cognitive and visual distraction. Therefore, recognition of manual distraction can contribute to the monitoring of overall drivers’ distraction. In this study, a computer vision-based method to track hand movement from the recorded driving behavior is proposed. This method integrates a low computational cost template matching algorithm using fast normalized cross coefficient (NCC) and a novel searching strategy. The proposed method was evaluated by the VIVA hand tracking data set. It achieves 50.83% of marginal accuracy percentage (mAP), 42.18% of multiple object tracking accuracy (MOTA), 31.56% of mostly tracked (MT), and 19.29% of mostly lost (ML), and it outperformed a state of the art algorithm in MOTA and MT. Additionally, the computational cost of the proposed method is greatly improved, and it can run at around 11.1 frames per second. The outcome of this research will further assist driving distraction recognition and mitigation, and improve driving safety.},
 author = {Li Li and Clayton M. Hutmacher and Xu Xu},
 doi = {10.1177/0361198119841554},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0361198119841554},
 journal = {Transportation Research Record},
 number = {8},
 pages = {233–241},
 title = {Video-Based Driver’s Hand Tracking using Fast Normalized Cross Coefficient with Improved Computational Efficiency},
 url = {https://doi-org.crai.referencistas.com/10.1177/0361198119841554},
 volume = {2673},
 year = {2019l}
}

@article{doi:10.1177/0361198120927390,
 abstract = {Hydraulic conductivity (k) is critical for designing permeable pavements for safety and environmental reasons. A novel approach for estimating k is through computational fluid dynamics (CFD) applied to permeable asphalt (PA). Specimens of PA are examined in this study to evaluate CFD applicability. Tortuosity, effective porosity, pore size distribution, and specific surface area were determined based on three-dimensional specimen structures reconstructed using x-ray tomography analyses. Using CFD, estimates of k were determined and compared with physical measurements and also with the results obtained from the semi-empirical modified Kozeny-Carman model (KCM). The comparison shows that the numerical simulations with CFD can be a reasonable tool for estimating k and for examining transport of water through PA. Within the constraints of this study, results infer that CFD can provide more representative results for low k in comparison with KCM. For higher k, CFD and KCM results were reasonably comparable.},
 author = {Veronica Fedele and Nicola Berloco and Pasquale Colonna and Ashton Hertrich and Paolo Intini and Vittorio Ranieri and John J. Sansalone},
 doi = {10.1177/0361198120927390},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0361198120927390},
 journal = {Transportation Research Record},
 number = {8},
 pages = {370–383},
 title = {Computational Fluid Dynamics as a Tool to Estimate Hydraulic Conductivity of Permeable Asphalts},
 url = {https://doi-org.crai.referencistas.com/10.1177/0361198120927390},
 volume = {2674},
 year = {2020e}
}

@article{doi:10.1177/03611981211006731,
 abstract = {Intelligent driving has been benefiting from advances in automotive big data analysis and on-demand data communications, where efficient vehicle-to-cloud communication is a key technology able to collect a huge amount of data from vehicles. However, as the capacities of the network and computational resources of the cloud are not unlimited, simultaneous data transfer based on a best-effort manner results in a resource starvation problem. Therefore, an application is needed to prioritize data communication to alleviate the problem and complete the data transfer just in time. This paper highlights the problem of resource starvation led by best-effort data transfers. It proposes protocols for data communication and control that could enable just-in-time data communication, notably for data collection. We demonstrate through experiments that the proposed mechanism enables efficient and effective data collection over the shared network resources.},
 author = {Hirochika Asai and Yusuke Doi and Ryokichi Onishi},
 doi = {10.1177/03611981211006731},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/03611981211006731},
 journal = {Transportation Research Record},
 number = {11},
 pages = {54–63},
 title = {Toward Just-in-Time Data Communications over Shared Networks and Computational Resources on Massive Client Environment},
 url = {https://doi-org.crai.referencistas.com/10.1177/03611981211006731},
 volume = {2675},
 year = {2021b}
}

@article{doi:10.1177/03611981211007848,
 abstract = {Lime stabilization is a common technique used to improve the engineering properties of clayey soils. The process of lime stabilization can be split into two parts. First, the mobilization and crowding of ions or molecules from hydrated lime at net negative surface charge sites on expansive clay colloids. Second, the formation of pozzolanic products including calcium-silicate-hydrate (C-S-H) because of reactions within lime-soil mixtures. The pozzolanic reaction is generally considered to be more durable, while the adsorption has been associated with more easily reversible consistency changes. This study offers a protocol to assess whether the stabilization process is dominated by durable C-S-H (pozzolanic) reactions or a combination of cation exchange and pozzolanic reactions. Expansive clays with plasticity indices >45% from a major highway project in Texas are the focus of lime treatment in this study. The protocol consists of subjecting lime-soil mixtures to a reasonable curing period followed by a rigorous but realistic durability test and investigating the quality and quantity of the pozzolanic reaction product. Mineralogical analyses using quantitative X-ray diffraction (XRD) and thermogravimetric analysis (TGA) indicates the formation of different forms of C-S-H. In addition, geochemical modeling is used to simulate the lime-soil reactions and evaluate the effect of pH on the stability of C-S-H. The results indicate C-S-H with Ca/Si ratio of 0.66 as most the stable form of C-S-H among other forms with Ca/Si ratio ranging from 0.66 to 2.25. The effect of reducing equilibrium pH on C-S-H is also evaluated. A reduction in pH favored dissolution of all forms of C-S-H indicating the need to maintain a pH ≥ 10.},
 author = {Pavan Akula and Saureen Rajesh Naik and Dallas N. Little},
 doi = {10.1177/03611981211007848},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/03611981211007848},
 journal = {Transportation Research Record},
 number = {9},
 pages = {1469–1481},
 title = {Evaluating the Durability of Lime-Stabilized Soil Mixtures using Soil Mineralogy and Computational Geochemistry},
 url = {https://doi-org.crai.referencistas.com/10.1177/03611981211007848},
 volume = {2675},
 year = {2021a}
}

@article{doi:10.1177/03611981241254113,
 abstract = {Snow-drifting disasters frequently occur in snowy regions with strong winds and can cause traffic interruptions. Cuttings are particularly vulnerable to severe snowdrift disasters, making designing cutting sections in regions prone to drifting snow challenging. To address this issue, a series of Reynolds-averaged Navier–Stokes (RANS) computational fluid dynamics (CFD) simulations were conducted to study the impact of cutting section parameters on snowdrift development in cuttings with varying depths and slopes. Initially, the wind velocity and wall friction velocity were carefully analyzed in the initial state of the cutting. The evolution process of snow distribution in different cuttings was then compared. Furthermore, the influence of parameters on wind velocity development was analyzed. Based on the flow patterns and snow distribution, cuttings involved in drifting snow disasters can be divided into two types, depending on whether there are vortices in the cutting. When the slope gradient changes from 1/1 to 1/2, the flow in the cutting will not separate and reattach, indicating that the critical slope angle for two types of cutting is 27°. As the slope gradient decreases, the leeward drift will move from the area near the crest to the foot of the slope. Increasing the depth of the cutting not only reduces the accumulation of snow on the road surface but also provides scope to improve the safety reserve capacity of the slopes.},
 author = {Sai Li and Wenyong Ma and Peng Wang and Shiyi Guo and Yi Gao and Ziyue Zhan},
 doi = {10.1177/03611981241254113},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/03611981241254113},
 journal = {Transportation Research Record},
 number = {0},
 pages = {03611981241254113},
 title = {Computational Fluid Dynamics Simulation of Drifting Snow Disaster on Cutting: Effect of Cross-Section Parameters},
 url = {https://doi-org.crai.referencistas.com/10.1177/03611981241254113},
 volume = {0},
 year = {2024j}
}

@article{doi:10.1177/0361198196154900101,
 abstract = {Bridge aesthetics, an area of great interest to engineers, is recognized as a difficult area to research because of its subjective nature and ill-defined concepts. An innovative computational decision support tool for use in bridge aesthetics is described. The system contains a knowledge base created by using an extensive literature survey and by completing in-depth knowledge elicitation from leading bridge designers in the United Kingdom and with the support of the Department of Transport. To further validate the knowledge base, the rules elicited from the designers were tested against public opinion through a series of questionnaires. This procedure enabled the construction of a knowledge base that supports many aspects of aesthetics for small- to medium-sized span road bridges. This limited domain was chosen because this form of bridge is currently the most commonly constructed in Britain, yet generally receives little attention in terms of aesthetics because of time and economic constraints. The knowledge base has been incorporated into a system aimed at providing interactive advice on comparative bridge design. The system was developed in Visual C + + to run on a personal computer and to operate in a Windows environment. It is currently undergoing prolonged evaluation in practicing bridge design offices. To date, this evaluation has shown that engineers like the system and would find it useful. Therefore, commercial exploitation is currently being explored. The comments received during the evaluation are also being used to further tailor the system to suit practicing engineers’ needs.},
 author = {C. J. Moore and J. C. Miles and S. N. Evans},
 doi = {10.1177/0361198196154900101},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0361198196154900101},
 journal = {Transportation Research Record},
 number = {1},
 pages = {1–11},
 title = {Innovative Computational Support in Bridge Aesthetics},
 url = {https://doi-org.crai.referencistas.com/10.1177/0361198196154900101},
 volume = {1549},
 year = {1996n}
}

@article{doi:10.1177/0361198196155000101,
 abstract = {With regard to road safety issues, a deep understanding of the driver as a logic system is crucial to predict the most probable behavior according to the contextual elements. Knowledge and data about human functional abilities exist. But the problem is to organize and structure them. The development of a computational approach in driver modelization is addressed. In the first part, a brief historical overview is presented of available driver models in ergonomics and psychological areas, and the distinction between predictive and explicative models in an implementation perspective is the focus. In the second part, the computational aspect of the work is described, along with the software concepts, the cognitive modeling needs, and the implementation choices. Object-oriented techniques were chosen because they provide a modular overview of the general system and offer a convenient representation of cognitive processes. Object-oriented formalism, in particular object modeling technique diagrams, acts as a bridge between the two domains of computer science and the human sciences. The objective is to determine whether it is possible to implement reliably a driver model using the techniques from artificial intelligence and based on the theoretical knowledge from cognitive sciences research. This attempt to establish links between different scientific domains, requiring a common tool, is a challenge. A first step of a work that will have to be developed in a long-term time scale, taking into account its quite ambitious objective, is described.},
 author = {Hέlène Tattegrain-Veste and Thierry Bellet and Annie Pauziέ and Andrέ Chapon},
 doi = {10.1177/0361198196155000101},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0361198196155000101},
 journal = {Transportation Research Record},
 number = {1},
 pages = {1–7},
 title = {Computational Driver Model in Transport Engineering: COSMODRIVE},
 url = {https://doi-org.crai.referencistas.com/10.1177/0361198196155000101},
 volume = {1550},
 year = {1996r}
}

@article{doi:10.1177/0361684320977178,
 abstract = {We combined established psychological measures with techniques in machine learning to measure changes in gender stereotypes over the course of the 20th century as expressed in large-scale historical natural language data. Although our analysis replicated robust gender biases previously documented in the literature, we found that the strength of these biases has diminished over time. This appears to be driven by changes in gender biases for stereotypically feminine traits (rather than stereotypically masculine traits) and changes in gender biases for personality-related traits (rather than physical traits). Our results illustrate the dynamic nature of stereotypes and show how recent advances in data science can be used to provide a long-term historical analysis of core psychological variables. In terms of practice, these findings may, albeit cautiously, suggest that women and men can be less constrained by prescriptions of feminine traits. Additional online materials for this article are available on PWQ’s website at 10.1177/0361684320977178},
 author = {Nazlı Bhatia and Sudeep Bhatia},
 doi = {10.1177/0361684320977178},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0361684320977178},
 journal = {Psychology of Women Quarterly},
 number = {1},
 pages = {106–125},
 title = {Changes in Gender Stereotypes Over Time: A Computational Analysis},
 url = {https://doi-org.crai.referencistas.com/10.1177/0361684320977178},
 volume = {45},
 year = {2021c}
}

@article{doi:10.1177/0363546508322496,
 abstract = {Background Patellar tendinopathy (jumper’s knee) is characterized by localized tenderness of the patellar tendon at its origin on the inferior pole of the patella and a characteristic increase in signal intensity on magnetic resonance imaging at this location. However, it is unclear why the lesion typically occurs in this area of the patellar tendon as surface strain gauge studies of the patellar tendon through the range of motion have produced conflicting results. Hypothesis The predicted patellar tendon strains that occur as a result of the tendon loads and patella-patellar tendon angles (PPTAs) experienced during a jump landing will be significantly increased in the area of the patellar tendon associated with patellar tendinopathy. Study Design Descriptive laboratory study. Methods A 2-dimensional, computational, finite element model of the patella-patellar tendon complex was developed using anatomic measurements taken from lateral radiographs of a normal knee. The patella was modeled with plane strain rigid elements, and the patellar tendon was modeled with 8-node plane strain elements with neo-Hookean material properties. A tie constraint was used to join the patellar tendon and patella. Patella-patellar tendon angles corresponding to knee flexion angles between 0° and 60° and patellar tendon strains ranging from 5% to 15% were used as input variables into the computational model. To determine if the location of increased strain predicted by the computational model could produce isolated tendon fascicle damage in that same area, 5 human cadaveric patella-patellar tendon-tibia specimens were loaded under conditions predicted by the model to significantly increase localized tendon strain. Pre- and posttesting ultrasound images of the patella–patellar tendon specimens were obtained to document the location of any injured fascicles. Results Localized tendon strain at the classic location of the jumper’s knee lesion was found to increase in association with an increase in the magnitude of applied patellar tendon strain and a decrease in the PPTA. The principal stresses and strains predicted by the model for this localized area were tensile and not compressive in nature. Applying the tendon strain conditions and PPTA predicted by the model to significantly increase localized strain resulted in disruption of tendon fascicles in 3 of the 5 cadaveric specimens at the classic location of the patellar tendinopathy lesion. Conclusion The localized increase in patellar tendon strain that occurs in response to the application of tendon loads and decreased PPTA could induce microdamage at the classic location of the jumper’s knee lesion. Clinical Relevance The association of decreasing PPTA with increasing localized tendon strain would implicate the role of knee-joint angle as well as tendon force in the etiopathogenesis of jumper’s knee.},
 author = {Michael Lavagnino and Steven P. Arnoczky and Niell Elvin and Julie Dodds},
 doi = {10.1177/0363546508322496},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0363546508322496},
 journal = {The American Journal of Sports Medicine},
 note = {PMID:18768702},
 number = {11},
 pages = {2110–2118},
 title = {Patellar Tendon Strain is Increased at the Site of the Jumper’s Knee Lesion during Knee Flexion and Tendon Loading: Results and Cadaveric Testing of a Computational Model},
 url = {https://doi-org.crai.referencistas.com/10.1177/0363546508322496},
 volume = {36},
 year = {2008j}
}

@article{doi:10.1177/039139880102400306,
 abstract = {Coronary artery bypass graft (CABG) operation for coronary artery disease with different types of grafts has a large clinical application world wide. Immediately after this operation patients are usually relieved of their chest pain and have improved cardiac function. However, after a while, these bypass grafts may fail due to for example, neointimal hyperplasia or thrombosis. One of the causes for this bypass graft failure is assumed to be the blood flow with low wall shear stress. The aim of this research is to estimate the wall shear stress in a graft and thus to locate areas were wall shear stress is low. This was done with the help of a blood flow computer model. Postoperative biplane angiograms of the graft were recorded, and from these the three-dimensional geometry of the graft was reconstructed and imported into the computational fluid dynamics (CFD) program FLUENT. The stationary diastolic flow through the grafts was calculated, and the wall shear stress distribution was estimated. This procedure was carried out for one native vessel and two different types of bypass grafts. One bypass graft was a saphenous vein and the other one was a varicose saphenous vein encased in a fine, flexible metal mesh. The mesh was attached to give the graft a defined diameter. The computational results show that each graft has distinct areas of low wall shear stress. The graft with the metal mesh has an area of low wall shear stress (< 1 Pa, stationary flow), which is four times smaller than the respective areas in the other graft and in the native vessel. This is thought to be caused by the smaller and more uniform diameter of the metal mesh-reinforced graft.},
 author = {L. Goubergrits and K. Affeld and E. Wellnhofer and R. Zurbrügg and T. Holmer},
 doi = {10.1177/039139880102400306},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/039139880102400306},
 journal = {The International Journal of Artificial Organs},
 note = {PMID:11314808},
 number = {3},
 pages = {145–151},
 title = {Estimation of wall shear stress in bypass grafts with computational fluid dynamics method},
 url = {https://doi-org.crai.referencistas.com/10.1177/039139880102400306},
 volume = {24},
 year = {2001g}
}

@article{doi:10.1177/039139880102400826,
 doi = {10.1177/039139880102400826},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/039139880102400826},
 journal = {The International Journal of Artificial Organs},
 number = {8},
 pages = {550–551},
 title = {Computational Fluid Dynamics and Artificial Organs},
 url = {https://doi-org.crai.referencistas.com/10.1177/039139880102400826},
 volume = {24},
 year = {2001t}
}

@article{doi:10.1177/039139880502800713,
 abstract = {For a better insight in dialyzer efficiency with respect to local mass transport in a low flux dialyzer (Fresenius F6HPS), blood and dialysate flow distributions were visualized with computational fluid dynamic (CFD) simulations, which were validated with single photon emission computed tomography (SPECT) imaging. To visualize blood-side flow while avoiding transport through the fiber membrane, a bolus of 99m-Technetium labeled MAA (Macro Aggregated Albumin) was injected in the flow using an electronic valve. Water was used to simulate blood, but flow rate was adjusted according to laws of dynamic similarity to account for the viscosity difference (factor 2.75). For the visualization of dialysate flow, a bolus of 99m-Technetium labeled DMSA (Dimercaptosuccinic Acid) was injected, while pressurized air in the blood compartment avoided transmembrane flow. For each test series, 3D acquisitions were made on a two respectively three-headed SPECT camera. By evaluating the images at different time steps, dynamic 3D intensity plots were obtained, which were further used to derive local flow velocities. Additionally, three-dimensional CFD models were developed for simulating the overall blood and dialysate flow, respectively. In both models, the whole fiber compartment was defined as a porous medium with overall axial and radial permeability derived theoretically and from in vitro tests. With the imaging as well as with the computational technique, a homogeneous blood flow distribution was found, while vortices and fluid stagnation were observed in the dialyzer inlet manifold. The non-homogeneous dialysate distribution, as found with SPECT imaging, implies the occurrence of non-efficient sites with respect to mass transfer. The discrepancy between the dialysate results of both techniques indicated that the assumption of a constant fiber bundle permeability in the CFD model was too optimistic. In conclusion, medical imaging techniques like SPECT are very helpful to validate CFD models, which can be further applied for dialyzer design and optimization.},
 author = {S. Eloot and Y. D’asseler and P. De Bondt and P. Verdonck},
 doi = {10.1177/039139880502800713},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/039139880502800713},
 journal = {The International Journal of Artificial Organs},
 note = {PMID:16049908},
 number = {7},
 pages = {739–749},
 title = {Combining SPECT Medical Imaging and Computational Fluid Dynamics for Analyzing Blood and Dialysate Flow in Hemodialyzers},
 url = {https://doi-org.crai.referencistas.com/10.1177/039139880502800713},
 volume = {28},
 year = {2005h}
}

@article{doi:10.1177/039139880602900211,
 abstract = {Extracorporeal endotoxin removal by means of the Toraymyxin device is based on the ability of polymyxin B to bind endotoxins with a high specificity. The endotoxins/polymyxin molecular interactions were computationally analyzed in a parallel work (Part I). In this paper we investigate with a multi-scale approach the phenomena involving blood and plasma fluid dynamics inside the device. The macro- and mesoscale phenomena were studied by means of 3D models using computational fluid dynamics. The flow behavior in the sorbent material was focused, modeling the sorbent as a homogeneous porous medium at the macroscale level, or accounting for the realistic geometry of its knitted fibers at the mesoscale level. A microscale model was then developed to analyze the behavior of endotoxin molecules subjected to the competition of flow drag and molecular attraction by fibergrafted polymyxin B. The macroscale results showed that a very regular flow field develops in the sorbent, furthermore supplying the peak velocity to be input in the lower-scale model. The mesoscale analysis yielded the realistic range for wall shear stresses (WSSs) acting on fiber walls. With WSS values in the entire range, the results of the microscale analysis demonstrated that the capability of polymyxin B to capture endotoxin molecules from the flow extends at distances one order of magnitude greater than the characteristic distance of the stable intermolecular bond. We conclude that the use of an integrated, multi-scale analysis allows for a comprehensive understanding of the complex mechanisms involved in endotoxin sorption phenomena with immobilized polymyxin B.},
 author = {G.B. Fiore and M. Soncini and S. Vesentini and A. Penati and G. Visconti and A. Redaelli},
 doi = {10.1177/039139880602900211},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/039139880602900211},
 journal = {The International Journal of Artificial Organs},
 note = {PMID:16552672},
 number = {2},
 pages = {251–260},
 title = {Multi-Scale Analysis of the Toraymyxin Adsorption Cartridge Part II: Computational Fluid-Dynamic Study},
 url = {https://doi-org.crai.referencistas.com/10.1177/039139880602900211},
 volume = {29},
 year = {2006h}
}

@article{doi:10.1177/039139880703000413,
 abstract = {Background Today Computational Fluid Dynamics (CFD) is used for simulating flow in many applications. The quality of the results, however, depends on various factors, like grid quality, boundary conditions and the computational model of the fluid. For this reason, it is important to validate the performed computation with experimental results. In this work, a comparison of numerical simulation with the oil film method was performed for two cardiovascular applications. Methods The investigations were conducted at various geometries, such as a bended cannula tubing, an impeller of a magnetically levitated rotary blood pump and tips of inflow cannulas. The oil film for the experimental validation was composed of black oil color and varnish. In the numerical simulation, color abrasion was displayed with a special post-processing tool by means of wall-attached pathlines. Results With the proper choice of numerical parameters, the computer simulations and the oil film method demonstrated good correlation. Improper generation of the simulation grid did lead to divergent results between the numerical simulation and the experiment. For the pump impeller as well as for the inflow cannulas, the calculation and the experiment showed similar flow patterns with backflow and stall zones. Conclusion The oil film method represents a fast and simple approach to help validate numerical simulations of fluid flow. The experimentally generated near wall flow patterns can be easily compared with the solution of the CFD analysis.},
 author = {M. Stoiber and C. Grasl and S. Pirker and L. Huber and P. Gittler and H. Schima},
 doi = {10.1177/039139880703000413},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/039139880703000413},
 journal = {The International Journal of Artificial Organs},
 note = {PMID:17520575},
 number = {4},
 pages = {363–368},
 title = {Experimental Validation of Numerical Simulations: A Comparison of Computational Fluid Dynamics and the Oil Film Method},
 url = {https://doi-org.crai.referencistas.com/10.1177/039139880703000413},
 volume = {30},
 year = {2007q}
}

@article{doi:10.1177/0391398818762359,
 abstract = {Purpose: Rotary blood pumps are a promising treatment approach for patients with a total cavopulmonary connection and a failing cardiovascular system. The aim of this study was to investigate the hemodynamic effects of cavopulmonary support using a numerical model with closed-loop baroreflex and exercise mechanisms. Methods: A numerical model of the univentricular cardiovascular system was developed, mimicking the hemodynamics during rest and exercise. Rotary blood pumps with different hydraulic pump characteristics (flat vs steep pressure-flow relationships) were investigated in the cavopulmonary position. Furthermore, two support modes—a constant speed setting and a physiologically controlled speed—were examined. Results: Hemodynamics without rotary blood pumps were achieved with less than 10% deviation from reported values during rest and exercise. Rotary blood pumps at constant speed improve the hemodynamics at rest, however, they constitute a hydraulic resistance during light (steep characteristics) or moderate (flat characteristics) exercise. In contrast, physiologic control increases cardiac output (moderate exercise: 8.2 vs 7.4 L/min) and reduces sympathetic activation (heart rate at moderate exercise: 111 vs 123 bpm). Conclusion: In this simulation study, the necessity of an automatically controlled rotary blood pump in the cavopulmonary position was shown. A pump at constant speed might constitute an additional resistance to venous return during physical activity. Therefore, a physiologic control algorithm based on the pressure difference between the caval veins and the atrial pressure is proposed to improve hemodynamics, especially during physical activity.},
 author = {Marcus Granegger and Martin Schweiger and Marianne Schmid Daners and Mirko Meboldt and Michael Hübler},
 doi = {10.1177/0391398818762359},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0391398818762359},
 journal = {The International Journal of Artificial Organs},
 note = {PMID:29521133},
 number = {5},
 pages = {261–268},
 title = {Cavopulmonary mechanical circulatory support in Fontan patients and the need for physiologic control: A computational study with a closed-loop exercise model},
 url = {https://doi-org.crai.referencistas.com/10.1177/0391398818762359},
 volume = {41},
 year = {2018h}
}

@article{doi:10.1177/0391398818777697,
 abstract = {Purpose: Numerical flow analysis (computational fluid dynamics) in combination with the prediction of blood damage is an important procedure to investigate the hemocompatibility of a blood pump, since blood trauma due to shear stresses remains a problem in these devices. Today, the numerical damage prediction is conducted using unsteady Reynolds-averaged Navier–Stokes simulations. Investigations with large eddy simulations are rarely being performed for blood pumps. Hence, the aim of the study is to examine the viscous shear stresses of a large eddy simulation in a blood pump and compare the results with an unsteady Reynolds-averaged Navier–Stokes simulation. Methods: The simulations were carried out at two operation points of a blood pump. The flow was simulated on a 100M element mesh for the large eddy simulation and a 20M element mesh for the unsteady Reynolds-averaged Navier-Stokes simulation. As a first step, the large eddy simulation was verified by analyzing internal dissipative losses within the pump. Then, the pump characteristics and mean and turbulent viscous shear stresses were compared between the two simulation methods. Results: The verification showed that the large eddy simulation is able to reproduce the significant portion of dissipative losses, which is a global indication that the equivalent viscous shear stresses are adequately resolved. The comparison with the unsteady Reynolds-averaged Navier–Stokes simulation revealed that the hydraulic parameters were in agreement, but differences for the shear stresses were found. Conclusion: The results show the potential of the large eddy simulation as a high-quality comparative case to check the suitability of a chosen Reynolds-averaged Navier–Stokes setup and turbulence model. Furthermore, the results lead to suggest that large eddy simulations are superior to unsteady Reynolds-averaged Navier–Stokes simulations when instantaneous stresses are applied for the blood damage prediction.},
 author = {Benjamin Torner and Lucas Konnigk and Sebastian Hallier and Jitendra Kumar and Matthias Witte and Frank-Hendrik Wurm},
 doi = {10.1177/0391398818777697},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0391398818777697},
 journal = {The International Journal of Artificial Organs},
 note = {PMID:29898615},
 number = {11},
 pages = {752–763},
 title = {Large eddy simulation in a rotary blood pump: Viscous shear stress computation and comparison with unsteady Reynolds-averaged Navier–Stokes simulation},
 url = {https://doi-org.crai.referencistas.com/10.1177/0391398818777697},
 volume = {41},
 year = {2018q}
}

@article{doi:10.1177/0391398818790343,
 abstract = {Anatomic pathologies such as stenosed or regurgitating heart valves and artificial organs such as heart assist devices or heart valve prostheses are associated with non-physiological flow. This regime is associated with regions of spatially high-velocity gradients, high-velocity and/or pressure fluctuations as well as neighbouring regions with stagnant flow associated with high residence time. These hemodynamic conditions cause destruction and/or activation of blood components and their accumulation in regions with high residence time. The development of next-generation artificial organs, which allow long-term patient care by reducing adverse events and improve quality of life, requires the development of blood damage models serving as a cost function for device optimization. We summarized the studies underlining the key findings with subsequent elaboration of the requirements for blood damage models as well as a decision tree based on the classification of existing blood damage models. The four major classes are Lagrangian or Eulerian approaches with stress- or strain-based blood damage. Key challenges were identified and future steps towards the translation of blood damage models into the device development pipeline were formulated. The integration of blood damage caused by turbulence into models as well as in vitro and in vivo validation of models remain the major challenges for future developments. Both require the development of novel experimental setups to provide reliable and well-documented experimental data.},
 author = {Leonid Goubergrits and Ulrich Kertzscher and Michael Lommel},
 doi = {10.1177/0391398818790343},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0391398818790343},
 journal = {The International Journal of Artificial Organs},
 note = {PMID:30073891},
 number = {3},
 pages = {125–132},
 title = {Past and future of blood damage modelling in a view of translational research},
 url = {https://doi-org.crai.referencistas.com/10.1177/0391398818790343},
 volume = {42},
 year = {2019d}
}

@article{doi:10.1177/0391398818792757,
 abstract = {Despite the evolution of ventricular assist devices, ventricular assist device patients still suffer from complications due to the damage to blood by fluid dynamic stress. Since rotary ventricular assist devices are assumed to exert mainly shear stress, studies of blood damage are based on shear flow experiments. However, measurements and simulations of cell and protein deformation show normal and shear stresses deform, and potentially damage, cells and proteins differently. The aim was to use computational fluid dynamics to assess the prevalence of normal stress, in comparison with shear stress, in rotary ventricular assist devices. Our calculations showed normal stresses do occur in rotary ventricular assist devices: the fluid volumes experiencing normal stress above 10 Pa were 0.011 mL (0.092%) and 0.027 mL (0.39%) for the HeartWare HVAD and HeartMate II (HMII), and normal stresses over 100 Pa were present. However, the shear stress volumes were up to two orders of magnitude larger than the normal stress volumes. Considering thresholds for red blood cell and von Willebrand factor deformation by normal and shear stresses, the fluid volumes causing deformation by normal stress were between 2.5 and 5 times the size of those causing deformation by shear stress. The exposure times to the individual normal stress deformation regions were around 1 ms. The results clearly show, for the first time, that while blood within rotary ventricular assist devices experiences more shear stress at much higher magnitudes as compared with normal stress, there is sufficient normal stress exposure present to cause deformation of, and potentially damage to, the blood components. This study is the first to quantify the fluid stress components in real blood contacting devices.},
 author = {Dominica PY Khoo and Andrew N Cookson and Harinderjit S Gill and Katharine H Fraser},
 doi = {10.1177/0391398818792757},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0391398818792757},
 journal = {The International Journal of Artificial Organs},
 note = {PMID:30141359},
 number = {11},
 pages = {738–751},
 title = {Normal fluid stresses are prevalent in rotary ventricular assist devices: A computational fluid dynamics analysis},
 url = {https://doi-org.crai.referencistas.com/10.1177/0391398818792757},
 volume = {41},
 year = {2018h}
}

@article{doi:10.1177/0391398820917145,
 abstract = {To fully study the relationship between the internal flow field and hemolysis index in an axial flow blood pump, a computational fluid dynamics–discrete element method coupled calculation method was used. Through numerical analysis under conditions of 6000, 8000, and 10,000 r/min, it was found that there was flow separation of blood cell particles in the tip of the impeller and the guide vane behind the impeller. The flow field has a larger pressure gradient distribution, which reduces the lift ratio of the blood pump and easily causes blood cell damage. The study shows that the hemolysis index obtained by the computational fluid dynamics—discrete element method is 4.75% higher than that from the traditional computational fluid dynamics method, which indicates the impact of microcollision between erythrocyte particles and walls on hemolysis index and also further verifies the validity of the computational fluid dynamics–discrete element coupling method. Through the hydraulic and particle image velocimetry experiments of the blood pump, the coincidence between numerical calculation and experiment is analyzed from macro and micro aspects, which shows that the numerical calculation method is feasible.},
 author = {Lizhi Cheng and Jianping Tan and Zhong Yun and Shuai Wang and Zheqin Yu},
 doi = {10.1177/0391398820917145},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0391398820917145},
 journal = {The International Journal of Artificial Organs},
 note = {PMID:32393086},
 number = {1},
 pages = {46–54},
 title = {Analysis of flow field and hemolysis index in axial flow blood pump by computational fluid dynamics–discrete element method},
 url = {https://doi-org.crai.referencistas.com/10.1177/0391398820917145},
 volume = {44},
 year = {2021c}
}

@article{doi:10.1177/03913988211045405,
 abstract = {Heart failure is a major health risk, and with limited availability of donor organs, there is an increasing need for developing cardiac assist devices (CADs). Mock circulatory loops (MCL) are an important in-vitro test platform for CAD’s performance assessment and optimisation. The MCL is a lumped parameter model constructed out of hydraulic and mechanical components aiming to simulate the native cardiovascular system (CVS) as closely as possible. Further development merged MCLs and numerical circulatory models to improve flexibility and accuracy of the system; commonly known as hybrid MCLs. A total of 128 MCLs were identified in a literature research until 25 September 2020. It was found that the complexity of the MCLs rose over the years, recent MCLs are not only capable of mimicking the healthy and pathological conditions, but also implemented cerebral, renal and coronary circulations and autoregulatory responses. Moreover, the development of anatomical models made flow visualisation studies possible. Mechanical MCLs showed excellent controllability and repeatability, however, often the CVS was overly simplified or lacked autoregulatory responses. In numerical MCLs the CVS is represented with a higher order of lumped parameters compared to mechanical test rigs, however, complex physiological aspects are often simplified. In hybrid MCLs complex physiological aspects are implemented in the hydraulic part of the system, whilst the numerical model represents parts of the CVS that are too difficult to represent by mechanical components per se. This review aims to describe the advances, limitations and future directions of the three types of MCLs.},
 author = {Femke Cappon and Tingting Wu and Theodore Papaioannou and Xinli Du and Po-Lin Hsu and Ashraf W Khir},
 doi = {10.1177/03913988211045405},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/03913988211045405},
 journal = {The International Journal of Artificial Organs},
 note = {PMID:34581613},
 number = {11},
 pages = {793–806},
 title = {Mock circulatory loops used for testing cardiac assist devices: A review of computational and experimental models},
 url = {https://doi-org.crai.referencistas.com/10.1177/03913988211045405},
 volume = {44},
 year = {2021c}
}

@article{doi:10.1177/03913988221084342,
 abstract = {Design modification is a main step in developing machines and artificial organ. A new dialysis apparatus air chamber detects clot formation and interruption of blood refining circuit. Due to using enough anticoagulant, thrombosis may occur because of vortex formed by turbulent flow. Turbulent blood flow causes to endothelial injury thus supporting the formation of a thrombus. Computational fluid dynamics can estimate the flow velocity and turbulence distribution and it can be used as applicable tool to design diagnosis and modifying. In this paper CFD simulation used to find the high turbulent intensity region within the chamber and an optimization method is adopted based on the geometry changing and trying the simulation results. The turbulent intensity is chosen as a criterion to achieving to an optimized condition. Finally, a best geometry is derived for the chamber entrance by this process and modified prototype is manufactured. This refined chamber maintains on the dialysis machine and tested for several patients with different blood characteristics. The results show that no more clot formation has been observed in this new designed chamber.},
 author = {Amir Torabi and Mohammad Amin Shahrokhian Dehkordi},
 doi = {10.1177/03913988221084342},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/03913988221084342},
 journal = {The International Journal of Artificial Organs},
 note = {PMID:35356821},
 number = {5},
 pages = {488–496},
 title = {Using computational fluid dynamic for hemodialysis air chamber design modification},
 url = {https://doi-org.crai.referencistas.com/10.1177/03913988221084342},
 volume = {45},
 year = {2022m}
}

@article{doi:10.1177/05390184231170567,
 abstract = {This article synthesizes a large body of research in the social and cognitive sciences to develop a distinctly cognitive understanding of political identity. Building on dual-process and computational theories of mind, the article defends three claims about the mental and behavioral implications of identity in political domains: (1) identity-based thinking is people’s default (often fast, automatic, and cognitively inaccessible) way of mentally representing politics and of drawing inferences based on those representations; (2) people are not limited to identity-based thinking and can sometimes learn to override it via slow, volitional, and conscious reasoning; and (3) the cognitive complexity of identity-based thinking is in-between the levels of mental sophistication that the Michigan and Rochester Schools in political science posit. This account of political identity illuminates, inter alia, why even low-information voters can quickly identify their political allies and opponents, why even high-information politicians can misperceive their constituents, why affective polarization has been increasing in the United States in recent decades, and why many normative theories of justice advise people to override identity-based thinking.},
 author = {Jonathan Bendor and Philip Petrov},
 doi = {10.1177/05390184231170567},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/05390184231170567},
 journal = {Social Science Information},
 number = {1},
 pages = {3–30},
 title = {Between Michigan and Rochester: Identity-based thinking is cognitively primary},
 url = {https://doi-org.crai.referencistas.com/10.1177/05390184231170567},
 volume = {62},
 year = {2023b}
}

@article{doi:10.1177/05390184241258370,
 abstract = {This study reflects on Harold Kincaid and Jule Zahle’s view that there is no necessary association between methodological individualism and agent-based models because the analysis of social phenomena in terms of the latter cannot always be regarded as an implementation of the former. Their view remains in contention with the standpoint of several philosophers of science and social scientists, including Chen and Di Iorio. Kincaid and Zahle’s main argument against such a standpoint is that agent-based simulation is compatible with holistic explanations that are at odds with methodological individualism. The following study argues that Kincaid and Zahle’s conclusion remains untenable since it stems from inaccurate historical assumptions concerning the tradition of methodological individualism and the way the individualism-holism debate is understood within this tradition.},
 author = {Francesco Di Iorio},
 doi = {10.1177/05390184241258370},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/05390184241258370},
 journal = {Social Science Information},
 number = {2},
 pages = {155–167},
 title = {Methodological individualism and agent-based computational simulation: A reply to Kincaid and Zahle},
 url = {https://doi-org.crai.referencistas.com/10.1177/05390184241258370},
 volume = {63},
 year = {2024f}
}

@article{doi:10.1177/07067437221094552,
 abstract = {Objective The COVID-19 pandemic has had a complex impact on risks of suicide and non-fatal self-harm worldwide with some evidence of increased risk in specific populations including women, young people, and people from ethnic minority backgrounds. This review aims to systematically address whether SARS-CoV-2 infection and/or COVID-19 disease confer elevated risk directly. Method As part of a larger Living Systematic Review examining self-harm and suicide during the pandemic, automated daily searches using a broad list of keywords were performed on a comprehensive set of databases with data from relevant articles published between January 1, 2020 and July 18, 2021. Eligibility criteria for our present review included studies investigating suicide and/or self-harm in people infected with SARS-CoV-2 with or without manifestations of COVID-19 disease with a comparator group who did not have infection or disease. Suicidal and self-harm thoughts and behaviour (STBs) were outcomes of interest. Studies were excluded if they reported data for people who only had potential infection/disease without a confirmed exposure, clinical/molecular diagnosis or self-report of a positive SARS-CoV-2 test result. Studies of news reports, treatment studies, and ecological studies examining rates of both SARS-CoV-2 infections and suicide/self-harm rates across a region were also excluded. Results We identified 12 studies examining STBs in nine distinct samples of people with SARS-CoV-2. These studies, which investigated STBs in the general population and in subpopulations, including healthcare workers, generally found positive associations between SARS-CoV-2 infection and/or COVID-19 disease and subsequent suicidal/self-harm thoughts and suicidal/self-harm behaviour. Conclusions This review identified some evidence that infection with SARS-CoV-2 and/or COVID-19 disease may be associated with increased risks for suicidal and self-harm thoughts and behaviours but a causal link cannot be inferred. Further research with longer follow-up periods is required to confirm these findings and to establish whether these associations are causal.},
 author = {Mark Sinyor and Rabia Zaheer and Roger T. Webb and Duleeka Knipe and Emily Eyles and Julian P.T. Higgins and Luke McGuinness and Lena Schmidt and Catherine Macleod-Hall and Dana Dekel et al.},
 doi = {10.1177/07067437221094552},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07067437221094552},
 journal = {The Canadian Journal of Psychiatry},
 note = {PMID:35532916},
 number = {11},
 pages = {813–828},
 title = {SARS-CoV-2 Infection and the Risk of Suicidal and Self-Harm Thoughts and Behaviour: A Systematic Review},
 url = {https://doi-org.crai.referencistas.com/10.1177/07067437221094552},
 volume = {67},
 year = {2022p}
}

@article{doi:10.1177/07255136231169061,
 abstract = {Drawing on struggles within academe between faculty that promote critical education and advocates of New Public Management (NPM) who endorse instrumental learning, I reimagine the university as a counter-space that positions it as a counter-power to informational capitalism. Initially, I outline its twin threats: ethical, as self-entrepreneurial academics are valorised by NPM; and political, with informationalisation conflating spaces of thinking. I then detail Scott Lash’s specific account of how the info-comm society negates critique. However, his monistic understanding of informationalisation means Lash’s alternative of Informationskritik risks subsumption by it. I therefore defer to Jacques Derrida’s idea of the university. To ensure the autonomy of the principle of reason in a world of info-comm flows, the university is a supplementary body to society, yet intimately linked to it by its critical reflexivity, which is on behalf of society. Because Derrida does not elaborate the requisite institutional architecture, I conclude with Michel Foucault’s notion of heterotopia as a quasi-illicit site that is different and other. Such an institutional design enables the university as a counter-space that is a bank of reason and an archive of its manifestation in social practices. It also upholds a space for thinking, which in the form of nominalist critical history proffers a counter-power to society as an informational homotopia.},
 author = {Bregham Dalgliesh},
 doi = {10.1177/07255136231169061},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07255136231169061},
 journal = {Thesis Eleven},
 number = {1},
 pages = {81–107},
 title = {The idea of the university as a heterotopia: The ethics and politics of thinking in the age of informational capitalism},
 url = {https://doi-org.crai.referencistas.com/10.1177/07255136231169061},
 volume = {175},
 year = {2023d}
}

@article{doi:10.1177/0731684404032864,
 abstract = {Experimental characterization of time-dependent properties for rectangular hollowcored, continuous fiber reinforced, commingled recycled plastic extruded forms under long-term (creep) flexural loading has been presented in this paper. Finite element based computer models have been developed to predict the effects of damage progression in such reinforced extruded plastic forms. In the long-term (creep) tests, reinforced and unreinforced extrusions with varying compositions were used as specimens. These extruded specimens were submerged in heated water and subjected to different loads. Experimental results indicate that fiber micro-buckling and fiber–matrix interface failure occur during the creep loading environment. The fiber micro-buckling occurs over time, compared with similar but dramatic damage that occurs in a short-period of time during short-term (static) loading, as discussed in Part I of this work. Experimental data also shows that these damage modes significantly reduce the long-term (life cycle) flexural properties, and the specimens with a coupling agent demonstrated much better performance. “Damage dependent” finite element models were developed using different material property types to represent the glass-fiber roving, fiber–matrix interface and plastic matrix respectively. Material nonlinearity of the plastic matrix has been incorporated along with stress-based failure criteria to account for fiber–matrix interfacial shear failure and local fiber micro-buckling. A user-defined material model has been incorporated into an industrial standard finite element software package to accommodate damage progression. The developed finite element based model(s) have correlated well with the long-term (creep) test results, and can provide a valuable tool in designing reinforced plastics for long-term loading conditions.},
 author = {Zhiyin Zheng and John J. Engblom},
 doi = {10.1177/0731684404032864},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0731684404032864},
 journal = {Journal of Reinforced Plastics and Composites},
 number = {8},
 pages = {799–810},
 title = {Computational and Experimental Characterization of Continuously Fiber                Reinforced Plastic Extrusions: Part II – Long-Term Flexural Loading},
 url = {https://doi-org.crai.referencistas.com/10.1177/0731684404032864},
 volume = {23},
 year = {2004t}
}

@article{doi:10.1177/0731684414565224,
 abstract = {In wind turbines, blades are critical design members because performance depends on blade material, shape, twist angle, etc. The problem of internal, mechanical design and material selection for a prototypical high-power horizontal axis wind turbine blade under static, flap-wise loading is investigated in this study. At first a materials selection methodology has been proposed. A very detailed computational analysis based on finite element modes is developed representing the load-carrying box girder of the blade with a given airfoil shape, size, and the type and position of the interior load-bearing longitudinal beams–shear webs. Results concerning displacements and stresses are generated using both plane and shell elements with linear and nonlinear analyses.},
 author = {Efstathios E Theotokoglou and Georgios A Balokas},
 doi = {10.1177/0731684414565224},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0731684414565224},
 journal = {Journal of Reinforced Plastics and Composites},
 number = {2},
 pages = {101–115},
 title = {Computational analysis and material selection in cross-section of a composite wind turbine blade},
 url = {https://doi-org.crai.referencistas.com/10.1177/0731684414565224},
 volume = {34},
 year = {2015s}
}

@article{doi:10.1177/0731684415573981,
 abstract = {Partly presented in Interdisciplinary Transport Phenomena VII, Dresden, Germany, 19–23 September 2011.Compression moulding experiments of sheet moulding compound, visual observations of a vacuum test with prepregs and numerical models with two main approaches for computational fluid dynamics simulations of the mould filling phase are presented. One assumes that there are layers near the mould surfaces with much less viscosity and the other only use one viscosity model. The numerical experiments showed that the pressure could be accurately predicted with both approaches. The property necessary to predict correct pressure with altered mould closing velocities was that the bulk material had to obey shear-thinning effects. Preheating effects before compression were neglected, but altering the heating time until the prepreg was assumed to start flow had a significant effect. The experiments confirmed that the pressure is predominantly affected by the mould closing velocity. Regardless of the considered process settings, a first pressure top always appeared approximately at the logarithmic strain 0.25. A second top was associated with a slowdown of the press. The location of this was affected by the velocity and the vacuum, the latter indicating that vacuum assistance prevents a build-up of back pressure. Furthermore, heated prepreg above a critical temperature is observed to swell immediately as vacuum assistance is applied.},
 author = {NE Jimmy Kluge and T Staffan Lundström and Lars G Westerberg and Kurt Olofsson},
 doi = {10.1177/0731684415573981},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0731684415573981},
 journal = {Journal of Reinforced Plastics and Composites},
 number = {6},
 pages = {479–492},
 title = {Compression moulding of sheet moulding compound: Modelling with computational fluid dynamics and validation},
 url = {https://doi-org.crai.referencistas.com/10.1177/0731684415573981},
 volume = {34},
 year = {2015o}
}

@article{doi:10.1177/07316844231197258,
 abstract = {This study investigated the effects of intercalation and exfoliation of the graphite flake particles on the mechanical properties of elastomeric composites using analytical, computational, and experimental techniques. Relying on the modified Eshelby and incremental Mori–Tanaka approach, a new micromechanics model was proposed to estimate Young’s modulus and develop a constitutive model for expanded graphite (EG)-filled elastomer in finite strain deformation. It was found that when the initial graphite flakes expanded, the actual volume fraction and ultimate aspect ratio of the dispersed particles significantly affected the mechanical properties of the EG-reinforced composites. The present model and subsequent predictions reflect the initial flake diameter, actual volume fraction, ultimate aspect ratio of the particles, and other convenient physical properties of the composite constituents. In addition, molecular dynamics simulations were performed to evaluate the elastic properties of the EG-Matrix interphase region using the COMPASS force field function. The MD results showed that Young’s modulus of the interphase region increased significantly as the quantity of the graphene layer increased. The micromechanics-constitutive model results were validated using the experimentally determined stress–strain relation of the composites. EG- and CB-elastomer nanocomposite specimens were fabricated and tested in the experimental program. Field emission scanning electron microscopy (FESEM) analysis of the cryo-fractured surfaces of the elastomer compounds showed good interactions between the constituents. Numerical computations indicated that the proposed model incorporating the new parameters could accurately predict the experiment at a small particle concentration.},
 author = {Mohammad Hosein Karimi Dona and Fathollah Taheri-behrooz and Bijan Mohammadi},
 doi = {10.1177/07316844231197258},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07316844231197258},
 journal = {Journal of Reinforced Plastics and Composites},
 number = {15–16},
 pages = {866–878},
 title = {Effect of intercalated and exfoliated graphite particles on the mechanical properties of polymer composite: A micromechanics-computational approach},
 url = {https://doi-org.crai.referencistas.com/10.1177/07316844231197258},
 volume = {43},
 year = {2024j}
}

@article{doi:10.1177/0731948719865499,
 abstract = {The purpose of the study was to compare the deficit profiles of two important types of mathematics difficulties. Three cognitive measures (working memory, processing speed, and reasoning), two mathematics measures (numerical facts retrieval and mathematics vocabulary), and reading comprehension were assessed among 237 Chinese fourth-grade students, among whom 28 were classified as students with only computational difficulties (CD), 34 were classified as having only word problem-solving difficulties (WPD), 20 were classified as students with computational and word problem-solving difficulties (CD + WPD), and 43 typically developing (TD) peers. Multivariate analysis showed that, compared with TD, CD was associated with weakness in numerical working memory; WPD was associated with weakness in reading comprehension; both CD and WPD were associated with weakness in mathematics vocabulary. However, CD and WPD did not differ from each other on any of those profiling measures. Implications for understanding mathematics competence and identification of mathematics difficulties are discussed.},
 author = {Xin Lin and Peng Peng and Hongjing Luo},
 doi = {10.1177/0731948719865499},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0731948719865499},
 journal = {Learning Disability Quarterly},
 number = {2},
 pages = {110–122},
 title = {The Deficit Profile of Elementary Students With Computational Difficulties Versus Word Problem-Solving Difficulties},
 url = {https://doi-org.crai.referencistas.com/10.1177/0731948719865499},
 volume = {44},
 year = {2021l}
}

@article{doi:10.1177/0731948720912417,
 abstract = {Science, Technology, Engineering, and Mathematics (STEM) education initiatives have placed pressure on teachers to bring technology tools into classroom, including three-dimensional (3D) printing. Yet, little research has examined what specific math skills are required for 3D printing technology. This article describes a follow-up analysis of findings from a quasi-experimental study that tested feasibility of 3D geometry instruction, Anchored Instruction with Technology Applications (AITA), designed to help students visualize and construct 3D models based on Enhanced Anchored Instruction. Although we found that AITA improved math outcomes of students with math learning disabilities (MLD) in the previous analysis, we only used composite scores encompassing a variety of math and spatial tasks. In this study, we employed item response theory and differential item functioning to examine the impacts of MLD on students’ spatial thinking skills, understand the types of items to assess the intended skills in a valid way, and provide a detailed information of whether student ability and MLD status have caused different results to assess students’ spatial thinking skills. Results showed that students with MLD struggle to learn spatial thinking skills, and AITA was a significant positive indicator to improve spatial thinking skills for both students with and without MLD.},
 author = {Sam Choo and Sunhi Park and Nancy J. Nelson},
 doi = {10.1177/0731948720912417},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0731948720912417},
 journal = {Learning Disability Quarterly},
 number = {2},
 pages = {68–81},
 title = {Evaluating Spatial Thinking Ability Using Item Response Theory: Differential Item Functioning Across Math Learning Disabilities and Geometry Instructions},
 url = {https://doi-org.crai.referencistas.com/10.1177/0731948720912417},
 volume = {44},
 year = {2021a}
}

@article{doi:10.1177/0734282918809793,
 abstract = {The goal of this study was to provide normative scores and examine the psychometric properties of the Math Computation subtest of the Wide Range Achievement Test–IV (WRAT-IV) for Mexican adolescents after the completion of junior high school. We group-administered this subtest to 1,318 first-year Mexican high school students. We then obtained its overall internal reliability and examined its underlying factor structure. Finally, we determined its concurrent and criterion validity by evaluating a subsample of 106 students that included adolescents with mathematical difficulty, mathematical talent, and typical performance. Results showed that the subtest has a good internal reliability and appropriate psychometric characteristics, suggesting its appropriateness for the detection of adolescents with particular difficulty or ability in mathematics. The exploratory factor analysis identified three factors: arithmetic, fractions and basic algebra, and rational numbers. There were also sex differences in the number of correct responses, but the effect size was small.},
 author = {Roberto A. Abreu-Mendoza and Yaira Chamorro and Esmeralda Matute},
 doi = {10.1177/0734282918809793},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0734282918809793},
 journal = {Journal of Psychoeducational Assessment},
 number = {8},
 pages = {957–972},
 title = {Psychometric Properties of the WRAT Math Computation Subtest in Mexican Adolescents},
 url = {https://doi-org.crai.referencistas.com/10.1177/0734282918809793},
 volume = {37},
 year = {2019a}
}

@article{doi:10.1177/0734904118794844,
 abstract = {This article presents computational fluid dynamics results of the impact of a water spray on the fire smoke layer inside a hood. The models and the settings of parameters are discussed. Three experiments are performed by means of computational fluid dynamics simulations, and the comparisons show good agreement between measured data and predicted results. The simulation results provide insight into the temperature and flow fields for the configuration at hand, revealing an entrainment effect. The influence of the water spray characteristics on the downward smoke displacement due to drag and cooling is explained. Furthermore, an extensive sensitivity study of the simulation results to input parameters and mesh size is performed. The inner spray angle (related to vertical water flux) and droplet size are shown to be key parameters when simulating downward smoke displacement caused by a spray.},
 author = {Zhi Tang and Zheng Fang and Jiayun Sun and Tarek Beji and Bart Merci},
 doi = {10.1177/0734904118794844},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0734904118794844},
 journal = {Journal of Fire Sciences},
 number = {5},
 pages = {380–405},
 title = {Computational fluid dynamics simulations of the impact of a water spray on a fire-induced smoke layer inside a hood},
 url = {https://doi-org.crai.referencistas.com/10.1177/0734904118794844},
 volume = {36},
 year = {2018p}
}

@article{doi:10.1177/0734904120905579,
 abstract = {With the aim of minimising the losses produced by fire accidents, fire engineering applies physics and engineering principles to preserve the integrity of people, environment and infrastructure. Fire modelling is complex due to the interaction between chemistry, heat transfer and fluid dynamics. Commercially available simulation tools necessarily simplify this complexity, excluding less fundamental processes, such as soot production. By not including this compound in the simulations, the interactions of radiation heat transfer, fire propagation and toxicity must be approximated based on input parameters that are often not well defined. In this work, two semi-empirical soot models are incorporated in the fire dynamics simulator. The models are compared against experimental data. For the operational viability in large-scale scenarios, a correction factor for the local variables is proposed as a function of the cell size, achieving good agreement with experimental data in terms of the amount of soot generated.},
 author = {Oscar Mariño and Felipe Muñoz and Wolfram Jahn},
 doi = {10.1177/0734904120905579},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0734904120905579},
 journal = {Journal of Fire Sciences},
 number = {3},
 pages = {284–308},
 title = {Soot production modelling for operational computational fluid dynamics fire simulations},
 url = {https://doi-org.crai.referencistas.com/10.1177/0734904120905579},
 volume = {38},
 year = {2020g}
}

@article{doi:10.1177/0734904121993449,
 abstract = {Synthesising data from fire scenarios using fire simulations requires iterative running of these simulations. For real-time synthesising, faster-than-real-time simulations are thus necessary. In this article, different model types are assessed according to their complexity to determine the trade-off between the accuracy of the output and the required computing time. A threshold grid size for real-time computational fluid dynamic simulations is identified, and the implications of simplifying existing field fire models by turning off sub-models are assessed. In addition, a temperature correction for two zone models based on the conservation of energy of the hot layer is introduced, to account for spatial variations of temperature in the near field of the fire. The main conclusions are that real-time fire simulations with spatial resolution are possible and that it is not necessary to solve all fine-scale physics to reproduce temperature measurements accurately. There remains, however, a gap in performance between computational fluid dynamic models and zone models that must be explored to achieve faster-than-real-time fire simulations.},
 author = {Wolfram Jahn and Frane Sazunic and Carlos Sing-Long},
 doi = {10.1177/0734904121993449},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0734904121993449},
 journal = {Journal of Fire Sciences},
 number = {3},
 pages = {224–239},
 title = {Towards real-time fire data synthesis using numerical simulations},
 url = {https://doi-org.crai.referencistas.com/10.1177/0734904121993449},
 volume = {39},
 year = {2021h}
}

@article{doi:10.1177/073490418600400601,
 abstract = {A methodology is presented for computing the spread of fire effects such as heat, smoke, and other combustion products between rooms in a multi-room fire scenario. The methodology extends the Harvard Computer Fire Code V for computing the dynamic history of a fire in a single room to connecting rooms by adding the appropriate flow physics. Finally, an example is presented where an object is burning in a room which is connected to a second room via a door way. The evolution of the hot layer and vent flows are discussed, and compared to the same evolution of a single room structure.},
 author = {Charles E. Anderson and Donna K. O’Kelley and Arthur F. Grand},
 doi = {10.1177/073490418600400601},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/073490418600400601},
 journal = {Journal of Fire Sciences},
 number = {6},
 pages = {365–396},
 title = {Spread of Fire Effects Between Rooms: A Computational Model},
 url = {https://doi-org.crai.referencistas.com/10.1177/073490418600400601},
 volume = {4},
 year = {1986b}
}

@article{doi:10.1177/073491490502900303,
 abstract = {This paper discusses the implications of the sciences of complexity for public policy and administration. It is argued that the sciences of complexity have implications for our thinking in mainly three areas. First, they revise our conceptions of systems, causal relations, and determinism and depict a picture of mostly indeterministic reality composed of open systems. Second, they offer an “endophysical” and phenomenological view of system - observer relations. Third, although they are heavily quantitative, they illustrate the importance of qualitative interpretations in quantitative analyses and thus bridge the chasm between quantitative and qualitative methodologies. The insights of the sciences of complexity can help us improve our understanding of the complexities of public policy and administrative processes.},
 author = {Göktuğ Morçöl},
 doi = {10.1177/073491490502900303},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/073491490502900303},
 journal = {Public Administration Quarterly},
 number = {3},
 pages = {297–320},
 title = {A New Systems Thinking: Implications of the Sciences of Complexity for Public Policy and Administration},
 url = {https://doi-org.crai.referencistas.com/10.1177/073491490502900303},
 volume = {29},
 year = {2005m}
}

@article{doi:10.1177/0735275114536387,
 abstract = {In a new theory of conflict escalation, Randall Collins engages critical issues of violent conflict and presents a compellingly plausible theoretical description based on his extensive empirical research. He also sets a new challenge for sociology: explaining the time dynamics of social interaction. However, despite heavy reliance on the quantitative concept of positive feedback loops in his theory, Collins presents no mathematical specification of the dynamic relationships among his variables. This article seeks to fill that gap by offering a computational model that can parsimoniously account for many features of Collins’s theory. My model uses perceptual control theory to create an agent-based computational model of the time dynamics of conflict. With greater conceptual clarity and more wide-ranging generalizability, my alternative model opens the door to further advances in theory development by revealing dynamic aspects of conflict escalation not found in Collins’s model.},
 author = {Kent McClelland},
 doi = {10.1177/0735275114536387},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735275114536387},
 journal = {Sociological Theory},
 number = {2},
 pages = {100–127},
 title = {Cycles of Conflict: A Computational Modeling Alternative to Collins’s Theory of Conflict Escalation},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735275114536387},
 volume = {32},
 year = {2014k}
}

@article{doi:10.1177/0735633115598492,
 abstract = {Teaching programming and creating games have attracted much attention over the years, mostly the attention of curriculum developers and teachers. This study designed and developed a video game-based intervention, then investigated the effects of this intervention on middle school students’ learning of probability concepts. In the study, the students learned and used Scratch as a game programming tool. Initially, they received hands-on learning activities on how to use Scratch, and then developed video games based on scenarios authored by the researchers. The study collected quantitative as well as qualitative data using two different measurement tools: probability achievement test and student project assessment rubric, respectively. The data revealed that students were able to learn and use Scratch and develop probability-related and probability-based algorithms that generate random results successfully. The effect of Scratch intervention on students’ learning of probability was statistically significant. Findings were discussed in relation to similar studies reported in the literature. Finally, the study raised a set of further research questions in the Conclusion section.},
 author = {Yavuz Akpinar and Ümit Aslan},
 doi = {10.1177/0735633115598492},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633115598492},
 journal = {Journal of Educational Computing Research},
 number = {2},
 pages = {228–259},
 title = {Supporting Children’s Learning of Probability Through Video Game Programming},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633115598492},
 volume = {53},
 year = {2015a}
}

@article{doi:10.1177/0735633115608444,
 abstract = {Computational thinking (CT) is a fundamental skill for students, and assessment is a critical factor in education. However, there is a lack of effective approaches to CT assessment. Therefore, we designed the Three-Dimensional Integrated Assessment (TDIA) framework in this article. The TDIA has two aims: one was to integrate three dimensions (directionality, openness, and process) into the design of effective assessment tasks; and the other was to assess comprehensively the three dimensions of CT including computational concepts, practices, and perspectives. Guided by the TDIA framework, we designed three pairs of tasks: closed forward tasks and closed reverse tasks, semiopen forward tasks and semiopen reverse tasks, and open tasks with a creative design report and open tasks without a creative design report. To further confirm each task’s applicability and its advantages and disadvantages, we conducted a test experiment at the end of the autumn semester in 2014 in a primary school for 3 weeks. The results indicated that (a) the reverse tasks were not more superior than the forward tasks; (b) the semiopen tasks and the open tasks were more effective than the closed tasks, and the semiopen tasks had higher difficulty and discrimination than the others; (c) the self-reports provided a helpful function for learning diagnosis and guidance; (d) the scores had no significant difference between the schoolboys and the schoolgirls in all six tasks; and (e) the six tasks’ difficulty and discrimination were all acceptable, and the semiopen tasks had higher difficulty and discrimination than the others. To effectively apply them, the following suggestions for teachers to design computational tasks are proposed: motivating students’ interest and enthusiasm, incorporating semifinished artifacts, involving learning diagnosis and guidance, and including multiple types of tasks in an assessment.},
 author = {Baichang Zhong and Qiyun Wang and Jie Chen and Yi Li},
 doi = {10.1177/0735633115608444},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633115608444},
 journal = {Journal of Educational Computing Research},
 number = {4},
 pages = {562–590},
 title = {An Exploration of Three-Dimensional Integrated Assessment for Computational Thinking},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633115608444},
 volume = {53},
 year = {2016y}
}

@article{doi:10.1177/0735633115612785,
 abstract = {While pedagogical and technological affordances of three-dimensional (3D) multiuser virtual worlds in various educational disciplines are largely well-known, a study about their effect on high school students’ engagement in introductory programming courses is still lacking. This case study presents students’ opinions about their participation in a 3D multiuser game-like environment, by harnessing Second Life in combination with the two-dimensional (2D) programming environment of Scratch4SL. Following a blended instructional format (face-to-face in a computer laboratory and supplementary online courses), 56 students utilizing Scratch4SL participated in this study, with a view to reduce the “steep learning curve” created during their first-time entrance into Second Life. This study identifies Papert’s theory of Constructionism as a potentially appropriate theoretical foundation for the development of an instructional framework, in order to assist students to coordinate and manage learning materials with other teammates, using their computational thinking skills in collaborative problem-based programming tasks. The study findings based on a mixed-method research (a close-ended questionnaire and an open-ended interview) indicated the effectiveness of this “constructionist-oriented” instructional process for students’ engagement to acquire or empower social, cognitive, higher-order, and computational thinking skills. Educational implications and recommendations for future research are also discussed.},
 author = {Nikolaos Pellas and Efstratios Peroutseas},
 doi = {10.1177/0735633115612785},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633115612785},
 journal = {Journal of Educational Computing Research},
 number = {1},
 pages = {108–143},
 title = {Gaming in Second Life via Scratch4SL: Engaging High School Students in Programming Courses},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633115612785},
 volume = {54},
 year = {2016q}
}

@article{doi:10.1177/0735633115625247,
 abstract = {Thriving in Our Digital World is a technology-enhanced dual enrollment course introducing high school students to computer science through project- and problem-based learning. This article describes the evolution of the course and five lessons learned during the design, development, implementation, and iteration of the course from its first through third year of implementation. The design principles that we describe have guided our design endeavors and may be helpful to instructional designers, learning technologists, and others who are engaged in the design and development of in situ interventions to improve the teaching and learning of computer science.},
 author = {George Veletsianos and Bradley Beth and Calvin Lin and Gregory Russell},
 doi = {10.1177/0735633115625247},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633115625247},
 journal = {Journal of Educational Computing Research},
 number = {4},
 pages = {443–461},
 title = {Design Principles for Thriving in Our Digital World: A High School Computer Science Course},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633115625247},
 volume = {54},
 year = {2016p}
}

@article{doi:10.1177/0735633116642774,
 abstract = {Computational thinking has been gaining new impetus in the academic community and in K-12 level education. Scratch is a visual programming environment that can be utilized to teach and learn introductory computing concepts. There are some studies investigating the effectiveness of Scratch for K-12 level education. However, studies that have been conducted at the collegiate level, especially in the context of preservice computing teacher education, are very rare. The present study aimed to investigate the effect of Scratch-based instruction on preservice teachers’ understanding of basic programming concepts and their attitudes toward programming. In the present study, a mixed method design was utilized. In the first phase of the study, the data were collected using an achievement test, a practice test, and a computer programming attitude scale. In the second phase of the study, data were collected through a semistructured interview. The results of the study indicated that preservice teachers in Scratch-based instruction had significantly better understanding of basic computing concepts. Qualitative data indicated that Scratch-based instruction was useful in constructing a more meaningful learning environment for preservice teachers. The results of this study have implications for researchers and preservice computing teacher educators when designing an introductory computing course.},
 author = {Ibrahim Cetin},
 doi = {10.1177/0735633116642774},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633116642774},
 journal = {Journal of Educational Computing Research},
 number = {7},
 pages = {997–1021},
 title = {Preservice Teachers’ Introduction to Computing: Exploring Utilization of Scratch},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633116642774},
 volume = {54},
 year = {2016f}
}

@article{doi:10.1177/0735633116656454,
 abstract = {Previous research has shown the effectiveness of peer reviewing on the improvement of writing quality. However, the fact that students themselves, arguably novices, judged the improvement leads to concerns about the validity of peer reviewing. We measured writing quality before and after peer reviewing using Coh-Metrix, which computationally evaluates the linguistic properties of writings. Participants also evaluated the quality of their peer writings using the SWoRD system. Both measurements, particularly the computational measurement, confirmed the effectiveness of peer reviewing. In addition, the computational measures found that awareness of cohesion, including the clarity, explicitness, and concreteness of writing, improved over the course of peer reviewing. The results are discussed, along with their possible implications for the complementary roles of peer reviewing and computational writing measurements.},
 author = {Juhwa Park and Kwangsu Cho},
 doi = {10.1177/0735633116656454},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633116656454},
 journal = {Journal of Educational Computing Research},
 number = {1},
 pages = {123–144},
 title = {Toward the Integration of Peer Reviewing and Computational Linguistics Approaches},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633116656454},
 volume = {55},
 year = {2017m}
}

@article{doi:10.1177/0735633117706109,
 abstract = {Although the learning of programming language is critical in science and technology education, it might be difficult for some students, especially novices. One possible reason might be the fact that programming language, especially for three-dimensional (3D) applications, is too complex and abstract for these students to understand. Programming for 3D applications requires understanding the spatial relationship of 3D objects and hence needs a visualization technique more. In view of this, this article presents an augmented reality (AR)-enhanced learning system that offers visual representation and interactivity to help students learn programming for 3D applications. To examine the influences of such an AR-enhanced system on student learning, a within-group experiment with 34 college students was conducted. All students used both of an AR-enhanced version and an ordinary version. The findings revealed that the AR-enhanced version made students have better learning efficiency than the ordinary system. In addition, the AR-enhanced system also made students have enhanced perceptions in terms of system usability, flow experience, and usage perception. Based on the results, further development of AR-enhanced learning systems is also suggested and discussed.},
 author = {Chin-Hung Teng and Jr-Yi Chen and Zhi-Hong Chen},
 doi = {10.1177/0735633117706109},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633117706109},
 journal = {Journal of Educational Computing Research},
 number = {2},
 pages = {254–271},
 title = {Impact of Augmented Reality on Programming Language Learning: Efficiency and Perception},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633117706109},
 volume = {56},
 year = {2018t}
}

@article{doi:10.1177/0735633117710860,
 author = {Robert H. Seidman},
 doi = {10.1177/0735633117710860},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633117710860},
 journal = {Journal of Educational Computing Research},
 number = {4},
 pages = {447–448},
 title = {Tribute to Seymour Papert (1928–2016)},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633117710860},
 volume = {55},
 year = {2017t}
}

@article{doi:10.1177/0735633117743918,
 abstract = {The Computer Science Teachers Association has asserted that computational thinking equips students with essential critical thinking which allows them to conceptualize, analyze, and solve more complex problems. These skills are applicable to all content area as students learn to use strategies, ideas, and technological practices more effectively as digital natives. This research examined over 200 elementary students’ pre- and posttest changes in computational thinking from a 10-week coding program using adapted lessons from code.org’s Blockly programming language and CSUnplugged that were delivered as part of the regular school day. Participants benefited from early access to computer science (CS) lessons with increases in computational thinking and applying coding concepts to the real world. Interviews from participants included examples of CS connections to everyday life and interdisciplinary studies at school. Thus, the study highlights the importance of leveraging CS access in diverse elementary classrooms to promote young students’ computational thinking; motivation in CS topics; and the learning of essential soft-skills such as collaboration, persistence, abstraction, and creativity to succeed in today’s digital world.},
 author = {Yune Tran},
 doi = {10.1177/0735633117743918},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633117743918},
 journal = {Journal of Educational Computing Research},
 number = {1},
 pages = {3–31},
 title = {Computational Thinking Equity in Elementary Classrooms: What Third-Grade Students Know and Can Do},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633117743918},
 volume = {57},
 year = {2019s}
}

@article{doi:10.1177/0735633117746747,
 abstract = {Computer programming has been gradually emphasized in recent computer literacy education and regarded as a requirement for all middle school students in some countries. To understand young students’ perceptions about their own learning in computer programming, this study aimed to develop an instrument, Computer Programming Self-Efficacy Scale (CPSES), for all students above middle school levels. Based on Berland and Lee’s computational thinking framework, this study developed the CPSES items at a literacy level and finally the instrument included the five subscales: Logical Thinking, Algorithm, Debug, Control, and Cooperation. An exploratory factor analysis and reliability tests were conducted in this study. The reliability alpha was .96 for the overall scale, and ranged from .84 to .96 for the subscales. This study also confirmed the positive correlation between computer programming experience and computer programming self-efficacy. In addition, for low- and middle-experienced learners, significant gender differences were found in two subscales: Algorithm and Debug. The CPSES can be applied as an evaluation tool in computer education, robotics education, as well as integrated STEM or STEAM education in which computer programming was regarded as a part of computer literacy.},
 author = {Meng-Jung Tsai and Ching-Yeh Wang and Po-Fen Hsu},
 doi = {10.1177/0735633117746747},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633117746747},
 journal = {Journal of Educational Computing Research},
 number = {8},
 pages = {1345–1360},
 title = {Developing the Computer Programming Self-Efficacy Scale for Computer Literacy Education},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633117746747},
 volume = {56},
 year = {2019w}
}

@article{doi:10.1177/0735633118783182,
 abstract = {Schools around the globe increasingly realized the importance of technology and its application in the education system. To guarantee a successful educational innovation, schools seek out different parties for valuable opinions. Among them, parents are the important feedback providers, because their attitudes are influential on children’s academic performance. Moreover, their involvement and support are considered the key factor that facilitates an effective implementation of programming education at schools. This study aimed at developing and validating an instrument measuring parents’ perceptions of programming education among P-12 schools in Hong Kong. We propose that parents’ perceptions of programming education is a multidimensional construct which constitutes (a) understanding, (b) support, and (c) expectation. In total, 524 questionnaires were collected from the parents who attended programming workshops and seminars. Exploratory factor analysis shows evidence for the three-dimensional construct. Confirmatory factor analysis reconfirms the measurement structure. Implications of the study are discussed.},
 author = {Siu-Cheung Kong and Robert Kwok-Yiu Li and Ron Chi-Wai Kwok},
 doi = {10.1177/0735633118783182},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633118783182},
 journal = {Journal of Educational Computing Research},
 number = {5},
 pages = {1260–1280},
 title = {Measuring Parents’ Perceptions of Programming Education in P-12 Schools: Scale Development and Validation},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633118783182},
 volume = {57},
 year = {2019m}
}

@article{doi:10.1177/0735633118806747,
 abstract = {The emergence of block-based environments aims to facilitate the problems caused by the abstractness of text-based languages. Recent studies generally focus on the effect of having block-based experience on programming education. This study is an attempt to observe the transfer of previous programming experiences (block-based vs. text-based) into a three-dimensional game-making environment through the use of backwards fading. In addition to observation of transfer, students’ perceptions about the difficulty of practices were also investigated. Twenty-one senior university students participated in the study. They practiced through worked example, completion example, and full practice. Moreover, the comparison of the contribution of three examples, their perceived difficulty, and cognitive load has also been observed. There are four main findings that add value to the current literature. First, students having text-based programming experience had higher scores, which may be a sign of far transfer; second, completion example format was more efficient for students having block-based programming experience; third, full practice format was perceived as more difficult than either worked example or completion example; and fourth, based on the efficiency of example formats, completion example represented high efficiency for all students. However, average efficiency of all example formats has represented high efficiency for students, who had text-based programming experience.},
 author = {Nomin Boldbaatar and Emine Şendurur},
 doi = {10.1177/0735633118806747},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633118806747},
 journal = {Journal of Educational Computing Research},
 number = {6},
 pages = {1468–1494},
 title = {Developing Educational 3D Games With StarLogo: The Role of Backwards Fading in the Transfer of Programming Experience},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633118806747},
 volume = {57},
 year = {2019b}
}

@article{doi:10.1177/0735633119827956,
 abstract = {Learning the basic concepts of programming and its foundations is considered as a challenging task for students to figure out. It is a challenging process for lecturers to learn these concepts, as well. The current literature on programming training abounds with the examples of a wide range of methods employed. Within this context, one of the prominent approaches in programming training is flipped classroom (FC) model. This article has sought to illuminate the effect of cognitive flexibility, problem-solving skills (PSS), and flipped learning readiness (FLR) levels on students’ programming achievements in programming training through FC model. A total of 149 freshmen computer science students studying in a state university in Turkey were recruited for this study. In this study, designed as a relational screening model, a personal form, an achievement test, and three different data collection instruments were employed to collect data. For the data analysis, structural equation modeling, a multivariate statistical analysis technique, was used to reveal a model explaining and predicting the relations between programming achievement and different variables. The findings clearly indicate that FLR is the most important predictor of the programming achievements of students in FC. Other important predictors were found as PSS and cognitive flexibility. The research model demonstrates that an increase or development in FLR, PSS, and cognitive flexibility levels in FC will enhance the achievements of students in programming.},
 author = {Hatice Yildiz Durak},
 doi = {10.1177/0735633119827956},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633119827956},
 journal = {Journal of Educational Computing Research},
 number = {1},
 pages = {160–199},
 title = {Modeling Different Variables in Learning Basic Concepts of Programming in Flipped Classrooms},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633119827956},
 volume = {58},
 year = {2020b}
}

@article{doi:10.1177/0735633119829191,
 abstract = {In Robotics Education (RE), the hands-on experience with troubleshooting problems is seen as a good catalyst to enhance the participants’ problem-solving skills. Based on the pedagogical technique of collaborative learning and pair programming, pair learning is an emerging and potential method in RE, which means that students collaborate in pairs to construct, build, and program a robot under scripted but switchable roles. As a special collaborative learning, can pair learning also improve students’ troubleshooting performance in RE? Therefore, we conducted a comparison experiment (pair learning versus individual learning) in two classes at a Robot Summer Camp for high school students. The results indicated that (a) in the process of learning making robot artifacts, students in pair learning group (PLG) had a higher success rate in troubleshooting than that of individual learning group (ILG), but failed in other indicators and (b) in the summative test, the troubleshooting performance of students in PLG was similar with that of ILG. The findings showed that, in terms of troubleshooting, collaborative (pair) learning does not always has the superiority over individual learning in RE, which implicates other factors to be explored in future research. Implications for teaching are also discussed in this exploratory study.},
 author = {Baichang Zhong and Tingting Li},
 doi = {10.1177/0735633119829191},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633119829191},
 journal = {Journal of Educational Computing Research},
 number = {1},
 pages = {220–248},
 title = {Can Pair Learning Improve Students’ Troubleshooting Performance in Robotics Education?},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633119829191},
 volume = {58},
 year = {2020r}
}

@article{doi:10.1177/0735633119845694,
 abstract = {This study aims to investigate problem-solving with dataset (PSWD) as a computational thinking learning implementation as reflected in academic publications. Specifically, the purpose is to specify the scope of PSWD, which overlaps with the data literacy, thinking with data, big data literacy, and data-based thinking concepts in the literature. Subaims of the study are to identify the conceptual structure of PSWD based on definitions in academic publications and to classify the reasons given in the literature to show the need for PSWD. For the purposes to investigate PSWD conceptually, to classify the reasons given for the need for PSWD, the obtained 54 publications were analyzed via content analysis. Moreover, this study investigates the most frequently suggested or used teaching strategies (in terms of instructional methods, instructional tools, and grade level) for PSWD in the literature. The frequencies of used words in selected publications referring instructional methods, instructional tools, and grade levels were shown in the findings of study. The importance of the study stems from its focus on a new approach to computational thinking instructional implementation.},
 author = {Burcu Berikan and Selçuk Özdemir},
 doi = {10.1177/0735633119845694},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633119845694},
 journal = {Journal of Educational Computing Research},
 number = {2},
 pages = {502–534},
 title = {Investigating “Problem-Solving With Datasets” as an Implementation of Computational Thinking: A Literature Review},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633119845694},
 volume = {58},
 year = {2020b}
}

@article{doi:10.1177/0735633119872908,
 abstract = {This study investigated young children’s computational thinking (CT) development by integrating ScratchJr into a programming curriculum. Twelve third graders (six males and six females) voluntarily participated in an experiment-based computer class conducted at a public elementary school in Taiwan. This study adopted a case study methodology to investigate research questions in one specific case (8-week CT educational training). A one-group quasi-experimental pretest and posttest design with the support of qualitative observation was used to examine four research topics: CT competence progress, programming behaviors in a CT framework, factors influencing CT competence, and learning responses to CT training. The quantitative results indicated that students immersing in weekly programming projects significantly improved in terms of their CT competence, which was mostly retained 1 month after completion of the class. The programming behaviors indicated that students’ CT concepts (sequence, event, and parallelism) and practice (testing and debugging as well as reusing and remixing) significantly improved. Moreover, parents’ active involvement in take-home assignments influenced students’ long-term CT competence retention. The qualitative results indicated that students enjoyed using tablet computers to learn ScratchJr programming and demonstrated various leaning behaviors in a three-stage instructional design model.},
 author = {Pao-Nan Chou},
 doi = {10.1177/0735633119872908},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633119872908},
 journal = {Journal of Educational Computing Research},
 number = {3},
 pages = {570–595},
 title = {Using ScratchJr to Foster Young Children’s Computational Thinking Competence: A Case Study in a Third-Grade Computer Class},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633119872908},
 volume = {58},
 year = {2020g}
}

@article{doi:10.1177/0735633119887187,
 abstract = {Persistence has proven to be a great challenge in online learning environments. Gaming and interactivity have been suggested as essential features in reducing dropout and increasing persistence in online learning. Yet in interactive game-based learning environments, persistence in moving forward in the game may come at the expense of investing in each of the game’s levels. That is, the motivation to complete the game may have a deleterious effect on learning at specific levels and hence on learning from the game in general. Therefore, we have chosen to focus on microlevel persistence (i.e., persistence during each component of the learning process). We study microlevel persistence in the context of acquiring computational thinking—the thought process of solving problems through abstraction—which is a key component of the new literacies needed for tomorrow’s citizens. In this study, we analyze data collected from an online, game-based learning environment (CodeMonkeyTM). The data document the activity of first to sixth graders (N = 2,040). Overall, we find that persistence is positively associated with difficulty and that the most determined learners were highly persistent across topics in achieving the best solution.},
 author = {Rotem Israel-Fishelson and Arnon Hershkovitz},
 doi = {10.1177/0735633119887187},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633119887187},
 journal = {Journal of Educational Computing Research},
 number = {5},
 pages = {891–918},
 title = {Persistence in a Game-Based Learning Environment: The Case of Elementary School Students Learning Computational Thinking},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633119887187},
 volume = {58},
 year = {2020j}
}

@article{doi:10.1177/0735633119887508,
 abstract = {This study implemented a data-driven approach to identify Chinese high school students’ common errors in a Java-based introductory programming course using the data in an automated assessment tool called the Mulberry. Students’ error-related behaviors were also analyzed, and their relationships to success in introductory programming were investigated. This study identified 15 common compilation errors and 6 common test errors. The results showed that these common errors accounted for a large proportion of all errors, so identifying the common errors is important to help students succeed in introductory programming courses. Based on these common errors, five underlying student difficulties were identified and are discussed. In addition, after analyzing existing measures of students’ error-related behaviors, we developed a measure called improvement rate to quantify students’ success in fixing errors. The results of our study suggest that students’ competence of improving code is important to their success in introductory programming. We recommend researchers design and develop automated assessment tools that provide feedback messages for common student errors and instructors who explicitly teach knowledge and skills of improving code in class.},
 author = {Yizhou Qian and James Lehman},
 doi = {10.1177/0735633119887508},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633119887508},
 journal = {Journal of Educational Computing Research},
 number = {5},
 pages = {919–945},
 title = {An Investigation of High School Students’ Errors in Introductory Programming: A Data-Driven Approach},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633119887508},
 volume = {58},
 year = {2020j}
}

@article{doi:10.1177/0735633120905605,
 abstract = {In this work, we examine whether repeated participation in an after-school computing program influenced student learning of computational thinking concepts, practices, and perspectives. We also examine gender differences in learning outcomes. The program was developed through a school–university partnership. Data were collected from 138 students over a 2.5-year period. Data sources included pre–post content assessments of computational concepts related to programming in addition to computational artifacts and interviews with a purposeful sample of 12 participants. Quantitative data were analyzed using statistical methods to identify gains in pre- and post-learning of computational thinking concepts and examine potential gender differences. Interview data were analyzed qualitatively. Results indicated that students made significant gains in their learning of computational thinking concepts and that gains persisted over time. Results also revealed differences in learning of computational thinking concepts among boys and girls both at the beginning and end of the program. Finally, results from student interviews provided insights into the development of computational thinking practices and perspectives over time. Results have implications for the design of after-school computing programs that help broaden participation in computing.},
 author = {Chrystalla Mouza and Yi-Cheng Pan and Hui Yang and Lori Pollock},
 doi = {10.1177/0735633120905605},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633120905605},
 journal = {Journal of Educational Computing Research},
 number = {5},
 pages = {1029–1056},
 title = {A Multiyear Investigation of Student Computational Thinking Concepts, Practices, and Perspectives in an After-School Computing Program},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633120905605},
 volume = {58},
 year = {2020p}
}

@article{doi:10.1177/0735633120930673,
 abstract = {This exploratory study attempts to determine problem solving steps in block based programming environments. The study was carried out throughout one term within Code.org. Participants were 15 6th grade secondary school students enrolled in an IT course at a public secondary school. Observations, screenshots and interviews were analyzed together to find out what students do and what they think during problem solving process. As a result, three main steps (focus, fight and finalize) were extracted from students’ behavioral patterns. The results suggest that three steps occur in linear or cyclic manner with regard to the programming constructs required for the solution of the problem. Implications for instructors who desire to provide a better learning experience on problem solving through block-based programming are also included.},
 author = {Ünal Çakıroğlu and Suheda Mumcu},
 doi = {10.1177/0735633120930673},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633120930673},
 journal = {Journal of Educational Computing Research},
 number = {7},
 pages = {1279–1310},
 title = {Focus-Fight-Finalize (3F): Problem-Solving Steps Extracted From Behavioral Patterns in Block Based Programming},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633120930673},
 volume = {58},
 year = {2020e}
}

@article{doi:10.1177/0735633120932871,
 abstract = {The recent shift in compulsory education from ICT-focused computing curricula to informatics, digital literacy and computer science, has resulted in children being taught computing using block-based programming tools such as Scratch, with teaching that is often limited by school resources and teacher expertise. Even without these limitations, Scratch users often produce code with ‘code smells’ such as duplicate blocks and long scripts which impact how they understand and debug projects. These code smells can be removed using procedural abstraction, an important concept in computer science rarely taught to this age group. This article describes the design of a novel educational block-based programming game, Pirate Plunder, which concentrates on how procedural abstraction is introduced and reinforced. The article then reports an extended evaluation to measure the game’s efficacy with children aged 10 and 11, finding that children who played the game were then able to use procedural abstraction in Scratch. The article then uses game analytics to explore why the game was effective and gives three recommendations for educational game design based on this research: using learning trajectories and restrictive success conditions to introduce complex content, increasing learner investment through customisable avatars and suggestions for improving the evaluations of educational games.},
 author = {Simon P. Rose and M. P. Jacob Habgood and Tim Jay},
 doi = {10.1177/0735633120932871},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633120932871},
 journal = {Journal of Educational Computing Research},
 number = {7},
 pages = {1372–1411},
 title = {Designing a Programming Game to Improve Children’s Procedural Abstraction Skills in Scratch},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633120932871},
 volume = {58},
 year = {2020o}
}

@article{doi:10.1177/0735633120940954,
 abstract = {Computational Thinking (CT) and creativity are considered two vital skills for the 21st century that should be incorporated into future curricula around the world. We studied the relationship between these two constructs while focusing on learners’ personal characteristics. Two types of creativity were examined: creative thinking and computational creativity. The research was conducted among 174 middle school students from Spain. Data collected using a standardized creativity test (Torrance’s TTCT) were triangulated with data drawn from students’ log files that documented their activity in a game-based learning environment for CT (Kodetu). We found some interesting associations between CT and the two constructs of creativity. These associations shed light on positive associations between each of the two creativity constructs and CT acquisition, as well as between the two creativity constructs themselves. Additionally, we highlight differences between boys and girls, as girls were found to be more creative on both creativity measures. Other differences associated with school affiliation, prior coding knowledge, and technology affinity are also discussed.},
 author = {Rotem Israel-Fishelson and Arnon Hershkovitz and Andoni Eguíluz and Pablo Garaizar and Mariluz Guenaga},
 doi = {10.1177/0735633120940954},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633120940954},
 journal = {Journal of Educational Computing Research},
 number = {8},
 pages = {1415–1447},
 title = {The Associations Between Computational Thinking and Creativity: The Role of Personal Characteristics},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633120940954},
 volume = {58},
 year = {2021f}
}

@article{doi:10.1177/0735633120945935,
 abstract = {Block-based visual programming tools, such as Scratch, Alice, and MIT App Inventor, provide an intuitive and easy-to-use editing interface through which to promote programming learning for novice students of various ages. However, very little attention has been paid to investigating these tools’ overall effects on students’ academic achievement and the study features that may moderate the effects of block-based visual programming from a comprehensive perspective. Thus, the present study carried out a meta-analysis to systemically examine 29 empirical studies (extracting 34 effect sizes) using experimental or quasi-experiments involving the programming learning effects of employing block-based visual programming tools to date (until the end of 2019). The results showed a small to medium significant positive overall mean effect size (fixed-effect model g = 0.37; random-effects model g = 0.47) of the use of these block-based visual programming tools with respect to students’ academic achievement. Furthermore, the overall mean effect size was significantly affected by the educational stage, programming tool used, experimental treatment, and school location. Discussions and implications based on the findings are provided.},
 author = {Yue Hu and Cheng-Huan Chen and Chien-Yuan Su},
 doi = {10.1177/0735633120945935},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633120945935},
 journal = {Journal of Educational Computing Research},
 number = {8},
 pages = {1467–1493},
 title = {Exploring the Effectiveness and Moderators of Block-Based Visual Programming on Student Learning: A Meta-Analysis},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633120945935},
 volume = {58},
 year = {2021h}
}

@article{doi:10.1177/0735633120964402,
 abstract = {In this research, a scale was developed to determine the programming-oriented computational thinking skills of university students. The participants were 360 students studying in various departments at different universities in Turkey for computer programming. The scale consists of 33 items under conceptual knowledge, algorithmic thinking, and evaluation subscale. While there was no significant difference between the students’ conceptual knowledge and algorithmic thinking skills, the evaluation skills of male students differed significantly compared to females. Programming experience has a significant effect on conceptual knowledge, algorithmic thinking, and evaluation. The algorithmic thinking skills of the students who have low, middle, and high-level programming experience differed significantly. In terms of the development of conceptual knowledge and evaluation skills, it was observed that students should have at least one year of programming experience, but this experience will not make a significant difference if it is four years or more. It is thought that this scale, which is structured for different applications (e.g., web, game, robot) and learning environments (e.g., text, block) within the framework of its programming capabilities (conceptual, semantic, strategic knowledge), will contribute significantly to the evaluation of computational thinking as programming oriented.},
 author = {Servet Kılıç and Seyfullah Gökoğlu and Mücahit Öztürk},
 doi = {10.1177/0735633120964402},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633120964402},
 journal = {Journal of Educational Computing Research},
 number = {2},
 pages = {257–286},
 title = {A Valid and Reliable Scale for Developing Programming-Oriented Computational Thinking},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633120964402},
 volume = {59},
 year = {2021k}
}

@article{doi:10.1177/0735633120965919,
 abstract = {Educational games have been increasingly used to improve students’ computational thinking. However, most existing games have focused on the theoretical knowledge of computational thinking, ignoring the development of computational thinking skills. Moreover, there is a lack of integration of adaptivity into educational computer games for computational thinking, which is crucial to addressing individual needs in developing computational thinking skills. In this study, we present an adaptive educational computer game, called AutoThinking, for developing students’ computational thinking skills in addition to their conceptual knowledge. To evaluate the effects of the game, we conducted an experimental study with 79 elementary school students in Estonia, where the experimental group learned with AutoThinking, while the control group used a traditional technology-enhanced learning approach. Our findings show that learning with the adaptive educational computer game significantly improved students’ computational thinking related to both conceptual knowledge and skills. Moreover, students using the adaptive educational computer game showed a significantly higher level of interest, satisfaction, flow state, and technology acceptance in learning computational thinking. Implications of the findings are also discussed.},
 author = {Danial Hooshyar and Margus Pedaste and Yeongwook Yang and Liina Malva and Gwo-Jen Hwang and Minhong Wang and Heuiseok Lim and Dejan Delev},
 doi = {10.1177/0735633120965919},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633120965919},
 journal = {Journal of Educational Computing Research},
 number = {3},
 pages = {383–409},
 title = {From Gaming to Computational Thinking: An Adaptive Educational Computer Game-Based Learning Approach},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633120965919},
 volume = {59},
 year = {2021f}
}

@article{doi:10.1177/0735633120966048,
 abstract = {Learning programming is a painful process for most students, especially those learning text- based programming languages. In this study, based on the principle of Bandura’s social learning theory, the vicarious real-life experiences of several pioneers in the field of IT and programming were presented as 15-minutes stories to a group of 9th graders learning to code in Python. A quasi-experimental research design was used to examine the effects of the theory and associated technique on student achievement. The 27 participants in the experiment group watched these presentations before engaging in programming activities while a control group were also observed who received no extra motivational intervention. Those in the experiment group were found to have significantly improved their programming achievement compared to those in the control group. Additionally, the functionality and originality of the experimental group projects were significantly better than those produced in the control group. They reported that they had been motivated by the pioneer stories, and their interest and self-efficacy beliefs in relation to creating high level programming projects were increased. The results highlight the benefits obtained from proper application of a “pioneer-story technique” to improve text-based programming skills.},
 author = {Can Fatih Efecan and Serkan Sendag and Nuray Gedik},
 doi = {10.1177/0735633120966048},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633120966048},
 journal = {Journal of Educational Computing Research},
 number = {3},
 pages = {453–469},
 title = {Pioneers on the Case for Promoting Motivation to Teach Text-Based Programming},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633120966048},
 volume = {59},
 year = {2021h}
}

@article{doi:10.1177/0735633120967315,
 abstract = {This paper examines a method which can be used by instructors pursuing innovative methods for language teaching, which expands learners’ motivation in second language learning. Computational thinking (CT) is a problem-solving skill which can motivate students’ English language learning. Designing a learning activity which integrates CT into English language learning has been considered in only a few academic studies. This study aimed to explore whether integrating CT into English language learning can be useful for improving learners’ motivation and performance. The method of “present, practice, and produce” was applied as a method of presenting computational thinking in the English language learning classroom. Fifty-two elementary school students (52) participated in the experimental study. Following an experimental design, data were collected and analyzed from a combination of knowledge test scores, storytelling, motivation, and anxiety surveys. The experimental results indicate that the CT strategy improves students’ language learning and raises their motivation in the two dimensions of extrinsic and intrinsic goal orientation. These results imply the positive effect of CT strategy on strengthening problem-solving skills of students participating in digital storytelling and increases their motivation and performance in English language learning.},
 author = {Nadia Parsazadeh and Pei-Yu Cheng and Ting-Ting Wu and Yueh-Min Huang},
 doi = {10.1177/0735633120967315},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633120967315},
 journal = {Journal of Educational Computing Research},
 number = {3},
 pages = {470–495},
 title = {Integrating Computational Thinking Concept Into Digital Storytelling to Improve Learners’ Motivation and Performance},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633120967315},
 volume = {59},
 year = {2021q}
}

@article{doi:10.1177/0735633120967326,
 abstract = {To explore the role of design thinking in contemporary computer literacy education, this study aimed to examine the relationship between young students’ design thinking disposition and their computer programming self-efficacy. To assess students’ design thinking disposition, this study developed the Design Thinking Disposition Scale (DTDS) with a sample of 350 junior high school students who had computer programming experience in a STEAM course. A principle axis factor analysis with the promax rotation method was used to verify the DTDS’s construct under the four dimensions: empathize, define, ideate and prototype. The Cronbach’s alpha reliability was .90 for the overall scale. Correlation analyses results showed that all the four dimensions were significantly correlated with computer programming self-efficacy assessed by CPSES. A significant regression model was found in which the three factors, ideate, prototype and define, significantly predicted the overall computer programming self-efficacy. Meanwhile, except for the ideate subscale, no gender difference was found in the young students’ design thinking dispositions. The students’ self-directed programming learning experience was shown to benefit their design thinking disposition. The DTDS can be applied to design-thinking-embedded computer literacy curricula such as makers, STEAM, or robotics education. Several further studies are also suggested.},
 author = {Meng-Jung Tsai and Ching-Yeh Wang},
 doi = {10.1177/0735633120967326},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633120967326},
 journal = {Journal of Educational Computing Research},
 number = {3},
 pages = {410–428},
 title = {Assessing Young Students’ Design Thinking Disposition and Its Relationship With Computer Programming Self-Efficacy},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633120967326},
 volume = {59},
 year = {2021s}
}

@article{doi:10.1177/0735633120972050,
 abstract = {Computational thinking is a means to understand and solve complex problems through using computer science concepts and techniques. While there is an increase in the number of initiatives focusing on coding, whether they (a) address computational thinking and problem-solving skills and (b) use methods of teaching problem solving is yet to be explored. As a preliminary step, this study focuses on examining 3rd grade lesson plans at Code.org. The analysis identified various components of computational thinking and problem solving as well as specific problem-solving teaching methods used to address these skills. Besides such cognitive outcomes, the findings also revealed affective, reflective, and social aspects of learning supported in the lessons.},
 author = {Ugur Kale and Jiangmei Yuan},
 doi = {10.1177/0735633120972050},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633120972050},
 journal = {Journal of Educational Computing Research},
 number = {4},
 pages = {620–644},
 title = {Still a New Kid on the Block? Computational Thinking as Problem Solving in Code.org},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633120972050},
 volume = {59},
 year = {2021g}
}

@article{doi:10.1177/0735633120972356,
 abstract = {Computational thinking has received tremendous attention from computer science educators and educational researchers in the last decade. However, most prior literature defines computational thinking as thinking outcomes rather than thinking processes. Based on Selby and Woodland’s framework, this study developed and validated the Computational Thinking Scale (CTS) to assess all students’ thought processes of computational thinking for both general and specific problem-solving contexts in five dimensions: abstraction, decomposition, algorithmic thinking, evaluation and generalization. A survey including 25 candidate items for CTS as well as demographic variables was administered to 388 junior high school students in Taiwan. An explorative factor analysis using the principal axis method with the oblimin rotation was used to validate the scale. Finally, 19 items were extracted successfully under the designed five dimensions, with a total explained variance of 64.03% and an overall reliability of 0.91. Results of the demographic comparisons showed that boys had a greater disposition than girls in decomposition thinking when solving problems using computer programming. In addition, programming learning experience, especially self-directed learning and after-school learning, had significant positive effects on all dimensions of CTS. Several future studies are suggested using this tool.},
 author = {Meng-Jung Tsai and Jyh-Chong Liang and Chung-Yuan Hsu},
 doi = {10.1177/0735633120972356},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633120972356},
 journal = {Journal of Educational Computing Research},
 number = {4},
 pages = {579–602},
 title = {The Computational Thinking Scale for Computer Literacy Education},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633120972356},
 volume = {59},
 year = {2021o}
}

@article{doi:10.1177/0735633120973429,
 abstract = {Creativity and Computational Thinking (CT) have been both extensively researched in recent years. However, the associations between them are still not fully understood despite their recognition as essential competencies for the digital age. This study looks to bridge this gap by examining the association between CT and two types of creativity, i.e., Creative Thinking and Computational Creativity. The research was conducted among 124 middle school students from Spain, who were divided into control and experimental groups; the intervention included an explicit encouragement to be as creative as possible (i.e., to submit multiple correct solutions) in a given learning task. Data were analyzed from a standardized creativity test (Torrance’s TTCT) and cross-referenced with log files that documented the students’ activities in the Kodetu game-based learning environment. Our research findings indicate some interesting associations between CT and Creativity. First, we found that creativity contributes to CT. Second, we found that CT is transferable across different domains. Finally, we found that Computational Creativity can develop and improve over time.},
 author = {Rotem Israel-Fishelson and Arnon Hershkovitz and Andoni Eguíluz and Pablo Garaizar and Mariluz Guenaga},
 doi = {10.1177/0735633120973429},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633120973429},
 journal = {Journal of Educational Computing Research},
 number = {5},
 pages = {926–959},
 title = {A Log-Based Analysis of the Associations Between Creativity and Computational Thinking},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633120973429},
 volume = {59},
 year = {2021g}
}

@article{doi:10.1177/0735633120973430,
 abstract = {Pair programming is a collaborative learning mode to foster novice learners’ computer programming. Previous empirical research has reported contrasting conclusions about the effect of pair programming on student learning. To further understand students’ pair programming, this study uses a mixed method to analyze three contrasting pairs’ collaborative behaviors, discourses, and perceptions from a multi-dimensional perspective. The analysis results show that the high-ranked student pair is characterized as the interactive, socially-supportive, and goal-oriented pair; the middle-ranked student pair is characterized as the highly-interactive, socially-supportive, and process-oriented pair; and the low-ranked student pair is characterized as the lowly-interactive, socially-unsupportive, and programming-distracted pair. The research reveals complex relations between collaborative behaviors, discourses, and performances, which have critical influences on students’ pair programming quality. Based on the results, this research proposes pedagogical, analytical, and theoretical implications for future instructional design, learning analytics, and empirical research of collaborative programming.},
 author = {Dan Sun and Fan Ouyang and Yan Li and Hongyu Chen},
 doi = {10.1177/0735633120973430},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633120973430},
 journal = {Journal of Educational Computing Research},
 number = {4},
 pages = {740–762},
 title = {Three Contrasting Pairs’ Collaborative Programming Processes in China’s Secondary Education},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633120973430},
 volume = {59},
 year = {2021o}
}

@article{doi:10.1177/0735633120973432,
 abstract = {This study explored (1) pedagogical strategies in Educational Coding and Robotics (ECR) learning which can develop computational thinking of students and (2) the degree of teacher centrality in the ECR classroom. In addition, we investigated (3) the added value of the Small Private Online Course (SPOC) to teacher professional development (TPD). We analyzed reflections of 80 in-service teachers on TPD through the SPOC (1,091 statements) and conducted semi-structured interviews with 13 of them one year after completing the course and teaching ECR in the classroom (328 statements). The most prominent strategies immediately after the TPD were constructing learning experiences, tinkering & debugging, and interdisciplinary learning, while one year later, experiential learning and collaborative learning were more common. Regarding the degree of teacher centrality, a year after teaching ECR curriculum the teachers had a significantly higher percentage of statements reflecting their role as a guide-on-the-side and as a partner of students in the learning process. Regarding the contribution of the SPOC for TPD, teacher statements revealed significantly more benefits than challenges in both points of time. Interestingly, the same categories emerged bottom-up as benefits and challenges: a variety of control dimensions, independent learning, learning and knowledge management and collaboration. Implications for educational theory and ECR practice are discussed.},
 author = {Shlomit Hadad and Tamar Shamir-Inbal and Ina Blau and Eynat Leykin},
 doi = {10.1177/0735633120973432},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633120973432},
 journal = {Journal of Educational Computing Research},
 number = {4},
 pages = {763–791},
 title = {Professional Development of Code and Robotics Teachers Through Small Private Online Course (SPOC): Teacher Centrality and Pedagogical Strategies for Developing Computational Thinking of Students},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633120973432},
 volume = {59},
 year = {2021c}
}

@article{doi:10.1177/0735633120978530,
 abstract = {Computer science and computational thinking (CT) education in K-12 schools have been escalating in recent years. A couple of CT instructional models have been proposed to depict the roles of CT in K-16 education. Yet, neither of them discussed CT infusion into a subject course. In this article, we proposed a CT-integration model called TPC2T. In this model, we suggested considering CT as a second subject and using an appropriate technological pedagogical approach to make students’ learning of two subjects meaningful and engaging. We implemented this model in a CT-integrated lesson in two sections of a high-school Spanish course. Students worked in small groups and coded three small and one comprehensive digital Spanish-culture stories in Scratch. Results showed that students taking the CT-integrated lesson had the same degree of improvement in their Spanish culture knowledge as their peers who did not take the CT-integrated lesson. Besides, students taking the CT-integrated lesson had a significant improvement in their CT knowledge. At the same time, their CT self-efficacy outperformed those who did not take the CT-integrated lesson. We discussed the results and offered suggestions for researchers and educators at the end of the article.},
 author = {Shenghua Zha and Debra A. L. Morrow and Jennifer Curtis and Shane Mitchell},
 doi = {10.1177/0735633120978530},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633120978530},
 journal = {Journal of Educational Computing Research},
 number = {5},
 pages = {844–869},
 title = {Learning Culture and Computational Thinking in a Spanish Course: A Development Model},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633120978530},
 volume = {59},
 year = {2021v}
}

@article{doi:10.1177/0735633120979930,
 abstract = {In this paper, we explore the challenges experienced by a group of Primary 5 to 6 (age 12–14) students as they engaged in a series of problem-solving tasks through block-based programming. The challenges were analysed according to a taxonomy focusing on the presence of computational thinking (CT) elements in mathematics contexts: preparing problems, programming, create computational abstractions, as well as troubleshooting and debugging. Our results suggested that the challenges experienced by students were compounded by both having to learn the CT-based environment as well as to apply mathematical concepts and problem solving in that environment. Possible explanations for the observed challenges stemming from differences between CT and mathematical thinking are discussed in detail, along with suggestions towards improving the effectiveness of integrating CT into mathematics learning. This study provides evidence-based directions towards enriching mathematics education with computation.},
 author = {Zhihao Cui and Oi-Lam Ng},
 doi = {10.1177/0735633120979930},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633120979930},
 journal = {Journal of Educational Computing Research},
 number = {5},
 pages = {988–1012},
 title = {The Interplay Between Mathematical and Computational Thinking in Primary School Students’ Mathematical Problem-Solving Within a Programming Environment},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633120979930},
 volume = {59},
 year = {2021e}
}

@article{doi:10.1177/0735633120985235,
 abstract = {In recent years, the government has actively set up computer programming courses to train those with the relevant talent; however, the learning performance of the students is not ideal. Therefore, in order to learn programming skills, students usually adopt note-taking strategies because, due to the pressure of the course, the teachers do not have sufficient time to help the students to fully understand the course content. This means that some students take notes without thinking, so their academic performance is usually poor. This study, therefore, proposes an innovative curriculum design that is based on the “Note-taking System and Teaching Strategy’’ (NSTS), which combines learning style concepts and peer learning concepts to achieve student interaction and promote their thinking skills. In the learning activity, students are asked to search for additional supplementary material and to write their own notes, and then the members of group can read their notes and learn from them. However, the results of the study show that the NSTS curriculum design exhibits significant differences for improving the students’ academic performance, and that it also has a certain influence on their learning motivation.},
 author = {Sheng-Bo Huang and Yu-Lin Jeng and Chin-Feng Lai},
 doi = {10.1177/0735633120985235},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633120985235},
 journal = {Journal of Educational Computing Research},
 number = {5},
 pages = {870–895},
 title = {Note-Taking Learning System: The Use of the Learning Style Theory and the Peer Learning Method on Computer Programming Course},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633120985235},
 volume = {59},
 year = {2021k}
}

@article{doi:10.1177/0735633120988807,
 abstract = {Currently, many countries actively cultivate students to develop computational thinking ability. Many visual programming environments (VPEs) and physical robot courses have been integrated into computational thinking learning in the elementary education stage. This study explores the relationship between the programming learning environment (including VPE, physical robots, and no experience) and the computational thinking ability of higher-grade elementary school students of different genders. The results show that learning through VPE or physical robots can help students improve their computational thinking ability and that students learn better via physical robots. In addition, among the four dimensions of computational thinking ability, most students are weak in algorithm design. In terms of gender, no differences exist in computational thinking ability. Further analysis reveals that female students have better decomposition performance in VPE learning, while male students have better algorithm design performance.},
 author = {Sheng-Yi Wu and Yu-Sheng Su},
 doi = {10.1177/0735633120988807},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633120988807},
 journal = {Journal of Educational Computing Research},
 number = {6},
 pages = {1075–1092},
 title = {Visual Programming Environments and Computational Thinking Performance of Fifth- and Sixth-Grade Students},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633120988807},
 volume = {59},
 year = {2021q}
}

@article{doi:10.1177/07356331211004048,
 abstract = {As schools and districts across the United States adopt computer science standards and curriculum for K-12 computer science education, they look to integrate the foundational concepts of computational thinking (CT) into existing core subjects of elementary-age students. Research has shown the effectiveness of teaching CT elements (abstraction, generalization, decomposition, algorithmic thinking, debugging) using non-programming, unplugged approaches. These approaches address common barriers teachers face with lack of knowledge, familiarity, or technology tools. Picture books and graphic novels present an unexplored non-programming, unplugged resource for teachers to integrate computational thinking into their CT or CT-integrated lessons. This analysis examines 27 picture books and graphic novels published between 2015 and 2020 targeted to K-6 students for representation of computational thinking elements. Using the computational thinking curriculum framework for K-6, we identify the grade-level competencies of the CT elements featured in the books compared to the books’ target age groups. We compare grade-level competencies to interest level to identify each CT element representation as “foundational,” “on-target,” or “advanced.” We conclude that literature offers teachers a non-programming unplugged resource to expose students to CT and enhance CT and CT-integrated lessons, while also personalizing learning based on CT readiness and interest level.},
 author = {Evan David Ballard and Rachelle Haroldson},
 doi = {10.1177/07356331211004048},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331211004048},
 journal = {Journal of Educational Computing Research},
 number = {8},
 pages = {1487–1516},
 title = {Analysis of Computational Thinking in Children’s Literature for K-6 Students: Literature as a Non-Programming Unplugged Resource},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331211004048},
 volume = {59},
 year = {2022a}
}

@article{doi:10.1177/07356331211017793,
 abstract = {The integration of visual programming in early formal education has been found to promote computational thinking of students. Teachers’ intuitive perspectives about optimal learning processes – “folk psychology” – impact their perspectives about teaching “folk pedagogy” and play a significant role in integrating educational technologies, such as visual programming, within the formal curriculum. This study was conducted based on the mixed method research paradigm. First, a folk pedagogy questionnaire was distributed to 89 teachers who integrate differing technologies in their classroom in order to identify the teachers’ pedagogical perspectives: constructivist versus instructivist. Then, semi-structured interviews were conducted with 24 teachers who teach Scratch in order to gain a deeper understanding of their instructivist/constructivist perspectives and actual pedagogical practices and strategies. Finally, we analyzed 96 students’ programming artifacts to explore differences, if any, in students’ outcomes related to the pedagogical perspectives of their teachers. Findings revealed that pedagogical perspectives are reflected in teaching strategies and assessment practices employed in a visual programming environment. It is promising that teaching visual programming promoted constructivist pedagogy even among instructivist teachers and was consequently reflected in student perspectives and expressed in their programming artifacts. We discuss theoretical and educational implications of these findings.},
 author = {Avital Kesler and Tamar Shamir-Inbal and Ina Blau},
 doi = {10.1177/07356331211017793},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331211017793},
 journal = {Journal of Educational Computing Research},
 number = {1},
 pages = {28–55},
 title = {Active Learning by Visual Programming: Pedagogical Perspectives of Instructivist and Constructivist Code Teachers and Their Implications on Actual Teaching Strategies and Students’ Programming Artifacts},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331211017793},
 volume = {60},
 year = {2022l}
}

@article{doi:10.1177/07356331211017794,
 abstract = {A prior study developed the Computational Thinking Scale (CTS) for assessing individuals’ computational thinking dispositions in five dimensions: decomposition, abstraction, algorithmic thinking, evaluation, and generalization. This study proposed the Developmental Model of Computational Thinking through validating the structural relationships among the five factors of the CTS. To examine the model, a questionnaire including the CTS was administered to 472 middle school students. A confirmatory factor analysis was used to confirm the construct of the measurements, and a PLS-SEM analysis was used to validate the structural relationships among the factors. The results confirmed that the 19-item CTS has good item reliability, internal consistency, and construct reliability for measuring computational thinking (CT). In the Developmental Model of CT, decomposition and abstraction significantly predict all other three CT dispositions, suggesting that they are the two fundamental factors required for CT development. Moreover, a significant linear prediction path was shown starting from algorithmic thinking, evaluation, until generalization. Thus, a multi-level model was confirmed for the conceptual framework of CT. This model suggests a possible sequence for CT development which may provide a guideline for the teaching objectives of CT for different learning stages in different school levels. Decomposition and abstraction are especially suggested to be emphasized in school curricula before teaching algorithmic thinking or algorithm designs.},
 author = {Meng-Jung Tsai and Jyh-Chong Liang and Silvia Wen-Yu Lee and Chung-Yuan Hsu},
 doi = {10.1177/07356331211017794},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331211017794},
 journal = {Journal of Educational Computing Research},
 number = {1},
 pages = {56–73},
 title = {Structural Validation for the Developmental Model of Computational Thinking},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331211017794},
 volume = {60},
 year = {2022p}
}

@article{doi:10.1177/07356331211027387,
 abstract = {There are increasing calls to introduce computational thinking in schools; the arguments in favor call upon research suggesting that even kindergarten children can successfully engage in coding. This contribution presents a cross-sectional study examining the coding practices and computational thinking of fifty-one primary school children using the ScratchJr software; children were organized in two cohorts (Cohort 1: 6–9 years old; Cohort 2: 10–12 years old). Each cohort participated in a six-hour intervention, as part of a four-day summer club. During the intervention children were introduced to ScratchJr and were asked to collaboratively design a digital story about environmental waste management actions, thus adopting a disciplinary perspective to computational thinking. Data analyses examined children’s final artifacts, in terms of coding practices and the level of computational thinking demonstrated by each cohort. Furthermore, analysis of selected groups’ storyboard interviews was used to shed light on differences between the two cohorts. Results are presented and contrasted across the two age cohorts via a developmental perspective. The findings of this study can be useful in considering the instructional support that is necessary to scaffold the development of primary school children’s coding practices and computational thinking.},
 author = {Eleni A. Kyza and Yiannis Georgiou and Andria Agesilaou and Markos Souropetsis},
 doi = {10.1177/07356331211027387},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331211027387},
 journal = {Journal of Educational Computing Research},
 number = {1},
 pages = {220–257},
 title = {A Cross-Sectional Study Investigating Primary School Children’s Coding Practices and Computational Thinking Using ScratchJr},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331211027387},
 volume = {60},
 year = {2022j}
}

@article{doi:10.1177/07356331211033158,
 abstract = {This article provides an overview of the diverse ways in which computational thinking has been operationalised in the literature. Computational thinking has attracted much interest and debatably ranks in importance with the time-honoured literacy skills of reading, writing, and arithmetic. However, learning interventions in this subject have modelled computational thinking differently. We conducted a systematic review of 81 empirical studies to examine the nature, explicitness, and patterns of definitions of computational thinking. Data analysis revealed that most of the reviewed studies operationalised computational thinking as a composite of programming concepts and preferred definitions from assessment-based frameworks. On the other hand, a substantial number of the studies did not establish the meaning of computational thinking when theorising their interventions nor clearly distinguish between computational thinking and programming. Based on these findings, this article proposes a model of computational thinking that focuses on algorithmic solutions supported by programming concepts which advances the conceptual clarity between computational thinking and programming.},
 author = {Ndudi O. Ezeamuzie and Jessica S. C. Leung},
 doi = {10.1177/07356331211033158},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331211033158},
 journal = {Journal of Educational Computing Research},
 number = {2},
 pages = {481–511},
 title = {Computational Thinking Through an Empirical Lens: A Systematic Review of Literature},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331211033158},
 volume = {60},
 year = {2022e}
}

@article{doi:10.1177/07356331211035182,
 abstract = {In this work, we studied the influence of different programming approaches on the development of students’ computational thinking (CT) skills, the programming experience and gender differences in CT development were also discussed. A total of 158 junior high school students and one teacher participated in the study over 5 months. The sample students were divided into four experimental groups in four single or combined programming approaches (i.e., plugged-in, unplugged, unplugged first, and plugged-in first) and one control group without programming. Data sources included the results of four CT tests, as well as interviews with the teacher and surveys with 24 representative participants. The results showed that the four programming approaches can effectively improve students’ CT skills and can be retained after two months. Among them, the form of implementing unplugged activities before plugged-in can most effectively improve CT skills, and can better weaken the impact of previous programming experience. Finally, the qualitative analysis results provided insights into the process of programming and CT education. These findings will provide implications for the introduction of CT in junior high school, and help expand students’ participation in computing.},
 author = {Lihui Sun and Linlin Hu and Danhua Zhou},
 doi = {10.1177/07356331211035182},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331211035182},
 journal = {Journal of Educational Computing Research},
 number = {2},
 pages = {283–321},
 title = {Single or Combined? A Study on Programming to Promote Junior High School Students’ Computational Thinking Skills},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331211035182},
 volume = {60},
 year = {2022t}
}

@article{doi:10.1177/07356331211037757,
 abstract = {Computational thinking (CT) has attracted significant interest among many educators around the globe. Despite this growing interest, research on CT and programming education in elementary school remains at an initial stage. Many relevant studies have adopted only one type of method to assess students’ CT, which may lead to an incomplete view of student development on CT, while other studies employed small sample sizes, which may increase the chance of assuming a false premise to be true. Moreover, conventional programming courses typically have two limitations (e.g., limited student active learning and student low engagement). Given these gaps, this study investigates the effects of a theory-based (5E framework) flipped classroom model (FCM) on elementary school students’ understanding of CT concepts, computational problem-solving performance, and perceptions of flipped learning. To achieve this, a pretest-posttest quasi-experimental study was conducted in a rural elementary school, including 125 students in the experimental group and 122 students in the control group. The results showed that the 5E-based FCM significantly improved student understanding of CT concepts and computational problem-solving abilities. The results also revealed positive student perception toward the FCM. The benefits and challenges of the 5E-based FCM are discussed.},
 author = {Xuemin Gao and Khe Foon Hew},
 doi = {10.1177/07356331211037757},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331211037757},
 journal = {Journal of Educational Computing Research},
 number = {2},
 pages = {512–543},
 title = {Toward a 5E-Based Flipped Classroom Model for Teaching Computational Thinking in Elementary School: Effects on Student Computational Thinking and Problem-Solving Performance},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331211037757},
 volume = {60},
 year = {2022d}
}

@article{doi:10.1177/07356331211039961,
 abstract = {Many countries have incorporated computational thinking (CT) and programming languages into their science and technology courses. Students can improve their CT ability by learning programming languages. Moreover, situated learning enables students to generate knowledge and master problem-solving skills through interaction with situations. This study incorporated Webduino learning and the situated learning strategy into a programming course and analyzed its impact on high school students’ CT ability, learning motivation, and course satisfaction. A quasi-experimental research method was adopted, wherein the experimental group was subjected to the situated learning strategy and the control group was subjected to a traditional teaching method. The study results revealed that integrating Webduino programming with situated learning could effectively improve five categories of CT skills; moreover, the activity models of situated learning enhanced the value and expectation dimensions of learning motivation. In addition, satisfaction with the course content and self-identity slightly improved. However, because teachers were required to elaborate on stories to promote learner engagement with life situations, the time available for programming was limited. Thus, no significant difference was observed in teaching satisfaction.},
 author = {Ting-Ting Wu and Jian-Ming Chen},
 doi = {10.1177/07356331211039961},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331211039961},
 journal = {Journal of Educational Computing Research},
 number = {3},
 pages = {631–660},
 title = {Combining Webduino Programming With Situated Learning to Promote Computational Thinking, Motivation, and Satisfaction Among High School Students},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331211039961},
 volume = {60},
 year = {2022w}
}

@article{doi:10.1177/07356331211043547,
 abstract = {As a dynamic and multifaceted construct, computational thinking (CT) has proven to be challenging to conceptualize and assess, which impedes the development of a workable ontology framework. To address this issue, the current article describes a novel approach towards understanding the ontological aspects of CT by using text mining and graph-theoretic techniques to elucidate teachers’ perspectives collected in an online survey (N = 105). In particular, a hierarchical cluster analysis, a knowledge representation method, was applied to identify sub-groups in CT conceptualization and assessment amongst teachers. Five clusters in conceptualization and two clusters in assessment were identified; several relevant and distinct themes were also extracted. The results suggested that teachers attributed CT as a competence domain, relevant in the problem- solving context, as well as applicable and transferrable to various disciplines. The results also shed light on the importance of using multiple approaches to assess the diversity of CT. Overall, the findings collectively contributed to a comprehensive and multi-perspective representation of CT that refine both theory and practice. The methodology employed in this article has suggested a minor but significant step towards addressing the quintessential questions of “what is CT?” and “how is it evidenced?”.},
 author = {Rina P.Y. Lai},
 doi = {10.1177/07356331211043547},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331211043547},
 journal = {Journal of Educational Computing Research},
 number = {3},
 pages = {661–695},
 title = {Teachers’ Ontological Perspectives of Computational Thinking and Assessment: A Text Mining Approach},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331211043547},
 volume = {60},
 year = {2022l}
}

@article{doi:10.1177/07356331211051043,
 abstract = {This study aimed to develop the Computational Thinking Test for Elementary School Students (CTT-ES) to assess young children’s CT competencies in non-programming contexts and also examine the relationship between CT competencies and CT dispositions. A survey including a pool of CTT-ES candidate items and the Computational Thinking Scale (CTS) was administered to 631 elementary school students. Rasch model of the Item Response Theory and the discrimination analysis of the Classical Testing Theory were conducted for item analyses. Pearson’s correlation analyses and hierarchical multiple regression analyses were used to examine the relationships between CTT-ES and CTS scores. The results showed that the final CTT-ES including 16 items had a good fitness, discrimination, and reliability to evaluate elementary students’ domain-general CT competencies. The convergent validity of CTT-ES was confirmed by its significant correlations with the CTS scores. The significant regression model not only showed students’ CT competencies can be predicted by their CT dispositions but also supported The Developmental Model of CT. This study provided a valid and reliable tool for assessing young children’s CT abilities. It also furthered our understanding about the developmental orders of CT abilities and contributed to the theoretical construction of CT.},
 author = {Meng-Jung Tsai and Francis Pingfan Chien and Silvia Wen-Yu Lee and Chung-Yuan Hsu and Jyh-Chong Liang},
 doi = {10.1177/07356331211051043},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331211051043},
 journal = {Journal of Educational Computing Research},
 number = {5},
 pages = {1110–1129},
 title = {Development and Validation of the Computational Thinking Test for Elementary School Students (CTT-ES): Correlate CT Competency With CT Disposition},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331211051043},
 volume = {60},
 year = {2022u}
}

@article{doi:10.1177/07356331211053383,
 abstract = {Scratch, a kind of visual programming software, has been widely used in instruction for primary school children. Scratch constructs a digital world for children to design, develop, and create coursework in which their creative thinking is fostered. Different instructional methods have been designed and implemented to stimulate children’s creative thinking skills through their coursework. This study investigated whether scaffolding construction with mind mapping promoted children’s creative thinking in a Scratch course. Two groups of 84 fifth-grade pupils participated in the study. The experimental group of 44 students adopted the scaffolding construction with mind mapping in the Scratch course, while the control group of 40 students did not use the mind mapping method. The Torrance Tests of Creative Thinking-Figural (TTCT-F) and Torrance Creative Personality Self-Report Scale were used three times over the 16-week learning period. The results show that learning in the Scratch course promoted the children’s creative thinking. The difference between the two groups indicates that mind mapping was beneficial to improve the children’s creative thinking.},
 author = {Yu-Sheng Su and Mingming Shao and Li Zhao},
 doi = {10.1177/07356331211053383},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331211053383},
 journal = {Journal of Educational Computing Research},
 number = {4},
 pages = {906–929},
 title = {Effect of Mind Mapping on Creative Thinking of Children in Scratch Visual Programming Education},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331211053383},
 volume = {60},
 year = {2022m}
}

@article{doi:10.1177/07356331211055379,
 abstract = {Although abstraction is widely understood to be one of the primary components of computational thinking, the roots of abstraction may be traced back to different fields. Hence, the meaning of abstraction in the context of computational thinking is often confounded, as researchers interpret abstraction through diverse lenses. To disentangle these conceptual threads and gain insight into the operationalisation of abstraction, a systematic review of 96 empirical studies was undertaken. Analysis revealed that identifying features of entities, extracting relevant features, discovering patterns, creating rules and assembling the parts together were the core actions of abstraction. With the primary aim of simplifying practical procedures, abstraction was operationalised as the sophistication of a program, the matching of patterns, the creation of alternative representations, the transfer of solutions, the measurement of a learner’s activity and reading program codes. There is an obvious need for researchers to align the conceptual meanings they have established of abstraction with the practical facts of operationalisation. The need to empirically validate emerging models and the implications for future research are discussed.},
 author = {Ndudi O. Ezeamuzie and Jessica S.C. Leung and Fridolin S.T. Ting},
 doi = {10.1177/07356331211055379},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331211055379},
 journal = {Journal of Educational Computing Research},
 number = {4},
 pages = {877–905},
 title = {Unleashing the Potential of Abstraction From Cloud of Computational Thinking: A Systematic Review of Literature},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331211055379},
 volume = {60},
 year = {2022c}
}

@article{doi:10.1177/07356331211057143,
 abstract = {This paper examined the effect of the Unplugged Programming Teaching Aids (UPTA) on students’ computational thinking and classroom interaction. A set of UPTA was created and used in a primary school in southern China. A total of 48 students aged 6–8 were assigned to two classes, with the same instructor and learning materials, but only the treatment group was provided with the UPTA. Both groups were tested on computational thinking ability, children’s concrete operation status, degree of ego-centricity, and in-classroom interaction. Results indicated that the children aged 6–8 years old could classify things according to two kinds of criteria at the same time, but their cognitive style was still ego-centered and they found it difficult to deal with problems from a third-party perspective, no matter whether in the treatment group or the control group. However, students in the treatment group achieved significantly higher scores on the test of computational thinking and were more engaged in the classroom interaction patterns. These findings provide evidence of the positive effect of the UPTA on promoting children’s computational thinking by guiding them to decompose and solve problems, as well as enhancing their interaction and communication in the classroom, so as to transform from simple imitation to collaborative inquiry.},
 author = {Zehui Zhan and Wenchang He and Xitian Yi and Shuyao Ma},
 doi = {10.1177/07356331211057143},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331211057143},
 journal = {Journal of Educational Computing Research},
 number = {5},
 pages = {1277–1300},
 title = {Effect of Unplugged Programming Teaching Aids on Children’s Computational Thinking and Classroom Interaction: with Respect to Piaget’s Four Stages Theory},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331211057143},
 volume = {60},
 year = {2022x}
}

@article{doi:10.1177/07356331211057819,
 abstract = {This study describes the development and validation process of a computational thinking (CT) test for adults. The team designed a set of items and explored a subset of those through two qualitative pilots. Then, in order to provide validity evidence based on the test content, a team of 11 subject-matter experts coded the initial pool of items using two different systems of categories based on CT components and contents. Next, the items were piloted on a sample of 289 participants, 137 experts in CT, and 152 novices. After a series of confirmatory factor analyses, a unidimensional model that represents algorithmic thinking was adopted. Further analyzing the psychometric quality of the 27 items, 20 of them with excellent reliability indices were selected for the test. Thus, this study provides a tool to evaluate adults’ CT: the Algorithmic Thinking Test for Adults (ATTA), which was developed according to psychometric standards. This article also reflects on the nature of CT as a construct.},
 author = {Marc Lafuente Martínez and Olivier Lévêque and Isabel Benítez and Cécile Hardebolle and Jessica Dehler Zufferey},
 doi = {10.1177/07356331211057819},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331211057819},
 journal = {Journal of Educational Computing Research},
 number = {6},
 pages = {1436–1463},
 title = {Assessing Computational Thinking: Development and Validation of the Algorithmic Thinking Test for Adults},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331211057819},
 volume = {60},
 year = {2022k}
}

@article{doi:10.1177/07356331211060470,
 abstract = {Scholars believe that computational thinking is one of the essential competencies of the 21st century and computer programming courses have been recognized as a potential means of fostering students’ computational thinking. In tradition instruction, PFCT (problem identification, flow definition, coding, and testing) is a commonly adopted procedure to guide students to learn and practice computer programming. However, without further guidance, students might focus on learning the syntax of computer programming language rather than the concept of solving problems. This study proposes a peer-assessment-supported PFCT (PA-PFCT) approach for boosting students’ computer programming knowledge and computational thinking awareness. A quasi-experiment was conducted on a computer programming course in a high school to evaluate its influence on students’ learning achievement, computational thinking awareness, learning motivation, and self-efficacy. An experimental group of 51 students learned with the proposed approach, while a control group of 49 students learned with the traditional PFCT (T-PFCT) approach. The experimental results show that the proposed approach significantly enhanced the students’ computational thinking awareness, learning motivation, and self-efficacy, while not having significant impacts on their computer programming knowledge test scores.},
 author = {Jian-Wen Fang and Dan Shao and Gwo-Jen Hwang and Shao-Chen Chang},
 doi = {10.1177/07356331211060470},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331211060470},
 journal = {Journal of Educational Computing Research},
 number = {5},
 pages = {1301–1324},
 title = {From Critique to Computational Thinking: A Peer-Assessment-Supported Problem Identification, Flow Definition, Coding, and Testing Approach for Computer Programming Instruction},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331211060470},
 volume = {60},
 year = {2022h}
}

@article{doi:10.1177/0735633121992479,
 abstract = {Recently educational robotics has expanded into curriculum beyond traditional STEM fields, and which can also be used to foster computational thinking (CT) skills. Prior research has shown numerous interdisciplinary benefits related to CT, however, these influential factors have often been investigated with relatively few variables. This study investigated factors that may lead to 4th and 5th grade elementary school students’ development of computational thinking skills in collaborative robotics activities by hypothesizing a model which proposed that a problem solving inventory, intrinsic motivation, and enjoyment were the main predictors of computational thinking skills. The model was then tested by surveying students with several psychometric inventories where a revised model was then constructed. The study found significant relationships between perceived competence and enjoyment, and learning motivation, and intrinsic motivation. Another important finding was that problem solving was a significant predictor of computational thinking skills. Results were interpreted with reference to implications for possible means of improving learning outcomes when using collaborative robotics in an educational setting.},
 author = {William H. Stewart and Youngkyun Baek and Gina Kwid and Kellie Taylor},
 doi = {10.1177/0735633121992479},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633121992479},
 journal = {Journal of Educational Computing Research},
 number = {6},
 pages = {1208–1239},
 title = {Exploring Factors That Influence Computational Thinking Skills in Elementary Students’ Collaborative Robotics},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633121992479},
 volume = {59},
 year = {2021r}
}

@article{doi:10.1177/0735633121992480,
 abstract = {This study proposed plugged and unplugged approaches for young students to simultaneously improve their interdisciplinary learning performance in English and Computational Thinking (CT). The plugged approach involved adopting educational robots to enhance CT and to provide English vocabulary and sentence practice via a board game. The unplugged version of the educational board game involved using a conventional board game without a computer, although it was designed for practicing CT as well as some foreign language vocabulary and conversational sentences. The results show that both approaches were helpful for simultaneously improving the students’ English proficiency of the target vocabulary and sentences, and their CT competence. The students’ foreign language learning anxiety during the English conversation in the plugged game was significantly lower than that of the students playing the unplugged game. On one hand, the cooperation tendency of the CT scale improved significantly for the students playing the unplugged game. On the other hand, the critical thinking of the CT scale improved significantly for those using the plugged approach. This research provides an innovation development and evaluation for plugged and unplugged approaches.},
 author = {Ting-Chia Hsu and Yi-Sian Liang},
 doi = {10.1177/0735633121992480},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633121992480},
 journal = {Journal of Educational Computing Research},
 number = {6},
 pages = {1184–1207},
 title = {Simultaneously Improving Computational Thinking and Foreign Language Learning: Interdisciplinary Media With Plugged and Unplugged Approaches},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633121992480},
 volume = {59},
 year = {2021j}
}

@article{doi:10.1177/0735633121992594,
 abstract = {Robotics education has gradually been emphasized in contemporary school curricula; however, assessment tools for robotics learning are still limited. Based on Bloom’s Taxonomy of educational objectives, this study aimed to develop the Robotics Learning Self-Efficacy Scale (RLSES) with a two-level construct of five dimensions for assessing students’ self-efficacy for learning robotics. A total of 181 elementary, junior high and senior high school students (5th–12th graders) with robotics learning experience were selected as the sample of this study. A questionnaire including 32 candidate items designed for the initial version of the RLSES was administered to the sample. An exploratory factor analysis was conducted and, finally, 16 items were drawn for the final RLSES under five subscales (Comprehension, Practice, Analysis, Application, and Collaboration), with a total explained variance of 85.28%. The Cronbach’s alpha reliability was .97 for the overall scale, ranging from .87 to .95 for the subscales. The inter-correlation analysis showed evidence of discriminant validity. Regression analysis results supported that Practice and Comprehension self-efficacy were significant predictors of Analysis, Application, and Collaboration self-efficacy, confirming the two-level (2 × 3) construct of the RLSES. Significant differences among school levels were found and are discussed.},
 author = {Meng-Jung Tsai and Ching-Yeh Wang and An-Hsuan Wu and Chun-Ying Hsiao},
 doi = {10.1177/0735633121992594},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633121992594},
 journal = {Journal of Educational Computing Research},
 number = {6},
 pages = {1056–1074},
 title = {The Development and Validation of the Robotics Learning Self-Efficacy Scale (RLSES)},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633121992594},
 volume = {59},
 year = {2021v}
}

@article{doi:10.1177/0735633121994070,
 abstract = {Due to the interdisciplinary nature of robotics, more and more attention has been paid to its effectiveness in the field of education in recent years. This systematic review evaluated existing studies in improving K-12 students’ computational thinking and STEM attitudes. Research articles published between 2010 and 2019 were collated from major databases according to six criteria, and 17 studies were eligible. A meta-analysis was conducted to evaluate the effectiveness of educational robots in terms of standardized mean differences (SMD) or mean differences (MD) of test scores as outcome measures. The overall effect size was medium (SMD = 0.46, 95% CI: 0.23–0.69). Subgroup analysis found that some groups to have better effectiveness. Specifically, the effect of STEM attitudes (SMD = 0.01) was smaller than computational thinking (SMD = 0.48). Educational robots had more significant effect on boys (MD = 0.39) than girls (MD = 0.27). The effect in primary school (SMD = 0.27) was higher than in middle school (SMD = 0.04), and the effect was great on short-term instruction with educational robots (SMD = 0.35). Based on these results, the study makes some recommendations for educators about strengthening the influence of educational robots on STEM attitudes, improving the persistence of their learning effects, and further exploring their application models.},
 author = {Yanjun Zhang and Ronghua Luo and Yijin Zhu and Yuan Yin},
 doi = {10.1177/0735633121994070},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633121994070},
 journal = {Journal of Educational Computing Research},
 number = {7},
 pages = {1450–1481},
 title = {Educational Robots Improve K-12 Students’ Computational Thinking and STEM Attitudes: Systematic Review},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633121994070},
 volume = {59},
 year = {2021w}
}

@article{doi:10.1177/0735633121997360,
 abstract = {Although the current landscape in education emphasizes the importance of developing students’ information literacy in formal education settings, little attention has been paid to information literacy within the context of social media use. This study investigated the relationship between information literacy and social media competence (SMC) among 1843 university students. This was done in order to increase knowledge of the components that may be important for preparing university students to be information literate citizens in social media environments. Students’ information literacy and SMC were measured by the Student Information Literacy Test and the SMC-CS scale respectively. Correlation and regression analyses were utilized to explore the relationship between university students’ information literacy and their SMC. The results showed that university students’ ability to utilize information technology to solve problems, and their sense of responsible behavior in cyberspace, are the most critical factors in predicting students’ SMC. Based on the findings, theoretical and practical implications are discussed in terms of enhancing university students’ information literacy and SMC.},
 author = {Sha Zhu and Harrison Hao Yang and Di Wu and Feixiong Chen},
 doi = {10.1177/0735633121997360},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633121997360},
 journal = {Journal of Educational Computing Research},
 number = {7},
 pages = {1425–1449},
 title = {Investigating the Relationship Between Information Literacy and Social Media Competence Among University Students},
 url = {https://doi-org.crai.referencistas.com/10.1177/0735633121997360},
 volume = {59},
 year = {2021t}
}

@article{doi:10.1177/07356331221081484,
 abstract = {The evolving digital world requires scientifically literate citizens who are able to critically evaluate Internet sources of varying credibility. Instruction on evidence evaluation in postsecondary education often focuses on peer-review as a singular indicator of credibility. With increased access to web-based scientific information, students must also learn to think critically in real-time about the dimensions of credibility. This study describes the integration of sInvestigator, a computational evidence-based scientific reasoning tool, with a class of 32 students in an undergraduate honors course focused on socio-scientific issues. A cross-disciplinary team of researchers with expertise in science education, scientific literacy, and evidence evaluation developed and implemented an online questionnaire to measure students’ development of digital scientific literacy. After using sInvestigator to evaluate sources of scientific evidence based on publisher reputation, author competence, and author objectivity, students were better able to assess the credibility of online information. Results of this study also confirm the potential to authentically assess students’ use of author and publisher information to evaluate digital scientific sources. The need for further research on the operationalization and measurement of digital scientific literacy is discussed.},
 author = {Nancy Holincheck and Terrie M. Galanti and James Trefil},
 doi = {10.1177/07356331221081484},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331221081484},
 journal = {Journal of Educational Computing Research},
 number = {7},
 pages = {1796–1817},
 title = {Assessing the Development of Digital Scientific Literacy With a Computational Evidence-Based Reasoning Tool},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331221081484},
 volume = {60},
 year = {2022k}
}

@article{doi:10.1177/07356331221081753,
 abstract = {With the increasing importance of Computational Thinking (CT) at all levels of education, it is essential to have valid and reliable assessments. Currently, there is a lack of such assessments in upper primary school. That is why we present the development and validation of the competent CT test (cCTt), an unplugged CT test targeting 7–9 year-old students. In the first phase, 37 experts evaluated the validity of the cCTt through a survey and focus group. In the second phase, the test was administered to 1519 students. We employed Classical Test Theory, Item Response Theory, and Confirmatory Factor Analysis to assess the instruments’ psychometric properties. The expert evaluation indicates that the cCTt shows good face, construct, and content validity. Furthermore, the psychometric analysis of the student data demonstrates adequate reliability, difficulty, and discriminability for the target age groups. Finally, shortened variants of the test are established through Confirmatory Factor Analysis. To conclude, the proposed cCTt is a valid and reliable instrument, for use by researchers and educators alike, which expands the portfolio of validated CT assessments across compulsory education. Future assessments looking at capturing CT in a more exhaustive manner might consider combining the cCTt with other forms of assessments.},
 author = {Laila El-Hamamsy and María Zapata-Cáceres and Estefanía Martín Barroso and Francesco Mondada and Jessica Dehler Zufferey and Barbara Bruno},
 doi = {10.1177/07356331221081753},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331221081753},
 journal = {Journal of Educational Computing Research},
 number = {7},
 pages = {1818–1866},
 title = {The Competent Computational Thinking Test: Development and Validation of an Unplugged Computational Thinking Test for Upper Primary School},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331221081753},
 volume = {60},
 year = {2022c}
}

@article{doi:10.1177/07356331221087773,
 abstract = {Fostering students’ computer programming skills has become an important educational issue in the globe. However, it remains a challenge for students to understand those abstract concepts when learning computer programming, implying the need to provide instant learning diagnosis and feedback in computer programming activities. In this study, a Two-Tier Test-Based Programming Training (T3PT) approach was proposed. Accordingly, an online learning system was developed to provide students with precision feedback for guiding them to identify misconceptions of computer programming to improve their computer programming learning achievement. In order to examine the effects of the proposed approach, a learning system was developed and a quasi-experiment was conducted. Two classes of 99 eighth-grade students from Taiwan were divided into an experimental group and a control group. The students in the experimental group used the learning system based on the T3PT approach, while the control group used the conventional learning system. The experimental results showed that the proposed approach was significantly superior to the conventional programming learning approach in terms of students’ programming logic concepts, problem-solving awareness, technology acceptance, and satisfaction with the learning approach. Accordingly, discussion and suggestions are provided for future research.},
 author = {Gwo-Jen Hwang and Li-Hsien Tung and Jian-Wen Fang},
 doi = {10.1177/07356331221087773},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331221087773},
 journal = {Journal of Educational Computing Research},
 number = {8},
 pages = {1895–1917},
 title = {Promoting Students’ Programming Logic and Problem-Solving Awareness With Precision Feedback: A Two-Tier Test-Based Online Programming Training Approach},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331221087773},
 volume = {60},
 year = {2023f}
}

@article{doi:10.1177/07356331221098793,
 abstract = {The importance of computational thinking (CT) as a 21st-century skill for future generations has been a key consideration in the reforms of many national and regional educational systems. Much attention has been paid to integrating CT into the traditional subject classrooms. This paper describes a scoping review of learning tools for integrating CT and mathematics in current empirical studies published from 2015 to 2021. The review showed that most of the studies implemented CT-intensive Math-connected integration. Five major types of CT tools had been identified, i.e., digital tangibles, apps and games, programming languages, formative or summative assessments, and other technological tools. In many instances, the tools also provide functions of assessment of CT skills. The most assessed CT competencies were including algorithms and algorithmic thinking, abstraction, testing and debugging, loops, and sequences. Geometry and Measurement was the most assessed mathematics topic. Our scoping review is beneficial in the investigation of the literature on CT and mathematics education, as well as guides those who are interested in developing curriculum, programs, or assessments that involve the integration of CT and mathematics.},
 author = {Shiau-Wei Chan and Chee-Kit Looi and Weng Kin Ho and Mi Song Kim},
 doi = {10.1177/07356331221098793},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331221098793},
 journal = {Journal of Educational Computing Research},
 number = {8},
 pages = {2036–2080},
 title = {Tools and Approaches for Integrating Computational Thinking and Mathematics: A Scoping Review of Current Empirical Studies},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331221098793},
 volume = {60},
 year = {2023d}
}

@article{doi:10.1177/07356331221100740,
 abstract = {This meta-analysis determined game-based learning’s (GBL) overall effect on students’ computational thinking (CT) and tested for moderators, using 28 effect sizes from 24 studies of 2,134 participants. The random effects model results showed that GBL had a significant positive overall effect on students’ CT (g = 0.677, 95% confidence interval 0.532–0.821) with significant heterogeneity among effect sizes. Among game types, role-playing yielded the largest GBL effect size, followed by action, puzzles, and adventures. Moreover, the effect of GBL on CT was weaker among students in countries that were more individualistic than others. Lastly, interventions between four hours and one week showed the largest GBL effect size, followed by those over four weeks, up to four hours, and between one week and four weeks.},
 author = {Zhuotao Lu and Ming M. Chiu and Yunhuo Cui and Weijie Mao and Hao Lei},
 doi = {10.1177/07356331221100740},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331221100740},
 journal = {Journal of Educational Computing Research},
 number = {1},
 pages = {235–256},
 title = {Effects of Game-Based Learning on Students’ Computational Thinking: A Meta-Analysis},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331221100740},
 volume = {61},
 year = {2023o}
}

@article{doi:10.1177/07356331221102312,
 abstract = {Block programming has been suggested as a way of engaging young learners with the foundations of programming and computational thinking in a syntax-free manner. Indeed, syntax errors—which form one of two broad categories of errors in programming, the other one being logic errors—are omitted while block programming. However, this does not mean that errors are omitted at large in such environments. In this exploratory case study of a learning environment for early programming (Kodetu), we explored errors in block programming of middle school students (N = 123), using log files drawn from a block-based online. Analyzing 1033 failed executions, we found that errors may be driven by either learners’ knowledge and behavior, or by the learning environment design. The rate of error types was not associated with the learners’ and contextual variables examined, with the exception of task complexity (as defined by SOLO taxonomy). Our findings highlight the importance of learning from errors and of learning environment design.},
 author = {Anat Ben-Yaacov and Arnon Hershkovitz},
 doi = {10.1177/07356331221102312},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331221102312},
 journal = {Journal of Educational Computing Research},
 number = {1},
 pages = {178–207},
 title = {Types of Errors in Block Programming: Driven by Learner, Learning Environment},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331221102312},
 volume = {61},
 year = {2023c}
}

@article{doi:10.1177/07356331221106918,
 abstract = {Higher-order thinking skills (HOTS) are reliable predictors of success in school and the workplace. A typical technique for encouraging higher-order thinking is to use instructional design interventions that engage learners in simple cognitive activities. Business simulation game (BSG) is one of the types of interactive learning environments that can increase HOTS. In addition, student engagement and attitude toward technology use are considered strong influences on HOTS. The study explored the effect of using a BSG on HOTS and student engagement. It examined the influence of attitude toward the use of a BSG on HOTS and student engagement. The results of the t-test analysis showed that learning activities using BSG had a positive effect on HOTS and student engagement. Additionally, PLS-SEM analysis results indicated that immersion, interaction, and intention to use the game influence student engagement. Furthermore, student engagement significantly influenced HOTS. The findings indicate that students must demonstrate that they are engaging actively in a course to improve HOTS and that a BSG can be a valuable and effective tool for promoting engagement. Moreover, the COVID-19 pandemic caused limitations in sampling and representativeness of respondents. Future research should involve a bigger sample size and students who have attended related courses.},
 author = {Yueh-Min Huang and Lusia M. Silitonga and Astrid T. Murti and Ting-Ting Wu},
 doi = {10.1177/07356331221106918},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331221106918},
 journal = {Journal of Educational Computing Research},
 number = {1},
 pages = {96–126},
 title = {Learner Engagement in a Business Simulation Game: Impact on Higher-Order Thinking Skills},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331221106918},
 volume = {61},
 year = {2023n}
}

@article{doi:10.1177/07356331221114183,
 abstract = {Computational thinking is believed to be beneficial for Science, Technology, Engineering, and Mathematics (STEM) learning as it is closely related to many other skills required by STEM disciplines. There has been an increasing interest in integrating computational thinking into STEM and many studies have been conducted to examine the effects of this intervention. This meta-analysis examined the effects of computational thinking integration in STEM on students’ STEM learning performance in the K-12 education context. Following systematic procedures, we identified 20 publications with 21 studies meeting the inclusion and exclusion criteria from a range of academic databases. We extracted effect sizes on student learning outcomes in one-group pretest-posttest designs. We also examined a range of moderating variables in the models, including student levels, STEM disciplines, intervention durations, alignment with content standards (e.g., CSTA/NGSS), types of intervention (e.g., simulation), and the use of unplugged/plugged activities. Overall, we found a statistically significant large effect size (g = 0. 85 [95% CI of 0.57–1.14]; p < .001), indicating a large overall effect of computational thinking integration on STEM learning outcomes. The effect sizes were significantly moderated by intervention durations. We provide a discussion of the findings and present implications for future research and practice.},
 author = {Li Cheng and Xiaoman Wang and Albert D. Ritzhaupt},
 doi = {10.1177/07356331221114183},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331221114183},
 journal = {Journal of Educational Computing Research},
 number = {2},
 pages = {416–443},
 title = {The Effects of Computational Thinking Integration in STEM on Students’ Learning Performance in K-12 Education: A Meta-analysis},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331221114183},
 volume = {61},
 year = {2023f}
}

@article{doi:10.1177/07356331221115661,
 abstract = {Creativity, one of the cornerstones of students’ 21st-century skills, is regarded as an important learning outcome of science, technology, engineering, arts, and mathematics (STEAM) education. Meanwhile, problem-based digital making (DM), which combines the child-friendly programming activities of DM with problem-solving elements, is an emerging instructional design to facilitate STEAM learning. This qualitative case study examines the implementation of a problem-based DM instructional program that used the block-based programming tool Scratch to cultivate the participants’ creativity. Fifty-four middle school students (aged 10–14 years) in Hong Kong participated in the program, which totaled 10 contact hours over five consecutive weeks. Through triangulating students’ DM artifacts, video recordings, field notes, and interviews, the researchers characterized the students’ creative expression, examined the role of problem-based DM in encouraging creative work, and investigated the use of Scratch for mediating student creativity. The results showed that problem-based DM activities fostered students’ creative expressions in the dimensions of novelty, utility, aesthetics, and authenticity. While Scratch mediated the way the students presented their solutions, it had limitations that hindered the students’ digital artifact construction. The findings provide theoretical insights for framing creativity and offer practical implications for the implementation of problem-based DM in K–12 contexts.},
 author = {Xiaojing Weng and Oi-Lam Ng and Zhihao Cui and Suzannie Leung},
 doi = {10.1177/07356331221115661},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331221115661},
 journal = {Journal of Educational Computing Research},
 number = {2},
 pages = {304–328},
 title = {Creativity Development With Problem-Based Digital Making and Block-Based Programming for Science, Technology, Engineering, Arts, and Mathematics Learning in Middle School Contexts},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331221115661},
 volume = {61},
 year = {2023u}
}

@article{doi:10.1177/07356331221121052,
 abstract = {Computational thinking (CT) is an emerging and multifaceted competence important to the computing era. However, despite the growing consensus that CT is a competence domain, its theoretical and empirical account remain scarce in the current literature. To address this issue, rigorous psychometric evaluation procedures were adopted to investigate the structure of CT competency, as measured by Computational Thinking Challenge (Lai, 2021a), in a large sample of 1,130 British secondary school students (Mage = 14.14 years, SDage = 1.45). Based on model comparison from an exploratory multidimensional item response theory approach, the results supported the multidimensional operationalization of CT competency. A confirmatory bi-factor item response theory model further suggested CT competency is comprised of a general CT competency factor and two specific factors for programming and non-programming problem-solving. Despite the multidimensionality, the common variance is largely explained by a primary general factor of CT competency, thus the use of a single scale score is recommended. Psychometric evaluation from the bi-factor model indicated good psychometric properties of the assessment tool. Overall, the bi-factor model provides a useful approach to investigating CT competency and serves as a robust test validation tool.},
 author = {Rina PY Lai and Michelle R Ellefson},
 doi = {10.1177/07356331221121052},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331221121052},
 journal = {Journal of Educational Computing Research},
 number = {2},
 pages = {259–282},
 title = {How Multidimensional is Computational Thinking Competency? A Bi-Factor Model of the Computational Thinking Challenge},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331221121052},
 volume = {61},
 year = {2023i}
}

@article{doi:10.1177/07356331221121106,
 abstract = {This study investigates how digital game co-creation promotes Computational Thinking (CT) skills among children in sub-urban primary schools. Understanding how CT skills can be fostered in learning programming concepts through co-creating digital games is crucial to determine instructional strategies that match the young students’ interests and capacities. The empirical study has successfully produced a new checklist that can be used as a tool to describe the learning of CT skills when children co-create digital games. The checklist consists of 10 core CT skills: abstraction, decomposition, algorithmic thinking, generalisation, representation, socialisation, code literacy, automation, coordination, and debugging. Thirty-six 10–12 year-olds from sub-urban primary schools in Borneo participated in creating games in three separate eight-hour sessions. In addition, one pilot session with five participants was conducted. The game co-creation process was recorded to identify and determine how these young, inexperienced, untrained young learners collaborated while using CT skills. Analysis of their narratives while co-creating digital games revealed a pattern of using CT while developing the games. Although none of the groups demonstrated the use of all ten CTs, conclusively, all ten components of the CT were visibly present in their co-created digital games.},
 author = {Mohd Kamal Othman and Syazni Jazlan and Fatin Afiqah Yamin and Shaziti Aman and Fitri Suraya Mohamad and Nurfarahani Norman Anuar and Abdulrazak Yahya Saleh and Ahmad Azaini Abdul Manaf},
 doi = {10.1177/07356331221121106},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331221121106},
 journal = {Journal of Educational Computing Research},
 number = {2},
 pages = {355–389},
 title = {Mapping Computational Thinking Skills Through Digital Games Co-Creation           Activity Amongst Malaysian Sub-urban Children},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331221121106},
 volume = {61},
 year = {2023n}
}

@article{doi:10.1177/07356331221133560,
 abstract = {Reintroducing computer science (CS) education in K–12 schools to promote computational thinking (CT) has attracted significant attention among scholars and educators. Among the several essential components included in CS and CT education, program debugging is an indispensable skill. However, debugging teaching has often been overlooked in K–12 contexts, and relevant empirical studies are lacking in the literature. Moreover, novices generally have poor performance in domain knowledge and strategic knowledge concerning debugging. They also consistently experience a high cognitive burden in debugging learning. To address these gaps, we developed a flipped systematic debugging approach combined with a systematic debugging process (SDP) and the modeling method. A quasi-experimental study was conducted to explore the effectiveness of this flipped systematic debugging approach, in which 83 fifth-grade students attended the flipped debugging training lessons with the SDP–modeling method, and 75 fifth-grade students attended the unassisted flipped debugging training lessons without the SDP–modeling method. The results indicated that flipped debugging training using the SDP–modeling method improved students’ debugging skills. The results from the questionnaire showed that the proposed teaching approach increased the students’ investment in germane cognitive load by promoting schema construction. It also helped reduce students’ intrinsic and extraneous cognitive load in learning.},
 author = {Xuemin Gao and Khe Foon Hew},
 doi = {10.1177/07356331221133560},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331221133560},
 journal = {Journal of Educational Computing Research},
 number = {5},
 pages = {1064–1095},
 title = {A Flipped Systematic Debugging Approach to Enhance Elementary Students’ Program Debugging Performance and Optimize Cognitive Load},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331221133560},
 volume = {61},
 year = {2023f}
}

@article{doi:10.1177/07356331221133822,
 abstract = {Underpinning the teaching of coding with Computational Thinking has proved relevant for diverse learners, particularly given the increasing demand in upskilling for today’s labour market. While literature on computing education is vast, it remains unexplored how existing CT conceptualisations relate to the learning opportunities needed for a meaningful application of coding in non-Computer Scientists’ lives and careers. In order to identify and organise the learning opportunities in the literature about CT, we conducted a configurative literature review of studies published on Web of Science, between 2006 and 2021. Our sample gathers 34 papers and was analysed on NVivo to find key themes. We were able to organise framings of CT and related learning opportunities into three dimensions: functional, collaborative, and critical and creative. These dimensions make visible learning opportunities that range from individual cognitive development to interdisciplinary working with others, and to active participation in a technologically evolving society. By comparing and contrasting frameworks, we identify and explain different perspectives on skills. Furthermore, the three-dimensional model can guide pedagogical design and practice in coding courses.},
 author = {Ana Melro and Georgie Tarling and Taro Fujita and Judith Kleine Staarman},
 doi = {10.1177/07356331221133822},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331221133822},
 journal = {Journal of Educational Computing Research},
 number = {4},
 pages = {901–924},
 title = {What Else Can Be Learned When Coding? A Configurative Literature Review of Learning Opportunities Through Computational Thinking},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331221133822},
 volume = {61},
 year = {2023m}
}

@article{doi:10.1177/07356331221134423,
 abstract = {Most studies suggest that students develop computational thinking (CT) through learning programming. However, when the target of CT is decoupled from programming, emerging evidence challenges the assertion of CT transferability from programming. In this study, CT was operationalized in everyday problem-solving contexts in a learning experiment (n = 59) that investigated whether learning programming enhances students’ CT skills. Specifically, this study examined the influence of a novel, systematic and micro instructional strategy that is grounded in abstraction and comprised of four independent but related processes – discover, extract, create, and assemble (DECA) towards simplification of problem-solving. Subsidiary questions explored the effects of students’ age, gender, computer proficiency, and prior programming experience on the development of CT. No significant difference was found between the CT skill and programming knowledge of the groups at the posttest. However, within-group paired t-tests showed that the experimental group that integrated DECA had significant improvement in CT but not in the control group across the pretest-posttest axis. Implications of the inconclusive finding about the transfer of programming skills to CT are emphasized and the arguments for disentangling CT from programming are highlighted.},
 author = {Ndudi O. Ezeamuzie},
 doi = {10.1177/07356331221134423},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331221134423},
 journal = {Journal of Educational Computing Research},
 number = {3},
 pages = {605–638},
 title = {Abstractive-Based Programming Approach to Computational Thinking: Discover, Extract, Create, and Assemble},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331221134423},
 volume = {61},
 year = {2023d}
}

@article{doi:10.1177/07356331221143832,
 abstract = {This study implemented and evaluated the innovative use of a performance-based assessment platform to support the development of self-regulated learning (SRL) in senior primary students as they completed programming tasks. We embedded SRL support features into a performance-based assessment platform as scaffolding to help the students implement problem-solving strategies. A mixed-methods approach was adopted to evaluate the intervention. The students’ perceptions of their SRL skills after working through the programming tasks were measured by a survey of 45 students. The quantitative results suggested that the students benefited from the performance-based assessment platform in developing their SRL skills. A thematic analysis of interview data from 20 students further indicated that the embedded SRL scaffolding and automatic marking function helped them to solve the programming tasks. The results demonstrate that a well-designed performance-based assessment platform with embedded SRL support can be an effective tool for developing students’ SRL. The qualitative results further revealed that algorithmic thinking is an aspect of programming for which students need more SRL support.},
 author = {Siu-Cheung Kong and Bowen Liu},
 doi = {10.1177/07356331221143832},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331221143832},
 journal = {Journal of Educational Computing Research},
 number = {5},
 pages = {977–1007},
 title = {Supporting the Self-Regulated Learning of Primary School Students With a Performance-Based Assessment Platform for Programming Education},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331221143832},
 volume = {61},
 year = {2023f}
}

@article{doi:10.1177/07356331231151393,
 abstract = {Serious games are a growing field in academic research and they are considered an effective tool for education. Game-based learning invokes motivation and engagement in students resulting in effective instructional outcomes. An essential aspect of a serious game is the method of support for presenting the teaching material and providing feedback. A support design that evaluates students’ progress and adapts accordingly, has the potential of producing better learning results. This paper presents an adaptive model based on fuzzy logic that adjusts the support acquisition according to student knowledge level. A serious game for teaching the concepts of sequence and iteration in programming to novice students was built to assess the model. It employs working examples as a support method since previous research indicated that it produced less cognitive load during problem-solving. An empirical study with 102 students has been conducted to evaluate the learning efficiency of the model. The analysis indicates positive results and a potential solution for balancing the amount of assistance in serious games.},
 author = {Pavlos Toukiloglou and Stelios Xinogalos},
 doi = {10.1177/07356331231151393},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331231151393},
 journal = {Journal of Educational Computing Research},
 number = {4},
 pages = {766–789},
 title = {Adaptive Support With Working Examples in Serious Games About Programming},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331231151393},
 volume = {61},
 year = {2023r}
}

@article{doi:10.1177/07356331231160294,
 abstract = {Computational thinking is a way of thinking that helps people “think like a computer scientist” to solve practical problems. However, practicing computational thinking through programming is dependent on the problem solvers’ metacognition. This study investigated students’ metacognitive planning and problem-solving performance in programming through two quantitative studies. First, we analyzed the performance of metacognitive planning and of problem solving through the programming of 21 freshmen, and found that the metacognitive planning performance related to “problem description” and “program comprehension” was significantly correlated with problem-solving performance. Second, semi-scaffolding and full-scaffolding were designed based on the first study. Another 89 freshmen were randomly divided into three groups and were asked to write their programming plan with no-scaffolding, semi-scaffolding, or with full-scaffolding. ANCOVA revealed that the problem-solving performance of the no-scaffolding group was significantly weaker than that of the other two groups, but there was no significant difference between the semi-scaffolding and the full-scaffolding groups. The study indicated that semi-scaffolding had a similar effect to full-scaffolding on problem-solving performance. The study suggests that teachers should emphasize supporting students’ “problem description” and “program comprehension” using semi-scaffolding. This scaffolding technique is sufficient and efficient for training students’ computational thinking through problem solving in programming.},
 author = {Ying Zhou and Ching Sing Chai and Xiuting Li and Chao Ma and Baoping Li and Ding Yu and Jyh-Chong Liang},
 doi = {10.1177/07356331231160294},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331231160294},
 journal = {Journal of Educational Computing Research},
 number = {6},
 pages = {1123–1142},
 title = {Application of Metacognitive Planning Scaffolding for the Cultivation of Computational Thinking},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331231160294},
 volume = {61},
 year = {2023t}
}

@article{doi:10.1177/07356331231165099,
 abstract = {The current study aims to determine the effect of teaching a mechanic neuro-computerized course through virtual learning environments (VLE) to develop computational thinking among mathematics pre-service teachers. The neuro-computerized virtual learning environments (NCVLE) model was designed to be used to teach the mechanics course to third-year students of the mathematics department. To achieve the targeted learning outcomes, the study recruited (102) third-year students of the Faculty of Education and classified them into a control group of (50) students and an experimental group of (52) students. The experiment lasted for 14 weeks during one semester of the 2021-2022 academic year. The results agreed with most of what has been found from relevant literature and studies. Also, the results indicated that the NCVLE model played a vital role in the purposeful teaching, learning, and assessment processes and enhanced the learning of computational thinking.},
 author = {Yousri Attia Mohamed Abouelenein and Mohamed Ali Nagy Elmaadaway},
 doi = {10.1177/07356331231165099},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331231165099},
 journal = {Journal of Educational Computing Research},
 number = {6},
 pages = {1175–1206},
 title = {Impact of Teaching a Neuro-Computerized Course Through VLE to Develop Computational Thinking Among Mathematics Pre-service Teachers},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331231165099},
 volume = {61},
 year = {2023a}
}

@article{doi:10.1177/07356331231170382,
 abstract = {Computer programming is a difficult course for many students. Prior works advocated for group learning pedagogies in pursuit of higher-level reasoning and conceptual understanding. However, the methodological gaps in existing implementations warrant further research. This study conducted a three-armed cluster-randomized controlled trial to comparatively evaluate the social and cognitive effects of group learning pedagogies in computer programming. Following an apprenticeship model, each group has a designated master: drivers in pair programming (PP), peer leaders in peer-led team learning (PLTL), and practitioners in practitioner-assisted group learning (PAGL). In all course deliverables, the PP group received the lowest mean scores. Meanwhile, no significant difference was found between the PLTL and PAGL groups. Except for psychological safety, social factors such as task cohesion, interdependence, and group potency were significantly different between the groups. Both PLTL and PAGL groups reported a significant increase in social factors after 14 weeks of intervention. These findings provide a rationale for educational leaders and teachers to formulate curricular plans that integrate PLTL and PAGL in computer programming education. Overall, this study contributes to the literature on group learning, expands the pedagogies in computer programming, and serves as additional empirical evidence on cognitive apprenticeship and sociocultural perspectives of learning.},
 author = {Manuel B. Garcia},
 doi = {10.1177/07356331231170382},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331231170382},
 journal = {Journal of Educational Computing Research},
 number = {6},
 pages = {1207–1231},
 title = {Facilitating Group Learning Using an Apprenticeship Model: Which Master is More Effective in Programming Instruction?},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331231170382},
 volume = {61},
 year = {2023i}
}

@article{doi:10.1177/07356331231170383,
 abstract = {Technology has become an integral part of teaching and learning, but there is still limited understanding of how it is utilized to support computing education in early childhood. To address this knowledge gap, this review investigated the current implementation of computing technologies in early childhood settings, the implementation of computing activities, the learning outcomes achieved by students, and the utilization of assessment strategies to evaluate student learning. Through a systematic review and synthesis of 31 empirical studies published between 2014 and 2020, this review identified: (1) Twenty-two computing technologies that feature three types of computing environments; (2) Eight types of learning activities to engage children in computing; (3) A variety of learning outcomes accomplished in cognitive and non-cognitive dimensions; (4) A wide range of assessment strategies to evaluate students’ outcomes in different dimensions. This review strengthened the evidence base for the benefits of teaching computing with technology to children, informed the design of age-appropriate computing technology and learning activities, and identified research gaps to inform future research. Implications were provided to inform the future design and delivery of computing instruction to early childhood learners.},
 author = {Ruohan Liu and Feiya Luo and Maya Israel},
 doi = {10.1177/07356331231170383},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331231170383},
 journal = {Journal of Educational Computing Research},
 number = {6},
 pages = {1275–1311},
 title = {Technology-Integrated Computing Education in Early Childhood: A Systematic Literature Review},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331231170383},
 volume = {61},
 year = {2023h}
}

@article{doi:10.1177/07356331231171622,
 abstract = {Pattern recognition is an important skill in computational thinking. In this study, an equation puzzle game was developed by combining pattern recognition with algebraic reasoning, and scaffolding was designed to support learners’ learning. Sixty participants were enrolled in this study, divided into a control group and an experimental group to compare the results and differences in game achievement, flow, anxiety, and motivation of participants with and without algebraic reasoning scaffolding. The results of the study showed that the participants in both groups had positive flow and motivation during the game, did not feel over-anxious, and there was no significant difference in the game achievement of the two groups. In addition, the game with the scaffolding may have the potential to make a positive correlation between game achievement and psychological status. The results of this study indicated that the game did not cause too much anxiety to the participants. The scaffolding-based design achieves the intended effect on the participants’ assistance and facilitates the participants’ engagement in pattern recognition problem solving. And as learners became more focused and engaged, they could also perform better in the game. This game mechanism can be used as a reference for designing pattern recognition games.},
 author = {Yu-Chi Chen and Chi-Yu Chao and Huei-Tse Hou},
 doi = {10.1177/07356331231171622},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331231171622},
 journal = {Journal of Educational Computing Research},
 number = {6},
 pages = {1232–1251},
 title = {Learning Pattern Recognition Skills From Games: Design of an Online Pattern Recognition Educational Mobile Game Integrating Algebraic Reasoning Scaffolding},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331231171622},
 volume = {61},
 year = {2023h}
}

@article{doi:10.1177/07356331231174454,
 abstract = {This study explores the effects of worked-out examples and metacognitive scaffolding on novice learners’ knowledge performance, cognitive loads, and self-regulation skills in problem-solving programming. 126 undergraduate students in a computer programming fundamentals course were randomly assigned to one of four groups: 1) task performance with a traditional WOE (TW), 2) task performance with a faded WOE (FW), 3) task performance with traditional WOE and metacognitive scaffolding (TWM), and 4) task performance with a faded WOE and metacognitive scaffolding (FWM). Over the course of 3 weeks, participants in these four groups were asked to solve programming problems using Python with WOE and metacognitive scaffolding. The results demonstrate that the provision of metacognitive scaffolding with faded WOE (FWM) is the most effective for problem-solving programming and self-regulation skills. In addition, an interaction effect exists between the two treatments for the germane load in FWM. Therefore, results in this study provide empirical insights into ways to effectively apply WOE and metacognitive scaffolding to problem-solving processes for programming-based complex problem-solving, especially for novice learners.},
 author = {Yoonhee Shin and Jaewon Jung and Joerg Zumbach and Eunseon Yi},
 doi = {10.1177/07356331231174454},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331231174454},
 journal = {Journal of Educational Computing Research},
 number = {6},
 pages = {1312–1331},
 title = {The Effects of Worked-Out Example and Metacognitive Scaffolding on Problem-Solving Programming},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331231174454},
 volume = {61},
 year = {2023p}
}

@article{doi:10.1177/07356331231174929,
 abstract = {Coding is increasingly popular in schools around the world and is often taught by non-specialist teachers as an integrated task with other subject areas. In this article, we explore the relationship between computer science (CS) concepts and students’ multimodal expression in a coding animated narrative (CAN) task in the context of an integrated English-Technology unit of learning. Through this collective case study, we explore how CS concepts underpin semiotic elements of an animated narrative, analyse the factors that influence the extent to which students exercise those concepts, and reveal the tensions and opportunities that a CAN task may present for learning computer science concepts in regular, non-specialist, cross-curricular classrooms. The findings suggest that CAN tasks are unique in presenting opportunities for students to learn challenging CS concepts such as synchronisation and parallelism. At the same time, CAN tasks present tensions for teaching CS concepts in non-specialist classrooms, where student projects are often judged on their visual qualities. In such settings, procedural, rather than conceptual knowledge, may be a more efficient route to creative outcomes. It also means that drawing skills need to be prioritised. Role specialisation often led to better quality projects but at the expense of individual students’ conceptual development in computer science.},
 author = {Karen Woo and Garry Falloon},
 doi = {10.1177/07356331231174929},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331231174929},
 journal = {Journal of Educational Computing Research},
 number = {7},
 pages = {1335–1358},
 title = {The Search for Computer Science Concepts in Coding Animated Narratives: Tensions and Opportunities},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331231174929},
 volume = {61},
 year = {2023s}
}

@article{doi:10.1177/07356331231178948,
 abstract = {Computational thinking (CT) education has drawn increasing attention from educators and researchers. This study conducted a meta-analysis of 27 empirical studies to examine the effectiveness of game-based learning (GBL) for fostering students’ CT. The effects of various factors on the learning process for acquiring CT were also examined. The results showed that (a) conducting GBL can foster students’ CT, and the overall effect was at the upper-middle level (Hedges’ g = .600, 95% CI [.465, .735], p < .001). (b) Furthermore, conducting GBL can improve students’ CT concepts (Hedges’ g = .916, 95% CI [.410, 1.423], p < .001), CT skills (Hedges’ g = .494, 95% CI [.389, .600], p < .001), and CT perspectives (Hedges’ g = .927, 95% CI [.039, 1.816], p < .05). (c) Additionally, game mode, teaching context, and participant characteristics have positive effects on CT. Based on the findings, it is suggested that using more unplugged games and video games, designing collaborative game activities, and tailoring approaches according to gender difference and programming experience can effectively promote CT. The results have significance for fostering students’ CT in GBL; it is further suggested that instruction processes be rationally designed.},
 author = {Jingsi Ma and Yi Zhang and Zhifang Zhu and Sunan Zhao and Qiyun Wang},
 doi = {10.1177/07356331231178948},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331231178948},
 journal = {Journal of Educational Computing Research},
 number = {7},
 pages = {1430–1463},
 title = {Game-Based Learning for Students’ Computational Thinking: A Meta-Analysis},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331231178948},
 volume = {61},
 year = {2023o}
}

@article{doi:10.1177/07356331231180951,
 abstract = {Computational thinking (CT) is considered a fundamental skill that everyone in the 21st century should have. Game-based learning (GBL) may be used to teach CT, and it’s necessary to clarify how to design and implement game-based CT teaching. The literature was systematically searched for empirical studies published between 2011 and 2021. Thirty-nine studies were included in the review and findings suggested that GBL has positive effects on CT, but has non-significant effects on some CT elements (e.g., conditions, triggers, and abstraction) because of limited time to learn these elements and students’ preferences for using CT elements. Game elements, particularly clear goals and rules, progressive challenges, immediate feedback, storyline, avatar, social interaction and various reward mechanisms were used to motivate students to engage in activities to develop CT. Furthermore, single or multiple theoretical foundations, such as constructivist learning theory and experiential learning theory, may guide the design and implementation of game-based activities. Problem-solving, project-based approaches were used to encourage students to use CT to solve problems or complete a project. Finally, guidelines for designing and implementing game-based learning activities for promoting CT were discussed.},
 author = {Xinyue Wang and Mengmeng Cheng and Xinfeng Li},
 doi = {10.1177/07356331231180951},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331231180951},
 journal = {Journal of Educational Computing Research},
 number = {7},
 pages = {1505–1536},
 title = {Teaching and Learning Computational Thinking Through Game-Based Learning: A Systematic Review},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331231180951},
 volume = {61},
 year = {2023t}
}

@article{doi:10.1177/07356331231183450,
 abstract = {This article presents the design, construct validation, and reliability of a self-report instrument in Spanish that aims to characterize different types of strategies that students can use to learn computer programming. We provide a comprehensive overview of the identification of learning strategies in the existing literature, the design and development of preliminary questionnaire items, the refinement of item wording, and the examination of the internal structure and reliability of the final instrument. The construction of the items was based on the educational theory of Self-Regulated Learning. The final version of the questionnaire, called the Computer Programming Learning Strategies Questionnaire (CEAPC), was administered to 647 students enrolled in computer programming courses. The data collected from the participants were used to examine the construct validity and reliability of the questionnaire. The CEAPC consists of 13 subscales, each corresponding to a different type of learning strategy, and a total of 89 items. Statistical analyses of the data indicate that the CEAPC has adequate construct validity. In addition, the results of the internal consistency analysis indicate satisfactory reliability across the different subscales of the instrument. This study contributes to the field of educational research, particularly in the area of self-regulated learning in computer programming.},
 author = {Stephanie Torres Jiménez and Jhon Jairo Ramírez-Echeverry and Felipe Restrepo-Calle},
 doi = {10.1177/07356331231183450},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331231183450},
 journal = {Journal of Educational Computing Research},
 number = {8},
 pages = {103–138},
 title = {The Development and Validation of the Questionnaire to Characterize Learning Strategies in Computer Programming (CEAPC)},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331231183450},
 volume = {61},
 year = {2024t}
}

@article{doi:10.1177/07356331231193142,
 abstract = {In this article, we delve into a hermeneutic process that analyzes the term Computational Thinking as it was constructed through Wing’s series of iterations in conceptualization attempts (2006, 2008, 2011 and 2014). On the one hand, this brings us to analyze the relations and intersections between different process of thought (analytical, logical, mathematical, system, engineering, algorithmic) and the role of search for simplicity, generalization, and scalability in the layers of abstraction in Computational Thinking. On the other hand, we explore the roots and the discursive environment of authors that could contribute to this conceptualizing process in the years around its popularization following Jeannette Wing’s founding article in 2006. We have also included in our analysis the points of convergence with Seymour Papert’s work related to the computer-machine seen as an “object-to-think-with” helping to the construction of knowledge from an epistemological perspective to the computing-human. Even though a consensus on the definition of the concept has not yet ben encountered, the analysis helps to highlight the solid reference points that address what is at the core of Computational thinking and what should be the framework of educational interventions and research that revolve around it.},
 author = {Marta Peracaula-Bosch and Juan González-Martínez},
 doi = {10.1177/07356331231193142},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331231193142},
 journal = {Journal of Educational Computing Research},
 number = {8},
 pages = {139–158},
 title = {Towards a Hermeneutics of Computational Thinking in Wing’s Approximations},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331231193142},
 volume = {61},
 year = {2024l}
}

@article{doi:10.1177/07356331231201342,
 abstract = {Artificial intelligence (AI) has emerged as a prominent topic in K-12 education recently. However, pedagogical design has remained a major challenge, especially among young learners. Guided by the Zone of Proximal Development theory and AI education research literature, this design-based study proposes an analogy-based pedagogical approach to support AI teaching and learning in upper primary education. This pedagogical approach is centered on human–AI comparison, where humans are gradually shifted from an analogue to a contrast to make visible the attributes, mechanisms, and processes of AI. To evaluate its effectiveness, a quasi-experimental study with mixed methods was conducted. The quantitative comparison shows that the participants in the experimental group learning with the analogy-based pedagogical approach significantly outperformed their peers with the conventional direct instructional approach in all three dimensions of AI knowledge, skills, and ethical awareness. Qualitative analyses further reveal its pedagogical benefits, including demystifying AI through relatable and engaging learning, supporting student comprehension and skill mastery, and nurturing critical thinking and attitudes. The analogy-based approach contributes to the field of K-12 AI education with an age-appropriate, child-friendly pedagogical approach. Notably, AI education should prioritize teaching for student understanding, and AI should be recognized as an independent subject with interdisciplinary applications.},
 author = {Yun Dai and Ziyan Lin and Ang Liu and Dan Dai and Wenlan Wang},
 doi = {10.1177/07356331231201342},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331231201342},
 journal = {Journal of Educational Computing Research},
 number = {8},
 pages = {159–186},
 title = {Effect of an Analogy-Based Approach of Artificial Intelligence Pedagogy in Upper Primary Schools},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331231201342},
 volume = {61},
 year = {2024d}
}

@article{doi:10.1177/07356331231204653,
 abstract = {Although previous research has provided some insights into the effects of block-based and text-based programming modalities, there is a dearth of a detailed, multi-dimensional analysis of the transition process from different introductory programming modalities to professional programming learning. This study employed a quasi-experimental design to address this gap, involving 64 secondary school students in two groups. For the beginning five weeks, the first group used an introductory block-based programming environment, while the second group used an introductory text-based programming environment. Then, both groups transitioned to professional text-based programming for the subsequent eight weeks. The results showed that participants who transitioned from introductory text-based programming to professional text-based programming (1) significantly outperformed in computational thinking skills; (2) had more code-writing and debugging behaviors and fewer irrelevant behaviors, and (3) had more interactions with the instructor. No significant differences were observed between the two groups regarding enjoyment, confidence, and interest in programming. Drawing on these findings, this study proposes pedagogical implications that could facilitate the adoption of programming modalities within the broader context of STEM education.},
 author = {Dan Sun and Chengcong Zhu and Fan Xu and Yan Li and Fan Ouyang and Miaoting Cheng},
 doi = {10.1177/07356331231204653},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331231204653},
 journal = {Journal of Educational Computing Research},
 number = {3},
 pages = {647–674},
 title = {Transitioning From Introductory to Professional Programming in Secondary Education: Comparing Learners’ Computational Thinking Skills, Behaviors, and Attitudes},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331231204653},
 volume = {62},
 year = {2024n}
}

@article{doi:10.1177/07356331231205052,
 abstract = {Computational thinking (CT) has gained considerable attention and in-depth discussion over the last two decades. Although the significance of CT has been highlighted, it could be challenging for educators to teach CT. Fortunately, adopting robots in education has been evidenced to be of benefit to promoting students’ learning motivation, CT, and higher-order thinking skills. However, several significant factors affecting students’ programming performances in robot-assisted learning activities have been identified, such as cognitive needs and organization. In this study, a CMR-BBP (concept map robot block-based programming) approach was designed by integrating concept maps into robot block-based programming to enhance students’ programming learning. Moreover, a three-group experiment was carried out in an elementary school to evaluate their learning outcomes. The experimental results revealed that the CMR-BBP approach benefited the students’ perceptions of their computational thinking and problem solving in comparison with the R-BBP (robot block-based programming) and C-BBP (conventional block-based programming) approaches. Furthermore, regarding cognitive load, both the CMR-BBP and R-BBP approaches enhanced the students’ germane cognitive load, while the CMR-BBP approach effectively reduced their extrinsic cognitive load. This study could be a notable reference for designing other courses in conjunction with programming learning activities.},
 author = {Chih-Hung Chen and Hsiang-Yu Chung},
 doi = {10.1177/07356331231205052},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331231205052},
 journal = {Journal of Educational Computing Research},
 number = {1},
 pages = {406–427},
 title = {Fostering Computational Thinking and Problem-Solving in Programming: Integrating Concept Maps Into Robot Block-Based Programming},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331231205052},
 volume = {62},
 year = {2024e}
}

@article{doi:10.1177/07356331231206071,
 abstract = {In higher education, it is challenging to cultivate non-computer science majors’ programming concepts. This study used the GAME model (gamification, assessment, modeling, and enquiry) in a programming education course to enhance undergraduates’ self-efficacy and performance of basic programming concepts. There were 83 undergraduates taking part in this study, which adopted a quasi-experimental research design. Students in the experimental group (n = 43) experienced a course in which the GAME model was used to design the block-based programming course. The control group (n = 40) was given a general information education course covering similar learning concepts without the game-based learning strategy. The analysis of covariance (ANCOVA) was adopted to investigate the effect of the GAME model on students’ learning outcomes for the quantitative data. In the qualitative analysis, students’ responses to the course perception questionnaire were coded and analyzed. The results showed that students in the experimental group outperformed their counterparts regarding self-efficacy and basic programming concepts. The experimental treatment resulted in a small to medium effect size difference between the two groups. The results showed that incorporating the GAME model into block-based programming teaching helped improve undergraduates’ self-efficacy and performance of basic programming concepts. In addition, these experimental group undergraduates also perceived the pedagogic GAME model positively. Several research suggestions are proposed based on the findings of the present study.},
 author = {Chun-Yen Tsai and Yun-An Chen and Fu-Pei Hsieh and Min-Hsiung Chuang and Chien-Liang Lin},
 doi = {10.1177/07356331231206071},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331231206071},
 journal = {Journal of Educational Computing Research},
 number = {3},
 pages = {702–724},
 title = {Effects of a Programming Course Using the GAME Model on Undergraduates’ Self-Efficacy and Basic Programming Concepts},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331231206071},
 volume = {62},
 year = {2024u}
}

@article{doi:10.1177/07356331231209773,
 abstract = {Computational thinking (CT) has received much attention in mathematics education in recent years, and researchers have begun to experiment with the integration of CT into mathematics education to promote students’ CT and mathematical thinking (MT) development. However, there is a lack of empirical evidence and new theoretical perspectives on the mechanisms of interaction between CT and MT. To address this research gap, this study analyses the participants’ thinking processes in solving programming-based mathematical problems from a flexibility perspective, focusing on the interplay between computational and mathematical thinking, that is, how CT and MT work together to influence and determine the problem-solver’s choice of solution strategy. Using data collected from a large design-based study, we summarise two types of flexibility and six subtypes of flexibility demonstrated by participants in the programming-based mathematical problem-solving process using thematic analysis. These different types of flexibility provide researchers and mathematics educators with new theoretical perspectives to examine the interplay of CT and MT. Findings will also contribute toward student learning characteristics in programming-based mathematical problem-solving to sketch the big picture of how CT and MT emerge in complementary or mismatching ways.},
 author = {Huiyan Ye and Oi-Lam Ng and Zhihao Cui},
 doi = {10.1177/07356331231209773},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331231209773},
 journal = {Journal of Educational Computing Research},
 number = {2},
 pages = {594–619},
 title = {Conceptualizing Flexibility in Programming-Based Mathematical Problem-Solving},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331231209773},
 volume = {62},
 year = {2024y}
}

@article{doi:10.1177/07356331231210560,
 abstract = {Pair programming (PP) can help improve students’ computational thinking (CT), but the trajectory of CT skills and the differences between high-scoring and low-scoring students in PP are unknown and need further exploration. In this study, a total of 32 fifth graders worked on Scratch tasks in 16 pairs. The group discourse of three learning topics (comprising 9 projects) was collected. After the audio files were transcribed, 1,303 conversations were obtained. They were analyzed via Epistemic Network Analysis (ENA) Webkit, which can reveal the trajectory of students’ CT development via analyzing codes of discourse related to CT in PP. Three Scratch learning topics were assessed based on the Dr. Scratch platform to acquire the level of students’ CT and to determine the low- and high-scoring groups. Results indicated that CT concepts and CT practices were always closely related in PP and CT practices, and CT perspectives could be gradually and closely related after a long period of CT training. A significant difference between the two groups’ CT structures was found. The high-scoring group had more fragments of CT practice and connecting of CT perspectives, while the low-scoring group showed more fragments of CT concepts and expressing of CT perspectives. This research provides insights into cultivating primary school students’ CT using Scratch in the context of PP. The findings can provide suggestions for instructors to design instructional interventions to facilitate students’ CT skills via PP learning. Instructors can improve CT skills by guiding students to constantly ask questions, and specifying the role swap between driver and navigator in PP. Besides, instructors could give more consideration to the development of CT perspectives, and especially the ability to question.},
 author = {Yu-Sheng Su and Shuwen Wang and Xiaohong Liu},
 doi = {10.1177/07356331231210560},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331231210560},
 journal = {Journal of Educational Computing Research},
 number = {2},
 pages = {559–593},
 title = {Using Epistemic Network Analysis to Explore Primary School Students’ Computational Thinking in Pair Programming Learning},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331231210560},
 volume = {62},
 year = {2024q}
}

@article{doi:10.1177/07356331231210946,
 abstract = {Educational technologists and practitioners have made substantial strides in developing affordable digital and tangible resources to support both formal and informal computer science instruction. However, there is a lack of research on practice-based assignments, such as Internet of Things (IoT) projects, that allow undergraduate students to design and demonstrate educational robots using digital or physical assistance, especially when it comes to computational thinking (CT) and programming skills development in association with their psycho-emotional experience. This study compares the impact of Scratch and LEGO® WeDo robotic kits on students’ CT and programming skills development. A quasi-experimental approach was conducted, involving two hundred forty-six participants (n = 246), who were equally divided between Scratch and LEGO® WeDo groups. Results indicate that the LEGO® WeDo group showed greater improvement in CT and programming skills development, while designing and presenting IoT projects. Nevertheless, no significant association between motivation, grit, and CT skills was observed. The findings highlight the potential of tangible robotics in facilitating students’ hands-on learning and enhancing motivation to foster CT and programming skills. This study provides a wide range of implications for instructional designers on how to use tangible robotics to support hands-on IoT projects in computer science courses.},
 author = {Nikolaos Pellas},
 doi = {10.1177/07356331231210946},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331231210946},
 journal = {Journal of Educational Computing Research},
 number = {2},
 pages = {620–644},
 title = {Assessing Computational Thinking, Motivation, and Grit of Undergraduate Students Using Educational Robots},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331231210946},
 volume = {62},
 year = {2024o}
}

@article{doi:10.1177/07356331231211916,
 abstract = {Science, Technology, Engineering, and Mathematics (STEM) education is essential for developing future-ready learners in both secondary and higher education levels. However, as students transition to higher education, many encounter challenges with independent learning and research. This can negatively impact their Higher-Order Thinking Skills (HOTS), engagement, and practical expertise. This study introduces a solution: Computational Thinking Scaffolding (CTS) in the Jupyter Notebook environment, designed to enhance STEM education at the tertiary level. CTS incorporates five phases: Decomposition, Pattern Recognition, Abstraction, Algorithm Design, and Evaluation. Utilizing a quasi-experimental method, we assessed the impact of CTS on the HOTS, engagement, and practical skills of undergraduate and postgraduate students. Our findings hold substantial relevance for university educators, academic advisors, and curriculum designers aiming to enhance students’ HOTS and hands-on capabilities in STEM disciplines. The results validate the effectiveness of CTS in elevating tertiary STEM learning outcomes, and they spotlight the adaptability of the Jupyter Notebook as a valuable tool in higher education. In conclusion, our research underscores the merits of CTS for improving outcomes in higher STEM education and sets a benchmark for future endeavors in this domain.},
 author = {Hsin-Yu Lee and Ting-Ting Wu and Chia-Ju Lin and Wei-Sheng Wang and Yueh-Min Huang},
 doi = {10.1177/07356331231211916},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331231211916},
 journal = {Journal of Educational Computing Research},
 number = {2},
 pages = {431–467},
 title = {Integrating Computational Thinking Into Scaffolding Learning: An Innovative Approach to Enhance Science, Technology, Engineering, and Mathematics Hands-On Learning},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331231211916},
 volume = {62},
 year = {2024h}
}

@article{doi:10.1177/07356331231213548,
 abstract = {Robotics education has received widespread attention in K-12 education. Studies have pointed out that in robotics courses, learners face challenges in learning abstract content, such as constructing a robot with a good structure and writing programs to drive a robot to complete specific learning tasks. The present study proposed the embodied learning-based computer programming approach and applied it to the LEGO Mindstorms EV3 robotics course. To evaluate its effectiveness, a quasi-experiment was conducted in one public primary school to explore its effects on students’ learning achievement, learning motivation, learning attitudes, learning engagement, and cognitive load. The experimental group (40 students) adopted the embodied learning-based computer programming approach, while the control group (40 students) adopted the conventional computer programming approach. The results showed that the experimental group had significantly better learning achievement in robotics than the control group, and that there was no significant difference in the cognitive load of the two groups. In terms of learning motivation, although both groups showed improvement, the experimental group had higher intrinsic learning motivation. In addition, the experimental group outperformed the control group with regard to learning attitudes and learning engagement (including cognitive, behavioral, and emotional engagement). Accordingly, this study could contribute to future research for developing more effective robotics teaching approaches and computer programming activity design.},
 author = {Xinli Zhang and Yuchen Chen and Danqing Li and Lailin Hu and Gwo-Jen Hwang and Yun-Fang Tu},
 doi = {10.1177/07356331231213548},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331231213548},
 journal = {Journal of Educational Computing Research},
 number = {2},
 pages = {532–558},
 title = {Engaging Young Students in Effective Robotics Education: An Embodied Learning-Based Computer Programming Approach},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331231213548},
 volume = {62},
 year = {2024s}
}

@article{doi:10.1177/07356331231220313,
 abstract = {The study examined the effect of teaching text-based programming with a physical programming tool on secondary school students’ computational thinking skills and self-efficacy perceptions. The study was conducted according to a sequential explanatory design as a mixed method research. The study group consisted of 85 secondary school students. Within the scope of the study, a physical programming tool called Micro:bit was used to teach Python programming for a period of 6 weeks. Data were collected using the Self-Efficacy Perception Scale for Computational Thinking Skill, Bebras: International Challenge on Informatics and Computational Thinking Tasks, tests focused on programming tool, concepts, and processes, and through semi-structured interview questioning. According to the findings obtained from pretests and posttests, a significant and positive difference was found in the students’ computational thinking skills and self-efficacy perceptions towards computational thinking skill. As a result of having received instruction in programming, the students satisfactorily learnt the required programming concepts and processes. Through learning Python programming with a physical programming tool, the students not only gained the skills required to write appropriate syntax, and to test and debug code, but they also learnt programming concepts such as variables, conditional expressions, loops, and functions.},
 author = {Ezgi Arzu Yurdakök and Filiz Kalelioğlu},
 doi = {10.1177/07356331231220313},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331231220313},
 journal = {Journal of Educational Computing Research},
 number = {3},
 pages = {785–815},
 title = {The Effect of Teaching Physical Programming on Computational Thinking Skills and Self-Efficacy Perceptions Towards Computational Thinking},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331231220313},
 volume = {62},
 year = {2024s}
}

@article{doi:10.1177/07356331231225269,
 abstract = {Computational thinking (CT), as a new future-oriented literacy, has gained attention at the basic education level. Graphical programming is the common way to develop CT in primary students, but this drag-and-drop programming may weaken students’ understanding of programming’s abstract concepts and code logic. Text-based programming approaches can solve the problems faced by graphical programming, but few studies have explored the impact of text-based programming on CT. Therefore, we conducted a quasi-experimental study with 98 6th graders to explore the impact of gamified Python programming on CT. The findings showed that CT skills, as well as abstraction and decomposition, pattern recognition and evaluation in CT sub-skills, were significantly higher with students in the experimental group than in the control group. Furthermore, we found that gamified Python programming enabled boys and girls to reach the same level of CT skills. However, in terms of CT sub-skills, we found that gamified Python programming was more beneficial to the development of pattern recognition and evaluation skills for boys and abstraction and decomposition skills for girls. This demonstrated the effectiveness of gamified Python programming to improve primary students’ CT skills while clarifying the impact of gender and enriching research in the field of text-based programming.},
 author = {Lihui Sun and Junjie Liu},
 doi = {10.1177/07356331231225269},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331231225269},
 journal = {Journal of Educational Computing Research},
 number = {3},
 pages = {846–874},
 title = {Effects of Gamified Python Programming on Primary School Students’ Computational Thinking Skills: A Differential Analysis of Gender},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331231225269},
 volume = {62},
 year = {2024q}
}

@article{doi:10.1177/07356331231225468,
 abstract = {The relationship between computational thinking (CT) and academic self-efficacy for building students’ academic resilience—a trait crucial for problem-solving, peer relationships, and confidence development—was investigated. A mixed-methods approach was employed in a study involving 60 participants; half were given CT instruction and half were instructed traditionally. Quantitative data were analyzed using analysis of covariance and path analysis, while MAXQDA software was employed for qualitative interview data. The study found a positive correlation between CT instruction and academic self-efficacy with increased academic resilience, particularly in the experimental group, and identified key factors contributing to resilience. This study demonstrates the effectiveness of computational thinking (CT) and academic self-efficacy in enhancing academic performance, offering a new understanding of how these elements can be integrated into education to boost student resilience. It establishes a direct empirical link between CT instruction and increased academic self-efficacy, underscoring the value of specific teaching methodologies in fostering resilience. The findings are significant for educators, policymakers, and stakeholders in developing strategies to enhance students’ academic and personal success, thereby promoting their overall well-being. Recognizing the importance of CT and self-efficacy paves the way for customized educational programs that effectively support and empower students to thrive.},
 author = {Ting-Ting Wu and Lusia Maryani Silitonga and Budi Dharmawan and Astrid Tiara Murti},
 doi = {10.1177/07356331231225468},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331231225468},
 journal = {Journal of Educational Computing Research},
 number = {3},
 pages = {816–845},
 title = {Empowering Students to Thrive: The Role of CT and Self-Efficacy in Building Academic Resilience},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331231225468},
 volume = {62},
 year = {2024q}
}

@article{doi:10.1177/07356331241226459,
 abstract = {Computational Thinking (CT) is essential for developing creativity, problem-solving, and digital competence in the 21st century. Coding tools like robotic toys and tablet apps have become popular in early childhood education to support CT development, but there is a debate on which tool is more effective. Little evidence exists on the effect of coding on children’s Social-Emotional Competence (SEC), which is crucial for lifelong development and extends beyond cognitive development. This experimental study aimed to compare the effectiveness of two 9-week programs on promoting 73 preschool children’s CT and SEC, one using coding robots and the other using coding apps. The results showed that children who participated in the Coding Robot Program had higher CT scores than those in the Coding App Program, after controlling for age, gender, family socioeconomic status, and baseline CT scores. While the SEC scores showed no substantial disparities between the groups, it was revealed that the initial performance levels moderated the intervention effects on emotional regulation and overall SEC. This suggests that the Coding Robot Program could be especially advantageous for a subset of children who initially have difficulties with emotional regulation and social skills. Implications of this study are presented for research and practice.},
 author = {Weipeng Yang},
 doi = {10.1177/07356331241226459},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331241226459},
 journal = {Journal of Educational Computing Research},
 number = {4},
 pages = {938–960},
 title = {Coding With Robots or Tablets? Effects of Technology-Enhanced Embodied Learning on Preschoolers’ Computational Thinking and Social-Emotional Competence},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331241226459},
 volume = {62},
 year = {2024r}
}

@article{doi:10.1177/07356331241226746,
 abstract = {This study was grounded in the spatial computational thinking model developed by the 3D Weather project funded by the NSF STEM+C program. The model reflects a discipline-based perspective towards computational thinking and captures the spatial nature of computational thinking in meteorology and the reliance of computational thinking on spatial thinking for geospatial analysis. The research was conducted among nineteen teachers attending the summer workshop offered by the project in its third project year to prepare them for teaching spatial computational thinking with IDV (Integrated Data Viewer, downloadable at https://www.unidata.ucar.edu/software/idv/) visualization of weather data. Quantitative survey data were collected measuring these teachers’ meteorology content knowledge, spatial computational thinking, self-efficacy for teaching spatial computational thinking, and epistemic cognition of teaching meteorology. The data were analyzed to examine the effects of the workshop in terms of these variables and the correlations among them were also explored.},
 author = {Yan Sun and Jamie Dyer and Jonathan Harris},
 doi = {10.1177/07356331241226746},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331241226746},
 journal = {Journal of Educational Computing Research},
 number = {4},
 pages = {1061–1086},
 title = {Preparing Teachers for Teaching Spatial Computational Thinking With Integrated Data Viewer Visualization of Weather Data: A Discipline-Based Perspective of Computational Thinking},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331241226746},
 volume = {62},
 year = {2024n}
}

@article{doi:10.1177/07356331241227793,
 abstract = {Computational thinking (CT), an essential 21st century skill, incorporates key computer science concepts such as abstraction, algorithms, and debugging. Debugging is particularly underrepresented in the CT training literature. This multi-level meta-analysis focused on debugging as a core CT skill, and investigated the effects of various debugging interventions. Moderator analyses revealed which interventions were effective, in which situations, and for what kind of learner. A significant overall mean effect of debugging interventions ( = 0.64, CI = (0.32, 0.96), p < .001), was found based on 62 effect sizes from 18 source articles. Significant between-studies variation indicated that true effects could range from −0.54 to 1.82. In addition, sensitivity analyses and checks on confounding provided further understandings of intervention features and their impacts. Interventions using enhanced debuggers and systematic instruction were particularly effective in fostering debugging skills. Debugging intervention effects varied by participant population and potentially by publication type. Moreover, debugging interventions had impact regardless of how debugging skills were measured, programming medium used, control-group type, and whether the study was randomized. Future studies should investigate the best practices for improving debugging abilities for whom and under what circumstances.},
 author = {Chen Sun and Stephanie Yang and Betsy Becker},
 doi = {10.1177/07356331241227793},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331241227793},
 journal = {Journal of Educational Computing Research},
 number = {4},
 pages = {1087–1121},
 title = {Debugging in Computational Thinking: A Meta-analysis on the Effects of Interventions on Debugging Skills},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331241227793},
 volume = {62},
 year = {2024s}
}

@article{doi:10.1177/07356331241236744,
 abstract = {The interest in Computational Thinking (CT) development among young learners increases with the number of studies located in literature. In this study, a meta-analysis was conducted to address two main objectives: (a) the effectiveness of empirical interventions on the development of CT in children aged of 3–8 years; and (b) the variables that influence the effectiveness of the interventions. Following PRISMA procedures, we identified 17 empirical studies with 34 effect sizes and 1665 participants meeting the inclusion criteria from Web of Science database. Overall, we found a statistically significant large effect size (d = .83 [95% CI: 730, .890]; p < .001) on the CT development of 3–8 years old children, which provides empirical support for having young children to engage in CT experiences. The effect size was significantly influenced by moderating variables including gender, scaffolding, and education level. Intervention length showed a marginally significant effect. Therefore, educators could refer to the significant moderators when designing tailored interventions for CT development in early childhood education while a call for more empirical studies of CT development in young children is proposed.},
 author = {Xiaowen Wang and Kan Kan Chan and Qianru Li and Shing On Leung},
 doi = {10.1177/07356331241236744},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331241236744},
 journal = {Journal of Educational Computing Research},
 number = {5},
 pages = {1182–1208},
 title = {Do 3–8 Years Old Children Benefit From Computational Thinking Development? A Meta-Analysis},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331241236744},
 volume = {62},
 year = {2024v}
}

@article{doi:10.1177/07356331241236882,
 abstract = {This research aimed to investigate the structural relationships among teachers’ computational thinking (CT), design thinking (DT), robotics teaching beliefs, and robotics pedagogical content knowledge (RPCK). A total of 98 in-service and pre-service teachers who participated in a robotics teaching professional development workshop served as the sample of the study. A survey including the Computational Thinking Scale, the Design Thinking Disposition Scale, the Robotics Teaching Beliefs Scale and the Technological Pedagogical Content Knowledge–Robotics Scale was conducted after the workshop. A confirmatory factor analysis was employed to validate the measurement constructs, and Partial Least Squares - Structural Equation Modeling (PLS-SEM) analysis was utilized to examine the relationships among the factors. The results revealed that both CT and DT dispositions could positively predict teachers’ robotics teaching beliefs, which subsequently predicted their RPCK. Moreover, a direct positive relationship between CT and RPCK was identified, while such a relationship was not evident for DT. The model demonstrates the critical role of CT in shaping teachers’ beliefs and pedagogical strategies of robotics teaching, and provides insights into the indirect influence of DT. Finally, the Model of Robotics Teaching Professional Development (MRTPD) was proposed to profile how to promote teachers’ pedagogical content knowledge of robotics teaching from their CT and DT dispositions.},
 author = {Chung-Yuan Hsu and Meng-Jung Tsai},
 doi = {10.1177/07356331241236882},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331241236882},
 journal = {Journal of Educational Computing Research},
 number = {5},
 pages = {1159–1181},
 title = {Predicting Robotics Pedagogical Content Knowledge: The Role of Computational and Design Thinking Dispositions via Teaching Beliefs},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331241236882},
 volume = {62},
 year = {2024e}
}

@article{doi:10.1177/07356331241240047,
 abstract = {The use of a pedagogical approach mediated transfer with the bridging method has been successful in facilitating the transitions from block-based to text-based programming languages. Nevertheless, there is a lack of research addressing the impact of this transfer on programming misconceptions during the transition. The way programming concepts are taught to K-12 learners can later result in misconceptions for adult learners. The main objective was to examine the impact of mediated transfer using the bridging method pedagogical approach on the prevalence of programming misconceptions. We conducted a quasi-experimental study in school settings during informatics (computer science) classes among 163 sixth-grade students. The control group received traditional programming lectures using the text-based programming language, Python. Conversely, the experimental group utilized a mediated transfer pedagogical approach by starting with the block-based programming language MakeCode for micro:bit before transitioning to the text-based Python. Our findings indicate that the experimental group significantly reduced programming misconceptions in fundamental programming concepts: variables, sequencing, selection, and loops - compared to the control group. This suggests that the use of block-based programming language as an initial step in programming education, followed by a structured transition to text-based programming language, can effectively mitigate common misconceptions among K-12 learners.},
 author = {Monika Mladenović and Žana Žanko and Goran Zaharija},
 doi = {10.1177/07356331241240047},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331241240047},
 journal = {Journal of Educational Computing Research},
 number = {5},
 pages = {1302–1326},
 title = {From Blocks to Text: Bridging Programming Misconceptions},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331241240047},
 volume = {62},
 year = {2024o}
}

@article{doi:10.1177/07356331241240460,
 abstract = {Pair Programming is considered an effective approach to programming education, but the synchronous collaboration of two programmers involves complex coordination, making this method difficult to be widely adopted in educational settings. Artificial Intelligence (AI) code-generation tools have outstanding capabilities in program generation and natural language understanding, creating conducive conditions for pairing with humans in programming. Now some more mature tools are gradually being implemented. This review summarizes the current status of educational applications and research on AI-assisted programming technology. Through thematic coding of literature, existing research focuses on five aspects: underlying technology and tool introduction, performance evaluation, the potential impacts and coping strategies, exploration of behavioral patterns in technological application, and ethical and safety issues. A systematic analysis of current literature provides the following insights for future academic research related to the practice of “human-machine pairing” in programming: (1) Affirming the value of AI code-generation tools while also clearly defining their technical limitations and ethical risks; (2) Developing adaptive teaching ecosystems and educational models, conducting comprehensive empirical research to explore the efficiency mechanisms of AI-human paired programming; (3) Further enriching the application of research methods by integrating speculative research with empirical research, combining traditional methods with emerging technologies.},
 author = {Jiangyue Liu and Siran Li},
 doi = {10.1177/07356331241240460},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331241240460},
 journal = {Journal of Educational Computing Research},
 number = {5},
 pages = {1385–1415},
 title = {Toward Artificial Intelligence-Human Paired Programming: A Review of the Educational Applications and Research on Artificial Intelligence Code-Generation Tools},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331241240460},
 volume = {62},
 year = {2024m}
}

@article{doi:10.1177/07356331241240670,
 abstract = {This study employs meta-analysis to synthesize findings from 30 articles investigating gender differences in computational thinking (CT) among K-12 students. Encompassing 51 independent effect sizes, the meta-analysis involves a participant pool of 9181 individuals from various countries, comprising 4254 males and 4927 females. Results indicate statistically significant gender differences in CT (Hedges’s g = 0.091, p < .05), albeit with a modest effect size, revealing higher CT scores among males compared to females. Further moderation analyses unveil the multifaceted nature of these gender differences. Specifically, while gender differences become significant during high school, recent studies suggest a gradual reduction in CT gender differences with societal progress among K-12 students. Moreover, findings illustrate variations in gender differences across geographical regions. Notably, while the overall gender disparity in CT is non-significant in the “East Asia and Pacific” region, it widens in “North America” and “Europe”, with males scoring higher than females. Conversely, in “Europe and Central Asia”, such gender differences present inconsistent outcomes, with females scoring higher than males. Importantly, assessment tool type does not significantly influence gender differences. Lastly, this study offers recommendations to address CT gender gaps, providing valuable insights for promoting gender equality in education.},
 author = {Linlin Hu},
 doi = {10.1177/07356331241240670},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331241240670},
 journal = {Journal of Educational Computing Research},
 number = {5},
 pages = {1358–1384},
 title = {Exploring Gender Differences in Computational Thinking Among K-12 Students: A Meta-Analysis Investigating Influential Factors},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331241240670},
 volume = {62},
 year = {2024k}
}

@article{doi:10.1177/07356331241242435,
 abstract = {Addressing cognitive disparities has become a paramount concern in computational thinking (CT) education. The intricate and nuanced relationships between CT and cognitive variations emphasize the needs to accommodate diverse cognitive profiles when fostering CT skills, recognizing that these cognitive functions can manifest as either strengths or limitations in different students. Consequently, understanding the connections between students’ cognitive functions and CT skills assumes pivotal importance in the design of personalized instructional strategies for CT. Despite a general consideration of learning variability in CT education, empirical insights exploring the correlation between cognitive skills and CT competencies remain notably scarce. This study endeavors to bridge this research gap by investigating the links between executive functions and CT skills, as well as the associations between their sub-dimensions. The results reveal a statistically significant correlation coefficient of 0.452 between these two domains, underscoring the notable connection between executive functions and CT abilities. Furthermore, the sub-dimensional analysis offers a comprehensive understanding of how specific executive functions uniquely contribute to certain CT skills. In light of these findings, this research offers a promising pathway for the development of tailored CT education programs that can cater to the unique needs of each individual, ultimately facilitating inclusive CT programs and making significant contributions to broaden STEM education and future workforce.},
 author = {Tongxi Liu},
 doi = {10.1177/07356331241242435},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331241242435},
 journal = {Journal of Educational Computing Research},
 number = {5},
 pages = {1267–1301},
 title = {Relationships Between Executive Functions and Computational Thinking},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331241242435},
 volume = {62},
 year = {2024j}
}

@article{doi:10.1177/07356331241248686,
 abstract = {A growing body of research is focusing on integrating artificial intelligence (AI) and computational thinking (CT) to enhance student learning outcomes. Many researchers have designed instructional activities to achieve various learning goals within this field. Despite the prevalence of studies focusing on instructional design and student learning outcomes, how instructional efforts result in the integration of AI and CT in students’ learning processes remains unclear. To address this research gap, we conducted a systematic literature review of empirical studies that have integrated AI and CT for student development. We collected 18 papers from four prominent research databases in the fields of education and AI technology: Web of Science, Scopus, IEEE, and ACM. We coded the collected studies, highlighting the students’ learning processes in terms of research methodology and context, learning tools and instructional design, student learning outcomes, and the interaction between AI and CT. The integration of AI and CT occurs in two ways: the integration of disciplinary knowledge and leveraging AI tools to learn CT. Specifically, we discovered that AI- and CT-related tools, projects, and problems facilitated student-centered instructional designs, resulting in productive AI and CT learning outcomes.},
 author = {Xiaojing Weng and Huiyan Ye and Yun Dai and Oi-lam Ng},
 doi = {10.1177/07356331241248686},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331241248686},
 journal = {Journal of Educational Computing Research},
 number = {6},
 pages = {1640–1670},
 title = {Integrating Artificial Intelligence and Computational Thinking in Educational Contexts: A Systematic Review of Instructional Design and Student Learning Outcomes},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331241248686},
 volume = {62},
 year = {2024p}
}

@article{doi:10.1177/07356331241249956,
 abstract = {In the past decade, Computational Thinking (CT) education has received growing attention from researchers. Although many reviews have provided synthesized information on CT teaching and learning, few have paid particular attention to collaborative learning (CL) strategies. CL has been widely implemented in CT classes and has become the most popular pedagogy among educators. Therefore, a systematic review of CL in CT classes would provide practical guidance on teaching strategies to enhance CT interventions and improve the quality of teaching and learning, ultimately benefiting students’ CT skills development. To address this gap, this study examined 43 empirical studies that have applied CL strategies, ranging from 2006 to 2022. Several findings were revealed in the analysis. First, a wide range of theories and frameworks were applied to inform research questions, pedagogical design, and research methodologies. Second, despite the acknowledged importance of group composition in effective CL, a large number of studies did not provide details on how the students were grouped. Third, six types of CL activities and instructional designs have been identified in CT classrooms. The synthesized information provides valuable insights that can inform future research directions and guide the design and implementation of CL activities in future CT classes.},
 author = {Stella Xin Yin and Dion Hoe-Lian Goh and Choon Lang Quek},
 doi = {10.1177/07356331241249956},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331241249956},
 journal = {Journal of Educational Computing Research},
 number = {6},
 pages = {1440–1474},
 title = {Collaborative Learning in K-12 Computational Thinking Education: A Systematic Review},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331241249956},
 volume = {62},
 year = {2024u}
}

@article{doi:10.1177/07356331241251397,
 abstract = {Previous research has not adequately explored students’ behavioral processes when addressing computational thinking (CT) problems of varying difficulty, limiting insights into students’ detailed CT development characteristics. This study seeks to fill this gap by employing gamified CT items across multiple difficulty levels to calculate comprehensive behavioral sequence quality indicators. And then, through latent profile analysis, we identified four distinct latent classes of behavioral process. We then examined the in-game performance differences among these classes, uncovering each class’s unique attributes. Class 1 students consistently demonstrated high-quality, efficient behavioral sequences regardless of item difficulty. In contrast, class 2 students applied significant cognitive effort and trial-and-error strategies, achieving acceptable scores despite low behavioral sequence quality. Class 3 students excelled in simpler items but faltered with more complex ones. Class 4 students displayed low motivation for challenging items, often guessing answers quickly. Additionally, we investigated the predictive value of students’ performance in gamified items and their behavioral process classes for their external CT test scores. The study finally elaborated on the theoretical implications for researchers and the practical suggestions for teachers in CT cultivation.},
 author = {Qing Guo and Huan Li and Sha Zhu},
 doi = {10.1177/07356331241251397},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331241251397},
 journal = {Journal of Educational Computing Research},
 number = {6},
 pages = {1475–1508},
 title = {Understanding the Characteristics of Students’ Behavioral Processes in Solving Computational Thinking Problems Based on the Behavioral Sequences},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331241251397},
 volume = {62},
 year = {2024g}
}

@article{doi:10.1177/07356331241254575,
 abstract = {In the context of digital technologies, computational thinking (CT) is considered a key competence of primary school teachers. However, assessment tools to measure in-service teachers’ CT competence are rare. The current study explains the design, development, and validation of a scale to determine the in-service teachers’ mastery level of their Computational Thinking Competence (CTC) in the Chinese context. The validity and reliability of the scale have been studied by conducting exploratory factor analysis (EFA), confirmatory factor analysis (CFA) and calculating internal consistency coefficients. The sample of this study consisted of 631 in-service teachers from different Chinese primary schools (EFA N = 189; CFA N = 442). The 31-item scale mirrors five factors: cognitive knowledge of CT, practices and skills of CT, attitudes and perspectives of CT, CT teaching design, and CT teaching implementation. The discussion of the results looks at the future use of the CT scale in Chinese educational settings that mirror a specific national education policy, talent demand, and educational culture.},
 author = {Xinlei Li and Guoyuan Sang and Martin Valcke and Johan van Braak},
 doi = {10.1177/07356331241254575},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331241254575},
 journal = {Journal of Educational Computing Research},
 number = {6},
 pages = {1538–1567},
 title = {The Development of an Assessment Scale for Computational Thinking Competence of In-Service Primary School Teachers},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331241254575},
 volume = {62},
 year = {2024m}
}

@article{doi:10.1177/07356331241268474,
 abstract = {Computational thinking (CT), an essential competency for comprehending and addressing intricate issues in the digital world, has been incorporated into curriculum planning as a goal for programming education. This study introduced flow design into programming curricula to investigate its impact on undergraduates ’CT skills during pair work. Two types of flow design approaches, construct-by-self flow design (CBS-FD) and construct-on-scaffold flow design (COS-FD), were proposed to determine which approach better enhances students’ CT skills. Seventy-six first-year undergraduates participated, including thirty in the CBS group and thirty-six in the COS group. Evaluations made from the results and processes of programming tasks were employed to describe computational performance and computational practices, respectively. Data gathered from CT skill surveys were thoroughly analyzed to gain a deeper understanding of computational perspectives. Our findings highlighted that COS-FD significantly improved participants’ computational performance compared with CBS-FD. The COS groups fostered an engaging, sharing atmosphere, while CBS groups spent more energy on negotiating the manipulation of flow design. Moreover, both COS-FD and CBS-FD proved beneficial in enhancing participants’ computational perspectives, with the COS groups better improving their algorithm thinking. The study presents valuable perspectives on the design and implementation of collaborative programming activities within curriculum education.},
 author = {Ruijie Zhou and Chong Xie and Xiuling He and Yangyang Li and Qiong Fan and Ying Yu and Zhonghua Yan},
 doi = {10.1177/07356331241268474},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331241268474},
 journal = {Journal of Educational Computing Research},
 number = {7},
 pages = {1865–1895},
 title = {Effect of Different Flow Design Approaches on Undergraduates’ Computational Thinking During Pair Programming},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331241268474},
 volume = {62},
 year = {2024y}
}

@article{doi:10.1177/07356331241270707,
 abstract = {The study investigates the potential of anxiety clusters in predicting programming performance in two distinct coding environments. Participants comprised 83 second-year programming students who were randomly assigned to either a block-based or a text-based group. Anxiety-induced behaviors were assessed using physiological measures (Apple Watch and Electrocardiogram machine), behavioral observation, and self-report. Utilizing the Hidden Markov Model and Optimal Matching algorithm, we found three representative clusters in each group. In the block-based group, clusters were designated as follows: “stay calm” (students allocating more of their time to a calm state), “stay hesitant” (students allocating more of their time to a hesitant state), and “to-calm” (those allocating minimal time to a hesitant and anxious state but displaying a pronounced propensity to transition to a calm state). In contrast, clusters in the text-based group were labeled as: “to-hesitant” (exhibiting a higher propensity to transition to a hesitant state), “stay hesitant” (allocating significant time to a hesitant state), and “stay anxious” (remaining persistently anxious in a majority of the coding time). Additionally, our results indicate that novice programmers are more likely to experience anxiety during text-based coding. We discussed the findings and highlighted the policy implications of the study.},
 author = {Abdullahi Yusuf and Amiru Yusuf Muhammad},
 doi = {10.1177/07356331241270707},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331241270707},
 journal = {Journal of Educational Computing Research},
 number = {7},
 pages = {1798–1836},
 title = {Exploring Clusters of Novice Programmers’ Anxiety-Induced Behaviors During Block- and Text-Based Coding: A Predictive and Moderation Analysis of Programming Quality and Error Debugging Skills},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331241270707},
 volume = {62},
 year = {2024t}
}

@article{doi:10.1177/07356331241277967,
 abstract = {This study tested the potential of a technological intervention procedure for promoting letter-naming and initial-phoneme detection skills among preschoolers at risk for Specific Learning Disorder. The study rational is based on evidence for paired associated learning of visual-verbal stimuli, integrated with the use of a tangible technological device (educational floor robot) as means for promoting letter knowledge. Two intervention groups (N = 60) participated. The intervention lasted for eight sessions of phonological training; each dedicated to one target letter. Participants were asked to navigate the robot’s path from a current stopping point to the next according to phonological information processing. Both groups went through the same procedure, but using a different type of mat (one with and one without a visual form of the letter). A control group (N = 30) of at-risk preschoolers received no intervention except for their preschool educational routine. We found that children who participated in phonological training with linkage to the visual form of the letter scored higher in post-intervention tests. Among other proposed explanations, we mention the advantages of using a tangible robot and seemingly three-dimensional letters, along with the tendency to consider visual features while processing language stimulus among children with dyslexia.},
 author = {Gila Apelboim-Dushnitzky and Oren Tova},
 doi = {10.1177/07356331241277967},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331241277967},
 journal = {Journal of Educational Computing Research},
 number = {0},
 pages = {07356331241277967},
 title = {Promoting Letter-Naming and Initial-Phoneme Detection Abilities Among Preschoolers at Risk for Specific Learning Disorder Using Technological Intervention With Two Types of Mats: With and Without Target Letter Forms},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331241277967},
 volume = {0},
 year = {2024a}
}

@article{doi:10.1177/07356331241291173,
 abstract = {This study examined the effects of embodied learning experiences on students’ understanding of computational thinking (CT) concepts and their ability to solve CT problems. In a mixed-reality learning environment, students mapped CT concepts, such as sequencing and loops, onto their bodily movements. These movements were later applied to robot programming tasks, where students used the same CT concepts in a different modality. By explicitly connecting embodied actions with programming tasks, the intervention aimed to enhance students’ comprehension and transfer of CT skills. Forty-four first- and second-grade students participated in the study. The results showed significant improvements in students’ CT competency and positive attitudes toward CT. Additionally, an analysis of robot programming performance identified common errors and revealed how students employed embodied strategies to overcome challenges. The effects of embodied learning and the impact of embodied learning strategies were discussed.},
 author = {Kyungbin Kwon and Thomas A. Brush and Keunjae Kim and Minhwi Seo},
 doi = {10.1177/07356331241291173},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331241291173},
 journal = {Journal of Educational Computing Research},
 number = {0},
 pages = {07356331241291173},
 title = {Embodied Learning for Computational Thinking in a Mixed-Reality Context},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331241291173},
 volume = {0},
 year = {2024i}
}

@article{doi:10.1177/07356331241291174,
 abstract = {Debugging is essential for identifying and rectifying errors in programming, yet time constraints and students’ trivialization of errors often hinder progress. This study examines differences in debugging challenges and strategies among students with varying computational thinking (CT) competencies using weekly coding journals from an online undergraduate CT course. Participants used Scratch, a block-based programming language, and their journals from five assignments were analyzed using Term Frequency-Inverse Document Frequency and Structural Topic Modeling. High-performing students engaged with diverse topics and specific blocks tied to their weekly projects while low-performing students faced repetitive and broad challenges, such as understanding motion blocks and broadcast concepts. These patterns reveal that low-performing students struggle particularly during the ‘diagnose the fault’ phase of debugging, often hindering their progress in the final stage. Such challenges highlight the necessity for targeted educational interventions to improve the debugging proficiency and overall CT skills. The study underscores the importance of further research into students’ logical thinking processes during code review and debugging, suggesting the use of think-aloud protocols and detailed tracking of debugging practices for deeper insights. This research contributes to the field by showing that differentiated instruction and strategic support can enhance debugging skills across different student performance levels.},
 author = {Eunsung Park and Jongpil Cheon},
 doi = {10.1177/07356331241291174},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331241291174},
 journal = {Journal of Educational Computing Research},
 number = {0},
 pages = {07356331241291174},
 title = {Exploring Debugging Challenges and Strategies Using Structural Topic Model: A Comparative Analysis of High and Low-Performing Students},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331241291174},
 volume = {0},
 year = {2024p}
}

@article{doi:10.1177/07356331241292767,
 abstract = {Tangible programming tools (TPTs) are promising teaching aids in programming courses due to their interactivity and ability to enhance early childhood students’ computational thinking, spatial reasoning, and executive function skills. However, it remains unclear whether TPTs support these skills simultaneously. This study examines the impact of TPTs on enhancing cognitive thinking skills among students at different developmental stages, with a focus on early childhood education. A quasi-experimental study with pre- and post-tests was conducted involving 82 preschoolers aged 5-7. Participants were categorized into three developmental stages (beginner, intermediate, advanced) based on their prior programming experience. A TPT called “Bee-bot Brushing Challenge: A Computational Adventure” was employed during a STEM summer camp program. The findings revealed significant improvements in students’ abstract thinking, problem decomposition, and spatial reasoning skills, particularly among beginners. Participants at intermediate levels showed notable improvement in algorithm design and efficiency. Results also indicate significant differences in cognitive flexibility and inhibitory control between groups, with advanced students outperforming beginners and intermediates while working memory remained unaffected. This research provides important evidence supporting the inclusion of TPTs in early childhood curricula to foster comprehensive cognitive development. It offers valuable insights for educators and policymakers in designing similar learning environments.},
 author = {Nikolaos Pellas},
 doi = {10.1177/07356331241292767},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07356331241292767},
 journal = {Journal of Educational Computing Research},
 number = {0},
 pages = {07356331241292767},
 title = {Enhancing Computational Thinking, Spatial Reasoning, and Executive Function Skills: The Impact of Tangible Programming Tools in Early Childhood and Across Different Learner Stages},
 url = {https://doi-org.crai.referencistas.com/10.1177/07356331241292767},
 volume = {0},
 year = {2024j}
}

@article{doi:10.1177/0738894215570433,
 abstract = {Many accounts of civil war violence assume that a conflict’s master cleavage also explains the local occurrence of violence. Some scholars, however, have argued that violence is often the result of local cleavages and feuds, many of which may be unrelated to the conflict’s master cleavage. How is local violence related to the conflict’s master cleavage? Using a computational model, this paper studies an alliance mechanism proposed by Kalyvas (2006, The Logic of Violence in Civil War, Cambridge University Press), where macro-actors support local ones that fight on their behalf. While these alliances create a principal–agent problem, the model shows that they can raise the overall severity of the conflict and serve the interests of the macro-actor. However, the model also shows that these mechanisms work only under limited conditions. Alliances can increase the level of violence perpetrated in the interest of the macro-actor, but only if (a) the latter supports agents that have in the past fought along the master cleavage and if (b) this happens in rural areas. This emphasizes again the importance of the rural dimension in the study of civil war.},
 author = {Nils B. Weidmann},
 doi = {10.1177/0738894215570433},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0738894215570433},
 journal = {Conflict Management and Peace Science},
 number = {5},
 pages = {539–558},
 title = {Micro-cleavages and violence in civil wars: A computational assessment},
 url = {https://doi-org.crai.referencistas.com/10.1177/0738894215570433},
 volume = {33},
 year = {2016o}
}

@article{doi:10.1177/0738894219881425,
 abstract = {Many scholars of contentious politics claim there is no such thing as a group that uses only one tactic, yet scholars, pundits, and the public routinely use single-minded terms like protestors, dissidents, and terrorists. Other scholars and research programs suggest that some groups are specialists who tend to stick to a single tactic to achieve their goals, such as non-violence, violence, or specific kinds of violence, like terror. We make the claim that both sides of the debate are empirically valid and that both types of group exist. That is, some groups tend to specialize in a single tactic while others use a variety of tactics. This paper examines the empirical distribution of group types by examining the mix of tactics that groups employ. The analysis helps resolve part of the debate and pushes scholarly thinking in new directions about how often, why, and when groups operate across this spectrum.},
 author = {Joseph K Young and Steve Shellman},
 doi = {10.1177/0738894219881425},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0738894219881425},
 journal = {Conflict Management and Peace Science},
 number = {6},
 pages = {645–660},
 title = {Protestors, terrorists or something else? How to think about dissident groups},
 url = {https://doi-org.crai.referencistas.com/10.1177/0738894219881425},
 volume = {36},
 year = {2019t}
}

@article{doi:10.1177/0739456X211053653,
 abstract = {The paper suggests a focused examination of the processes of drafting-based design and parametric design in urbanism. It discusses how spatial design’s settled cognition would differ by using algorithmic systems through the altered relationships between the basic operations in design. To reveal the commonalities and distinctions between the two design methods, the authors present the detailed documentation of the workshop series, which experimented with both techniques within similar design contexts. By the design analysis, the idea of “parametric thinking” is revisited in the specific context of urban design.},
 author = {Olgu Çalışkan and Yavuz Baver Barut and Gökhan Ongun},
 doi = {10.1177/0739456X211053653},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0739456X211053653},
 journal = {Journal of Planning Education and Research},
 number = {3},
 pages = {1010–1029},
 title = {Parametric Urban Design Thinking: Shared Patterns in Design by Algorithm and Design by Drawing},
 url = {https://doi-org.crai.referencistas.com/10.1177/0739456X211053653},
 volume = {44},
 year = {2024a}
}

@article{doi:10.1177/0739532917739870,
 abstract = {Similar to prior cycles of newsroom specialization, news organizations must integrate the expertise of data journalists. Based upon 18 in-depth interviews with data journalism leaders within American newspapers, this study examines how newsrooms are restructuring to accommodate data news work. More specifically, the research identifies four “critical junctures” by which newspapers expand data journalism operations. The interviews establish that expanding a paper’s commitment to data journalism requires reorganizing the newsroom with new layers of structural complexity.},
 author = {Jan Lauren Boyles and Eric Meyer},
 doi = {10.1177/0739532917739870},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0739532917739870},
 journal = {Newspaper Research Journal},
 number = {4},
 pages = {428–438},
 title = {Newsrooms accommodate data-based news work},
 url = {https://doi-org.crai.referencistas.com/10.1177/0739532917739870},
 volume = {38},
 year = {2017b}
}

@article{doi:10.1177/0741088316650178,
 abstract = {Idea generation is an important component of most major theories of writing. However, few studies have linked idea generation in writing samples to assessments of writing quality or examined links between linguistic features in a text and idea generation. This study uses human ratings of idea generation, such as idea fluency, idea flexibility, idea originality, and idea elaboration, to analyze the extent to which idea generation relates to human judgments of essay quality in a corpus of college student essays. In conjunction with this analysis, linguistic features extracted from the essays are used to develop a predictive model of idea generation to further understand relations between the language features in an essay and the idea generation scores assigned to that essay. The results indicate that essays rated as containing a greater number of ideas that were flexible, original, and elaborated were judged to be of higher quality. Two of these features (elaboration and originality) were significant predictors of essay quality scores in a regression analysis that explained 33% of the variance in human scores. The results also indicate that idea generation is strongly linked to language features in essays. Specifically, the use of unique multiword units, more difficult words, semantic but not lexical similarities between paragraphs, and fewer word repetitions explained 80% of the variance in human scores of idea generation. These results have implications for writing theories and writing practice.},
 author = {Scott A. Crossley and Kasia Muldner and Danielle S. McNamara},
 doi = {10.1177/0741088316650178},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0741088316650178},
 journal = {Written Communication},
 number = {3},
 pages = {328–354},
 title = {Idea Generation in Student Writing: Computational Assessments and Links to Successful Writing},
 url = {https://doi-org.crai.referencistas.com/10.1177/0741088316650178},
 volume = {33},
 year = {2016e}
}

@article{doi:10.1177/07417136231165007,
 abstract = {Developing adults’ reflective thinking habits is an aim of adult education, but the best way to do it has been overlooked. Common strategies communicate the skills and knowledge needed to reflect while providing practice opportunities. Yet research indicates that reflective habits are comprised of not only skills and knowledge but also of attitudes toward reflection. This study investigated whether attitudinal change strategies in a reflective thinking workshop induced cognitive dissonance by helping adults appreciate reflection and calibrate their reflective behavior. Participants were randomly assigned to a skills-based or skills-based plus attitudinal change workshop. Pre-post measures of learners’ need for and engagement in reflection were taken. Multivariate analysis of covariance revealed that attitudinal change strategies induced dissonance by increasing the need for reflection while decreasing perceived engagement in reflection. Exclusively skills-based strategies failed to affect the need for reflection but increased perceived engagement in reflection, creating overconfidence. Implications for research and practice are offered.},
 author = {Kevin M. Roessger},
 doi = {10.1177/07417136231165007},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07417136231165007},
 journal = {Adult Education Quarterly},
 number = {3},
 pages = {266–285},
 title = {Attitudes Matter: Examining How Teaching Strategies for Attitudinal Change Help Adults Value Reflection and Calibrate Their Reflective Thinking},
 url = {https://doi-org.crai.referencistas.com/10.1177/07417136231165007},
 volume = {73},
 year = {2023n}
}

@article{doi:10.1177/07419325070280050401,
 abstract = {The purpose of this descriptive study was to develop a computational fluency performance profile of 224 high school (Grades 9—12) students with mathematics disabilities (MD). Computational fluency performance was examined by grade-level expectancy (Grades 2—6) and skill area (whole numbers: addition, subtraction, multiplication, division; rational numbers: fractions, decimals) using the Mathematics Operations Test— Revised (MOT-R). The findings indicated that these high school students with MD were fluent only in computational skills at the second- and third-grade levels. Computational fluency was found with most whole number basic facts in addition, subtraction, multiplication, and division. Lack of computational fluency was demonstrated on many items dealing with subtraction of multiple digits or items requiring regrouping, most multiplication and division items, and rational number items involving fractions and decimals. The results are discussed in regard to instructional programs and curriculum organization for high school students with MD.},
 author = {Mary Beth Calhoon and Robert Wall Emerson and Margaret Flores and David E. Houchins},
 doi = {10.1177/07419325070280050401},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07419325070280050401},
 journal = {Remedial and Special Education},
 number = {5},
 pages = {292–303},
 title = {Computational Fluency Performance Profile of High School Students With Mathematics Disabilities},
 url = {https://doi-org.crai.referencistas.com/10.1177/07419325070280050401},
 volume = {28},
 year = {2007d}
}

@article{doi:10.1177/0741932519843998,
 abstract = {The purpose of this literature review was to synthesize recent research (2009–2018) for teaching science to students with intellectual disability and intellectual disability/autism. Authors identified a total of 15 studies; of these, 12 were determined to be methodologically sound studies using the Council for Exceptional Children quality indicators. Based on the methodologically sound studies, authors analyzed the evidence base of the instructional practices to teach science content and science practices to students with intellectual disability and intellectual disability/autism. Unlike previous literature reviews in which the focus has been on teaching science content, authors contribute to the literature on teaching science to this population by determining the evidence for teaching the science practices (e.g., asking questions, communicating findings). Resulting analysis was used to offer research-based recommendations for providing quality science instruction to students with intellectual disability and intellectual disability/autism. We conclude with limitations and possibilities for future research.},
 author = {Victoria F. Knight and Leah Wood and Bethany R. McKissick and Emily M. Kuntz},
 doi = {10.1177/0741932519843998},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0741932519843998},
 journal = {Remedial and Special Education},
 number = {6},
 pages = {327–340},
 title = {Teaching Science Content and Practices to Students With Intellectual Disability and Autism},
 url = {https://doi-org.crai.referencistas.com/10.1177/0741932519843998},
 volume = {41},
 year = {2020f}
}

@article{doi:10.1177/0741932521989091,
 abstract = {This study, which was reviewed through the Registered Report process, examined the initial efficacy of the Scientific Explorers program (Sci2) on second-grade students’ science achievement. Sci2 is grounded in the growing body of empirical research on science instruction, embedding principles of explicit instruction within a guided inquiry-based design framework. Eighteen second-grade classrooms were randomly assigned to treatment or control conditions. A cluster randomized controlled trial was employed, with 294 students nested within classrooms and classrooms nested within condition. The Sci2 program was implemented for a total of 10 lessons (5 hr) in treatment classrooms, whereas control classrooms provided business-as-usual science instruction. Overall treatment effects were observed on three of four science outcome measures. The reported effects were moderate to large, with effect sizes (Hedges’ g) ranging from 0.48 to 0.94. Moderation analyses indicated that science knowledge at pretest did not moderate Sci2’s effects. Implications for practice and research are discussed.},
 author = {Christian T. Doabler and William J. Therrien and Maria A. Longhi and Greg Roberts and Katherine E. Hess and Steven A. Maddox and Jasmine Uy and Gail E. Lovette and Anna-Maria Fall and Georgia L. Kimmel et al.},
 doi = {10.1177/0741932521989091},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0741932521989091},
 journal = {Remedial and Special Education},
 number = {3},
 pages = {140–154},
 title = {Efficacy of a Second-Grade Science Program: Increasing Science Outcomes for All Students},
 url = {https://doi-org.crai.referencistas.com/10.1177/0741932521989091},
 volume = {42},
 year = {2021c}
}

@article{doi:10.1177/07419325221117293,
 abstract = {The effects of whole number computation interventions among school students with learning disabilities in Grades K to 5 were examined using a multilevel meta-analysis. Applying a correlated and hierarchical effect model of robust variance estimation, we examined the intervention effects among 15 peer-reviewed articles and dissertations (two group-design and 13 single-case design studies) published between 1989 and 2021. Whole number computation interventions demonstrated a statistically significant and large effect on whole number computation outcomes ( = 3.74). The type of mathematical operations, type of whole number computation measures, and the number of instructional components did not significantly affect the magnitude of the effect size estimate. However, the results showed slightly larger average effect sizes for the addition problem, the accuracy measure, and the additional number of instructional components by one. The limitations and implications for the practice of the meta-analysis are discussed, and future research directions are proposed.},
 author = {Sun A. Kim and Diane P. Bryant and Brian R. Bryant and Mikyung Shin and Min Wook Ok},
 doi = {10.1177/07419325221117293},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07419325221117293},
 journal = {Remedial and Special Education},
 number = {4},
 pages = {332–347},
 title = {A Multilevel Meta-Analysis of Whole Number Computation Interventions for Students With Learning Disabilities},
 url = {https://doi-org.crai.referencistas.com/10.1177/07419325221117293},
 volume = {44},
 year = {2023i}
}

@article{doi:10.1177/07419325231206483,
 abstract = {Whole number computations are a critical skill that serves as a foundation upon which higher-order concepts in mathematics are taught to children. To facilitate their instruction, educators often use multiple representations to support a child’s cognition. Representations with physical manipulatives are widely studied through a graduated instructional sequence featuring concrete, representational, and abstract stages of learning. In contrast, research on representational sequences featuring virtual manipulatives is less robust. Thus, this study evaluated an instructional strategy with virtual manipulatives, static representational drawings, and abstract algorithms to teach multiplication to three U.S. elementary students with mathematics difficulty. A functional relation was established via a single-subject multiple probe design between the treatment and students’ accuracy performance. Baseline-corrected Tau estimates confirmed a medium effect size for all three students, while student performance on measures assessing the number of errors committed and the duration of sessions also returned favorable findings.},
 author = {Rajiv Satsangi and Stephanie D. Sigmon},
 doi = {10.1177/07419325231206483},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07419325231206483},
 journal = {Remedial and Special Education},
 number = {4},
 pages = {216–229},
 title = {Teaching Multiplicative Thinking With Virtual Representations to Children With Mathematics Difficulty},
 url = {https://doi-org.crai.referencistas.com/10.1177/07419325231206483},
 volume = {45},
 year = {2024q}
}

@article{doi:10.1177/07482337221144143,
 abstract = {Computational fluid dynamics (CFD) is an indispensable simulation tool for predicting the emission of pollutants in the work environment. Welding is one of the most common industrial processes that might expose the operators and surrounding workers to certain hazardous gaseous metal fumes. In the present study, we used computational fluid dynamics (CFD) methodology for simulating the emission of iron fumes from the shielded metal arc welding (SMAW) procedure. A galvanized steel chamber was fabricated to measure the pollutant concentration and identify the size of the fume created by the SMAW. Then, the emission of welding aerosol was simulated using a method of computational fluid-particle dynamics with the ANSYS 2020 R1 software. The highest amount of welding fumes concentration was related to iron fumes (i.e., 3045 μg/m3 with a diameter of 0.25 μm). The results of the current study indicated that the local exhaust and general ventilation system can prevent the spreading of welding fumes to the welder’s breathing zone and the surrounding environment. CFD was also found to be an efficient method for predicting the emission of the iron fumes created by SMAW as well as for selecting an appropriate ventilation system. However, further studies that take the modeling of welding-generated emission of additional metal particles and gases into account will need to be undertaken.},
 author = {Fatemeh Paridokht and Shiva Soury and Sara Karimi Zeverdegani},
 doi = {10.1177/07482337221144143},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07482337221144143},
 journal = {Toxicology and Industrial Health},
 note = {PMID:36464906},
 number = {1},
 pages = {36–48},
 title = {The simulation of the emission of iron fumes caused by shielded metal arc welding using a computational fluid dynamics method},
 url = {https://doi-org.crai.referencistas.com/10.1177/07482337221144143},
 volume = {39},
 year = {2023n}
}

@article{doi:10.1177/0748730412469499,
 abstract = {An internal circadian clock regulates the electrical activity of cardiac myocytes controlling the expression of potassium channel interacting protein–2 (KChIP2), which is a key regulator of cardiac electrical activity. Here, we examine how the circadian rhythm of KChIP2 expression affects the dynamics of human and murine ventricular action potentials (APs), as well as the intervals in the equivalent electrocardiograms (ECGs) reflecting the duration of depolarization and repolarization phases of the cardiac ventricular APs (QRS and QT intervals), with mathematical modeling. We show how the internal circadian clock can control the shape of APs and, in particular, predict AP, QRS, and QT interval prolongation following KChIP2 downregulation, as well as shortening of AP, QRS, and QT interval duration following KChIP2 upregulation. Based on the circadian expression of KChIP2, we can accurately predict the circadian rhythm in cardiac electrical activity and suggest the transient outward potassium currents as the key current for circadian rhythmicity. Our modeling work predicts a smaller effect of KChIP2 on AP and QT interval dynamics in humans. Taken together, these results support the role of KChIP2 as the key regulator of circadian rhythms in the electrical activity of the heart; we provide computational models that can be used to explore circadian rhythms in cardiac electrophysiology and susceptibility to arrhythmia.},
 author = {Panagiotis Fotiadis and Daniel B. Forger},
 doi = {10.1177/0748730412469499},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0748730412469499},
 journal = {Journal of Biological Rhythms},
 note = {PMID:23382593},
 number = {1},
 pages = {69–78},
 title = {Modeling the Effects of the Circadian Clock on Cardiac Electrophysiology},
 url = {https://doi-org.crai.referencistas.com/10.1177/0748730412469499},
 volume = {28},
 year = {2013e}
}

@article{doi:10.1177/0748730420901929,
 abstract = {From 1980 to 1991, Kyriacou, Hall, and collaborators (K&H) reported that the Drosophila melanogaster courtship song has a 1-min cycle in the length of mean interpulse intervals (IPIs) that is modulated by circadian rhythm period mutations. In 2014, Stern failed to replicate these results using a fully automated method for detecting song pulses. Manual annotation of Stern’s song records exposed a ~50% error rate in detection of IPIs, but the corrected data revealed period-dependent IPI cycles using a variety of statistical methods. In 2017, Stern et al. dismissed the sine/cosine method originally used by K&H to detect significant cycles, claiming that randomized songs showed as many significant values as real data using cosinor analysis. We first identify a simple mathematical error in Stern et al.’s cosinor implementation that invalidates their critique of the method. Stern et al. also concluded that although the manually corrected wild-type and perL mutant songs show similar periods to those observed by K&H, each song is usually not significantly rhythmic by the Lomb-Scargle (L-S) periodogram, so any genotypic effect simply reflects “noise.” Here, we observe that L-S is extremely conservative compared with 3 other time-series analyses in assessing the significance of rhythmicity, both for conventional locomotor activity data collected in equally spaced time bins and for unequally spaced song records. Using randomization of locomotor and song data to generate confidence limits for L-S instead of the theoretically derived values, we find that L-S is now consistent with the other methods in determining significant rhythmicity in locomotor and song records and that it confirms period-dependent song cycles. We conclude that Stern and colleagues’ failure to identify song cycles stems from the limitations of automated methods in accurately reflecting song parameters, combined with the use of an overly stringent method to discriminate rhythmicity in courtship songs.},
 author = {Charalambos P. Kyriacou and Harold B. Dowse and Lin Zhang and Edward W. Green},
 doi = {10.1177/0748730420901929},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0748730420901929},
 journal = {Journal of Biological Rhythms},
 note = {PMID:32096437},
 number = {3},
 pages = {235–245},
 title = {A Computational Error and Restricted Use of Time-series Analyses Underlie the Failure to Replicate period-Dependent Song Rhythms in Drosophila},
 url = {https://doi-org.crai.referencistas.com/10.1177/0748730420901929},
 volume = {35},
 year = {2020k}
}

@article{doi:10.1177/07591063231196161,
 abstract = {This article proposes a methodological approach that integrates quantitative analysis into ethnographic studies of online communities. By considering traces of interactions in digital environments, it demonstrates the potential benefits of computational analysis in enhancing online ethnography. The application of quantitative methods, which may be unconventional in the field of ethnography, effectively complements qualitative understanding. Computational analysis of digital traces provides a valuable framework for observations within online ethnography. Through a case study of a Russian-speaking sociological online community, this article highlights the points where research questions intersect, fostering a dialogue between computational analysis of digital traces and online ethnography. It describes the implementation of methods such as content analysis, network analysis, and topic modeling, while also reflecting on their analytical capabilities within the context of online ethnography.},
 author = {Larisa A. Barkhatova},
 doi = {10.1177/07591063231196161},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/07591063231196161},
 journal = {Bulletin of Sociological Methodology/Bulletin de Méthodologie Sociologique},
 number = {1},
 pages = {30–56},
 title = {The computational analysis of digital traces in ethnographic studies of online communities},
 url = {https://doi-org.crai.referencistas.com/10.1177/07591063231196161},
 volume = {160},
 year = {2023a}
}

@article{doi:10.1177/0829573520973087,
 abstract = {Although students with stronger executive functions (EFs) tend to do better on math computation (MC) assessments than students with weaker EFs, stressful testing situations may lower or affect their mathematical ability. Rumination is one maladaptive coping strategy that can negatively affect EF processes, but little is known about how it impacts the relationship between EFs and MC. This study aimed to examine the relationship between students’ performance on a standardized MC task and ratings of EF ability as a function of their level of rumination. In a sample of students from Grades 4 to 6 (n = 72, mean age = 10.74), there was an interaction between EF scores and rumination in predicting MC. Students with weaker EF scores demonstrated worse math performance than students with stronger EF scores. Interestingly, their level of rumination moderated this association. Specifically, EF difficulties were only associated with less proficient MC performance among high ruminators; this association was not observed among those students reporting low rumination levels. For school psychologists, these findings provide insight into the potential causes of poor MC performance among students with average or better EFs.},
 author = {Melissa Kang and Anne-Claude Bedard and Rhonda Martinussen},
 doi = {10.1177/0829573520973087},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0829573520973087},
 journal = {Canadian Journal of School Psychology},
 number = {3},
 pages = {206–220},
 title = {Rumination as a Moderating Effect Between Math Computation and Executive Function Skills in Elementary Students},
 url = {https://doi-org.crai.referencistas.com/10.1177/0829573520973087},
 volume = {36},
 year = {2021h}
}

@article{doi:10.1177/0846537120949974,
 abstract = {Breast cancer screening has been shown to significantly reduce mortality in women. The increased utilization of screening examinations has led to growing demands for rapid and accurate diagnostic reporting. In modern breast imaging centers, full-field digital mammography (FFDM) has replaced traditional analog mammography, and this has opened new opportunities for developing computational frameworks to automate detection and diagnosis. Artificial intelligence (AI), and its subdomain of deep learning, is showing promising results and improvements on diagnostic accuracy, compared to previous computer-based methods, known as computer-aided detection and diagnosis. In this commentary, we review the current status of computational radiology, with a focus on deep neural networks used in breast cancer screening and diagnosis. Recent studies are developing a new generation of computer-aided detection and diagnosis systems, as well as leveraging AI-driven tools to efficiently interpret digital mammograms, and breast tomosynthesis imaging. The use of AI in computational radiology necessitates transparency and rigorous testing. However, the overall impact of AI to radiology workflows will potentially yield more efficient and standardized processes as well as improve the level of care to patients with high diagnostic accuracy.},
 author = {William T. Tran and Ali Sadeghi-Naini and Fang-I Lu and Sonal Gandhi and Nicholas Meti and Muriel Brackstone and Eileen Rakovitch and Belinda Curpen},
 doi = {10.1177/0846537120949974},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0846537120949974},
 journal = {Canadian Association of Radiologists Journal},
 note = {PMID:32865001},
 number = {1},
 pages = {98–108},
 title = {Computational Radiology in Breast Cancer Screening and Diagnosis Using Artificial Intelligence},
 url = {https://doi-org.crai.referencistas.com/10.1177/0846537120949974},
 volume = {72},
 year = {2021p}
}

@article{doi:10.1177/0885066620965167,
 abstract = {Purpose: Purpose of this report is to describe the feasibility of lingual pulse oximetry and lingual near-infrared spectroscopy (NIRS) in a COVID-19 patient to assess lingual tissue viability after several days of mechanical ventilation in the prone position. Materials & Methods: In a COVID-19 ICU-patient, the tongue became grotesquely swollen, hardened and protruding from the oral cavity after 20 h of mechanical ventilation uninterrupted in the prone position. To assess the doubtful viability of the tongue, pulse-oximetric hemoglobin O2-saturation (SpO2; Nellcor, OxiMax MAX-NI, Covidien, MA, USA) and NIRS-based, regional tissue O2-saturation measurements (rSO2; SenSmart, Nonin, MN, USA) were performed at the tongue. Results: At the tongue, regular pulse-oximetric waveforms with a pulse-oximetric hemoglobin O2-saturation (SpO2) of 88% were recorded, i.e. only slightly lower than the SpO2 reading at the extremities at that time (90%). Lingual NIRS-based rSO2 measurements yielded stable tissue rSO2-values of 76-78%, i.e. values expected also in other adequately perfused and oxygenated (muscle-) tissues. Conclusion: Despite the alarming, clinical finding of a grotesquely swollen, rubber-hard tongue and clinical concerns on the adequacy of the tongue perfusion and oxygenation, our measurements of both arterial pulsatility (SpO2) and NIRS-based tissue oxygenation (rSO2) suggested adequate perfusion and oxygenation of the tongue, rendering non-vitality of the tongue, e.g. by lingual venous thrombosis, unlikely. To our knowledge, this is the first clinical report of lingual rSO2 measurement.},
 author = {Patrick Schober and Erik J. Lust and Leo M. A. Heunks and Lothar A. Schwarte},
 doi = {10.1177/0885066620965167},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0885066620965167},
 journal = {Journal of Intensive Care Medicine},
 note = {PMID:33034231},
 number = {3},
 pages = {376–380},
 title = {Thinking Out-of-the-Box: A Non-Standard Application of Standard Pulse-Oximetry and Standard Near-Infrared Spectroscopy in a COVID-19 Patient},
 url = {https://doi-org.crai.referencistas.com/10.1177/0885066620965167},
 volume = {36},
 year = {2021m}
}

@article{doi:10.1177/0888406421992376,
 abstract = {Increasingly in K–12 schools, students are gaining access to computational thinking (CT) and computer science (CS). This access, however, is not always extended to students with disabilities. One way to increase CT and CS (CT/CS) exposure for students with disabilities is through preparing special education teachers to do so. In this study, researchers explore exposing special education preservice teachers to the ideas of CT/CS in the context of a mathematics methods course for students with disabilities or those at risk of disability. Through analyzing lesson plans and reflections from 31 preservice special education teachers, the researchers learned that overall emerging promise exists with regard to the limited exposure of preservice special education teachers to CT/CS in mathematics. Specifically, preservice teachers demonstrated the ability to include CT/CS in math lesson plans and showed understanding of how CT/CS might enhance instruction with students with disabilities via reflections on these lessons. The researchers, however, also found a need for increased experiences and opportunities for preservice special education teachers with CT/CS to more positively impact access for students with disabilities.},
 author = {Emily C. Bouck and Phil Sands and Holly Long and Aman Yadav},
 doi = {10.1177/0888406421992376},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0888406421992376},
 journal = {Teacher Education and Special Education},
 number = {3},
 pages = {221–238},
 title = {Preparing Special Education Preservice Teachers to Teach Computational Thinking and Computer Science in Mathematics},
 url = {https://doi-org.crai.referencistas.com/10.1177/0888406421992376},
 volume = {44},
 year = {2021b}
}

@article{doi:10.1177/08902070231161869,
 abstract = {Assessing evolution of cognitive structures across historical periods has remained challenging in the absence of direct access to humans from the past. Overcoming some of these challenges, we examined shifts in the implicit cognitive structures in the Epic of Gilgamesh, which is one of the earliest surviving pieces of literature, circulating in various versions over a period of approx. 2000 years in ancient Mesopotamia. Using a canonical English translation, we applied natural language processing (NLP) and human coding to extract low-dimensional representations of the implicit personality structure in three different historical epochs. We found systematic shifts over time with increasing complexity and increasing resemblance of contemporary personality models in later periods. We discuss how lexical analyses of ancient texts using trait co-occurrence analyses can provide novel insights on the evolution of human behaviour of relevance for contemporary social and behavioural science and the study of ancient societies.},
 author = {Amy He Du and Johannes A. Karl and Velichko Fetvadjiev and Markus Luczak-Roesch and Reinhard Pirngruber and Ronald Fischer},
 doi = {10.1177/08902070231161869},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/08902070231161869},
 journal = {European Journal of Personality},
 number = {2},
 pages = {274–290},
 title = {Tracing the evolution of personality cognition in early human civilisations: A computational analysis of the Gilgamesh epic},
 url = {https://doi-org.crai.referencistas.com/10.1177/08902070231161869},
 volume = {38},
 year = {2024g}
}

@article{doi:10.1177/0892705705041156,
 abstract = {A theoretical study is presented for predicting the hygrothermal behavior of laminated composite plates during moisture desorption. This investigation is aimed at overcoming the computational difficulties in the determination of transient hygroscopic stress distribution through laminated plates and in the evaluation of the effect of anisotropy on the evolution of such stresses. To calculate such stresses during desorption phase, we present a new method by which we are able to predict directly the evolution of transient hygroscopic stresses within laminated composite plates without the calculation of moisture concentrations as in the case of Benkeddad’s method in which we are obliged to determine the moisture distribution across the plate before the calculation of such stresses. This is very important and must be taken into account in the design of composite material, particularly aerospace structures for example, aircraft. The second part of this investigation is devoted to the analysis of the effect of anisotropy on the hygrothermal behavior of laminated plates during moisture desorption. The polar representation method of anisotropic properties is selected as a convenient tool for this study. By this method, the author is able to assess the anisotropy of composite laminated plates using the degree of anisotropy. Some stacking sequences with different in-plane degree of anisotropy were investigated in order to show the effect of anisotropy on the transverse hygroscopic stresses. Finally, the results of this paper can be used to guide the derivation of future by opening a way for the optimal design of composite structures submitted to a moist environment.},
 author = {A. Tounsi and E. A. Adda Bedia and A. Benachour},
 doi = {10.1177/0892705705041156},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0892705705041156},
 journal = {Journal of Thermoplastic Composite Materials},
 number = {1},
 pages = {37–58},
 title = {A New Computational Method for Prediction of Transient Hygroscopic Stresses                 during Moisture Desorption in Laminated Composite Plates with Different Degrees of Anisotropy},
 url = {https://doi-org.crai.referencistas.com/10.1177/0892705705041156},
 volume = {18},
 year = {2005o}
}

@article{doi:10.1177/089443902237317,
 abstract = {There has been significant recent interest in Agent Based Modeling in many social sciences including economics, sociology, anthropology, political science, and game theory. This article describes three problems that need to be addressed in order for such models to become effective tools for formulating new social theory and informing policy debates and suggests approaches to meeting them. These issues are computational epistemology, research methodology, and software technology. These innovations augment Agent Based Modeling to create an effective new tool base to help better understand complex social systems.},
 author = {Steven Bankes and Robert Lempert and Steven Popper},
 doi = {10.1177/089443902237317},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/089443902237317},
 journal = {Social Science Computer Review},
 number = {4},
 pages = {377–388},
 title = {Making Computational Social Science Effective: Epistemology, Methodology, and Technology},
 url = {https://doi-org.crai.referencistas.com/10.1177/089443902237317},
 volume = {20},
 year = {2002c}
}

@article{doi:10.1177/089443930001800210,
 abstract = {The capacity to explain important elements of social life is central both to the development of sociological theory and to teaching sociology. This research seeks to expand our understanding of sociological explanation through a computational approach. Explanations commonly encountered in introductory sociology texts are used to develop a typology of explanatory forms. A computational strategy that represents sociological knowledge using a combination of frames, semantic networks, and procedural rules is described. It is then demonstrated that this approach can generate the full range of these explanations for all logical combinations of conditions and for the full scope of sociological knowledge. This approach is also shown to be capable of identifying appropriate explanations, assessing the quality of explanations, and generating new insights.},
 author = {Edward Brent and Alan Thompson and Whitley Vale},
 doi = {10.1177/089443930001800210},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/089443930001800210},
 journal = {Social Science Computer Review},
 number = {2},
 pages = {223–235},
 title = {Sociology: A Computational Approach to Sociological Explanations},
 url = {https://doi-org.crai.referencistas.com/10.1177/089443930001800210},
 volume = {18},
 year = {2000c}
}

@article{doi:10.1177/089443930101900105,
 abstract = {There has been a continued expansion of the uses of computer-based tools and techniques in public sector endeavors: from traditional notions of data collection and management (bean counting) to the processing of data into information that supports managerial activities. Advances in technology based on emerging work in decision theory, information science, and cognitive science will allow for use of these computational models in more expansive “advisory” roles to decision makers of all types. To what degree can public sector decision makers use computational models to support or advise decision making? As these new technologies become a routine part of the policy process, will a belief in computer omnipotence tempt public sector decision makers to abdicate personal responsibility for poor choices? The authors explore the choice-related implications associated with the increased use of computer-based models, define possible computer-based decision support models (CBDSMs), and present a typology of credible uses of CBDSMs.},
 author = {Desmond Saunders-Newton and Harold Scott},
 doi = {10.1177/089443930101900105},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/089443930101900105},
 journal = {Social Science Computer Review},
 number = {1},
 pages = {47–65},
 title = {“But the Computer Said!”: Credible Uses of Computational Modeling in Public Sector Decision Making},
 url = {https://doi-org.crai.referencistas.com/10.1177/089443930101900105},
 volume = {19},
 year = {2001l}
}

@article{doi:10.1177/0894439303262565,
 abstract = {Sociologists of earlier eras had much more limited computational tools available, but even in the founding era, there were slide rules and printed tables of mathematical functions, mechanical calculators were becoming more common, and punched card tabulating equipmentwas being developed. This article focuses on one particular founder, Franklin H. Giddings, and the kind of computations he did or did not do, using or not using various computational devices then available. Giddings’s reputation, as the leading quantitative sociologist among the founders of the discipline, is compared with his published work. Giddings himself, although a prolific author, did very little original quantitative research. His main contribution to quantitative sociology consisted of his programmatic statements, which helped provide legitimation for subsequent scholars who actually performed quantitative research.},
 author = {David D. McFarland},
 doi = {10.1177/0894439303262565},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0894439303262565},
 journal = {Social Science Computer Review},
 number = {2},
 pages = {249–255},
 title = {Adding Machines and Logarithms: Franklin H. Giddings and Computation for the Exact Science of Sociology},
 url = {https://doi-org.crai.referencistas.com/10.1177/0894439303262565},
 volume = {22},
 year = {2004m}
}

@article{doi:10.1177/0894439305282430,
 abstract = {Approaches to modeling social phenomena vary on a continuum from simple models, in which causality is clear and parameters few, to realistic, high-fidelity models designed to capture the most detailed system behavior possible in a specific setting. Anthropologists have produced both simple and high-fidelity models. The focus of this article is on high-fidelity modeling in anthropology and the special challenge its complexity presents for model comparison. Useful model comparison requires docking, or rendering models comparable, and the author presents a framework for docking based on the work of Axelrod, and Cioffi-Revilla and Gotts. Docking not only renders models more comparable, allowing for more traditional theory testing, but it also sharpens the discussion about the ontology of anthropological phenomena and how they are best represented as theories and models.},
 author = {Lawrence A. Kuznar},
 doi = {10.1177/0894439305282430},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0894439305282430},
 journal = {Social Science Computer Review},
 number = {1},
 pages = {15–29},
 title = {High-Fidelity Computational Social Science in Anthropology: Prospects for Developing a Comparative Framework},
 url = {https://doi-org.crai.referencistas.com/10.1177/0894439305282430},
 volume = {24},
 year = {2006e}
}

@article{doi:10.1177/0894439306293820,
 abstract = {Human, social, and international conflicts are inescapable facts of life. This article elaborates on how advances in simulation theory and methodology can improve exploring and studying the dynamics of conflicts. The premise of the proposed strategy is based on the observation that agent-based social simulation, which enables high-level and powerful problem-solving capabilities, needs to be significantly enhanced to model complex, multilevel, and multistaged conflicts. To this end, the issues, challenges, and rationale underlying a novel simulation methodology, called multisimulation, are explained. The conceptual foundations needed for the realization of multisimulation are presented to contribute to the development of advanced simulation-based computational laboratories for conflict studies. Finally, by using the Bloomfield-Leiss dynamic phase model of conflict, the author argues for the utility of multisimulation.},
 author = {Levent Yilmaz},
 doi = {10.1177/0894439306293820},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0894439306293820},
 journal = {Social Science Computer Review},
 number = {1},
 pages = {48–60},
 title = {Toward Next-Generation, Simulation-Based Computational Tools for Conflict and Peace Studies},
 url = {https://doi-org.crai.referencistas.com/10.1177/0894439306293820},
 volume = {25},
 year = {2007p}
}

@article{doi:10.1177/0894439312455914,
 abstract = {Kinship is a fundamental feature and basis of human societies. We describe a set of computational tools and services, and the logic that underlies these, developed to improve how we understand both the fundamental facts of kinship and how people use kinship as a resource in their lives. Mathematical formalism applied to cultural concepts is more than an exercise in model building, as it provides a way to represent and explore their logical consistency and implications. Not surprisingly, kinship terminologies are particularly amenable to formal representation. Researchers throughout the history of kinship studies have noted the logicality of kinship terminology systems. The logic is explored here through the kin term computations made by users of a terminology when computing the kinship relation one person has to another by referring to a third person for whom each has a kin term relationship. Kinship Algebra Modeler provides a set of tools, services, and an architecture to explore kinship terminologies and their properties in an accessible manner.},
 author = {Dwight Read and Michael Fischer and Murray Leaf},
 doi = {10.1177/0894439312455914},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0894439312455914},
 journal = {Social Science Computer Review},
 number = {1},
 pages = {16–44},
 title = {What Are Kinship Terminologies, and Why Do We Care? A Computational Approach to Analyzing Symbolic Domains},
 url = {https://doi-org.crai.referencistas.com/10.1177/0894439312455914},
 volume = {31},
 year = {2013q}
}

@article{doi:10.1177/0894439314534810,
 abstract = {Replication is a critical element of the scientific process. This article is an effort to contribute to the slowly growing literature concerning the replication of agent-based computational models. We present a replication of Kollman, Miller, and Page’s model of Tiebout sorting. In that model, individual agents with heterogeneous preferences for government policies select among jurisdictions that offer the most satisfactory package of government services. This project makes three contributions to the literature. First, our successful replication provides the research community with a modernized version of that seminal model. Second, we confirm that earlier results with respect to the single jurisdiction setting are highly robust with respect to voter preferences, while the results for multiple jurisdiction settings are more sensitive. Finally, we demonstrate a technique for conducting sensitivity analyses that leverages a high-dimensional experimental design.},
 author = {Chad W. Seagren},
 doi = {10.1177/0894439314534810},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0894439314534810},
 journal = {Social Science Computer Review},
 number = {2},
 pages = {198–216},
 title = {A Replication and Analysis of Tiebout Competition Using an Agent-Based Computational Model},
 url = {https://doi-org.crai.referencistas.com/10.1177/0894439314534810},
 volume = {33},
 year = {2015o}
}

@article{doi:10.1177/0894439320914505,
 abstract = {Anxiety is a pervasive emotional state that tends to arise in situations involving uncertainty due partly to social and contextual issues including competition, economic disparity, and social insecurity. Thus, distribution of aggregate emotions, such as in anxiety, may reveal an important picture of otherwise invisible social processes in which individuals interact with local and global opportunities, constraints, and potential threats. The aim of this study is to present a computational approach to the dynamic distribution of anxiety extracted from natural language expressions of users of Twitter, a popular global social media platform. We develop an unsupervised machine learning procedure based on a naive Bayes model to classify contents of anxiety, estimate the degree of anxiety, and construct a geographic map of spatiotemporal distribution of anxiety. To validate our mapping results, a multilevel statistical analysis was performed to examine how anxiety distribution is correlated with other district-level sociodemographic statistics such as rates of birth and early divorce. Implications for further research and extension are discussed.},
 author = {Yong Suk Choi and Hansung Kim and Dongyoung Sohn},
 doi = {10.1177/0894439320914505},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0894439320914505},
 journal = {Social Science Computer Review},
 number = {3},
 pages = {598–617},
 title = {Mapping Social Distress: A Computational Approach to Spatiotemporal Distribution of Anxiety},
 url = {https://doi-org.crai.referencistas.com/10.1177/0894439320914505},
 volume = {40},
 year = {2022d}
}

@article{doi:10.1177/0894439320951766,
 abstract = {The increasing availability of data, along with sophisticated computational methods for analyzing them, presents researchers with new opportunities and challenges. In this article, we address both by describing computational and network methods that can be used to identify cases of rare phenomena. We evaluate each method’s relative utility in the identification of a specific rare phenomenon of interest to social movement researchers: the spillover of social movement claims from one movement to another. We identify and test five different approaches to detecting cases of spillover in the largest data set of protest events currently available, finding that an ensemble approach that combines clique and correspondence analysis and an ensemble approach combining all methods perform considerably better than others. Our approach is preferable to other ways of analyzing such cases; compared to qualitative approaches, our computational process identifies many more cases of spillover—some of which are surprising and would likely not be otherwise investigated. At the same time, compared to crude quantitative measures, our approach substantially reduces the “noise,” or identification of false-positive cases, of movement spillover. We argue that this technique, which can be adapted to other research topics, is a good illustration of how the thoughtful implementation of computational methods can allow for the efficient identification of rare events and also bridge deductive and inductive approaches to scientific inquiry.},
 author = {Thomas Elliott and Misty Ring-Ramirez and Jennifer Earl},
 doi = {10.1177/0894439320951766},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0894439320951766},
 journal = {Social Science Computer Review},
 number = {5},
 pages = {981–1002},
 title = {Spillover as Movement Agenda Setting: Using Computational and Network Techniques for Improved Rare Event Identification},
 url = {https://doi-org.crai.referencistas.com/10.1177/0894439320951766},
 volume = {39},
 year = {2021c}
}

@article{doi:10.1177/08944393211053743,
 abstract = {The U.S. confronts an unprecedented public health crisis, the COVID-19 pandemic, in the presidential election year in 2020. In such a compound situation, a real-time dynamic examination of how the general public ascribe the crisis responsibilities taking account to their political ideologies is helpful for developing effective strategies to manage the crisis and diminish hostility toward particular groups caused by polarization. Social media, such as Twitter, provide platforms for the public’s COVID-related discourse to form, accumulate, and visibly present. Meanwhile, those features also make social media a window to monitor the public responses in real-time. This research conducted a computational text analysis of 2,918,376 tweets sent by 829,686 different U.S. users regarding COVID-19 from January 24 to May 25, 2020. Results indicate that the public’s crisis attribution and attitude toward governmental crisis responses are driven by their political identities. One crisis factor identified by this study (i.e., threat level) also affects the public’s attribution and attitude polarization. Additionally, we note that pandemic fatigue was identified in our findings as early as in March 2020. This study has theoretical, practical, and methodological implications informing further health communication in a heated political environment.},
 author = {Weilu Zhang and Lingshu Hu and Jihye Park},
 doi = {10.1177/08944393211053743},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/08944393211053743},
 journal = {Social Science Computer Review},
 number = {3},
 pages = {790–811},
 title = {Politics Go “Viral”: A Computational Text Analysis of the Public Attribution and Attitude Regarding the COVID-19 Crisis and Governmental Responses on Twitter},
 url = {https://doi-org.crai.referencistas.com/10.1177/08944393211053743},
 volume = {41},
 year = {2023s}
}

@article{doi:10.1177/08944393211055429,
 abstract = {This research quantitatively investigates the impact of violence on the propagation of images in social media in the context of political protest. Using a computational approach, we measure the relative violence of a large set of images shared on Twitter during the protests against the G20 summit in Frankfurt am Main in 2017. This allows us to investigate if more violent content is shared more times and faster than less violent content on Twitter, and if different online communities can be characterized by the level of violence of the visual content they share. The results show that the level of violence in an image tweet does not correlate with the number of retweets and mentions it receives that the time to retweet is marginally lower for image tweets containing a high level of violence and that the level of violence in image tweets differs between communities.},
 author = {Luca Rossi and Christina Neumayer and Jesper Henrichsen and Lucas K. Beck},
 doi = {10.1177/08944393211055429},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/08944393211055429},
 journal = {Social Science Computer Review},
 number = {3},
 pages = {905–925},
 title = {Measuring Violence: A Computational Analysis of Violence and Propagation of Image Tweets From Political Protest},
 url = {https://doi-org.crai.referencistas.com/10.1177/08944393211055429},
 volume = {41},
 year = {2023s}
}

@article{doi:10.1177/08944393211058112,
 abstract = {Since its inception, computational social science (CSS) has been characterized as an interdisciplinary field. Nevertheless, unlike a mature interdisciplinary field that duly integrates knowledge beyond disciplinary boundaries, CSS has arguably been fragmented into multiple lines of inquiry. Although such fragmentation has been a constant concern, limited empirical evidence exists to substantiate any degree of integration in research on CSS. To systematically map the landscape of research on CSS, we examined the dynamic topology of the bibliometric network of CSS during the past 20 years. By comparing the structure of the coupling network and the co-subject network with three prototypical network models that we simulated—the centralized model, the fragmented model, and the cohesive model—our analysis revealed that the citation networks of research on CSS are highly centralized but less fragmented than often assumed. Beyond that, a driving factor shaping the coupling network’s cohesive structure is the citation to high-impact articles. Those and other findings contribute to current understandings of the process of integration in the evolution of disciplines.},
 author = {Xiaohui Wang and Yunya Song and Youzhen Su},
 doi = {10.1177/08944393211058112},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/08944393211058112},
 journal = {Social Science Computer Review},
 number = {3},
 pages = {946–966},
 title = {Less Fragmented but Highly Centralized: A Bibliometric Analysis of Research in Computational Social Science},
 url = {https://doi-org.crai.referencistas.com/10.1177/08944393211058112},
 volume = {41},
 year = {2023r}
}

@article{doi:10.1177/08944393211073169,
 abstract = {We perform a socio-computational interrogation of the google search by image algorithm, a main component of the google search engine. We audit the algorithm by presenting it with more than 40 thousands faces of all ages and more than four races and collecting and analyzing the assigned labels with the appropriate statistical tools. We find that the algorithm reproduces white male patriarchal structures, often simplifying, stereotyping and discriminating females and non-white individuals, while providing more positive descriptions of white men. By drawing from Bourdieu’s theory of cultural reproduction, we link these results to the attitudes of the algorithm’s designers, owners, and the dataset the algorithm was trained on. We further underpin the problematic nature of the algorithm by using the ethnographic practice of studying-up: We show how the algorithm places individuals at the top of the tech industry within the socio-cultural reality that they shaped, many times creating biased representations of them. We claim that the use of social-theoretic frameworks such as the above are able to contribute to improved algorithmic accountability, algorithmic impact assessment and provide additional and more critical depth in algorithmic bias and auditing studies. Based on the analysis, we discuss the scientific and design implications and provide suggestions for alternative ways to design just socio-algorithmic systems.},
 author = {Orestis Papakyriakopoulos and Arwa M. Mboya},
 doi = {10.1177/08944393211073169},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/08944393211073169},
 journal = {Social Science Computer Review},
 number = {4},
 pages = {1100–1125},
 title = {Beyond Algorithmic Bias: A Socio-Computational Interrogation of the Google Search by Image Algorithm},
 url = {https://doi-org.crai.referencistas.com/10.1177/08944393211073169},
 volume = {41},
 year = {2023p}
}

@article{doi:10.1177/08944393221117268,
 abstract = {With the advent of Industry 4.0, future workspaces are expected to evolve in tandem with technological advances in industry and education. Industry 4.0 calls for transformation and effective talent development is vital in ensuring national aspirations are achieved while eliminating redundancy and ensuring consistency. As such, this study aims to understand the impact of Industry 4.0 on computer engineering-related workforce and skills development within Multinational Companies (MNCs) to Small Medium Enterprises (SMEs) in Malaysia. In this study, online questionnaires were distributed to evaluate the current and future hiring trends. The study reveals that most of the employees have positive perceptions about the industrial current practice on Industry 4.0 and identified the prospective demands on the professions that will be affected. Five significant areas of required competencies found in this study are adaptability, soft skills, software engineering, data analytics, and technical skills. The findings provide empirical evidence about current and future employment scenarios in Malaysia concerning the possible impact of Industry 4.0 on the companies and issues involved in managing the transition to Industry 4.0. Besides, the emergent skills required by workforces that are previously unaddressed in the literature were identified. Empirical evidence from the analysis contributes to shaping the educational systems of the future and helps to proactively identify specific skills shortages at an early stage.},
 author = {Mohd Heikal Husin and Noor Farizah Ibrahim and Nor Athiyah Abdullah and Sharifah Mashita Syed-Mohamad and Nur Hana Samsudin and Leonard Tan},
 doi = {10.1177/08944393221117268},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/08944393221117268},
 journal = {Social Science Computer Review},
 number = {5},
 pages = {1671–1690},
 title = {The Impact of Industrial Revolution 4.0 and the Future of the Workforce: A Study on Malaysian IT Professionals},
 url = {https://doi-org.crai.referencistas.com/10.1177/08944393221117268},
 volume = {41},
 year = {2023i}
}

@article{doi:10.1177/08944393231167692,
 abstract = {The domains of computational social anthropology and computational ethnography refer to the computational processing or computational modelling of data for anthropological or ethnographic research. In this context, the article surveys the use of computational methods regarding the production and the representation of knowledge. The ultimate goal of the study is to highlight the significance of modelling ethnographic data and anthropological knowledge by harnessing the potential of the semantic web. The first objective was to review the use of computational methods in anthropological research focusing on the last 25 years, while the second objective was to explore the potential of the semantic web focusing on existing technologies for ontological representation. For these purposes, the study explores the use of computers in anthropology regarding data processing and data modelling for more effective data processing. The survey reveals that there is an ongoing transition from the instrumentalisation of computers as tools for calculations, to the implementation of information science methodologies for analysis, deduction, knowledge representation, and reasoning, as part of the research process in social anthropology. Finally, it is highlighted that the ecosystem of the semantic web does not subserve quantification and metrics but introduces a new conceptualisation for addressing and meeting research questions in anthropology.},
 author = {Manolis Peponakis and Sarantos Kapidakis and Martin Doerr and Eirini Tountasaki},
 doi = {10.1177/08944393231167692},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/08944393231167692},
 journal = {Social Science Computer Review},
 number = {1},
 pages = {84–102},
 title = {From Calculations to Reasoning: History, Trends and the Potential of Computational Ethnography and Computational Social Anthropology},
 url = {https://doi-org.crai.referencistas.com/10.1177/08944393231167692},
 volume = {42},
 year = {2024j}
}

@article{doi:10.1177/08944393241252640,
 abstract = {Hashtag campaigns calling out sexual violence and rape myths offer a unique context for disclosing sexual victimization on social media. This study investigates the applicability of adaptive self-reflection as a potential self-effect from such public disclosures of unreported sexual victimization experiences by analyzing 92,583 tweets that invoked #WhyIDidntReport. A supervised machine learning classifier determined that 61.8% of the tweets were self-disclosures of sexual victimization. Linguistic Inquiry and Word Count (LIWC) analysis showed statistically significant differences in four psycholinguistic dimensions (greater use of past focus, cognitive processes, insight, and causation words) connected with reflective processing in tweets with self-disclosed sexual victimization compared to those without. Additionally, topic modeling and thematic analysis identified nine salient topics within the self-disclosing tweets, comprising three self-distanced representations (i.e., relatively abstract and insightful construals) of the unwanted experiences: (a) acknowledging one’s previously unacknowledged victimization, (b) reaffirming one’s rationale for not reporting, and (c) decrying invalidating response to one’s disclosure. Moving beyond reception effects and social support in extant research about social media as a coping tool, this study provides new empirical insights into the potential of social media to promote expressive meaning-making of upsetting and traumatic experiences in ways that support recovery and resilience.},
 author = {Tien Ee Dominic Yeo and Tsz Hang Chu},
 doi = {10.1177/08944393241252640},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/08944393241252640},
 journal = {Social Science Computer Review},
 number = {0},
 pages = {08944393241252640},
 title = {Adaptive Self-Reflection as a Social Media Self-Effect: Insights from Computational Text Analyses of Self-Disclosures of Unreported Sexual Victimization in a Hashtag Campaign},
 url = {https://doi-org.crai.referencistas.com/10.1177/08944393241252640},
 volume = {0},
 year = {2024t}
}

@article{doi:10.1177/089443939301100407,
 abstract = {This paper proposes a computational approach to sociology. I argue that computational sociology is not just another specialty within an already fragmented discipline but that it provides an opportunity to integrate traditionally disparate approaches within sociology and provides a foundation for transforming sociology in the 2Ist century. I discuss important changes in computing that offer new opportunities for sociology and illustrate those points with examples of ongoing sociological research. The potential advantages of such an approach are identified and some of the characteristics of computational sociology are described. Keywords: sociology, computers, computational sociology, paradigm change, research methodology.},
 author = {Edward E. Brent},
 doi = {10.1177/089443939301100407},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/089443939301100407},
 journal = {Social Science Computer Review},
 number = {4},
 pages = {487–499},
 title = {Computational Sociology: Reinventing Sociology for the Next Millennium},
 url = {https://doi-org.crai.referencistas.com/10.1177/089443939301100407},
 volume = {11},
 year = {1993d}
}

@article{doi:10.1177/0895904818802116,
 abstract = {This ethnographic case investigates the relationship between the daily organizing work of one education technology “intermediary organization” (IO) in Silicon Valley, California and federal education technology policies. I argue that the IO constructed policy knowledge that reified discourses of “digital meritocracy”: a belief in digital technologies as a means of evaluating individual success, regardless of historic, place-based material inequities. To develop this concept, I trace themes of “personalization” and “everywhere” as they emerge in the IO’s daily work and in federal education technology policies. This study extends research on IOs as “brokers” of information, resources, and social ties between public schools and private service providers and argues that IOs also construct “policy knowledge,” or “definitions of what counts as education.”},
 author = {Ethan Chang},
 doi = {10.1177/0895904818802116},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0895904818802116},
 journal = {Educational Policy},
 number = {5},
 pages = {760–784},
 title = {Digital Meritocracy: Intermediary Organizations and the Construction of Policy Knowledge},
 url = {https://doi-org.crai.referencistas.com/10.1177/0895904818802116},
 volume = {34},
 year = {2020d}
}

@article{doi:10.1177/0895904819843597,
 abstract = {Many countries have launched national educational technology policies in the past number of decades aimed at increasing technology integration in schools. This article analyses educational technology policy in Ireland from 1997 to 2017 and draws attention to an underlying economic agenda for technology integration in schools, set against a backdrop of neoliberal discourses. The study found that the challenge of integrating technology is no longer understood as simply a challenge of “integration” but rather as a realignment of the existing education system toward a more student-centered experience. Most recent policy also recognizes the complex and contextually bound nature of the associated change process suggesting a maturation of understanding in respect of the intersection between technology and education. The article also highlights the symbolic function of policy and the role it plays in representing the educational system in a particular light to national and international audiences. Recognizing the symbolic function of educational technology policy and the neoliberal ideology underpinning it can help identify the reasons for the apparent failures of past attempts to integrate technology in schools as well as informing future policy iterations.},
 author = {Oliver McGarr and Keith Johnston},
 doi = {10.1177/0895904819843597},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0895904819843597},
 journal = {Educational Policy},
 number = {6},
 pages = {841–865},
 title = {Exploring the Evolution of Educational Technology Policy in Ireland: From Catching-Up to Pedagogical Maturity},
 url = {https://doi-org.crai.referencistas.com/10.1177/0895904819843597},
 volume = {35},
 year = {2021l}
}

@article{doi:10.1177/0951629812473007,
 abstract = {Representative democracy translates the preferences of the electorate into policy outcomes. Individual voters do not directly vote on policy; rather, their elected representatives create and establish policy. How well do the institutions of representative democracy translate the preferences of the electorate into policy? Is there any systematic bias in a representative democracy system? After formulating a series of computational models it appears that the degree to which legislative districts are ‘gerrymandered’ with respect to preferences about the policy is a source of policy bias. To illustrate the phenomenon, household income is used as a proxy for voters’ preferences with respect to redistribution. Even when the majority of voters are in favor of redistribution, if districts are constructed with a sufficient level of conservative gerrymandering, the policy outcome under representative democracy will favor far less redistribution than the policy outcome under direct democracy.},
 author = {Robi Ragan},
 doi = {10.1177/0951629812473007},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0951629812473007},
 journal = {Journal of Theoretical Politics},
 number = {4},
 pages = {467–491},
 title = {Institutional sources of policy bias: A computational investigation},
 url = {https://doi-org.crai.referencistas.com/10.1177/0951629812473007},
 volume = {25},
 year = {2013r}
}

@article{doi:10.1177/0954405415624657,
 abstract = {A numerical model based on computational fluid dynamics is developed to analyze fluid flow and thermal aspects in grinding. The model uses multiphase fluid flow with heat transfer based on the volume-of-fluid method, convection, conduction in solids and a multiple reference frame model of the porous grinding zone. Fluid velocity vectors, useful flow rate, grinding temperatures and energy partition are predicted using the model. In lieu of direct measurements of these quantities, the verification relies on the indirect assessment of surface integrity. The simulation results provide adequate agreement with the measured residual stress, depth of heat-affected zone and full width at half maximum profile with respect to the grinding temperatures.},
 author = {Stefan Mihić and Radovan Dražumerič and Franci Pušavec and Jeffrey Badger and Peter Krajnik},
 doi = {10.1177/0954405415624657},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954405415624657},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part B: Journal of Engineering Manufacture},
 number = {12},
 pages = {2103–2111},
 title = {The use of computational fluid dynamics in the analysis of fluid flow and thermal aspects in grinding},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954405415624657},
 volume = {231},
 year = {2017p}
}

@article{doi:10.1177/09544054JEM2013,
 abstract = {A fully coupled thermomechanical finite element analysis of the friction-stir welding (FSW) process developed in the authors’ previous work is combined with the basic physical metallurgy of Ti—6Al—4V to predict/assess the structural response of FSW joints. A close examination of the experimental results reported in the open literature reveals that in most cases the heat-affected zone (HAZ) of the weld possesses the most inferior properties and tends to control the overall structural performance of the weld. Taking this observation into account, a microstructure evolution model is developed and parameterized for the Ti—6Al—4V material residing in the HAZ. Specifically, this model addresses the problem of temporal evolution of the globular α-phase particles located within prior β-phase grains (the dominant microstructural parameter in the HAZ) during the FSW process. Next this model is combined with the well-established property versus microstructure correlations in Ti—6Al—4V in order to predict the overall structural performance of the weld. The results obtained are found to be in reasonably good agreement with their experimental counterparts, suggesting that the present computational approach may be used to guide the selection of FSW process parameters in order to optimize the structural performance of FSW joints (at least while they are controlled by the HAZ-material microstructure/properties).},
 author = {M Grujicic and G Arakere and B Pandurangan and A Hariharan and B A Cheeseman and C-F Yen and C Fountzoulas},
 doi = {10.1177/09544054JEM2013},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09544054JEM2013},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part B: Journal of Engineering Manufacture},
 number = {2},
 pages = {208–223},
 title = {Computational analysis and experimental validation of the friction-stir welding behaviour of Ti—6Al—4V},
 url = {https://doi-org.crai.referencistas.com/10.1177/09544054JEM2013},
 volume = {225},
 year = {2011i}
}

@article{doi:10.1177/095440620121500807,
 abstract = {Abstract This paper describes the comparison between experimental and computational results for the flow in an inlet system that contains a plenum. Here, the main focus is on the details of the computations and the comparison with experimental results for flow in the plenum. Details of the experiment are described elsewhere. By varying mesh density, boundary conditions, discretization schemes and turbulence models, a wide-ranging study of the accuracy of computational techniques for an industrial problem has been made. In particular, levels of mesh refinement for converged numerical solutions have been determined, together with performance levels for the various settings used. In terms of velocity prediction, when the flow is driven by boundary conditions specified at the outlet more accurate results are achieved. This is due to the flow at the inlet being calculated more accurately in these cases. The effects of different discretization schemes and turbulence models on the velocity predictions are small. Pressure predictions are, however, improved by more complex turbulence models such as the renormalization group (RNG) model.},
 author = {C. T. Shaw and D. J. Lee and S. H. Richardson and S Pierson},
 doi = {10.1177/095440620121500807},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/095440620121500807},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
 number = {8},
 pages = {943–953},
 title = {The flow through a plenum-runner system—a comparison of experiment and computation},
 url = {https://doi-org.crai.referencistas.com/10.1177/095440620121500807},
 volume = {215},
 year = {2001r}
}

@article{doi:10.1177/0954406211407260,
 author = {RE Mickens},
 doi = {10.1177/0954406211407260},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954406211407260},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
 number = {8},
 pages = {2003–2004},
 title = {Thinking About Equations: A practical Guide for Developing Mathematical Intuition in the Physical Sciences and Engineering},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954406211407260},
 volume = {225},
 year = {2011m}
}

@article{doi:10.1177/0954406211411866,
 abstract = {As a special measure in lightweight design, residual stresses can be used for improving the stability and vibration behaviour of thin-walled structures. In contrast to the common practice of inducing proper residual stresses by mechanically caused local plastic deformation, in the present paper it is shown by numerical procedures as well as by experiments how such beneficial residual stresses can be produced by a proper heat treatment using laser. This is shown by simple plate examples, for proof of concept. A finite element approach is combined with algorithms simulating the heat input from laser beams. The analyses comprise investigations of a single laser dot for studying the fundamental behaviour of laser treatment according to the abovementioned aspects, and – for practical applications – continuously moving and intermittently acting beams are considered. Unwanted effects, such as distortions and possible instabilities appearing during the laser treatment, are considered as well, and methods are presented for avoiding them. A number of generic laser tracks as well as patterns of laser dots of different configurations are investigated. Experiments were performed which confirm the potential of laser treatment of plates for improving stability and dynamic behaviour.},
 author = {C Bilik and F G Rammerstorfer and G Figala and B Buchmayr},
 doi = {10.1177/0954406211411866},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954406211411866},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
 number = {10},
 pages = {2385–2398},
 title = {Computational modelling of laser treatment of plates for increased buckling loads and natural frequencies},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954406211411866},
 volume = {225},
 year = {2011c}
}

@article{doi:10.1177/0954406211415777,
 abstract = {The aim of this study is to investigate the use of computational fluid dynamics in predicting the performance and geometry of the optimal design of a steam ejector used in a steam turbine. Many scholars have analysed the steam ejector using the ideal gas model, which lacks accuracy in terms of calculating the flow field of the ejector. This study is reported in a series of two papers. The first part covers the validation of CFX 11.0 results using different equations of state (EOS) on the converging–diverging nozzle flow field carried out with the experimental value. The IAPWS IF97 real gas model works well with the experimental value. The flow field of the ejector was analysed using different EOS after grid-dependent learning. The results show that the performance of the ejector was underestimated under the ideal gas model; the entrainment ratio was 20–40 per cent lower than when using the real gas model. The effect of the optimal geometrical design and operating conditions will be discussed in Part 2.},
 author = {H T Zheng and L Cai and Y J Li and Z M Li},
 doi = {10.1177/0954406211415777},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954406211415777},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
 number = {3},
 pages = {709–714},
 title = {Computational fluid dynamics simulation of the supersonic steam ejector. Part 1: Comparative study of different equations of state},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954406211415777},
 volume = {226},
 year = {2012t}
}

@article{doi:10.1177/0954406216631780,
 abstract = {In this study, three-dimensional computational fluid dynamics simulation was adopted to evaluate the valve-induced water hammer phenomena in a typical tank-pipeline-valve-tank system. Meanwhile, one-dimensional analysis based on method of characteristics was also used for comparison and reference. As for the computational fluid dynamics model, the water hammer event was successfully simulated by using the sliding mesh technology and considering water compressibility. The key factors affecting simulation results were investigated in detail. It is found that the size of time step has an obvious effect on the attenuation of the wave and there exists a best time step. The obtained simulation results have a good agreement with the experimental data, which shows an unquestionable advantage over the method of characteristics calculation in predicting valve-induced water hammer. In addition, the computational fluid dynamics simulation can also provide a visualization of the pressure and flow evolutions during the transient process.},
 author = {Shuai Yang and Dazhuan Wu and Zhounian Lai and Tao Du},
 doi = {10.1177/0954406216631780},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954406216631780},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
 number = {12},
 pages = {2263–2274},
 title = {Three-dimensional computational fluid dynamics simulation of valve-induced water hammer},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954406216631780},
 volume = {231},
 year = {2017s}
}

@article{doi:10.1177/0954406216666874,
 abstract = {In this paper, two alternative numerical approaches for the simulation of Gerotor units are compared: a fast lumped parameter approach for the fluid dynamics through the unit that permits the co-simulation of the radial micro-motion of the rotors, and a computational fluid dynamics approach that puts emphasis on the description of the detailed features of the flow through the unit. Each approach provides unique insights on the unit operation, although with different assumptions and level of result details. For an objective comparison of these two state-of-art models, the authors compared their results with experiments. A commercial pump taken as reference, and tests focused on steady-state volumetric performance as well as the transient features of the outlet port pressure oscillations. The results presented in the paper permit to gain a high level of understanding of the operation of the unit and of the critical aspects that a designer should consider while analyzing such design of positive displacement machines. While comparing the two simulation approaches, the paper highlights the limits and the strengths of each one of the two approaches. In particular, it is shown how both models can match the experimental results considering proper assumptions, in terms of technological clearances and rotors’ micro-motions. The paper constitutes a unique contribution in the field of numerical simulation of Gerotor units and represents a useful reference to the designers looking for suitable methods for simulating existing or novel design solutions.},
 author = {Matteo Pellegri and Andrea Vacca and Emma Frosina and Dario Buono and Adolfo Senatore},
 doi = {10.1177/0954406216666874},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954406216666874},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
 number = {23},
 pages = {4413–4430},
 title = {Numerical analysis and experimental validation of Gerotor pumps: A comparison between a lumped parameter and a computational fluid dynamics-based approach},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954406216666874},
 volume = {231},
 year = {2017l}
}

@article{doi:10.1177/0954406216670684,
 abstract = {Increasing demands for high-performance screw pumps in oil and gas as well as other applications require deep understanding of the fluid flow field inside the machine. Important effects on the performance such as dynamic losses, influence of the leakage gaps and presence and extent of cavitation are difficult to observe by experiments. However, it is possible to study such effects using well-validated computational fluid dynamics models. The novel-structured numerical mesh consisting of a single-computational domain for moving screw pump rotors was developed to allow three-dimensional computational fluid dynamics simulation of such machine possible. Based on finite volume method, the instantaneous mass flow rates, rotor torque, local pressure field, velocity field and other performance indicators including the indicated power were predicted. A calculation model for the bearing friction losses was introduced to account for mechanical losses. The geometry of the inlet and outlet passages and piping system are taken into consideration to evaluate their influences on the pressure distribution and shaft power. The paper also shows the influence of rotor clearances on the pump performance. The computational fluid dynamics model was validated by comparing the numerical results with the measured performance obtained in the experimental test rig through the comprehensive experiment performed for a set of discharge pressures and rotational speeds. Validation includes comparison of mass flow rates, shaft power and efficiency under variety of speeds and discharge pressure. It has been found that the predicted results match well with the measurements. The results also showed that the radial clearances have larger influence on the mass flow rate than the interlobe clearance. The correct design of the flow passages within the screw pump plays significant role in minimizing required power consumption. The analysis presented in this paper contributes to better understanding of the working process inside the screw pump and offers a good reference to improve design and optimize such machines in terms of clearance selection, shape of the ports, piping system, etc. In future, this model will be used for analysis of cavitating flows and determining performance of other multiphase screw pumps.},
 author = {Di Yan and Ahmed Kovacevic and Qian Tang and Sham Rane and Wenhua Zhang},
 doi = {10.1177/0954406216670684},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954406216670684},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
 number = {24},
 pages = {4617–4634},
 title = {Numerical modelling of twin-screw pumps based on computational fluid dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954406216670684},
 volume = {231},
 year = {2017r}
}

@article{doi:10.1177/0954406217713520,
 abstract = {Due to the low electricity prices in central Europe, cost optimisations related to all parts of a new hydropower plant have become increasingly important. In case of a run-of-river hydropower plant using a vertical axis Kaplan turbine, one of the cost drivers are the excavation works. Thus, a decisive factor for the reduction of construction costs is the minimisation of the construction depth of the elbow-type draft tube. In course of the design phase of a new hydropower plant in Austria, an analysis of the impact of draft tube modifications on the performance of the Kaplan turbine was carried out by applying computational fluid dynamics. The net head of the turbine with a diameter of D = 3.15 m accounts for Hnet = 9.00 m and the maximum discharge per unit is Qmax = 57.5 m3/s. After it was proven that there is a good agreement of the numerically calculated and experimentally measured turbine efficiency for the original turbine configuration, various draft tube designs were tested in order to find out their impact on the turbine efficiency and to analyse the sources of draft tube losses in detail. Finally, it was possible to find a new draft tube design representing a compromise of reduced construction costs and acceptable turbine efficiency.},
 author = {Juergen Schiffer and Helmut Benigni and Helmut Jaberg},
 doi = {10.1177/0954406217713520},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954406217713520},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
 number = {11},
 pages = {1937–1952},
 title = {An analysis of the impact of draft tube modifications on the performance of a Kaplan turbine by means of computational fluid dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954406217713520},
 volume = {232},
 year = {2018s}
}

@article{doi:10.1177/0954406217721257,
 abstract = {A novel pilot-control angle globe valve is proposed, and it has an obvious advantage of energy conservation during its opening and closing process. In pilot-control angle globe valve, the opening and closing forces are related to the orifice located inside the valve core. In this paper, the effects of orifice diameter are thoroughly studied under different working conditions such as valve core displacements and inlet velocities. To begin with, the numerical model is validated by comparing similar angle valves, and then the flow and loss coefficients under different orifice diameters are discussed. It is found that the effects of orifice diameter on force acting on valve core depend on valve core displacement and inlet velocity. Thus different valve core displacements and inlet velocities combined with different orifice diameters are further studied. It is also found that when the orifice diameter is larger than 12 mm, pilot-control angle globe valve cannot be used under small inlet velocity or large valve core displacement. In addition, formulas to calculate forces on valve core are proposed for further orifice design. This work can be referred in process industries especially in a piping system with orifice plates or globe valves.},
 author = {Zhi-Jiang Jin and Zhi-Xin Gao and Ming Zhang and Bu-Zhan Liu and Jin-Yuan Qian},
 doi = {10.1177/0954406217721257},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954406217721257},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
 number = {13},
 pages = {2419–2429},
 title = {Computational fluid dynamics analysis on orifice structure inside valve core of pilot-control angle globe valve},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954406217721257},
 volume = {232},
 year = {2018l}
}

@article{doi:10.1177/0954406217737104,
 abstract = {Symbolic computational dynamic solvers are currently under development in order to provide new and powerful tools for modelling nonlinear dynamical systems. Such solvers consist of two parts; the core solver, which comprises an approximate analytical method based on perturbation, averaging, or harmonic balance, and a specialised term-tracker. A term-tracking approach has been introduced to provide a powerful new feature into computational approximate analytical solutions by highlighting the many mathematical connections that exist, but which are invariably lost through processing, between the physical model of the system, the solution procedure itself, and the final result which is usually expressed in equation form. This is achieved by a highly robust process of term-tracking, recording, and identification of all the symbolic mathematical information within the problem. In this paper, the novel source and evolution encoding method is introduced for the first time and an implementation in Mathematica is described through the development of a specialised algorithm.},
 author = {Niloufar Motazedi and Matthew P Cartmell and Jem A Rongong},
 doi = {10.1177/0954406217737104},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954406217737104},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
 number = {19},
 pages = {3439–3452},
 title = {Extending the functionality of a symbolic computational dynamic solver by using a novel term-tracking method},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954406217737104},
 volume = {232},
 year = {2018n}
}

@article{doi:10.1177/0954406217752745,
 abstract = {This work presents thermal lattice Boltzmann method simulation of magneto-hydrodynamic, buoyancy-driven convection in a partially differentially heated cavity (aspect ratio = 1) subjected to a magnetic field along the vertical direction, i.e. at 90°. Lattice Boltzmann method simulations are performed for three different cooler lengths (Lc = H/4, H/2, H) placed along the middle of one vertical wall for a wide range of Rayleigh and Hartmann numbers (103 ≤ Ra ≤ 105; Ha = 0, 60, 120) at fixed Prandtl number (Pr = 0.71, air). A partial heater is placed at the center of other vertical walls and its size is kept as half of the characteristic length (H/2). The physical insights of the systems are delineated by systematic analysis of stream function and temperature contours. Heat transfer characteristics of the cavity are elucidated by using averaged values of the Nusselt number. It is noted that average Nusselt number has a proportional dependence with cooler length and Rayleigh number, while it varied inversely with Hartmann number. Further, the functional dependence of average Nusselt number with cooler size, Rayleigh number, and Hartmann number is established for possible use in engineering design purpose.},
 author = {Krunal M Gangawane and Ram P Bharti},
 doi = {10.1177/0954406217752745},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954406217752745},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
 number = {3},
 pages = {515–528},
 title = {Computational analysis of magneto-hydrodynamic natural convection in partially differentially heated cavity: Effect of cooler size},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954406217752745},
 volume = {232},
 year = {2018g}
}

@article{doi:10.1177/0954406218769919,
 abstract = {This paper examines how new and creative relationships in datasets, not easily revealed by conventional information retrieval methods and technologies, can be identified using a mix of established and new methods. The authors present how the integration of computerised morphological analysis with new computational models, incorporating web crawler, data processing networking and data mining algorithms, can help facilitate the identification of new ideas. Boden’s concept of ‘Combinational Creativity’ indicates a structured process, which generates unfamiliar combinations of familiar concepts and constructs allowing creative styles of thought. This structured approach has been constrained by the resultant combinatorial explosion and the dearth of easily accessible computer software and supporting methodologies, to help identify viable new solutions. Feature-enhanced computerised morphological analysis provides a new structural support tool for creativity and innovation. Morphological analysis systematically structures and examines all the possible relationships in a multidimensional, highly complex, usually non-quantifiable problem space. Computerisation of the process now permits large number of configurations (millions) in the problem space to be majorly reduced (typically > 95%), identifying only internally consistent solutions. These solutions are likely to embrace configurations containing something, which has not previously been considered, thus increasing the probability of some form of technological or design breakthrough and hence truly creative.},
 author = {Bruce Garvey and Liuqing Chen and Feng Shi and Ji Han and Peter RN Childs},
 doi = {10.1177/0954406218769919},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954406218769919},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
 number = {2},
 pages = {425–431},
 title = {New directions in computational, combinational and structural creativity},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954406218769919},
 volume = {233},
 year = {2019e}
}

@article{doi:10.1177/0954406219865920,
 abstract = {In recent years, the computational fluid dynamics method has developed into very useful tools for investigating the oil flow and the no-load losses in geared transmissions. It has neither restriction on the housing shape nor limitations on the lubrication configuration. For this work, a computational fluid dynamics method model based on the finite volume method of a single-stage, injection-lubricated gearbox was built. The influence of different parameters including the injection volume, oil viscosity, and gear speed on the oil supply and distribution were investigated. The results also include a comparison of the simulated no-load losses with empirical no-load loss equations. This work provides first results on computational fluid dynamics method investigations of injection-lubricated geared transmissions and a starting point for comprehensive validation and more complex modeling.},
 author = {H Liu and F Link and T Lohner and K Stahl},
 doi = {10.1177/0954406219865920},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954406219865920},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
 number = {21–22},
 pages = {7412–7422},
 title = {Computational fluid dynamics simulation of geared transmissions with injection lubrication},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954406219865920},
 volume = {233},
 year = {2019m}
}

@article{doi:10.1177/0954406220954485,
 abstract = {This work presents a higher order finite element (FE) model to gauge the static behaviour of hygrothermo-magneto-electro-elastic (HTMEE) plates with different electro-magnetic circuits. The FE model is composed of eight-noded isoparametric elements. The displacement equations are presumed to obey the higher order shear deformation theory (HSDT) and the governing equations of motion are obtained with the support of the virtual energy principle. The correctness of the FE model is verified against previously published results. The influence of electro-magnetic boundary conditions associated with hygrothermal loads, stacking sequences, empirical constants and different loading profiles have been studied in detail. The results of this article are the first of its kind in literature. Hence, it is believed to set benchmark for future investigations on multifunctional structures.},
 author = {M Vinyas and D Harursampath},
 doi = {10.1177/0954406220954485},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954406220954485},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
 number = {15},
 pages = {2832–2850},
 title = {Computational evaluation of electro-magnetic circuits’ effect on the coupled response of multifunctional magneto-electro-elastic composites plates exposed to hygrothermal fields},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954406220954485},
 volume = {235},
 year = {2021t}
}

@article{doi:10.1177/0954406220982020,
 abstract = {Hysteretic damping of a material or structure loaded within its elastic region is the dissipation of mechanical energy at a rate independent of the frequency of vibration while at the same time directly proportional to the square of the displacement. Generally, reproducing this frequency-independent damping can be computationally complex and requires prior knowledge of the system’s natural frequencies or the full time history of the system’s response. In this paper, a new model and numerical procedure are proposed whereby hysteretic material damping is achieved in the time domain. The proposed procedure is developed based on modifying the viscous model through a correction factor calculated exclusively using the local response. The superiority of the proposed approach lies in its ability to capture material hysteresis without any knowledge of the eigen- or modal frequencies of the system and without knowledge of the past time history of the system’s response or the characteristics of any excitation forces. A numerical procedure is also presented for implementing the proposed model in vibration analysis. The simplicity of the approach enables its generalisation to continuous systems and to systems of multi-degrees of freedom as demonstrated herein. The proposed model is presented as a correction to the viscous damping model which makes it attractive to implement into commercial finite element package using user-defined element subroutines as demonstrated in this study.},
 author = {MMS Dwaikat and C Spitas and V Spitas},
 doi = {10.1177/0954406220982020},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954406220982020},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
 number = {20},
 pages = {4625–4636},
 title = {A non-linear model for elastic hysteresis in the time domain: Computational procedure},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954406220982020},
 volume = {235},
 year = {2021g}
}

@article{doi:10.1177/09544062221115346,
 abstract = {A fast computational method of the aerodynamic characteristics of fan and booster with inlet distortion is developed in this paper. This method is based on a time-marching throughflow model, and the governing equations are circumferentially averaged Navier-Stokes equations. A model of distributed inviscid blade force and viscid blade force is adopted to reproduce the flow deflection and loss. The deviation angle and loss parameters are interpolated from a database which is extracted from 3-D simulation results of compressor with uniform inlet. After a validation case is performed, the aerodynamical performances of a multistage fan and booster with inlet radial and circumferential distortion are computed. The results show the predictive ability of this method, and the acceptable computational time cost indicates the future application potential of this method during the design stage of a new turbomachinery.},
 author = {Qitian Tao and Hailiang Jin and Xiaohua Liu and Zhongyu Zhu},
 doi = {10.1177/09544062221115346},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09544062221115346},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
 number = {1},
 pages = {67–82},
 title = {A fast computational method of the aerodynamic performance of fan and booster with inlet distortion},
 url = {https://doi-org.crai.referencistas.com/10.1177/09544062221115346},
 volume = {237},
 year = {2023q}
}

@article{doi:10.1177/09544062241277310,
 abstract = {In recent years, impact-resistant structures are highly sought after in various fields such automotive and aerospace applications as they proved notable performances, garnering significant success. The aim of this study is to assess the behavior of the 3D-printable honeycombs subjected to low velocity impact, for providing insights into absorbed energy for different core designs: hexagonal, auxetic, and rectangular, considering an equal number of cells across all designs. This work reports the computational and experimental studies conducted for sandwich structures under different impact loading. The experimental impact tests are carried out using a drop weight impact-testing machine. The examined specimen comprises two face-sheets and architected cell core fabricated through the Fused Filament Fabrication (FFF) process made of polylactic acid (PLA). Variations in the geometric design of the cells result in the formation of cores with auxetic and non-auxetic topologies. Uniaxial tensile tests are performed to identify the mechanical properties of the involved biopolymer. The second attempt consists on comparing three architectural core structures under impact test using experimental and computational methods. Our findings highlight the specific influence of core topology on energy absorption in 3D-printed sandwich structures. Results indicate that while all three configurations (hexagonal, re-entrant, and rectangular) demonstrate comparable energy absorption values, the specific mechanisms and efficiencies vary, with re-entrant cores exhibiting distinct behaviors under impact.},
 author = {Marwa Allouch and Hana Mellouli and Hanen Mallek and Mondher Wali and Fakhreddine Dammak},
 doi = {10.1177/09544062241277310},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09544062241277310},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
 number = {0},
 pages = {09544062241277310},
 title = {Behavior of sandwich structures with 3D-printed auxetic and non-auxetic cores under low velocity impact: Experimental and computational analysis},
 url = {https://doi-org.crai.referencistas.com/10.1177/09544062241277310},
 volume = {0},
 year = {2024a}
}

@article{doi:10.1177/0954407011400153,
 abstract = {Computational fluid dynamic (CFD) simulations and experimental steady flow tests (flow discharge, swirl, and tumble) were carried out to study the in-cylinder flow in a commercial four-valve spark ignition engine. The present investigation was aimed at analysing and controlling the generation of macro-vortex structures (swirl and tumble) during the inlet process. A comparative study of the most commonly employed tumble benches along with in-house design was performed, the last showing some advantages with respect to the others. The outcomes from the simulations were in agreement with experimental results. Mainly, the tumble generation rate was in general proportional to the valve lift. However, tumble was reduced drastically at medium valve lift due to a change in the vortex pattern. A stagnation zone was observed between inlet valves. CFD calculations successfully captured this tumble-fall effect, which was related to characteristic changes in the vortex pattern downstream of the inlet valves at medium valve lift. This affects tumble production without affecting the mass flowrate efficiency. Finally, at high valve lifts the tumble production and the vortex pattern were recovered. The capability of the cylinder head to induce swirl, tumble, or combined swirl–tumble by modifying the valve timing or by introducing adjustable flow deflectors was evaluated using CFD. Several valve timing strategies were analysed: some of them produced significant swirl, but introduced high mass flowrate losses. On the other hand, adjustable flow deflectors were shown to be an interesting alternative to induce swirl–tumble at low load and to improve tumble at high load.},
 author = {D Ramajo and A Zanotti and N Nigro},
 doi = {10.1177/0954407011400153},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954407011400153},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering},
 number = {6},
 pages = {813–828},
 title = {In-cylinder flow control in a four-valve spark ignition engine: numerical and experimental steady rig tests},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954407011400153},
 volume = {225},
 year = {2011p}
}

@article{doi:10.1177/0954407016659199,
 abstract = {Fully three-dimensional computational fluid dynamic simulations with detailed combustion chemistry of a turbulent jet ignition system installed in a rapid compression machine are presented. The turbulent jet ignition system is a prechamber-initiated combustion system intended to allow lean-burn combustion in spark ignition internal-combustion engines. In the presented configuration, the turbulent jet ignition prechamber has a volume that is 2% of the volume of the main combustion chamber in the rapid compression machine and is separated from the main chamber by a nozzle containing a single orifice. Four simulations with orifice diameters of 1.0 mm, 1.5 mm, 2.0 mm, and 3.0 mm respectively are presented in order to demonstrate the effect of the orifice diameter on the combustion behavior of the turbulent jet ignition process. Data generated by the simulations is shown including combustion chamber pressures, temperature fields, jet velocities and mass fraction burn durations. From the combustion pressure trace, the jet velocity, and other field data, five distinct phases of the turbulent jet ignition process are identified. These phases are called the compression phase, the prechamber combustion initiation phase, the cold jet phase, the hot jet phase, and the flow reversal phase. The four simulations show that the orifice diameter of 1.5 mm provides the fastest ignition and the fastest overall combustion as reflected in the 0–10% and 10–90% mass fraction burn duration data generated. Meanwhile, the simulation for the orifice diameter of 1.0 mm produces the highest jet velocity and has the shortest delay between the spark and the exit of a jet of hot gases into the main chamber but produces a slower burn duration than the simulation for the larger orifice diameter of 1.5 mm. The simulations for orifice diameters of 2.0 mm and 3.0 mm demonstrate that the combustion speed is reduced as the orifice diameter increases above 1.5 mm. Finally, a discussion is given which examines the implications that the results generated have in regard to implementation of the turbulent jet ignition system in an internal-combustion engine.},
 author = {Bryce C Thelen and Elisa Toulson},
 doi = {10.1177/0954407016659199},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954407016659199},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering},
 number = {4},
 pages = {536–554},
 title = {A computational study on the effect of the orifice size on the performance of a turbulent jet ignition system},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954407016659199},
 volume = {231},
 year = {2017r}
}

@article{doi:10.1177/0954407016685496,
 abstract = {This study is an evaluation of the computational methods in reproducing experimental data for a generic sports utility vehicle (SUV) geometry and an assessment on the influence of fixed and rotating wheels for this geometry. Initially, comparisons are made in the wake structure and base pressures between several CFD codes and experimental data. It was shown that steady-state RANS methods are unsuitable for this geometry due to a large scale unsteadiness in the wake caused by separation at the sharp trailing edge and rear wheel wake interactions. unsteady RANS (URANS) offered no improvements in wake prediction despite a significant increase in computational cost. The detached-eddy simulation (DES) and Lattice–Boltzmann methods showed the best agreement with the experimental results in both the wake structure and base pressure, with LBM running in approximately a fifth of the time for DES. The study then continues by analysing the influence of rotating wheels and a moving ground plane over a fixed wheel and ground plane arrangement. The introduction of wheel rotation and a moving ground was shown to increase the base pressure and reduce the drag acting on the vehicle when compared to the fixed case. However, when compared to the experimental standoff case, variations in drag and lift coefficients were minimal but misleading, as significant variations to the surface pressures were present.},
 author = {David Forbes and Gary Page and Martin Passmore and Adrian Gaylard},
 doi = {10.1177/0954407016685496},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954407016685496},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering},
 number = {9},
 pages = {1222–1238},
 title = {A study of computational methods for wake structure and base pressure prediction of a generic SUV model with fixed and rotating wheels},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954407016685496},
 volume = {231},
 year = {2017i}
}

@article{doi:10.1177/0954407020915104,
 abstract = {The computational aero-acoustic study of an isolated passenger car tire is carried out to understand the effect of dimensions of longitudinal tire grooves and operational parameters (velocity and temperature) on tire noise. The computational fluid dynamics and acoustic models are used to obtain aero-acoustic tire noise at near-field and far-field receivers around the tire and artificial neural networks-based regression are used to study the highly non-linear and interactive causal relationships in the system. Unsteady Reynolds-Averaged Navier-Stokes based realizable k-epsilon model is used to solve the flow field in the computational domain. The Ffowcs Williams and Hawkings model is used to obtain aero-acoustic tire noise at far-field positions. Spectral analysis is used to convert the output time domain to frequency domain and to obtain A-weighted sound pressure level. Artificial neural network–based response surface regression is conducted to understand casual relationships between A-weighted sound pressure level and control variables (Groove depth, Groove width, Temperature and velocity). Maximum A-weighted sound pressure level is observed in the wake region of the tire model. The interaction study indicates that ∼10% reduction in the aero-acoustic emissions is possible by selecting appropriate combinations of groove width and groove depth. The interaction of velocity with width is found to be most significant with respect to A-weighted sound pressure level at all receivers surrounding the tire. The interaction of operational parameters, that is, velocity and temperature are found to be significant with respect to A-weighted sound pressure level at wake and front receivers. Therefore, the regional speed limits and seasonal temperatures need to be considered while designing the tire to achieve minimum aero-acoustic emissions.},
 author = {Ghulam Moeen Uddin and Sajawal Gul Niazi and Syed Muhammad Arafat and Muhammad Sajid Kamran and Muhammad Farooq and Nasir Hayat and Sher Afghan Malik and Abe Zeid and Sagar Kamarthi and Sania Saqib et al.},
 doi = {10.1177/0954407020915104},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954407020915104},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering},
 number = {10–11},
 pages = {2561–2577},
 title = {Neural networks assisted computational aero-acoustic analysis of an isolated tire},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954407020915104},
 volume = {234},
 year = {2020w}
}

@article{doi:10.1177/09544070211022099,
 abstract = {In the present work, effects of nozzle hole size and rail pressure under non-evaporating spray condition are demonstrated. Three single hole injectors with the bore size of 0.101, 0.122, and 0.133 mm are experimented with injection pressures of 140, 45, and 38 MPa respectively to achieve similar injection rate profile. Diesel spray experiments implement Diffused Backlight Illumination Technique where diffused background is obtained for the High Speed Video camera imaging. Experimental results are then validated with computational and analytical studies. The CFD simulation requires the injection rate profile and spray cone angle as a primary input; thus, based on the High Speed Video Camera start of injection frame the 5 kHz Butterworth low-pass frequency filter is applied to the injection rate raw data. While, the spray cone angle is predicted using a simple model obtained from the relationship between the injection velocity, fluctuating velocity at the nozzle exit and total pressure loss factor of the injector. The experimental spray tip penetration of all three injectors is almost identical as the similar injection rate profile is adopted. Although, the mixture characteristics are better for 0.101 mm hole diameter since the smaller hole diameter with highest injection pressure depicts larger spray angle and better atomization. The computational study agrees with experiments qualitatively; however, the quantitative and qualitative agreements are seen in the analytical study.},
 author = {Safiullah and Keiya Nishida and Youichi Ogata and Tetsuya Oda and Katsuyuki Ohsawa},
 doi = {10.1177/09544070211022099},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09544070211022099},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering},
 number = {2–3},
 pages = {310–321},
 title = {Effects of nozzle hole size and rail pressure on diesel spray and mixture characteristics under similar injection rate profile – experimental, computational and analytical studies under non-evaporating spray condition},
 url = {https://doi-org.crai.referencistas.com/10.1177/09544070211022099},
 volume = {236},
 year = {2022n}
}

@article{doi:10.1177/09544070221078458,
 abstract = {The friction type tensioner is an energy absorbing component in belt drive system which has hysteretic torque and nonlinear damping. In this paper, a hysteretic model is used for describing the tensioner’s operating torque versus the imposed angle, and considered into the dynamic analysis of a belt drive system. The modeling process for calculating the vibration responses of a belt drive system is presented, and two iterative methods are proposed for estimating the tensioner’s nonlinear damping in a varying excitation frequency. One timing belt drive system is taken as a case study. The system’s vibration responses, such as the oscillation angle of tensioner arm, the transmission error between pulleys and the hub load on pulley are calculated and compared with the measured values, which validate the presented methods. The influence of iterative method on the computational efficiency and iterative accuracy are both discussed. At last a modified method is presented to improve iterative efficiency. The presented technique can improve the computational efficiency with a good computational accuracy.},
 author = {Shangbin Long and Wenlong Wang and Xia Yue and Chunliang Zhang and Wu Qin and Chao Zhou},
 doi = {10.1177/09544070221078458},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09544070221078458},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering},
 number = {1},
 pages = {3–20},
 title = {Computational methods of nonlinear tensioner damping in belt drive systems},
 url = {https://doi-org.crai.referencistas.com/10.1177/09544070221078458},
 volume = {237},
 year = {2023l}
}

@article{doi:10.1177/09544070221093884,
 abstract = {Application of direct injection (DI) technology in small-bore engines, the type used in two- and three-wheelers, could improve their performance significantly. It is recognised that the use of high fuel injection pressure is beneficial in large-bore engines for a good mixture preparation. However, simple systems incorporated with low-pressure DI are desirable in small-engine segment of automobiles. Further, high fuel pressures will result in excessive wall wetting when cylinder dimensions are small. Extensive studies were carried out to investigate the minimum fuel injection pressure required for homogeneous and lean modes of operation in such small bore DI engine. The effect of spark plug protrusion in the combustion chamber was also investigated under the spray-guided configuration. Comprehensive experiments and CFD simulations were performed for estimating the engine efficiency, emissions, mixture preparation characteristics, fuel spray and fuel impingement on combustion chamber walls. Results have demonstrated that engine performance and emissions did not deteriorate when fuel injection pressure was reduced from 150 to 50 bar at full load. However, at very low pressures, like 20–30 bar, THC, CO and smoke emissions increased. Fuel injection pressure did not influence the lean limit, that is, equivalence ratio of about 0.77, but influenced the thermal efficiency at lean conditions. In order to attain high efficiency, under lean conditions, a minimum fuel pressure of 80 bar was required. The spark plug protrusion that resulted in a gap of 0.75 mm with respect to the incoming fuel spray cone has given the best engine performance, while higher protrusions affected the tumble flow and led to the stratification of charge near the spark plug, which resulted in elevated CO and smoke emissions. Hence, this work highlights that relatively lower direct injection pressures are suitable in small bore engines, which will impact the development of cost effective components for such applications.},
 author = {Jubin V Jose and Mayank Mittal and Asvathanarayanan Ramesh},
 doi = {10.1177/09544070221093884},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09544070221093884},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering},
 number = {7},
 pages = {1721–1737},
 title = {Experimental and computational studies on the effects of reduced fuel injection pressure and spark plug protrusion on the performance and emissions of a small-bore gasoline direct-injection engine},
 url = {https://doi-org.crai.referencistas.com/10.1177/09544070221093884},
 volume = {237},
 year = {2023h}
}

@article{doi:10.1177/09544070241264087,
 abstract = {Compared with the traditional in-cylinder direct-injection spark ignition engine, the side-injection and side-spark-ignition characteristics of the two-stroke opposed-piston engine increase the ignition kernel offset and flame propagation distance. Increasing the flame propagation speed can to some extent solve the drawbacks caused by the non-central arrangement of spark plugs. The combustion chamber structure plays a crucial role in gas flow, fuel-charge mixing, and combustion characteristics. Therefore, three pistons were designed and comparatively analyzed in this study. The results show that: The pancake piston is beneficial to maintaining the intake swirl strength due to its simple and smooth spherical arc structure. The swirl strength of the pit and pit-guided piston decreases obviously, and the tumble strength can be maintained well. Compared to pancake and pit-guided pistons, the average TKE for the pit piston increased by approximately 25%, with a more concentrated distribution at the spark timing. The pancake piston exhibits the best scavenging performance, reducing the residual exhaust gas ratio by 2.1% and fresh air loss by 3.3% to the pit piston. A stable ignition core can be formed at the spark timing, but significant differences are observed in the flame propagation process for three pistons. Compared to the pit-guided piston, the pit piston has a 0.3% decrease in the indicated thermal efficiency, but a 13.1% decrease in combustion duration, which reduces knock tendency.},
 author = {Zhaojun Zhu and Ziming Yang and Yikai Li and Chenghan Sun},
 doi = {10.1177/09544070241264087},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09544070241264087},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering},
 number = {0},
 pages = {09544070241264087},
 title = {Computational analysis of piston shape effects on in-cylinder gas flow, fuel-charge mixing, and combustion characteristics in a two-stroke rod-less spark ignition opposed-pistons engine},
 url = {https://doi-org.crai.referencistas.com/10.1177/09544070241264087},
 volume = {0},
 year = {2024t}
}

@article{doi:10.1177/0954408910396785,
 abstract = {This article presents a computational model of the thermal bonding of nonwovens using convective hot air and its experimental validation. A computational fluid dynamics model based on the continuum modelling approach and the theory of porous media is developed to treat the flow behaviour and heat transfer within the thermal bonding system. The model includes several components of a typical industrial machine including the conveyer belt, drum cover, drum, and the nonwoven web. Experimental measurements are used to supply appropriate boundary conditions for the simulations and to provide data for the validation of the numerically computed results. The model is concluded to be an accurate computational tool that could potentially replace the costly experiments and be employed in product development, process optimization, and machine design.},
 author = {M Peksen and M Acar and W Malalasekera},
 doi = {10.1177/0954408910396785},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954408910396785},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part E: Journal of Process Mechanical Engineering},
 number = {3},
 pages = {173–182},
 title = {Computational modelling and experimental validation of the thermal fusion bonding process in porous fibrous media},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954408910396785},
 volume = {225},
 year = {2011j}
}

@article{doi:10.1177/0954408910397586,
 abstract = {Previous publications show that computational fluid dynamics (CFD) can be readily used for the flow prediction and analysis of screw compressors. Several case studies are presented in this article to show the scope and applicability of such methods. These include solid–fluid interaction in screw compressors, prediction of flow generated noise in screw machines, cavitation modelling in gear pumps, and flow in multiphase pumps for oil and gas industry. Numerical grids for all these cases were generated by the authors using an in-house grid generator, while the CFD calculations were performed with a variety of commercially available CFD codes. In order to validate the accuracy of the CFD calculations, an extended test programme was carried out using laser Doppler velocimetry to measure the mean and fluctuating velocity distribution in screw compressor flow domains. The measurement results are then compared with the CFD simulations. The results confirm the viability of the developed techniques. It is shown in this publication that the flexibility of the developed method creates further opportunities for a broader use of CFD for analysis of twin screw machines in a range of new applications.},
 author = {A Kovacevic and N Stosic and E Mujic and I K Smith and D Guerrato},
 doi = {10.1177/0954408910397586},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954408910397586},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part E: Journal of Process Mechanical Engineering},
 number = {2},
 pages = {83–97},
 title = {Extending the role of computational fluid dynamics in screw machines},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954408910397586},
 volume = {225},
 year = {2011n}
}

@article{doi:10.1177/0954408912459161,
 abstract = {In order to improve the correctness and efficiency of fault diagnosis, a novel hybrid intelligence method based on integrating rough set, genetic algorithms, and radial basic function neural network (RGRN) was proposed for motor fault diagnosis in the complicated CNC system in this article. In the proposed RGRN method, combination and condition supplement algorithm was used to deal with the incomplete fault data and the original data were discretized using genetic algorithms to construct a decision table. Rough set theory as a new mathematical tool was used to eliminate the redundant and irrelevant attributes in order to obtain the minimum rule set for reducing the number of input nodes of the radial basic function neural network. Genetic algorithms were directly used to optimize the structure and weights of radial basic function neural network to establish an optimized radial basic function neural network (GRN) model; then, the minimum rule set was inputted into the GRN model in order to obtain the optimized RGRN model. Finally, the completed fault symptom information was inputted into the RGRN model to obtain the fault diagnosis results. The robustness of the RGRN method was tested. Simulating experiments on motor fault diagnosis in the complicated CNC system show the RGRN method not only improves the global optimization performance and quickens the convergence speed, but also obtains the robust solution with a better quality.},
 author = {Wu Deng and Xinhua Yang and Jingjing Liu and Huimin Zhao and Zhengguang Li and Xiaolin Yan},
 doi = {10.1177/0954408912459161},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954408912459161},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part E: Journal of Process Mechanical Engineering},
 number = {3},
 pages = {198–210},
 title = {A novel fault analysis and diagnosis method based on combining computational intelligence methods},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954408912459161},
 volume = {227},
 year = {2013c}
}

@article{doi:10.1177/0954408914557375,
 abstract = {In this paper, steady, laminar, incompressible, and two-dimensional micropolar flow between a porous disk and a nonporous disk is considered. By introducing suitable similarity transformations, the problem is reduced to a set of nonlinear boundary value problems. Optimal homotopy asymptotic method is employed to obtain the series solutions for velocity and microrotation distribution. The accuracy of results is examined by the fourth-order Runge–Kutta numerical method. The results are presented to study the velocity and rotation profiles for different physical parameters such as: Reynolds number, vortex viscosity parameter, spin gradient viscosity, and microinertia density parameter. As a result, the magnitude of the injection velocity has strong influence on the flow velocities and the microrotation.},
 author = {M Vatani and SE Ghasemi and DD Ganji},
 doi = {10.1177/0954408914557375},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954408914557375},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part E: Journal of Process Mechanical Engineering},
 number = {6},
 pages = {413–424},
 title = {Investigation of micropolar fluid flow between a porous disk and a nonporous disk using efficient computational technique},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954408914557375},
 volume = {230},
 year = {2016r}
}

@article{doi:10.1177/0954408916646403,
 abstract = {The overhead multiple outlets ventilation duct system of 18 m long is used to maintain the specified indoor thermal comfort environment for each railway passenger car. Therefore, the flow uniformity of the overhead ventilation duct system is very important for heating, ventilation, and air conditioning performance of a train. In this study, design optimization was conducted to increase the flow uniformity of the overhead ventilation duct system for a train by combining computational fluid dynamics and design of experiment methods. To perform the study, the flow uniformity of the base model was evaluated using numerical analysis whose reliability was verified. Design parameters of the overhead ventilation duct system were selected, and an effectiveness evaluation was performed for each design parameter by using 2k factorial design. Based on the results of the effectiveness evaluation for the design parameters, optimum models having improved flow uniformity were designed using the response surface method. The performances of the optimum models were also evaluated by the same numerical analysis that was applied to the base model. The flow uniformity of the optimum models was improved by controlling the opening ratios of the perforated plates and guide vane shape. In addition, nonuniform flow components locally existing in the base model were suppressed.},
 author = {Joon-Hyung Kim and Joo-Hyun Rho},
 doi = {10.1177/0954408916646403},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954408916646403},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part E: Journal of Process Mechanical Engineering},
 number = {5},
 pages = {914–929},
 title = {Design optimization for overhead ventilation duct system for a train using computational fluid dynamics and design of experiment},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954408916646403},
 volume = {231},
 year = {2017j}
}

@article{doi:10.1177/0954408919889417,
 abstract = {In the present paper, two configurations of structured reactors (with and without) chimney tray placed below the packed bed have been investigated to study their effect on maldistribution factor and pressure drop characteristics. A simulation result based on a three-dimensional computational fluid dynamics was involved using ANSYS Fluent. First, maldistribution factors without chimney tray were calculated and compared to the results taken from the literature. The results were found to be in good agreement with the experimental data of Yuan et al. Second, the reactor with a chimney tray was modeled in Fluent, and steady-state simulations were performed. The uniformity due to the turbulence of the fluid was carried out using different turbulence models, and the velocity profiles along the axial direction inside the reactor were obtained. As a result, the comparison shows that the presence of a chimney tray yields lower maldistribution factor enhancement by 23% compared to the conventional structured reactor under the same operating condition. The effect of the plate orientation is also determined, and it is found that the maximum pressure drop is achieved through the rows with an orientation angle of α = 45° up to 2.3% higher than of α = 0° and α = 90°.},
 author = {Hajer Troudi and Moncef Ghiss and Mohamed Ellejmi and Zoubeir Tourki},
 doi = {10.1177/0954408919889417},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954408919889417},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part E: Journal of Process Mechanical Engineering},
 number = {1},
 pages = {83–97},
 title = {Performance comparison of a structured bed reactor with and without a chimney tray on the gas-flow maldistribution: A computational fluid dynamics study},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954408919889417},
 volume = {234},
 year = {2020s}
}

@article{doi:10.1177/0954408920916580,
 abstract = {In this article, a novel tray humidifier column for humidification dehumidification desalination was proposed. The performance of the humidifier column has been investigated with experimental and computational fluid dynamics simulations. The hydrodynamics and heat transfer characteristics of this tray humidifier has been studied. A stainless steel sieve tray with a rectangular cross section with a dimension of 20 × 50 cm was used in the experimental study. In computational fluid dynamics modeling, a transient three-dimensional model has been developed based on the volume of fluid framework by using standard k-epsilon model. The effect of air and seawater flow rate and inlet seawater temperature on the exit air temperature has been investigated. The results show that the humidifier effectiveness of the tray humidifier column varies between 0.67 and 0.87 depending on operating conditions. Then, tray column can be used in humidification dehumidification desalination systems with advantages such as compact equipment, low-pressure drop, and handling solids or other sources of fouling.},
 author = {Taleb Zarei and Reza Hamidi Jahromi and Arash Mohammadi Karachi},
 doi = {10.1177/0954408920916580},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954408920916580},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part E: Journal of Process Mechanical Engineering},
 number = {3},
 pages = {275–284},
 title = {Experimental and computational fluid dynamics modeling of a novel tray humidifier column in humidification dehumidification desalination; evaluation of hydrodynamic and heat transfer characteristics},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954408920916580},
 volume = {234},
 year = {2020s}
}

@article{doi:10.1177/0954408920943809,
 abstract = {Cuttings transport from wellbore annulus to the surface via drilling fluids is one of the most important problems in gas and oil industries. In the present paper, the effects of viscoelastic property of drilling fluids on flow through wellbore annulus are studied numerically by use of computational fluid dynamics simulation in OpenFOAM software. This problem is simulated as the flow between two coaxial annulus cylinders and the inner cylinder is rotating through its axes. Here, the Giesekus model is used as the nonlinear constitutive equation. This model brings the nonlinear viscosity, normal stress differences, extensional viscosity and elastic property. The numerical solution is obtained using the second order finite volume method by considering PISO algorithm for pressure correction. The effect of elasticity, Reynolds number, Taylor number and mobility factor on the velocity and stress fields, pressure drop, and important coefficient of drilling mud flow is studied in detail. The results predicted that increasing elastic property of drilling mud lead to an initial sharp drop in the axial pressure gradient as well as Darcy-Weisbach friction coefficient. Increasing the Reynolds number at constant Taylor number, resulted an enhancing in the axial pressure drop of the fluid but Darcy-Weisbach friction coefficient mainly reduced.},
 author = {Mohammad Amir Hasani and Mahmood Norouzi and Morsal Momeni Larimi and Reza Rooki},
 doi = {10.1177/0954408920943809},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954408920943809},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part E: Journal of Process Mechanical Engineering},
 number = {1},
 pages = {66–79},
 title = {Computational study on drilling mud flow through wellbore annulus by Giesekus viscoelastic model},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954408920943809},
 volume = {235},
 year = {2021a}
}

@article{doi:10.1177/09544089221134449,
 abstract = {The cold box heat exchangers are used in petrochemical and gas refinery industries. Here, an industrial complex cold box equipped with a number of plate-fins is simulated by computational fluid dynamics. The model predicts the outlet vapor fraction, pressure drop, and outlet temperature with average absolute relative deviations of 0.17%, 3.3%, and 12.89% for all streams, respectively. The influence of obstruction in streams B and C on the computational fluid dynamics results are studied. When stream B or C is blocked, the remaining open streams experience an increase in pressure drop, temperature, and vapor fraction, which negatively affects the heat exchanger’s performance over a long time. Finally, the computational fluid dynamics results of the cold box are compared with those of commercial software Aspen-EDR. Even though Aspen-EDR predicts an acceptable mean temperature and vapor fraction (phase change) along channels, it calculates pressure drop incorrectly. So, Aspen-EDR, computationally efficient software, can be used for modeling of mean temperature and phase change of flow in an industrial multi-stream cold box.},
 author = {Seyyed Hossein Hosseini and Abedin Zargoushi and Farhad Tablebi},
 doi = {10.1177/09544089221134449},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09544089221134449},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part E: Journal of Process Mechanical Engineering},
 number = {6},
 pages = {2362–2372},
 title = {Effect of side stream obstruction on the results of an industrial three-stream cold box: Numerical study},
 url = {https://doi-org.crai.referencistas.com/10.1177/09544089221134449},
 volume = {237},
 year = {2023h}
}

@article{doi:10.1177/09544089231209314,
 abstract = {Heat affected zone on the micromachined material, a hindrance in achieving good precision and accuracy, which are adequate requirements for micromachined materials, is a drawback of nanosecond micromachining assisted by laser. The primary objective of this paper is to take into consideration a number of input parameters, such as the pulse width and the temperature of the laser beam, and study the effect that these parameters have on the heat flux generated as well as on the temperature distribution along the square grooves of 200 µm width and depth that are to be micro-machined on an alumina ceramic workpiece. However, processing of alumina ceramic on a micron-scale requires careful consideration to raise the quality standard. This will be accomplished by analyzing the data collected from the micro-machining process. Using the ANSYS® software, it is possible to visualize not only the region where heat is generated but also the way in which the input parameters influence the structural qualities of the micro machined groove surface. The many thermal models that have been constructed have revealed specific changes in the structural qualities that have had a significant impact on the upper width, the lower width, and the depth of the groove when applied to a variety of different parametric settings. Finally, developed analytical model is compared with the experimental models. The highest prediction error was found to be 1.68%, while, for constant 50 W power and 150–250 mm/s of speed, the average prediction error was determined to be 1.18%. As a result, the proposed model closely matches the experimental findings, and the proposed model can be used to estimate channel profile.},
 author = {Subhadip Pradhan and Samir Kumar Panda and Kalyani Panigrahi and Debabrata Dhupal},
 doi = {10.1177/09544089231209314},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09544089231209314},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part E: Journal of Process Mechanical Engineering},
 number = {0},
 pages = {09544089231209314},
 title = {Computational approach for structural and thermal behavior of laser-machined micro-grooves on alumina ceramic using ANSYS},
 url = {https://doi-org.crai.referencistas.com/10.1177/09544089231209314},
 volume = {0},
 year = {2023m}
}

@article{doi:10.1177/09544089231215899,
 abstract = {The parabolic trough solar collector (PTSC) is renowned for its impressive ability to harness solar energy effectively. Nevertheless, it does suffer from a low thermal efficiency , which presents an exciting opportunity for its performance enhancement using novel coolant hybrid nanofluids (NFs) and reconfigured receiver tubes. This study performs computational simulations to analyze the thermal and hydraulic performance of PTSC equipped with a V-ribbed receiver tube for MWCNT—/water hybrid. All turbulent flow simulations are performed for different values of parameters (relative rib height , relative streamwise pitch ratio and relative spanwise pitch ratio of V-ribbed receiver tube with different Reynolds number (10,000–50,000). Various performance indicators (Nusselt number , friction factor , and performance evaluation criterion) had been investigated to assess the collector’s performance. The results indicate that the highest Nu achieved of 760 with . Additionally, the maximum of 0.29 was observed at with and . Furthermore, the peak enhanced of 1.9 was attained with , specifically at of 50,000. The findings indicate that using a V-ribbed receiver tube augments the heat transfer of the solar collector by 134.57% at Re of 50,000 with hybrid NF.},
 author = {Priyanka and Sunil Kumar and Anil Kumar and Naveen Kumar Gupta and Tabish Alam and Gitanjali Raghav and Saboor Shaik and Chanduveetil Ahamed Saleel},
 doi = {10.1177/09544089231215899},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09544089231215899},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part E: Journal of Process Mechanical Engineering},
 number = {0},
 pages = {09544089231215899},
 title = {Computational study of heat transfer and fluid flow in a solar trough collector with a ribbed surface and an MWCNT—Al2O3/water-based hybrid nanofluid},
 url = {https://doi-org.crai.referencistas.com/10.1177/09544089231215899},
 volume = {0},
 year = {2023m}
}

@article{doi:10.1177/09544089241248129,
 abstract = {For industries using multi-phase transportation pipelines, erosion has been identified as one of the main challenges. Thus, extensive experimental and computational studies on slurry erosion wear have been conducted already for smooth pipes, particularly bends and junctions which were more susceptible to erosion wear because of the flow of multi-phase, but findings of erosive wear rate on the unbounded flexible pipe are not yet established. The purpose of the current study is to examine the effects of slurry wear on the unbounded flexible pipe material using CFD analysis when multi-phase fluid is flowing in the UFP at different curvature angles (30°, 50°, 70°, 90°). Flow speed (2 ms−1, 4 ms−1, 6 ms−1, 8 ms−1, 10 ms−1) and concentration (5%, 10%, 15%) were the key variables taken into account, and is discovered that these characteristics had a substantial effect on the flexible pipe. Further, to analyze how particle collision tendency affects the erosion wear rate of UPF when compared to smooth pipe, the streamlines for the UFP and smooth pipe are extracted.},
 author = {Latchupatula Ananya and Vivek Kumar Patel},
 doi = {10.1177/09544089241248129},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09544089241248129},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part E: Journal of Process Mechanical Engineering},
 number = {0},
 pages = {09544089241248129},
 title = {Computational analysis of erosion wear on unbounded flexible pipe of different curvature angles},
 url = {https://doi-org.crai.referencistas.com/10.1177/09544089241248129},
 volume = {0},
 year = {2024a}
}

@article{doi:10.1177/0954409713504395,
 abstract = {In a crosswind scenario, the risk of high-speed trains overturning increases when they run on viaducts since the aerodynamic loads are higher than on the ground. In order to increase safety, vehicles are sheltered by fences that are installed on the viaduct to reduce the loads experienced by the train. Windbreaks can be designed to have different heights, and with or without eaves on the top. In this paper, a parametric study with a total of 12 fence designs was carried out using a two-dimensional model of a train standing on a viaduct. To asses the relative effectiveness of sheltering devices, tests were done in a wind tunnel with a scaled model at a Reynolds number of 1 × 105, and the train’s aerodynamic coefficients were measured. Experimental results were compared with those predicted by Unsteady Reynolds-averaged Navier-Stokes (URANS) simulations of flow, showing that a computational model is able to satisfactorily predict the trend of the aerodynamic coefficients. In a second set of tests, the Reynolds number was increased to 12 × 106 (at a free flow air velocity of 30 m/s) in order to simulate strong wind conditions. The aerodynamic coefficients showed a similar trend for both Reynolds numbers; however, their numerical value changed enough to indicate that simulations at the lower Reynolds number do not provide all required information. Furthermore, the variation of coefficients in the simulations allowed an explanation of how fences modified the flow around the vehicle to be proposed. This made it clear why increasing fence height reduced all the coefficients but adding an eave had an effect mainly on the lift force coefficient. Finally, by analysing the time signals it was possible to clarify the influence of the Reynolds number on the peak-to-peak amplitude, the time period and the Strouhal number.},
 author = {Ignacio Sesma and Gorka S. Larraona and Jordi Vinolas and Alejandro Rivas and Sergio Avila-Sanchez},
 doi = {10.1177/0954409713504395},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954409713504395},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part F: Journal of Rail and Rapid Transit},
 number = {2},
 pages = {186–200},
 title = {A two-dimensional computational parametric analysis of the sheltering effect of fences on a railway vehicle standing on a bridge and experiencing crosswinds},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954409713504395},
 volume = {229},
 year = {2015t}
}

@article{doi:10.1177/0954409715606748,
 abstract = {This paper discusses the possibility of using computational fluid dynamics (CFD) to assess the influence of aerodynamic forces (aerodynamic uplift) on the mean contact force acting between a pantograph and the contact wire. The analysis consists of experimental tests and CFD simulations, performed in both wind tunnel and on-track scenarios. A method for the computation of the aerodynamic uplift and for the assessment of the contact force imbalance between the leading and trailing collectors is proposed. The method is based on the virtual work principle, and exploits both the numerical forces resulting from CFD analysis, and the Jacobian terms obtained from a kinematic analysis of the pantograph. The proposed model takes into account stationary phenomena, and it is fully validated by means of experimental results.},
 author = {Marco Carnevale and Alan Facchinetti and Luca Maggiori and Daniele Rocchi},
 doi = {10.1177/0954409715606748},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954409715606748},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part F: Journal of Rail and Rapid Transit},
 number = {7},
 pages = {1698–1713},
 title = {Computational fluid dynamics as a means of assessing the influence of aerodynamic forces on the mean contact force acting on a pantograph},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954409715606748},
 volume = {230},
 year = {2016d}
}

@article{doi:10.1177/0954409715615374,
 abstract = {A computational procedure is developed in the present paper, allowing the prediction of the ballasted track profile degradation under railway traffic loading. In this procedure, an integration of the short-term and long-term mechanical processes of track deterioration is taken into account, using a track degradation model. This degradation model is incorporated into a finite element code where two modes of calculation are implemented: the “implicit mode” concerns the short-term track deterioration, in which the hypoplastic model is used for the ballast layer and the dynamic response to an instantaneous train axle passage is obtained to serve as input data for the “explicit mode”, which concerns the simulation of long-term track deterioration, using the accumulation model for ballast layer. The whole procedure is illustrated on the prediction of the ballasted track profile degradation of a track section of 100 m. The results show a significant influence of the type of track geometry defects and the vehicle velocity on the evolution of track deterioration and the capability of the proposed procedure in reproducing the track profile degradation.},
 author = {K Nguyen and D I Villalmanzo and J M Goicolea and F Gabaldon},
 doi = {10.1177/0954409715615374},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954409715615374},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part F: Journal of Rail and Rapid Transit},
 number = {8},
 pages = {1812–1827},
 title = {A computational procedure for prediction of ballasted track profile degradation under railway traffic loading},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954409715615374},
 volume = {230},
 year = {2016n}
}

@article{doi:10.1177/0954409720902897,
 abstract = {Locomotive sanders are used to optimize the traction between the train wheels and the railhead by spraying sand into the interface. It has been previously shown that a large fraction of sand sprayed by the sanders does not make it through the wheel–rail nip, leading to sand wastage and thereby increasing the cost and refilling effort. In this study, pneumatic conveying of sand through the wheel–rail nip is numerically modeled through coupled computational fluid dynamics and discrete element method simulations. The gas phase, discrete phase, and coupled two-phase flows are separately validated against the literature, and the parameters affecting the deposition of sand into the nip are analyzed to determine their impact on sander efficiency. The aerodynamics associated with the particle-laden jet play a critical role in optimizing the amount of sand going through the wheel–rail interface, with the particle velocities being directly correlated with the sander efficiency. Particle–geometry interactions (e.g. particle bouncing) are found to have a negligible effect on the deposition. In the absence of crosswinds, it is recommended to employ particles with a smaller Stokes number to enhance the sander efficiency. A larger airflow rate through the nozzle is also recommended. Crosswinds strongly and adversely affect sander efficiency. The effects of crosswinds can be mitigated by reducing the nip–nozzle distance and using coarser particles.},
 author = {Aishwarya Gautam and Sheldon I Green},
 doi = {10.1177/0954409720902897},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954409720902897},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part F: Journal of Rail and Rapid Transit},
 number = {1},
 pages = {12–21},
 title = {Computational fluid dynamics–discrete element method simulation of locomotive sanders},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954409720902897},
 volume = {235},
 year = {2021j}
}

@article{doi:10.1177/09544097231226148,
 abstract = {The paper assesses the effect on the air flow regime underneath a simplified high-speed train of changing the ballast depth and the sleeper shape, with regard to its potential for causing ballast flight or pickup. The study was carried out numerically using the commercial Computational Fluid Dynamics (CFD) software AnSys Fluent. The flow profile beneath the underbody of the train was generated by means of a moving wall above the track. The Delayed Detached Eddy Simulation (DDES) with the SST turbulence model was used to simulate turbulent flow, and the ballast bed roughness was applied parametrically using the wall roughness feature when resolving the boundary layer. CFD simulations were validated for flow over a cube, showing good agreement with experimental results. Up to three different depths to the ballast surface and three different sleeper profiles were investigated. Velocity profiles and aerodynamic forces on cubes placed between or on top of the sleeper blocks were used to assess the propensity of individual ballast grains for movement. For a standard G44 sleeper, increasing the ballast depth and/or the ballast bed roughness was found to reduce aerodynamic loads on an individual ballast grain. A ballast grain on top of the sleeper is more prone to uplift than a grain on the surface of the ballast bed in the crib. A curved upper surface to the sleeper is beneficial in that it prevents ballast from settling on top, the most vulnerable position. However, the reduced flow separation associated with the curved top may increase the likelihood of ballast pickup from the crib. Hence new sleeper shapes intended to reduce the potential for ballast flight should not only prevent ballast from settling on top, but also increase flow separation through the provision of a sharp surface. A prismatic sleeper shape that achieves both is suggested.},
 author = {Lee Pardoe and William Powrie and Zhiwei Hu},
 doi = {10.1177/09544097231226148},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09544097231226148},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part F: Journal of Rail and Rapid Transit},
 number = {6},
 pages = {692–705},
 title = {A computational fluid dynamics study of the influence of sleeper shape and ballast depth on ballast flight during passage of a simplified train},
 url = {https://doi-org.crai.referencistas.com/10.1177/09544097231226148},
 volume = {238},
 year = {2024l}
}

@article{doi:10.1177/0954410011408763,
 abstract = {The flexion effect of a flexible wing during the hovering flight is systematically studied by using computer simulations when Reynolds number is kept at 140. The aerodynamic forces and flow vortex are investigated by solving the two-dimensional time-dependent incompressible Navier–Stokes equations using the finite volume method. The wing is modelled as a thin plate which has a rigid leading portion and a flexible aft part. The simulations are performed for a variation of the location of rotational centre and the amplitude of the flexion, and the corresponding changes of the aerodynamic forces and vortex shedding mechanism in the wake structure are investigated. Form the quantitative comparisons, the present simulations reveal that a moderate flexion during the hovering fly can provide a better performance for a flapping wing, while an excessive flexion will lead the aerodynamic performance become worse. Though the flexion does not change the force trace of the hovering wing, the magnitude of the instantaneous peak forces and mean periodic forces are definitely changed by the chordwise deformation. Compared with a rigid hovering wing, the dynamic flexion during hovering alters the intensity of the leading edge vorticity and the transfer direction of the wake. Here, the present results indicate that the aerodynamic forces can be altered by adding some deformation characteristics to the trailing edge of the wing. Furthermore, the appropriate flexion can be used to alter the aerodynamic performance of a wing and will be helpful for design and control of the flexible microflying vehicles.},
 author = {X W Zhang and C Y Zhou},
 doi = {10.1177/0954410011408763},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954410011408763},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {1},
 pages = {3–14},
 title = {Computational study on the hovering mechanisms of a chordwise flexible wing},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954410011408763},
 volume = {226},
 year = {2012t}
}

@article{doi:10.1177/0954410011414320,
 abstract = {Angular injection of hydrogen fuel in a scramjet combustor is explored numerically. Three-dimensional Navier–Stokes equations with turbulence and combustion models are solved using commercial computational fluid dynamics software. Both infinitely fast kinetics and single-step finite rate H2–air kinetics are used to find out the effect of chemical kinetics in the thermochemical behaviour of the flow field. Grid independence of the results is demonstrated and gridconvergence index-based error estimate provided. k-ω turbulence model performs better, in comparison to k–ϵ and shear stress transport models, in predicting the surface pressure. Single-step finite rate chemistry (SSC) performs extremely well in predicting the flow features in the combustor. Computed temperature and species mole fraction and wall pressure distributions with SSC match better with the experimental results compared to fast chemistry calculation and detailed chemistry calculation of other workers. It has been observed that simple chemistry can describe H2–air reaction in scramjet combustor reasonably well.},
 author = {M S R Chandra Murty and D Chakraborty},
 doi = {10.1177/0954410011414320},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954410011414320},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {7},
 pages = {861–872},
 title = {Numerical simulation of angular injection of hydrogen fuel in scramjet combustor},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954410011414320},
 volume = {226},
 year = {2012d}
}

@article{doi:10.1177/0954410011417541,
 abstract = {The application of a previously developed computational method to the prediction of high-lift performance for multi-element aerofoil sections operating at transonic flow conditions is described. The flows are computed by solving the Reynolds-averaged Navier–Stokes equations, using a full differential Reynolds-stress turbulence model to evaluate the various Reynolds-stress components appearing in the governing mean-flow equations. Algebraic wall functions are used to bridge the molecular viscosity-dominated region immediately adjacent to the aerofoil surfaces. An unstructured grid-based computational fluid dynamics (CFD) methodology is used to deal with the geometric complexity of the multi-element aerofoil configurations. Initial results are presented for the viscous, transonic flow development around the SKF 1.1 supercritical aerofoil section, equipped with either a trailing-edge flap or a leading-edge slat. Predicted surface pressure distributions generally compare well with experimental data for the two high-lift aerofoil geometries considered, at a free-stream Mach number of 0.6 and over a range of incidence angles. There are some discrepancies in the regions immediately downstream of shock wave/boundary layer interactions, possibly resulting from the use of wall-function boundary conditions in the computations. Predicted Mach number contours indicate the complexity of the transonic flow fields for high-lift configurations, with the slat wake passing through an extensive supersonic-flow region, terminated by a normal shock wave, on the main aerofoil upper surface, for example.},
 author = {LJ Johnston},
 doi = {10.1177/0954410011417541},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954410011417541},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {8},
 pages = {912–929},
 title = {Computational fluid dynamics analysis of multi-element, high-lift aerofoil sections at transonic manoeuvre conditions},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954410011417541},
 volume = {226},
 year = {2012d}
}

@article{doi:10.1177/0954410011418223,
 doi = {10.1177/0954410011418223},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954410011418223},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {10},
 pages = {1063–1064},
 title = {Special Issue On Evolutionary Computation In Aerospace Sciences},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954410011418223},
 volume = {225},
 year = {2011t}
}

@article{doi:10.1177/0954410011427107,
 abstract = {A trajectory computation tool designed to compute global trajectories (from take off to landing) of commercial transport aircraft is presented. The global trajectory is defined by a general flight intent, considering flight segments usually flown by transport aircraft, including standard airline procedures, and air traffic control regulations. The computation is based on a trajectory computation solver designed for a general aircraft performance model (general drag polar and general engine model); the formulation takes into account wind effects and temperature corrections for a non-standard atmosphere. In the computation of the global trajectory, the top-of-descent point is determined iteratively, using the actual aircraft weight computed along the flight. Global properties such as total fuel consumption and flight time are computed; the influence of the actual take-off weight is analysed, as well as the effects of wind and non-standard temperatures. The computation tool provides a quantitative evaluation of these effects. Results are presented for a medium-range global trajectory and a model of a typical twin-engine, wide-body and, transport aircraft.},
 author = {Damián Rivas and Alfonso Valenzuela and José L de Augusto},
 doi = {10.1177/0954410011427107},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954410011427107},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {1},
 pages = {142–158},
 title = {Computation of global trajectories of commercial transport aircraft},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954410011427107},
 volume = {227},
 year = {2013l}
}

@article{doi:10.1177/0954410012453390,
 abstract = {Continuous requirements for more efficient aircrafts lead to the design and analysis of novel propulsion configurations, with an example being the boundary layer ingestion. The complexity and integration challenges in such aircraft synergistic propulsion system characterize the research in this field, driven by the potential benefits. The aim of this article is to investigate the effects of boundary layer ingestion on the aerodynamics of a transonic wing, together with the quality of the flow ingested by the propulsion system. A two-dimensional computational model of a transonic airfoil with boundary layer ingesting propulsion system is developed in order to assess boundary layer ingestion for a commercial air transport at cruise conditions and highlight the complex integration issues arising from such configuration. A parametric analysis of the effects of flight conditions, nacelle geometry and engine operating point, on lift, pressure recovery, distortion, total pressure and velocity distribution at the intake, comes to enhance understanding of the performance of this configuration. The pressure distribution around the airfoil and the boundary layer growth are both substantially affected by the engine operating condition, which is represented by the mass flow ratio, with a direct impact on pressure recovery and lift. Mach number and angle of attack influences on lift and drag ingested are also investigated. Intake size and position on the airfoil appear to have significant effects on lift and losses ingested. In general, the results of this study include several aspects related to wing aerodynamics and ingested flow quality, which may facilitate design and integration of the boundary layer ingestion propulsion system for future commercial aircraft.},
 author = {Vladislav Mantič-Lugo and Georgios Doulgeris and Riti Singh},
 doi = {10.1177/0954410012453390},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954410012453390},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {8},
 pages = {1215–1232},
 title = {Computational analysis of the effects of a boundary layer ingesting propulsion system in transonic flow},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954410012453390},
 volume = {227},
 year = {2013m}
}

@article{doi:10.1177/0954410012464602,
 abstract = {A computational fluid dynamics method has been applied to simulate the exhaust gas flow and the additional thrust during a missile launching from concentric canister launcher. The unsteady, axisymmetric Reynolds-averaged Navier–Stokes equations with renormalization group turbulence model are numerically solved here. The dynamic mesh method is utilized to simulate the movement of the missile. Computational fluid dynamics results show that the additional thrust is an important thrust and fluctuates with the movement of the missile for launching from concentric canister launcher. The mechanism for producing and influencing the additional thrust is typically relevant to the choking states of exhaust gases at the inlet and outlet of the annular tube of concentric canister launcher, responding to the jet impinging on the bottom of the launcher, the approximate wall jet, and the friction effect in the tube.},
 author = {Debin Fu and Yong Yu},
 doi = {10.1177/0954410012464602},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954410012464602},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {12},
 pages = {1977–1987},
 title = {Simulation of gas flow and additional thrust with missile launching from concentric canister launcher},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954410012464602},
 volume = {227},
 year = {2013g}
}

@article{doi:10.1177/0954410013504689,
 abstract = {The efficient global optimization method is a global optimization technique that can select the next sample point automatically by infill sampling criteria and search for the global minimum with less samples than the conventional global optimization needs. Infill sampling criteria function consists of the predictor and mean square error provided from the kriging model, which is a stochastic metamodel. Also, the constrained efficient global optimization method can minimize the objective function when dealing with constraints under the efficient global optimization concept. In this study, the constrained efficient global optimization method was applied to the RAE2822 airfoil shape design formulated with constraint. But the noisy computational fluid dynamics data caused the kriging model to fail to depict the true function. The distorted kriging model would make the efficient global optimization deviate from the correct search. This distortion of kriging model can be handled with the interpolation (p = free) kriging model. With the interpolation (p = free) kriging model, however, the search for efficient global optimization solution was stalled in a narrow feasible region, so there were less chances to update the objective and constraint functions. Then the accuracy of the efficient global optimization solution may not be good enough, so the three-step search method is proposed to obtain an accurate global minimum as well as prevent the distortion of kriging model for the noisy constrained computational fluid dynamics problem.},
 author = {Hyo Gil Bae and Jang Hyuk Kwon},
 doi = {10.1177/0954410013504689},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954410013504689},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {6},
 pages = {908–919},
 title = {A study on constrained efficient global optimization method for noisy computational fluid dynamics data},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954410013504689},
 volume = {228},
 year = {2014a}
}

@article{doi:10.1177/0954410014564402,
 abstract = {Infrared radiation signatures of gas turbine engine exhaust are suppressed markedly when equipped with a serpentine nozzle compared to an axisymmetric nozzle. The aim of this paper is to research more detailed flow characteristics of the serpentine nozzle, and to this end a double serpentine nozzle cold fluid test was conducted in this paper, static pressures on the nozzle walls surface were measured, and schlieren flow visualizations downstream of the nozzle exit were observed. Then numerical simulations of the experimental model were carried out using CFD software with k-ɛ turbulence model adopted. And the effects of geometric design parameters (the length ratio of first S length to second S length and the centerline distributions) on serpentine nozzle performance were investigated numerically. Detailed flow characteristics were presented including the distributions of static pressure, Ma number (streamlines), wall shear stress (limited streamlines), and the total pressure. Results show good agreement between the experimental data and computation. Static pressure distributions on the upper and down walls surface of double serpentine nozzle are completely different compared to the traditional axisymmetric nozzle. The rapid turning and steep passage slope of the serpentine nozzle would result in high friction loss and strong secondary flow loss, hence the value of the length ratio of first S passage to second S passage is recommended to be chosen from 2:5 to 2:3. The centerline distributions are crucial to the nozzle design for its influence on air acceleration inside the nozzle. The centerlines with a rapid turning at the exit would result in a high Ma number, which brings on high friction loss and secondary loss at the turnings. For maximum efficiency of centerline distributions, it is recommended that curves with a gentle turning at each serpentine passage exit should be chosen.},
 author = {Xiao-lin Sun and Zhan-xue Wang and Li Zhou and Jing-wei Shi and Zeng-wen Liu},
 doi = {10.1177/0954410014564402},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954410014564402},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {11},
 pages = {2035–2050},
 title = {Experimental and computational investigation of double serpentine nozzle},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954410014564402},
 volume = {229},
 year = {2015s}
}

@article{doi:10.1177/0954410016660873,
 abstract = {Mass capture ratio of a hypersonic air intake is one of the most important performance parameters. However, no a priori estimate of its value exists for use in initial design exercise of a hypersonic vehicle. In the present work, an air intake of a non-axisymmetric scramjet engine, designed using stream thrust methodology, is studied using computational fluid dynamic techniques. A large amount of air mass flow rate is observed to spill from the sides, which is not accounted for in the initial design phase. In absence of even an approximate estimate of this spillage, computational fluid dynamic studies become the only available tool to evaluate the mass capture ratio. Simulations are also carried out with a side wall at the intake to stop spillage. Although mass capture ratio and static pressure at combustor entry improve, deterioration in other flow parameters such as static temperature, Mach number and total pressure is observed.},
 author = {Afroz Javed and Debasis Chakraborty},
 doi = {10.1177/0954410016660873},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954410016660873},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {11},
 pages = {2111–2119},
 title = {Evaluation of side spillage for a hypersonic air intake using computational fluid dynamic techniques},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954410016660873},
 volume = {231},
 year = {2017g}
}

@article{doi:10.1177/0954410016671343,
 abstract = {A fluid–structure interaction numerical simulation was performed to investigate the flow field around a flexible flapping wing using an in-house developed computational fluid dynamics/computational structural dynamics solver. The three-dimensional (3D) fluid–structure interaction of the flapping locomotion was predicted by loosely coupling preconditioned Navier–Stokes solutions and non-linear co-rotational structural solutions. The computational structural dynamic solver was specifically developed for highly flexible flapping wings by considering large geometric non-linear characteristics. The high fidelity of the developed methodology was validated by benchmark tests. Then, an analysis of flexible flapping wings was carried out with a specific focus on the unsteady aerodynamic mechanisms and effects of flexion on flexible flapping wings. Results demonstrate that the flexion will introduce different flow fields, and thus vary thrust generation and pressure distribution significantly. In the meanwhile, relationship between flapping frequency and flexion plays an important role on efficiency. Therefore, appropriate combination of frequency and flexion of flexible flapping wings provides higher efficiency. This study may give instruction for further design of flexible flapping wings.},
 author = {Long Liu and Hongda Li and Haisong Ang and Tianhang Xiao},
 doi = {10.1177/0954410016671343},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954410016671343},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {1},
 pages = {85–95},
 title = {Numerical investigation of flexible flapping wings using computational fluid dynamics/computational structural dynamics method},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954410016671343},
 volume = {232},
 year = {2018k}
}

@article{doi:10.1177/0954410016673394,
 abstract = {In this paper, a computational fluid dynamics trimming method is proposed and compared with wind tunnel experiment and the blade element method. The NASA’s generic ROBIN helicopter model is adopted for transient simulations to obtain the final main rotor trimming conditions. Totally three steps were applied to the computational fluid dynamics method. The first step is associated with no cyclic pitch motion, the second is regarding pure longitudinal cyclic pitch motion and the last is concerning with pure lateral cyclic pitch motion. At the same time, a simple linear equation system between the roll and pitching moment was established to get the final longitudinal and lateral cyclic pitch angles for the main rotor through the above three steps. An overset grid approach was used where the volume around each blade was modeled in an individual overset grid region. The rotor rotation was resolved with three degrees per time-step. Turbulence was modeled with the well-known SST K-omega model with second-order convection. The helicopter was in straight forward flight with an advance ratio of . Three sources of stick angles, which are also called rotor trimming angles, were shown and compared with each other. And the corresponding results were also plotted with a type of history plot in the computational fluid dynamics condition. In the simulations, the results became quasi periodic after about 1.5 rotations and four rotor rotations were simulated for each case. The pitch moment coefficient and roll moment coefficient were all trimmed to about zero by the computational fluid dynamics trimming method while moments were not removed thoroughly in the other two source conditions.},
 author = {Yimin Ma and Ming Chen and Qiang Wang and Fang Wang},
 doi = {10.1177/0954410016673394},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954410016673394},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {1},
 pages = {169–179},
 title = {Main helicopter rotor trimming using computational fluid dynamics method in forward flight},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954410016673394},
 volume = {232},
 year = {2018g}
}

@article{doi:10.1177/0954410017740922,
 abstract = {Complicated flowfields near casing in a transonic axial flow compressor rotor have been numerically investigated in this paper. Two vortex identification methods, namely the Eigenvector Method and Lambda 2 Method, are introduced as important tools for the graphical representation of the concentrated vortices arising from tip leakage flow and blade boundary layer separation. The analysis of the numerical results reveals that multiple tip vortices whose development are dependent on the variation of shock wave configuration are observed at conditions around the peak efficiency point. However, with the decrease of the massflow rate, only the well-known tip leakage vortex and the second tip vortex are left in the tip region due to the disappearance of the second shock wave. Then when the massflow rate further decreases to the stall limit, an deceleration flow region emerges downstream of the shock wave due to an increasing interaction between the first shock wave and the well-known tip leakage vortex. The tip leakage vortex further experiences a bubble-type and then spiral-type breakdown at near stall flow conditions. In addition, the validity of the two vortex identification methods is also discussed in this paper. It is found that both methods are able to identify and accentuate the concentrated streamwise vortices near casing when a vortex is not disrupted. However, if the vortex breakdown occurs, only Eigenvector Method can describe the breakdown region in a deep view.},
 author = {Yanhui Wu and Guangyao An and Zhiyang Chen and Bo Wang},
 doi = {10.1177/0954410017740922},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954410017740922},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {2},
 pages = {710–724},
 title = {Computational analysis of vortices near casing in a transonic axial compressor rotor},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954410017740922},
 volume = {233},
 year = {2019s}
}

@article{doi:10.1177/0954410019831852,
 abstract = {At the preliminary design stage for an aero-engine, the evaluation of the nozzle performance is an important aspect as it affects the overall engine cycle behaviour. Currently, there is a lack of systematic, extensive data on the nozzle performance and its dependence on the geometric and aerodynamic aspects. This paper presents a method that can be used to build characteristic maps for a nozzle as a function of a number of geometric and aerodynamic parameters. The proposed method encompasses the design of a nozzle configuration, a parameterisation of the nozzle pressure ratio, nozzle contraction ratio, plug half-angle (β), mesh generation, and an aerodynamic assessment using the Favre-averaged Navier–Stokes method. The method has been validated against experimental performance data of a plug nozzle configuration and then used for the aerodynamic assessment. The derived nozzle maps show that the thrust coefficient (Cfg) for this type of nozzle is significantly sensitive to the combined effect of the variation of the proposed parameters on the nozzle performance. These maps were used to build low-order models to predict Cfg, using response surface methods. The performance was assessed, and the results show that these low-order methods are capable of providing Cfg estimates with sufficient accuracy for use in preliminary design assessments.},
 author = {Aws Al-Akam and Theoklis Nikolaidis and David G MacManus},
 doi = {10.1177/0954410019831852},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954410019831852},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {13},
 pages = {4879–4894},
 title = {Computational fluid dynamics-based approach for low-order models of propelling nozzle performance},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954410019831852},
 volume = {233},
 year = {2019a}
}

@article{doi:10.1177/0954410019841798,
 abstract = {This paper presents a mixed-fidelity model to investigate the effect of complex inlet distortion on aero gas turbine engines. The approach is developed by integrating a three-dimensional body force model of multistage compressors and a classical two-dimensional parallel engine model. The internal flow field of a high bypass ratio turbofan engine under different forms of total pressure distortion is simulated by the model. The simulations are discussed in depth to reveal the features of the distorted flow field in the compression components under the whole engine environment. The relationship between the overall performance of the engine and the intensity of inlet total pressure distortion is investigated quantitatively. It is evident that the mixed-fidelity model has the capability to quantitatively evaluate the effect of complex inlet distortion on the performance and the internal flow field of engines with low computational costs.},
 author = {Jin Guo and Jun Hu and Baofeng Tu and Zhiqiang Wang},
 doi = {10.1177/0954410019841798},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954410019841798},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {14},
 pages = {5295–5309},
 title = {A mixed-fidelity computational model of aero engine for inlet distortion},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954410019841798},
 volume = {233},
 year = {2019i}
}

@article{doi:10.1177/0954410019852553,
 abstract = {The wing of an unmanned aerial vehicle, RQ-7 Shadow, is modified to study the changes in the aerodynamics of the wing. The main focus is to investigate the effects of changing the components of wing design when the aircraft climbs and accelerates. These component modifications included changing the airfoil, planform, aspect ratio, and adding a winglet. Another objective is to study the efficacy of employing high-lift airfoils like the EPPLER 559 for subsonic unmanned aerial vehicle applications. For this, five wing designs are considered in this paper. Computational fluid dynamics simulations using ANSYS FLUENT® are conducted for each wing design. The CL/CD ratios for all the wings are calculated at increasing angles of attack (simulating Climbing) and increasing speed (simulating Acceleration). Compared to the NACA 4415 airfoil, which is utilized by the RQ-7 Shadow, the EPPLER 559 provides an increase in lift at the low angles of attack, but yields less of these benefits as the angle of attack increases. The tapered planform significantly reduces the high drag associated with the EPPLER 559 airfoil. The generation of higher lift forces with lower drag is further achieved by increasing the aspect ratio and through the addition of a winglet. When compared to the NACA 4415 airfoil, it is concluded that the EPPLER 559 airfoil is a viable candidate for subsonic unmanned aerial vehicle applications only when the components of wing design are altered. The performance of the wings that employ the EPPLER 559 airfoil improves when the planform is changed from rectangular to tapered, when the aspect ratio is increased and when a winglet is added.},
 author = {Z Siddiqi and JW Lee},
 doi = {10.1177/0954410019852553},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954410019852553},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {15},
 pages = {5543–5552},
 title = {A computational fluid dynamics investigation of subsonic wing designs for unmanned aerial vehicle application},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954410019852553},
 volume = {233},
 year = {2019r}
}

@article{doi:10.1177/0954410019882275,
 abstract = {This work compares the principle of a basic fin-controlled sounding rocket with coupled computational fluid dynamic and rigid body dynamic simulations of two coupling environments: (1) a low-fidelity approach using Missile DATCOM as semi-empirical aerodynamic solver, and (2) a high-fidelity approach using DLR TAU as URANS CFD code. The flight mechanics solver REENT is used in both cases. A closed-loop flight path control is developed and adjusted via low-fi simulations and then verified via high-fi simulations. For simple roll and pitching maneuvers the environments match well, whereas differences can be seen in complex maneuvers, e.g. body–body interactions of separation procedures.},
 author = {Marius Franze},
 doi = {10.1177/0954410019882275},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954410019882275},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {10},
 pages = {1611–1623},
 title = {Comparison of a closed-loop control by means of high-fidelity and low-fidelity coupled CFD/RBD computations},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954410019882275},
 volume = {234},
 year = {2020c}
}

@article{doi:10.1177/0954410020983721,
 abstract = {The geometric effects of Coanda trailing edges on the aerodynamic performance of an airfoil are numerically evaluated for a range of different freestream Mach numbers and momentum coefficients. A Circulation control (CC) airfoil with a circular trailing edge (ACTE) proves to have better control effectiveness at low subsonic freestream speeds (Mach = 0.1). A CC airfoil having an elliptic trailing edge (AETE) outperforms the ACTE at high subsonic flow conditions. The occurrence of Cμ-stall for the AETE is greatly postponed, and meanwhile the maximum net lift coefficient increment achieved for the AETE (ΔCL = 0.51) is slightly higher than that of the ACTE (ΔCL = 0.50) at Mach 0.6. Compared to the ACTE, the AETE is found to have better control consistency at different operating velocities and better control stability when the Coanda jet is supersonic. Through careful consideration of the aerodynamic performance and the control effects, the most appropriate axial ratio for an AETE ellipse is within the interval from 1.5 to 2. Finally, the flow field instability phenomenon and the jet detachment induced by the supersonic Coanda jet are investigated. A self-sustained shock-wave instability phenomenon without jet detachment is first observed in this paper.},
 author = {Yu-Wei Chu and He-Yong Xu and Chen-Liang Qiao and Yu-Hang Wang and Yue Xu},
 doi = {10.1177/0954410020983721},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954410020983721},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {12},
 pages = {1717–1733},
 title = {Computational evaluation of geometric effects on aerodynamic performance of circulation control airfoils},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954410020983721},
 volume = {235},
 year = {2021a}
}

@article{doi:10.1177/0954410021999557,
 abstract = {In recent years, the computational fluid dynamics (CFD) techniques have attracted enormous interest in the throughflow calculations, and one of the major concerns in the CFD-based throughflow method is the modeling of blade forces. In this article, a viscous blade force model in the CFD-based throughflow program was proposed to account for the loss generation. The throughflow code is based on the axisymmetric Navier–Stokes equations. The inviscid blade force is determined by calculating a pressure difference between the pressure and suction surfaces, and the viscous blade force is related to the local kinetic energy through a skin friction coefficient. The viscous blade force model was validated by a linear controlled diffusion airfoil cascade, and the results showed that it can correctly introduce the loss into the CFD-based throughflow model. Then, the code was applied to calculate the transonic NASA rotor 67, and the calculated results were in good agreement with the measured results, which showed that the calculated shock losses reduce the dependence of the throughflow calculation on the empirical correlation. Last, the 3.5-stage compressor P&W3S1 at 85%, 100%, and 105% of the design speed was performed to demonstrate the reliability of the viscous blade force model in a multistage environment. The results showed that the CFD-based throughflow method can easily predict the spanwise mixing due to the inclusion of the turbulence model, and predicted results were in acceptable agreement with the experimental results. In a word, the proposed viscous blade force model and CFD-based throughflow model can achieve the throughflow analysis with an acceptable level of accuracy and a little time-consuming.},
 author = {Jian Li and Jinfang Teng and Mingmin Zhu and Xiaoqing Qiang},
 doi = {10.1177/0954410021999557},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954410021999557},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {16},
 pages = {2493–2503},
 title = {A viscous blade body force model for computational fluid dynamics–based throughflow analysis of axial compressors},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954410021999557},
 volume = {235},
 year = {2021o}
}

@article{doi:10.1177/09544100221130384,
 abstract = {Flame stabilization distribution and efficient fuel mixing are highly significant in the performance of ramjet engine. In this paper, numerical studies are done to investigate the supersonic combustion of hydrogen and air co-flow in presence of the ramp edge. In this study, flame maintenance and combustion formation in a dual-combustion ramjet engine are fully investigated. The primary emphasis of this work is to disclose the impact of produced shocks due to the existence of ramp edge on the structure of fuel and air-jet immediately downstream of injector. The shock interactions of two different angles (10 deg and 30 deg) of the ramp edge are investigated. Favre-averaged conservation equations are considered for the simulation of compressible flows inside the combustor. Various mechanisms expressing the flame spreading characteristics are also examined. Depending on the angle of the ramp edge, the produced shock waves influence on size and strength of vortices. Our findings also show that high angles of ramp edge augment the fluctuation of the vortices by the splitter and consequently, fuel mixing enhances in the combustion chamber of ramjet engine. Our findings also confirm that the similar jet pressure would be more effective on the vortex production downstream of the splitter. The application of ramp edge with angle of 30 deg would increase average momentum thickness of the mixing layer up to 55%.},
 author = {MohammadKazem Rostamian and Soroush Maddah and Yasser Rostamiyan},
 doi = {10.1177/09544100221130384},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09544100221130384},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {7},
 pages = {1511–1519},
 title = {Computational study of hydrogen and air co-flow jets mixing in dual-combustor ramjet in presence of ramp shock generator},
 url = {https://doi-org.crai.referencistas.com/10.1177/09544100221130384},
 volume = {237},
 year = {2023o}
}

@article{doi:10.1177/09544100231163202,
 abstract = {Based on the open-source computational fluid dynamics (CFD) platform, OpenFOAM, two numerical simulation methods for gusty inflow characterization and gust response prediction are implemented by solving the fundamental incompressible unsteady Reynolds averaged Navier–Stokes (URANS) equations. One is the Field Velocity Method (FVM) and the other is the Oscillating Vane Method (OVM). The gust velocity field is characterized and the aerodynamic responses of some airfoils under the Sears-type sinusoidal gusts are predicted by both gust simulation methods. The results indicate that both methods are capable of obtaining satisfactory gusty inflow conditions as expected as well as the airfoil aerodynamic responses. Comparatively, from the perspective of computing cost, the FVM is more advantageous in reducing the computational resources than the OVM while simultaneously ensuring the computational accuracy.},
 author = {Zhenlong Wu and Yuan Gao and Xiaoming He and Weizhe Fu and Jianqiang Shi and Zhibo Zhang and Ruitao Zhou},
 doi = {10.1177/09544100231163202},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09544100231163202},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {12},
 pages = {2833–2843},
 title = {Comparison between two computational fluid dynamics methods for gust response predictions},
 url = {https://doi-org.crai.referencistas.com/10.1177/09544100231163202},
 volume = {237},
 year = {2023s}
}

@article{doi:10.1177/09544100241234374,
 abstract = {The capability for solving compressible fluid flows in the rotating frame of reference is added to an existed open-source CFD solver, namely, HiSA solver. The new implementation is explained and validated using the experimental data of the Sikorsky S-76 rotor. A comparison is presented between the moving mesh results obtained from the original HiSA code and the single rotating frame results achieved through the new implementation. The comparison includes an analysis of torque and thrust values, as well as computational costs. The results imply that, for evaluating the performance of an isolated rotor or for shape optimization purposes at the transonic regime, the single rotating frame method, like the one introduced in the current work, can provide accurate results within an acceptable computational budget. Furthermore, the results show that, at least 25 revolutions are required for the transient analysis to reach an acceptable steady-state converged solution like the one obtained by the single rotating frame method.},
 author = {Saleh Abuhanieh},
 doi = {10.1177/09544100241234374},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09544100241234374},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {6},
 pages = {582–595},
 title = {Development of a steady-state computational fluid dynamics solver for transonic rotorcraft flows},
 url = {https://doi-org.crai.referencistas.com/10.1177/09544100241234374},
 volume = {238},
 year = {2024a}
}

@article{doi:10.1177/09544100241259904,
 abstract = {Functionally graded materials have been of great interest to researchers for the past two decades. The reason for this is that these materials have outstanding and special material properties compared to many other materials. One of the branches of engineering sciences that has studied these materials in particular during these years is computational mechanics. With this approach, especially static, vibration and buckling analysis, thousands of studies have been done on functionally graded materials. One drawback of these studies is that they are mostly done theoretically and the results of their modeling are not compared with laboratory results although one can find some experimental results in the literature. One reason for not comparing with experimental results could be that these experimental results are buried under tons of theoretical results and do not appear to researchers at all. Our aim in this mini-review is to bring some of these experimental results on functionally graded materials to the showcase for the attention of researchers. The experimental results presented in this article are categorized as follow: a) axially layered FG structure b) axially continuous FG structure c) layered FG structure with variation of properties in the thickness direction d) continuous FG structure with variation of properties in the thickness direction e) FG nanocomposite.},
 author = {Farshad Heidari and Keyvan Taheri and Maziar Janghorban},
 doi = {10.1177/09544100241259904},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09544100241259904},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {0},
 pages = {09544100241259904},
 title = {On the experimental results of functionally graded materials with computational mechanics approach: Review},
 url = {https://doi-org.crai.referencistas.com/10.1177/09544100241259904},
 volume = {0},
 year = {2024i}
}

@article{doi:10.1177/0954411911411605,
 abstract = {Ceramic-on-ceramic hip resurfacing can potentially offer the bone-conserving advantages of resurfacing while eliminating metal ion release. Thin-walled ceramic resurfacing heads are conceivable following developments in the strength and reliability of ceramic materials, but verification of new designs is required. The present study aimed to develop a mechanical pre-clinical analysis verification process for ceramic resurfacing heads, using the DeltaSurf prosthesis design as a case study. Finite element analysis of a range of in vivo scenarios was used to design a series of physiologically representative mechanical tests, which were conducted to verify the strength of the prosthesis. Tests were designed to simulate ideal and worst-case in vivo loading and support, or to allow comparison with a clinically successful metallic device. In tests simulating ideal loading and support, the prosthesis sustained a minimum load of 39 kN before fracture, and survived 10 000 000 fatigue cycles of 0.534 kN to 5.34 kN. In worst-case tests representing a complete lack of superior femoral head bone support or pure cantilever loading of the prosthesis stem, the design demonstrated strength comparable to that of the equivalent metal device. The developed mechanical verification test programme represents an improvement in the state of the art where international test standards refer largely to total hip replacement prostheses. The case study’s novel prosthesis design performed with considerable safety margins compared with extreme in vivo loads, providing evidence that the proposed ceramic resurfacing heads should have sufficient strength to perform safely in vivo. Similar verification tests should be designed and conducted for novel ceramic prosthesis designs in the future, leading the way to clinical evaluation.},
 author = {A S Dickinson and M Browne and K C Wilson and J R T Jeffers and A C Taylor},
 doi = {10.1177/0954411911411605},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954411911411605},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine},
 note = {PMID:22070024},
 number = {9},
 pages = {866–876},
 title = {Pre-clinical evaluation of ceramic femoral head resurfacing prostheses using computational models and mechanical testing},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954411911411605},
 volume = {225},
 year = {2011e}
}

@article{doi:10.1177/0954411912437124,
 abstract = {We developed a novel concept of endovascular thrombolysis that employs a vibrating electroactive polymer actuator. In order to predict the efficacy of thrombolysis using the developed vibrating actuator, enzyme (plasminogen activator) perfusion into a clot was analyzed by solving flow fields and species transport equations considering the fluid structure interaction. In vitro thrombolysis experiments were also performed. Computational results showed that plasminogen activator perfusion into a clot was enhanced by actuator vibration at frequencies of 1 and 5 Hz. Plasminogen activator perfusion was affected by the actuator oscillation frequencies and amplitudes that were determined by electromechanical characteristics of a polymer actuator. Computed plasminogen activator perfused volumes were compared with experimentally measured dissolved clot volumes. The computed plasminogen activator perfusion volumes with threshold concentrations of 16% of the initial plasminogen activator concentration agreed well with the in vitro experimental data. This study showed the effectiveness of actuator oscillation on thrombolysis and the validity of the computational plasminogen activator perfusion model for predicting thrombolysis in complex flow fields induced by an oscillating actuator.},
 author = {Jeong Hyun Lee and Jin Sun Oh and Bye Ri Yoon and Seung Hong Choi and Kyehan Rhee and Jae Young Jho and Moon Hee Han},
 doi = {10.1177/0954411912437124},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954411912437124},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine},
 note = {PMID:22611874},
 number = {4},
 pages = {337–340},
 title = {Computational analysis of blood clot dissolution using a vibrating catheter tip},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954411912437124},
 volume = {226},
 year = {2012g}
}

@article{doi:10.1177/0954411912470239,
 abstract = {This review focuses on the modeling of articular cartilage (at the tissue level), chondrocyte mechanobiology (at the cell level) and a combination of both in a multiscale computation scheme. The primary objective is to evaluate the advantages and disadvantages of conventional models implemented to study the mechanics of the articular cartilage tissue and chondrocytes. From monophasic material models as the simplest form to more complicated multiscale theories, these approaches have been frequently used to model articular cartilage and have contributed significantly to modeling joint mechanics, addressing and resolving numerous issues regarding cartilage mechanics and function. It should be noted that attentiveness is important when using different modeling approaches, as the choice of the model limits the applications available. In this review, we discuss the conventional models applicable to some of the mechanical aspects of articular cartilage such as lubrication, swelling pressure and chondrocyte mechanics and address some of the issues associated with the current modeling approaches. We then suggest future pathways for a more realistic modeling strategy as applied for the simulation of the mechanics of the cartilage tissue using multiscale and parallelized finite element method.},
 author = {Hadi Mohammadi and Kibret Mequanint and Walter Herzog},
 doi = {10.1177/0954411912470239},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954411912470239},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine},
 note = {PMID:23637216},
 number = {4},
 pages = {402–420},
 title = {Computational aspects in mechanical modeling of the articular cartilage tissue},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954411912470239},
 volume = {227},
 year = {2013m}
}

@article{doi:10.1177/0954411914542170,
 abstract = {In computational fluid dynamics models for hemodynamics applications, boundary conditions remain one of the major issues in obtaining accurate fluid flow predictions. For major cardiovascular models, the realistic boundary conditions are not available. In order to address this issue, the whole computational domain needs to be modeled, which is practically impossible. For simulating fully developed turbulent flows using the large eddy simulation and dynamic numerical solution methods, which are very popular in hemodynamics studies, periodic boundary conditions are suitable. This is mainly because the computational domain can be reduced considerably. In this study, a novel periodic boundary condition is proposed, which is based on mass flow condition. The proposed boundary condition is applied on a square duct for the sake of validation. The mass-based condition was shown to obtain the solution in 15% less time. As such, the mass-based condition has two decisive advantages: first, the solution for a given Reynolds number can be obtained in a single simulation because of the direct specification of the mass flow, and second, simulations can be made more quickly.},
 author = {Fereshteh Bahramian and Hadi Mohammadi},
 doi = {10.1177/0954411914542170},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954411914542170},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine},
 note = {PMID:25015666},
 number = {7},
 pages = {643–651},
 title = {A novel periodic boundary condition for computational hemodynamics studies},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954411914542170},
 volume = {228},
 year = {2014a}
}

@article{doi:10.1177/0954411915624451,
 abstract = {To date, to the best of the authors’ knowledge, in almost all of the studies performed around the hemodynamics of bileaflet mechanical heart valves, a heart rate of 70–72 beats/min has been considered. In fact, the heart rate of ~72 beats/min does not represent the entire normal physiological conditions under which the aortic or prosthetic valves function. The heart rates of 120 or 50 beats/min may lead to hemodynamic complications, such as plaque formation and/or thromboembolism in patients. In this study, the hemodynamic performance of the bileaflet mechanical heart valves in a wide range of normal and physiological heart rates, that is, 60–150 beats/min, was studied in the opening phase. The model considered in this study was a St. Jude Medical bileaflet mechanical heart valve with the inner diameter of 27 mm in the aortic position. The hemodynamics of the native valve and the St. Jude Medical valve were studied in a variety of heart rates in the opening phase and the results were carefully compared. The results indicate that peak values of the velocity profile downstream of the valve increase as heart rate increases, as well as the location of the maximum velocity changes with heart rate in the St. Jude Medical valve model. Also, the maximum values of shear stress and wall shear stresses downstream of the valve are proportional to heart rate in both models. Interestingly, the maximum shear stress and wall shear stress values in both models are in the same range when heart rate is <90 beats/min; however, these values significantly increase in the St. Jude Medical valve model when heart rate is >90 beats/min (up to ~40% growth compared to that of the native valve). The findings of this study may be of importance in the hemodynamic performance of bileaflet mechanical heart valves. They may also play an important role in design improvement of conventional prosthetic heart valves and the design of the next generation of prosthetic valves, such as percutaneous valves.},
 author = {Mehdi Jahandardoost and Guy Fradet and Hadi Mohammadi},
 doi = {10.1177/0954411915624451},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954411915624451},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine},
 note = {PMID:26786673},
 number = {3},
 pages = {175–190},
 title = {Effect of heart rate on the hemodynamics of bileaflet mechanical heart valves’ prostheses (St. Jude Medical) in the aortic position and in the opening phase: A computational study},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954411915624451},
 volume = {230},
 year = {2016i}
}

@article{doi:10.1177/0954411916683221,
 abstract = {Respiratory disease is a significant problem worldwide, and it is a problem with increasing prevalence. Pathology in the upper airways and lung is very difficult to diagnose and treat, as response to disease is often heterogeneous across patients. Computational models have long been used to help understand respiratory function, and these models have evolved alongside increases in the resolution of medical imaging and increased capability of functional imaging, advances in biological knowledge, mathematical techniques and computational power. The benefits of increasingly complex and realistic geometric and biophysical models of the respiratory system are that they are able to capture heterogeneity in patient response to disease and predict emergent function across spatial scales from the delicate alveolar structures to the whole organ level. However, with increasing complexity, models become harder to solve and in some cases harder to validate, which can reduce their impact clinically. Here, we review the evolution of complexity in computational models of the respiratory system, including successes in translation of models into the clinical arena. We also highlight major challenges in modelling the respiratory system, while making use of the evolving functional data that are available for model parameterisation and testing.},
 author = {Alys R Clark and Haribalan Kumar and Kelly Burrowes},
 doi = {10.1177/0954411916683221},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954411916683221},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine},
 note = {PMID:28427314},
 number = {5},
 pages = {355–368},
 title = {Capturing complexity in pulmonary system modelling},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954411916683221},
 volume = {231},
 year = {2017c}
}

@article{doi:10.1177/0954411916683222,
 abstract = {Cement augmentation in vertebrae (vertebroplasty) is usually used to restore mechanical strength after spinal fracture but could also be used as a prophylactic treatment. So far, the mechanical competence has been determined immediately post-treatment, without considering long-term effects of bone adaptation. In this work, we investigated such long-term effects of vertebroplasty on the stiffness of the augmented bone by means of computational simulation of bone adaptation. Using micro-finite element analysis, we determined sites of increased mechanical stress (stress raisers) and stress shielding and, based on the simulations, regions with increased or decreased bone loss due to augmentation. Cement volumes connecting the end plates led to increased stress shielding and bone loss. The increased stiffness due to the augmentation, however, remained constant over the simulation time of 30 years. If the intervention was performed at an earlier time point, it did lead to more bone loss, but again, it did not affect long-term stability as this loss was compensated by bone gains in other areas. In particular, around the augmentation cement, bone structures were preserved, suggesting a long-term integration of the cement in the augmented bone. We conclude that, from a biomechanical perspective, the impact of vertebroplasty on the bone at the microstructural level is less detrimental than previously thought.},
 author = {Sandro D Badilatti and Patrik Christen and Stephen J Ferguson and Ralph Müller},
 doi = {10.1177/0954411916683222},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954411916683222},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine},
 note = {PMID:28427315},
 number = {5},
 pages = {423–431},
 title = {Computational modeling of long-term effects of prophylactic vertebroplasty on bone adaptation},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954411916683222},
 volume = {231},
 year = {2017f}
}

@article{doi:10.1177/0954411917704733,
 author = {Marco Viceconti},
 doi = {10.1177/0954411917704733},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954411917704733},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine},
 note = {PMID:28427312},
 number = {5},
 pages = {353–354},
 title = {Special Issue on ‘Computational Modelling in Medicine’: Guest editor introduction},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954411917704733},
 volume = {231},
 year = {2017t}
}

@article{doi:10.1177/0954411920914859,
 abstract = {It has been hypothesized that the muscular efforts exerted during standing may be altered by changes in personal factors, such as the body stature and muscular strength. The goal of this work was to assess the contribution of leg muscles using a biomechanical model in different physical conditions and various initial postures. An optimized inverse dynamics model was employed to find the maximum muscular effort in 23,040 postures. The simulation results showed that mid-range knee flexion could help the healthy and strong individuals maintain balance, but those with weaker muscle strength required more knee flexion. Individuals of weak muscular constitution as well as those with tall stature are at the highest risk of imbalance/falling. The number of imbalanced postures due to deficits in the calf and hamstring muscles was reduced by 7.5 times by strengthening the whole body musculature. The calf and the hamstring muscles play a key role in balance regardless of stature.},
 author = {Mohammed N Ashtiani and Mahmood-Reza Azghani and Mohamad Parnianpour and Kinda Khalaf},
 doi = {10.1177/0954411920914859},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954411920914859},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine},
 note = {PMID:32267825},
 number = {7},
 pages = {674–685},
 title = {Effects of human stature and muscle strength on the standing strategies: A computational biomechanical study},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954411920914859},
 volume = {234},
 year = {2020c}
}

@article{doi:10.1177/0954411920923253,
 abstract = {Atherosclerosis at the early stage in coronary arteries has been associated with low cycle-average wall shear stress magnitude. However, parallel to the identification of an established active role for low wall shear stress in the onset/progression of the atherosclerotic disease, a weak association between lesions localization and low/oscillatory wall shear stress has been observed. In the attempt to fully identify the wall shear stress phenotype triggering early atherosclerosis in coronary arteries, this exploratory study aims at enriching the characterization of wall shear stress emerging features combining correlation-based analysis and complex networks theory with computational hemodynamics. The final goal is the characterization of the spatiotemporal and topological heterogeneity of wall shear stress waveforms along the cardiac cycle. In detail, here time-histories of wall shear stress magnitude and wall shear stress projection along the main flow direction and orthogonal to it (a measure of wall shear stress multidirectionality) are analyzed in a representative dataset of 10 left anterior descending pig coronary artery computational hemodynamics models. Among the main findings, we report that the proposed analysis quantitatively demonstrates that the model-specific inlet flow-rate shapes wall shear stress time-histories. Moreover, it emerges that a combined effect of low wall shear stress magnitude and of the shape of the wall shear stress–based descriptors time-histories could trigger atherosclerosis at its earliest stage. The findings of this work suggest for new experiments to provide a clearer determination of the wall shear stress phenotype which is at the basis of the so-called arterial hemodynamic risk hypothesis in coronary arteries.},
 author = {Karol Calò and Giuseppe De Nisco and Diego Gallo and Claudio Chiastra and Ayla Hoogendoorn and David A Steinman and Stefania Scarsoglio and Jolanda J Wentzel and Umberto Morbiducci},
 doi = {10.1177/0954411920923253},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954411920923253},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine},
 note = {PMID:32460666},
 number = {11},
 pages = {1209–1222},
 title = {Exploring wall shear stress spatiotemporal heterogeneity in coronary arteries combining correlation-based analysis and complex networks with computational hemodynamics},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954411920923253},
 volume = {234},
 year = {2020b}
}

@article{doi:10.1177/0954411920960256,
 abstract = {Lateral ankle instability, resulting from the inability of ankle ligaments to heal after injury, is believed to cause a change in the articular contact mechanics that may promote cartilage degeneration. Considering that lateral ligaments’ insufficiency has been related to rotational instability of the talus, and that few studies have addressed the contact mechanics under this condition, the aim of this work was to evaluate if a purely rotational ankle instability could cause non-physiological changes in contact pressures in the ankle joint cartilages using the ﬁnite element method. A finite element model of a healthy ankle joint, including bones, cartilages and nine ligaments, was developed. Pure internal talus rotations of 3.67°, 9.6° and 13.43°, measured experimentally for three ligamentous configurations, were applied. The ligamentous configurations consisted in a healthy condition, an injured condition in which the anterior talofibular ligament was cut, and an injured condition in which the anterior talofibular and calcaneofibular ligaments were cut. For all simulations, the contact areas and maximum contact pressures were evaluated for each cartilage. The results showed not only an increase of the maximum contact pressures in the ankle cartilages, but also novel contact regions at the anteromedial and posterolateral sections of the talar cartilage with increasing internal rotation. The anteromedial and posterolateral contact regions observed due to pathological internal rotations of the talus are a computational evidence that supports the link between a pure rotational instability and the pattern of pathological cartilaginous load seen in patients with long-term lateral chronic ankle instability.},
 author = {G Marta and C Quental and J Folgado and F Guerra-Pinto},
 doi = {10.1177/0954411920960256},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0954411920960256},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine},
 note = {PMID:33008273},
 number = {1},
 pages = {82–88},
 title = {Contact patterns in the ankle joint after lateral ligamentous injury during internal rotation: A computational study},
 url = {https://doi-org.crai.referencistas.com/10.1177/0954411920960256},
 volume = {235},
 year = {2021k}
}

@article{doi:10.1177/09544119211040998,
 abstract = {Distal femoral fractures associated with the femoral stem in a well-fixed hip arthroplasty pose a risk of an interprosthetic fracture, the treatment of which is known as difficult. To effectively prevent and treat IP fractures, biomechanical effects must be demonstrated. We defined eight variations of the interprosthetic distance ranging from 48 mm overlap to 128 mm gap. Femoral geometries with normal and reduced cortical thickness were modeled to evaluate the effects of cortical thickness. In addition to the intact model, a total of 16 finite element models were analyzed under physiological boundary conditions. Maximum and minimum principal strains on the lateral and medial cortex surfaces were always found to be greater in models with reduced cortical thickness than in normal femurs. The model with 48 mm overlapping interprosthetic distance produced the least peak strain and the model with 16 mm interprosthetic gap produced the greatest strain with both normal and reduced cortical thickness. The screw holes produced local strain concentrations and increased the peak strains on the cortex surfaces, especially close to the stem tip. Statistically, a significant correlation (R2 = 0.9483) was found between strain shielding and interprosthetic distance. Axial stiffness, interfragmentary shear motion, and maximum von-Mises stress on the distal plate showed a high correlation with the interprosthetic distance. It was concluded that the overlapping structures are superior to other fixations we analyzed in that they offer better mechanical stability and eliminates the local strain concentrations.},
 author = {Musa Güngörürler and Onur Gürsan and Hasan Havıtçıoğlu},
 doi = {10.1177/09544119211040998},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09544119211040998},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine},
 note = {PMID:34425723},
 number = {2},
 pages = {169–178},
 title = {Computational analysis of the effects of interprosthetic distance on normal and reduced cortical thickness femur models},
 url = {https://doi-org.crai.referencistas.com/10.1177/09544119211040998},
 volume = {236},
 year = {2022g}
}

@article{doi:10.1177/09544119221102704,
 abstract = {In recent years, the triply periodic minimal surface (TPMS)-based scaffolds have been served as one of the crucial types of structures for biological replacements, the energy absorber, etc. Meanwhile, the development of additive manufacturing (AM) has facilitated the production of TPMS scaffolds with complex microstructures. However, the design maps of TPMS scaffolds, especially considering the AM constraints, remain unclear, which has hindered the design and application of TPMS scaffolds. The aims of the present study were to develop an efficient computational modeling framework for investigating the design maps of TPMS scaffolds simultaneously considering the AM constraints, the biological requirements, and the structural anisotropy. To demonstrate the computational framework, five widely-used topologies of the TPMS-based scaffolds (i.e. the Diamond, the Gyroid, the Fischer-Koch S, the F-RD, and the Schwarz P) were used, whose design maps for the surface-to-volume ratio and the effective elastic modulus were also investigated. The results showed that as the porosities increase, the design ranges of the surface-to-volume ratios decreases for all the structures. Compared with the effect of the constraint for the pore size, the minimal structural thickness for AM constraint has a greater effect on the surface-to-volume ratio. Regarding the elastic modulus, in the region of low porosity (approximately 0.5–0.7), the range for the effective elastic modulus of Schwarz P is the widest (approximately 2.24–32.6 GPa), but the Gyroid can achieve both high porosity and low effective elastic modulus (e.g. 0.61 GPa at the porosity of 0.90). These results and the method developed in the present study provided important basis and guidance for the design and application of the TPMS-based porous structures.},
 author = {Yongtao Lu and Yi Huo and Jia’ao Zou and Yanchen Li and Zhuoyue Yang and Hanxing Zhu and Chengwei Wu},
 doi = {10.1177/09544119221102704},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09544119221102704},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine},
 note = {PMID:35647704},
 number = {8},
 pages = {1157–1168},
 title = {Comparison of the design maps of TPMS based bone scaffolds using a computational modeling framework simultaneously considering various conditions},
 url = {https://doi-org.crai.referencistas.com/10.1177/09544119221102704},
 volume = {236},
 year = {2022l}
}

@article{doi:10.1177/09544119231208223,
 abstract = {Evaluation of cell response to mechanical stimuli at in vitro conditions is known as one of the important issues for modulating cell behavior. Mechanical stimuli, including mechanical vibration and oscillatory fluid flow, act as important biophysical signals for the mechanical modulation of stem cells. In the present study, mesenchymal stem cell (MSC) consists of cytoplasm, nucleus, actin, and microtubule. Also, integrin and primary cilium were considered as mechanoreceptors. In this study, the combined effect of vibration and oscillatory fluid flow on the cell and its components were investigated using numerical modeling. The results of the FEM and FSI model showed that the cell response (stress and strain values) at the frequency of mechanical vibration has the highest value. The achieved results on shear stress caused by the fluid flow on the cell showed that the cell experiences shear stress in the range of . Mechanoreceptors that bind separately to the cell surface, can be highly stimulated by hydrodynamic pressure and, therefore, can play a role in the mechanical modulation of MSCs at in vitro conditions. The results of this research can be effective in future studies to optimize the conditions of mechanical stimuli applied to the cell culture medium and to determine the mechanisms involved in mechanotransduction.},
 author = {Mohammadreza Mohseni and Bahman Vahidi and Hamidreza Azizi},
 doi = {10.1177/09544119231208223},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09544119231208223},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine},
 note = {PMID:37982187},
 number = {12},
 pages = {1377–1389},
 title = {Computational simulation of applying mechanical vibration to mesenchymal stem cell for mechanical modulation toward bone tissue engineering},
 url = {https://doi-org.crai.referencistas.com/10.1177/09544119231208223},
 volume = {237},
 year = {2023r}
}

@article{doi:10.1177/09544119241272782,
 abstract = {The selection of internal fixation as the primary fixation modality for the patient is one of the challenges for the surgeon treating the patient in question. A model of the lateral tibial plateau fracture was established. Three different configurations of internal fixators namely L bone plate, T bone plate, and screw-washer were analyzed. Three stages after surgery were simulated to assess the displacement of bone plates, screws, washers, and the stress shielding ratio in the fracture area. At three stages after surgery, the T bone plate showed better stability for patients during rehabilitation compared with the remaining two schemes, and the screw-washer scheme was the least stable due to the larger internal fixation displacement and stress shielding ratio in the fracture area. In contrast, the L bone plate scheme showed better stability in the early stages after surgery but was second only to the screw-washer scheme in the middle and late stages after surgery. The T bone plate showed better stability and became a new selection for surgeons to treat related patients. At three stages after surgery, the T bone plate has better biomechanical stability compared to the L bone plate and screw-washer schemes.},
 author = {Yafeng Li and Fengyuan Lu and Peng Yi and Zichun Zou and Xi Zhang and Jing Zhang},
 doi = {10.1177/09544119241272782},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09544119241272782},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine},
 note = {PMID:39180409},
 number = {8–9},
 pages = {897–908},
 title = {Computational analysis of the biomechanical stability of internal fixation of the lateral tibial plateau fracture: A mechanical stability study},
 url = {https://doi-org.crai.referencistas.com/10.1177/09544119241272782},
 volume = {238},
 year = {2024h}
}

@article{doi:10.1177/09560599221081104,
 abstract = {This paper presents the use of a computational design algorithm in combination with robotic fabrication and sensing to augment the design and construction process for non-standardized material. Although reusing reclaimed material can significantly reduce the environmental impact in building construction, current design processes are not set up for this shift in thinking. Contrary to conventional practices, designing within the constraints of available material means that geometry and topology cannot be fully pre-determined. This paper introduces a design methodology for corrugated shell structures from folded sheet metal of variable geometries and properties, in which the design goal adapts to available material. It follows a two-fold approach of digital algorithm development and scanning and physical prototyping for robotic fabrication. The scanned materials database is classified based on object geometry data and material properties; such as thickness, type of metal, and spring-back values for fabrication purposes. Together with a target surface, it is an input for a generative design algorithm consisting of surface generation and optimization. The surface generation tries to approximate the target through a translation of search algorithm results for object placement into a 2D mesh graph which is then linked to 3D particle spring based form-finding. The optimization consists of evaluation of structural, fabrication, and design criteria, with finally user selection. Robotic fabrication included object recognition, metal sheet folding and consideration of different metal spring back behavior. These methods were tested on a single curved arch surface and applied to a double curved cantilever canopy as a final demonstrator. The algorithm results showed a generation of different corrugated shell topologies based on iterated object placement. As a demonstrator, a part of the selected canopy was robotically fabricated from local industrial leftovers.},
 author = {Seyed Mobin Moussavi and Hana Svatoš-Ražnjević and Axel Körner and Yasaman Tahouni and Achim Menges and Jan Knippers},
 doi = {10.1177/09560599221081104},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09560599221081104},
 journal = {International Journal of Space Structures},
 number = {2},
 pages = {119–134},
 title = {Design based on availability: Generative design and robotic fabrication workflow for non-standardized sheet metal with variable properties},
 url = {https://doi-org.crai.referencistas.com/10.1177/09560599221081104},
 volume = {37},
 year = {2022l}
}

@article{doi:10.1177/0956797612437427,
 abstract = {A central question in cognitive science is whether natural language provides combinatorial operations that are essential to diverse domains of thought. In the study reported here, we addressed this issue by examining the role of linguistic mechanisms in forging the hierarchical structures of algebra. In a 3-T functional MRI experiment, we showed that processing of the syntax-like operations of algebra does not rely on the neural mechanisms of natural language. Our findings indicate that processing the syntax of language elicits the known substrate of linguistic competence, whereas algebraic operations recruit bilateral parietal brain regions previously implicated in the representation of magnitude. This double dissociation argues against the view that language provides the structure of thought across all cognitive domains.},
 author = {Martin M. Monti and Lawrence M. Parsons and Daniel N. Osherson},
 doi = {10.1177/0956797612437427},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0956797612437427},
 journal = {Psychological Science},
 note = {PMID:22760883},
 number = {8},
 pages = {914–922},
 title = {Thought Beyond Language: Neural Dissociation of Algebra and Natural Language},
 url = {https://doi-org.crai.referencistas.com/10.1177/0956797612437427},
 volume = {23},
 year = {2012l}
}

@article{doi:10.1177/0956797616640237,
 abstract = {This article introduces a generative model of category representation that uses computer vision methods to extract category-consistent features (CCFs) directly from images of category exemplars. The model was trained on 4,800 images of common objects, and CCFs were obtained for 68 categories spanning subordinate, basic, and superordinate levels in a category hierarchy. When participants searched for these same categories, targets cued at the subordinate level were preferentially fixated, but fixated targets were verified faster when they followed a basic-level cue. The subordinate-level advantage in guidance is explained by the number of target-category CCFs, a measure of category specificity that decreases with movement up the category hierarchy. The basic-level advantage in verification is explained by multiplying the number of CCFs by sibling distance, a measure of category distinctiveness. With this model, the visual representations of real-world object categories, each learned from the vast numbers of image exemplars accumulated throughout everyday experience, can finally be studied.},
 author = {Chen-Ping Yu and Justin T. Maxfield and Gregory J. Zelinsky},
 doi = {10.1177/0956797616640237},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0956797616640237},
 journal = {Psychological Science},
 note = {PMID:27142461},
 number = {6},
 pages = {870–884},
 title = {Searching for Category-Consistent Features: A Computational Approach to Understanding Visual Category Representation},
 url = {https://doi-org.crai.referencistas.com/10.1177/0956797616640237},
 volume = {27},
 year = {2016s}
}

@article{doi:10.1177/0956797617708234,
 abstract = {People with attention-deficit/hyperactivity disorder (ADHD) have difficulties sustaining their attention on external tasks. Such attentional lapses have often been characterized as the simple opposite of external sustained attention, but the different types of attentional lapses, and the subjective experiences to which they correspond, remain unspecified. In this study, we showed that unmedicated children (ages 6–12) with ADHD, when probed during a standard go/no-go task, reported more mind blanking (a mental state characterized by the absence of reportable content) than did control participants. This increase in mind blanking happened at the expense of both focused and wandering thoughts. We also found that methylphenidate reverted the level of mind blanking to baseline (i.e., the level of mind blanking reported by control children without ADHD). However, this restoration led to mind wandering more than to focused attention. In a second experiment, we extended these findings to adults who had subclinical ADHD. These results suggest that executive functions impaired in ADHD are required not only to sustain external attention but also to maintain an internal train of thought.},
 author = {Charlotte Van den Driessche and Mikaël Bastian and Hugo Peyre and Coline Stordeur and Éric Acquaviva and Sara Bahadori and Richard Delorme and Jérôme Sackur},
 doi = {10.1177/0956797617708234},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0956797617708234},
 journal = {Psychological Science},
 note = {PMID:28800281},
 number = {10},
 pages = {1375–1386},
 title = {Attentional Lapses in Attention-Deficit/Hyperactivity Disorder: Blank Rather Than Wandering Thoughts},
 url = {https://doi-org.crai.referencistas.com/10.1177/0956797617708234},
 volume = {28},
 year = {2017s}
}

@article{doi:10.1177/0956797618823540,
 abstract = {Learning to read is foundational for literacy development, yet many children in primary school fail to become efficient readers despite normal intelligence and schooling. This condition, referred to as developmental dyslexia, has been hypothesized to occur because of deficits in vision, attention, auditory and temporal processes, and phonology and language. Here, we used a developmentally plausible computational model of reading acquisition to investigate how the core deficits of dyslexia determined individual learning outcomes for 622 children (388 with dyslexia). We found that individual learning trajectories could be simulated on the basis of three component skills related to orthography, phonology, and vocabulary. In contrast, single-deficit models captured the means but not the distribution of reading scores, and a model with noise added to all representations could not even capture the means. These results show that heterogeneity and individual differences in dyslexia profiles can be simulated only with a personalized computational model that allows for multiple deficits.},
 author = {Conrad Perry and Marco Zorzi and Johannes C. Ziegler},
 doi = {10.1177/0956797618823540},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0956797618823540},
 journal = {Psychological Science},
 note = {PMID:30730792},
 number = {3},
 pages = {386–395},
 title = {Understanding Dyslexia Through Personalized Large-Scale Computational Models},
 url = {https://doi-org.crai.referencistas.com/10.1177/0956797618823540},
 volume = {30},
 year = {2019o}
}

@article{doi:10.1177/0956797620968787,
 abstract = {What happens to an acoustic signal after it enters the mind of a listener? Previous work has demonstrated that listeners maintain intermediate representations over time. However, the internal structure of such representations—be they the acoustic-phonetic signal or more general information about the probability of possible categories—remains underspecified. We present two experiments using a novel speaker-adaptation paradigm aimed at uncovering the format of speech representations. We exposed adult listeners (N = 297) to a speaker whose utterances contained acoustically ambiguous information concerning phones (and thus words), and we manipulated the temporal availability of disambiguating cues via visually presented text (presented before or after each utterance). Results from a traditional phoneme-categorization task showed that listeners adapted to a modified acoustic distribution when disambiguating text was provided before but not after the audio. These results support the position that speech representations consist of activation over categories and are inconsistent with direct maintenance of the acoustic-phonetic signal.},
 author = {Spencer Caplan and Alon Hafri and John C. Trueswell},
 doi = {10.1177/0956797620968787},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0956797620968787},
 journal = {Psychological Science},
 note = {PMID:33617735},
 number = {3},
 pages = {410–423},
 title = {Now You Hear Me, Later You Don’t: The Immediacy of Linguistic Computation and the Representation of Speech},
 url = {https://doi-org.crai.referencistas.com/10.1177/0956797620968787},
 volume = {32},
 year = {2021e}
}

@article{doi:10.1177/09567976211043426,
 abstract = {People make subjective judgments about the healthiness of different foods every day, and these judgments in turn influence their food choices and health outcomes. Despite the importance of such judgments, there are few quantitative theories about their psychological underpinnings. This article introduces a novel computational approach that can approximate people’s knowledge representations for thousands of common foods. We used these representations to predict how both lay decision-makers (the general population) and experts judge the healthiness of individual foods. We also applied our method to predict the impact of behavioral interventions, such as the provision of front-of-pack nutrient and calorie information. Across multiple studies with data from 846 adults, our models achieved very high accuracy rates (r2 = .65–.77) and significantly outperformed competing models based on factual nutritional content. These results illustrate how new computational methods applied to established psychological theory can be used to better predict, understand, and influence health behavior.},
 author = {Natasha Gandhi and Wanling Zou and Caroline Meyer and Sudeep Bhatia and Lukasz Walasek},
 doi = {10.1177/09567976211043426},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09567976211043426},
 journal = {Psychological Science},
 note = {PMID:35298316},
 number = {4},
 pages = {579–594},
 title = {Computational Methods for Predicting and Understanding Food Judgment},
 url = {https://doi-org.crai.referencistas.com/10.1177/09567976211043426},
 volume = {33},
 year = {2022f}
}

@article{doi:10.1177/09567976211043428,
 abstract = {Integration to boundary is an optimal decision algorithm that accumulates evidence until the posterior reaches a decision boundary, resulting in the fastest decisions for a target accuracy. Here, we demonstrated that this advantage incurs a cost in metacognitive accuracy (confidence), generating a cognition/metacognition trade-off. Using computational modeling, we found that integration to a fixed boundary results in less variability in evidence integration and thus reduces metacognitive accuracy, compared with a collapsing-boundary or a random-timer strategy. We examined how decision strategy affects metacognitive accuracy in three cross-domain experiments, in which 102 university students completed a free-response session (evidence terminated by the participant’s response) and an interrogation session (fixed number of evidence samples controlled by the experimenter). In both sessions, participants observed a sequence of evidence and reported their choice and confidence. As predicted, the interrogation protocol (preventing integration to boundary) enhanced metacognitive accuracy. We also found that in the free-response sessions, participants integrated evidence to a collapsing boundary—a strategy that achieves an efficient compromise between optimizing choice and metacognitive accuracy.},
 author = {David Rosenbaum and Moshe Glickman and Stephen M. Fleming and Marius Usher},
 doi = {10.1177/09567976211043428},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09567976211043428},
 journal = {Psychological Science},
 note = {PMID:35333670},
 number = {4},
 pages = {613–628},
 title = {The Cognition/Metacognition Trade-Off},
 url = {https://doi-org.crai.referencistas.com/10.1177/09567976211043428},
 volume = {33},
 year = {2022p}
}

@article{doi:10.1177/09567976221140828,
 abstract = {In April 2019, Psychological Science published its first issue in which all Research Articles received the Open Data badge. We used that issue to investigate the effectiveness of this badge, focusing on the adherence to its aim at Psychological Science: sharing both data and code to ensure reproducibility of results. Twelve researchers of varying experience levels attempted to reproduce the results of the empirical articles in the target issue (at least three researchers per article). We found that all 14 articles provided at least some data and six provided analysis code, but only one article was rated to be exactly reproducible, and three were rated as essentially reproducible with minor deviations. We suggest that researchers should be encouraged to adhere to the higher standard in force at Psychological Science. Moreover, a check of reproducibility during peer review may be preferable to the disclosure method of awarding badges.},
 author = {Sophia Crüwell and Deborah Apthorp and Bradley J. Baker and Lincoln Colling and Malte Elson and Sandra J. Geiger and Sebastian Lobentanzer and Jean Monéger and Alex Patterson and D. Samuel Schwarzkopf et al.},
 doi = {10.1177/09567976221140828},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09567976221140828},
 journal = {Psychological Science},
 note = {PMID:36730433},
 number = {4},
 pages = {512–522},
 title = {What’s in a Badge? A Computational Reproducibility Investigation of the Open Data Badge Policy in One Issue of Psychological Science},
 url = {https://doi-org.crai.referencistas.com/10.1177/09567976221140828},
 volume = {34},
 year = {2023f}
}

@article{doi:10.1177/0957456516655224,
 abstract = {Ever-rising fuel costs necessitate design of fuel-efficient vehicles. Consequently, modern vehicle manufacturers are focused on designing low aerodynamic drag vehicles which would in-turn reduce the fuel consumption. This study analyses the contribution of external rear-view mirrors to the total drag force and the overall sound pressure level at the A, B and C pillars, while optimising the external rear-view mirror design accordingly. Solid Works renditions of external rear-view mirror models mounted on a reference luxury sedan were analysed using a commercially available computational fluid dynamic package ANSYS FLUENT. A different approach was followed to carry out the empirical flow visualisation and predict sound pressure levels. The aerodynamic characterisation of the vehicle was done utilising the widely used shear stress transport turbulence model, while the analysis of wind noise and the contributing vortices employed a large eddy simulation. This approach significantly reduced computational time without compromising on accuracy.},
 author = {Yagnavalkya Mukkamala and Sandeep Devabhaktuni and Vishnu Ganesh Rajkumar},
 doi = {10.1177/0957456516655224},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0957456516655224},
 journal = {Noise & Vibration Worldwide},
 number = {1–2},
 pages = {7–16},
 title = {Computational aero-acoustic modelling of external rear-view mirrors on a mid-sized Sedan},
 url = {https://doi-org.crai.referencistas.com/10.1177/0957456516655224},
 volume = {47},
 year = {2016n}
}

@article{doi:10.1177/0957650911399013,
 abstract = {Film cooling effectiveness has been studied by using a computational approach based on solving the Reynolds-averaged Navier–Stokes equations. A wind tunnel test configuration is considered with a total of four cooling hole geometries as a cylindrical hole, a cylindrical hole with an upstream wedge (called ‘ramp’ thereafter), a shaped diffuser, and a double console slot. In all cases, the hole centreline has an inclination angle of 35° against the mainstream airflow and the blowing ratio is unity. Choosing the cylindrical model as a baseline, simulations have been carried out for grid convergence and turbulence model influence studies. Results are compared with available experimental data and other numerical predictions and good agreement has been achieved. Further computations continue with three remaining geometries, using the baseline flow conditions and configuration. Comparing to the results from the baseline model, it was found that the centreline adiabatic cooling effectiveness has shown incremental increases for the ‘ramp’ model, while results from the console slot model and the shape diffuser model have exhibited significant improvements by a factor of 1.5 and 2, respectively. The reason for such a step change in cooling effectiveness is mainly due to the weakening of the vortex structures in the vicinity of the hole exit, thus significantly reducing the entrainment of surrounding ‘hot’ fluids.},
 author = {J Yao and Y F Yao},
 doi = {10.1177/0957650911399013},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0957650911399013},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {4},
 pages = {505–519},
 title = {Computational study of hole shape effect on film cooling performance},
 url = {https://doi-org.crai.referencistas.com/10.1177/0957650911399013},
 volume = {225},
 year = {2011o}
}

@article{doi:10.1177/0957650911407316,
 abstract = {In this study, to define the distribution of CO2 and methanol concentration in the anode channel, three-dimensional (3D) two-phase homogeneous computational fluid dynamics (CFD) modelling for the anode channel and one-dimensional (1D) two-phase mathematical modelling for the porous media have been considered. In the anode channel, two flow configurations, single-serpentine flow field (SSFF), and parallel flow field (PFF) were studied. Here also, the effects of inlet mass flowrate, flow configurations, inlet temperature of aqueous solution, and inlet feed concentration on the CO2 concentration in the anode channel and cell performance have been investigated. To define the interface boundary conditions between the channel and diffuser layers, the CFD modelling of the anode channel was coupled with the mathematical modelling in the porous media. The results show that the corner of the channel rib is the proper site for coalescence of CO2 gas bubbles. The finer distribution of methanol concentration and less number of gas bubbles at the SSFF configuration have been observed. This leads to a better performance of the cell in the SSFF configuration relative to the PFF configuration. With increase of the mass flowrate, the molar concentration of CO2 (gas bubbles) reduces and the cell performance improves. With increase in the temperature of aqueous methanol solution, cell performance will improve. The main reasons should be attributed to the enhanced activity of the catalyst and increase of diffusion coefficient of the methanol solution. The CO2 gas bubbles will emerge more at higher temperatures, but it is clear that the effect of enhanced activity of the catalyst and increase of the diffusion coefficient of the methanol surmounts that of numerous CO2 gas bubbles, thus leading to the improvement of the cell performance.},
 author = {S Khoshmanesh and M Mirzaei},
 doi = {10.1177/0957650911407316},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0957650911407316},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {6},
 pages = {718–733},
 title = {Performance evaluation of a direct methanol fuel cell: combination of three-dimensional computational fluid dynamic modelling and one-dimensional diffusion layer mathematical modelling},
 url = {https://doi-org.crai.referencistas.com/10.1177/0957650911407316},
 volume = {225},
 year = {2011h}
}

@article{doi:10.1177/0957650911407979,
 abstract = {This article presents an experimental and computational study of the efficiency of an unshrouded transonic turbine. This research formed part of the EU Turbine Aero-Thermal External Flows II programme. The experiments were performed in the Oxford Turbine Research Facility (previously the Turbine Test Facility at QinetiQ, Farnborough). This facility is an engine scale, short duration, rotating transonic facility, in which M, Re, , and are matched to engine conditions. For these experiments, the MT1 turbine stage was installed. Historically, turbine efficiency measurements are conducted in steady state adiabatic facilities. However, short-duration facilities allow simultaneous aerodynamic and heat transfer measurements with a significant reduction in cost. An efficiency measurement system was developed for this investigation, and this is briefly described. The system allows efficiency to be evaluated to bias and precision errors of approximately ±1.45 per cent and ±0.16 per cent, respectively, to 95 per cent confidence. The results of accurate area surveys of the turbine inlet and exit flows are presented and discussed. At the turbine exit, data were taken at two traverse planes, approximately 0.5 and 4.5 rotor axial chords downstream of the rotor. The turbine efficiency was experimentally evaluated based on the data at both planes, using a number of mixing models, which are discussed and compared. The experimental result of turbine efficiency is also compared to that estimated from a mean-line prediction. Full-stage steady and unsteady computational fluid dynamics of the experiment using the Rolls-Royce HYDRA code was conducted and is also presented. The predicted and measured rotor exit flow-fields are compared at both downstream traverse planes.},
 author = {P F Beard and A D Smith and T Povey},
 doi = {10.1177/0957650911407979},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0957650911407979},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {8},
 pages = {1166–1179},
 title = {Experimental and computational fluid dynamics investigation of the efficiency of an unshrouded transonic high pressure turbine},
 url = {https://doi-org.crai.referencistas.com/10.1177/0957650911407979},
 volume = {225},
 year = {2011a}
}

@article{doi:10.1177/0957650912473539,
 abstract = {An engine is being developed to utilise some of the heat of a wood-burning stove to produce a small quantity of electricity. The heat is used to maintain a temperature gradient in a regenerator consisting of very narrow passages in which the gas (air) is in intimate thermal contact with the material. By the thermo-acoustic principle, a strong sound wave can be developed if the regenerator is incorporated in an acoustic loop so that acoustic energy is returned to it and amplified. The excess power generated can be extracted by a linear alternator. This article describes a model of the thermal, acoustic and electrical processes, and a procedure for calculating the performance of a complete loop. Comparison of the computed results with data from three configurations of apparatus shows that the acoustic elements of the problem are quite accurately described. However, there are discrepancies between the calculated thermal and electrical parameters, and the limited measurements for a practically useful arrangement of the components.},
 author = {Chris Lawn},
 doi = {10.1177/0957650912473539},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0957650912473539},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {4},
 pages = {498–514},
 title = {Computational modelling of a thermo-acoustic travelling wave engine},
 url = {https://doi-org.crai.referencistas.com/10.1177/0957650912473539},
 volume = {227},
 year = {2013k}
}

@article{doi:10.1177/0957650913499565,
 abstract = {There is a growing interest in organic Rankine cycle turbogenerators because of their ability to efficiently utilize external heat sources at low-to-medium temperature in the small-to-medium power range. High-temperature organic Rankine cycle turbines typically operate at very high pressure ratio and expand the organic working fluid in the dense-vapour thermodynamic region, thus requiring computational fluid dynamics solvers coupled with accurate thermodynamic models for their performance assessment and design. In this article we present a steady-state three-dimensional viscous computational fluid dynamics study of the Tri-O-Gen organic Rankine cycle radial turbine, including the radial nozzle, the rotor and the diffuser. The turbine operates with toluene as the working fluid, whose accurate thermophysical properties are obtained with a look-up table approach. Based on the three-dimensional simulation results, together with a two-dimensional fluid dynamic optimisation procedure documented elsewhere, an improved nozzle geometry is designed, manufactured and experimentally tested. Measurements show it delivers 5 kWe or 4% more net power output, as well as improved off-design performance.},
 author = {John Harinck and David Pasquale and Rene Pecnik and Jos van Buijtenen and Piero Colonna},
 doi = {10.1177/0957650913499565},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0957650913499565},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {6},
 pages = {637–645},
 title = {Performance improvement of a radial organic Rankine cycle turbine by means of automated computational fluid dynamic design},
 url = {https://doi-org.crai.referencistas.com/10.1177/0957650913499565},
 volume = {227},
 year = {2013g}
}

@article{doi:10.1177/0957650913509088,
 abstract = {Application of computational fluid dynamics to real steam flows including non-equilibrium condensing flows requires an accurate and, at the same time, computationally inexpensive formulation of thermodynamic properties of steam. The present formulation enables fast computation of all thermodynamic properties, using mass density and specific internal energy as independent variables. This choice of independent variables follows the needs of time-marching computational fluid dynamics computations of dry and metastable steam flows in steam turbines. The formulation comprises an ideal-gas part and a residual part. The residual part is expressed in a form analogous to the virial equation of state but the coefficients are functions of internal energy, rather than temperature. Here we present a variant retaining only one coefficient of the density series, corresponding to the second virial coefficient. The uncertainties of properties computed from the present formulation only slightly exceed those of the fundamental formulation IAPWS-95. The formulation is valid from 253.15 K to 1073.15 K. Up to pressures given by the isentrope 7.5 kJ/(kg K), the computed properties are within the experimental uncertainties. Beyond this limit, the accuracy continuously decreases as it can be determined from presented deviation plots.},
 author = {Jan Hrubý and Jaroslav Pátek and Michal Duška},
 doi = {10.1177/0957650913509088},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0957650913509088},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {2},
 pages = {120–128},
 title = {An analytical formulation of thermodynamic properties of dry and metastable steam suitable for computational fluid dynamics modelling of steam turbine flows},
 url = {https://doi-org.crai.referencistas.com/10.1177/0957650913509088},
 volume = {228},
 year = {2014j}
}

@article{doi:10.1177/0957650913513253,
 abstract = {Two-phase computational fluid dynamics modelling is used to investigate the magnitude of different contributions to the wet steam losses in a three-stage model low pressure steam turbine. The thermodynamic losses (due to irreversible heat transfer across a finite temperature difference) and the kinematic relaxation losses (due to the frictional drag of the drops) are evaluated directly from the computational fluid dynamics simulation using a concept based on entropy production rates. The braking losses (due to the impact of large drops on the rotor) are investigated by a separate numerical prediction. The simulations show that in the present case, the dominant effect is the thermodynamic loss that accounts for over 90% of the wetness losses and that both the thermodynamic and the kinematic relaxation losses depend on the droplet diameter. The numerical results are brought into context with the well-known Baumann correlation, and a comparison with available measurement data in the literature is given. The ability of the numerical approach to predict the main wetness losses is confirmed, which permits the use of computational fluid dynamics for further studies on wetness loss correlations.},
 author = {Jörg Starzmann and Michael M Casey and Jürgen F Mayer and Frank Sieverding},
 doi = {10.1177/0957650913513253},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0957650913513253},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {2},
 pages = {216–231},
 title = {Wetness loss prediction for a low pressure steam turbine using computational fluid dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1177/0957650913513253},
 volume = {228},
 year = {2014l}
}

@article{doi:10.1177/0957650914530295,
 abstract = {A computational fluid dynamics model of the middle chamber of a triplex positive displacement reciprocating pump is presented to assess the feasibility of a transient numerical method to investigate the performance of the pump throughout the 180° of crank rotation of the inlet stroke. The paper also investigates, by means of computational fluid dynamics, the pressure drop occurring in the pump chamber during the first part of the inlet stroke in order to gain a better understanding of the mechanisms leading to cavitation. The model includes the compressibility of the working fluid and the lift of the inlet valve as a function of the pressure field on the inlet valve surfaces. It also takes into account the valve spring preload in the overall balance of forces moving the valve. Simulation of the valve motion was achieved by providing the solver with two user-defined functions. The plunger lift–time history was defined by the crank diameter and connecting rod length. This paper will demonstrate the feasibility and reasonable accuracy of the method adopted by comparison with experimental data.},
 author = {Aldo Iannetti and Matthew T Stickland and William M Dempster},
 doi = {10.1177/0957650914530295},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0957650914530295},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {5},
 pages = {574–584},
 title = {A computational fluid dynamics model to evaluate the inlet stroke performance of a positive displacement reciprocating plunger pump},
 url = {https://doi-org.crai.referencistas.com/10.1177/0957650914530295},
 volume = {228},
 year = {2014i}
}

@article{doi:10.1177/0957650914552697,
 abstract = {In gas turbines, rim seals are fitted at the periphery of the wheel-space between the turbine disc and its adjacent casing; their purpose is to reduce the ingress of hot mainstream gases. This paper describes the use of a three-dimensional, steady-state model to investigate ingress through engine-representative single and double radial-clearance seals. The three-dimensional Reynolds-averaged Navier–Stokes computations of a simplified turbine stage are carried out using the commercial computational fluid dynamics code ANSYS CFX v13, and the model is based on the geometry of an experimental test rig at the University of Bath. The measured variation of the peak-to-trough pressure difference in the annulus, which is the main driving mechanism for ingress, is reproduced well qualitatively by the computations; quantitatively, the maximum local differences between computation and experiment are less than 20% of the measured peak-to-trough circumferential variation. The radial variation of swirl ratio in the rotor–stator wheel-space is well predicted over the range of flow rates and rim seal geometries studied. The radial distribution of sealing effectiveness determined from experiments is reproduced inward of the mixing region near the seal clearance over a range of sealing flow rates; some over-prediction of the effectiveness was found for both seals at high radius, probably due to limitations in the turbulent mixing modelling. The three-dimensional steady-state approach may be a practical tool for the engine designer where there is a lack of experimental data, providing quantitative predictions of the flow structure within the rotor–stator wheel-space and qualitative predictions of the sealing effectiveness for a given rim seal geometry.},
 author = {Yogesh Lalwani and Carl M Sangan and Mike Wilson and Gary D Lock},
 doi = {10.1177/0957650914552697},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0957650914552697},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {1},
 pages = {2–15},
 title = {Steady computations of ingress through gas turbine rim seals},
 url = {https://doi-org.crai.referencistas.com/10.1177/0957650914552697},
 volume = {229},
 year = {2015k}
}

@article{doi:10.1177/0957650915589674,
 abstract = {In recent years, a considerable effort has been devoted towards the application of advanced techniques for turbomachinery efficient fluid dynamic control during operations. A novel strategy to dynamically modify and optimize the performance during operations takes advantage of shape memory alloys properties. Experimental and numerical analyses on a morphing polymeric blade for an automotive axial fan are presented. The blade shape change was achieved by shape memory alloys strips, thermomechanically treated, embedded in the blade and thermally activated by hot air stream flow. Measurement of fluid temperature, blade surface temperature pattern and three-dimensional shape change of the blade during activation was performed by means of an innovative image analysis technique in a purpose-built wind tunnel. Computational fluid dynamics numerical simulations were performed to study performance variations and three-dimensional fluid dynamic behavior of the fan originated from the shape memory effect.},
 author = {Alessio Suman and Annalisa Fortini and Nicola Aldi and Michele Pinelli and Mattia Merlin},
 doi = {10.1177/0957650915589674},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0957650915589674},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {5},
 pages = {477–486},
 title = {Using shape memory alloys for improving automotive fan blade performance: experimental and computational fluid dynamics analysis},
 url = {https://doi-org.crai.referencistas.com/10.1177/0957650915589674},
 volume = {229},
 year = {2015q}
}

@article{doi:10.1177/0957650918783923,
 abstract = {In this paper, the effect of co-firing semi-coke in a 300 MW tangentially fired boiler was numerically investigated. The results indicate that the incomplete combustion heat loss and NOx emission both increase with semi-coke co-fired ratio. Semi-coke may be injected into the furnace at a different height, which can lead to different thermal efficiency and NOx emission. It is suggested that semi-coke should not be fed from the top or bottom layer burners, since this could give rise to high carbon content respectively in fly ash and bottom slag. In addition, injecting semi-coke from the top burners could significantly increase the NOx emission. Under 1/2 co-firing ratio, the optimal fuel allocation is that feeding semi-coke from the B, D, and E layer burners. The growth in semi-coke particle size could increase the unburned carbon loss and NOx emission. It is highly recommended to reduce the unburned carbon loss under semi-coke co-fired condition by increasing the stoichiometric ratio of primary air for semi-coke. As it is increased from 0.25 to 0.3, the combustion efficiency of the co-fired condition is 99.47%, the same as when only firing bituminous coal, and the NOx emission is about 30% higher.},
 author = {Yongbo Du and Chang’an Wang and Pengqian Wang and Yi Meng and Zhichao Wang and Wei Yao and Defu Che},
 doi = {10.1177/0957650918783923},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0957650918783923},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {2},
 pages = {221–231},
 title = {Computational fluid dynamics investigation on the effect of co-firing semi-coke and bituminous coal in a 300 MW tangentially fired boiler},
 url = {https://doi-org.crai.referencistas.com/10.1177/0957650918783923},
 volume = {233},
 year = {2019g}
}

@article{doi:10.1177/0957650918790671,
 abstract = {Vertical axis wind turbines present several advantages over the horizontal axis machines that make them suitable to a variety of wind conditions. However, due to the complexity of vertical axis wind turbine (VAWT) aerodynamics, available literature on VAWT performance in steady and turbulent wind conditions is limited. This paper aims to numerically predict the performance of a 5 kW VAWT under steady wind conditions through computational fluid dynamics modeling by varying turbine configuration parameters. Two-dimensional VAWT models using a cambered blade (1.5%) were created with open field boundary extents. Turbine configuration parameters studied include blade mounting position, blade fixing angle, and rotor solidity. Baseline case with peak Cp of 0.31 at tip-speed ratio of 4 has the following parameters: mounting position at 0.5c, zero fixing angle, and three blades (solidity = 0.3). Independent parametric studies were carried out and results show that a blade mounting position of 0.7c from the leading edge produces the best performance with maximum Cp = 0.315 while the worst case is a mounting position of 0.15c with peak Cp = 0.273. Fixing angle study reveals a toe-out setting of −1° producing the best performance with peak Cp of 0.315 and the worst setting at toe-in of 1.5° with peak Cp of 0.287. The solidity study resulted in the best case of four blades (solidity = 0.4) with peak Cp = 0.316 and the worst case of two blades (solidity = 0.2) with peak Cp = 0.283.},
 author = {John Keithley Difuntorum and Louis Angelo M Danao},
 doi = {10.1177/0957650918790671},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0957650918790671},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {4},
 pages = {489–509},
 title = {Improving VAWT performance through parametric studies of rotor design configurations using computational fluid dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1177/0957650918790671},
 volume = {233},
 year = {2019f}
}

@article{doi:10.1177/0957650918822949,
 abstract = {This paper presents several approaches for efficient estimation of airfoil response to gust via computational fluid dynamics and reduced-order modeling. A computational fluid dynamics code enabling simulation of aerodynamics under an arbitrary-shaped discrete gust is adopted. Convolution models using baseline sharp-edge gust response either obtained by the closed-form Küssner functions or computational fluid dynamics methods are established. A parametric approximation function model for gust response is identified via the least square optimization of the computational fluid dynamics-obtained sharp-edge responses. Finally, an example taking advantage of the aerodynamic response by the above methods to simulate the aeroelastics of an airfoil performing a plunging-twisting coupled motion under various gusts is presented. The present practice indicates that the reduced-order modelings are not only more efficient compared to direct computational fluid dynamics simulations, but also have a satisfactory accuracy in gust response predictions.},
 author = {Zhenlong Wu and Qiang Wang and Hao Huang},
 doi = {10.1177/0957650918822949},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0957650918822949},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {6},
 pages = {738–750},
 title = {A methodological exploration for efficient prediction of airfoil response to gusts in wind engineering},
 url = {https://doi-org.crai.referencistas.com/10.1177/0957650918822949},
 volume = {233},
 year = {2019s}
}

@article{doi:10.1177/0957650920922074,
 abstract = {Many new turbine designs may take large timelines to prove their worth. For getting duty condition at optimum efficiency, one can always scale speed, diameter, if a very efficient benchmark is available. This paper examines the similarity-based scaling strategy to develop radial inflow turbines for different compressible fluids from a well-established NASA radial flow turbine designed and experimentally tested with air as the working fluid. The NASA 1730 air turbine experimental data have been used as the benchmark here and adopted multiple fluids to understand scaling. The considered fluids are supercritical carbon dioxide for the Brayton cycle, helium for the cryogenic liquefaction cycle, and R143a for the organic Rankine cycle. The uniqueness here is to have three types of cycles, viz. closed-loop Brayton cycle, organic Rankine cycle, and cryogenic helium liquefaction cycle, which employ different working fluids, adapting the same NASA turbine geometry. This paper has described the scaling methodology and presented the simulated turbine performance of SCO2, helium, and R143a using computational fluid dynamics. The dimensionless curves for these fluids are plotted on the corresponding experimental characteristics of the NASA turbine. Out of the three fluids, SCO2 showed the perfect Mach number matching for the flow and torque coefficient curves. The Mach number deviations in the case of helium were small, and the variations were slightly higher for R143a. The efficiencies were the highest for R143a, followed by SCO2 and helium. Thus, the scaling was found to be effective in all cases. Thus, the standard turbomachinery space developed for air as fluid can be used effectively for the development of turboexpanders for various cycles with different working fluids without redesigning the entire shape using similarity-based scaling. The benchmark NASA 1730 turbine has proven this in three special cases. This paper is not against designing new machines but is only trying to say that when such good benchmark machines like NASA 1730 turbine is available; designers must use the power of similitude to adapt it to match new fluids and new conditions.},
 author = {K Vijayaraj and Punit Singh},
 doi = {10.1177/0957650920922074},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0957650920922074},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {2},
 pages = {242–261},
 title = {Scaling a radial flow turbine from air to multiple compressible fluids and performance evaluation using computational fluid dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1177/0957650920922074},
 volume = {235},
 year = {2021s}
}

@article{doi:10.1177/09576509211070118,
 abstract = {The heat pipes in solar applications uses are important and it is significantly growing nowadays. Capable of meeting the world’s challenges with a threat to the climate; a shortage of conventional energy sources, usually fossil fuels, high electricity costs, and it is inexpensive; therefore, the renewable energy of solar energy is an excellent source, reaching total amount of energy is 34,00,000 EJ in each year, all over the world. This is between 7000 and 8000 times the annual global primary energy consumption. Thereby, a Concentrating Solar Parabolic Trough Collector is suggested in this paper, which can be used to analyze the performance of various heat pipes. Using solar parabolic trough collector to analyze the performance of different heat pipes rather than considering the supply of electric power input. Using a parabolic trough collector that concentrates solar energy on an evacuated tube heat pipe, which converts radiation energy into electrical energy heat, the experimental work demonstrates the use of solar energy. Condensers, evaporators, and three heat pipes consisting of aluminum galvanized iron, and stainless steel materials are the main components. A porous wick structure and an ammonia solution-filled working fluid consist of each heat pipe. Continuously recirculating the working fluid through temperature variation. The readings are taken using connected thermocouples on three heat pipes by continuously varying the mass flow rate. The vacuum pressure gauge to maintain the heat pipes generates the vacuum inside space. At various mass flow rates of heated water from the parabolic trough collector, experimentation was also performed. The result of this study calculates the mass flow rate at the end of the process of the different heat pipes used here. Furthermore, different inclinations are estimated in a variety of situations, such as at temperatures of 15°, 30°, 45°, and 60°. In the experimental analysis, outlet temperatures of heat pipes are measured and the temperature distribution contours are evaluated using the analysis of computational fluid dynamics.},
 author = {P Ravindra Kumar and Nelakuditi N Babu and K Lakshmi Prasad},
 doi = {10.1177/09576509211070118},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09576509211070118},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {5},
 pages = {875–892},
 title = {Computational comparison and experimental performance analysis on heat pipes using concentrating solar parabolic trough collector},
 url = {https://doi-org.crai.referencistas.com/10.1177/09576509211070118},
 volume = {236},
 year = {2022k}
}

@article{doi:10.1177/09576509221117936,
 abstract = {Free convection of various nanofluids inside concentric and eccentric annular cavity with internal heat flux, fully filled with porous media is studied numerically and its heat transfer rate and pressure drop are examined. The impact of inner cylinder inclination angles (βecc = 45°, 135°, 225°, and 270°) and nanofluids (Al2O3/water, CuO/water and SiO2/water) for different heat flux (1000, 2000, 3000, and 4000 w/m2) on the heat transfer rate are evaluated. Initially, a validation process has been implemented to ensure that the predicted results obtained from this model are accurate. Results indicate that Al2O3 Nanofluid provides a better enhancement in the heat transfer rate when compared to SiO2 and Cu Nanofluids. Furthermore, the heat transfer coefficient increased with the inclination angle of βecc = 225°, and maximum heat transfer occurred at a heat flux of 4000 W/m2, which was roughly 6% greater than the concentric annular cylinder. The findings also indicated that while increasing the eccentricity has a minor impact on the heat transfer rate for convection-dominated flows (high heat flux), on the other hand, it has a significant impact on the heat transfer rate for conduction-dominated flows (low heat flux).},
 author = {Itimad DJ Azzawi and Ahmed F Hasan and Samir Gh Yahya},
 doi = {10.1177/09576509221117936},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09576509221117936},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {3},
 pages = {517–526},
 title = {Computational optimum design of natural convection in a concentric and eccentric annular cylinder using nanofluids},
 url = {https://doi-org.crai.referencistas.com/10.1177/09576509221117936},
 volume = {237},
 year = {2023b}
}

@article{doi:10.1177/09576509221132939,
 abstract = {CFD (Computational fluid dynamics) is a tool, through which, the entire combustion process for pulverized coal combustion (PCC) can be predicted. The solid fuel combustion modeling is not as simple as gaseous or liquid fuel combustion modeling. In the solid fuel combustion process, various physical transformations and chemical reactions occur simultaneously. Therefore in the computational modeling approach for PCC, basic physical and chemical processes such as; turbulence, radiation, devolatilization, gas-phase combustion, char reaction and soot formation/reduction are integrated to obtain the desired results. However, in order to reach the desired accuracy, different computational models should be chosen carefully. In this review, various common models that are widely used in PCC simulation are discussed scientifically along with their modeling aspects. The basic advantages and disadvantages of these models are also discussed in this study. Moreover, different models preferred by various authors for pulverized coal combustion simulation process are also summarized. From the review, it has been observed that the eddy viscosity based two-equation turbulent models and Eulerian-Lagrangian multiphase approach are mostly preferred due to better computational economy. Two competing rate devolatilization model from various finite rate models and FG-DVC devolatilization model from various phenomenological models are mostly used for PCC. Diffusion kinetic control and Intrinsic char combustion models are mostly chosen to predict the char reaction. Finite reaction rate models are preferred when the computational economy is a concern, whereas phenomenological models provide results with better accuracy because in phenomenological models realistic inputs are given as input for simulation whereas, in finite rate models, finite rate constants are mostly empirical. Most of the gas phase models are given almost equal importance. Discrete ordinate (DO) is mostly used for PCC due to its ability of volumetric radiation calculation and the ability to evaluate radiation effect on the particle phase.},
 author = {Prakash Ghose and Tarak Kumar Sahoo and Ajay K Sahu},
 doi = {10.1177/09576509221132939},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09576509221132939},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {4},
 pages = {797–818},
 title = {Pulverized coal combustion computational modeling approach: A review},
 url = {https://doi-org.crai.referencistas.com/10.1177/09576509221132939},
 volume = {237},
 year = {2023i}
}

@article{doi:10.1177/09576509231197881,
 abstract = {Bifacial photovoltaic cells can produce electricity from incoming solar radiation on both sides. These cells have a strong potential to reduce electricity generation costs and may play an important role in the energy system of the future. However, today, these cells are mostly deployed with one side receiving only ground reflection, which leads to a profound sub-optimal utilization of one of the sides of the bifacial cells. Concentration allows a better usage of the potential of bifacial cells, which can lead to a lower cost per kWh. However, concentration also adds complexity due to the higher temperatures reached which add the requirement of cooling in order to achieve higher outputs. This way, this paper focuses on the effectiveness of forced air circulation methods by comparing the thermal performance of three specific concentrating bi-facial collector designs. This paper developed a computational model, using ANSYS Fluent intending to assess the thermal performance of a covered concentrating collector with bifacial Photovoltaic (PV) cells. These results have then been validated by outdoor measurements. Results show that even a simple natural ventilation mechanism such as removing the side gable can effectively reduce the receiver temperature, thus resulting in favourable cell operation conditions when compared to the case of an airtight collector. Therefore, compared with a standard model, a decrease of 13.5% on the cell operating temperature was reported when the side gables are removed. However, when forced ventilation is apllied a 22.8% reduction on temperature is found compared to the standard air-tight model. The validated CFD model has proven to be a useful and robust tool for the thermal analysis of solar concentrating systems.},
 author = {Miguel Lança and João Gomes and Diogo Cabral},
 doi = {10.1177/09576509231197881},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09576509231197881},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {1},
 pages = {140–156},
 title = {Thermal performance of three concentrating collectors with bifacial photovoltaic cells part I – Experimental and computational fluid dynamics study},
 url = {https://doi-org.crai.referencistas.com/10.1177/09576509231197881},
 volume = {238},
 year = {2024f}
}

@article{doi:10.1177/09576509241240012,
 abstract = {The objective of the paper is to investigate the internal flow characteristics inside the inducer under the gas-water condition using a combination of experiment and numerical simulations. Gas-water mixing is achieved by an annular gas mixing device and visualized by high-speed imaging technology. Meanwhile, the shear stress transport (SST) k-ω model coupled with the VOF model is applied to investigate the gas phase distribution. Numerically simulated inducer energy characteristics and internal gas-phase distribution are in good agreement with experimental data. The results show that the mixed gas-liquid flow inside the inducer exhibits three flow patterns: wavy flow, bubbly flow, and plug flow. For the wavy flow, a series of wavy structures on the blade surface and gas-pocket bubbles near the blade rim are observed. From wavy flow to bubbly flow, uniform bubble streams on the blade surface and spiraling strip bubbles near the edge of the blade are observed. For plug flow, bubble accumulation results in a bullet-shaped plug, and the strip-shaped bubbles near the blade rim are more obvious. In the same axial position of the inducer, the proportion of gas phase is increasing as the flow coefficient increases, thus affecting the inducer performance. The main frequency amplitude of the pressure pulsations gradually rise when the flow pattern changes from wavy to plug flow.},
 author = {Jie Chen and Yong Wang and Houlin Liu and Lilin Lv and Yanhong Mao and Linglin Jiang},
 doi = {10.1177/09576509241240012},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09576509241240012},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {5},
 pages = {910–921},
 title = {A combined experimental and computational investigation of an inducer’s characteristics with gas-water two-phase flow},
 url = {https://doi-org.crai.referencistas.com/10.1177/09576509241240012},
 volume = {238},
 year = {2024f}
}

@article{doi:10.1177/09579265241231593,
 abstract = {Massive anti-government protests erupted during the COVID-19 pandemic in Germany. The crisis activated a potential for resistance that has been simmering under the impositions of late-modern knowledge society. Made salient by the pandemic conditions of sudden extreme reliance on scientific (non) knowledge, the corona protestors activated this potential for resistance and constructed their own counter-knowledge order bound by shared resentment of and distrust in the established order and facilitated by digital platforms. Utilising social network analysis and structural topic modeling for digital critical discourse analysis, in this paper I explore how the corona protest counter-knowledge order is constructed with a particular focus on its contexts, roles, and hierarchies. I find that far-right and conspiracy imaginations are used to level out hierarchies and detach epistemic roles from their contexts to reinstate a superior self into interpretative power. The counter-knowledge order’s inherent construction of unwarranted omnipotence points to a more fundamental resistance to the established normative orders of our society that should be addressed more effectively if we want to be prepared for future crises and not lose common ground for making sense of them.},
 author = {Florian Primig},
 doi = {10.1177/09579265241231593},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09579265241231593},
 journal = {Discourse & Society},
 number = {4},
 pages = {481–498},
 title = {Thinking different as an act of resistance: Reconceptualizing the German protests in the COVID-19 pandemic as an emergent counter-knowledge order},
 url = {https://doi-org.crai.referencistas.com/10.1177/09579265241231593},
 volume = {35},
 year = {2024o}
}

@article{doi:10.1177/0959354303136005,
 abstract = {The debate over symbolic versus sub-symbolic representations of human cognition has been continuing for thirty years, with little indication of a resolution. The argument is this: Does the human cognitive system use symbols as a representation of knowledge, and does it process symbols and their respective constituents? Or does the human cognitive system use a distributed representation of knowledge, and is it somehow capable of processing this distributed representation of knowledge in a complex and meaningful way? This paper argues for an integrated symbolic and sub-symbolic approach to the representation of cognition. The lines of reasoning used as evidence to bolster this argument for an integrated approach are the cognitive architecture the Adaptive Character of Thought-Rational (ACT-R), and biology, where it is argued that symbolic and sub-symbolic representations of cognition are part of an intellectual continuum, with sub-symbolic representations at the low end and symbolic representations at the higher end.},
 author = {Troy D. Kelley},
 doi = {10.1177/0959354303136005},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0959354303136005},
 journal = {Theory & Psychology},
 number = {6},
 pages = {847–860},
 title = {Symbolic and Sub-Symbolic Representations in Computational Models of Human Cognition: What Can be Learned from Biology?},
 url = {https://doi-org.crai.referencistas.com/10.1177/0959354303136005},
 volume = {13},
 year = {2003i}
}

@article{doi:10.1177/0959354307081619,
 abstract = {This paper introduces the compost heap as a metaphor for autobiographical memory. As an alternative to the computer, such a metaphor, it is argued, comes closer to capturing the dynamics of memory across the lifespan and how it feels to us as we age, particularly memory’s narrative dimensions. After citing concerns expressed by psychologists and others regarding computationalism, the paper considers four entailments of the compost heap analogy that may serve, very roughly, as counterparts to such concepts as encoding, storage, and retrieval. They are: laying it on, breaking it down, stirring it up, and mixing it in. The paper concludes with reflections on the advantages of a more organic model of memory and some suggestions for further inquiry concerning issues of interest to the psychology of aging.},
 author = {William L. Randall},
 doi = {10.1177/0959354307081619},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0959354307081619},
 journal = {Theory & Psychology},
 number = {5},
 pages = {611–633},
 title = {From Computer to Compost: Rethinking Our Metaphors for Memory},
 url = {https://doi-org.crai.referencistas.com/10.1177/0959354307081619},
 volume = {17},
 year = {2007p}
}

@article{doi:10.1177/0959354317722867,
 abstract = {Two options fuel the debate on the cognitive processes underlying the perception of affordances. On the one hand, the ecological theory of affordance fits with the methodological assumptions of the dynamical systems theory of cognition. On the other hand, it is nowadays common to conceive the perception of affordances within a computational framework. This article defends the explanatory power of a computational approach and aims to extend the concept of affordance beyond the boundaries of the dynamical systems theory of cognition. For that purpose, I consider the case of patients suffering from optic ataxia, a condition in which some aspects of visual guidance over reaching with the hand are lost following a lesion in the left parietal cortex. Etiological considerations, indeed, reveal that a computational approach to the perception of affordances allows for an explanation of ataxic behavior that is not available to the dynamical systems theory.},
 author = {Silvano Zipoli Caiani},
 doi = {10.1177/0959354317722867},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0959354317722867},
 journal = {Theory & Psychology},
 number = {5},
 pages = {663–682},
 title = {When the affordances disappear: Dynamical and computational explanations of optic ataxia},
 url = {https://doi-org.crai.referencistas.com/10.1177/0959354317722867},
 volume = {27},
 year = {2017t}
}

@article{doi:10.1177/0959354319867392,
 abstract = {We compare three theoretical frameworks for pursuing explanatory integration in psychiatry: a new dimensional framework grounded in the notion of computational phenotype, a mechanistic framework, and a network of symptoms framework. Considering the phenomenon of alcoholism, we argue that the dimensional framework is the best for effectively integrating computational and mechanistic explanations with phenomenological analyses.},
 author = {Matteo Colombo and Andreas Heinz},
 doi = {10.1177/0959354319867392},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0959354319867392},
 journal = {Theory & Psychology},
 number = {5},
 pages = {697–718},
 title = {Explanatory integration, computational phenotypes, and dimensional psychiatry: The case of alcohol use disorder},
 url = {https://doi-org.crai.referencistas.com/10.1177/0959354319867392},
 volume = {29},
 year = {2019g}
}

@article{doi:10.1177/095965180321700207,
 abstract = {Abstract This paper contains a summary of research undertaken at QinetiQ (Winfrith) to simulate velocity-dependent ‘stick/slip’ or ‘sliding’ friction for use in the modelling of complex engineering systems. The research arises from combined simulation and experimental investigations conducted into the performance optimization of towed array outboard handling systems on submarines and surface ships. However, the technique developed is general and suitable for any time-domain system simulation, entailing the solution of a set of non-linear differential equations. The actual friction-velocity relationship may be either equation based or determined from experimental data as a set of x-y values. The simulation technique is novel in that it avoids the traditional problem of a friction discontinuity as the surface relative velocity changes sign. It thereby overcomes the computational problems that often occur when multiple discontinuities are encountered, such as simulations stopping prematurely, failing or giving erroneous results.},
 author = {S P Tomlinson and R W Cooke and D C Cosserat},
 doi = {10.1177/095965180321700207},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/095965180321700207},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part I: Journal of Systems and Control Engineering},
 number = {2},
 pages = {139–146},
 title = {A computationally efficient technique for modelling velocity-dependent sliding friction},
 url = {https://doi-org.crai.referencistas.com/10.1177/095965180321700207},
 volume = {217},
 year = {2003r}
}

@article{doi:10.1177/095965180421800705,
 abstract = {Abstract The parameter identification of magnetorheological dampers by an inverse method is proposed. A modified Bouc-Wen modified dynamic model is considered and its parameters are obtained by using genetic algorithms. The experimental data consist of time histories of current, displacement, velocity, and force measured for both constant and variable current. The model parameters are determined using a set of experimental measurements corresponding to different constant current values and the resulting model is validated on the data measured for variable current. Based on this model a semi-active control of vehicle suspension is studied and a fuzzy controller is developed to reduce the chattering effect.},
 author = {M Giuclea and T Sireteanu and D Stancioiu and C W Stammers},
 doi = {10.1177/095965180421800705},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/095965180421800705},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part I: Journal of Systems and Control Engineering},
 number = {7},
 pages = {569–581},
 title = {Model parameter identification for vehicle vibration control with magnetorheological dampers using computational intelligence methods},
 url = {https://doi-org.crai.referencistas.com/10.1177/095965180421800705},
 volume = {218},
 year = {2004h}
}

@article{doi:10.1177/0959651813520147,
 abstract = {Most of the process plants have intrinsic non-linear, time-varying, non-minimum phase and coupling characteristics that result in a highly time-consuming and tedious task to derive complicated non-linear dynamic equations governing on such types of processes. On the other hand, even though such equations can be analytically extracted, they have not worked out in most cases yet. To find a simple fix that can address these hurdles associated with analytical modelling, two types of non-analytical models, namely, simulator and predictor, are proposed based on a computationally efficient technique so-called locally linear neuro-fuzzy modelling. That is, a simulator model as well as a specially structured long-term predictor model that is built based on a sequential arrangement of single-stage predictor models is presented for multi-input multi-output non-linear dynamical process plants and feasibly applied to a real water-tube steam generator for the first time. An adaptive evolving algorithm named linear model tree contributes to estimate the parameters of neuro-fuzzy simulator and predictor models. Furthermore, an order selection method based on Lipschitz theory, whose merit is being totally independent from developing any model prior to commencing tedious modelling trials, is also proposed for the first time by defining a modified Lipschitz index to remedy the order determination problem within the very first steps of black-box non-linear system identification. The blend of such a fully modelling-independent order selection algorithm and a unique type of computationally inexpensive non-linear neuro-fuzzy model lessens the overall computational burden of modelling procedure that represents a great concern in a black-box system identification approach. The recorded data from a real water-tube steam generator operating at Abbot Power plant unit of Champaign, IL, were exploited to carry out experimental modelling in order to reveal the pros and cons of the presented models.},
 author = {Hasan A Nozari and Hamed D Banadaki},
 doi = {10.1177/0959651813520147},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0959651813520147},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part I: Journal of Systems and Control Engineering},
 number = {5},
 pages = {278–294},
 title = {Intelligent computationally efficient modelling of multi-input multi-output non-linear dynamical process plants: An industrial steam generator case study},
 url = {https://doi-org.crai.referencistas.com/10.1177/0959651813520147},
 volume = {228},
 year = {2014q}
}

@article{doi:10.1177/0960327115605440,
 abstract = {Predictive toxicology plays a critical role in reducing the failure rate of new drugs in pharmaceutical research and development. Despite recent gains in our understanding of drug-induced toxicity, however, it is urgent that the utility and limitations of our current predictive tools be determined in order to identify gaps in our understanding of mechanistic and chemical toxicology. Using recently published computational regression analyses of in vitro and in vivo toxicology data, it will be demonstrated that significant gaps remain in early safety screening paradigms. More strategic analyses of these data sets will allow for a better understanding of their domain of applicability and help identify those compounds that cause significant in vivo toxicity but which are currently mis-predicted by in silico and in vitro models. These ‘outliers’ and falsely predicted compounds are metaphorical lighthouses that shine light on existing toxicological knowledge gaps, and it is essential that these compounds are investigated if attrition is to be reduced significantly in the future. As such, the modern computational toxicologist is more productively engaged in understanding these gaps and driving investigative toxicology towards addressing them.},
 author = {RT Naven and S Louise-May},
 doi = {10.1177/0960327115605440},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0960327115605440},
 journal = {Human & Experimental Toxicology},
 note = {PMID:26614820},
 number = {12},
 pages = {1304–1309},
 title = {Computational toxicology: Its essential role in reducing drug attrition},
 url = {https://doi-org.crai.referencistas.com/10.1177/0960327115605440},
 volume = {34},
 year = {2015k}
}

@article{doi:10.1177/0960336020943987,
 abstract = {First of all, I want to transmit my most humble thanks to all people who believe that I deserve the “2019 Thomas Hirschfeld” award (kindly supported by FOSS) for my work on near-infrared spectroscopy and, especially, applied on hyperspectral images. I must confess that this award caught me by surprise and that I felt a bit overwhelmed when I received it. It is an honour full of respect and responsibility. I have been given the opportunity of writing this article, and I will profit it to express different personal thoughts about general but relevant aspects of near infrared applied to hyperspectral imaging. Also, since I am more a practitioner in chemometrics (or machine learning or data mining, or …) than a developer, I will also include some insights about the beautiful combination of near-infrared hyperspectral image with chemometrics. This article is just a glimpse of constructive criticism with personal thoughts that comes from my little experience in this field. Therefore, and of course, all opinions here are open for constructive discussion with the only purpose of learning (like the machines do nowadays).},
 author = {José Manuel Amigo},
 doi = {10.1177/0960336020943987},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0960336020943987},
 journal = {NIR news},
 number = {5–6},
 pages = {8–14},
 title = {Near-infrared hyperspectral image at a glance: Some personal thoughts},
 url = {https://doi-org.crai.referencistas.com/10.1177/0960336020943987},
 volume = {31},
 year = {2020b}
}

@article{doi:10.1177/0961000616668572,
 abstract = {This study proposes an objective methodology for identifying and computing the factors relevant to the assessment of information security risks for digital libraries that is also compliant with the ISO 27000 and the GB/T 20984 standards. By introducing a fuzzy comprehensive assessment method and an expert investigation method to the dimensions of assets and threats, this study proposes a model for computing the value of assets and the severity of threats. In the dimension of vulnerabilities, a vulnerability computation model based on the multi-channel weighted average method is proposed. By considering the digital library of a typical public library in China as the object of assessment, this study acquires assessment data by using a combination of a questionnaire survey, an on-site survey and vulnerability scanning. Research findings consisted of the following: (1) the digital library identified a total of 3111 information security risk items; (2) according to the assessment results attained using a combination of the factor identification and computational methodologies proposed here in conjunction with the multiplicative method specified in GB/T 20984, the high-risk (or higher risk) items accounted for 0.9% of all risky items, which is consistent with the status quo in information security risks faced by digital libraries. The analysis showed that the proposed methodology is more scientific than the currently prevailing direct value assignment method.},
 author = {Shuiqing Huang and Zhengbiao Han and Bo Yang and Ni Ren},
 doi = {10.1177/0961000616668572},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0961000616668572},
 journal = {Journal of Librarianship and Information Science},
 number = {1},
 pages = {78–94},
 title = {Factor identification and computation in the assessment of information security risks for digital libraries},
 url = {https://doi-org.crai.referencistas.com/10.1177/0961000616668572},
 volume = {51},
 year = {2019k}
}

@article{doi:10.1177/0961000620948568,
 abstract = {Recognizing individual needs and customer preferences is key to succeeding and increasing competitiveness in both the commercial and public sectors. In the public sector, this is one of the ways to increase the efficiency of public funds allocated to public libraries. However, in order to make the right decisions, library management needs quality information about preferences and consumer behaviour, even from customers who cannot be the subject of routine research due to their young age. Therefore, this article proposes a new methodology for surveying children in order to design library services. The proposed methodology, which integrates principles from ethnographic and sociological methods, aims to overcome the problems of conventional sociological methods, such as how to conceive a child as a respondent and how to determine their preferences from hypothetical situations. This article uses a specially designed questionnaire tool to identify preferences, behaviour and information from children (as visitors of a library). The authors learned that Czech children went to the library primarily for books and games. In addition, the data from the questionnaire revealed patterns in the children’s and adults’ behaviour when visiting the library. All of the data in this article can be used for further research into consumer behaviour.},
 author = {Jan Stejskal and Petr Hajek and Pavel Cerny},
 doi = {10.1177/0961000620948568},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0961000620948568},
 journal = {Journal of Librarianship and Information Science},
 number = {2},
 pages = {307–320},
 title = {A novel methodology for surveying children for designing library services: A case study of the Municipal Library of Prague},
 url = {https://doi-org.crai.referencistas.com/10.1177/0961000620948568},
 volume = {53},
 year = {2021o}
}

@article{doi:10.1177/09610006221084126,
 abstract = {This study examines the current state of assessment of computational thinking (CT) programming in public libraries in the United States. In particular, this study identifies the assessment tools and strategies that public library staff use to evaluate the success of CT youth programming, as well as how they share these assessment results, what they share, and with whom. This work also examines the perceptions of library staff on assessment of CT learning in libraries. Through our work, we highlight the need for a change of mindset in the perception of library staff toward assessment of CT learning in libraries. We also demonstrate the need for suitable assessment strategies to measure learning in CT programming in libraries beyond attendance and retention, that communicate to library staff on how they can revise their programs and to share their program impact with library stakeholders who make decisions on budget and resource allocations.},
 author = {Mega Subramaniam and Nitzan Koren and Shandra Morehouse and David Weintrop},
 doi = {10.1177/09610006221084126},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09610006221084126},
 journal = {Journal of Librarianship and Information Science},
 number = {2},
 pages = {358–370},
 title = {Capturing computational thinking in public libraries: An examination of assessment strategies, audience, and mindset},
 url = {https://doi-org.crai.referencistas.com/10.1177/09610006221084126},
 volume = {55},
 year = {2023r}
}

@article{doi:10.1177/096228020601500513,
 author = {Berwin A. Turlach},
 doi = {10.1177/096228020601500513},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/096228020601500513},
 journal = {Statistical Methods in Medical Research},
 number = {5},
 pages = {521–523},
 title = {Book Review: Computational statistics},
 url = {https://doi-org.crai.referencistas.com/10.1177/096228020601500513},
 volume = {15},
 year = {2006s}
}

@article{doi:10.1177/09622802231167436,
 abstract = {In medical statistics, when the effect of a binary risk factor on a binary response is of interest, relative risk is often the preferred measure due to its direct interpretation. However, statistical inference on this quantity is not as straightforward as for other measures of association, especially when further explanatory variables have to be taken into account. Starting from a review of available methods for inference on relative risk, this paper deals with small and moderate sample size settings for which we show that classical approaches can be problematic. For this reason, we propose the use of improved estimation procedures, aiming at mean or median bias reduction of the maximum likelihood estimator. In particular, these methods are developed for a new alternative specification of a model recently proposed by Richardson et al, where higher computational stability of the estimation methods is achieved. A real-data example and extensive simulation studies show that the proposed methods perform remarkably better than the standard ones.},
 author = {Francesco Pozza and Euloge Clovis Kenne Pagui and Alessandra Salvan},
 doi = {10.1177/09622802231167436},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09622802231167436},
 journal = {Statistical Methods in Medical Research},
 note = {PMID:37032617},
 number = {6},
 pages = {1234–1246},
 title = {Improved and computationally stable estimation of relative risk regression with one binary exposure},
 url = {https://doi-org.crai.referencistas.com/10.1177/09622802231167436},
 volume = {32},
 year = {2023m}
}

@article{doi:10.1177/096228029400300103,
 abstract = {This article is a case-based review that introduces applied statisticians to a number of issues and methods that arise in clinical studies with paired digital images as outcomes. Single photon emission computed tomography (SPECT) is the imaging modality used in two examples. The first is a physical simulation of relevant clinical features of SPECT images using a customized head phantom scanned under different experimental conditions. The objective is to demonstrate and compare several current methods for image registration, i.e., image superimposition in some optimal manner to obtain a common frame of reference within which to make pixel-by-pixel comparisons. Image registration together with image normalization to correct for spurious differences in background activity levels enable quantification of differences in paired images. The physical simulation assesses quantification accuracy. The second example involves two SPECT images of the same brain tumour patient taken about two months apart. The objective here is to demonstrate several tools from morphological image analysis for image segmentation, i.e., separation of “figure” and “ground”, and simple image regression to detect patterns of differences in time, again after registration and normalization. The clinical context of both examples helps to keep the review in practical focus and is a useful starting point for descriptions of many standard tools in applied medical image analysis. References to relevant literature are provided for readers wanting to study particular subjects in depth. As medical image analysis becomes more sophisticated, more cost-conscious and scrutinized more intensively, proper attention to the computational and statistical bases for practical conclusions become even more crucial.},
 author = {Nicholas Lange},
 doi = {10.1177/096228029400300103},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/096228029400300103},
 journal = {Statistical Methods in Medical Research},
 note = {PMID:8044351},
 number = {1},
 pages = {23–40},
 title = {Some computational and statistical tools for paired comparisons of digital images},
 url = {https://doi-org.crai.referencistas.com/10.1177/096228029400300103},
 volume = {3},
 year = {1994g}
}

@article{doi:10.1177/096228029700600305,
 abstract = {This paper reviews recent developments by the Washington/Brown groups for the study of anatomical shape in the emerging new discipline of computational anatomy. Parametric representations of anatomical variation for computational anatomy are reviewed, restricted to the assumption of small deformations. The generation of covariance operators for probabilistic measures of anatomical variation on coordinatized submanifolds is formulated as an empirical procedure. Populations of brains are mapped to common coordinate systems, from which template coordinate systems are constructed which are closest to the population of anatomies in a minimum distance sense. Variation of several one-, two and three-dimensional manifolds, i.e. sulci, surfaces and brain volumes are examined via Gaussian measures with mean and covariances estimated directly from maps of templates to targets. Methods are presented for estimating the covariances of vector fields from a family of empirically generated maps, posed as generalized spectrum estimation indexed over the submanifolds. Covariance estimation is made parametric, analogous to autoregressive modelling, by introducing small deformation linear operators for constraining the spectrum of the fields.},
 author = {Michael Miller and Ayananshu Banerjee and Gary Christensen and Sarang Joshi and Navin Khaneja and Ulf Grenander and Larissa Matejic},
 doi = {10.1177/096228029700600305},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/096228029700600305},
 journal = {Statistical Methods in Medical Research},
 note = {PMID:9339500},
 number = {3},
 pages = {267–299},
 title = {Statistical methods in computational anatomy},
 url = {https://doi-org.crai.referencistas.com/10.1177/096228029700600305},
 volume = {6},
 year = {1997n}
}

@article{doi:10.1177/09636625221137815,
 abstract = {Drawing from network theory and previous findings from US-based analyses, we measure the structure and interconnectedness of climate contrarian think tanks in Europe. This exploratory analysis can illustrate European organizations’ capacity to promote or disrupt political discourse. To this end, we use social network analysis to conduct actor-focused research. We identify the individuals bridging European think tanks, as well as their ties with the US climate change contrarian network. Our analysis reveals a discernible network structure for European climate change contrarian think tanks, with a profile connected to neoliberal organizations, including a few, but highly relevant links, with the US countermovement. We also find that the European think tanks’ institutional structure is very much shaped by a strong predominance of men, which aligns with previous research on masculinity and climate contrarianism.},
 author = {Núria Almiron and Jose A. Moreno and Justin Farrell},
 doi = {10.1177/09636625221137815},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09636625221137815},
 journal = {Public Understanding of Science},
 note = {PMID:36519411},
 number = {3},
 pages = {268–283},
 title = {Climate change contrarian think tanks in Europe: A network analysis},
 url = {https://doi-org.crai.referencistas.com/10.1177/09636625221137815},
 volume = {32},
 year = {2023a}
}

@article{doi:10.1177/09636625241246076,
 abstract = {Numerous studies have been conducted to identify the factors that predict trust/distrust in science. However, most of these studies are based on closed-ended survey research, which does not allow researchers to gain a more nuanced understanding of the phenomenon. This study integrated survey analysis conducted within the United States with computational text analysis to reveal factors previously obscured by traditional survey methodologies. Even after controlling for political ideology—which has been the most significant explanatory factor in determining trust in science within a survey framework—we found those with concerns over boundary-crossing (i.e. concerns or perceptions that science overlaps with politics, the government, and funding) were less likely to trust science than their counterparts.},
 author = {Sangwon Lee and Marshall A. Taylor and Saifuddin Ahmed and Won-Ki Moon},
 doi = {10.1177/09636625241246076},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09636625241246076},
 journal = {Public Understanding of Science},
 note = {PMID:38659212},
 number = {0},
 pages = {09636625241246076},
 title = {Going beyond political ideology: A computational analysis of civic trust in science},
 url = {https://doi-org.crai.referencistas.com/10.1177/09636625241246076},
 volume = {0},
 year = {2024i}
}

@article{doi:10.1177/0963721410386677,
 abstract = {Scientists can reason about natural systems, including the mind and brain, in many ways, with each form of reasoning being associated with its own set of limitations. The limitations on human reasoning imply that the process of reasoning about theories and communicating those theories will be error prone; we must therefore be concerned about the reproducibility of theories whose very nature is shaped by constraints on human reasoning. The problem of reproducibility can be alleviated by computational modeling, which maximizes correspondence between the actual behavior of a posited system and its behavior inferred through reasoning and increases the fidelity of communication of our theories to others.},
 author = {Simon Farrell and Stephan Lewandowsky},
 doi = {10.1177/0963721410386677},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0963721410386677},
 journal = {Current Directions in Psychological Science},
 number = {5},
 pages = {329–335},
 title = {Computational Models as Aids to Better Reasoning in Psychology},
 url = {https://doi-org.crai.referencistas.com/10.1177/0963721410386677},
 volume = {19},
 year = {2010e}
}

@article{doi:10.1177/0963721412470685,
 abstract = {Cognitive control refers to the processes by which individual cognitive functions are coordinated in the service of higher-level goals. The anterior cingulate cortex (ACC) in the middle front of the brain monitors performance, and it is activated when the need for control increases—for instance, in difficult situations or when errors occur. Since the late 1990s, the ACC has been thought to signal when there is conflict between competing action plans so that the conflict can be resolved. More recently, an alternative model has reconceptualized the role of ACC as that of a predictor and evaluator of the likely outcomes of planned actions before the actions are performed. This new Predicted Response Outcome model accounts for a broader range of findings than the earlier framework and suggests that the ACC might support the cognitive operations by which individuals can “think before they act” in order to avoid risky or otherwise poor choices.},
 author = {Joshua W. Brown},
 doi = {10.1177/0963721412470685},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0963721412470685},
 journal = {Current Directions in Psychological Science},
 note = {PMID:25360064},
 number = {3},
 pages = {179–185},
 title = {Beyond Conflict Monitoring: Cognitive Control and the Neural Basis of Thinking Before You Act},
 url = {https://doi-org.crai.referencistas.com/10.1177/0963721412470685},
 volume = {22},
 year = {2013c}
}

@article{doi:10.1177/0963721415604610,
 abstract = {We review recent evidence revealing that the mere willingness to engage analytic reasoning as a means to override intuitive gut feelings is a meaningful predictor of key psychological outcomes in diverse areas of everyday life. For example, those with a more analytic thinking style are more skeptical about religious, paranormal, and conspiratorial concepts. In addition, analytic thinking relates to having less traditional moral values, making less emotional or disgust-based moral judgments, and being less cooperative and more rationally self-interested in social dilemmas. Analytic thinkers are even less likely to offload thinking to smartphone technology and may be more creative. Taken together, these results indicate that the propensity to think analytically has major consequences for individual psychology.},
 author = {Gordon Pennycook and Jonathan A. Fugelsang and Derek J. Koehler},
 doi = {10.1177/0963721415604610},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0963721415604610},
 journal = {Current Directions in Psychological Science},
 number = {6},
 pages = {425–432},
 title = {Everyday Consequences of Analytic Thinking},
 url = {https://doi-org.crai.referencistas.com/10.1177/0963721415604610},
 volume = {24},
 year = {2015m}
}

@article{doi:10.1177/0963721415609581,
 abstract = {We argue for the advantages of the probabilistic language of thought (pLOT), a recently emerging approach to modeling human cognition. Work using this framework demonstrates how the pLOT (a) refines the debate between symbols and statistics in cognitive modeling, (b) permits theories that draw on insights from both nativist and empiricist approaches, (c) explains the origins of novel and complex computational concepts, and (d) provides a framework for abstraction that can link sensation and conception. In each of these areas, the pLOT provides a productive middle ground between historical divides in cognitive psychology, pointing to a promising way forward for the field.},
 author = {Steven T. Piantadosi and Robert A. Jacobs},
 doi = {10.1177/0963721415609581},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0963721415609581},
 journal = {Current Directions in Psychological Science},
 number = {1},
 pages = {54–59},
 title = {Four Problems Solved by the Probabilistic Language of Thought},
 url = {https://doi-org.crai.referencistas.com/10.1177/0963721415609581},
 volume = {25},
 year = {2016m}
}

@article{doi:10.1177/0963721415618485,
 abstract = {Human reasoning and creativity represent perhaps the two highest evolutionary reaches of cognition. These two capacities are distinct from each other, but research on creativity in analogical reasoning has identified a point of convergence between them at one of the farthest forward and most recently evolved reaches of the brain. Analogy is a form of relational cognition because analogies form connections that relate otherwise separate concepts. Quantitative tools for measuring the semantic distance between concepts have advanced the measurement of creativity in relational cognition (more creative relational cognition forms connections across greater semantic distance). These tools are especially useful for the emerging neuroscience of creativity. I describe this semantic-distance approach and how it is being leveraged in my laboratory and elsewhere to investigate not only differences in creative ability between individuals but also creativity as a dynamic state that varies across time within an individual.},
 author = {Adam E. Green},
 doi = {10.1177/0963721415618485},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0963721415618485},
 journal = {Current Directions in Psychological Science},
 number = {1},
 pages = {28–35},
 title = {Creativity, Within Reason: Semantic Distance and Dynamic State Creativity in Relational Thinking and Reasoning},
 url = {https://doi-org.crai.referencistas.com/10.1177/0963721415618485},
 volume = {25},
 year = {2016e}
}

@article{doi:10.1177/0963721418818441,
 abstract = {Computational approaches to understanding the algorithms of the mind are just beginning to pervade the field of clinical psychology. In the present article, we seek to explain in simple terms why this approach is indispensable to pursuing explanations of psychological phenomena broadly, and we review nascent efforts to use this lens to understand anxiety. We conclude with future directions that will be required to advance algorithmic accounts of anxiety. Ultimately, the surplus explanatory value of computational models of anxiety, above and beyond existing neurobiological models of anxiety, impugns the naively reductionist claim that neurobiological models are sufficient to explain anxiety.},
 author = {Paul B. Sharp and Eran Eldar},
 doi = {10.1177/0963721418818441},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0963721418818441},
 journal = {Current Directions in Psychological Science},
 number = {2},
 pages = {170–176},
 title = {Computational Models of Anxiety: Nascent Efforts and Future Directions},
 url = {https://doi-org.crai.referencistas.com/10.1177/0963721418818441},
 volume = {28},
 year = {2019r}
}

@article{doi:10.1177/0963721419834547,
 abstract = {Computational cognitive models typically focus on individual behavior in isolation. Models frequently employ closed-form solutions in which a state of the system can be computed if all parameters and functions are known. However, closed-form models are challenged when used to predict behaviors for dynamic, adaptive, and heterogeneous agents. Such systems are complex and typically cannot be predicted or explained by analytical solutions without application of significant simplifications. In addressing this problem, cognitive and social psychological sciences may profitably use agent-based models, which are widely employed to simulate complex systems. We show that these models can be used to explore how cognitive models scale in social networks to calibrate model parameters, to validate model predictions, and to engender model development. Agent-based models allow for controlled experiments of complex systems and can explore how changes in low-level parameters impact the behavior at a whole-system level. They can test predictions of cognitive models and may function as a bridge between individually and socially oriented models.},
 author = {Jens Koed Madsen and Richard Bailey and Ernesto Carrella and Philipp Koralus},
 doi = {10.1177/0963721419834547},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0963721419834547},
 journal = {Current Directions in Psychological Science},
 number = {3},
 pages = {299–305},
 title = {Analytic Versus Computational Cognitive Models: Agent-Based Modeling as a Tool in Cognitive Sciences},
 url = {https://doi-org.crai.referencistas.com/10.1177/0963721419834547},
 volume = {28},
 year = {2019m}
}

@article{doi:10.1177/0963721419887361,
 abstract = {Heightened risk taking in adolescence has long been attributed to valuation systems overwhelming the deployment of cognitive control. However, this explanation of why adolescents engage in risk taking is insufficient given increasing evidence that risk-taking behavior can be strategic and involve elevated cognitive control. We argue that applying the expected-value-of-control computational model to adolescent risk taking can clarify under what conditions control is elevated or diminished during risky decision-making. Through this lens, we review research examining when adolescent risk taking might be due to—rather than a failure of—effective cognitive control and suggest compelling ways to test such hypotheses. This effort can resolve when risk taking arises from an immaturity of the control system itself, as opposed to arising from differences in what adolescents value relative to adults. It can also identify promising avenues for channeling cognitive control toward adaptive outcomes in adolescence.},
 author = {Kathy T. Do and Paul B. Sharp and Eva H. Telzer},
 doi = {10.1177/0963721419887361},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0963721419887361},
 journal = {Current Directions in Psychological Science},
 number = {1},
 pages = {102–109},
 title = {Modernizing Conceptions of Valuation and Cognitive-Control Deployment in Adolescent Risk Taking},
 url = {https://doi-org.crai.referencistas.com/10.1177/0963721419887361},
 volume = {29},
 year = {2020c}
}

@article{doi:10.1177/0963721420915873,
 abstract = {How do children learn to read? How do deficits in various components of the reading network affect learning outcomes? How does remediating one or several components change reading performance? In this article, we summarize what is known about learning to read and how this can be formalized in a developmentally plausible computational model of reading acquisition. The model is used to understand normal and impaired reading development (dyslexia). In particular, we show that it is possible to simulate individual learning trajectories and intervention outcomes on the basis of three component skills: orthography, phonology, and vocabulary. We therefore advocate a multifactorial computational approach to understanding reading that has practical implications for dyslexia and intervention.},
 author = {Johannes C. Ziegler and Conrad Perry and Marco Zorzi},
 doi = {10.1177/0963721420915873},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0963721420915873},
 journal = {Current Directions in Psychological Science},
 note = {PMID:32655213},
 number = {3},
 pages = {293–300},
 title = {Learning to Read and Dyslexia: From Theory to Intervention Through Personalized Computational Models},
 url = {https://doi-org.crai.referencistas.com/10.1177/0963721420915873},
 volume = {29},
 year = {2020t}
}

@article{doi:10.1177/09637214211046955,
 abstract = {Understanding language requires applying cognitive operations (e.g., memory retrieval, prediction, structure building) that are relevant across many cognitive domains to specialized knowledge structures (e.g., a particular language’s lexicon and syntax). Are these computations carried out by domain-general circuits or by circuits that store domain-specific representations? Recent work has characterized the roles in language comprehension of the language network, which is selective for high-level language processing, and the multiple-demand (MD) network, which has been implicated in executive functions and linked to fluid intelligence and thus is a prime candidate for implementing computations that support information processing across domains. The language network responds robustly to diverse aspects of comprehension, but the MD network shows no sensitivity to linguistic variables. We therefore argue that the MD network does not play a core role in language comprehension and that past findings suggesting the contrary are likely due to methodological artifacts. Although future studies may reveal some aspects of language comprehension that require the MD network, evidence to date suggests that those will not be related to core linguistic processes such as lexical access or composition. The finding that the circuits that store linguistic knowledge carry out computations on those representations aligns with general arguments against the separation of memory and computation in the mind and brain.},
 author = {Evelina Fedorenko and Cory Shain},
 doi = {10.1177/09637214211046955},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09637214211046955},
 journal = {Current Directions in Psychological Science},
 note = {PMID:35295820},
 number = {6},
 pages = {526–534},
 title = {Similarity of Computations Across Domains Does Not Imply Shared Implementation: The Case of Language Comprehension},
 url = {https://doi-org.crai.referencistas.com/10.1177/09637214211046955},
 volume = {30},
 year = {2021g}
}

@article{doi:10.1177/09637214221078325,
 abstract = {Words are fundamental to language, linking sound, articulation, and spelling to meaning and syntax; and lexical deficits are core to communicative disorders. Work in language acquisition commonly focuses on how lexical knowledge—knowledge of words’ sound patterns and meanings—is acquired. But lexical knowledge is insufficient to account for skilled language use. Sophisticated real-time processes must decode the sound pattern of words and interpret them appropriately. We review work that bridges this gap by using sensitive real-time measures (eye tracking in the visual world paradigm) of school-age children’s processing of highly familiar words. This work reveals that the development of word recognition skills can be characterized by changes in the rate at which decisions unfold in the lexical system (the activation rate). Moreover, contrary to the standard view that these real-time skills largely develop during infancy and toddlerhood, they develop slowly, at least through adolescence. In contrast, language disorders can be linked to differences in the ultimate degree to which competing interpretations are suppressed (competition resolution), and these differences can be mechanistically linked to deficits in inhibition. These findings have implications for real-world problems such as reading difficulties and second-language acquisition. They suggest that developing accurate, flexible, and efficient processing is just as important a developmental goal as is acquiring language knowledge.},
 author = {Bob McMurray and Keith S. Apfelbaum and J. Bruce Tomblin},
 doi = {10.1177/09637214221078325},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09637214221078325},
 journal = {Current Directions in Psychological Science},
 number = {4},
 pages = {305–315},
 title = {The Slow Development of Real-Time Processing: Spoken-Word Recognition as a Crucible for New Thinking About Language Acquisition and Language Disorders},
 url = {https://doi-org.crai.referencistas.com/10.1177/09637214221078325},
 volume = {31},
 year = {2022n}
}

@article{doi:10.1177/09637214231205220,
 abstract = {Psychiatric research is undergoing significant advances in an emerging subspeciality of computational psychiatry, building on cognitive neuroscience research by expanding to neurocomputational modeling. Here, we illustrate some research trends in this domain using work on proactive cognitive control deficits in schizophrenia as an example. We provide a selective review of formal modeling approaches to understanding cognitive control deficits in psychopathology, focusing primarily on biologically plausible connectionist-level models as well as mathematical models that generate parameter estimates of putatively dissociable psychological or neural processes. We illustrate some of the advantages of these models in terms of understanding both cognitive control deficits in schizophrenia and the potential roles of effort and motivation. Further, we highlight critical future directions for this work, including a focus on establishing psychometric properties, additional work modeling psychotic symptoms and their interaction with cognitive control, and the need to expand both behavioral and neural modeling to samples that include individuals with different mental health conditions, allowing for the examination of dissociable neural or psychological substrates for seemingly similar cognitive impairments across disorders.},
 author = {Deanna M. Barch and Adam J. Culbreth and Julia M. Sheffield},
 doi = {10.1177/09637214231205220},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09637214231205220},
 journal = {Current Directions in Psychological Science},
 number = {1},
 pages = {35–42},
 title = {Cognitive Control in Schizophrenia: Advances in Computational Approaches},
 url = {https://doi-org.crai.referencistas.com/10.1177/09637214231205220},
 volume = {33},
 year = {2024b}
}

@article{doi:10.1177/09713557221097178,
 abstract = {The aim of this study was to investigate the correlational and causal relationship between middle school students’ entrepreneurial competencies and science, technology, engineering and mathematics (STEM) attitudes. A total of 648 middle school students (seventh and eighth grade) participated in this study. STEM attitude scale and two entrepreneurial competency scales that were developed in different studies in the literature were used as data collection tools. In addition, a simple regression, a multiple regression and a stepwise multiple regression analysis were executed to analyse the data. The correlational analysis showed that there was a moderate level, positive correlation between the STEM attitudes of students and their overall entrepreneurial competencies. Also, the multiple regression analysis showed that entrepreneurial competencies consisting of professionalism, risk-taking, creativity and tenacity explained 41% of the change in STEM attitude. The stepwise multiple regression analysis indicated that professionalism predicted the most the STEM attitude statistically more.},
 author = {İsa Deveci and Fatma Zehra Konuş},
 doi = {10.1177/09713557221097178},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09713557221097178},
 journal = {The Journal of Entrepreneurship},
 number = {2},
 pages = {425–457},
 title = {The Predictive Power of Turkish Middle School Students’ Entrepreneurial Competencies on STEM Attitudes},
 url = {https://doi-org.crai.referencistas.com/10.1177/09713557221097178},
 volume = {31},
 year = {2022e}
}

@article{doi:10.1177/09720634241278878,
 abstract = {Coronavirus disease-19 (COVID-19) was declared a global pandemic caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) that targets the lower respiratory tract in human species. Since the target receptor molecule is angiotensin-converting enzyme 2 (ACE2), which is highly expressed on the cell lining of the respiratory tract, the virus is responsible for causing acute respiratory distress syndrome (ARDS). Computational tools such as artificial intelligence, machine learning and deep learning-based platforms are used for drug repurposing, identification, and selection of target molecules that can be used for vaccine development and antiviral drug production. Computer-aided drug designing, molecular docking and homology modelling are some of the most widely used in silico models for drug discovery against COVID-19. It is important to take preventive measures to control the spread of the virus. The present review focuses on the computational tools used for recognising the target molecules for vaccine production, the transmission networks of COVID-19 and the long-term strategies to prevent future pandemics such as COVID-19.},
 author = {Shruti Rathore and Praveen Dahiya},
 doi = {10.1177/09720634241278878},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09720634241278878},
 journal = {Journal of Health Management},
 number = {0},
 pages = {09720634241278878},
 title = {Drug Repurposing Using Computational Tools and Preventive Strategies for COVID-19 and Future Pandemics},
 url = {https://doi-org.crai.referencistas.com/10.1177/09720634241278878},
 volume = {0},
 year = {2024p}
}

@article{doi:10.1177/097215090500600109,
 abstract = {While it is useful and necessary to know where the industry and competitive market now stands and how it works today in order to formulate threshold or survival strategies, more is needed. Gaining competitive advantage and deriving financial profit from the results over the intermediate and longer term requires studying where the competitive market might be heading. Therefore, systematically listening to what that market is saying about the future becomes a creative art as much as it is a science. This article discusses exploring future trends and possible effective responses to these trends.},
 author = {Robert J. Mockler},
 doi = {10.1177/097215090500600109},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/097215090500600109},
 journal = {Global Business Review},
 number = {1},
 pages = {125–152},
 title = {Stimulating Innovative Thinking: Learning How to Listen to What a Situation is Trying to Tell Us},
 url = {https://doi-org.crai.referencistas.com/10.1177/097215090500600109},
 volume = {6},
 year = {2005f}
}

@article{doi:10.1177/09732586221116464,
 abstract = {This study investigates whether and how analytical thinking, overclaiming, and social approval are associated with the intention of sharing fake news on social media. To randomize each respondent to a group and treatment and to test of several hypotheses simultaneously, two by two factorial design was used. An online survey (N = 1160) on Iranian social media revealed that overclaiming and social approval are positively related to sharing fake news on social media. Surprisingly, analytical thinking yielded no significance. We believe that in order to show more knowledge users tend to share information with high social approval irrespective of their credibility. Although CRT proved no relation with sharing, significant differences among male and female users were found. The proven relation between sharing more and overclaiming more reveals a marketing opportunity. Gamification of communication which provides a vehicle for users to overclaim their knowledge to their peers on social media might be a suitable strategy on social media to spread the message.},
 author = {Emad Rahmanian and Mohammad Rahim Esfidani},
 doi = {10.1177/09732586221116464},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09732586221116464},
 journal = {Journal of Creative Communications},
 number = {1},
 pages = {7–25},
 title = {It Is Probably Fake but Let Us Share It! Role of Analytical Thinking, Overclaiming and Social Approval in Sharing Fake News},
 url = {https://doi-org.crai.referencistas.com/10.1177/09732586221116464},
 volume = {18},
 year = {2023q}
}

@article{doi:10.1177/09732586231206651,
 abstract = {The marriage of twenty-first-century horizons of technology and the global ideal constitutes techno-global rationality as it reflects contemporary impulses, frames and teleologies. Fast-paced automation, the importance of cosmopolitanism and the colonial legacy have come to dominate educational discourse and drive calls for streamlined educative practice. Although such efficiency models empower a transactional/linear mode of teaching and learning, they do little to privilege integrative voices, deliberation and intersubjective care found in global citizenship education (GCE) definitions. We argue such rationality has exacerbated a neo-colonial ethic that promulgates economic, political and cultural pressure to control and narrow otherwise diverse learning opportunities. Drawing from recent research into technology and GCE in two International Baccalaureate international schools, we note the importance of communicative outreach and agency in diversity. We also highlight the distorting effects of hyper-rationalised neo-colonial interpretations of global agency. This article will interest those seeking to develop global educational policy and practice along with revitalising interpretations of technology integration.},
 author = {Nicholas Palmer and Harsha Chandir},
 doi = {10.1177/09732586231206651},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09732586231206651},
 journal = {Journal of Creative Communications},
 number = {1},
 pages = {59–73},
 title = {Education Beyond Techno-global Rationality: Transnational Learning, Communicative Agency and the Neo-colonial Ethic},
 url = {https://doi-org.crai.referencistas.com/10.1177/09732586231206651},
 volume = {19},
 year = {2024k}
}

@article{doi:10.1177/0973801020976607,
 abstract = {The aim of this article was to analyse the employability of currently enrolled secondary education (classes 9–12) students aged 14–21, specifically their functional digital skills. Digital skills are increasingly being recognised as a key foundational skill that also enhances employability. Gaining digital skills at the secondary education is important because it is one of the foundational skills that help prepare students transition to work and prepares them for life. There were three major objectives in this article: (a) to examine the trends and transitions in the acquisition of functional digital skills of currently enrolled secondary school Indian students according to their socio-economic and demographic profiles; (b) empirically investigate the factors influencing the acquisition of functional digital skills in urban areas and (c) examine whether the policy of providing schools with computers has had any discernible impact on the acquisition of functional digital skills of these students. Using the National Statistical Office 2017–2018 data on expenditure on education, we found that at the national level only 42% of the enrolled secondary school students had the ability to operate a computer and 46% had the ability to browse Internet in 2017–2018. The attainment of functional digital skills differed across rural and urban regions. The individual characteristics, socio-economic profile of households and school-related indicators were factors that explained the likelihood of students’ being equipped with functional digital skills. A key result was that students who had digital devices at home were more likely to have functional digital skills. The government had introduced computers in secondary schools in 2004 in India. We found evidence of a positive association between the provision of functional computers at secondary schools and attainment of digital skills, even for students from households with no computers at home. A key policy recommendation is that providing computers at schools can help overcome barriers to access to digital devices at home and improve the attainment of digital skills. JEL Codes: I2, O15, O3},
 author = {Bornali Bhandari and Charu Jain and Ajaya K. Sahu},
 doi = {10.1177/0973801020976607},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0973801020976607},
 journal = {Margin: The Journal of Applied Economic Research},
 number = {1},
 pages = {73–100},
 title = {Are Secondary Schools Imparting Digital Skills? An Empirical Assessment},
 url = {https://doi-org.crai.referencistas.com/10.1177/0973801020976607},
 volume = {15},
 year = {2021c}
}

@article{doi:10.1177/0973801020981161,
 author = {Bornali Bhandari},
 doi = {10.1177/0973801020981161},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/0973801020981161},
 journal = {Margin: The Journal of Applied Economic Research},
 number = {1},
 pages = {7–21},
 title = {Overview},
 url = {https://doi-org.crai.referencistas.com/10.1177/0973801020981161},
 volume = {15},
 year = {2021b}
}

@article{doi:10.1177/09763996231158229,
 abstract = {The rise of artificial intelligence (AI) is rapidly influencing our education system. It is apparent that the students of today are mostly attached with their smart mobile phones, tablets, laptops, and various other forms of advanced technologies for their quality of learning. It has become an urgent necessity for school students to become future AI ready. Understanding the wide potential impact of AI, India has started initiatives to prepare young learners for future AI ready. Central Board of Secondary Education in the direction of National Education Policy (2020) introduces two-fold AI in its affiliated school curricula. Using a systematic review technique, the present study has attempted to explore the promise and potentiality of AI in school education, and provide a comprehensive overview of the current status and development trends of AI in school, the initiatives, planning, strategies, and steps taken by India and other countries regarding AI integration in their school system. Finally, the study brings out some concluding remarks towards innovative AI integration.},
 author = {Bablu Karan and G. R. Angadi},
 doi = {10.1177/09763996231158229},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/09763996231158229},
 journal = {Millennial Asia},
 number = {0},
 pages = {09763996231158229},
 title = {Artificial Intelligence Integration into School Education: A Review of Indian and Foreign Perspectives},
 url = {https://doi-org.crai.referencistas.com/10.1177/09763996231158229},
 volume = {0},
 year = {2023g}
}

@article{doi:10.1177/1023263X20981472,
 abstract = {This article informs about certain legal analytics tools that can be used to predict the outcome of cases. It identifies and assesses some challenges to the right to a fair trial that appear in case this kind of tools are employed by judges and parties in judicial proceedings. Further, the article offers a reflection on possible strategies to tackle the identified challenges, including the adoption of the appropriate method to address legal analytics in court procedures, the need for proper training of judges or certification and auditing schemes.},
 author = {Vilte Kristina Steponenaite and Peggy Valcke},
 doi = {10.1177/1023263X20981472},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1023263X20981472},
 journal = {Maastricht Journal of European and Comparative Law},
 number = {6},
 pages = {759–773},
 title = {Judicial analytics on trial: An assessment of legal analytics in judicial systems in light of the right to a fair trial},
 url = {https://doi-org.crai.referencistas.com/10.1177/1023263X20981472},
 volume = {27},
 year = {2020o}
}

@article{doi:10.1177/10298649020050S104,
 abstract = {This article presents a computational model of expression in music performance: the GERM model. The purpose of the GERM model is to (a) describe the principal sources of variability in music performance, (b) emphasize the need to integrate different aspects of performance in a common model, and (c) provide some preliminaries (germ = a basis from which a thing may develop) for a computational model that simulates the different aspects. Drawing on previous research on performance, we propose that performance expression derives from four main sources of variability: (1) Generative Rules, which function to convey the generative structure in a musical manner (e.g., Clarke, 1988; Sundberg, 1988); (2) Emotional Expression, which is governed by the performer’s expressive intention (e.g., Juslin, 1997a); (3) Random Variations, which reflect internal timekeeper variance and motor delay variance (e.g., Gilden, 2001; Wing and Kristofferson, 1973); and (4) Movement Principles, which prescribe that certain features of the performance are shaped in accordance with biological motion (e.g., Shove and Repp, 1995). A preliminary version of the GERM model was implemented by means of computer synthesis. Synthesized performances were evaluated by musically trained participants in a listening test. The results from the test support a decomposition of expression in terms of the GERM model. Implications for future research on music performance are discussed.},
 author = {Patrik N. Juslin and Anders Friberg and Roberto Bresin},
 doi = {10.1177/10298649020050S104},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/10298649020050S104},
 journal = {Musicae Scientiae},
 number = {1_suppl},
 pages = {63–122},
 title = {Toward a computational model of expression in music performance: The GERM model},
 url = {https://doi-org.crai.referencistas.com/10.1177/10298649020050S104},
 volume = {5},
 year = {2001g}
}

@article{doi:10.1177/1029864918757595,
 abstract = {In musicology, there has been a long debate about a meaningful partitioning and description of music history regarding composition styles. Particularly, concepts of historical periods have been criticized since they cannot account for the continuous and interwoven evolution of style. To systematically study this evolution, large corpora are necessary suggesting the use of computational strategies. This article presents such strategies and experiments relying on a dataset of 2000 audio recordings, which cover more than 300 years of music history. From the recordings, we extract different tonal features. We propose a method to visualize these features over the course of history using evolution curves. With the curves, we re-trace hypotheses concerning the evolution of chord transitions, intervals, and tonal complexity. Furthermore, we perform unsupervised clustering of recordings across composition years, individual pieces, and composers. In these studies, we found independent evidence of historical periods that broadly agrees with traditional views as well as recent data-driven experiments. This shows that computational experiments can provide novel insights into the evolution of styles.},
 author = {Christof Weiß and Matthias Mauch and Simon Dixon and Meinard Müller},
 doi = {10.1177/1029864918757595},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1029864918757595},
 journal = {Musicae Scientiae},
 number = {4},
 pages = {486–507},
 title = {Investigating style evolution of Western classical music: A computational approach},
 url = {https://doi-org.crai.referencistas.com/10.1177/1029864918757595},
 volume = {23},
 year = {2019s}
}

@article{doi:10.1177/1032373208095480,
 abstract = {Paralleling the advent of different conceptions of accounting in the past two decades or so is the distinction between what are now known as the “traditional” and “new” schools of accounting history research. Viewing accounting as a social practice, as opposed to a mere technical practice, orientates the historical researcher firmly into the arena of the new accounting history, which recognizes the pervasive and enabling characteristics of accounting and gives rise to concerns about studying the implications of accounting change on organizational and social functioning. This literature study examines the interplay of conceptions of accounting with schools of thought in the historical accounting literature. It seeks to enhance an understanding of the underlying connections between the conceptions of accounting embraced by researchers of contemporary accounting and the schools of thought adopted by historical accounting researchers. As the state of play in accounting history research appears to have become a little predictable, certain challenges are identified for accounting historians of the future.},
 author = {Delfina Gomes},
 doi = {10.1177/1032373208095480},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1032373208095480},
 journal = {Accounting History},
 number = {4},
 pages = {479–509},
 title = {The interplay of conceptions of accounting and schools of thought in accounting history},
 url = {https://doi-org.crai.referencistas.com/10.1177/1032373208095480},
 volume = {13},
 year = {2008e}
}

@article{doi:10.1177/104346396008003004,
 abstract = {Recently there has been an increase in the number of researchers who use rational choice models to explain single cases and rare events. Because of the small number of cases under study, these researchers must rely either explicitly or implicitly on counterfactual reasoning. This paper argues that computational methods provide a profitable means of carrying out rigorous counterfactual analysis. The authors advocate robustness analysis as one important part of the counterfactual analysis of formal theories. Specifically, they evaluate the robustness of the behavioral assumptions of two formal models using various heuristic search algorithms and Markov chains. They find that Kuran’s (1989) threshold model of mass protest and Ingberman’s (1985) model of direct-democracy referenda are robust to perturbations in their behavioral assumptions. These findings increase the plausibility of causal claims made by scholars who use these models to explain specific events.},
 author = {Andrew D. Martin and Kevin M. Quinn},
 doi = {10.1177/104346396008003004},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/104346396008003004},
 journal = {Rationality and Society},
 number = {3},
 pages = {295–323},
 title = {USING COMPUTATIONAL METHODS TO PERFORM COUNTERFACTUAL ANALYSES OF FORMAL THEORIES},
 url = {https://doi-org.crai.referencistas.com/10.1177/104346396008003004},
 volume = {8},
 year = {1996m}
}

@article{doi:10.1177/1043986215608532,
 abstract = {Aspects of criminological theory are premised on the belief that criminals make poor decisions. There have been suggestions that individual differences in self-control, willpower, impulsivity, time orientation, or more recently, thoughtfully reflective decision making (TRDM) influence choices and, ultimately, deviant outcomes. While much of the literature suggests there are differences among these concepts, they are often used interchangeably or at least noted to share common ground. Using survey data collected from university undergraduate students, this article explores the conceptual and empirical overlap and areas of distinction between key theoretical constructs. Using hypotheses derived from a dual-systems model, findings suggest impulsivity, self-control, temptation, and TRDM are distinct but interrelated constructs. Impulsivity was positively related to intentions to drink and drive in a hypothetical scenario, but temptation, self-control, and TRDM had no significant effect on intentions. Consistent with a dual-systems conceptualization, we found impulsivity and self-control work in tandem, as the risk of drinking and driving was highest for those respondents who were highly impulsive and had low self-control. Future research may seek to use a dual-system model to further reconcile trait-based and decision-based models of criminological theory.},
 author = {Chae Mamayek and Thomas Loughran and Ray Paternoster},
 doi = {10.1177/1043986215608532},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1043986215608532},
 journal = {Journal of Contemporary Criminal Justice},
 number = {4},
 pages = {426–448},
 title = {Reason Taking the Reins From Impulsivity: The Promise of Dual-Systems Thinking for Criminology},
 url = {https://doi-org.crai.referencistas.com/10.1177/1043986215608532},
 volume = {31},
 year = {2015j}
}

@article{doi:10.1177/104538902761696814,
 abstract = {In this paper we discuss the sound absorption property of arrays of micro-acoustic actuators at a control surface. We use the wave equation over the half plane for the velocity potential with a boundary dissipation by a proportional pressure feedback law along the half plane boundary. The feedback gain over the array is described by a distributed shape function. We develop a computational method based on the Fourier transform and employ it for analyzing and evaluating the decay rate of acoustic energy. Specifically, we carry out computations for a diffusive random initial field and report on our resulting numerical findings.},
 author = {H. T. Banks and D. G. Cole and K. M. Furati and K. Ito and G. A. Pinter},
 doi = {10.1177/104538902761696814},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/104538902761696814},
 journal = {Journal of Intelligent Material Systems and Structures},
 number = {4},
 pages = {231–240},
 title = {A Computational Model for Sound Field Absorption by Acoustic Arrays},
 url = {https://doi-org.crai.referencistas.com/10.1177/104538902761696814},
 volume = {13},
 year = {2002a}
}

@article{doi:10.1177/104538903038023,
 abstract = {A micro-electro-mechanical model of the behavior of piezoelectric ceramics including thermal effects is presented and compared to experimental data. Results include analytical and numerical investigations of the behavior of piezoelectric ceramics. The model is based on physical mechanisms and includes elastic, dielectric, and piezoelectric anisotropy. Moreover, the model is based on an internal energy approach so that work-energy relations may be directly applied. Results from the model give insight into material behavior.},
 author = {Lisa D. Mauck and Christopher S. Lynch},
 doi = {10.1177/104538903038023},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/104538903038023},
 journal = {Journal of Intelligent Material Systems and Structures},
 number = {9},
 pages = {587–602},
 title = {Thermo-Electro-Mechanical Behavior of Ferroelectric Materials           Part I: A Computational Micromechanical Model Versus Experimental Results},
 url = {https://doi-org.crai.referencistas.com/10.1177/104538903038023},
 volume = {14},
 year = {2003k}
}

@article{doi:10.1177/1045389X13508335,
 abstract = {We investigate a new approach for characterizing class separability based on topological measures for use with identifying flaw severity in plates. Multi-mode Lamb waves are propagated across a flat-bottom hole of varying depth. Lamb wave tomography reconstructions are first generated to locate and size the flaw at each depth. As the flaw depth increases, scattering and mode conversion effects dominate the raw time-domain signals, obscuring information about flaw severity. Pattern classification provides an alternate means for processing the ultrasonic waveforms to identify flaw severity. High-dimensional feature spaces are generated from the Lamb wave signals using the dynamic wavelet fingerprinting technique. In order to achieve high classification accuracy, an optimal feature space is required. An intelligent feature selection routine is explored here that identifies favorable class distributions in multidimensional feature spaces using computational homology theory. Betti numbers and formal classification accuracies are calculated for each feature space subset to establish a correlation between the topology of the class distribution and the corresponding classification accuracy.},
 author = {Corey A Miller and Mark K Hinders},
 doi = {10.1177/1045389X13508335},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1045389X13508335},
 journal = {Journal of Intelligent Material Systems and Structures},
 number = {12},
 pages = {1511–1527},
 title = {Multiclass feature selection using computational homology for Lamb wave–based damage characterization},
 url = {https://doi-org.crai.referencistas.com/10.1177/1045389X13508335},
 volume = {25},
 year = {2014j}
}

@article{doi:10.1177/1045389X13517314,
 abstract = {In this study, a computational model is developed using finite-element techniques within a continuum micromechanics framework to capture the effect of electron-hopping-induced conductive paths at the nanoscale which contribute to the macroscale piezoresistive response of the nanocomposite. This is achieved by tracking the position of the nanotubes under applied deformations and modifying the conductivity of the intertube region depending on the relative proximity of individual pairs of nanotubes. The formation and disruption of the electron-hopping pathways are highly dependent on intertube distances and under deformations can result in microstructural rearrangements in terms of electrostatic properties leading to transitions in material symmetries and component magnitudes of the effective electrostatic properties. Thus, in order to capture the complexities of changing inhomogeneous nanoscale electrostatic behavior, where analytical Eshelby’s approaches cannot be used, a computational micromechanics model is needed. The effective conductivity and piezoresistive strain tensor coefficients are evaluated using volume-averaged energy equivalencies for aligned CNT–polymer nanocomposites in the transverse direction exploring different volume fractions of CNTs in the polymer and the maximum electron-hopping range. The impact of the electron-hopping mechanism on the effective piezoresistive response is studied through the macroscale effective gauge factors under different loading conditions. The effective piezoresistive strain coefficients and macroscale effective gauge factors are observed to be nonlinear with applied macroscale strain and are highly dependent on the type of boundary conditions. The effective macroscale gauge factors observed in the current study have magnitudes comparable to experimental observations reported in the literature with higher gauge factors observed closer to the percolation threshold.},
 author = {A K Chaurasia and G D Seidel},
 doi = {10.1177/1045389X13517314},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1045389X13517314},
 journal = {Journal of Intelligent Material Systems and Structures},
 number = {17},
 pages = {2141–2164},
 title = {Computational micromechanics analysis of electron-hopping-induced conductive paths and associated macroscale piezoresistive response in carbon nanotube–polymer nanocomposites},
 url = {https://doi-org.crai.referencistas.com/10.1177/1045389X13517314},
 volume = {25},
 year = {2014d}
}

@article{doi:10.1177/1045389X14546650,
 abstract = {The magnetorheological fluids are classified as smart materials with controllable rheological properties. The fast growing application of magnetorheological fluids in recent years has increased the demand for simulation and modeling of these fluids. From the invention of magnetorheological fluids up to now, many experimental and also theoretical investigations have been carried out to study these types of smart materials; also many attempts have been made to formulate and simulate their behavior. The aim of this investigation is to present a review on the different models and simulation methods that were applied in the studying of magnetorheological fluids. In this study, the different simulation methods of magnetorheological fluid have been categorized into two general approaches: continuum and discrete phase approaches. The different rheological and structural models of magnetorheological fluids in continuum approach have been summarized in this study. The computational framework of discrete approach and the basic models for magnetorheological fluid in this approach are also discussed.},
 author = {Ali Ghaffari and Seyed Hassan Hashemabadi and Mahshid Ashtiani},
 doi = {10.1177/1045389X14546650},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1045389X14546650},
 journal = {Journal of Intelligent Material Systems and Structures},
 number = {8},
 pages = {881–904},
 title = {A review on the simulation and modeling of magnetorheological fluids},
 url = {https://doi-org.crai.referencistas.com/10.1177/1045389X14546650},
 volume = {26},
 year = {2015g}
}

@article{doi:10.1177/1045389X16685440,
 abstract = {In this study, NiTi shape memory alloys coupled in series with Al are considered as building blocks for thermal diodes. It is shown that the strong nonlinearity in the temperature-dependent thermal properties of NiTi in conjunction with the very different thermal properties of Al can result into a thermal diode of high thermal rectification ratio. As a first level of study, Ni50Ti50 is considered and the effects of various NiTi-Al geometrical configurations, initial temperature, and temperature difference at two ends on the thermal rectification ratio are studied numerically. Within the adopted temperature range (300–400 K, where phase transformation in NiTi occurs), it is shown that NiTi-Al thermal diodes are feasible with rectification ratio up to 4.8, which is quite higher than the ratios in currently known solid-state thermal diodes. This fundamental computational study could provide an important basis and motivation for the development of the next generation of high-temperature solid-state thermal diodes based on smart material such as NiTi shape memory alloys or others.},
 author = {George N Frantziskonis and Sourav Gur},
 doi = {10.1177/1045389X16685440},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1045389X16685440},
 journal = {Journal of Intelligent Material Systems and Structures},
 number = {15},
 pages = {2082–2094},
 title = {Computational simulations for the development of novel solid-state smart NiTi-Al thermal diodes},
 url = {https://doi-org.crai.referencistas.com/10.1177/1045389X16685440},
 volume = {28},
 year = {2017i}
}

@article{doi:10.1177/1045389X18798957,
 abstract = {Self-expanding stents made of Nitinol, a Nickel–Titanium shape memory alloy, are used in standard medical implants for the treatment of cardiovascular diseases. Despite the increasing success, clinical studies have reported stent failure after the deployment in the human body, thus undermining patient’s safety and life. This study aims to fill the gap of reliable assessment of the fatigue life of Nitinol stents. We propose a global computational design method for preclinical validation of Nitinol stents, which can be extended to patient-specific computations. The proposed methodology is composed of a mechanical finite element analysis and a fatigue analysis. The latter analysis is based on a novel multiaxial fatigue criterion of the Dang Van type, combining the shakedown response of the stent and the complexity of phase transformation taking place within the material. The method is implemented in the case of a carotid artery stent. The implant configuration as well as the applied cyclic loading are shown to affect material phase evolution as well as stent lifetime. The comparison with the results obtained by applying a strain-based constant-life diagram approach allows to critically discuss both fatigue criteria and to provide useful recommendations about their applicability.},
 author = {Giulia Scalet and Costantino Menna and Andrei Constantinescu and Ferdinando Auricchio},
 doi = {10.1177/1045389X18798957},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1045389X18798957},
 journal = {Journal of Intelligent Material Systems and Structures},
 number = {19},
 pages = {3710–3724},
 title = {A computational approach based on a multiaxial fatigue criterion combining phase transformation and shakedown response for the fatigue life assessment of Nitinol stents},
 url = {https://doi-org.crai.referencistas.com/10.1177/1045389X18798957},
 volume = {29},
 year = {2018m}
}

@article{doi:10.1177/1045389X18803461,
 abstract = {A computational study of the electromechanical response of micro-structure engineered two port surface acoustic wave delay lines on gallium arsenide is presented. The influence on the results of geometrical, material, and mesh parameters is also discussed. Furthermore, experimental results are provided to validate the numerical study. The device consists of two interdigital transducers composed of 40, 80, and 120 pairs of electrodes, respectively, with a pitch and distant . In particular, a microwave burst of surface acoustic waves propagating on gallium arsenide is fully characterized including multiple transit effects. These results are of major interest for understanding the dynamical behavior of complex systems such as surface acoustic wave–based sensors or energy harvesting devices at the nano and microscale.},
 author = {Claudio Maruccio and Marco Scigliuzzo and Silvia Rizzato and Pasquale Scarlino and Giuseppe Quaranta and Maria Serena Chiriaco and Anna Grazia Monteduro and Giuseppe Maruccio},
 doi = {10.1177/1045389X18803461},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1045389X18803461},
 journal = {Journal of Intelligent Material Systems and Structures},
 number = {6},
 pages = {801–812},
 title = {Frequency and time domain analysis of surface acoustic wave propagation on a piezoelectric gallium arsenide substrate: A computational insight},
 url = {https://doi-org.crai.referencistas.com/10.1177/1045389X18803461},
 volume = {30},
 year = {2019l}
}

@article{doi:10.1177/1045389X19898252,
 abstract = {Carbon nanofiller–modified polymers have been the subject of intense study for years due to their potential use in diverse and far-reaching applications. The effect of nanofiller network parameters on macroscale direct current electrical transport has been thoroughly elucidated by extensive nano-to-microscale modeling. As a result, we now have great insight into how the conductive and piezoresistive properties of nanocomposites can be tailored through judicious control of the underlying nanofiller network. It is also well-known that carbon nanofiller–modified polymers possess frequency-dependent alternating current electrical properties. Even though work has been done to understand the alternating current properties of nanocomposites via experimental characterization and through the development of macroscale equivalent circuit models, much less has been done to understand how macroscale alternating current conductivity depends on microscale effects such as nanofiller alignment and aspect ratio. This is an important knowledge gap because, like direct current conductivity, the underlying nanofiller network ultimately gives rise to macroscale alternating current transport in these materials. To this end, we herein present an alternating current microscale percolation model for carbon filler–based polymer nanocomposites. After calibration against experimental complex impedance data from randomly ordered carbon nanofiber–modified epoxy, this model is used to explore the effect of carbon nanofiber alignment and aspect ratio on alternating current conductivity. These simulations show that alternating current conductivity generally increases with increasing alignment and with aspect ratio; however, the competing effects of alternating current and direct current percolation give rise to substantial variation in alternating current conductivity at low frequencies and with poor percolation. The methodology presented in this article provides a modeling tool by which nanocomposites with highly optimized alternating current properties can be developed through careful control and tailoring of nanofiller network properties for the realization of exotic, next-generation material functionality.},
 author = {Tyler N Tallman and Hashim Hassan},
 doi = {10.1177/1045389X19898252},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1045389X19898252},
 journal = {Journal of Intelligent Material Systems and Structures},
 number = {5},
 pages = {756–770},
 title = {A computational exploration of the effect of alignment and aspect ratio on alternating current conductivity in carbon nanofiber–modified epoxy},
 url = {https://doi-org.crai.referencistas.com/10.1177/1045389X19898252},
 volume = {31},
 year = {2020q}
}

@article{doi:10.1177/1045389X20930093,
 abstract = {In this work, the authors present a two-dimensional computational model for predicting the aeroelastic response as well as the output power of vertically arranged harvesters by taking into account all aerodynamic interactions. The piezo-aeroelastic framework consists of the following: (1) an aerodynamic model based on the unsteady vortex-lattice method to compute the aerodynamic forces; (2) a discrete parameter model for each harvester with 3 degrees of freedom (plunge motion, pitch motion, and the voltage generated by the piezoelectric effect); (3) an inter-model connection to exchange information between models at each time step; and (4) a numerical scheme based on the Hamming’s fourth-order predictor–corrector method to integrate all the governing equations in the time domain. The results obtained allow us to infer new insights into the flutter onset as well as the post-critical behavior of harvester arrangements. An interesting finding is that the flutter speed is significantly decreased as the distance between the harvesters is reduced. The results suggest the strong possibility of effective energy extraction at low flow speeds using properly distributed harvester arrangements. However, in post-critical conditions, the output power is significantly enhanced as the free-stream speed is increased.},
 author = {Bruno A Roccia and Marcos L Verstraete and Luis R Ceballos and Balakumar Balachandran and Sergio Preidikman},
 doi = {10.1177/1045389X20930093},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1045389X20930093},
 journal = {Journal of Intelligent Material Systems and Structures},
 number = {13},
 pages = {1578–1593},
 title = {Computational study on aerodynamically coupled piezoelectric harvesters},
 url = {https://doi-org.crai.referencistas.com/10.1177/1045389X20930093},
 volume = {31},
 year = {2020s}
}

@article{doi:10.1177/1045389X211026380,
 abstract = {Peripheral artery stenting (PAS) is an effective alternative for peripheral endarterectomy. Smart stents can be used to minimize the problems of interaction between peripheral arteries and smart stent. To evaluate the biomechanical properties of smart stents and their interactions with the peripheral artery, a 3D nonlinear finite element method (FEM) model that was composed of a peripheral artery and a smart stent was built. The present simulation modeled smart superelasticity material based on thermodynamics of the Helmholtz free energy (Auricchio theory) and Peripheral artery based on hyperelastic models (Mooney-Rivlin and Ogden). Additionally, the present study used FEM to assess the influences of the material properties, strain level, and friction coefficient (with attention to smart stent’s materials properties) of the newly designed smart stent during crimping and its interaction with the peripheral artery. The results showed that the smart stent with high Af: Austenite finish temperature, 90% crimping, and friction coefficient of 0.1 between smart stent and peripheral artery performed better mechanically and clinically, which can be attributed to suitable Chronic Outward Force (COF), high Radial Resistive Force (RRF), whole mechanical hysteresis regarding superelastic performance, the high martensite formation, the low stress on the peripheral artery, and the high strain on the internal curvature of the smart stent and peripheral artery. Moreover, it was found that the Mooney-Rivlin model showed a better mechanical performance that the Ogden model given the distribution of stress and strain level on the peripheral artery. These models were appropriate for the description of the peripheral artery performance and smart stent behavior with considering material properties, strain level, and friction coefficient during the interaction process.},
 author = {Fardin Nematzadeh},
 doi = {10.1177/1045389X211026380},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1045389X211026380},
 journal = {Journal of Intelligent Material Systems and Structures},
 number = {5},
 pages = {703–714},
 title = {A computational study of effects of material properties, strain level, and friction coefficient on smart stent behavior and peripheral artery performance during the interaction process},
 url = {https://doi-org.crai.referencistas.com/10.1177/1045389X211026380},
 volume = {33},
 year = {2022n}
}

@article{doi:10.1177/1046496410369561,
 abstract = {Many creative activities take place in a group context, whether in short-term meetings, work teams, or by means of electronic interaction. The group creative process necessarily involves the exchange of ideas or information. Recent models of group creativity have focused on the cognitive underpinnings of this type of group creative process, primarily based on the group brainstorming literature. The authors describe an elaborated computational version of their cognitive model of group creativity and related computational models, and highlight some plausible neural bases for various involved processes. The major findings and theoretical perspectives in this literature are summarized and some potentially fruitful empirical and theoretical directions are highlighted. It is hoped that this comprehensive treatment can be a basis for integrating the present literature and providing useful predictions for further research on this topic.},
 author = {Paul B. Paulus and Daniel S. Levine and Vincent Brown and Ali A. Minai and Simona Doboli},
 doi = {10.1177/1046496410369561},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1046496410369561},
 journal = {Small Group Research},
 number = {6},
 pages = {688–724},
 title = {Modeling Ideational Creativity in Groups: Connecting Cognitive, Neural, and Computational Approaches},
 url = {https://doi-org.crai.referencistas.com/10.1177/1046496410369561},
 volume = {41},
 year = {2010n}
}

@article{doi:10.1177/1046496418755511,
 abstract = {Richard Kettner-Polley and Charles Gavin founded Small Group Research (SGR) to present research, build theory, and generally advance the study of small groups by combining insights from multiple disciplines. Currently, we evaluate the extent to which this interdisciplinary mission has been upheld over time. To do this, we apply the perspective and tools of big data analytics to the nearly 3 million words that span the 829 articles that comprise the SGR corpus from February 1990 to June 2017. Keyword analysis, ontological ordering, and interdisciplinary content analyses identify intriguing patterns and detect latent trends. Our results speak to the consistent interdisciplinarity of SGR while identifying opportunities for further development and more complex disciplinary integration in research on small groups.},
 author = {Kurt Norder and Kyle J. Emich and Aman Sawhney},
 doi = {10.1177/1046496418755511},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1046496418755511},
 journal = {Small Group Research},
 number = {4},
 pages = {391–408},
 title = {Evaluating the Interdisciplinary Mission of Small Group Research Using Computational Analytics},
 url = {https://doi-org.crai.referencistas.com/10.1177/1046496418755511},
 volume = {49},
 year = {2018m}
}

@article{doi:10.1177/10464964241279164,
 abstract = {Small group researchers are increasingly called to engage the computational sciences. One challenge in answering this call is the lack of information concerning what the study of small groups looks like in these domains. This paper addresses this challenge through a prospecting review of research that computationally models or trains computers to learn small group and team behavior and is published in computing disciplines from 2016 to 2023. This review investigated how groups were modeled, for what purpose, what group elements were modeled, and whether social science informed the model design. Qualitative methods were used to analyze 119 published articles. Suggestions are presented for increasing the influence of small group research on the computational modeling of groups and teams, particularly for researchers with limited access to large research teams or resources. This review helps bridge the gap between small group research and computational sciences to advance the understanding of small groups and teams.},
 author = {Michele H. Jackson},
 doi = {10.1177/10464964241279164},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/10464964241279164},
 journal = {Small Group Research},
 number = {0},
 pages = {10464964241279164},
 title = {Modeling of Small Groups in Computational Sciences: A Prospecting Review},
 url = {https://doi-org.crai.referencistas.com/10.1177/10464964241279164},
 volume = {0},
 year = {2024m}
}

@article{doi:10.1177/1046878119872797,
 abstract = {Background. Learning programming is a cognitively demanding field of study accompanied with various difficulties. Although there is a high demand in the market for programmers, software analysts and engineers, a high dropout rate is recorded in relevant fields of study. Serious games are a promising means of engaging students in learning programming by giving them more incentives and making the process of learning programming concepts and languages more entertaining. Aim. This article introduces a new serious game called PY-RATE ADVENTURES, which aims to assist young students in their introduction to the basic programming concepts using Python. The game does not have any prerequisites and is suitable for players with no previous knowledge of programming. This article aims to present important information regarding the analysis, design and pilot evaluation of PY-RATE ADVENTURES. Method. The game was evaluated by 31 people that had recently graduated or were students of an Interdepartmental Programme of Postgraduate Studies in Information Systems. The participants voluntarily played the game and answered a questionnaire based on the MEEGA+ model, after their hands on experience with the game. This questionnaire’s purpose was to evaluate PY-RATE ADVENTURES in terms of perceived player experience and short-term learning. Results. The participants positively evaluated the game almost in all the elements of player experience. Furthermore, the majority of the users consider that the game helped them to learn basic programming concepts in Python and stated that they would prefer to learn programming with this game rather than other teaching methods. Conclusion. The positive results of the pilot evaluation give us the motivation to proceed and evaluate the game with students in secondary education, in order to extract stronger and generalisable conclusions regarding the impact of the game as an educational tool for learning programming concepts.},
 author = {Grigorios Sideris and Stelios Xinogalos},
 doi = {10.1177/1046878119872797},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1046878119872797},
 journal = {Simulation & Gaming},
 number = {6},
 pages = {754–770},
 title = {PY-RATE ADVENTURES: A 2D Platform Serious Game for Learning the Basic Concepts of Programming With Python},
 url = {https://doi-org.crai.referencistas.com/10.1177/1046878119872797},
 volume = {50},
 year = {2019r}
}

@article{doi:10.1177/1046878119901286,
 abstract = {Background Recent years have seen the resurgence of board games designed for entertainment, and to teach or explicate real life problems. The revival of board gameplay has been discussed in mainstream media, and has drawn the attention of researchers. Yet, in the field of games studies, the conception of games as learning spaces is mostly emphasized through digital/video games. Aim This literature review reveals the current knowledge regarding the learning potential of board games in various settings, subjects, and diverse learners. Results Board games are spaces for mathematical learning and learning spaces that can enable the learning of various contents. Board games allow for various interactions that result in players engaging in computational thinking, teamwork, and creativity. Conclusion The relationship between board gameplay and learning is evidenced across disciplines and countries. Board games simplify complex issues and systems, which make them appropriate to further explore learning and concepts such as motivation and computational thinking in formal and informal settings. Furthermore, there is need to expand research on learning in commercial board games.},
 author = {Rebecca Yvonne Bayeck},
 doi = {10.1177/1046878119901286},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1046878119901286},
 journal = {Simulation & Gaming},
 number = {4},
 pages = {411–431},
 title = {Examining Board Gameplay and Learning: A Multidisciplinary Review of Recent Research},
 url = {https://doi-org.crai.referencistas.com/10.1177/1046878119901286},
 volume = {51},
 year = {2020b}
}

@article{doi:10.1177/10468781221120690,
 abstract = {Background According to the Committee on STEM Education, K-12 science students need access to learning experiences that promote collaboration and engagement. To fill that void, we need to develop activities that stimulate engaged learning and scaffold effective collaboration. K-12 teacher candidates see value in utilizing games for this purpose. Specifically, tabletop science games can help teachers engage students in science learning and scaffold collaboration. Aim For this study, we designed a collaborative, STEM-themed card game called MOUNTAIN RESCUE and explored its capacity to promote engaged learning and collaboration. Method Four groups of STEM campers (n = 14) in a suburban Mid-Atlantic region played MOUNTAIN RESCUE. All groups had a mix of boys and girls. Play-testers ranged from 10–13 years old. The tabletop game took approximately 30-minutes. During gameplay, players embodied unique STEM roles: physicist, chemist, structural engineer, and electrical engineer. They collaborated to solve challenges related to electricity, physics, chemistry, and engineering design. Discourse was audio-recorded throughout gameplay. Immediately after gameplay, self-report survey data were collected to assess flow and perceptions of collaborative learning. Results Findings demonstrated that the game promoted engagement and collaboration. Specifically, students had a flow-like experience and felt positively about the game’s value for collaborative learning. Utterances demonstrating active engagement and constructive thinking became more group-focused over time. Conclusion This study contributes to science education by demonstrating potential benefits of a well-designed, low-tech, science learning environment or—in other words—a tabletop game.},
 author = {Denise M. Bressler and M. Shane Tutwiler and Amanda Siebert-Evenstone and Leonard A. Annetta and Jason A. Chen},
 doi = {10.1177/10468781221120690},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/10468781221120690},
 journal = {Simulation & Gaming},
 number = {5},
 pages = {564–576},
 title = {“What if We Explore…” Promoting Engaged Learning and Collaboration with MOUNTAIN RESCUE},
 url = {https://doi-org.crai.referencistas.com/10.1177/10468781221120690},
 volume = {53},
 year = {2022b}
}

@article{doi:10.1177/10497315211002646,
 abstract = {Purpose: This study sought to validate the Skills for Future Work Scale for its use with professional/vocational training (PT) and baccalaureate students. Methods: A total of 1,159 students were recruited. Structural equation analysis was performed using IBM Amos Graphics®. Results: Excellent fit indices were obtained, forming a final scale composed of nine indicators and two factors. The first dimension was associated with cognitive skills and adaptability, grouping together the skills of finding meaning, adaptable thinking, and understanding management. The second dimension comprises collaborative skills, as social intelligence and virtual collaboration. Discussion: Collaboration skills were more developed within those undertaking PT, while baccalaureate students had more strongly developed mastery over new media. This scale allows to know effectively the development of the work skills set by the European Union in the 2030 horizon. These findings contribute interesting data regarding the development of training strategies to support insertion into the job market.},
 author = {Ramón Chacón-Cuberos and Jorge Expósito-López and José Javier Romero-Díaz de la Guardia and Eva María Olmedo-Moreno},
 doi = {10.1177/10497315211002646},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/10497315211002646},
 journal = {Research on Social Work Practice},
 number = {7},
 pages = {758–769},
 title = {Skills for Future Work (H2030): Multigroup Analysis in Professional and Baccalaureate Training},
 url = {https://doi-org.crai.referencistas.com/10.1177/10497315211002646},
 volume = {31},
 year = {2021b}
}

@article{doi:10.1177/1053451214560892,
 abstract = {Many students with learning disabilities (LD) in mathematics receive their mathematics education in general education inclusive classes; therefore, these students must be capable of learning algebraic concepts, including developing algebraic thinking abilities, that are part of the general education curriculum. To help students develop algebraic thinking, teachers should ask questions in different ways to promote the ability to think algebraically. This article describes three types of questions—reversibility, flexibility, and generalizations—which support the acquisition of broader concepts leading to algebraic thinking. Examples of the question types within the contexts of rational numbers and integers are provided to assist teachers in creating similar questions for teaching mathematics to students with LD.},
 author = {Barbara Dougherty and Diane Pedrotty Bryant and Brian R. Bryant and Rebecca L. Darrough and Kathleen Hughes Pfannenstiel},
 doi = {10.1177/1053451214560892},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1053451214560892},
 journal = {Intervention in School and Clinic},
 number = {5},
 pages = {273–281},
 title = {Developing Concepts and Generalizations to Build Algebraic Thinking: The Reversibility, Flexibility, and Generalization Approach},
 url = {https://doi-org.crai.referencistas.com/10.1177/1053451214560892},
 volume = {50},
 year = {2015g}
}

@article{doi:10.1177/10534512211024939,
 abstract = {States increasingly are adopting computer science standards to help students develop coding and computational thinking skills. In an effort to support teachers in introducing computer science content to their students with high-incidence disabilities, a new model, computer science integration planning plus universal design for learning (CSIP+), offers ways to integrate computational thinking and coding into content area instruction. This column presents an example of how a teacher might implement the CSIP+ model when designing instruction accessible to all learners. Guiding questions to support teachers at each phase of the planning cycle are provided.},
 author = {Amy Hutchison and Anya S. Evmenova},
 doi = {10.1177/10534512211024939},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/10534512211024939},
 journal = {Intervention in School and Clinic},
 number = {4},
 pages = {262–267},
 title = {Planning Computer Science Instruction for Students With High-Incidence Disabilities},
 url = {https://doi-org.crai.referencistas.com/10.1177/10534512211024939},
 volume = {57},
 year = {2022f}
}

@article{doi:10.1177/105345128401900508,
 abstract = {The use of story problems in nontraditional ways can help LD students develop thinking skills.},
 author = {Cheri Hoy},
 doi = {10.1177/105345128401900508},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/105345128401900508},
 journal = {Academic Therapy},
 number = {5},
 pages = {567–572},
 title = {Story Problems: Vehicles to Develop Thinking Skills},
 url = {https://doi-org.crai.referencistas.com/10.1177/105345128401900508},
 volume = {19},
 year = {1984g}
}

@article{doi:10.1177/1053825919887407,
 abstract = {Background: Women and people of color are consistently underrepresented in science, technology, engineering, and math (STEM) fields and careers. Though there are myriad factors underlying these gaps, one potential variable may be the extent to which these students feel connected to their STEM classroom experiences. Purpose: The present study investigated the potential of a service-learning experience to support STEM engagement for underrepresented youth. Methodology/Approach: Two cohorts of high school students participated in a summer program through which they built “solar suitcases” to provide electricity for communities in need. Observations and student surveys measured student engagement and changes in STEM attitudes, dispositions, and beliefs. Findings/Conclusions: Students were highly engaged in service-learning activities and demonstrated improved Science Fascination, Science Values, and STEM Career Affinity after participating in the program. Implications: Implications for the design and delivery of culturally relevant service-learning experiences are discussed.},
 author = {Melissa A. Collins and Joanna Totino and Ardice Hartry and Valeria F. Romero and Rosio Pedroso and Rosalinda Nava},
 doi = {10.1177/1053825919887407},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1053825919887407},
 journal = {Journal of Experiential Education},
 number = {1},
 pages = {55–70},
 title = {Service-Learning as a Lever to Support STEM Engagement for Underrepresented Youth},
 url = {https://doi-org.crai.referencistas.com/10.1177/1053825919887407},
 volume = {43},
 year = {2020b}
}

@article{doi:10.1177/105678790000900208,
 abstract = {Fragmentation is the process by which we take things apart and study the parts as a means to understanding the whole. This is an appropriate model for mechanical systems, but for organizations, it is the interaction among the parts that makes things happen. —(Graff, 1995)},
 author = {William K. Poston},
 doi = {10.1177/105678790000900208},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/105678790000900208},
 journal = {International Journal of Educational Reform},
 number = {2},
 pages = {163–175},
 title = {Superintendent Compensation and System Quality: Exploring a Chronicle of Wishful Thinking},
 url = {https://doi-org.crai.referencistas.com/10.1177/105678790000900208},
 volume = {9},
 year = {2000m}
}

@article{doi:10.1177/10567879221076077,
 abstract = {Computational Thinking (CT) and the understanding of how programs are being executed is internationally acknowledging as a necessity for today’s students and citizens of tomorrow. Despite the multifaceted nature of CT, the introduction of CT and associate concepts such as coding is regarded as developmental acceptable for preschool and kindergarten children. For a decade, there has been a focus on educational reform in the form of educational apps. For young children, an influx of mobile apps offering various interfaces and styles promote themselves as having educational value to introduce children aged 5–7 to essential CT, coding, and problem-solving skills. On the contrary, little is known about the educational value of these apps. The fast pace at which developers produce these apps and the breadth of the available apps have gone beyond what it is reasonable for researchers and experts in the domain to evaluate. This article presents a literature review on how the ScratchJr app affects young children’s CT, coding, and general literacy skills. The literature review includes 18 studies. The main conclusion is that although ScratchJr is not a panacea, it seems to be a helpful app that positively affects children’s CT and coding skills.},
 author = {Papadakis Stamatios},
 doi = {10.1177/10567879221076077},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/10567879221076077},
 journal = {International Journal of Educational Reform},
 number = {1},
 pages = {28–61},
 title = {Can Preschoolers Learn Computational Thinking and Coding Skills with ScratchJr? A Systematic Literature Review},
 url = {https://doi-org.crai.referencistas.com/10.1177/10567879221076077},
 volume = {33},
 year = {2024p}
}

@article{doi:10.1177/1056789508090748,
 abstract = {The primary objective of this investigation is to develop efficient and robust computational schemes for a damage-coupled cyclic thermoviscoplasticity model for solder materials. Three constitutive integration algorithms, Euler, modified Euler, and semi-implicit algorithm for the model are examined. The three algorithms for the model are coded in the commercial finite element (FE) code ABAQUS (version 6.21) via its user-defined material subroutine UMAT. Two single-step algorithms of the substep scheme are applied for the modified Euler algorithm to control the error in the integration of constitutive laws. A semi-empirical formulation is established for an adaptive time stepping algorithm that is based on the Euler algorithm. The simulations of single-element, miniature specimen and notched specimen simulations have been conducted and compared with the test results under monotonic tensile, creep, and fatigue tests of 63Sn-37Pb solder. It is observed that the explicit algorithm consistently requires much less CPU time than others. The modified Euler algorithm has shown, on the other hand, to be not only efficient but also accurate. The semi-implicit algorithm yields an accurate solution. It is worth noting that the method is also effective by applying an appropriate integration scheme.},
 author = {A.H. Zhao and C.L. Chow},
 doi = {10.1177/1056789508090748},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1056789508090748},
 journal = {International Journal of Damage Mechanics},
 number = {6},
 pages = {507–532},
 title = {Computational Algorithms for a Damage-coupled Cyclic Viscoplasticity Material Model},
 url = {https://doi-org.crai.referencistas.com/10.1177/1056789508090748},
 volume = {18},
 year = {2009t}
}

@article{doi:10.1177/1056789509359676,
 abstract = {The major objective of this study is to develop a computational fatigue test method for welded structures. The damage mechanics approach is used to derive explicit expressions for fatigue life estimation within the framework of continuum mechanics. Microdamage model is applied to calculate the quasi-brittle material damage prior to the crack initiation. Based on the two-scale model, that is, micro versus meso (macro) scale linkage scheme is implemented in conjunction with finite element method to describe the crack initiation or propagation. For the material damage model used at microscale, a finite element code is developed. Lemaitre’s unified damage model and constitutive equation were chosen so as to ensure the efficiency for developing finite element algorithm. Identification method for material parameters required to constitutive equation is proposed using inversion of Ramberg-Osgood type constitutive model. In order to consider weld-induced residual stresses, the Ueda’s inherent strain scheme is incorporated into the calculation of residual stresses. Computational procedure based on two-scale model with the use of commercial finite element code is successfully constructed. The numerical results obtained by proposed method are compared with experimental results for validation.},
 author = {Chi Seung Lee and Myung Hyun Kim and Jae Myung Lee and Mahen Mahendran},
 doi = {10.1177/1056789509359676},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1056789509359676},
 journal = {International Journal of Damage Mechanics},
 number = {3},
 pages = {423–463},
 title = {Computational Study on the Fatigue Behavior of Welded Structures},
 url = {https://doi-org.crai.referencistas.com/10.1177/1056789509359676},
 volume = {20},
 year = {2011a}
}

@article{doi:10.1177/1056789511411739,
 author = {Ali Nazari and Gholamreza Khalaj and Neda Didehvar},
 doi = {10.1177/1056789511411739},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1056789511411739},
 journal = {International Journal of Damage Mechanics},
 number = {5},
 pages = {623–646},
 title = {RETRACTED: Computational Investigations of the Impact Resistance of Aluminum–Epoxy–Laminated Composites},
 url = {https://doi-org.crai.referencistas.com/10.1177/1056789511411739},
 volume = {21},
 year = {2012m}
}

@article{doi:10.1177/1056789513500295,
 abstract = {This paper describes the formulation and numerical implementation of a family of anisotropic and unilateral damage models for the prediction of damage and final rupture in engineering structures. The damage can be load oriented, microstructure oriented, or (for the first time within this modeling framework) softening. The local equations are solved using a combination of fixed-point and Newton–Raphson algorithms, whose efficiencies are drastically improved through Aitken’s relaxation and BFGS approximation. A delay-effect method is used to control the localization of damage, which leads to an objective calculation of the final rupture of structures.},
 author = {Martin Genet and Lionel Marcin and Pierre Ladevèze},
 doi = {10.1177/1056789513500295},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1056789513500295},
 journal = {International Journal of Damage Mechanics},
 number = {4},
 pages = {483–506},
 title = {On structural computations until fracture based on an anisotropic and unilateral damage theory},
 url = {https://doi-org.crai.referencistas.com/10.1177/1056789513500295},
 volume = {23},
 year = {2014g}
}

@article{doi:10.1177/1056789515580184,
 abstract = {Calcium silicate hydrate Jennite is a molecular structure commonly accepted as a representation of the complex calcium silicate hydrate gel formed during the hydration of typical Portland cement. In this paper, the behavior of nanoscale calcium silicate hydrate Jennite under shear deformation was investigated using molecular dynamics simulations. Computational samples representing the nanoscale structure of calcium silicate hydrate Jennite were subjected to shear deformation in order to investigate not only their mechanical properties but also their deformation behavior. The simulation results indicated that the nanoscale calcium silicate hydrate Jennite under shear deformation displays a linear elastic behavior up to shear stress of approximately 1.0 GPa, and shear deformation of about 0.08 radians, after which point yielding and plastic deformation occurs. The shear modulus determined from the simulations was 11.2 ± 0.7 GPa. The deformation-induced displacements in molecular structures were analyzed dividing the system in regions representing calcium oxide layers. The displacement/deformation of the layers of calcium oxide forming the structure of nanoscale calcium silicate hydrate Jennite was analyzed. The non-linear stress–strain behavior in the molecular structure was attributed to a non-linear increase in the displacement due to sliding of the calcium oxide layers on top of each other with higher shearing. These results support the idea that by controlling the chemical reactions, the tailored morphologies can be used to increase the interlinking between the calcium oxide layers, thus minimizing the shearing of the layers and leading to molecular structures that can withstand larger deformation and have improved failure behavior.},
 author = {John S Rivas Murillo and Ahmed Mohamed and Wayne Hodo and Ram V Mohan and A Rajendran and R Valisetty},
 doi = {10.1177/1056789515580184},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1056789515580184},
 journal = {International Journal of Damage Mechanics},
 number = {1},
 pages = {98–114},
 title = {Computational modeling of shear deformation and failure of nanoscale hydrated calcium silicate hydrate in cement paste: Calcium silicate hydrate Jennite},
 url = {https://doi-org.crai.referencistas.com/10.1177/1056789515580184},
 volume = {25},
 year = {2016r}
}

@article{doi:10.1177/1056789515610707,
 abstract = {This paper is the second in a series implementing initial strain–energy-based thermo-elastoviscoplastic isotropic damage–self-healing formulations for bituminous composites to compare the model predictions with experimental measurements. Computational algorithms based on the two-step operator splitting methodology are systematically developed and employed for numerical simulations. The elastic damage self-healing predictor and the viscoplastic corrector coupled with the Arrhenius-type temperature term via the net stress concept in conjunction with the hypothesis of strain equivalence are entirely considered as numerical implementation of the models. Several numerical examples of one-dimensional driver problems are first presented to show the effect of temperature and rest period for healing. Experimental validation of the proposed formulations against monotonic constant–strain test under different temperatures and controlled-strain cyclic tension test is presented. Qualitative and quantitative agreement between experimental results and numerical simulations is also observed. In particular, the softening behavior of the bituminous materials is well predicted for monotonic constant–strain rate test. The viscous damage behavior can be reasonably captured by the proposed damage models with the Arrhenius-type temperature term and the step-by-step computational algorithms.},
 author = {S Hong and KY Yuan and JW Ju},
 doi = {10.1177/1056789515610707},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1056789515610707},
 journal = {International Journal of Damage Mechanics},
 number = {5},
 pages = {672–696},
 title = {New strain energy-based thermo-elastoviscoplastic isotropic damage–self-healing model for bituminous composites—Part II: Computational aspects},
 url = {https://doi-org.crai.referencistas.com/10.1177/1056789515610707},
 volume = {26},
 year = {2017k}
}

@article{doi:10.1177/1056789516663614,
 abstract = {Continuum damage mechanics models have been used for predicting failure in composites for several years now. However, their application to natural fibre composites is quite recent. In this work, an approach for evaluating damage processes in natural fibre composite has been developed by combining experimentally quantified damage rules with discrete representations of fabric geometry. This approach is demonstrated by predicting the tensile failure of both thermoplastic-based and thermoset-based flax fabric composites. Numerical models have been developed to estimate the damage rules from the fibre, matrix and interface properties incorporated into representative volume element (RVE) models of the composites. While both the experimentally quantified and numerically obtained sets of damage rules yield satisfactory predictions, the use of these numerical rules leads to predictions that are close to, and in some cases, better than those obtained from the quantified rules.},
 author = {SM Panamoottil and R Das and K Jayaraman},
 doi = {10.1177/1056789516663614},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1056789516663614},
 journal = {International Journal of Damage Mechanics},
 number = {1},
 pages = {120–137},
 title = {Experimentally quantified and computational anisotropic damage rules for flax fabric composites},
 url = {https://doi-org.crai.referencistas.com/10.1177/1056789516663614},
 volume = {27},
 year = {2018n}
}

@article{doi:10.1177/1056789519894379,
 abstract = {A detailed micromechanical finite element analysis methodology is presented to predict the transverse tensile (fiber perpendicular) failure behavior of a unidirectional (UD) glass fiber-reinforced plastic composite ply. In order to understand the constituent-level stress–strain and damage behavior, finite element analysis is accomplished using representative volume element (RVE) that consists of random fiber distribution as observed in the microscopic image of an actual composite ply. For modeling the fiber/matrix interface failure behavior, cohesive zone module (cohesive surface/cohesive element) of Abaqus® is used. In order to capture the epoxy matrix stiffness and strength degradation, the following two different approaches are used: (i) initially, the linear Drucker–Prager plasticity model in combination with a ductile fracture criterion is used; (ii) later, a brittle failure approach such as the quadratic normal stress criterion within the framework of eXtended finite element method is used. From the detailed micromechanical analysis of the RVE, it is observed that the initial damage in the RVE occurs in the form of fiber/matrix interface decohesion. With increasing tensile load, interface crack propagates and creates a stress concentration region in the matrix material, adjacent to the crack tip. Further load application causes both interface crack tip and matrix stress concentration to move away from the load application direction. As soon as the interface crack tip reaches approximately 60° to 70° away from the load application direction, the conjunction of the matrix damage with the interface crack leads to the RVE final failure. The predicted average stress–strain curves from the above-mentioned two different epoxy matrix failure criterions (ductile and brittle) correlate very well with the experimental results, indicating that the brittle failure behavior of a UD fiber-reinforced plastic composite ply under transverse tensile load is mainly controlled by the fiber/matrix interface properties.},
 author = {Akash Sharma and Subbareddy Daggumati},
 doi = {10.1177/1056789519894379},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1056789519894379},
 journal = {International Journal of Damage Mechanics},
 number = {6},
 pages = {943–964},
 title = {Computational micromechanical modeling of transverse tensile damage behavior in unidirectional glass fiber-reinforced plastic composite plies: Ductile versus brittle fracture mechanics approach},
 url = {https://doi-org.crai.referencistas.com/10.1177/1056789519894379},
 volume = {29},
 year = {2020q}
}

@article{doi:10.1177/1059712307078661,
 abstract = {Recent work by Amso and Johnson (Developmental Psychology, 42(6), 1236—1245, 2006) implicates the role of visual selective attention in the development of perceptual completion during early infancy. In the current article, we extend this finding by simulating the performance of 3-month-old infants on a visual search task, using a multi-channel, image-filtering model of early visual processing. Model parameters were systematically varied to simulate developmental change in three neural components of visual selective attention: degree of oculomotor noise, growth of horizontal connections in visual cortex, and duration of recurrent processing in parietal cortex. While two of the three components—horizontal connections and recurrent parietal processing—are each able to account for the visual search performance of 3-month-olds, recurrent parietal processing also suggests a coherent pattern of developmental change in visual selective attention during early infancy. We conclude by highlighting plausible neural mechanisms for modulating recurrent parietal activity, including the development of feedback from prefrontal cortex.},
 author = {Matthew Schlesinger and Dima Amso and Scott P. Johnson},
 doi = {10.1177/1059712307078661},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1059712307078661},
 journal = {Adaptive Behavior},
 number = {2},
 pages = {135–148},
 title = {The Neural Basis for Visual Selective Attention in Young Infants: A Computational Account},
 url = {https://doi-org.crai.referencistas.com/10.1177/1059712307078661},
 volume = {15},
 year = {2007p}
}

@article{doi:10.1177/1059712309342757,
 abstract = {In this article we propose a computational model that describes how observed behavior can influence an observer’s own behavior, including the acquisition of new task descriptions. The sources of influence on our model’s behavior are: beliefs about the world’s possible states and actions causing transitions between them; baseline preferences for certain actions; a variable tendency to infer and share goals in observed behavior; and a variable tendency to act efficiently to reach rewarding states. Acting on these premises, our model is able to replicate key empirical studies of social learning in children and chimpanzees. We demonstrate how a simple artificial system can account for a variety of biological social transfer phenomena, such as goal-inference and over-imitation, by taking into account action constraints and incomplete knowledge about the world dynamics.},
 author = {Manuel Lopes and Francisco S. Melo and Ben Kenward and José Santos-Victor},
 doi = {10.1177/1059712309342757},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1059712309342757},
 journal = {Adaptive Behavior},
 number = {6},
 pages = {467–483},
 title = {A Computational Model of Social-Learning Mechanisms},
 url = {https://doi-org.crai.referencistas.com/10.1177/1059712309342757},
 volume = {17},
 year = {2009o}
}

@article{doi:10.1177/1059712312445902,
 abstract = {Recent neuroscientific evidence in human and non-human primates indicates that the regions that become active during motor execution and motor observation overlap extensively in the cerebral cortex. This suggests that to observe an action, these primates employ their motor and somatosensation areas in order to simulate it internally. In line with this finding, in the current paper, we examine relevant neuroscientific evidence in order to design a computational agent that can facilitate observational learning of reaching movements. For this reason, we develop a novel motor control system, inspired from contemporary theories of motor control, and demonstrate how it can be used during observation to facilitate learning, without the active involvement of the agent’s body. Our results show that novel motor skills can be acquired only by observation, by optimizing the peripheral components of the agent’s motion.},
 author = {Emmanouil Hourdakis and Panos Trahanias},
 doi = {10.1177/1059712312445902},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1059712312445902},
 journal = {Adaptive Behavior},
 number = {4},
 pages = {237–256},
 title = {Computational modeling of observational learning inspired by the cortical underpinnings of human primates},
 url = {https://doi-org.crai.referencistas.com/10.1177/1059712312445902},
 volume = {20},
 year = {2012i}
}

@article{doi:10.1177/1059712313488782,
 abstract = {Mental rotation concerns the cognitive processes that allow an agent mentally to rotate the image of an object in order to solve a given task, for example to say if two objects with different orientations are the same or different. Here we present a system-level bio-constrained model, developed within a neurorobotics framework, that provides an embodied account of mental rotation processes relying on neural mechanisms involving motor affordance encoding, motor simulation and the anticipation of the sensory consequences of actions (both visual and proprioceptive). This model and methodology are in agreement with the most recent theoretical and empirical research on mental rotation. The model was validated through experiments with a simulated humanoid robot (iCub) engaged in solving a classical mental rotation test. The results of the test show that the robot is able to solve the task and, in agreement with data from psychology experiments, exhibits response times linearly dependent on the angular disparity between the objects. This model represents a novel detailed operational account of the embodied brain mechanisms that may underlie mental rotation.},
 author = {Kristsana Seepanomwan and Daniele Caligiore and Gianluca Baldassarre and Angelo Cangelosi},
 doi = {10.1177/1059712313488782},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1059712313488782},
 journal = {Adaptive Behavior},
 number = {4},
 pages = {299–312},
 title = {Modelling mental rotation in cognitive robots},
 url = {https://doi-org.crai.referencistas.com/10.1177/1059712313488782},
 volume = {21},
 year = {2013q}
}

@article{doi:10.1177/1059712313488789,
 abstract = {Recent experimental evidence indicates that animals can use mental simulation to make decisions about the actions to take during goal-directed navigation. The principal brain areas found to be active during this process are the hippocampus, the ventral striatum and the sensory-motor cortex. In this paper, we present a computational model that includes biological aspects of this circuit and explains mechanistically how it may be used to imagine and evaluate future events. Its most salient characteristic is that choices about actions are made by simulating movements and their sensory effects using the same brain areas that are active during overt execution. More precisely, the simulation of an action (e.g., walking) creates a new sensory pattern that is evaluated in the same way as real inputs. The model is validated in a navigation task in which a simulated rat is placed in a complex maze. We show that hippocampal and striatal cells are activated to simulate paths, to retrieve their estimated value and to make decisions. We link these results with a general framework that sees the brain as a predictive device that can ‘detach’ itself from the here-and-now of current perception using mechanisms such as episodic memories, motor and visual imagery.},
 author = {Fabian Chersi and Francesco Donnarumma and Giovanni Pezzulo},
 doi = {10.1177/1059712313488789},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1059712313488789},
 journal = {Adaptive Behavior},
 number = {4},
 pages = {251–262},
 title = {Mental imagery in the navigation domain: a computational model of sensory-motor simulation mechanisms},
 url = {https://doi-org.crai.referencistas.com/10.1177/1059712313488789},
 volume = {21},
 year = {2013d}
}

@article{doi:10.1177/1059712315589355,
 abstract = {Whether animals behave optimally is an open question of great importance, both theoretically and in practice. Attempts to answer this question focus on two aspects of the optimization problem, the quantity to be optimized and the optimization process itself. In this paper, we assume the abstract concept of cost as the quantity to be minimized and propose a reinforcement learning algorithm, called Value-Gradient Learning (VGL), as a computational model of behavior optimality. We prove that, unlike standard models of Reinforcement Learning, Temporal Difference in particular, VGL is guaranteed to converge to optimality under certain conditions. The core of the proof is the mathematical equivalence of VGL and Pontryagin’s Minimum Principle, a well-known optimization technique in systems and control theory. Given the similarity between VGL’s formulation and regulatory models of behavior, we argue that our algorithm may provide psychologists with a tool to formulate such models in optimization terms.},
 author = {Eduardo Alonso and Michael Fairbank and Esther Mondragón},
 doi = {10.1177/1059712315589355},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1059712315589355},
 journal = {Adaptive Behavior},
 number = {4},
 pages = {206–215},
 title = {Back to optimality: a formal framework to express the dynamics of learning optimal behavior},
 url = {https://doi-org.crai.referencistas.com/10.1177/1059712315589355},
 volume = {23},
 year = {2015b}
}

@article{doi:10.1177/1059712317726357,
 abstract = {J. J. Gibson’s concept of affordance, one of the central pillars of ecological psychology, is a truly remarkable idea that provides a concise theory of animal perception predicated on environmental interaction. It is thus not surprising that this idea has also found its way into robotics research as one of the underlying theories for action perception. The success of the theory in this regard has meant that existing research is both abundant and diffuse by virtue of the pursuit of multiple different paths and techniques with the common goal of enabling robots to learn, perceive, and act upon affordances. Up until now, there has existed no systematic investigation of existing work in this field. Motivated by this circumstance, in this article, we begin by defining a taxonomy for computational models of affordances rooted in a comprehensive analysis of the most prominent theoretical ideas of import in the field. Subsequently, after performing a systematic literature review, we provide a classification of existing research within our proposed taxonomy. Finally, by both quantitatively and qualitatively assessing the data resulting from the classification process, we highlight gaps in the research terrain and outline open questions for the investigation of affordances in robotics that we believe will help inform future work, prioritize research goals, and potentially advance the field toward greater robot autonomy.},
 author = {Philipp Zech and Simon Haller and Safoura Rezapour Lakani and Barry Ridge and Emre Ugur and Justus Piater},
 doi = {10.1177/1059712317726357},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1059712317726357},
 journal = {Adaptive Behavior},
 number = {5},
 pages = {235–271},
 title = {Computational models of affordance in robotics: a taxonomy and systematic classification},
 url = {https://doi-org.crai.referencistas.com/10.1177/1059712317726357},
 volume = {25},
 year = {2017t}
}

@article{doi:10.1177/1059712319839386,
 abstract = {Applied behavior analysis (ABA) is a behavioral science that aims to teach specific and socially relevant behaviors to people with different repertoires. Technology can aid behavior analysis interventions supporting different educational agents (e.g. psychologists, parents, and teachers) as well as patients. In this article, we reviewed how researchers are using technology in ABA interventions with children with autism spectrum disorder (ASD). We present the results obtained from a systematic review of the literature. The purpose was to map the employment of the computational technology in ABA interventions, as well as to discuss the primary technologies used, their benefits, and their limitations. Based on our review, we noticed that technology-based interventions are in the early stages of development. We evidenced it by the methodological limitations in many of the studies we found, and the relatively simplistic nature of many of the technological applications (e.g. inability to customize the software to meet individual learner needs). We also provide suggestions, based on these findings, for how researchers may advance the studies in this area to improve the lives of children with ASD and their families.},
 author = {Diogo Fernando Trevisan and Lorraine Becerra and Priscila Benitez and Thomas S Higbee and João Paulo Gois},
 doi = {10.1177/1059712319839386},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1059712319839386},
 journal = {Adaptive Behavior},
 number = {3},
 pages = {183–196},
 title = {A review of the use of computational technology in applied behavior analysis},
 url = {https://doi-org.crai.referencistas.com/10.1177/1059712319839386},
 volume = {27},
 year = {2019q}
}

@article{doi:10.1177/1059712320950539,
 abstract = {How the boundaries of the mind should be drawn with respect to action and the material world is a core research question that cognitive archaeology shares with contemporary cognitive sciences. The study of hominin technical thinking, as in the case of stone tool making, is a good way to bring that question to the fore. This article argues that archaeologists who study lithic artefacts and their transformations over the course of human evolution are uniquely well positioned to contribute to the ongoing debate about the marks of the mental. Adopting the material engagement approach, I propose to replace the internalist vision of mentality, that is, the vision of a brain-bound mind that is using the body to execute and externalise preconceived mental plan through the stone, with an ecological-enactive vision of participatory mentality where bodily acts and materials act together to generate rather than merely execute thought processes. I argue that the latter participatory view changes the geography of the cognitive and offers a better description for the continuity of mind and matter that we see in the lithic record.},
 author = {Lambros Malafouris},
 doi = {10.1177/1059712320950539},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1059712320950539},
 journal = {Adaptive Behavior},
 number = {2},
 pages = {107–121},
 title = {How does thinking relate to tool making?},
 url = {https://doi-org.crai.referencistas.com/10.1177/1059712320950539},
 volume = {29},
 year = {2021l}
}

@article{doi:10.1177/1059712321999421,
 abstract = {General value functions (GVFs) in the reinforcement learning (RL) literature are long-term predictive summaries of the outcomes of agents following specific policies in the environment. Affordances as perceived action possibilities with specific valence may be cast into predicted policy-relative goodness and modeled as GVFs. A systematic explication of this connection shows that GVFs and especially their deep-learning embodiments (1) realize affordance prediction as a form of direct perception, (2) illuminate the fundamental connection between action and perception in affordance, and (3) offer a scalable way to learn affordances using RL methods. Through an extensive review of existing literature on GVF applications and representative affordance research in robotics, we demonstrate that GVFs provide the right framework for learning affordances in real-world applications. In addition, we highlight a few new avenues of research opened up by the perspective of “affordance as GVF,” including using GVFs for orchestrating complex behaviors.},
 author = {Daniel Graves and Johannes Günther and Jun Luo},
 doi = {10.1177/1059712321999421},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1059712321999421},
 journal = {Adaptive Behavior},
 number = {4},
 pages = {307–327},
 title = {Affordance as general value function: a computational model},
 url = {https://doi-org.crai.referencistas.com/10.1177/1059712321999421},
 volume = {30},
 year = {2022i}
}

@article{doi:10.1177/105971239400200402,
 abstract = {Work in human infancy and behavior-based robotics that grounds intelligent abilities in sensorimotor exchanges between a system and its environment shares recurrent problems of when, whether, and how scaling up from basic to supposedly higher abilities is possible. An action-based model of the infant is introduced that converges with features of independently motivated animat models exploiting emergent functionality and challenges alternatives that invoke conceptual representations. Adaptive change routinely exhibited in infants’ everyday activities outstrips the scaling-up potential of current robotic systems and clarifies effective principles obeyed by naturally intelligent systems. A general form is outlined to subject-environment interaction that “engineers” restructuring of early abilities in the direction of greater anticipation (considered an upper boundary for the competence of concept-free human and animat systems); and an action-based account of the phenomena is provided. This emphasizes the relationship between representation and situated inference and the role of reciprocal constraints between cognitive and physical-motor mechanisms. Finally, this article questions how far typical self organizing connectionist networks take us toward understanding a system that is capable of mapping recurrent viable patterns of activity into more permanent adaptive changes.},
 author = {Julie C. Rutkowska},
 doi = {10.1177/105971239400200402},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/105971239400200402},
 journal = {Adaptive Behavior},
 number = {4},
 pages = {349–373},
 title = {Scaling Up Sensorimotor Systems: Constraints from Human Infancy},
 url = {https://doi-org.crai.referencistas.com/10.1177/105971239400200402},
 volume = {2},
 year = {1994p}
}

@article{doi:10.1177/105971239800600302,
 abstract = {Psychological experiments on children’s development of spatial knowledge suggest that experience at self-locomotion and visual tracking are important factors. Yet, the mechanism underlying development is unknown. We propose a robot that learns to track a target object mentally (i.e., maintaining a representation of an object’s position when outside the field of view) as a model for spatial development. Mental tracking is considered as prediction of an object’s position, given the previous environmental state and motor commands and the current environment state resulting from movement. Following Jordan and Rumelhart’s (1992) forward modeling architecture, the system consists of two components: an inverse model of sensory input to desired motor commands and a forward model of motor commands to desired sensory input (goals). The robot was tested on the “three cups”paradigm (in which children are required, under various movement conditions, to select the cup containing the hidden object). Consistent with child development, in the absence of the capacity for self-locomotion, the robot makes errors that are self-center-based. When given the ability for self-locomotion, the robot responds allocentrically.},
 author = {Kazuo Hiraki and Akio Sashima and Steven Phillips},
 doi = {10.1177/105971239800600302},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/105971239800600302},
 journal = {Adaptive Behavior},
 number = {3–4},
 pages = {371–391},
 title = {From Egocentric to Allocentric Spatial Behavior: A Computational Model of Spatial Development},
 url = {https://doi-org.crai.referencistas.com/10.1177/105971239800600302},
 volume = {6},
 year = {1998j}
}

@article{doi:10.1177/105971239900700203,
 abstract = {The ability to acquire a representation of the spatial environment and the ability to localize within it are essential for successful navigation in a-priori unknown environments. The hippocampal forma tion is believed to play a key role in spatial learning and localization in animals in general and rodents in particular. This paper briefly reviews the relevant neurobiological and cognitive data, and their relation to computational models of spatial learning and localization used in contempo rary mobile robots. It proposes a hippocampal model of spatial learning and localization, and characterizes it using a Kalman filter based tool for information fusion from multiple uncertain sources. The resulting model not only explains neurobiological and behavioral data from rodent experiments, but also allows a robot to learn a place-based metric representation of space and to localize itself in a stochastically optimal manner. The paper presents an algorithmic implementa tion of the model and results of several experiments that demonstrate its capabilities. These include the ability to disambiguate perceptually similar places, scale well with increasing errors, and the automatic acquisition of spatial information at multiple resolutions.},
 author = {Karthik Balakrishnan and Olivier Bousquet and Vasant Honavar},
 doi = {10.1177/105971239900700203},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/105971239900700203},
 journal = {Adaptive Behavior},
 number = {2},
 pages = {173–216},
 title = {Spatial Learning and Localization in Rodents: A Computational Model of the Hippocampus and its Implications for Mobile Robots},
 url = {https://doi-org.crai.referencistas.com/10.1177/105971239900700203},
 volume = {7},
 year = {1999c}
}

@article{doi:10.1177/1069397110393894,
 abstract = {Mel Ember was co-Principal Investigator in the Mason-HRAF Joint Project on Eastern Africa, a multiyear project aimed at developing and analyzing advanced computational agent-based models of human societies across 10 countries and 12 ecosystems. A major unsolved challenge in this kind of social science research is to devise a systematic way to compare, contrast, and communicate different models of social dynamics along relevant dimensions and characteristics, given the inherent complexity of most computational agent-based models. This article proposes a viable systematic framework for comparing models and illustrates its application using some of the models that Mel helped inspire and develop as senior project participant.},
 author = {Claudio Cioffi-Revilla},
 doi = {10.1177/1069397110393894},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1069397110393894},
 journal = {Cross-Cultural Research},
 number = {2},
 pages = {208–230},
 title = {Comparing Agent-Based Computational Simulation Models in Cross-Cultural Research},
 url = {https://doi-org.crai.referencistas.com/10.1177/1069397110393894},
 volume = {45},
 year = {2011e}
}

@article{doi:10.1177/1070496512471947,
 abstract = {The paradox motivating this article is why California has acted globally by enacting a comprehensive mitigation policy to reduce the emissions of Greenhouse gases, a true public good since the benefits will be shared across the planet, but has not mustered the will to act locally through the adoption of an equally comprehensive adaptation policy for the state to protect its own public and private assets and interests. We attempt to explain the paradox by identifying what it is that differentiates climate change adaptation from mitigation, both substantively and politically. The paradox notwithstanding, we identify several imaginable adaptation policies and strategies that would be commensurate with individual and collective self-interested behavior.},
 author = {Daniel A. Mazmanian and John Jurewitz and Hal T. Nelson},
 doi = {10.1177/1070496512471947},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1070496512471947},
 journal = {The Journal of Environment & Development},
 number = {2},
 pages = {186–206},
 title = {The Paradox of “Acting Globally While Thinking Locally”: Discordance in Climate Change Adaption Policy},
 url = {https://doi-org.crai.referencistas.com/10.1177/1070496512471947},
 volume = {22},
 year = {2013o}
}

@article{doi:10.1177/1071181311551093,
 abstract = {The current generation of advanced robots essentially considers the external world, including humans, vehicles, and other robots, as navigational issues rather than as team members, opponents, or part of the ambient culture. But robotic systems need true Soldier-Robot Teams where each part of the team understands the roles, responsibilities, and required actions of the others and has the capability to provide the communication necessary to make the team successful. Accomplishing this within a mission context, accepted military doctrine, and social norms of the society in which the human-robot teams operate, represents a major theoretical and technological challenge for research in human-robot teams.},
 author = {Randall Shumaker},
 doi = {10.1177/1071181311551093},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1071181311551093},
 journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
 number = {1},
 pages = {454–455},
 title = {From Teleoperation to Teammate: Applying Theory and Method from the Cognitive and Computational Sciences to Create Human-Robot Teams},
 url = {https://doi-org.crai.referencistas.com/10.1177/1071181311551093},
 volume = {55},
 year = {2011p}
}

@article{doi:10.1177/1071181312561043,
 abstract = {As air traffic control (ATC) becomes increasingly automated, software designers need to know how air traffic controllers process information as they manage operations in today’s system. Extracting knowledge from today’s controller workforce and representing that knowledge in the form of mental computations are essential steps toward needs assessment and development of advanced decision-aiding tools and technologies. A recent task analysis documented information derived by controllers from their cognitive integration of displayed information. This work envisions future, more detailed analyses of the controller’s mental computations as essential to identifying needs for advanced software tools, including predictive displays.},
 author = {Elizabeth D. Murphy and Harold A. Albert and Jennifer M. Chen and Gregory G. Anderson},
 doi = {10.1177/1071181312561043},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1071181312561043},
 journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
 number = {1},
 pages = {110–114},
 title = {The Role of Mental Computations in Current and Future En Route Air Traffic Control},
 url = {https://doi-org.crai.referencistas.com/10.1177/1071181312561043},
 volume = {56},
 year = {2012j}
}

@article{doi:10.1177/1071181312561292,
 abstract = {This paper proposes a framework for an evolutionary computation approach to supporting concept generation in engineering design. This approach attempts to assist designers in increasing quantity and diversity of design concepts. The framework integrates the Theory of Inventive Problem Solving (TRIZ) methodology into an evolution process. First, the product resources are analyzed. The relationships between the resources are constructed using Genetic Programming. Second, contradictions between the relationships are identified based on the physical features of the resources and their relationships. Finally, principles for resolving contradictions in the TRIZ matrix guide designers to generate alternative design solutions. A case study is conducted to show how the approach works.},
 author = {Ganyun Sun and Shengji Yao},
 doi = {10.1177/1071181312561292},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1071181312561292},
 journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
 number = {1},
 pages = {1972–1976},
 title = {A Framework for an Evolutionary Computation Approach to Supporting Concept Generation},
 url = {https://doi-org.crai.referencistas.com/10.1177/1071181312561292},
 volume = {56},
 year = {2012t}
}

@article{doi:10.1177/1071181319631155,
 abstract = {There is a currently a shortage of computer science professionals and this shortage is projected to continue into the foreseeable future as not enough students are selecting computer science majors. Researchers and policy-makers agree that development of this career pipeline starts in elementary school. Our study examined which collaborative programming setup, pair programming (two students collaborate on one computer) or side-by-side programming (two students collaborate on the same program from two computers), fifth-grade students preferred. We also sought to understand why students preferred one method over the other and explored ideas on how to effectively design a collaborative programming environment for this age group. Our study had participants first engage in five instructional days, alternating between pair and side-by-side programming, and then conducted focus groups. We found that students overwhelmingly preferred side-by-side programming. We explain this using self-determination theory which states that behavior is motivated by three psychological needs: autonomy, competence, and psychological relatedness which side-by-side programming was better able to meet.},
 author = {Amanda Bradbury and Eric Wiebe and Jessica Vandenberg and Jennifer Tsan and Collin Lynch and Kristy Boyer},
 doi = {10.1177/1071181319631155},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1071181319631155},
 journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
 number = {1},
 pages = {493–497},
 title = {The Interface Design of a Collaborative Computer Science Learning Environment for Elementary Aged Students},
 url = {https://doi-org.crai.referencistas.com/10.1177/1071181319631155},
 volume = {63},
 year = {2019b}
}

@article{doi:10.1177/1071181320641068,
 abstract = {One advantage of highly automated vehicles is drivers can use commute time for non-driving tasks, such as work-related tasks. The potential for an auto-mobile office—a space where drivers work in automated vehicles—is a complex yet underexplored idea. This paper begins to define a design space of the auto- mobile office in SAE Level 3 automated vehicles by integrating the affinity diagram (AD) with a computational representation of the abstraction hierarchy (AH). The AD uses a bottom-up approach where researchers starting with individual findings aggregate and abstract those into higher-level concepts. The AH uses a top-down approach where researchers start with first principles to identify means-ends links between system goals and concrete forms of the system. Using the programming language R, the means-ends links of AH can be explored statistically. This computational approach to the AH provides a systematic means to define the design space of the auto-mobile office.},
 author = {Mengyao Li and Atefeh Katrahmani and Amudha V. Kamaraj and John D. Lee},
 doi = {10.1177/1071181320641068},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1071181320641068},
 journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
 number = {1},
 pages = {293–297},
 title = {Defining A Design Space of The Auto-Mobile Office: A Computational Abstraction Hierarchy Analysis},
 url = {https://doi-org.crai.referencistas.com/10.1177/1071181320641068},
 volume = {64},
 year = {2020i}
}

@article{doi:10.1177/1071181321651047,
 abstract = {This paper describes a methodology to design computational models to evaluate the workload for driving tasks. A computational model was configured for a driving scenario used in a pilot study that included a secondary task at varying levels of difficulty to increase the driver’s workload. The computational model results provided a workload analysis of the concurrent driving tasks. This analysis can be used to explain the experimental findings from subject experiments and to evaluate the workload trade-offs between primary and secondary driving tasks.},
 author = {Holly Handley and Deborah Thompson},
 doi = {10.1177/1071181321651047},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1071181321651047},
 journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
 number = {1},
 pages = {690–694},
 title = {Computational Models for Workload Analysis of Driving Tasks},
 url = {https://doi-org.crai.referencistas.com/10.1177/1071181321651047},
 volume = {65},
 year = {2021h}
}

@article{doi:10.1177/1071181322661031,
 abstract = {This panel introduces the current practice of human performance modeling and its applications to Human Factors research and discusses how we can promote modeling education. We argue that there is a necessity for new and more engaging learning tools and environments that can include qualitative and quantitative modeling work in Human Factors curricula to fulfill different levels of approaches in Human Factors research. We hope that this panel can encourage more students and junior researchers to utilize modeling in their work and spark exciting discussions for better modeling practice and education.},
 author = {Myounghoon Jeon and Yiqi Zhang and Shan Bao and Heejin Jeong},
 doi = {10.1177/1071181322661031},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1071181322661031},
 journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
 number = {1},
 pages = {1586–1590},
 title = {“Dummies, Learning Modeling Made Easy”: Improving Modeling Education in Human Factors Research},
 url = {https://doi-org.crai.referencistas.com/10.1177/1071181322661031},
 volume = {66},
 year = {2022i}
}

@article{doi:10.1177/10711813241276450,
 abstract = {Designing integrated systems of humans and automation involves envisioning how work will be performed, how it is affected by technological capabilities, and what will be helpful to support system performance. This is challenging as interactions and dynamics can create emergent effects that are difficult to predict. This demonstration aims to demonstrate the power of computational simulation in designing, verifying, and validating integrated systems of humans and automation. The demo will include an introduction to the simulation framework Work Models that Compute (WMC) used for our ongoing research, how to develop a work model in WMC, a demo of the simulation, and a demo of postprocessing visualization to analyze the simulation results. The approach can be applied in all stages of system design. With an example in aviation, we show how we use computational modeling and simulation to evaluate the robustness of envisioned operations of humans working with automated systems.},
 author = {Abhinay Paladugu and Alicia Fernandes and Martijn IJtsma},
 doi = {10.1177/10711813241276450},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/10711813241276450},
 journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
 number = {0},
 pages = {10711813241276450},
 title = {The Use of Computational Modeling and Simulation to Design and Evaluate a Distributed Work System},
 url = {https://doi-org.crai.referencistas.com/10.1177/10711813241276450},
 volume = {0},
 year = {2024i}
}

@article{doi:10.1177/1073858409354384,
 abstract = {The authors review evidence that spontaneous, that is, not stimulus or task driven, activity in the brain at the level of large-scale neural systems is not noise, but orderly and organized in a series of functional networks that maintain, at all times, a high level of coherence. These networks of spontaneous activity correlation or resting state networks (RSN) are closely related to the underlying anatomical connectivity, but their topography is also gated by the history of prior task activation. Network coherence does not depend on covert cognitive activity, but its strength and integrity relates to behavioral performance. Some RSN are functionally organized as dynamically competing systems both at rest and during tasks. Computational studies show that one of such dynamics, the anticorrelation between networks, depends on noise-driven transitions between different multistable cluster synchronization states. These multistable states emerge because of transmission delays between regions that are modeled as coupled oscillators systems. Large-scale systems dynamics are useful for keeping different functional subnetworks in a state of heightened competition, which can be stabilized and fired by even small modulations of either sensory or internal signals.},
 author = {Gustavo Deco and Maurizio Corbetta},
 doi = {10.1177/1073858409354384},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1073858409354384},
 journal = {The Neuroscientist},
 note = {PMID:21196530},
 number = {1},
 pages = {107–123},
 title = {The Dynamical Balance of the Brain at Rest},
 url = {https://doi-org.crai.referencistas.com/10.1177/1073858409354384},
 volume = {17},
 year = {2011e}
}

@article{doi:10.1177/1073858417750466,
 abstract = {High-level motor computations reflect abstract components far apart from the mere motor performance. Neural correlates of these computations have been explored both in nonhuman and human primates, supporting the idea that our brain recruits complex nodes for motor representations. Of note, these computations have exciting implications for social cognition, and they also entail important challenges in the context of autism. Here, we focus on these challenges benefiting from recent studies addressing motor interference, motor resonance, and high-level motor planning. In addition, we suggest new ideas about how one maps and shares the (motor) space with others. Taken together, these issues inspire intriguing and fascinating questions about the social tendency of our high-level motor computations, and this tendency may indicate that we are “motorically” wired to others. Thus, after furnishing preliminary insights on putative neural nodes involved in these computations, we focus on how the hypothesized social nature of high-level motor computations may be anomalous or limited in autism, and why this represents a critical challenge for the future.},
 author = {Luca Casartelli and Alessandra Federici and Emilia Biffi and Massimo Molteni and Luca Ronconi},
 doi = {10.1177/1073858417750466},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1073858417750466},
 journal = {The Neuroscientist},
 note = {PMID:29271293},
 number = {6},
 pages = {568–581},
 title = {Are We “Motorically” Wired to Others? High-Level Motor Computations and Their Role in Autism},
 url = {https://doi-org.crai.referencistas.com/10.1177/1073858417750466},
 volume = {24},
 year = {2018d}
}

@article{doi:10.1177/1073858419834517,
 abstract = {Processing rewarding and aversive signals lies at the core of many adaptive behaviors, including value-based decision making. The brain circuits processing these signals are widespread and include the prefrontal cortex, amygdala and striatum, and their dopaminergic innervation. In this review, we integrate historic findings on the behavioral and neural mechanisms of value-based decision making with recent, groundbreaking work in this area. On the basis of this integrated view, we discuss a neuroeconomic framework of value-based decision making, use this to explain the motivation to pursue rewards and how motivation relates to the costs and benefits associated with different courses of action. As such, we consider substance addiction and overeating as states of altered value-based decision making, in which the expectation of reward chronically outweighs the costs associated with substance use and food consumption, respectively. Together, this review aims to provide a concise and accessible overview of important literature on the neural mechanisms of behavioral adaptation to reward and aversion and how these mediate motivated behaviors.},
 author = {Jeroen P. H. Verharen and Roger A. H. Adan and Louk J. M. J. Vanderschuren},
 doi = {10.1177/1073858419834517},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1073858419834517},
 journal = {The Neuroscientist},
 note = {PMID:30866712},
 number = {1},
 pages = {87–99},
 title = {How Reward and Aversion Shape Motivation and Decision Making: A Computational Account},
 url = {https://doi-org.crai.referencistas.com/10.1177/1073858419834517},
 volume = {26},
 year = {2020r}
}

@article{doi:10.1177/107385849700300511,
 abstract = {Circuit dynamics are an outcome of the constant interplay between the intrinsic properties of neurons and the strengths of the synaptic connections among them. Examples from the small networks that generate the rhythms of the crustacean stomatogastric ganglion are used to illustrate general features of the different time scales over which both synaptic and intrinsic properties are altered by circuit activity and neuromodulation. These demonstrate that neither intrinsic membrane properties nor synaptic strengths should be considered fixed parameters; rather, they are dynamic variables with time scales that range from milliseconds to days. NEUROSCIENTIST 3:295–302, 1997},
 author = {Eve Marder},
 doi = {10.1177/107385849700300511},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/107385849700300511},
 journal = {The Neuroscientist},
 number = {5},
 pages = {295–302},
 title = {Computational Dynamics in Rhythmic Neural Circuits},
 url = {https://doi-org.crai.referencistas.com/10.1177/107385849700300511},
 volume = {3},
 year = {1997f}
}

@article{doi:10.1177/1075547014556540,
 abstract = {Parallel advances in communication and visualization technologies have enabled the study and visualization of human behavior at a scale and level of detail never before possible. Nowhere are these advances more evident than within the emerging field of computational social science. Using Adamic and Glance’s image of the political blogosphere as an example and social representations theory as a guiding framework, we explore how computational social science visualizations may aid and complicate public understanding of this new science. We conclude with a discussion of best practices for the production and reuse of computational social science images for public consumption.},
 author = {Brooke Foucault Welles and Isabel Meirelles},
 doi = {10.1177/1075547014556540},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1075547014556540},
 journal = {Science Communication},
 number = {1},
 pages = {34–58},
 title = {Visualizing Computational Social Science: The Multiple Lives of a Complex Image},
 url = {https://doi-org.crai.referencistas.com/10.1177/1075547014556540},
 volume = {37},
 year = {2015i}
}

@article{doi:10.1177/10755470231165941,
 abstract = {This research note describes ByrdBot, a science communication tool that leverages bird songs to communicate data regarding human impacts on the environment. With ByrdBot, listeners can compare simulated soundscapes of 1970, 2017, and 2065 to immediately, and viscerally, experience decades of past or projected future environmental change. The communication tactic of ByrdBot—what we call emergent sonification—is discussed as one that capitalizes on computational media to facilitate attunement to nonhuman voices and, subsequently, to offer an affective grasping of the impacts of such phenomena as habitat destruction and climate change on wildlife displacement and loss.},
 author = {Miles C. Coleman and Brandon Simon and Matt Pierce and Charles A. Schutte},
 doi = {10.1177/10755470231165941},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/10755470231165941},
 journal = {Science Communication},
 number = {2},
 pages = {252–266},
 title = {Emergent Sonification: Using Computational Media to Communicate the Anthropocene in ByrdBot},
 url = {https://doi-org.crai.referencistas.com/10.1177/10755470231165941},
 volume = {45},
 year = {2023b}
}

@article{doi:10.1177/107621750703000403,
 doi = {10.1177/107621750703000403},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/107621750703000403},
 journal = {Gifted Child Today},
 number = {4},
 pages = {6–9},
 title = {News Briefs},
 url = {https://doi-org.crai.referencistas.com/10.1177/107621750703000403},
 volume = {30},
 year = {2007w}
}

@article{doi:10.1177/107621751003300107,
 author = {Thomas R. Tretter},
 doi = {10.1177/107621751003300107},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/107621751003300107},
 journal = {Gifted Child Today},
 number = {1},
 pages = {16–26},
 title = {Systematic and Sustained: Powerful Approaches for Enhancing Deep Mathematical Thinking},
 url = {https://doi-org.crai.referencistas.com/10.1177/107621751003300107},
 volume = {33},
 year = {2010s}
}

@article{doi:10.1177/1076217514533274,
 doi = {10.1177/1076217514533274},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1076217514533274},
 journal = {Gifted Child Today},
 number = {3},
 pages = {140–141},
 title = {Contests},
 url = {https://doi-org.crai.referencistas.com/10.1177/1076217514533274},
 volume = {37},
 year = {2014s}
}

@article{doi:10.1177/1076217515583746,
 author = {Joyce VanTassel-Baska},
 doi = {10.1177/1076217515583746},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1076217515583746},
 journal = {Gifted Child Today},
 number = {3},
 pages = {198–199},
 title = {Curriculum Issues: Error Analysis in Thinking About Curriculum for the Gifted},
 url = {https://doi-org.crai.referencistas.com/10.1177/1076217515583746},
 volume = {38},
 year = {2015r}
}

@article{doi:10.1177/1076217517707233,
 abstract = {Digital technology offers new possibilities for children to play, express themselves, learn, and communicate. A recent development in online practice is a shift toward youth engaged in computer programming online communities. Programming is argued to be the new literacy of the millennium. In this article, I examine the use of Scratch, an online programming community, as a means to support digital literacy for early adolescent gifted, talented, and creative students. In addition, I share the experiences of an early adolescent gifted student with Scratch and consider the use of Scratch to promote interdisciplinary curricular concepts.},
 author = {Julia Hagge},
 doi = {10.1177/1076217517707233},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1076217517707233},
 journal = {Gifted Child Today},
 number = {3},
 pages = {154–162},
 title = {Scratching Beyond the Surface of Literacy: Programming for Early Adolescent Gifted Students},
 url = {https://doi-org.crai.referencistas.com/10.1177/1076217517707233},
 volume = {40},
 year = {2017f}
}

@article{doi:10.1177/1076217517707236,
 abstract = {With the release of the Next Generation Science Standards and the adoption of the standards by many states, teachers are encouraged to use the engineering design process (EDP) as an instructional approach to teaching science. However, teachers have limited time to teach science and will often neglect science in favor of mathematics and literacy instruction. To make this feasible for elementary classrooms, teachers should be encouraged to implement integrated units of study utilizing EDP to cohesively bind content areas and to increase active learning, critical thinking, and problem solving among all learners. An additional benefit of using EDP as an instructional strategy is the focus on problem solving and the avoidance of one size fits all learning. Students actively engage in learning content (science, mathematics, literacy, social studies) as they collaboratively work together to solve societal and environmental problems. Knowledge is built as students progress through the challenges and content is provided on a need to know basis, thereby differentiating instruction based on learner needs and challenging gifted learners. In this article, the author provides four sample engineering challenges that can be used to create an integrated unit of study using the EDP as an instructional strategy.},
 author = {Debbie Dailey},
 doi = {10.1177/1076217517707236},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1076217517707236},
 journal = {Gifted Child Today},
 number = {3},
 pages = {137–143},
 title = {Using Engineering Design Challenges to Engage Elementary Students With Gifts and Talents Across Multiple Content Areas},
 url = {https://doi-org.crai.referencistas.com/10.1177/1076217517707236},
 volume = {40},
 year = {2017e}
}

@article{doi:10.1177/1076217517735352,
 abstract = {The Parallel Curriculum Model (PCM) lends itself to considering curriculum development from different angles. It begins with a solid Core Curriculum and can then be extended through the Curriculum of Connections, Practice, and Identity. This article showcases a way of thinking about the creation of a PCM unit by providing examples from an Antarctic research expedition. The Curriculum of Connections is showcased through the practices of science and engineering and crosscutting concepts. The Curriculum of Practice is elaborated on through vignettes detailing collaborative activities on the ship. Finally, a special focus is placed on the Curriculum of Identity through profiles of people working on the ship who followed both traditional and nontraditional routes to their current careers.},
 author = {Stephanie J. Hathcock},
 doi = {10.1177/1076217517735352},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1076217517735352},
 journal = {Gifted Child Today},
 number = {1},
 pages = {28–40},
 title = {Interdisciplinary Science Through the Parallel Curriculum Model: Lessons From the Sea},
 url = {https://doi-org.crai.referencistas.com/10.1177/1076217517735352},
 volume = {41},
 year = {2018h}
}

@article{doi:10.1177/1076217519880587,
 abstract = {The Thunkable online platform is an easy-to-use resource for creating apps for mobile devices. Computational thinking is at the heart of problem solving in computer science, and research suggests students’ computational thinking improves when they use simple block coding systems similar to the format used for Thunkable.},
 author = {Del Siegle},
 doi = {10.1177/1076217519880587},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1076217519880587},
 journal = {Gifted Child Today},
 number = {1},
 pages = {64–71},
 title = {There’s an App for That, and I Made It},
 url = {https://doi-org.crai.referencistas.com/10.1177/1076217519880587},
 volume = {43},
 year = {2020m}
}

@article{doi:10.1177/10762175221149256,
 abstract = {As technologically gifted students apply their abilities to computer science, they naturally flow through the talent development stages of potential, competency, and expertise. Processes that have always been important for gifted students to learn as they develop potential are embedded in learning code, which engages the beginning programmer in rich and complex authentic projects (Housand et al., 2017). As stakeholders present opportunities for open-ended, creative processes and products, the Computer Science Teachers Association (CSTA; 2017) and International Society for Technology in Education (ISTE; 2016) Standards can form guidelines for gifted students to self-direct their education through technology. Learning to code results in both cognitive and psychosocial skill development, including creative and critical thinking, logical and systematic reasoning, positive risk-taking and processing of feedback, perseverance through challenges, social skills, and collaboration.},
 author = {Maryann R. Hebda},
 doi = {10.1177/10762175221149256},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/10762175221149256},
 journal = {Gifted Child Today},
 number = {2},
 pages = {108–118},
 title = {Technology Talent Development: Beyond an Hour of Code},
 url = {https://doi-org.crai.referencistas.com/10.1177/10762175221149256},
 volume = {46},
 year = {2023e}
}

@article{doi:10.1177/10762175221149259,
 abstract = {The purpose of this research was to describe the impact of digital game building on fourth grade gifted and talented students’ problem-solving, creativity, and collaboration skills. Increasingly, there has been a call to involve students in real-world experiences through projects that explore authentic issues using technology. Game design-based learning with its unique set of affordances may offer a path to integrating technology, computer science education, creativity, and problem-solving. Increasingly, the ability to create rather than just consume technology has gained attention linking creativity and collaboration to using coding language. In this study, data collection included student reflection journals, classroom observations, classroom video recordings, a focus group interview, and students’ games. Participants came from two GT classes (n = 45). Qualitative analysis identified five themes: overcoming challenges of group work, developing a culture of collaboration, creating narrative, and connecting science, problem-solving in Scratch’s coding environment, and reflecting on learning. Findings indicated involving gifted students in game design-based learning in science had a positive impact on student perceptions of problem-solving, creativity, and collaboration.},
 author = {Mary O’Grady-Jones and Michael M. Grant},
 doi = {10.1177/10762175221149259},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/10762175221149259},
 journal = {Gifted Child Today},
 number = {2},
 pages = {84–107},
 title = {Ready Coder One: Collaborative Game Design-Based Learning on Gifted Fourth Graders’ 21st Century Skills},
 url = {https://doi-org.crai.referencistas.com/10.1177/10762175221149259},
 volume = {46},
 year = {2023k}
}

@article{doi:10.1177/1077546309341137,
 abstract = {Approximate stability analysis of nonlinear delay differential algebraic equations (DDAEs) with periodic coefficients is proposed with a geometric interpretation of evolution of the linearized system. Firstly, a numerical algorithm based on direct integration by expansion in terms of Chebyshev polynomials is derived for linear analysis. The proposed algorithm is shown to have deeper connections with and be computationally less cumbersome than the solution of the underlying semi-explicit system via a similarity transformation. The stability of time periodic DDAE systems is characterized by the spectral radius of a “monodromy matrix”, which is a finite-dimensional approximation of a compact infinite-dimensional operator. The monodromy matrix is essentially a map of the Chebyshev coefficients (or collocation vector) of the state from the delay interval to the next adjacent interval of time. The computations are entirely performed with the original system to avoid cumbersome transformations associated with the semi-explicit form of the system. Next, two computational algorithms, the first based on perturbation series and the second based on Chebyshev spectral collocation, are detailed to obtain solutions of nonlinear DDAEs with periodic coefficients for consistent initial functions.},
 author = {Venkatesh Deshmukh},
 doi = {10.1177/1077546309341137},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1077546309341137},
 journal = {Journal of Vibration and Control},
 number = {7–8},
 pages = {1235–1260},
 title = {Approximate Stability Analysis and Computation of Solutions of Nonlinear Delay Differential Algebraic Equations with Time Periodic Coefficients},
 url = {https://doi-org.crai.referencistas.com/10.1177/1077546309341137},
 volume = {16},
 year = {2010g}
}

@article{doi:10.1177/1077546317722897,
 abstract = {Multiphysics modeling, code development, and validation by full-scale experiments is presented for hydrodynamic/suspension-dynamic interactions of a novel ocean vehicle, the Wave Adaptive Modular Vessel (WAM-V). The boat is a pontoon catamaran with hinged engine pods and elevated payload supported by suspension and articulation systems. Computational fluid dynamics models specific to WAM-V are developed which include hinged pod dynamics, water-jet propulsion modeling, and immersed boundary method for flow in the gap between pontoon and pod. Multi-body dynamics modeling for the suspension and upper-structure dynamic is developed in MATLAB Simulink. Coupled equations of motion are developed and solved iteratively through either one-way or two-way coupling methods to converge on flow-field, pontoon motions, pod motions, waterjet forces, and suspension motions. Validation experiments include cylinder drop with suspended mass and 33-feet WAM-V sea-trials in calm water and waves. Computational results show that two-way coupling is necessary to capture the physics of the interactions. The experimental trends are predicted well and errors are mostly comparable to those for rigid boats, however, in some cases the errors are larger, which is expected due to the complexity of the current studies. Ride quality analyses show that WAM-V suspension is effective in reducing payload vertical accelerations in waves by 73% compared to the same boat with rigid upper-structure.},
 author = {Maysam Mousaviraad and Michael Conger and Shanti Bhushan and Frederick Stern and Andrew Peterson and Mehdi Ahmadian},
 doi = {10.1177/1077546317722897},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1077546317722897},
 journal = {Journal of Vibration and Control},
 number = {18},
 pages = {4260–4281},
 title = {Coupled computational fluid and multi-body dynamics suspension boat modeling},
 url = {https://doi-org.crai.referencistas.com/10.1177/1077546317722897},
 volume = {24},
 year = {2018q}
}

@article{doi:10.1177/1077546318788405,
 abstract = {The complex nature of the tire/road noise generation process makes it difficult to isolate and study each mechanism individually. This paper presents an experimental and numerical investigation of air-borne tire noise generation mechanisms for a realistic tire. Experimentally, a single slot is cut into the tire and the noise data are measured and studied. Air-borne noise is isolated by filling the slot with foam and comparing the resulting frequency spectra. Numerically, a previously developed computational fluid dynamics tire noise prediction model is employed to predict the air-borne noise for the same tire, under similar operating conditions. A direct comparison between the experimental and computational results is also presented in terms of pressure time traces and spectral characteristics. Comparisons indicate that the computational model is capable of predicting the noise generated by the air pockets in the tire. While providing a deeper understanding of the causes of air-borne noise, this paper also aims to demonstrate the use of a computational tool that can be used to obtain a reasonably accurate prediction of air-borne tire noise.},
 author = {Prashanta Gautam and Yousof Azizi and Abhilash J. Chandy},
 doi = {10.1177/1077546318788405},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1077546318788405},
 journal = {Journal of Vibration and Control},
 number = {3},
 pages = {529–537},
 title = {An experimental and computational investigation of air-borne noise generation mechanisms in tires},
 url = {https://doi-org.crai.referencistas.com/10.1177/1077546318788405},
 volume = {25},
 year = {2019g}
}

@article{doi:10.1177/1077695812440942,
 abstract = {The authors argue that journalism’s uncertain identity in academia has made it vulnerable to unreflective instrumentalism in the digital era. They show how instrumentalism intertwined with the digital sublime constitutes a rhetorically resonate rationale for closing a journalism school. Evidence comes from documents and testimony associated with discontinuance of the School of Journalism and Mass Communication at the University of Colorado. Vulnerability of the school became apparent in its own Advisory Board recommending closure. The authors warn against stakeholders in journalism education internalizing the fear and opportunism implicit in a discourse of the digital sublime, a discourse ultimately in service to discontinuance.},
 author = {Michael McDevitt and Shannon Sindorf},
 doi = {10.1177/1077695812440942},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1077695812440942},
 journal = {Journalism & Mass Communication Educator},
 number = {2},
 pages = {109–118},
 title = {How to Kill a Journalism School: The Digital Sublime in the Discourse of Discontinuance},
 url = {https://doi-org.crai.referencistas.com/10.1177/1077695812440942},
 volume = {67},
 year = {2012q}
}

@article{doi:10.1177/1077695814563981,
 abstract = {As the media industry moves to a post-industrial model, there is a need for journalists—current and future—to have a deeper understanding of the ways that technology impacts their work and how best to produce journalism for mobile and networked devices. This article examines a teaching initiative designed to introduce journalism students to elements of social and mobile technology theory and design practice. Using feedback gained from peers, students, and industry professionals, the initiative was deemed successful; however, we outline several recommendations for improvement.},
 author = {Daniel Angus and Skye Doherty},
 doi = {10.1177/1077695814563981},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1077695814563981},
 journal = {Journalism & Mass Communication Educator},
 number = {1},
 pages = {44–57},
 title = {Journalism Meets Interaction Design: An Interdisciplinary Undergraduate Teaching Initiative},
 url = {https://doi-org.crai.referencistas.com/10.1177/1077695814563981},
 volume = {70},
 year = {2015a}
}

@article{doi:10.1177/1077695820904971,
 abstract = {This syndicate offers four recommendations to help educators adjust curricula to accommodate the rapid integration of data into journalism. First, instruction in numeracy and basic descriptive statistics must be required as either modules in existing courses or as separate offerings. Second, students should be taught to avoid mistakes in interpreting and writing about data in both reporting and visual classes. Third, ethics courses should discuss data as a transparency tool that poses distinctive dilemmas. Fourth, computational thinking, or how to dissect and solve problems like a computer does, can be incorporated into existing classes that teach logic.},
 author = {Norman P. Lewis and Mindy McAdams and Florian Stalph},
 doi = {10.1177/1077695820904971},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1077695820904971},
 journal = {Journalism & Mass Communication Educator},
 number = {1},
 pages = {16–21},
 title = {Data Journalism},
 url = {https://doi-org.crai.referencistas.com/10.1177/1077695820904971},
 volume = {75},
 year = {2020o}
}

@article{doi:10.1177/1077695820924309,
 abstract = {A thematic evaluation of data journalism courses resulted in a typology that parses the field and offers guidance to educators. At the center is pattern detection, preceded by data acquisition and cleaning, and followed by data representation. The typology advances academic understanding by offering a precise conceptualization that distinguishes data journalism from peripheral technologies and identifies coding as a supportive skill. It also enables a fresh definition of data journalism as the primary reliance on numerical evidence as a journalistic tool in detecting patterns, or the visual representation of numerical evidence to enable audiences to discern patterns.},
 author = {Norman P. Lewis},
 doi = {10.1177/1077695820924309},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1077695820924309},
 journal = {Journalism & Mass Communication Educator},
 number = {1},
 pages = {78–90},
 title = {Defining and Teaching Data Journalism: A Typology},
 url = {https://doi-org.crai.referencistas.com/10.1177/1077695820924309},
 volume = {76},
 year = {2021k}
}

@article{doi:10.1177/1077699020911884,
 abstract = {This study examined political advertisements placed by the Russian-based Internet Research Agency on Facebook and Instagram. Advertisements were computationally analyzed for four rhetorical techniques presumed to elicit anger and fear: negative identity-based language, inflammatory language, obscene language, and threatening language. Congruent with extant research on arousing emotional responses, advertising clickthrough rates were positively associated with inflammatory, obscene, and threatening language. Surprisingly, however, a negative relationship between clickthrough rate and the use of negative identity-based language was observed. Additional analyses showed that the advertisements were engaged with at rates that exceed industry benchmarks, and that clickthrough rates increased over time.},
 author = {Chris J. Vargo and Toby Hopp},
 doi = {10.1177/1077699020911884},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1077699020911884},
 journal = {Journalism & Mass Communication Quarterly},
 number = {3},
 pages = {743–761},
 title = {Fear, Anger, and Political Advertisement Engagement: A Computational Case Study of Russian-Linked Facebook and Instagram Content},
 url = {https://doi-org.crai.referencistas.com/10.1177/1077699020911884},
 volume = {97},
 year = {2020q}
}

@article{doi:10.1177/1077800420943643,
 abstract = {Interest in new empiricisms and transdisciplinary methods has led many social inquirers to engage with 20th-century post-classical physical science. Many of these projects have focused on alternative matter–mind mixtures and in/organic variation, concerned that past theories of sociality have dismissed the vibrancy and animacy of the nonhuman material world. This paper explores the power of speculative fiction to help us rethink empiricism in posthuman ecologies of the Anthropocene, in the midst of post-truth conditions and growing science denialism. We foreground speculative fiction as a way to open up scientific imaginaries, rethinking the relationship between nature, technics, and human “sense” making. We show how such texts offer alternative images of research methods for studying pluralist ecologies and new forms of worldly belonging.},
 author = {Elizabeth de Freitas and Sarah E. Truman},
 doi = {10.1177/1077800420943643},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1077800420943643},
 journal = {Qualitative Inquiry},
 number = {5},
 pages = {522–533},
 title = {New Empiricisms in the Anthropocene: Thinking With Speculative Fiction About Science and Social Inquiry},
 url = {https://doi-org.crai.referencistas.com/10.1177/1077800420943643},
 volume = {27},
 year = {2021g}
}

@article{doi:10.1177/1077800420960191,
 abstract = {In 5 months of COVID isolation, living out of a suitcase in temporary housing, countless fractal patterns emerged. I can’t say if I created these patterns by looking for them, or that I know the whole world by looking at a grain of sand. The truth of the matter is that it feels like the key for massive scale change is just in front of us, but slipping from our grasp. As we move through these days, weeks, and months, we have very little time before the difference recedes again. I address this matter of concern as a matter of method in performative grounded theory piece.},
 author = {Annette N. Markham},
 doi = {10.1177/1077800420960191},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1077800420960191},
 journal = {Qualitative Inquiry},
 number = {7},
 pages = {914–927},
 title = {Pattern Recognition: Using Rocks, Wind, Water, Anxiety, and Doom Scrolling in a Slow Apocalypse (to Learn More About Methods for Changing the World)},
 url = {https://doi-org.crai.referencistas.com/10.1177/1077800420960191},
 volume = {27},
 year = {2021h}
}

@article{doi:10.1177/1081286508089845,
 abstract = {We study radial solutions of the equations of isotropic elasticity in two dimensions (for a disc) and three dimensions (for a sphere). We describe a numerical scheme for computing the critical boundary displacement for cavitation based on the solution of a sequence of initial value problems for punctured domains. We give examples for specific materials and compare our numerical computations with some previous analytical results. A key observation in the formulation of the method is that the strong—ellipticity condition implies that the specification of the normal component of the Cauchy stress on an inner pre—existing but small cavity, leads to a relation for the radial strain as a function of the circumferential strain. To establish the convergence of the numerical scheme we prove a monotonicity property for the inner deformed radius for punctured balls.},
 author = {Pablo V. Negrón-Marrero and Jeyabal Sivaloganathan},
 doi = {10.1177/1081286508089845},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1081286508089845},
 journal = {Mathematics and Mechanics of Solids},
 number = {8},
 pages = {696–726},
 title = {The Numerical Computation of the Critical Boundary Displacement for Radial Cavitation},
 url = {https://doi-org.crai.referencistas.com/10.1177/1081286508089845},
 volume = {14},
 year = {2009p}
}

@article{doi:10.1177/1081286513505472,
 abstract = {As a method for bonding powdered materials, sintering has distinct advantages, such as the production of a near final-shape of the desired product, without the need for significant post-processing. However, sintering has certain deficiencies, such as incomplete or weak bonding. Research is ongoing to improve the process. One approach to improve sintering processes of powdered materials is via electrically enhanced bonding, whereby electricity is pumped through the material, while it is compressed in a press, in order to induce Joule-heating. This paper develops a computationally based model for the direct simulation of electrically enhanced sintering of powdered materials using particle-based methods. The overall approach is to construct three coupled sub-models which primarily involve: (a) particle-to-particle mechanical contact, (b) particle-to-particle thermal exchange and (c) particle-to-particle electrical current flow. These physical processes are strongly coupled, since the dynamics dictates which particles are in contact and the contacts determine the electrical flow. The flow of electricity controls the Joule-heating and the induced thermal fields, which soften the material, leading to enhanced particle binding. The strong multiphysics-coupled sub-models are solved iteratively within each time-step using a recursive staggering scheme, which employs temporal adaptivity to control the error. If the process does not converge (to within an error tolerance) within a preset number of iterations, the time-step is adapted (reduced) by utilizing an estimate of the spectral radius of the coupled system. The modular approach allows for easy replacement of submodels, if needed. Numerical examples are provided to illustrate the model and numerical solution scheme.},
 author = {TI Zohdi},
 doi = {10.1177/1081286513505472},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1081286513505472},
 journal = {Mathematics and Mechanics of Solids},
 number = {1},
 pages = {93–113},
 title = {A direct particle-based computational framework for electrically enhanced thermo-mechanical sintering of powdered materials},
 url = {https://doi-org.crai.referencistas.com/10.1177/1081286513505472},
 volume = {19},
 year = {2014t}
}

@article{doi:10.1177/1081286516649654,
 abstract = {A report on the workshop Computational mechanics of generalized continua and applications to materials with microstructure (Catania 29–31 October 2015) is provided. The constructive atmosphere that was present at the workshop in 2012 and the Euromech Colloquium in 2014, both in Cisterna di Latina, was repeated for this workshop in Catania. The objective of this meeting was to bring together experts within the CNRS International Associate Laboratory (LIA) Francois Cosserat–Tullio Levi Civita Coss & Vita in order to discuss topics of common interest. Particularly, the workshop was dedicated to the following projects of LIA: (i) computational mechanics of generalized continua; and (ii) nonlinearity and stability in continuous media. Also subjects related to the application of generalized continua to multiscale and smart materials were discussed. Approximately 25 Coss & Vita LIA members and other experts (mostly from France and Italy) gathered together, including PhD students and those students from the Scuola Superiore di Catania.},
 author = {Anil Misra and Luca Placidi and Daria Scerrato},
 doi = {10.1177/1081286516649654},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1081286516649654},
 journal = {Mathematics and Mechanics of Solids},
 number = {9},
 pages = {1891–1904},
 title = {A review of presentations and discussions of the workshop Computational mechanics of generalized continua and applications to materials with microstructure that was held in Catania 29–31 October 2015},
 url = {https://doi-org.crai.referencistas.com/10.1177/1081286516649654},
 volume = {22},
 year = {2017m}
}

@article{doi:10.1177/1081286520975202,
 abstract = {A novel high-order three-scale (HOTS) computational method for elastic behavior analysis and strength prediction of axisymmetric composite structures with multiple spatial scales is developed in this paper. The multiple heterogeneities of axisymmetric composite structures we investigated are taken into account by periodic distributions of representative unit cells on the mesoscale and microscale. First, the new micro–meso–macro coupled HOTS computational model for elastic problems of axisymmetric composite structures is established based on multiscale asymptotic analysis, which breaks through the traditional multiscale assumptions and includes three main components. Two classes of mesoscopic and microscopic auxiliary cell functions are constructed on the mesoscale and microscale, respectively. The macroscopic homogenization problems are defined on global axisymmetric structures by twice up-scaling procedures from microscale to mesoscale and then from mesoscale to macroscale. Moreover, the asymptotic HOTS solutions are constructed for approximating multiscale elastic problems of axisymmetric structures and the numerical accuracy analysis of the HOTS solutions is given in detail. Then, the strength prediction formulas for axisymmetric composite structures with multiple spatial scales are presented based on the high-accuracy elastic behavior analysis from the proposed HOTS computational model. Furthermore, the corresponding HOTS numerical algorithm based on the finite element method (FEM) is presented for analyzing the mechanical behaviors and predicting the strength of axisymmetric composite structures with multiple spatial scales in detail. Finally, some numerical examples are reported to verify the feasibility and effectiveness of the proposed HOTS computational method. In this study, a unified three-scale computational framework is offered, which enables the simulation of mechanical behaviors of axisymmetric composite structures with multiple spatial scales.},
 author = {Hao Dong and Junzhi Cui and Yufeng Nie and Ke Jin and Xiaofei Guan and Zihao Yang},
 doi = {10.1177/1081286520975202},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1081286520975202},
 journal = {Mathematics and Mechanics of Solids},
 number = {6},
 pages = {905–936},
 title = {High-order three-scale computational method for elastic behavior analysis and strength prediction of axisymmetric composite structures with multiple spatial scales},
 url = {https://doi-org.crai.referencistas.com/10.1177/1081286520975202},
 volume = {26},
 year = {2021d}
}

@article{doi:10.1177/10812865221098352,
 abstract = {This paper proposes an innovative enriched second-order multi-scale (SOMS) computational method to simulate and analyze the nonlocal thermoelastic problems of blend composites with stress and heat flux gradient behaviors. The multiple periodical heterogeneities and periodic configurations of investigated blend composites in different substructures result in a huge computational cost for direct numerical simulations. The significant characteristics of this study are as follows. (1) The nonlocal properties of blend composites in constitutive equations are converted into the source terms of thermoelastic balance equations. The novel macro-micro coupled SOMS computational model for these transformed nonlocal multi-scale problems is derived on the basis of multi-scale asymptotic analysis. The nonlocal thermoelastic behaviors of blend composites can be merely uncovered in the enriched SOMS solutions. (2) The error analysis in the pointwise sense is presented to elucidate the importance and necessity of establishing the enriched SOMS solutions. Furthermore, an explicit error estimate for the SOMS approximate solutions is obtained in the integral sense for these nonlocal multi-scale problems. (3) A multi-scale numerical algorithm is presented to effectively simulate nonlocal thermoelastic problems of blend composites based on finite element method (FEM). Finally, the capability of the proposed enriched SOMS computational method is demonstrated by typical two-dimensional (2D) and three-dimensional (3D) blend composites, presenting not only the excellent numerical accuracy but also the less computational cost. This work proposes a unified multi-scale computational framework for enabling nonlocal thermoelastic behavior analysis of blend composites.},
 author = {Hao Dong and Yufeng Nie and Ruyun Ma and Yaochuang Han},
 doi = {10.1177/10812865221098352},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/10812865221098352},
 journal = {Mathematics and Mechanics of Solids},
 number = {3},
 pages = {795–832},
 title = {The analysis and computation on nonlocal thermoelastic problems of blend composites via enriched second-order multi-scale computational method},
 url = {https://doi-org.crai.referencistas.com/10.1177/10812865221098352},
 volume = {28},
 year = {2023e}
}

@article{doi:10.1177/10812865221102549,
 abstract = {Inverse Langevin function has an extensive use in statistical mechanics, polymer chemistry, and physics. Main struggle is that the inverse Langevin function cannot be expressed in an exact analytical form. To this end, many approaches to estimate the inverse Langevin function have been proposed. A trade-off can be observed between level of accuracy and mathematical complexity in the existing approximants in the literature. In the present contribution, a simple, yet efficient one-pass predictor-corrector algorithm is proposed for the accurate prediction of the inverse Langevin function. The predictor step uses the approximants yp(x) proposed in the literature. The corrector term is based on a single iteration applied to the error function (yp) – x. The correction term is based on the yp and has the same form irrespective of the specific approximant used in the predictor step. Hence, the additional computational cost is constant irrespective of the approximant used as predictor. In order to demonstrate the accuracy and efficiency of the approach, maximum relative error and the computational cost before and after corrector step are analysed for eight different approximants. The proposed one-pass predictor-corrector approach allows the use of relatively simpler approximants of the inverse Langevin function by improving their accuracy by at least an order of magnitude.},
 author = {Selçuk Başdemir and Hüsnü Dal},
 doi = {10.1177/10812865221102549},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/10812865221102549},
 journal = {Mathematics and Mechanics of Solids},
 number = {4},
 pages = {920–930},
 title = {A one-pass predictor-corrector algorithm for the inverse Langevin function},
 url = {https://doi-org.crai.referencistas.com/10.1177/10812865221102549},
 volume = {28},
 year = {2023b}
}

@article{doi:10.1177/10812865221114336,
 abstract = {In elasticity, microstructure-related deviations may be modeled by strain gradient elasticity. For so-called metamaterials, different implementations are possible for solving strain gradient elasticity problems numerically. Analytical solutions of simple problems are used to verify the numerical approach. We demonstrate such a case in a two-dimensional continuum as a benchmark case for computations. As strain gradient enforces higher regularity conditions in displacements, in the finite element method (FEM), the use of standard elements is often seen as inadequate. For such piecewise or elementwise continuous elements, we examine a possible remedy to correctly simulate strain gradient elasticity problems by implementing two techniques. First, we enforce continuity of displacement gradient across elements; second, we employ a mixed finite element method where displacement and its gradient are solved both as unknowns. The results show the pros and cons of each numerical technique. All methods converge monotonically, but the mixed method is more reliable than the other one.},
 author = {Navid Shekarchizadeh and Bilen Emek Abali and Alberto Maria Bersani},
 doi = {10.1177/10812865221114336},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/10812865221114336},
 journal = {Mathematics and Mechanics of Solids},
 number = {10},
 pages = {2218–2238},
 title = {A benchmark strain gradient elasticity solution in two-dimensions for verifying computational approaches by means of the finite element method},
 url = {https://doi-org.crai.referencistas.com/10.1177/10812865221114336},
 volume = {27},
 year = {2022m}
}

@article{doi:10.1177/1082013210368461,
 abstract = {The aim of the present study was to perform an in silico evaluation of bovine meat proteins as potential precursors of biologically active peptides, as well as to determine whether such peptides can be released by selected proteolytic enzymes. The sequences of 19 bovine meat proteins were processed using the BIOPEP database and program. The profiles of potential biological activity of protein fragments were determined and the following parameters were calculated: the frequency of occurrence of fragments with given activity (A), the frequency of release of fragments with given activity by selected enzymes (AE), and the relative frequency of release of fragments with given activity by selected enzymes (W). Among the examined proteins, collagen and elastin appear to be the richest potential source of bioactive peptides, in particular of angiotensin-converting enzyme inhibitors, antithrombotic fragments, inhibitors of dipeptidyl peptidase IV and peptides regulating gastric mucosal activity. The high number of bioactive fragments in collagen and elastin is associated with a high content of glycine and proline, amino acids that are most abundant in biologically active fragments. Of the two investigated proteolytic enzymes, Proteinase K — an enzyme with broad specificity (e.g., against peptide bonds formed by the carboxyl groups of proline) can release considerably more biologically active fragments than Proteinase P1 — an enzyme with narrow specificity, not including proline residues.},
 author = {P. Minkiewicz and J. Dziuba and J. Michalska},
 doi = {10.1177/1082013210368461},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1082013210368461},
 journal = {Food Science and Technology International},
 note = {PMID:21364044},
 number = {1},
 pages = {39–45},
 title = {Bovine Meat Proteins as Potential Precursors of Biologically Active Peptides - a Computational Study based on the BIOPEP Database},
 url = {https://doi-org.crai.referencistas.com/10.1177/1082013210368461},
 volume = {17},
 year = {2011l}
}

@article{doi:10.1177/1086296X231202722,
 abstract = {This study focused on the digital design practices of Raul, a 15-year-old participant at a summer video game design camp for adolescents. As Raul developed his original game, You Will Perish, I wondered what his design process might reveal about (a) the practice of affectively and procedurally literate video game design and (b) the literacy pedagogies that can support such design. Guided by the concept of serendipity, I describe Raul’s design practice as an open process characterized by bouts of failure, chance, and discovery, and I examine how such forces shaped the emergence of his game. Using transversal analysis, I trace Raul’s design through an account of frustration and failure, perseverance and pride, showing how the challenges of the game’s creator become those of the game’s players. The study highlights the generative potential of serendipitous literacies wherever and whenever literacy happens.},
 author = {Bradley Robinson},
 doi = {10.1177/1086296X231202722},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1086296X231202722},
 journal = {Journal of Literacy Research},
 number = {3},
 pages = {275–301},
 title = {You Will Perish: A Case Study of Serendipitous Literacies and Novice Video Game Design},
 url = {https://doi-org.crai.referencistas.com/10.1177/1086296X231202722},
 volume = {55},
 year = {2023s}
}

@article{doi:10.1177/1091142103254579,
 abstract = {A small-scale computational general equilibrium model is used to examine the efficiency costs of exempting commercial nonprofits from the corporate income tax when they compete directly with for-profit firms. Simulation results from differential-incidence experiments indicate significant welfare gains when the tax wedge is reduced at the margin between the for-profit and nonprofit firms and the change in government revenue is financed by lump-sum taxes. However, although welfare gains exist at the margin, average excess burden estimates suggest that some level of differential taxation is welfare-improving if nonprofits contribute to the production of a good with positive externalities. Results are compared with those from the literature on the differential taxation of corporate and noncorporate firms and are found generally consistent.},
 author = {Marianne F. Johnson},
 doi = {10.1177/1091142103254579},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1091142103254579},
 journal = {Public Finance Review},
 number = {6},
 pages = {623–647},
 title = {Differential Taxation of for-Profit and Nonprofit Firms: A Computational General Equilibrium Approach},
 url = {https://doi-org.crai.referencistas.com/10.1177/1091142103254579},
 volume = {31},
 year = {2003l}
}

@article{doi:10.1177/109434200001400407,
 abstract = {To support creation of nimble applications for computational grids, the authors believe one must eliminate the barrier that separates program creation from execution and post-mortem optimization. This paper outlines an approach to dynamic performance adaptation and distributed optimization in the grid environment based on a suite of performance instrumentation, analysis, and presentation tools that includes distributed performance sensors and resource policy actuators, fuzzy logic rule bases for adaptive control, and immersive visualization systems for real-time visualization and direct manipulation of software behavior.},
 author = {Jeffrey S. Vetter and Daniel A. Reed},
 doi = {10.1177/109434200001400407},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/109434200001400407},
 journal = {The International Journal of High Performance Computing Applications},
 number = {4},
 pages = {357–366},
 title = {Real-Time Performance Monitoring, Adaptive Control, and Interactive Steering                 of Computational Grids},
 url = {https://doi-org.crai.referencistas.com/10.1177/109434200001400407},
 volume = {14},
 year = {2000s}
}

@article{doi:10.1177/1094342003017002001,
 abstract = {In this paper we present the findings and recommendations that emerged from a one-day workshop held at Lawrence Berkeley National Laboratory (LBNL) on June 5, 2002, in conjunction with the National Energy Research Scientific Computing (NERSC) User Group (NUG) Meeting. The motivation for this workshop was to solicit direct input from the application science community on the subject of visualization. The workshop speakers and participants included computational scientists from a cross-section of disciplines that use the NERSC facility, as well as visualization researchers from across the country. We asked the workshop contributors how they currently visualize their results, and how they would like to do visualization in the future. We were especially interested in each individual’s view of how visualization tools and services could be improved in order to better meet the needs of future computational science projects. The outcome of this workshop is a set of findings and recommendations that are presented in more detail later in this paper, and are briefly summarized here. Scientific visualization is a crucial technological capability that plays an important role in understanding data created by computational science projects as well as experiments. In order to be effective, visualization technology should be easy to use for a non-expert. The term “easy to use” encompasses a number of different categories, including a short learning curve, tight integration with computational frameworks, availability on the desktop as well as the fixed visualization facility, tools that are tailored for each specific application domain, and low cost. Current visualization tools fall short in several key areas of capability. Few visualization tools are capable of processing large datasets, such as those commonly generated at NERSC. Better support for parallel visualization tools may prove useful in leveraging large parallel machines as visualization resources. Multivariate visualization - multiple grids, many species, and many dimensions - is needed in order to quickly gain insight into large datasets. Related “drill-down” capabilities, such as the ability to quickly move from macro to micro views (used in “data mining”), would be extremely helpful in understanding data but are missing from most visualization tools. Many application scientists perceive a conundrum when it comes to visualization support. Support for visualization within each individual program level is often inadequate or nonexistent due to funding constraints, yet support for visualization at the institutional level is also often inadequate or nonexistent. Better solutions are needed for remote visualization. Current approaches are further constrained by network bandwidth and access to resources. The proliferation of visualization tools and data formats poses challenges. Researchers must often master many different tools in order to achieve the desired results. Data format conversion is often required when moving between tools. Common data formats and frameworks for visualization tools are needed to reduce duplication of effort and better promote sharing of resources and results. Better communication is needed between the visualization and computational science communities. The computational scientists are often unaware of current trends and practices in the visualization community. By being more aware of the needs of the computational science community, the visualization research programs can be crafted so as to be more responsive to their needs. As a result of the workshop, we have developed a set of recommendations that can be summarized as follows: • Establish a coherent program that focuses on remote visualization. A remote visualization program should provide tools and infrastructure that can be used by multiple “virtual teams”. • Establish mechanisms whereby generally-applicable visualization technology is developed and deployed in a centralized fashion. • Develop a research program in interactive visualization with running codes that stresses the integrated design and development of coupled simulation-visualization methods. • Establish a research program in the areas of multi-field visualization and multi-dimensional data visualization. • Establish a research program in the area of automated data exploration for next generation petascale datasets.},
 author = {Bernd Hamann and E. Wes Bethel and Horst Simon and Juan Meza},
 doi = {10.1177/1094342003017002001},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342003017002001},
 journal = {The International Journal of High Performance Computing Applications},
 number = {2},
 pages = {097–123},
 title = {NERSC “Visualization Greenbook” Future Visualization                Needs of the Doe Computational Science Community Hosted at NERSC},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094342003017002001},
 volume = {17},
 year = {2003k}
}

@article{doi:10.1177/1094342004038951,
 abstract = {Large-scale scientific applications frequently compute sparse matrix–vector products in their computational core. For this reason, techniques for computing sparse matrix– vector products efficiently on modern architectures are important. In this paper we describe a strategy for improving the performance of sparse matrix–vector product computations using a loop transformation known as unrollandjam. We describe a novel sparse matrix representation that enables us to apply this transformation. Our approach is best suited for sparse matrices that have rows with a small number of predictable lengths. This work was motivated by sparse matrices that arise in SAGE, an application from Los Alamos National Laboratory. We evaluate the performance benefits of our approach using sparse matrices produced by SAGE for a pair of sample inputs. We show that our strategy is effective for improving sparse matrix–vector product performance using these matrices on MIPS R12000, Alpha Ev67, IBM Power 3, and Itanium 2 processors. Our measurements show that for this class of sparse matrices, our strategy improves sparse matrix–vector product performance from a low of 41% on MIPS to well over a factor of 2 on Itanium.},
 author = {John Mellor-Crummey and John Garvin},
 doi = {10.1177/1094342004038951},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342004038951},
 journal = {The International Journal of High Performance Computing Applications},
 number = {2},
 pages = {225–236},
 title = {Optimizing Sparse Matrix–Vector Product Computations Using Unroll                and Jam},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094342004038951},
 volume = {18},
 year = {2004m}
}

@article{doi:10.1177/1094342004038957,
 abstract = {In this paper we describe the architecture, implementation and performance of a service for the delivery of dynamic performance information in Grid environments. Based on usage requirements gleaned from real applications being developed as part of the Grid Application Development Software (GrADS) project, we have implemented a highperformance service for use by Grid schedulers. The organization of the system is discussed and performance results are presented.},
 author = {Martin Swany and Rich Wolski},
 doi = {10.1177/1094342004038957},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342004038957},
 journal = {The International Journal of High Performance Computing Applications},
 number = {2},
 pages = {255–265},
 title = {Building Performance Topologies for Computational Grids},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094342004038957},
 volume = {18},
 year = {2004p}
}

@article{doi:10.1177/1094342004048534,
 abstract = {Many institutions are now developing large-scale, complex, coupled multiphysics computational simulations for massively parallel platforms for the simulation of the performance of nuclear weapons and certification of the stockpile, and for research in climate and weather prediction, magnetic and inertial fusion energy, environmental systems, astrophysics, aerodynamic design, combustion, biological and biochemical systems, and other areas. The successful development of these simulations is aided by attention to sound software project management and software engineering. We have developed “lessons learned” from a set of code projects that the Department of Energy National Nuclear Security Agency has sponsored to develop nuclear weapons simulations over the last 50 years. We find that some, but not all, of the software project management and development practices (rather than processes) commonly employed for non-technical software add value to the development of scientific software and we identify those that we judge add value. Another key finding, consistent with general software industry experience, is that the optimal project schedule and resource level are solely determined by the requirements once the requirements are fixed.},
 author = {D. E. Post and R. P. Kendall},
 doi = {10.1177/1094342004048534},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342004048534},
 journal = {The International Journal of High Performance Computing Applications},
 number = {4},
 pages = {399–416},
 title = {Software Project Management and Quality Engineering Practices for Complex,                 Coupled Multiphysics, Massively Parallel Computational Simulations: Lessons Learned                 From ASCI},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094342004048534},
 volume = {18},
 year = {2004m}
}

@article{doi:10.1177/1094342004048538,
 abstract = {We propose a new performance metric based on computational action. We examine work as it evolves in time and compute computational action as the integral of the work function over time. We compare the action generated at less than full power with the action that could have been generated at full power. We claim that the goal of performance optimization is to minimize lost, or wasted, action. We calculate our metric for some computers in the Top500 list (http://www.top500.org) and propose a new ranking based on least action wasted. When work is a function of the resources applied, we use the classical techniques of the calculus of variations to minimize wasted action. From the result of this exercise, we calculate productivity as the ratio of work produced to resources used.},
 author = {Robert W. Numrich},
 doi = {10.1177/1094342004048538},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342004048538},
 journal = {The International Journal of High Performance Computing Applications},
 number = {4},
 pages = {449–458},
 title = {Performance Metrics Based on Computational Action},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094342004048538},
 volume = {18},
 year = {2004q}
}

@article{doi:10.1177/1094342006064504,
 abstract = {This paper describes the Aggregate Remote Memory Copy Interface (ARMCI), a portable high performance remote memory access communication interface, developed oriinally under the U.S. Department of Energy (DOE) Advanced Computational Testing and Simulation Toolkit project and currently used and advanced as a part of the run-time layer of the DOE project, Programming Models for Scalble Parallel Computing. The paper discusses the model, addresses challenges of portable implementations, and demonstrates that ARMCI delivers high performance on a variety of platforms. Special emphasis is placed on the latency hiding mechanisms and ability to optimize noncotiguous data transfers.},
 author = {J. Nieplocha and V. Tipparaju and M. Krishnan and D. K. Panda},
 doi = {10.1177/1094342006064504},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342006064504},
 journal = {The International Journal of High Performance Computing Applications},
 number = {2},
 pages = {233–253},
 title = {High Performance Remote Memory Access Communication: The Armci Approach},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094342006064504},
 volume = {20},
 year = {2006l}
}

@article{doi:10.1177/1094342006074849,
 abstract = {In the economic-based computational grids we need effective schedulers not only to minimize the makespan but also to minimize the costs that are spent for the execution of the jobs. In this work, a novel economy driven job scheduling heuristic is proposed and a simulation application is developed by using GridSim toolkit to investigate the performance of the heuristic. The simulation-based experiments demonstrate the effectiveness of the proposed heuristic both in terms of parameter sweep and sequential workflow type of applications.},
 author = {Omer Ozan Sonmez and Attila Gursoy},
 doi = {10.1177/1094342006074849},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342006074849},
 journal = {The International Journal of High Performance Computing Applications},
 number = {1},
 pages = {21–29},
 title = {A Novel Economic-Based Scheduling Heuristic for Computational Grids},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094342006074849},
 volume = {21},
 year = {2007o}
}

@article{doi:10.1177/1094342006074857,
 abstract = {Detailed experimental measurements of a compressor (pressure, temperature, speed) are not easily available because of size, cost, and access difficulties. A comprehensive computer model is necessary to obtain pressure-volume diagrams for the compressor. The valve is a key component of a compressor as it determines both the efficiency and reliability of the compressor. The valves operate as a result of pressure differences between the ports and the gas chamber. Undesired vibrations and fatigue fracture of thin valves (0.2 mm steel sheet) have not been well understood and experimental difficulties do not allow a thorough analysis of the causes. Initial investigations and modifications to KIVA-3V showed this code can be used to model both gas and suction valve motion. Since the code was written for internal combustion engines, its adaptation to compressors proved to be a difficult task, particularly because of the orientation and complexity of valves. Extensive modifications were made to model an angularly moving valve system, however as we moved down from engine scale (10 cm) to compressor scale (1 cm), grid resolution became a challenge from time to time. Preliminary results showed that the design of the suction valve and the chamber impacts the pressure difference between the input port and the gas chamber. The higher the piston speed, the lower the pressure of the gas, making it perhaps more plausible to avoid premature valve closings at higher speeds. Even though it is still under investigation, valve location and the orientation of its loose end makes a difference to the pressure, suggesting that new designs may be necessary by manufacturers. Experimental data will be needed to validate our model and verify our observations. Future steps include adding a discharge valve and a fluid-solid interface (FSI) module to the modified KIVA-3V code in order to predict the motion of the valves as a result of their interaction with the chamber gas.},
 author = {O. Yaşar and M. Koçaş},
 doi = {10.1177/1094342006074857},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342006074857},
 journal = {The International Journal of High Performance Computing Applications},
 number = {1},
 pages = {30–41},
 title = {Computational Modeling of Hermetic Reciprocating Compressors},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094342006074857},
 volume = {21},
 year = {2007t}
}

@article{doi:10.1177/1094342007095289,
 abstract = {We present a framework for making computation offloading decisions in computational grid settings in which schedulers determine when to move parts of a computation to more capable resources to improve performance. Such schedulers must predict when an offloaded computation will outperform one that is local by forecasting the local cost (execution time for computing locally) and remote cost (execution time for computing remotely and transmission time for the input/output of the computation to/from the remote system). Typically, this decision amounts to predicting the bandwidth between the local and remote systems to estimate these costs. Our framework unifies such decision models by formulating the problem as a statistical decision problem that can either be treated “classically” or using a Bayesian approach. Using an implementation of this framework, we evaluate the efficacy of a number of different decision strategies (several of which have been employed by previous systems). Our results indicate that a Bayesian approach employing automatic change-point detection when estimating the prior distribution is the best performing approach.},
 author = {Selim Gurun and Rich Wolski and Chandra Krintz and Dan Nurmi},
 doi = {10.1177/1094342007095289},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342007095289},
 journal = {The International Journal of High Performance Computing Applications},
 number = {4},
 pages = {460–479},
 title = {On the Efficacy of Computation Offloading Decision-Making Strategies},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094342007095289},
 volume = {22},
 year = {2008g}
}

@article{doi:10.1177/1094342009347892,
 abstract = {Sparse matrix operations achieve only small fractions of peak CPU speeds because of the use of specialized, index-based matrix representations, which degrade cache utilization by imposing irregular memory accesses and increasing the number of overall accesses. Compounding the problem, the small number of floating-point operations in a single sparse iteration leads to low floating-point pipeline utilization. Operation stacking addresses these problems for large ensemble computations that solve multiple systems of linear equations with identical sparsity structure. By combining the data of multiple problems and solving them as one, operation stacking improves locality, reduces cache misses, and increases floating-point pipeline utilization. Operation stacking also requires less memory bandwidth because it involves fewer index array accesses. In this paper we present the Operation Stacking Framework (OSF), an object-oriented framework that provides runtime and code generation support for the development of stacked iterative solvers. OSF’s runtime component provides an iteration engine that supports efficient ejection of converged problems from the stack. It separates the specific solver algorithm from the coding conventions and data representations that are necessary to implement stacking. Stacked solvers created with OSF can be used transparently without requiring significant changes to existing applications. Our results show that stacking can provide speedups up to 1.94× with an average of 1.46×, even in scenarios in which the number of iterations required to converge varies widely within a stack of problems. Our evaluation shows that these improvements correlate with better cache utilization, improved floating-point utilization, and reduced memory accesses.},
 author = {Mehmet Belgin and Godmar Back and Calvin J. Ribbens},
 doi = {10.1177/1094342009347892},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342009347892},
 journal = {The International Journal of High Performance Computing Applications},
 number = {2},
 pages = {194–212},
 title = {Operation Stacking for Ensemble Computations With Variable Convergence},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094342009347892},
 volume = {24},
 year = {2010a}
}

@article{doi:10.1177/1094342009358413,
 abstract = {With the current shift of increasing the computational power of a processor by including multiple cores instead of increasing the clock frequency, consideration of computational efficiency is gaining increased importance for computational fluid dynamics codes. This is especially critical for applications that require high throughput. For example, applying computational fluid dynamics simulations to multi-disciplinary design optimization requires a large number of similar simulations with different input parameters. Therefore, a reduction in the runtime of the code can lead to large reduction in the design process. In our case study, a two-dimensional, block-structured computational fluid dynamics code was optimized for performance on machines with hierarchical memory systems. This paper illustrates the techniques applied to transform an initial version of the code to an optimized version that yielded performance improvements of 10% for very small cases to about 50% for large test cases that did not fit into the cache memory of the target processor. A detailed performance analysis of the code starting at the global level down to subroutines and data structures is presented in this paper. The performance improvements can be explained through a reduction of cache misses in all levels of the memory hierarchy. The L1 cache misses were reduced by about 50%, the L2 cache misses by about 80% and the translation lookaside buffer misses by about 90% for the optimized version of the code. The code performance was also evaluated for multi-core processors, where efficiency is especially important when several instances of an application are running simultaneously. In this case, the most optimized version, a blocked version of the optimized code, more effectively maintained efficiency as more cores were activated compared to the unblocked version. This illustrates that optimizing cache performance may be increasingly important as the number of cores per processor continues to rise.},
 author = {Thomas Hauser and Raymond LeBeau},
 doi = {10.1177/1094342009358413},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342009358413},
 journal = {The International Journal of High Performance Computing Applications},
 number = {3},
 pages = {299–318},
 title = {Optimization of a Computational Fluid Dynamics Code for the Memory Hierarchy: A Case Study},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094342009358413},
 volume = {24},
 year = {2010i}
}

@article{doi:10.1177/1094342012436965,
 abstract = {With the fourth release of the Community Climate System Model, the ability to perform ultra-high-resolution climate simulations is now possible, enabling eddy-resolving ocean and sea-ice models to be coupled to a finite-volume atmosphere model for a range of atmospheric resolutions. This capability was made possible by enabling the model to use large scale parallelism, which required a significant refactoring of the software infrastructure. We describe the scalability of two ultra-high-resolution coupled configurations on leadership class computing platforms. We demonstrate the ability to utilize over 30,000 processor cores on a Cray XT5 system and over 60,000 cores on an IBM Blue Gene/P system to obtain climatologically relevant simulation rates for these configurations.},
 author = {John M. Dennis and Mariana Vertenstein and Patrick H. Worley and Arthur A. Mirin and Anthony P. Craig and Robert Jacob and Sheri Mickelson},
 doi = {10.1177/1094342012436965},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342012436965},
 journal = {The International Journal of High Performance Computing Applications},
 number = {1},
 pages = {5–16},
 title = {Computational performance of ultra-high-resolution capability in the Community Earth System Model},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094342012436965},
 volume = {26},
 year = {2012d}
}

@article{doi:10.1177/1094342012440466,
 abstract = {Given the computing industry trend of increasing processing capacity by adding more cores to a chip, the focus of this work is tuning the performance of a staple visualization algorithm, raycasting volume rendering, for shared-memory parallelism on multi-core CPUs and many-core GPUs. Our approach is to vary tunable algorithmic settings, along with known algorithmic optimizations and two different memory layouts, and measure performance in terms of absolute runtime and L2 memory cache misses. Our results indicate there is a wide variation in runtime performance on all platforms, as much as 254% for the tunable parameters we test on multi-core CPUs and 265% on many-core GPUs, and the optimal configurations vary across platforms, often in a non-obvious way. For example, our results indicate the optimal configurations on the GPU occur at a crossover point between those that maintain good cache utilization and those that saturate computational throughput. This result is likely to be extremely difficult to predict with an empirical performance model for this particular algorithm because it has an unstructured memory access pattern that varies locally for individual rays and globally for the selected viewpoint. Our results also show that optimal parameters on modern architectures are markedly different from those in previous studies run on older architectures. In addition, given the dramatic performance variation across platforms for both optimal algorithm settings and performance results, there is a clear benefit for production visualization and analysis codes to adopt a strategy for performance optimization through auto-tuning. These benefits will likely become more pronounced in the future as the number of cores per chip and the cost of moving data through the memory hierarchy both increase.},
 author = {E Wes Bethel and Mark Howison},
 doi = {10.1177/1094342012440466},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342012440466},
 journal = {The International Journal of High Performance Computing Applications},
 number = {4},
 pages = {399–412},
 title = {Multi-core and many-core shared-memory parallel raycasting volume rendering optimization and tuning},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094342012440466},
 volume = {26},
 year = {2012b}
}

@article{doi:10.1177/1094342012474997,
 abstract = {The simulation of cardiac electrophysiology is a mature field in computational physiology. Recent advances in medical imaging, high-performance computing and numerical methods mean that computational models of electrical propagation in human heart tissue are ripe for use in patient-specific simulation for diagnosis, for prognosis and for selection of treatment methods. However, in order to move in this direction, it is necessary to make efficient use of modern petascale computing resources. This paper focuses on an existing open source simulation framework (Chaste) and documents work done to improve the parallel scaling on a small range of electrophysiology benchmark problems. These benchmarks involve the numerical solution of the monodomain or bidomain equations via the finite-element method. At the beginning of this study the electrophysiology libraries within Chaste were already enabled to run in parallel and were able to solve for electrical propagation using the monodomain or bidomain equations, but parallel efficiency dropped rapidly when run on more than about 64 processors. Throughout the course of the study, improvements were made to problem definition input; geometric mesh partitioning; finite-element assembly of large, sparse linear systems; problem-specific matrix preconditioning; numerical solution of the linear system; and output of the approximate solution. The consequence of these improvements is that, at the end of the study, Chaste is able to solve a monodomain benchmark problem in close to real time. While some of the improvements made to the parallel Chaste code are specific to cardiac electrophysiology, many of the techniques documented in this paper are generic to the parallel finite-element method in other scientific application areas.},
 author = {Miguel O Bernabeu and James Southern and Nicholas Wilson and Peter Strazdins and Jonathan Cooper and Joe Pitt-Francis},
 doi = {10.1177/1094342012474997},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342012474997},
 journal = {The International Journal of High Performance Computing Applications},
 number = {1},
 pages = {13–32},
 title = {Chaste: A case study of parallelisation of an open source finite-element solver with          applications to computational cardiac electrophysiology simulation},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094342012474997},
 volume = {28},
 year = {2014a}
}

@article{doi:10.1177/1094342016677586,
 abstract = {Magnetic resonance imaging (MRI) is one of the most important diagnostic tools in modern medicine. Since it is a high-cost and highly-complex imaging modality, computational models are frequently built to enhance its understanding as well as to support further development. However, such models often have to be simplified to complete simulations in a reasonable time. Thus, the simulations with high spatial/temporal resolutions, with any motion consideration (like blood flow) and/or with 3D objects usually call for using parallel computing environments. In this paper, we propose to use graphics processing units (GPUs) for fast simulations of MRI of vascular structures. We apply a CUDA environment which supports general purpose computation on GPU (GPGPU). The data decomposition strategy is applied and thus the parts of each virtual object are spread over the GPU cores. The GPU cores are responsible for calculating the influence of blood flow behavior and MRI events after successive time steps. In the proposed approach, different data layouts, memory access patterns, and other memory improvements are applied to efficiently exploit GPU resources. Computational performance is thoroughly validated for various vascular structures and different NVIDIA GPUs. Results show that MRI simulations can be accelerated significantly thanks to GPGPU. The proposed GPU-based approach may be easily adopted in the modeling of other flow related phenomena like perfusion, diffusion or transport of contrast agents.},
 author = {Krzysztof Jurczuk and Marek Kretowski and Johanne Bezy–Wendling},
 doi = {10.1177/1094342016677586},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342016677586},
 journal = {The International Journal of High Performance Computing Applications},
 number = {4},
 pages = {496–511},
 title = {GPU-based computational modeling of magnetic resonance imaging of vascular structures},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094342016677586},
 volume = {32},
 year = {2018f}
}

@article{doi:10.1177/1094342016677599,
 abstract = {Computational biology allows and encourages the application of many different parallelism-based technologies. This special issue brings together high-quality state-of-the-art contributions about parallelism-based technologies in computational biology, from different points of view or perspectives, that is, from diverse high-performance computing applications. The special issue collects considerably extended and improved versions of the best papers, accepted and presented in PBio 2015 (the Third International Workshop on Parallelism in Bioinformatics, and part of IEEE ISPA 2015). The domains and topics covered in these seven papers are timely and important, and the authors have done an excellent job of presenting the material.},
 author = {Miguel A Vega-Rodríguez and Álvaro Rubio-Largo},
 doi = {10.1177/1094342016677599},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342016677599},
 journal = {The International Journal of High Performance Computing Applications},
 number = {3},
 pages = {317–320},
 title = {Parallelism in computational biology: A view from diverse high-performance computing applications},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094342016677599},
 volume = {32},
 year = {2018r}
}

@article{doi:10.1177/1094342017738352,
 abstract = {The ever-increasing computational requirements of HPC and service provider applications are becoming a great challenge for hardware and software designers. These requirements are reaching levels where the isolated development on either computational field is not enough to deal with such challenge. A holistic view of the computational thinking is therefore the only way to success in real scenarios. However, this is not a trivial task as it requires, among others, of hardware–software codesign. In the hardware side, most high-throughput computers are designed aiming for heterogeneity, where accelerators (e.g. Graphics Processing Units (GPUs), Field-Programmable Gate Arrays (FPGAs), etc.) are connected through high-bandwidth bus, such as PCI-Express, to the host CPUs. Applications, either via programmers, compilers, or runtime, should orchestrate data movement, synchronization, and so on among devices with different compute and memory capabilities. This increases the programming complexity and it may reduce the overall application performance. This article evaluates different offloading strategies to leverage heterogeneous systems, based on several cards with the first-generation Xeon Phi coprocessors (Knights Corner). We use a 11-point 3-D Stencil kernel that models heat dissipation as a case study. Our results reveal substantial performance improvements when using several accelerator cards. Additionally, we show that computing of an approximate result by reducing the communication overhead can yield 23% performance gains for double-precision data sets.},
 author = {Mario Hernández and Juan M Cebrián and José M Cecilia and José M García},
 doi = {10.1177/1094342017738352},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342017738352},
 journal = {The International Journal of High Performance Computing Applications},
 number = {2},
 pages = {199–207},
 title = {Offloading strategies for Stencil kernels on the KNC Xeon Phi architecture: Accuracy versus performance},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094342017738352},
 volume = {34},
 year = {2020h}
}

@article{doi:10.1177/1094342017747692,
 abstract = {A significant fraction of computational software for scientific research grows through accretion. In a common scenario, a small group develops a code for a specific purpose. Others find the software useful, so they add to it for their own use. The software grows to the point where its management becomes intractable and scientific results obtained from it become unreliable. This is in stark contrast with a small number of scientific codes that have undergone a design process, be it due to an upfront investment, or when haphazardly grown codes have reset and started again. At a minimum, these codes reduce the time to obtain research results for the communities they serve because individual researchers do not have to develop their own codes. They provide further benefits; the results they produce are more reproducible due to greater scrutiny, leading to better science. One of the more overlooked benefits, which is perhaps of greater significance, is that a well-designed code can expand to serve communities beyond the ones it was designed for. Thus, research communities with similar computational requirements can symbiotically improve computation-based research for each other. In this article, we present a case study of FLASH, a code that was designed and developed for simulating thermonuclear runaways such as novae and type Ia supernovae in astrophysics. Designed to be modular and extensible, users from several diverse research areas have added capabilities to it and adapted it for their own communities. Examples include cosmology, high-energy density physics, core-collapse supernovae, star formation, fluid–structure interactions, and chemical combustion. We give a summary of design features that facilitated the expansion and quantify the effort needed to expand into some of the above-mentioned fields. We also quantify the impact on different communities by mining the database of publications using FLASH, collected by its developers.},
 author = {Anshu Dubey and Petros Tzeferacos and Don Q Lamb},
 doi = {10.1177/1094342017747692},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342017747692},
 journal = {The International Journal of High Performance Computing Applications},
 number = {2},
 pages = {322–331},
 title = {The dividends of investing in computational software design: A case study},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094342017747692},
 volume = {33},
 year = {2019h}
}

@article{doi:10.1177/1094342018774126,
 abstract = {This work investigates the application and interaction of optimization techniques and performance models in a computational fluid dynamics (CFD) approach employing an OpenMP parallelized, explicit, weakly compressible, finite difference–based solver for the incompressible Navier–Stokes equations using a five-point wide stencil. The presented loop and stencil optimizations lead to a 6.8× increase in per core throughput. In order to verify optimal CPU utilization, performance models are applied to the tuned code. Three different performance models are considered: a roofline-based model, utilizing purely theoretical figures, one which is enhanced by measurements, and the execution cache memory model. It is shown that the models provide reliable estimates for simple benchmarks, such as seven-point stencils for scalar Laplacians, but the estimate quality is significantly worse for the complex and tuned stencil. While it is possible to include even more details in the model, it eventually leads to a state in which it purely reproduces the benchmarks from which it was derived. Thus, the applied general-purpose performance models are found to inaccurately predict the actual performance. They overestimate the achievable performance by more than about 97% for highly tuned code. Through further code tuning, 66% of the predicted performance could be achieved.},
 author = {Karl-Robert Wichmann and Martin Kronbichler and Rainald Löhner and Wolfgang A Wall},
 doi = {10.1177/1094342018774126},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342018774126},
 journal = {The International Journal of High Performance Computing Applications},
 number = {4},
 pages = {602–618},
 title = {Practical applicability of optimizations and performance models to complex stencil-based loop kernels in CFD},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094342018774126},
 volume = {33},
 year = {2019s}
}

@article{doi:10.1177/1094342018816377,
 abstract = {Writing high-performance solvers for engineering applications is a delicate task. These codes are often developed on an application to application basis, highly optimized to solve a certain problem. Here, we present our work on developing a general simulation framework for efficient computation of time-resolved approximations of complex industrial flow problems—Complex Unified Building cube method (CUBE). To address the challenges of emerging, modern supercomputers, suitable data structures and communication patterns are developed and incorporated into CUBE. We use a Cartesian grid together with various immersed boundary (IB) methods to accurately capture moving, complex geometries. The asymmetric workload of the IB is balanced by a predictive dynamic load balancer, and a multithreaded halo exchange algorithm is employed to efficiently overlap communication with computations. Our work also concerns efficient methods for handling the large amount of data produced by large-scale flow simulations, such as scalable parallel I/O, data compression, and in-situ processing.},
 author = {Niclas Jansson and Rahul Bale and Keiji Onishi and Makoto Tsubokura},
 doi = {10.1177/1094342018816377},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342018816377},
 journal = {The International Journal of High Performance Computing Applications},
 number = {4},
 pages = {678–698},
 title = {CUBE: A scalable framework for large-scale industrial simulations},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094342018816377},
 volume = {33},
 year = {2019i}
}

@article{doi:10.1177/1094342019839124,
 abstract = {We propose an approach for improved reproducibility that includes capturing and relating provenance characteristics and performance metrics. We discuss two use cases: scientific reproducibility of results in the Energy Exascale Earth System Model (E3SM—previously ACME) and performance reproducibility in molecular dynamics workflows on HPC platforms. To capture and persist the provenance and performance data of these workflows, we have designed and developed the Chimbuko and ProvEn frameworks. Chimbuko captures provenance and enables detailed single workflow performance analysis. ProvEn is a hybrid, queryable system for storing and analyzing the provenance and performance metrics of multiple runs in workflow performance analysis campaigns. Workflow provenance and performance data output from Chimbuko can be visualized in a dynamic, multilevel visualization providing overview and zoom-in capabilities for areas of interest. Provenance and related performance data ingested into ProvEn is queryable and can be used to reproduce runs. Our provenance-based approach highlights challenges in extracting information and gaps in the information collected. It is agnostic to the type of provenance data it captures so that both the reproducibility of scientific results and that of performance can be explored with our tools.},
 author = {Line Pouchard and Sterling Baldwin and Todd Elsethagen and Shantenu Jha and Bibi Raju and Eric Stephan and Li Tang and Kerstin Kleese Van Dam},
 doi = {10.1177/1094342019839124},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342019839124},
 journal = {The International Journal of High Performance Computing Applications},
 number = {5},
 pages = {763–776},
 title = {Computational reproducibility of scientific workflows at extreme scales},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094342019839124},
 volume = {33},
 year = {2019n}
}

@article{doi:10.1177/1094342019849618,
 abstract = {The approach of the next-generation computing platforms offers a tremendous opportunity to advance the state-of-the-art in global atmospheric dynamical models. We detail our incremental approach to utilize this emerging technology by enhancing concurrency within the High-Order Method Modeling Environment (HOMME) atmospheric dynamical model developed at the National Center for Atmospheric Research (NCAR). The study focused on improvements to the performance of HOMME which is a Fortran 90 code with a hybrid (MPIOpenMP) programming model. The article describes the changes made to the use of message passing interface (MPI) and OpenMP as well as single-core optimizations to achieve significant improvements in concurrency and overall code performance. For our optimization studies, we utilize the “Cori” system with an Intel Xeon Phi Knights Landing processor deployed at the National Energy Research Supercomputing Center and the “`Cheyenne” system with an Intel Xeon Broadwell processor installed at the NCAR. The results from the studies, using “workhorse” configurations performed at NCAR, show that these changes have a transformative impact on the computational performance of HOMME. Our improvements have shown that we can effectively increase potential concurrency by efficiently threading the vertical dimension. Further, we have seen a factor of two overall improvement in the computational performance of the code resulting from the single-core optimizations. Most notably from the work is that our incremental approach allows for high-impact changes without disrupting existing scientific productivity in the HOMME community.},
 author = {John M Dennis and Brian Dobbins and Christopher Kerr and Youngsung Kim},
 doi = {10.1177/1094342019849618},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342019849618},
 journal = {The International Journal of High Performance Computing Applications},
 number = {5},
 pages = {1030–1045},
 title = {Optimizing the HOMME dynamical core for multicore platforms},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094342019849618},
 volume = {33},
 year = {2019f}
}

@article{doi:10.1177/1094342020905971,
 abstract = {Helicopters can experience brownout when flying close to a dusty surface. The uplifting of dust in the air can remarkably restrict the pilot’s visibility area. Consequently, a brownout can disorient the pilot and lead to the helicopter collision against the ground. Given its risks, brownout has become a high-priority problem for civil and military operations. Proper helicopter design is thus critical, as it has a strong influence over the shape and density of the cloud of dust that forms when brownout occurs. A way forward to improve aircraft design against brownout is the use of particle simulations. For simulations to be accurate and comparable to the real phenomenon, billions of particles are required. However, using a large number of particles, serial simulations can be slow and too computationally expensive to be performed. In this work, we investigate an message passing interface (MPI) + graphics processing unit (multi-GPU) approach to simulate brownout. In specific, we use a semi-implicit Euler method to consider the particle dynamics in a Lagrangian way, and we adopt a precomputed aerodynamic field. Here, we do not include particle–particle collisions in the model; this allows for independent trajectories and effective model parallelization. To support our methodology, we provide a speedup analysis of the parallelization concerning the serial and pure-MPI simulations. The results show (i) very high speedups of the MPI + multi-GPU implementation with respect to the serial and pure-MPI ones, (ii) excellent weak and strong scalability properties of the implemented time-integration algorithm, and (iii) the possibility to run realistic simulations of brownout with billions of particles at a relatively small computational cost. This work paves the way toward more realistic brownout simulations, and it highlights the potential of high-performance computing for aiding and advancing aircraft design for brownout mitigation.},
 author = {Roberto Porcù and Edie Miglio and Nicola Parolini and Mattia Penati and Noemi Vergopolan},
 doi = {10.1177/1094342020905971},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342020905971},
 journal = {The International Journal of High Performance Computing Applications},
 number = {3},
 pages = {267–281},
 title = {HPC simulations of brownout: A noninteracting particles dynamic model},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094342020905971},
 volume = {34},
 year = {2020p}
}

@article{doi:10.1177/1094342020918305,
 abstract = {This article presents a low-rank decomposition algorithm based on subsampling of matrix entries. The proposed algorithm first computes rank-revealing decompositions of submatrices with a blocked adaptive cross approximation (BACA) algorithm, and then applies a hierarchical merge operation via truncated singular value decompositions (H-BACA). The proposed algorithm significantly improves the convergence of the baseline ACA algorithm and achieves reduced computational complexity compared to the traditional decompositions such as rank-revealing QR. Numerical results demonstrate the efficiency, accuracy, and parallel scalability of the proposed algorithm.},
 author = {Yang Liu and Wissam Sid-Lakhdar and Elizaveta Rebrova and Pieter Ghysels and Xiaoye Sherry Li},
 doi = {10.1177/1094342020918305},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342020918305},
 journal = {The International Journal of High Performance Computing Applications},
 number = {4},
 pages = {394–408},
 title = {A parallel hierarchical blocked adaptive cross approximation algorithm},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094342020918305},
 volume = {34},
 year = {2020h}
}

@article{doi:10.1177/1094342020964857,
 abstract = {Solving an N-body problem, electrostatic or gravitational, is a crucial task and the main computational bottleneck in many scientific applications. Its direct solution is an ubiquitous showcase example for the compute power of graphics processing units (GPUs). However, the naïve pairwise summation has computational complexity. The fast multipole method (FMM) can reduce runtime and complexity to for any specified precision. Here, we present a CUDA-accelerated, C++ FMM implementation for multi particle systems with potential that are found, e.g. in biomolecular simulations. The algorithm involves several operators to exchange information in an octree data structure. We focus on the Multipole-to-Local (M2L) operator, as its runtime is limiting for the overall performance. We propose, implement and benchmark three different M2L parallelization approaches. Approach (1) utilizes Unified Memory to minimize programming and porting efforts. It achieves decent speedups for only little implementation work. Approach (2) employs CUDA Dynamic Parallelism to significantly improve performance for high approximation accuracies. The presorted list-based approach (3) fits periodic boundary conditions particularly well. It exploits FMM operator symmetries to minimize both memory access and the number of complex multiplications. The result is a compute-bound implementation, i.e. performance is limited by arithmetic operations rather than by memory accesses. The complete CUDA parallelized FMM is incorporated within the GROMACS molecular dynamics package as an alternative Coulomb solver.},
 author = {Bartosz Kohnke and Carsten Kutzner and Andreas Beckmann and Gert Lube and Ivo Kabadshow and Holger Dachsel and Helmut Grubmüller},
 doi = {10.1177/1094342020964857},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342020964857},
 journal = {The International Journal of High Performance Computing Applications},
 number = {1},
 pages = {97–117},
 title = {A CUDA fast multipole method with highly efficient M2L far field evaluation},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094342020964857},
 volume = {35},
 year = {2021l}
}

@article{doi:10.1177/10943420211006169,
 abstract = {The aim of this work is a fair and unbiased comparison of a lattice Boltzmann method (LBM) against a finite difference method (FDM) for the simulation of fluid flows. Rather than reporting metrics such as floating point operation rates or memory throughput, our work considers the engineering quest of reaching a desired solution quality with the least computational effort. The specific lattice Boltzmann and finite difference methods selected here are of a very basic nature to emphasize the influence of the fundamentally different approaches. To minimize the skew in the measurements, complex boundary condition schemes and further advanced techniques are avoided and instead both methods are fully explicit, weakly compressible approaches. Due to the highly optimized nature of both codes, different sets of restrictions are imposed by either method. Using the common set of features, two relatively simple test cases in terms of a duct flow and the flow in a lid driven cavity are considered and are tuned to perform optimally with both approaches. As a third test case, a transient flow around a square cylinder is used to demonstrate the applicability to engineering oriented settings and in a temporal domain. The performance of the two methods is found to be very similar with no full advantage for any of the approaches. Overall a tendency toward better performance of the LBM at larger target errors and for indirect benchmark quantities, such as lift and drag, is observed, while the FDM excels at smaller target errors and direct comparisons of velocity and pressure profiles to analytical solutions. Other factors such as the difficulty of setting consistent boundary conditions in the LBM or the effect of stabilization in the FDM are likely to be the most important criteria when searching for a very fast flow solver for practical applications.},
 author = {Karl-Robert Wichmann and Martin Kronbichler and Rainald Löhner and Wolfgang A Wall},
 doi = {10.1177/10943420211006169},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/10943420211006169},
 journal = {The International Journal of High Performance Computing Applications},
 number = {4},
 pages = {370–390},
 title = {A runtime based comparison of highly tuned lattice Boltzmann and finite difference solvers},
 url = {https://doi-org.crai.referencistas.com/10.1177/10943420211006169},
 volume = {35},
 year = {2021s}
}

@article{doi:10.1177/10943420211020803,
 abstract = {Efficient exploitation of exascale architectures requires rethinking of the numerical algorithms used in many large-scale applications. These architectures favor algorithms that expose ultra fine-grain parallelism and maximize the ratio of floating point operations to energy intensive data movement. One of the few viable approaches to achieve high efficiency in the area of PDE discretizations on unstructured grids is to use matrix-free/partially assembled high-order finite element methods, since these methods can increase the accuracy and/or lower the computational time due to reduced data motion. In this paper we provide an overview of the research and development activities in the Center for Efficient Exascale Discretizations (CEED), a co-design center in the Exascale Computing Project that is focused on the development of next-generation discretization software and algorithms to enable a wide range of finite element applications to run efficiently on future hardware. CEED is a research partnership involving more than 30 computational scientists from two US national labs and five universities, including members of the Nek5000, MFEM, MAGMA and PETSc projects. We discuss the CEED co-design activities based on targeted benchmarks, miniapps and discretization libraries and our work on performance optimizations for large-scale GPU architectures. We also provide a broad overview of research and development activities in areas such as unstructured adaptive mesh refinement algorithms, matrix-free linear solvers, high-order data visualization, and list examples of collaborations with several ECP and external applications.},
 author = {Tzanio Kolev and Paul Fischer and Misun Min and Jack Dongarra and Jed Brown and Veselin Dobrev and Tim Warburton and Stanimire Tomov and Mark S Shephard and Ahmad Abdelfattah et al.},
 doi = {10.1177/10943420211020803},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/10943420211020803},
 journal = {The International Journal of High Performance Computing Applications},
 number = {6},
 pages = {527–552},
 title = {Efficient exascale discretizations: High-order finite element methods},
 url = {https://doi-org.crai.referencistas.com/10.1177/10943420211020803},
 volume = {35},
 year = {2021l}
}

@article{doi:10.1177/10943420211027539,
 abstract = {Clouds represent a key uncertainty in future climate projection. While explicit cloud resolution remains beyond our computational grasp for global climate, we can incorporate important cloud effects through a computational middle ground called the Multi-scale Modeling Framework (MMF), also known as Super Parameterization. This algorithmic approach embeds high-resolution Cloud Resolving Models (CRMs) to represent moist convective processes within each grid column in a Global Climate Model (GCM). The MMF code requires no parallel data transfers and provides a self-contained target for acceleration. This study investigates the performance of the Energy Exascale Earth System Model-MMF (E3SM-MMF) code on the OLCF Summit supercomputer at an unprecedented scale of simulation. Hundreds of kernels in the roughly 10K lines of code in the E3SM-MMF CRM were ported to GPUs with OpenACC directives. A high-resolution benchmark using 4600 nodes on Summit demonstrates the computational capability of the GPU-enabled E3SM-MMF code in a full physics climate simulation.},
 author = {Matthew R Norman and David C Bader and Christopher Eldred and Walter M Hannah and Benjamin R Hillman and Christopher R Jones and Jungmin M Lee and LR Leung and Isaac Lyngaas and Kyle G Pressel et al.},
 doi = {10.1177/10943420211027539},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/10943420211027539},
 journal = {The International Journal of High Performance Computing Applications},
 number = {1},
 pages = {93–105},
 title = {Unprecedented cloud resolution in a GPU-enabled full-physics atmospheric climate simulation on OLCF’s summit supercomputer},
 url = {https://doi-org.crai.referencistas.com/10.1177/10943420211027539},
 volume = {36},
 year = {2022n}
}

@article{doi:10.1177/10943420211027937,
 abstract = {The ExaStar project aims to deliver an efficient, versatile, and portable software ecosystem for multi-physics astrophysics simulations run on exascale machines. The code suite is a component-based multi-physics toolkit, built on the capabilities of current simulation codes (in particular Flash-X and Castro), and based on the massively parallel adaptive mesh refinement framework AMReX. It includes modules for hydrodynamics, advanced radiation transport, thermonuclear kinetics, and nuclear microphysics. The code will reach exascale efficiency by building upon current multi- and many-core packages integrated into an orchestration system that uses a combination of configuration tools, code translators, and a domain-specific asynchronous runtime to manage performance across a range of platform architectures. The target science includes multi-physics simulations of astrophysical explosions (such as supernovae and neutron star mergers) to understand the cosmic origin of the elements and the fundamental physics of matter and neutrinos under extreme conditions.},
 author = {J. Austin Harris and Ran Chu and Sean M Couch and Anshu Dubey and Eirik Endeve and Antigoni Georgiadou and Rajeev Jain and Daniel Kasen and M P Laiu and OE B Messer et al.},
 doi = {10.1177/10943420211027937},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/10943420211027937},
 journal = {The International Journal of High Performance Computing Applications},
 number = {1},
 pages = {59–77},
 title = {Exascale models of stellar explosions: Quintessential multi-physics simulation},
 url = {https://doi-org.crai.referencistas.com/10.1177/10943420211027937},
 volume = {36},
 year = {2022f}
}

@article{doi:10.1177/10943420211029302,
 abstract = {Rapid growth in data, computational methods, and computing power is driving a remarkable revolution in what variously is termed machine learning (ML), statistical learning, computational learning, and artificial intelligence. In addition to highly visible successes in machine-based natural language translation, playing the game Go, and self-driving cars, these new technologies also have profound implications for computational and experimental science and engineering, as well as for the exascale computing systems that the Department of Energy (DOE) is developing to support those disciplines. Not only do these learning technologies open up exciting opportunities for scientific discovery on exascale systems, they also appear poised to have important implications for the design and use of exascale computers themselves, including high-performance computing (HPC) for ML and ML for HPC. The overarching goal of the ExaLearn co-design project is to provide exascale ML software for use by Exascale Computing Project (ECP) applications, other ECP co-design centers, and DOE experimental facilities and leadership class computing facilities.},
 author = {Francis J Alexander and James Ang and Jenna A Bilbrey and Jan Balewski and Tiernan Casey and Ryan Chard and Jong Choi and Sutanay Choudhury and Bert Debusschere and Anthony M DeGennaro et al.},
 doi = {10.1177/10943420211029302},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/10943420211029302},
 journal = {The International Journal of High Performance Computing Applications},
 number = {6},
 pages = {598–616},
 title = {Co-design Center for Exascale Machine Learning Technologies (ExaLearn)},
 url = {https://doi-org.crai.referencistas.com/10.1177/10943420211029302},
 volume = {35},
 year = {2021b}
}

@article{doi:10.1177/10943420221085000,
 abstract = {Present day computational fluid dynamics (CFD) simulations generate considerable amounts of data, sometimes on the order of TB/s. Often, a significant fraction of this data is discarded because current storage systems are unable to keep pace. To address this, data compression algorithms can be applied to data arrays containing flow quantities of interest (QoIs) to reduce the overall required storage. The matrix column interpolative decomposition (ID) can be implemented as a type of lossy compression for data matrices that factors the original data matrix into a product of two smaller factor matrices. One of these matrices consists of a subset of the columns of the original data matrix, while the other is a coefficient matrix which approximates the original data matrix columns as linear combinations of the selected columns. Motivating this work is the observation that the structure of ID algorithms makes them well suited for the asynchronous nature of task-based parallelism; they can operate independently on subdomains of the system of interest and, as a result, provide varied levels of compression. Using the task-based Legion programming model, a single-pass ID algorithm (SPID) for CFD applications is implemented. Performance studies, scalability, and the accuracy of the compression algorithm are presented for a benchmark analytical Taylor-Green vortex problem, as well as large-scale implementations of both low and high Reynolds number (Re) compressible Taylor-Green vortices using a high-order Navier-Stokes solver. In the case of the analytical solution, the resulting compressed solution was rank-one, with error on the order of machine precision. For the low-Re vortex, compression factors between 1000 and 10,000 were achieved for errors in the range 10−2–10−3. Similar error values were seen for the high-Re vortex, this time with compression factors between 100 and 1000. Moreover, strong and weak scaling results demonstrate that introducing SPID to solvers leads to negligible increases in runtime.},
 author = {Heather Pacella and Alec Dunton and Alireza Doostan and Gianluca Iaccarino},
 doi = {10.1177/10943420221085000},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/10943420221085000},
 journal = {The International Journal of High Performance Computing Applications},
 number = {3},
 pages = {388–418},
 title = {Task-parallel in situ temporal compression of large-scale computational fluid dynamics data},
 url = {https://doi-org.crai.referencistas.com/10.1177/10943420221085000},
 volume = {36},
 year = {2022k}
}

@article{doi:10.1177/10943420221116056,
 abstract = {The fastest supercomputer in 2020, Fugaku, has not only achieved digital transformation of epidemiology in allowing end-to-end, detailed quantitative modeling of COVID-19 transmissions for the first time but also transformed the behavior of the entire Japanese public through its detailed analysis of transmission risks in multitudes of societal situations entailing heavy risks. A novel aerosol simulation methodology was synthesized out of a combination of a new CFD methods meeting industrial demands in the solver, CUBE (Jansson et al., 2019), which not only allowed the simulations to scale massively with high resolution required for micrometer virus-containing aerosol particles but also enabled extremely rapid time-to-solution due to its ability to generate the digital twins representing multitudes of societal situations in a matter of minutes, attaining true overall application high performance; such simulations have been running for the past 1.5°years on Fugaku, cumulatively consuming top supercomputer-class resources and the communicated by the media as well as becoming the basis for official public policies.},
 author = {Kazuto Ando and Rahul Bale and ChungGang Li and Satoshi Matsuoka and Keiji Onishi and Makoto Tsubokura},
 doi = {10.1177/10943420221116056},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/10943420221116056},
 journal = {The International Journal of High Performance Computing Applications},
 number = {5–6},
 pages = {568–586},
 title = {Digital transformation of droplet/aerosol infection risk assessment realized on “Fugaku” for the fight against COVID-19},
 url = {https://doi-org.crai.referencistas.com/10.1177/10943420221116056},
 volume = {36},
 year = {2022b}
}

@article{doi:10.1177/10943420221121151,
 abstract = {Reacting flow simulations for combustion applications require extensive computing capabilities. Leveraging the AMReX library, the Pele suite of combustion simulation tools targets the largest supercomputers available and future exascale machines. We introduce PeleC, the compressible solver in the Pele suite, and detail its capabilities, including complex geometry representation, chemistry integration, and discretization. We present a comparison of development efforts using both OpenACC and AMReX’s C++ performance portability framework for execution on multiple GPU architectures. We discuss relevant details that have allowed PeleC to achieve high performance and scalability. PeleC’s performance characteristics are measured through relevant simulations on multiple supercomputers. The success of PeleC’s design for exascale is exhibited through demonstration of a 160 billion cell simulation and weak scaling onto 100% of Summit, an NVIDIA-based GPU supercomputer at Oak Ridge National Laboratory. Our results provide confidence that PeleC will enable future combustion science simulations with unprecedented fidelity.},
 author = {Marc T Henry de Frahan and Jon S Rood and Marc S Day and Hariswaran Sitaraman and Shashank Yellapantula and Bruce A Perry and Ray W Grout and Ann Almgren and Weiqun Zhang and John B Bell et al.},
 doi = {10.1177/10943420221121151},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/10943420221121151},
 journal = {The International Journal of High Performance Computing Applications},
 number = {2},
 pages = {115–131},
 title = {PeleC: An adaptive mesh refinement solver for compressible reacting flows},
 url = {https://doi-org.crai.referencistas.com/10.1177/10943420221121151},
 volume = {37},
 year = {2023j}
}

@article{doi:10.1177/10943420221128529,
 abstract = {Reverse Time Migration (RTM) is a state-of-the-art algorithm used in seismic depth imaging in complex geological environments for the oil and gas exploration industry. It calculates high-resolution images by solving the three-dimensional acoustic wave equation using seismic datasets recorded at various receiver locations. Reverse Time Migration’s computational phases are predominantly composed of stencil computational kernels for the finite-difference time-domain scheme, applying the absorbing boundary conditions, and I/O operations needed for the imaging condition. In this paper, we integrate the asynchronous Multicore Wavefront Diamond (MWD) tiling approach into the full RTM workflow. Multicore Wavefront Diamond permits to further increase data reuse by leveraging spatial with Temporal Blocking (TB) during the stencil computations. This integration engenders new challenges with a snowball effect on the legacy synchronous RTM workflow as it requires rethinking of how the absorbing boundary conditions, the I/O operations, and the imaging condition operate. These disruptive changes are necessary to maintain the performance superiority of asynchronous stencil execution throughout the time integration, while ensuring the quality of the subsurface image does not deteriorate. We assess the overall performance of the new MWD-based RTM and compare against traditional Spatial Blocking (SB)-based RTM on various shared-memory systems using the SEG Salt3D model. The MWD-based RTM achieves up to 70% performance speedup compared to SB-based RTM. To our knowledge, this paper highlights for the first time the applicability of asynchronous executions with temporal blocking throughout the whole RTM. This may eventually create new research opportunities in improving hydrocarbon extraction for the petroleum industry.},
 author = {Long Qu and Rached Abdelkhalak and Hatem Ltaief and Issam Said and David Keyes},
 doi = {10.1177/10943420221128529},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/10943420221128529},
 journal = {The International Journal of High Performance Computing Applications},
 number = {2},
 pages = {132–150},
 title = {Exploiting temporal data reuse and asynchrony in the reverse time migration},
 url = {https://doi-org.crai.referencistas.com/10.1177/10943420221128529},
 volume = {37},
 year = {2023n}
}

@article{doi:10.1177/10943420241268288,
 abstract = {From partial differential equations to the convolutional neural networks in deep learning, to matrix operations in dense linear algebra, computations on structured grids dominate high-performance computing and machine learning. The performance of such computations is key to effective utilization of the billions of US dollar’s worth of GPU-accelerated systems such computations are run on. Concurrently, the end of Moore’s law and Dennard scaling are driving the specialization of compute and memory architectures. This specialization often makes performance brittle (small changes in function can have severe ramifications on performance), non-portable (vendors are increasingly motivated to develop their programming models tailored for their specialized architectures), and not performance portable (even a given computation may perform very differently from one architecture to the next). The mismatch between computations that reference data that is logically neighboring in N-dimensional space but physically distant in memory motivated the creation of Bricks — a novel data-structure transformation for multi-dimensional structured grids that reorders data into small, fixed-sized bricks of contiguously-packed data. Whereas a cache-line naturally captures spatial locality in only one dimension of a structured grid, Bricks can capture spatial locality in three or more dimensions. When coupled with a Python interface, a code-generator, and autotuning, the resultant BrickLib software provides not only raw performance, but also performance portability across multiple CPUs and GPUs, scalability in distributed memory, user productivity, and generality across computational domains. In this paper, we provide an overview of BrickLib and provide a series of vignettes on how it delivers on the aforementioned metrics.},
 author = {Mahesh Lakshminarasimhan and Oscar Antepara and Tuowen Zhao and Benjamin Sepanski and Protonu Basu and Hans Johansen and Mary Hall and Samuel Williams},
 doi = {10.1177/10943420241268288},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/10943420241268288},
 journal = {The International Journal of High Performance Computing Applications},
 number = {0},
 pages = {10943420241268288},
 title = {Bricks: A high-performance portability layer for computations on block-structured grids},
 url = {https://doi-org.crai.referencistas.com/10.1177/10943420241268288},
 volume = {0},
 year = {2024k}
}

@article{doi:10.1177/10943420241280060,
 abstract = {Many complex systems can be accurately modeled as a set of coupled time-dependent partial differential equations (PDEs). However, solving such equations can be prohibitively expensive, easily taxing the world’s largest supercomputers. One pragmatic strategy for attacking such problems is to split the PDEs into components that can more easily be solved in isolation. This operator splitting approach is used ubiquitously across scientific domains, and in many cases leads to a set of ordinary differential equations (ODEs) that need to be solved as part of a larger “outer-loop” time-stepping approach. The SUNDIALS library provides a plethora of robust time integration algorithms for solving ODEs, and the U.S. Department of Energy Exascale Computing Project (ECP) has supported its extension to applications on exascale-capable computing hardware. In this paper, we highlight some SUNDIALS capabilities and its deployment in combustion and cosmology application codes (Pele and Nyx, respectively) where operator splitting gives rise to numerous, small ODE systems that must be solved concurrently.},
 author = {Cody J Balos and Marcus Day and Lucas Esclapez and Anne M Felden and David J Gardner and Malik Hassanaly and Daniel R Reynolds and Jon S Rood and Jean M Sexton and Nicholas T Wimer et al.},
 doi = {10.1177/10943420241280060},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/10943420241280060},
 journal = {The International Journal of High Performance Computing Applications},
 number = {0},
 pages = {10943420241280060},
 title = {SUNDIALS time integrators for exascale applications with many independent systems of ordinary differential equations},
 url = {https://doi-org.crai.referencistas.com/10.1177/10943420241280060},
 volume = {0},
 year = {2024a}
}

@article{doi:10.1177/109434208700100102,
 author = {Erich Bloch},
 doi = {10.1177/109434208700100102},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/109434208700100102},
 journal = {The International Journal of Supercomputing Applications},
 number = {1},
 pages = {5–8},
 title = {Supercomputing and the Growth of Computational Science in the National Science Foundation},
 url = {https://doi-org.crai.referencistas.com/10.1177/109434208700100102},
 volume = {1},
 year = {1987d}
}

@article{doi:10.1177/109434208700100103,
 abstract = {The organizational structure is described for a new program that permits computa tions on a variety of quantum mechanical problems in chemical dynamics and spec troscopy. Particular attention was devoted to developing and using algorithms that exploit the capabilities of current vector supercomputers. A key component in this procedure is the recursive transformation of the large sparse Hamiltonian matrix into a much smaller tridiagonal matrix. An ap plication to time-dependent laser-mole cule energy transfer is presented. Rate of energy deposition in the multimode mole cule for systematic variations in the mo lecular intermode coupling parameters is emphasized.},
 author = {Richard A. Friesner and Jean-Philippe Brunet and Robert E. Wyatt and Claude Leforestier and Steven Binkley},
 doi = {10.1177/109434208700100103},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/109434208700100103},
 journal = {The International Journal of Supercomputing Applications},
 number = {1},
 pages = {9–23},
 title = {Computational Approach To Large Quam Dynamical Problems},
 url = {https://doi-org.crai.referencistas.com/10.1177/109434208700100103},
 volume = {1},
 year = {1987c}
}

@article{doi:10.1177/109434208800200406,
 abstract = {Neural computation is a style of computation that draws inspiration from the way the brain computes. It is an in trinsically collective paradigm characterized by high con nectivity among a very large number of simple pro cessors running in parallel, possibly asynchronously. Methods developed in the theory of many-particle systems can be brought to bear on important conceptual questions about the operation and programming of such computational assemblies. This paper reviews several basic problems that arise in this area: the mathematical formulation of the collective computation done by such a network and of algorithms for programming (“teaching”) them. The importance of phase transitions for understanding the generic behavior of such systems and algorithms is emphasized.},
 author = {J.A. Hertz},
 doi = {10.1177/109434208800200406},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/109434208800200406},
 journal = {The International Journal of Supercomputing Applications},
 number = {4},
 pages = {54–62},
 title = {Statistical Mechanics of Neural Computation},
 url = {https://doi-org.crai.referencistas.com/10.1177/109434208800200406},
 volume = {2},
 year = {1988f}
}

@article{doi:10.1177/109434209000400206,
 abstract = {Several flow visualization techniques using the Lagran gian approach are proposed for analyzing the numerical solutions of unsteady flow fields computed by the Eu lerian approach. We show how these methods can be used to assess the validity of solutions and to extract the nature of the flow fields. The numerical algorithms for the flow solver and the flow visualization are introduced. The incompressible Navier-Stokes equations are solved using the extended MAC method and the compressible Euler equations are solved using the TVD MacCormack method. Most of the flow visualization methods are conventional. For vector fields, however, we introduce a particle tracing algorithm that is suitable for large amounts of numerical data. We present four flow visu alizations using these methods: flow past a circular cyl inder in two dimensions, shock wave propagation over a circular cylinder, flow past a sphere, and flow around an entire automobile.},
 author = {Susumu Shirayama and Kunio Kuwahara},
 doi = {10.1177/109434209000400206},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/109434209000400206},
 journal = {The International Journal of Supercomputing Applications},
 number = {2},
 pages = {66–80},
 title = {Flow Visualization in Computational Fluid Dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1177/109434209000400206},
 volume = {4},
 year = {1990l}
}

@article{doi:10.1177/109434209100500204,
 abstract = {Little use is made of multiple processors available on current supercomputers (computers with a theoretical peak performance capability equal to 100 MFLOPS or more) to improve turnaround time in computational aerodynamics. The productivity of a computer user is directly related to this turnaround time. In a time-sharing environment, such improvement in this speed is achieved when multiple processors are used efficiently to execute an algorithm. We apply the concept of mul tiple instructions and multiple data (MIMD) through multitasking via a strategy that requires relatively minor modifications to an existing code for a single processor. This approach maps the available memory to multiple processors, exploiting the C-Fortran-Unix interface. The existing code is mapped without the need for devel oping a new algorithm. The procedure for building a code utilizing this approach is automated with the Unix stream editor.},
 author = {Maurice Yarrow and Unmeel B. Mehta},
 doi = {10.1177/109434209100500204},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/109434209100500204},
 journal = {The International Journal of Supercomputing Applications},
 number = {2},
 pages = {47–73},
 title = {Multiprocessing On Supercomputers for Computational Aerodynamics},
 url = {https://doi-org.crai.referencistas.com/10.1177/109434209100500204},
 volume = {5},
 year = {1991s}
}

@article{doi:10.1177/109434209200600201,
 author = {Matthew Witten},
 doi = {10.1177/109434209200600201},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/109434209200600201},
 journal = {The International Journal of Supercomputing Applications},
 number = {2},
 pages = {127–137},
 title = {Editorial— the Frankenstein Project: Building a Man in the Machine and the Arrival of the Computational Physician},
 url = {https://doi-org.crai.referencistas.com/10.1177/109434209200600201},
 volume = {6},
 year = {1992s}
}

@article{doi:10.1177/109434209200600203,
 abstract = {This article describes research in progress at the Yale University School of Medicine on the use of parallel computation in medicine and biology. We have experi mented with the parallelization of programs from sev eral different biomedical fields, using the machine-in dependent parallel programming language Linda. This research has helped us identify several general prob lem areas in which parallel computation can play a major role: namely, real-time computation, database searching, and computationally intensive algorithms. Methods for designing, debugging, and evaluating programs from each of these three problem areas are discussed. Results and evaluations of example pro grams implemented in each of these areas are pre sented. Finally, certain lessons learned are discussed.},
 author = {Dean F. Sittig and Mark A. Shifman and Prakash Nadkarni and Perry L. Miller},
 doi = {10.1177/109434209200600203},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/109434209200600203},
 journal = {The International Journal of Supercomputing Applications},
 number = {2},
 pages = {147–163},
 title = {Parallel Computation for Medicine and Biology: Applications of Linda At Yale University},
 url = {https://doi-org.crai.referencistas.com/10.1177/109434209200600203},
 volume = {6},
 year = {1992q}
}

@article{doi:10.1177/109434209200600303,
 abstract = {An all-to-all broadcast algorithm that exploits concur rent communication on all channels of the Connection Machine system CM-200 binary cube network is de scribed. Issues in integrating a physical all-to-all broad cast between processing nodes into a language envi ronment using a global address space are discussed. Timings for the physical broadcast between nodes and for the virtual broadcast are given. The peak data transfer rate for the physical broadcast on a CM-200 is 5.9 gigabytes/sec, and the peak rate for the virtual broadcast is 31 gigabytes/sec. Array reshaping is an effective performance optimization technique. An ex ample is given where reshaping improved perfor mance by a factor of 7 by reducing the amount of local data motion. We also show how to exploit symmetry for computation of an interaction matrix using the all- to-all broadcast function. Further optimizations are suggested for N-body-type calculations. Using the all- to-all broadcast function, a peak rate of 9.3 GFLOPS/ sec has been achieved for the N-body computations in 32-bit precision on a 2,048 node Connection Machine system CM-200.},
 author = {Jean-Philippe Brunet and S. Lennart Johnsson},
 doi = {10.1177/109434209200600303},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/109434209200600303},
 journal = {The International Journal of Supercomputing Applications},
 number = {3},
 pages = {241–256},
 title = {All-To-All Broadcast and Applications On the Connection Machine},
 url = {https://doi-org.crai.referencistas.com/10.1177/109434209200600303},
 volume = {6},
 year = {1992b}
}

@article{doi:10.1177/109434209200600403,
 abstract = {We describe a subset of the level-1, level-2, and level-3 BLAS implemented for each node of the Connection Machine system CM-200. The routines, collectively called LBLAS, have interfaces consistent with lan guages with an array syntax such as Fortran 90. One novel feature, important for distributed memory archi tectures, is the capability of performing computations on multiple instances of objects in a single call. The number of instances and their allocation across mem ory units, and the strides for the different axes within the local memories, are derived from an array descrip tor that contains type, shape, and data distribution in formation. Another novel feature of the LBLAS is a se lection of loop order for rank-1 updates and matrix- matrix multiplication based on array shapes, strides, and DRAM page faults. The peak efficiencies for the routines are in excess of 75%. Matrix-vector multiplica tion achieves a peak efficiency of 92%. The optimiza tion of loop ordering has a success rate exceeding 99.8% for matrices for which the sum of the lengths of the axes is at most 60. The success rate is even higher for all possible matrix shapes. The performance loss when a nonoptimal choice is made is less than ∼15% of peak and typically less than 1% of peak. We also show that the performance gain for high-rank updates may be as much as a factor of 6 over rank-1 updates.},
 author = {S. Lennart Johnsson and Luis F. Ortiz},
 doi = {10.1177/109434209200600403},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/109434209200600403},
 journal = {The International Journal of Supercomputing Applications},
 number = {4},
 pages = {322–350},
 title = {Local Basic Linear Algebra Subroutines (Lblas) for Distributed Memory Architectures and Languages With Array Syntax},
 url = {https://doi-org.crai.referencistas.com/10.1177/109434209200600403},
 volume = {6},
 year = {1992k}
}

@article{doi:10.1177/109434209200600407,
 abstract = {We describe a scalable version of the FLO67 program written in the loop style that is characteristic of Fortran 77. This single scalable source program can be com piled and run on conventional scalar processors, vec tor computers, or massively parallel computers, with respectable performance on all platforms. Initial runs have been on the IBM RISC System/6000, the Sun SPARC, and the Connection Machine supercomputer. This article presents and discusses interesting frag ments of the program, focusing on issues of code scalability.},
 author = {Skef Wholey and Clifford Lasser and Gyan Bhanot},
 doi = {10.1177/109434209200600407},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/109434209200600407},
 journal = {The International Journal of Supercomputing Applications},
 number = {4},
 pages = {383–388},
 title = {Correspondence: FLO67: a Case Study in Scalable Programming},
 url = {https://doi-org.crai.referencistas.com/10.1177/109434209200600407},
 volume = {6},
 year = {1992r}
}

@article{doi:10.1177/109434209300700204,
 abstract = {A model of the vortex structure and configuration for homogeneous type-II superconductors can be deter mined by minimization of the Ginzburg-Landau free energy functional. A generalization of this formulation, the Lawrence-Doniach model, can be used for layered systems. A finite-dimensional approximation of this functional leads to a very large, sparse optimization problem. A number of algorithms have been used to attempt to solve this problem. The most successful of these is a damped, inexact Newton algorithm. Its com putational kernel is the iterative solution of a large, sparse, linear system that is nearly singular. We present computational results for solving these three- dimensional problems on the Intel DELTA. Using a general-purpose, scalable, iterative solver based on the preconditioned conjugate gradient method, we ob tained sustained computational rates of up to 4.26 GFLOPS on 512 processors. We find improvements of over a factor of 100 in the total execution time when compared with the same problem run on the CRAY-2. This computational tool has given us the first compu tational view of three-dimensional vortex phenomena such as vortex locking.},
 author = {Mark T. Jones and Paul E. Plassmann},
 doi = {10.1177/109434209300700204},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/109434209300700204},
 journal = {The International Journal of Supercomputing Applications},
 number = {2},
 pages = {129–143},
 title = {Computation of Equilibrium Vortex Structures for Type-II Superconductors},
 url = {https://doi-org.crai.referencistas.com/10.1177/109434209300700204},
 volume = {7},
 year = {1993i}
}

@article{doi:10.1177/109434209300700302,
 abstract = {One goal of experimental economics is to provide data to identify models that best describe the behavior of experimental subjects and, more generally, human economic behavior. We discuss here what we think are the three main steps required to make experimen tal investigations of economic games as statistically informative as possible: finding the solution of the ex perimental game under the postulated equilibrium or other economic models, selecting from a potential class of experimental designs the optimal one for dis criminating between those models, and choosing an optimal stopping rule that indicates when to stop sam pling data and accept one model as the best explana tion of the data. Each step can be computationally in tensive. We offer an algorithmic presentation of the necessary computations in each of the three steps and illustrate these procedures by examples from our re search on learning models in experimental games with incomplete information. These three steps of experi mental design and analysis are not limited to experi mental games, but the computational burden of imple menting these algorithms in other experimental envi ronments—for example, market experiments—requires further considerations with which we have not dealt.},
 author = {Mahmoud A. EI-Gamal and Richard D. McKelvey and Thomas R. Palfrey},
 doi = {10.1177/109434209300700302},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/109434209300700302},
 journal = {The International Journal of Supercomputing Applications},
 number = {3},
 pages = {189–200},
 title = {Computational Issues in the Statistical Design and Analysis of Experimental Games},
 url = {https://doi-org.crai.referencistas.com/10.1177/109434209300700302},
 volume = {7},
 year = {1993f}
}

@article{doi:10.1177/109434209400800206,
 author = {B. Averick and C. Bischof and B. Bixby and A. Carle and J. Dennis and M. El-Alem and A. EI-Bakry and A. Griewank and G. Johnson and R. Lewis et al.},
 doi = {10.1177/109434209400800206},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/109434209400800206},
 journal = {The International Journal of Supercomputer Applications and High Performance Computing},
 number = {2},
 pages = {143–153},
 title = {Numerical Optimization At the Center for Research On Parallel Computation},
 url = {https://doi-org.crai.referencistas.com/10.1177/109434209400800206},
 volume = {8},
 year = {1994a}
}

@article{doi:10.1177/109434209701100301,
 abstract = {We report on our experiences in building a computational environment for tomographic image analysis for marine seismologists studying the structure and evolution of mid- ocean ridge volcanism. The computational environment is determined by an evolving set of requirements for this problem domain and includes needs for high performance parallel computing, large data analysis, model visualiza tion, and computation interaction and control. Although these needs are not unique in scientific computing, the integration of techniques for seismic tomography with tools for parallel computing and data analysis into a com putational environment was (and continues to be) an interesting, important learning experience for researchers in both disciplines. For the geologists, the use of the environment led to fundamental geologic discoveries on the East Pacific Rise, the improvement of parallel ray-trac ing algorithms, and a better regard for the use of compu tational steering in aiding model convergence. The com puter scientists received valuable feedback on the use of programming, analysis, and visualization tools in the en vironment. In particular, the tools for parallel program data query (DAQV) and visualization programming (Viz) were demonstrated to be highly adaptable to the problem do main. We discuss the requirements and the components of the environment in detail. Both accomplishments and limitations of our work are presented.},
 author = {Janice E. Cuny and Robert A. Dunn and Steven T. Hackstadt and Christopher W. Harrop and Harold H. Hersey and Allen D. Malony and Douglas R. Toomey},
 doi = {10.1177/109434209701100301},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/109434209701100301},
 journal = {The International Journal of Supercomputer Applications and High Performance Computing},
 number = {3},
 pages = {179–196},
 title = {Building Domain-Specific Environments for Computational Science: a Case Study in Seismic Tomography},
 url = {https://doi-org.crai.referencistas.com/10.1177/109434209701100301},
 volume = {11},
 year = {1997c}
}

@article{doi:10.1177/109434209701100304,
 abstract = {This paper presents a new system, called NetSolve, that allows users to access computational resources, such as hardware and software, distributed across the network. The development of NetSolve was motivated by the need for an easy-to-use, efficient mechanism for using compu tational resources remotely. Ease of use is obtained as a result of different interfaces, some of which require no programming effort from the user. Good performance is ensured by a load-balancing policy that enables NetSolve to use the computational resources available as efficiently as possible. NetSolve offers the ability to look for compu tational resources on a network, choose the best one available, solve a problem (with retry for fault tolerance), and return the answer to the user.},
 author = {Henri Casanova and Jack Dongarra},
 doi = {10.1177/109434209701100304},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/109434209701100304},
 journal = {The International Journal of Supercomputer Applications and High Performance Computing},
 number = {3},
 pages = {212–223},
 title = {Netsolve: a Network-Enabled Server for Solving Computational Science Problems},
 url = {https://doi-org.crai.referencistas.com/10.1177/109434209701100304},
 volume = {11},
 year = {1997c}
}

@article{doi:10.1177/109434209801200304,
 abstract = {Recent developments in I/O systems on scalable parallel computers have sparked renewed interest in out-of-core methods for computational chemistry. These methods can improve execution time significantly relative to direct methods, which perform many redundant computations. However, the widespread use of such out-of-core methods requires efficient and portable implementations of often complex I/O patterns. The ChemlO project has addressed this problem by defining an I/O interface that captures the I/O patterns found in important computational chemistry applications and by providing high performance imple mentations of this interface on multiple platforms. This development broadens the user community for parallel I/O techniques and provides new insights into the functionality required in general purpose scalable I/O libraries and the techniques required to achieve high performance I/O on scalable parallel computers.},
 author = {Jarek Nieplocha and Ian Foster and Rick A. Kendall},
 doi = {10.1177/109434209801200304},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/109434209801200304},
 journal = {The International Journal of High Performance Computing Applications},
 number = {3},
 pages = {345–363},
 title = {ChemIo: High Performance Parallel I/o for Computational Chemistry Applications},
 url = {https://doi-org.crai.referencistas.com/10.1177/109434209801200304},
 volume = {12},
 year = {1998p}
}

@article{doi:10.1177/1094428104271998,
 abstract = {To encourage the use of computational modeling in organizational behavior research, an example computational model is developed and rigorous tests of it presented. Specifically, a computational model based on control theory was created to test the theory’s explanation of the goal-level effect (e.g., higher goals lead to higher performance). Data from simulations of the model were compared with the behavior of 32 undergraduate students performing a scheduling task under various within-subject manipulations and across time. Correlational analyses indicated that the model accounted for most of the participants’data, with coefficients between the model and each participant’s behavior mostly in the high 90s.},
 author = {Jeffrey B. Vancouver and Dan J. Putka and Charles A. Scherbaum},
 doi = {10.1177/1094428104271998},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094428104271998},
 journal = {Organizational Research Methods},
 number = {1},
 pages = {100–127},
 title = {Testing a Computational Model of the Goal-Level Effect: An Example of a Neglected Methodology},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094428104271998},
 volume = {8},
 year = {2005n}
}

@article{doi:10.1177/1094428112449655,
 abstract = {Theorists in management and organizational science rarely use computational modeling to support theoretical development or refinement, particularly at the micro level of analysis. This article argues that organizational scholars, who strive to understand dynamic behavior in a complex context, are particularly in need of the support computational models offer. Moreover, organizational scholars can build on (a) the plethora of informal theories extant in the literature and (b) the computational architectures and model building platforms developed in recent years. To increase the number of organizational scholars building and evaluating computational models, the article provides a tutorial in model building and simulation. Specifically, a new computational model is built and assessed. Surprising realizations emerge in the process. There is also an extensive section on model evaluation involving empirical observations.},
 author = {Jeffrey B. Vancouver and Justin M. Weinhardt},
 doi = {10.1177/1094428112449655},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094428112449655},
 journal = {Organizational Research Methods},
 number = {4},
 pages = {602–623},
 title = {Modeling the Mind and the Milieu: Computational Modeling for Micro-Level Organizational Researchers},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094428112449655},
 volume = {15},
 year = {2012o}
}

@article{doi:10.1177/1094428113484970,
 abstract = {Team faultlines—hypothetical dividing lines based on member attributes that split a team into relatively homogeneous subgroups—influence team processes across contexts, as recent meta-analytic findings show. We review the available faultline measures with regard to their properties and identify several limitations, including dealing with more than two subgroups. We thus propose a new cluster-based approach, average silhouette width (ASW), that identifies the number of subgroups and subgroup membership. We then compare the measures with 1,400 simulated teams with varying properties and investigate their factor structure and their behavior under missing values. We also investigate the predictive validity of the measures with data from real work teams. Results show that different measures respond to different team features in different ways but that most of them load on two correlated factors. Taken together, the ASW measure had the most favorable attributes and was the only measure that accurately determined subgroup membership in the presence of more than two subgroups. We discuss limitations and further research opportunities pertaining to faultline measures and provide software for calculating all investigated measures at http://www.group-faultlines.org.},
 author = {Bertolt Meyer and Andreas Glenz},
 doi = {10.1177/1094428113484970},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094428113484970},
 journal = {Organizational Research Methods},
 number = {3},
 pages = {393–424},
 title = {Team Faultline Measures: A Computational Comparison and a New Approach to Multiple Subgroups},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094428113484970},
 volume = {16},
 year = {2013n}
}

@article{doi:10.1177/1094428118780308,
 abstract = {Theories are the core of any science, but many imprecisely stated theories in organizational and management science are hampering progress in the field. Computational modeling of existing theories can help address the issue. Computational models are a type of formal theory that are represented mathematically or by other formal logic and can be simulated, allowing theorists to assess whether the theory can explain the phenomena intended as well as make testable predictions. As an example of the process, Locke’s integrated model of work motivation is translated into static and dynamic computational models. Simulations of these models are compared to the empirical data used to develop and test the theory. For the static model, the simulations revealed largely strong associations with robust empirical findings. However, adding dynamics created several challenges to key precepts of the theory. Moreover, the effort revealed where empirical work is needed to further refine or refute the theory. Discussion focuses on the value of computational modeling as a method for formally testing, pruning, and extending extant theories in the field.},
 author = {Jeffrey B. Vancouver and Mo Wang and Xiaofei Li},
 doi = {10.1177/1094428118780308},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094428118780308},
 journal = {Organizational Research Methods},
 number = {2},
 pages = {238–274},
 title = {Translating Informal Theories Into Formal Theories: The Case of the Dynamic Computational Model of the Integrated Model of Work Motivation},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094428118780308},
 volume = {23},
 year = {2020n}
}

@article{doi:10.1177/1094428119881209,
 abstract = {Some of the most influential theories in organizational sciences explicitly describe a dynamic, multilevel process. Yet the inherent complexity of such theories makes them difficult to test. These theories often describe multiple subprocesses that interact reciprocally over time at different levels of analysis and over different time scales. Computational (i.e., mathematical) modeling is increasingly advocated as a method for developing and testing theories of this type. In organizational sciences, however, efforts that have been made to test models empirically are often indirect. We argue that the full potential of computational modeling as a tool for testing dynamic, multilevel theory is yet to be realized. In this article, we demonstrate an approach to testing dynamic, multilevel theory using computational modeling. The approach uses simulations to generate model predictions and Bayesian parameter estimation to fit models to empirical data and facilitate model comparisons. This approach enables a direct integration between theory, model, and data that we believe enables a more rigorous test of theory.},
 author = {Timothy Ballard and Hector Palada and Mark Griffin and Andrew Neal},
 doi = {10.1177/1094428119881209},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094428119881209},
 journal = {Organizational Research Methods},
 number = {2},
 pages = {251–284},
 title = {An Integrated Approach to Testing Dynamic, Multilevel Theory: Using Computational Models to Connect Theory, Model, and Data},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094428119881209},
 volume = {24},
 year = {2021b}
}

@article{doi:10.1177/1094428121991230,
 abstract = {The substantial volume, continued growth, and resulting complexity of the scientific literature not only increases the need for systematic, replicable, and rigorous literature reviews, but also highlights the natural limits of human researchers’ information processing capabilities. In search of a solution to this dilemma, computational techniques are beginning to support human researchers in synthesizing large bodies of literature. However, actionable methodological guidance on how to design, conduct, and document such computationally augmented literature reviews is lacking to date. We respond by introducing and defining computational literature reviews (CLRs) as a new review method and put forward a six-step roadmap, covering the CLR process from identifying the review objectives to selecting algorithms and reporting findings. We make the CLR method accessible to novice and expert users alike by identifying critical design decisions and typical challenges for each step and provide practical guidelines for tailoring the CLR method to four conceptual review goals. As such, we present CLRs as a literature review method where the choice, design, and implementation of a CLR are guided by specific review objectives, methodological capabilities, and resource constraints of the human researcher.},
 author = {David Antons and Christoph F. Breidbach and Amol M. Joshi and Torsten Oliver Salge},
 doi = {10.1177/1094428121991230},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1094428121991230},
 journal = {Organizational Research Methods},
 number = {1},
 pages = {107–138},
 title = {Computational Literature Reviews: Method, Algorithms, and Roadmap},
 url = {https://doi-org.crai.referencistas.com/10.1177/1094428121991230},
 volume = {26},
 year = {2023a}
}

@article{doi:10.1177/10944281241261913,
 abstract = {Computational modeling holds significant promise as a tool for improving how theory is developed, expressed, and used to inform empirical research and evaluation efforts. However, the knowledge and skillsets needed to build computational models are rarely developed in the training received by social and organizational scientists. The purpose of this manuscript is to provide an accessible introduction to and reference for building computational models to represent theory. We first discuss important principles and recommendations for “thinking about” theory and developing explanatory accounts in ways that facilitate translating their core assumptions, specifications, and ideas into a computational model. Next, we address some frequently asked questions related to building computational models that introduce several fundamental tasks/concepts involved in building models to represent theory and demonstrate how they can be implemented in the R programming language to produce executable model code. The accompanying supplemental materials describes additional considerations relevant to building and using computational models, provides multiple examples of complete computational model code written in R, and an interactive application offering guided practice on key model-building tasks/concepts in R.},
 author = {James A. Grand and Michael T. Braun and Goran Kuljanin},
 doi = {10.1177/10944281241261913},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/10944281241261913},
 journal = {Organizational Research Methods},
 number = {0},
 pages = {10944281241261912},
 title = {Hello World! Building Computational Models to Represent Social and Organizational Theory},
 url = {https://doi-org.crai.referencistas.com/10.1177/10944281241261913},
 volume = {0},
 year = {2024b}
}

@article{doi:10.1177/10949968221095546,
 abstract = {This study illuminates the varied emotional mechanisms underlying consumer response to ads paired with emotionally congruent versus incongruent content in different placement positions. This work expands the media planning literature that has narrowly focused on thematic (in)congruency. Focusing on music videos, Study 1 empirically tests the affect regulation effect on consumer response to ads paired with emotionally incongruent music videos and the affect priming effect in congruent pairings. Furthermore, this study incorporates affective computing algorithms to pair ads with music videos based on emotional (in)congruency to advance the emerging field of computational advertising. The results from two experimental studies demonstrate that consumers prefer the emotional flow from a negative to a positive state when the ad and media context are emotionally incongruent. In the emotional congruency condition, regardless of ad position, the positively valenced ad produces more favorable responses. Study 2 further illuminates the boundary conditions of emotional (in)congruency and ad valence, suggesting that consumers’ preference for positively valenced ads in the affect regulation and affect priming processes is more prominent when consumers are less involved. When involvement level is high, negative ads are rated as more persuasive than positive ones. Involvement level also reduces ad skipping, especially when the ad and the media program are emotionally incongruent.},
 author = {Taylor Jing Wen and Ching-Hua Chuan and Wanhsiu Sunny Tsai and Jing Yang},
 doi = {10.1177/10949968221095546},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/10949968221095546},
 journal = {Journal of Interactive Marketing},
 number = {3},
 pages = {421–441},
 title = {Decoding Emotional (In)Congruency: A Computational Approach Toward Ad Placement on YouTube},
 url = {https://doi-org.crai.referencistas.com/10.1177/10949968221095546},
 volume = {57},
 year = {2022r}
}

@article{doi:10.1177/10962506221145674,
 author = {Hsiu-Wen Yang and Philippa H. Campbell and Chih-Ing Lim},
 doi = {10.1177/10962506221145674},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/10962506221145674},
 journal = {Young Exceptional Children},
 number = {4},
 pages = {220–232},
 title = {Supporting STEM Learning Within Routines for Infants and Toddlers With Developmental Delays},
 url = {https://doi-org.crai.referencistas.com/10.1177/10962506221145674},
 volume = {26},
 year = {2023v}
}

@article{doi:10.1177/109804821702100210,
 abstract = {Advertising technology is advancing quickly incorporating digital techniques that may be beyond the experience of the individual faculty member. Collaborative teaching, where faculty members from different disciplines co-teach a course, may be a solution. This report assesses the learning outcomes of an advertising technology course taught by faculty from one university’s advertising, computer science and human-computer interaction programs. The course was run twice, with a third one in progress. Students were predominantly advertising majors, with a minority of computer science and design majors. Two semesters of pre- and post-tests were analyzed, finding increases in student comfort with preparing and presenting technologically advanced solutions to advertising challenges.},
 author = {Jay Newell and Wallapak Tavanapong and Sherry Berghefer},
 doi = {10.1177/109804821702100210},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/109804821702100210},
 journal = {Journal of Advertising Education},
 number = {2},
 pages = {45–53},
 title = {Teaching Ad Tech: Assessing Collaborative Teaching in an Advertising, Computer Science and Design Course},
 url = {https://doi-org.crai.referencistas.com/10.1177/109804821702100210},
 volume = {21},
 year = {2017i}
}

@article{doi:10.1177/1129729820944069,
 abstract = {Introduction: The volume of blood flowing through the vascular access is an important parameter necessary to provide adequate dialysis for a functional arteriovenous fistula. Higher blood flows are seen in arteriovenous access that receive inflow from larger arteries such as brachial or axillary compared to those based on medium-caliber radial or ulnar arteries. We hypothesized that an anatomic difference in the length and the diameter of the artery is an important determinant of the flow volume in arteriovenous fistula created at different anatomic locations. Methods: Using computational fluid dynamics, we evaluated the contribution of the length and diameter of inflow artery on simulations performed with geometric models constructed to represent arteriovenous fistula circuits. Lengths and diameters of the inflow artery were altered to mimic arteriovenous fistula created at various locations of the upper extremity with standard and variant anatomy. Results: Models of arteriovenous fistula created with variable lengths and diameters of the inflow artery suggest that the length of the vessel has an inverse linear relationship and the diameter has a direct linear relationship to flow volume. Conclusion: Computational fluid dynamic modeling of arteriovenous fistula can be used to understand the physiologic basis of clinical observations of function. Evaluation of the effect of inflow artery length and diameter helps explain the higher flows seen in arteriovenous fistula created using large caliber arteries for inflow. Computational fluid dynamic modeling helps operators understand the contributions of inflow artery in access function and can guide anastomotic site selection.},
 author = {Jeffrey Krampf and Ramesh Agarwal and Surendra Shenoy},
 doi = {10.1177/1129729820944069},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1129729820944069},
 journal = {The Journal of Vascular Access},
 note = {PMID:32729767},
 number = {3},
 pages = {417–423},
 title = {Contribution of inflow artery to observed flow in a vascular access: A computational fluid dynamic modeling study of an arteriovenous fistula circuit},
 url = {https://doi-org.crai.referencistas.com/10.1177/1129729820944069},
 volume = {22},
 year = {2021j}
}

@article{doi:10.1177/11356405241268982,
 abstract = {Innovative information technology is developing rapidly and is penetrating many areas of life, including education. The paper measures the impact of innovative information technology, using augmented reality (AR) as an example, on the development of creativity and critical thinking among students. The study was conducted among 180 students from Tonghua Normal University in China. The research design: a pre-test and post-test experiment with a control group. The following research findings were obtained: academic knowledge of the experimental group was higher (15.73) than that of the control group (12.13). ARLE model improved critical thinking (8.77 in AR group, 7.60 in the traditional group) and creativity (593 in AR group, 570 in the traditional group). The resulting data demonstrated the effectiveness of the proposed model of learning with the augmented reality technology. The research findings have practical implications. For example, they may be used to create new augmented reality apps for other disciplines and majors. The study presents recommendations on the development of creative and critical thinking, which teachers of various disciplines can apply in practice. The impact of other digital tools in education needs to be carefully studied to determine their effectiveness.},
 author = {Tao Li},
 doi = {10.1177/11356405241268982},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/11356405241268982},
 journal = {Culture and Education},
 number = {3},
 pages = {571–601},
 title = {The influence of information technologies on creative and critical thinking of students / La influencia de las tecnologías de la información en el pensamiento crítico y creativo de los estudiantes},
 url = {https://doi-org.crai.referencistas.com/10.1177/11356405241268982},
 volume = {36},
 year = {2024l}
}

@article{doi:10.1177/1176934318759299,
 abstract = {Modern phylogenetic studies may benefit from the analysis of complete genome sequences of various microorganisms. Evolutionary inferences based on genome-scale analysis are believed to be more accurate than the gene-based alternative. However, the computational complexity of current phylogenomic procedures, inappropriateness of standard phylogenetic tools to process genome-wide data, and lack of reliable substitution models which correlates with alignment-free phylogenomic approaches deter microbiologists from using these opportunities. For example, the super-matrix and super-tree approaches of phylogenomics use multiple integrated genomic loci or individual gene-based trees to infer an overall consensus tree. However, these approaches potentially multiply errors of gene annotation and sequence alignment not mentioning the computational complexity and laboriousness of the methods. In this article, we demonstrate that the annotation- and alignment-free comparison of genome-wide tetranucleotide frequencies, termed oligonucleotide usage patterns (OUPs), allowed a fast and reliable inference of phylogenetic trees. These were congruent to the corresponding whole genome super-matrix trees in terms of tree topology when compared with other known approaches including 16S ribosomal RNA and GyrA protein sequence comparison, complete genome-based MAUVE, and CVTree methods. A Web-based program to perform the alignment-free OUP-based phylogenomic inferences was implemented at http://swphylo.bi.up.ac.za/. Applicability of the tool was tested on different taxa from subspecies to intergeneric levels. Distinguishing between closely related taxonomic units may be enforced by providing the program with alignments of marker protein sequences, eg, GyrA.},
 author = {Xiaoyu Yu and Oleg N Reva},
 doi = {10.1177/1176934318759299},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1176934318759299},
 journal = {Evolutionary Bioinformatics},
 note = {PMID:29511354},
 number = { },
 pages = {1176934318759299},
 title = {SWPhylo – A Novel Tool for Phylogenomic Inferences by Comparison of Oligonucleotide Patterns and Integration of Genome-Based and Gene-Based Phylogenetic Trees},
 url = {https://doi-org.crai.referencistas.com/10.1177/1176934318759299},
 volume = {14},
 year = {2018t}
}

@article{doi:10.1177/1176934319840289,
 abstract = {Reversible phosphorylation maintained by protein kinases and phosphatases is an integral part of intracellular signalling, and phosphorylation on tyrosine is extensively utilised in higher eukaryotes. Tyrosine phosphatases are enzymes that not only scavenge phosphotyrosine but are also involved in wide range of signalling pathways. As a result, mutations in these enzymes have been implicated in the pathogenesis of several diseases like cancer, autoimmune disorders, and muscle-related diseases. The genes that harbour phosphatase domain also display diversity in co-existing domains suggesting the recruitment of the catalytic machinery in diverse pathways. We have examined the current draft of the human genome, using a combination of 3 sequence search methods and validations, and identified 101 genes encoding tyrosine phosphatase-containing gene products, agreeing with previous reports. Such gene products adopt 37 unique domain architectures (DAs), including few new ones and harbouring few co-existing domains that have not been reported before. This semi-automated computational approach for detection of gene products belonging to a particular superfamily can now be easily applied at whole genome level on other mammalian genomes and for other protein domains as well.},
 author = {Teerna Bhattacharyya and Ramanathan Sowdhamini},
 doi = {10.1177/1176934319840289},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1176934319840289},
 journal = {Evolutionary Bioinformatics},
 note = {PMID:31007525},
 number = { },
 pages = {1176934319840289},
 title = {Genome-Wide Search for Tyrosine Phosphatases in the Human Genome Through Computational Approaches Leads to the Discovery of Few New Domain Architectures},
 url = {https://doi-org.crai.referencistas.com/10.1177/1176934319840289},
 volume = {15},
 year = {2019a}
}

@article{doi:10.1177/1176934319892284,
 abstract = {Aquatic ecosystems that form major biodiversity hotspots are critically threatened due to environmental and anthropogenic stressors. We believe that, in this genomic era, computational methods can be applied to promote aquatic biodiversity conservation by addressing questions related to the evolutionary history of aquatic organisms at the molecular level. However, huge amounts of genomics data generated can only be discerned through the use of bioinformatics. Here, we examine the applications of next-generation sequencing technologies and bioinformatics tools to study the molecular evolution of aquatic animals and discuss the current challenges and future perspectives of using bioinformatics toward aquatic animal conservation efforts.},
 author = {Min Pau Tan and Li Lian Wong and Siti Aisyah Razali and Nor Afiqah-Aleng and Siti Azizah Mohd Nor and Yeong Yik Sung and Yves Van de Peer and Patrick Sorgeloos and Muhd Danish-Daniel},
 doi = {10.1177/1176934319892284},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1176934319892284},
 journal = {Evolutionary Bioinformatics},
 note = {PMID:31839703},
 number = { },
 pages = {1176934319892284},
 title = {Applications of Next-Generation Sequencing Technologies and Computational Tools in Molecular Evolution and Aquatic Animals Conservation Studies: A Short Review},
 url = {https://doi-org.crai.referencistas.com/10.1177/1176934319892284},
 volume = {15},
 year = {2019q}
}

@article{doi:10.1177/11769343221108218,
 abstract = {Introduction: In an effort to combat SARS-CoV-2 through multi-subunit vaccine design, during studies using whole genome and immunome, ORF10, located at the 3′ end of the genome, displayed unique features. It showed no homology to any known protein in other organisms, including SARS-CoV. It was observed that its nucleotide sequence is 100% identical in the SARS-CoV-2 genomes sourced worldwide, even in the recent-most VoCs and VoIs of B.1.1.529 (Omicron), B.1.617 (Delta), B.1.1.7 (Alpha), B.1.351 (Beta), and P.1 (Gamma) lineages, implicating its constant nature throughout the evolution of deadly variants. Aim: The structure and function of SARS-CoV-2 ORF10 and the role it may play in the viral evolution is yet to be understood clearly. The aim of this study is to predict its structure, function, and understand evolutionary dynamics on the basis of mutations and likely heightened immune responses in the immunopathogenesis of this deadly virus. Methods: Sequence analysis, ab-initio structure modeling and an understanding of the impact of likely substitutions in key regions of protein was carried out. Analyses of viral T cell epitopes and primary anchor residue mutations was done to understand the role it may play in the evolution as a molecule with likely enhanced immune response and consequent immunopathogenesis. Results: Few amino acid substitution mutations are observed, most probably due to the ribosomal frameshifting, and these mutations may not be detrimental to its functioning. As ORF10 is observed to be an expressed protein, ab-initio structure modeling shows that it comprises mainly an α-helical region and maybe an ER-targeted membrane mini-protein. Analyzing the whole proteome, it is observed that ORF10 presents amongst the highest number of likely promiscuous and immunogenic CTL epitopes, specifically 11 out of 30 promiscuous ones and 9 out of these 11, immunogenic CTL epitopes. Reactive T cells to these epitopes have been uncovered in independent studies. Majority of these epitopes are located on the α-helix region of its structure, and the substitution mutations of primary anchor residues in these epitopes do not affect immunogenicity. Its conserved nucleotide sequence throughout the evolution and diversification of virus into several variants is a puzzle yet to be solved. Conclusions: On the basis of its sequence, structure, and epitope mapping, it is concluded that it may function like those mini-proteins used to boost immune responses in medical applications. Due to the complete nucleotide sequence conservation even a few years after SARS-CoV-2 genome was first sequenced, it poses a unique puzzle to be solved, in view of the evolutionary dynamics of variants emerging in the populations worldwide.},
 author = {Seema Mishra},
 doi = {10.1177/11769343221108218},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/11769343221108218},
 journal = {Evolutionary Bioinformatics},
 note = {PMID:35909986},
 number = { },
 pages = {11769343221108218},
 title = {Computational Structural and Functional Analyses of ORF10 in Novel Coronavirus SARS-CoV-2 Variants to Understand Evolutionary Dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1177/11769343221108218},
 volume = {18},
 year = {2022l}
}

@article{doi:10.1177/11769343221110654,
 abstract = {The idea of computational processes, which take place in nature, for example, DNA computation, is discussed in the literature. DNA computation that is going on in the immunoglobulin locus of vertebrates shows how the computations in the biological possibility space could operate during evolution. We suggest that the origin of evolutionarily novel genes and genome evolution constitute the original intrinsic computation of the information about new structures in the space of unrealized biological possibilities. Due to DNA computation, the information about future structures is generated and stored in DNA as genetic information. In evolving ontogenies, search algorithms are necessary, which can search for information about evolutionary innovations and morphological novelties. We believe that such algorithms include stochastic gene expression, gene competition, and compatibility search at different levels of structural organization. We formulate the increase in complexity principle in terms of biological computation and hypothesize the possibility of in silico computing of future functions of evolutionarily novel genes.},
 author = {AP Kozlov},
 doi = {10.1177/11769343221110654},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/11769343221110654},
 journal = {Evolutionary Bioinformatics},
 number = { },
 pages = {11769343221110654},
 title = {Biological Computation and Compatibility Search in the Possibility Space as the Mechanism of Complexity Increase During Progressive Evolution},
 url = {https://doi-org.crai.referencistas.com/10.1177/11769343221110654},
 volume = {18},
 year = {2022j}
}

@article{doi:10.1177/11769343231182258,
 abstract = {SARS-CoV-2 has been highly susceptible to mutations since its emergence in Wuhan, China, and its subsequent propagation due to containing an RNA as its genome. The emergence of variants with improved transmissibility still poses a grave threat to global health. The spike protein mutation is mainly responsible for higher transmissibility and risk severity. This study retrieved SARS-CoV-2 variants structural and nonstructural proteins (NSPs) sequences from several geographic locations, including Africa, Asia, Europe, Oceania, and North and South America. First, multiple sequence alignments with BioEdit and protein homology modeling were performed using the SWISS Model. Then the structure visualization and structural analysis were performed by superimposing against the Wuhan sequence by Pymol to retrieve the RMSD values. Sequence alignment revealed familiar, uncommon regional among variants and, interestingly, a few unique mutations in Beta, Delta, and Omicron. Structural analysis of such unique mutations revealed that they caused structural deviations in Beta, Delta, and Omicron spike proteins. In addition, these variants were more severe in terms of hospitalization, sickness, and higher mortality, which have a substantial relationship with the structural deviations because of those unique mutations. Such evidence provides insight into the SARS-CoV-2 spike protein vulnerability toward mutation and their structural and functional deviations, particularly in Beta, Delta, and Omicron, which might be the cause of their broader coverage. This knowledge can help us with regional vaccine strain selection, virus pathogenicity testing, diagnosis, and treatment with more specific vaccines.},
 author = {Mohammad Mamun Alam and Sumaiya Binte Hannan and Tanvir Ahmed Saikat and Md Belayet Hasan Limon and Md Raihan Topu and Md Jowel Rana and Asma Salauddin and Sagar Bosu and Mohammed Ziaur Rahman},
 doi = {10.1177/11769343231182258},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/11769343231182258},
 journal = {Evolutionary Bioinformatics},
 number = { },
 pages = {11769343231182258},
 title = {Beta, Delta, and Omicron, Deadliest Among SARS-CoV-2 Variants: A Computational Repurposing Approach},
 url = {https://doi-org.crai.referencistas.com/10.1177/11769343231182258},
 volume = {19},
 year = {2023b}
}

@article{doi:10.1177/1176935120942216,
 abstract = {Genetic variations such as single nucleotide polymorphisms (SNPs) can cause susceptibility to cancer. Although thousands of genetic variants have been identified to be associated with different cancers, the molecular mechanisms of cancer remain unknown. There is not a particular dataset of relationships between cancer and SNPs, as a bipartite network, for computational analysis and prediction. Link prediction as a computational graph analysis method can help us to gain new insight into the network. In this article, after creating a network between cancer and SNPs using SNPedia and Cancer Research UK databases, we evaluated the computational link prediction methods to foresee new SNP-Cancer relationships. Results show that among the popular scoring methods based on network topology, for relation prediction, the preferential attachment (PA) algorithm is the most robust method according to computational and experimental evidence, and some of its computational predictions are corroborated in recent publications. According to the PA predictions, rs1801394-Non-small cell lung cancer, rs4880-Non-small cell lung cancer, and rs1805794-Colorectal cancer are some of the best probable SNP-Cancer associations that have not yet been mentioned in any published article, and they are the most probable candidates for additional laboratory and validation studies. Also, it is feasible to improve the predicting algorithms to produce new predictions in the future.},
 author = {Shahab Bakhtiari and Sadegh Sulaimany and Mehrdad Talebi and Kabmiz Kalhor},
 doi = {10.1177/1176935120942216},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1176935120942216},
 journal = {Cancer Informatics},
 note = {PMID:32728337},
 number = { },
 pages = {1176935120942216},
 title = {Computational Prediction of Probable Single Nucleotide Polymorphism-Cancer Relationships},
 url = {https://doi-org.crai.referencistas.com/10.1177/1176935120942216},
 volume = {19},
 year = {2020b}
}

@article{doi:10.1177/117762500700100010,
 abstract = {In recent years it has become clear that carcinogenesis is a complex process, both at the molecular and cellular levels. Understanding the origins, growth and spread of cancer, therefore requires an integrated or system-wide approach. Computational systems biology is an emerging sub-discipline in systems biology that utilizes the wealth of data from genomic, proteomic and metabolomic studies to build computer simulations of intra and intercellular processes. Several useful descriptive and predictive models of the origin, growth and spread of cancers have been developed in an effort to better understand the disease and potential therapeutic approaches. In this review we describe and assess the practical and theoretical underpinnings of commonly-used modeling approaches, including ordinary and partial differential equations, petri nets, cellular automata, agent based models and hybrid systems. A number of computer-based formalisms have been implemented to improve the accessibility of the various approaches to researchers whose primary interest lies outside of model development. We discuss several of these and describe how they have led to novel insights into tumor genesis, growth, apoptosis, vascularization and therapy.},
 author = {Wayne Materi and David S. Wishart},
 doi = {10.1177/117762500700100010},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/117762500700100010},
 journal = {Gene Regulation and Systems Biology},
 number = { },
 pages = {117762500700100020},
 title = {Computational Systems Biology in Cancer: Modeling Methods and Applications},
 url = {https://doi-org.crai.referencistas.com/10.1177/117762500700100010},
 volume = {1},
 year = {2007p}
}

@article{doi:10.1177/1177932217712471,
 abstract = {We present an approach for detecting enzymes that are specific of Leishmania major compared with Homo sapiens and provide targets that may assist research in drug development. This approach is based on traditional techniques of sequence homology comparison by similarity search and Markov modeling; it integrates the characterization of enzymatic functionality, secondary and tertiary protein structures, protein domain architecture, and metabolic environment. From 67 enzymes represented by 42 enzymatic activities classified by AnEnPi (Analogous Enzymes Pipeline) as specific for L major compared with H sapiens, only 40 (23 Enzyme Commission [EC] numbers) could actually be considered as strictly specific of L major and 27 enzymes (19 EC numbers) were disregarded for having ambiguous homologies or analogies with H sapiens. Among the 40 strictly specific enzymes, we identified sterol 24-C-methyltransferase, pyruvate phosphate dikinase, trypanothione synthetase, and RNA-editing ligase as 4 essential enzymes for L major that may serve as targets for drug development.},
 author = {Larissa Catharina and Carlyle Ribeiro Lima and Alexander Franca and Ana Carolina Ramos Guimarães and Marcelo Alves-Ferreira and Pierre Tuffery and Philippe Derreumaux and Nicolas Carels},
 doi = {10.1177/1177932217712471},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1177932217712471},
 journal = {Bioinformatics and Biology Insights},
 note = {PMID:28638238},
 number = { },
 pages = {1177932217712471},
 title = {A Computational Methodology to Overcome the Challenges Associated With the Search for Specific Enzyme Targets to Develop Drugs Against Leishmania major},
 url = {https://doi-org.crai.referencistas.com/10.1177/1177932217712471},
 volume = {11},
 year = {2017d}
}

@article{doi:10.1177/11779322211021430,
 abstract = {Background: A recent COVID-19 pandemic has resulted in a large death toll rate globally and even no cure or vaccine has been successfully employed to combat this disease. Patients have been reported with multi-organ dysfunction along with acute respiratory distress syndrome which implies a critical situation for patients and made them difficult to breathe and survive. Moreover, pathology of COVID-19 is also related to cytokine storm which indicates the elevated levels of interleukin (IL)-1, IL-6, IL-12, and IL-18 along with tumor necrosis factor (TNF)-α. Among them, the proinflammatory cytokine IL-6 has been reported to be induced via binding of severe acute respiratory syndrome coronavirus 2 (SARS)-CoV-2 to the host receptors. Methodology: Interleukin-6 blockade has been proposed to constitute novel therapeutics against COVID-19. Thus, in this study, 15 phytocompounds with known antiviral activity have been subjected to test for their inhibitory effect on IL-6. Based on the affinity prediction, top 3 compounds (isoorientin, lupeol, and andrographolide) with best scores were selected for 50 ns molecular dynamics simulation and MMGB/PBSA binding free energy analysis. Results: Three phytocompounds including isoorientin, lupeol, and andrographolide have shown strong interactions with the targeted protein IL-6 with least binding energies (−7.1 to −7.7 kcal/mol). Drug-likeness and ADMET profiles of prioritized phytocompounds are also very prominsing and can be further tested to be potential IL-6 blockers and thus benficial for COVID-19 treatment. The moelcular dynamics simulation couple with MMGB/PBSA binding free energy estimation validated conformational stability of the ligands and stronger intermolecular binding. The mean RMSD of the complexes is as: IL6-isoorientin complex (3.97 Å ± 0.77), IL6-lupeol (3.97 Å ± 0.76), and IL6-andrographolide complex (3.96 Å ± 0.77). In addition, the stability observation was affirmed by compounds mean RMSD: isoorientin (0.72 Å ± 0.32), lupeol (mean 0.38 Å ± 0.08), and andrographolide (1.09 Å ± 0.49). A similar strong agreement on systems stability was unraveled by MMGB/PBSA that found net binding net ~ −20 kcal/mol for the complexes dominated by van der Waal interaction energy. Conclusion: It has been predicted that proposing potential IL-6 inhibitors with less side effects can help critical COVID-19 patients because it may control the cytokine storm, a major responsible factor of its pathogenesis. In this study, 3 potential phytocompounds have been proposed to have inhibitory effect on IL-6 that can be tested as potential therapeutic options against SARS-CoV-2.},
 author = {Arif Malik and Anam Naz and Sajjad Ahmad and Mansoor Hafeez and Faryal Mehwish Awan and Tassadaq Hussain Jafar and Ayesha Zahid and Aqsa Ikram and Bisma Rauff and Mubashir Hassan},
 doi = {10.1177/11779322211021430},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/11779322211021430},
 journal = {Bioinformatics and Biology Insights},
 note = {PMID:34163151},
 number = { },
 pages = {11779322211021430},
 title = {Inhibitory Potential of Phytochemicals on Interleukin-6-Mediated T-Cell Reduction in COVID-19 Patients: A Computational Approach},
 url = {https://doi-org.crai.referencistas.com/10.1177/11779322211021430},
 volume = {15},
 year = {2021l}
}

@article{doi:10.1177/11786221241259949,
 abstract = {The concept of water quality has often generally revolved around the all-round safety of water for human consumption. The quality of much of the 3% of the earth’s humanly consumable water classed as freshwater is under threat of climate change, rising population numbers, indiscriminate land usage, detrimental agricultural practices and contamination from poor waste management. The need for optimal water quality enhancement has become more germane to sustainable socio-economic development. This paper examines the evolution of efforts made by the scientific community over the years to ensure water quality can be characterized and properly managed to ensure the global ever-growing demand for clean water for human consumption is continually met. The development of state-of-the-art computational decision support systems (DSS) should play a vital role. However, efforts in this regard are currently bedevilled by major challenges such as quantifying, measuring, processing and controlling the numerous metrics of water quality, as well as their adaptation and integration into a fully developed universal water quality model. In addressing these challenges, a shift towards simpler modelling approaches and the integration of uni-purpose models which can be cascaded into decision-making systems is being popularly proposed. However, with technological advancements already stimulating a water quality management revolution, there is a shift in paradigm to more universal modelling attempts with great optimism towards overcoming the challenges of developing universal water quality models and DSS. The prospects and opportunities of a water quality management renaissance offered by radical scientific innovations look promising, as the world races with time to provide support systems that can help deal better with the dynamics of sustainable water supply in increasingly contaminable environments and progressively unpredictable climates.},
 author = {Festus Oluwadare Fameso and Julius Musyoka Ndambuki and Williams Kehinde Kupolati and Jacques Snyman},
 doi = {10.1177/11786221241259949},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/11786221241259949},
 journal = {Air, Soil and Water Research},
 number = { },
 pages = {11786221241259948},
 title = {On the Development of State-of-the-Art Computational Decision Support Systems for Efficient Water Quality Management: Prospects and Opportunities in a Climate Changing World},
 url = {https://doi-org.crai.referencistas.com/10.1177/11786221241259949},
 volume = {17},
 year = {2024h}
}

@article{doi:10.1177/117959720900100001,
 author = {Kayvan Najarian},
 doi = {10.1177/117959720900100001},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/117959720900100001},
 journal = {Biomedical Engineering and Computational Biology},
 number = { },
 pages = {117959720900100000},
 title = {Biomedical Engineering and Computational Biology},
 url = {https://doi-org.crai.referencistas.com/10.1177/117959720900100001},
 volume = {1},
 year = {2009m}
}

@article{doi:10.1177/1326365X18769395,
 abstract = {This commentary draws on a decade’s experience of teaching data journalism within a variety of contexts to describe the lessons learned regarding different pedagogical techniques and choices about the aspects of data journalism to teach. What emerges is a difference between classes aimed at a general audience, who might be sceptical and/or ignorant of the diversity of data journalism practice and those aimed at a more specialist audience aiming to go into the increasing numbers of roles dedicated to data-driven techniques.},
 author = {Paul Bradshaw},
 doi = {10.1177/1326365X18769395},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1326365X18769395},
 journal = {Asia Pacific Media Educator},
 number = {1},
 pages = {55–66},
 title = {Data Journalism Teaching, Fast and Slow},
 url = {https://doi-org.crai.referencistas.com/10.1177/1326365X18769395},
 volume = {28},
 year = {2018d}
}

@article{doi:10.1177/1326365X20923200,
 abstract = {With emergent subspecialties like data journalism bringing new skillsets and job roles, professionals and journalism educators find it difficult to imbibe the fast-changing industry demands. Such challenges in some countries and media industries put journalism educators in an advantageous position, offering them an agency to actively shape the contours of industry practice than getting shaped by it. From this perspective, the present study tries to understand data journalism practices in India and suggests certain insights to integrate data journalism training in programmes offered by Indian journalism education. By probing insights from the literature on data journalism education and by examining existing data journalism practices in India, the study calls for intervention with a pedagogic strategy to impart better data-sourcing practices, coding skills and critical data literacy among the students as an antidote to the prevalent DIY culture and overdependence on data aggregates. The pedagogic strategy should convey the importance of audience centrality and ethics in data journalism practice. It argues that such an approach can, in effect, improve industry practices as well as the quality of journalism education in India.},
 author = {Geeta Kashyap and Harikrishnan Bhaskaran},
 doi = {10.1177/1326365X20923200},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1326365X20923200},
 journal = {Asia Pacific Media Educator},
 number = {1},
 pages = {44–58},
 title = {Teaching Data Journalism: Insights for Indian Journalism Education},
 url = {https://doi-org.crai.referencistas.com/10.1177/1326365X20923200},
 volume = {30},
 year = {2020e}
}

@article{doi:10.1177/1326365X20970421,
 author = {Kayt Davies},
 doi = {10.1177/1326365X20970421},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1326365X20970421},
 journal = {Asia Pacific Media Educator},
 number = {2},
 pages = {234–242},
 title = {Why I Do Not Talk About Computational Thinking in Journalism Classes: Sorry (Not Really Sorry)},
 url = {https://doi-org.crai.referencistas.com/10.1177/1326365X20970421},
 volume = {30},
 year = {2020b}
}

@article{doi:10.1177/1329878X221086042,
 author = {Luke Heemsbergen and Emiliano Treré and Gabriel Pereira},
 doi = {10.1177/1329878X221086042},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1329878X221086042},
 journal = {Media International Australia},
 number = {1},
 pages = {3–15},
 title = {Introduction to algorithmic antagonisms: Resistance, reconfiguration, and renaissance for computational life},
 url = {https://doi-org.crai.referencistas.com/10.1177/1329878X221086042},
 volume = {183},
 year = {2022g}
}

@article{doi:10.1177/1350507615592113,
 abstract = {In this conceptual article, the relations between sensemaking, learning, and big data in organizations are explored. The availability and usage of big data by organizations is an issue of emerging importance, raising new and old themes for diverse commentators and researchers to investigate. Drawing on sensemaking, learning, and complexity perspectives, this article highlights four key challenges to be addressed if organizations are to engage the phenomenon of big data effectively and reflexively: responding to the dynamic complexity of big data in terms of “simplexity,” analyzing big data using interdisciplinary processes, responsible reflection on ideologies of learning and knowledge production when handling big data, and mutually aligning sensemaking with big data topics to map domains of application. This article concludes with additional implications arising from considering sensemaking in conjunction with big data analytics as a critical way of understanding unique aspects of learning and technology in the 21st century.},
 author = {Thomas Stephen Calvard},
 doi = {10.1177/1350507615592113},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1350507615592113},
 journal = {Management Learning},
 number = {1},
 pages = {65–82},
 title = {Big data, organizational learning, and sensemaking: Theorizing interpretive challenges under conditions of dynamic complexity},
 url = {https://doi-org.crai.referencistas.com/10.1177/1350507615592113},
 volume = {47},
 year = {2016d}
}

@article{doi:10.1177/1350507620909130,
 author = {Mark Dawson},
 doi = {10.1177/1350507620909130},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1350507620909130},
 journal = {Management Learning},
 number = {5},
 pages = {652–657},
 title = {Book Review: The Age of Disruption: Technology and Madness in Computational Capitalism},
 url = {https://doi-org.crai.referencistas.com/10.1177/1350507620909130},
 volume = {52},
 year = {2021f}
}

@article{doi:10.1177/1350508408100474,
 abstract = {The paper seeks to lay open the computational logic by which reality is rendered as information. Computation is claimed to involve a drift away from the palpable and extendible character of things, a trend that both continues and breaks with the prevailing strategies of technological mediation in industrialism and modernity. Computation entails the relentless analytic reduction of the composite character and complexion of the world. Reality is meticulously dissolved and regained after a long analytic retreat and technological reconstruction. The outcome of this analytic strategy is that processes taking place at the human-technology interface are sustained by an elaborate vertical stratification, entailing a variety of other programmes and systems that reach down from the level of the interface to machine language and the mechanics of binary parsing. The deepening involvement of computation in instrumental settings thus reframes the perceptive and action modalities by which human agents confront the world. This way, a coherent set of techniques for building up reality is established accompanied by a new model of human agency that increasingly takes the form of a combinatoria of data and information items, remaking the shape of things out of the digital fragments produced by computation.},
 author = {Jannis Kallinikos},
 doi = {10.1177/1350508408100474},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1350508408100474},
 journal = {Organization},
 number = {2},
 pages = {183–202},
 title = {On the Computational Rendition of Reality: Artefacts and Human Agency},
 url = {https://doi-org.crai.referencistas.com/10.1177/1350508408100474},
 volume = {16},
 year = {2009j}
}

@article{doi:10.1177/1350650111428028,
 abstract = {Elastohydrodynamic lubrication modelling plays an important role in engineering design and analysis, since a number of important mechanical components operate under elastohydrodynamic lubrication conditions. In this article, methods are presented for solving both line and point contact cases using multiphysics software. The advantages, and the overheads, of using such an approach over developing highly specialised, bespoke software are highlighted. In order to calculate the deformation of the contacts three different methods are developed and their relative performance is assessed. The advantage of using a nested solution strategy has also been examined. The flexibility of the multiphysics software approach is highlighted in results involving a complex transient case modelling an involute gear.},
 author = {Xincai Tan and Christopher E Goodyer and Peter K Jimack and Robert I Taylor and Mark A Walkley},
 doi = {10.1177/1350650111428028},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1350650111428028},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part J: Journal of Engineering Tribology},
 number = {6},
 pages = {463–480},
 title = {Computational approaches for modelling elastohydrodynamic lubrication using multiphysics software},
 url = {https://doi-org.crai.referencistas.com/10.1177/1350650111428028},
 volume = {226},
 year = {2012s}
}

@article{doi:10.1177/1350650112466769,
 abstract = {Interfacial phenomena between the wafer and the polishing pad during chemical mechanical polishing are an area of great interest as they affect post-chemical mechanical polishing wafer topographies. Traditionally, the Reynolds equation has been used to predict the fluid pressure between the wafer and the polishing pad. However, with computational fluid dynamics it is possible to predict the fluid pressure and obtain insight into the fluid motion at the leading edge and trailing edge of the wafer. Additionally, computational fluid dynamics allows for the added ability to increase the resolution of the fluid physics to the asperity scale. In this study, a model is developed to predict phenomena related to mixed lubrication chemical mechanical polishing using computational fluid dynamics. Contact mechanics between the wafer and the pad are resolved through a Winkler elastic foundation formulation. The wafer is mounted on a ball joint which allows free rotation to occur. Friction between the wafer and the polishing pad causes the wafer to assume a position which produces a sub-ambient pressure distribution similar to that obtained from experiments. The effects of different table speeds on the interfacial fluid pressure, as predicted by the computational fluid dynamics are presented.},
 author = {Jeremiah N Mpagazehe and C Fred Higgs},
 doi = {10.1177/1350650112466769},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1350650112466769},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part J: Journal of Engineering Tribology},
 number = {7},
 pages = {777–786},
 title = {A three-dimensional transient model to predict interfacial phenomena during chemical mechanical polishing using computational fluid dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1177/1350650112466769},
 volume = {227},
 year = {2013m}
}

@article{doi:10.1177/1350650113496980,
 abstract = {In this work, experimental and theoretical analyses of the power losses in an industrial planetary speed reducer were performed in order to define the weight of the different types of power losses in the global efficiency as well as to understand the appropriateness and the effectiveness of the models and tools available in the literature for the prediction of these power losses. From this investigation, it has emerged that even if for all the kinds of power losses improvements are in principle possible, those for which, at the same time, more improvements seem possible and effective models are lacking are the load-independent power losses. For this reason, a computational fluid dynamics methodology able to evaluate load-independent gear power losses in planetary gearboxes is proposed. The other types of power losses were determined by means of analytical models available in the literature and, together with the calculated load-independent gear power losses, compared with the experimentally determined power losses showing the effectiveness of the adopted calculation models.},
 author = {Franco Concli and Edoardo Conrado and Carlo Gorla},
 doi = {10.1177/1350650113496980},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1350650113496980},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part J: Journal of Engineering Tribology},
 number = {1},
 pages = {11–21},
 title = {Analysis of power losses in an industrial planetary speed reducer: Measurements and computational fluid dynamics calculations},
 url = {https://doi-org.crai.referencistas.com/10.1177/1350650113496980},
 volume = {228},
 year = {2014d}
}

@article{doi:10.1177/1350650114550346,
 abstract = {Different geometry patterns of the surface of thrust bearings have been proven very beneficial in terms of bearing load-carrying capacity and friction coefficient. In this study, four different types of sector-pad thrust bearings have been cross-evaluated for operation under realistic operating conditions: (a) an open pocket bearing, (b) a closed pocket bearing, (c) a tapered-land bearing, and (d) a bearing partially textured with rectangular dimples. Bearing performance has been computed by means of computational fluid dynamics simulations based on the numerical solution of the Navier–Stokes and energy equations for incompressible flow. Conjugate heat transfer at the bearing pad and rotor has been taken into account. Initially, for a reference design of each bearing, the effects of varying rotational speed and minimum film thickness have been investigated. Further, characterization of each bearing for a constant level of thrust load has been performed. Finally, the effects of varying the depth of each geometry pattern have been studied. The present results illustrate a superior performance of the open pocket bearing in comparison to the other bearing types.},
 author = {Dimitrios G Fouflias and Anastassios G Charitopoulos and Christos I Papadopoulos and Lambros Kaiktsis and Michel Fillon},
 doi = {10.1177/1350650114550346},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1350650114550346},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part J: Journal of Engineering Tribology},
 number = {4},
 pages = {376–397},
 title = {Performance comparison between textured, pocket, and tapered-land sector-pad thrust bearings using computational fluid dynamics thermohydrodynamic analysis},
 url = {https://doi-org.crai.referencistas.com/10.1177/1350650114550346},
 volume = {229},
 year = {2015f}
}

@article{doi:10.1177/1350650117743684,
 abstract = {The performance of air bearing is highly influenced by the geometrical parameters of its restrictor. This study aims to maximize the load-carrying capacity and stiffness of air bearing, and minimize its volume flow rate by optimizing the geometrical parameters of restrictor. To facilitate the calculation of air bearing performance, a parametric computational fluid dynamics model is developed. Then, it is combined with multiobjective optimization genetic algorithm to search the Pareto optimal solutions. Furthermore, as a case study, the optimal design of an annular thrust air bearing is implemented. The stiffness of air bearing is improved 38.5%, the load-carrying capacity is improved 33.9%, and the volume flow rate is declined 19.6%, which are finally validated by experiments. It proves the reliability of proposed parametric computational fluid dynamics model and genetic optimization algorithm.},
 author = {Qiang Gao and Lihua Lu and Wanqun Chen and Guanglin Wang},
 doi = {10.1177/1350650117743684},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1350650117743684},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part J: Journal of Engineering Tribology},
 number = {10},
 pages = {1203–1214},
 title = {Optimal design of an annular thrust air bearing using parametric computational fluid dynamics model and genetic algorithms},
 url = {https://doi-org.crai.referencistas.com/10.1177/1350650117743684},
 volume = {232},
 year = {2018e}
}

@article{doi:10.1177/1350650119894167,
 abstract = {The tribological characteristic of journal bearing systems can be enhanced with the integrating of textures in the contact interfaces, or using the lubricating effect of non-Newtonian fluids. In this study, the combined effects of bearing surface texturing and non-Newtonian lubricants behavior, using micropolar fluid model, on static characteristics of hydrodynamic circular journal bearings of finite length are highlighted. The modified Reynolds equation of micropolar lubrication theory is solved using finite differences scheme and Elrod’s mass conservation algorithm, taking into account the presence of the cylindrical texture shape on full and optimum bearing surfaces. The optimization textured area is carried out through particle swarm optimization algorithm, in order to increase the load lifting capacity. Preliminary results are in good agreement with the reference ones, and present an enhancement in the performances of micro-textured journal bearings (load carrying capacity and friction). The results suggest that texturing the bearing convergent zone significantly increases the load carrying capacity and reduce friction coefficient, while fully texturing causes bad performances. It is also shown that the micropolar fluids exhibit better performances for smooth journal bearings than a Newtonian fluid depending on the size of material characteristic length and the coupling number. The combined effects of fully surface textured with micropolar fluids reduce the performance of journal bearing, especially at lower eccentricity ratios. Considering the optimal arrangement of textures on the contact surface, a significant improvement in terms of load capacity and friction can be achieved, particularly at high eccentricity ratios, high material characteristic lengths and high values of the coupling numbers of micropolar fluids.},
 author = {Belkacem Manser and Idir Belaidi and Sofiane Khelladi and Mohamed A Ait Chikh and Michael Deligant and Farid Bakir},
 doi = {10.1177/1350650119894167},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1350650119894167},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part J: Journal of Engineering Tribology},
 number = {8},
 pages = {1310–1331},
 title = {Computational investigation on the performance of hydrodynamic micro-textured journal bearing lubricated with micropolar fluid using mass-conserving numerical approach},
 url = {https://doi-org.crai.referencistas.com/10.1177/1350650119894167},
 volume = {234},
 year = {2020l}
}

@article{doi:10.1177/1350650121998519,
 abstract = {Water-lubricated bearings have attracted increasing attention in the field of high-speed machine tools for their low friction due to low viscosity. However, new problems, in particular, insufficient load capacity, are on the way. To the point, groove-textured journal bearing is adopted in this study. Aiming at investigating the effects of groove texture on high speed, water-lubricated, hydrodynamic journal bearing precisely, and thoroughly, three-dimensional computational fluid dynamic analyses considering cavitation and turbulence are undertaken to assess the tribological performances of the bearing. To reduce the amount of three-dimensional modeling and meshing work, mesh deformation is presented. The numerical results are compared with experiments to verify the validity of the present models and calculation procedures. Pressure distribution, load capacity, and friction of groove-textured water-lubricated journal bearing are analyzed with respect to operating conditions and geometric parameters. Comparisons between groove-textured water-lubricated journal bearing and smooth bearing are carried out to find out the influence of groove texture. It is found that the groove texture can achieve a remarkable improvement of load capacity at a smaller eccentricity ratio and higher rotary speed. The load capacity is affected by the combined effects of groove depth, width, and length. However, generally, the friction force of water-lubricated journal bearing is slightly influenced by groove texture. Results can provide theoretical guidance for the optimal design of groove-textured water-lubricated journal bearing under different operating parameters.},
 author = {Huihui Feng and Shuyun Jiang and Yanqin Shang-Guan},
 doi = {10.1177/1350650121998519},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1350650121998519},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part J: Journal of Engineering Tribology},
 number = {11},
 pages = {2272–2286},
 title = {Three-dimensional computational fluid dynamic analysis of high-speed water-lubricated hydrodynamic journal bearing with groove texture considering turbulence},
 url = {https://doi-org.crai.referencistas.com/10.1177/1350650121998519},
 volume = {235},
 year = {2021g}
}

@article{doi:10.1177/13506501241227708,
 abstract = {Hybrid porous tilting pad bearings with the advantages of gas porous bearings and tilting pad bearings exhibited excellent rotor-supporting performance with high accuracy and stability. Under hybrid lubrication, the transient adaptive motion of the tilting pads was important for the rotordynamic characteristics. A novel fluid–structure interaction model of hybrid porous tilting pad bearings was developed to investigate the transient hydro forces and the pad’s adaptive motion. The rotordynamic coefficients of the hybrid porous tilting pad bearings were identified using the rotor harmonic trajectory. The computational fluid dynamics-fluid–structure interaction model was validated with the experimental data. The effects of eccentricity ratios, rotor speeds, supply pressures, pivot radial stiffnesses, and dampings on the pad adaptive motion and the rotordynamic coefficients were evaluated. The pad self-adaption motions significantly influenced the supporting and vibration-absorbing characteristics of hybrid porous tilting pad bearings.},
 author = {Peng Xia and Xiaofei Jin and Wenlong Song and Yangqiao Liang and Shicheng Xu and Jiabao Liu and Zhansheng Liu and Wensheng Ma},
 doi = {10.1177/13506501241227708},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/13506501241227708},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part J: Journal of Engineering Tribology},
 number = {5},
 pages = {615–637},
 title = {A novel transient computational fluid dynamics-fluid–structure interaction model for the pad adaptive motion and rotordynamic coefficients of hybrid porous tilting pad bearings},
 url = {https://doi-org.crai.referencistas.com/10.1177/13506501241227708},
 volume = {238},
 year = {2024r}
}

@article{doi:10.1177/13524585211059308,
 abstract = {Background: Multiple sclerosis (MS) is commonly associated with decision-making, neurocognitive impairments, and mood and motivational symptoms. However, their relationship may be obscured by traditional scoring methods. Objectives: To study the computational basis underlying decision-making impairments in MS and their interaction with neurocognitive and neuropsychiatric measures. Methods: Twenty-nine MS patients and 26 matched control subjects completed a computer version of the Iowa Gambling Task (IGT). Participants underwent neurocognitive evaluation using an expanded version of the Brief Repeatable Battery. Hierarchical Bayesian Analysis was used to estimate three established computational models to compare parameters between groups. Results: Patients showed increased learning rate and reduced loss-aversion during decision-making relative to control subjects. These alterations were associated with: (1) reduced net gains in the IGT; (2) processing speed, executive functioning and memory impairments; and (3) higher levels of depression and current apathy. Conclusion: Decision-making deficits in MS patients could be described by the interplay between latent computational processes, neurocognitive impairments, and mood/motivational symptoms.},
 author = {Rodrigo S Fernández and Lucia Crivelli and María E Pedreira and Ricardo F Allegri and Jorge Correale},
 doi = {10.1177/13524585211059308},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/13524585211059308},
 journal = {Multiple Sclerosis Journal},
 note = {PMID:34931933},
 number = {8},
 pages = {1267–1276},
 title = {Computational basis of decision-making impairment in multiple sclerosis},
 url = {https://doi-org.crai.referencistas.com/10.1177/13524585211059308},
 volume = {28},
 year = {2022h}
}

@article{doi:10.1177/13524585221124307,
 author = {Massimiliano Di Filippo and Andrea Mancini},
 doi = {10.1177/13524585221124307},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/13524585221124307},
 journal = {Multiple Sclerosis Journal},
 note = {PMID:36250263},
 number = {13},
 pages = {1999–2000},
 title = {Rethinking the MS brain: Synaptic loss and computational modelling of brain networks},
 url = {https://doi-org.crai.referencistas.com/10.1177/13524585221124307},
 volume = {28},
 year = {2022g}
}

@article{doi:10.1177/1354068820974609,
 abstract = {It is often claimed that computational methods for examining textual data give good enough party position estimates at a fraction of the costs of many non-computational methods. However, the conclusive testing of these claims is still far from fully accomplished. We compare the performance of two computational methods, Wordscores and Wordfish, and four non-computational methods in estimating the political positions of parties in two dimensions, a left-right dimension and a progressive-conservative dimension. Our data comprise electoral party manifestos written in Finnish and published in Finland. The non-computational estimates are composed of the Chapel Hill Expert Survey estimates, the Manifesto Project estimates, estimates deriving from survey-based data on voter perceptions of party positions, and estimates derived from electoral candidates’ replies to voting advice application questions. Unlike Wordfish, Wordscores generates relatively well-performing estimates for many of the party positions, but despite this does not offer an even match to the non-computational methods.},
 author = {Juha Koljonen and Veikko Isotalo and Pertti Ahonen and Mikko Mattila},
 doi = {10.1177/1354068820974609},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1354068820974609},
 journal = {Party Politics},
 number = {2},
 pages = {306–317},
 title = {Comparing computational and non-computational methods in party position estimation: Finland, 2003–2019},
 url = {https://doi-org.crai.referencistas.com/10.1177/1354068820974609},
 volume = {28},
 year = {2022l}
}

@article{doi:10.1177/1354856517715164,
 abstract = {This article discusses the role of technological myths in the development of artificial intelligence (AI) technologies from 1950s to the early 1970s. It shows how the rise of AI was accompanied by the construction of a powerful cultural myth: The creation of a thinking machine, which would be able to perfectly simulate the cognitive faculties of the human mind. Based on a content analysis of articles on AI published in two magazines, the Scientific American and the New Scientist, which were aimed at a broad readership of scientists, engineers and technologists, three dominant patterns in the construction of the AI myth are identified: (1) the recurrence of analogies and discursive shifts, by which ideas and concepts from other fields were employed to describe the functioning of AI technologies; (2) a rhetorical use of the future, imagining that present shortcomings and limitations will shortly be overcome and (3) the relevance of controversies around the claims of AI, which we argue should be considered as an integral part of the discourse surrounding the AI myth.},
 author = {Simone Natale and Andrea Ballatore},
 doi = {10.1177/1354856517715164},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1354856517715164},
 journal = {Convergence},
 number = {1},
 pages = {3–18},
 title = {Imagining the thinking machine: Technological myths and the rise of artificial intelligence},
 url = {https://doi-org.crai.referencistas.com/10.1177/1354856517715164},
 volume = {26},
 year = {2020n}
}

@article{doi:10.1177/13548565211014449,
 abstract = {This article is set within a technological paradigm shift that denotes a transition from hardware to software and that, by means of applications and operating systems, plays a central role in the sociocultural sphere, intervening in the creation, classification and distribution of cultural objects. The aim of this research is to delve into how the graphical user interface (GUI) integrates into contemporary audiovisual discourse, focusing on its potential as a spatial metaphor. To that end, a theoretical framework is built on the concept of space as a tool and its instrumentalization in the GUI throughout the years, with a view to design a taxonomy in relation to the different ways in which the GUI integrates into digital image composition. The article concludes that the ability to operate in the virtual and physical space, moving between the logic of the tool and its potential as spatial metaphor, provides the GUI with the necessary specificity to be considered a key cultural element for analysing new media’s visual identity.},
 author = {Oswaldo García-Crespo and Diana Ramahí-García and Silvia García-Mirón},
 doi = {10.1177/13548565211014449},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/13548565211014449},
 journal = {Convergence},
 number = {6},
 pages = {1696–1713},
 title = {From the tool to the spatial metaphor: Graphical user interface and audiovisual creation in new media},
 url = {https://doi-org.crai.referencistas.com/10.1177/13548565211014449},
 volume = {27},
 year = {2021f}
}

@article{doi:10.1177/13548565221105192,
 abstract = {Media convergence is not a new concept in journalism studies, though available evidence indicates that convergence studies have been explored more in the global north than the global south. This study, contextualised in Nigeria, joins the media convergence conversation by exploring the sustainability of Facebook-radio convergence for distributing broadcast programmes by seventeen (17) licenced radio stations in Oyo State, Nigeria. As a computational content analysis study, researchers analysed 85 purposively selected programmes of the stations as broadcast live on their Facebook pages alongside the 9527 likes, 10,314 shares, 7007 comments and 170,681 views the programmes generated. Stakeholders’ interviews were also conducted for a broadcasting expert, presenters of some of the stations, together with audience of the selected stations. The main finding shows that programmes that focussed more on socioeconomic problems and opportunities, and were broadcast in the afternoon, evening and at night received more digital engagement than other programmes’ formats and time belts. Although high cost of Internet data subscription in the country, absence of Internet-enabled mobile phones among many adherents of radio programmes (both in rural and urban areas), epileptic power supply that sometimes leave many people with unpowered mobile phones as well as weak Internet broadband connectivity common to many locations in Nigeria threaten the sustainability of Facebook-radio broadcasting in Oyo State. Deployment of 5G network, installation of more network masts with strong bandwidth and training of radio presenters and radio stations’ social media handlers on innovative and audience-participatory programme production are recommended.},
 author = {Umar O Ajetunmobi and Mutiu I Lasisi},
 doi = {10.1177/13548565221105192},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/13548565221105192},
 journal = {Convergence},
 number = {5},
 pages = {1340–1357},
 title = {Sustainability of Facebook-radio convergence for content distribution in Nigeria: Computational content and stakeholders’ perspectives analyses},
 url = {https://doi-org.crai.referencistas.com/10.1177/13548565221105192},
 volume = {28},
 year = {2022a}
}

@article{doi:10.1177/13548565221111073,
 abstract = {Documents have been increasingly recognised as important objects of investigation in Science and Technology Studies (STS); however, so far, much less attention has been given to the study of documents produced in Digital Humanities. The author proposes therefore to use the method of the ‘STS of documents’ and analyse Feasibility documents that aim to assess technical and design requirements based on research questions and to organise a project workflow. Drawing on the ethnography of King’s Digital Lab, the article investigates Feasibility documents produced by the lab within the Agile-based Software Development Lifecycle framework. The article aims to show that Feasibility documents (1) inform ethnographic work about lab workflow and management and in doing so, are able to capture the interconnectedness of work layers and practices; (2) enable an empirical analysis of digital research projects and the process of translation from research questions, to methods, to technical solutions; (3) are critical structuring objects that structure the research process and relationships between involved actors and are structured by local institutional strategies and decisions. The author conducts a ‘feasibility analysis’ that reveals the project management and development stages: the analytical process (the translation of research questions into technical solutions); the production process (the move from technical and design practices to research answers) and the infrastructure and management process (project workflow and sustainability solutions). Drawing on Agre’s critical technical practice and Digital Humanities’ theories of critical production, the article seeks to shift attention from end-product digital artefacts towards the complex process of their creation, which can unpack a range of social, technological and management issues. In doing so, it also aims to provide a methodological framework for the analysis of documents produced in Digital Humanities that have the potential to unearth new questions about the socio-technical nature of digital production.},
 author = {Urszula Pawlicka-Deger},
 doi = {10.1177/13548565221111073},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/13548565221111073},
 journal = {Convergence},
 number = {3},
 pages = {746–765},
 title = {Feasibility documents as critical structuring objects: An approach to the study of documents in digital research production},
 url = {https://doi-org.crai.referencistas.com/10.1177/13548565221111073},
 volume = {29},
 year = {2023n}
}

@article{doi:10.1177/13548565231224157,
 abstract = {In this article, the authors demonstrate how the data center has become a key site, object, and metaphor for interdisciplinary scholarship of the internet. While the data center is a fabrication of engineering, computer science, and cognate fields, it has been the critical gaze of scholars outside of those industries. Together, this scholarship has established the field of Critical Data Center Studies. Critiques of the data center – often thought of more generally as ‘internet infrastructure’, and more evocatively as ‘the cloud’ – have emerged from the social sciences, humanities, journalism, and the arts. The authors do this by answering questions about the current social, cultural, political, and environmental landscapes of the data center. Scrutiny of the foundational imaginaries of the internet, real estate deals by Big Tech, the industry’s enabling policies, their connections to energy and other public infrastructure – among many other factors – serves, at the very least, to situate the data center as a media object, as more than simply a material infrastructure, as more than data warehouse, and as more than ‘the cloud’. Further to this, the authors reflect on how the data center has been and continues to be studied, and why critical interventions have been so fruitful within a vast array of disciplines – from history and anthropology, to media studies, information studies, and science & technology studies – for shifting the focus from questions of infrastructural visibility to questions that weave together concerns of efficiency, policy, popular culture, and planetary devastation.},
 author = {Dustin Edwards and Zane Griffin Talley Cooper and Mél Hogan},
 doi = {10.1177/13548565231224157},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/13548565231224157},
 journal = {Convergence},
 number = {0},
 pages = {13548565231224156},
 title = {The making of critical data center studies},
 url = {https://doi-org.crai.referencistas.com/10.1177/13548565231224157},
 volume = {0},
 year = {2024b}
}

@article{doi:10.1177/13548565241258956,
 abstract = {This article examines how the ‘refugee crisis’, sparked by the arrival of refugees from the Syrian civil war and other conflicts around the world, was articulated across Dutch television news programs and social media between 2013 and 2018. This crisis has been described as a key catalyst of the radicalization of European political discourse. Crucially, it took shape during a period of profound transformation of the media landscape, in which mass media lost significant ground to social media as authoritative sources of truth and norms. The research focuses on the crucial but underexplored link between television and social media discourse, which is at the heart of contemporary European public debate. Using a combination of digital methods and NLP techniques, the article compares automatic speech recognition (ASR) transcripts of Dutch televised news on the refugee crisis with responses from publics on Facebook and Twitter. This computational cross-media approach enables a longitudinal analysis of how social media users differ in their interpretation of key events characterizing the crisis, as well as what language is acceptable to debate issues around integration, tolerance and identity. A rejection of mainstream news media editorial guidelines by social media users eventually resulted in their consumption of populist right-wing (‘alternative’) news media and active transgression of anti-discriminatory speech norms.},
 author = {Emillie de Keulenaar and Thomas Poell and Anne Helmond and Bernhard Rieder and Jasmijn Van Gorp},
 doi = {10.1177/13548565241258956},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/13548565241258956},
 journal = {Convergence},
 number = {0},
 pages = {13548565241258956},
 title = {Computational cross-media research: tracing divergences between normative Dutch television and social media discourses on the ‘refugee crisis’ (2013-2018)},
 url = {https://doi-org.crai.referencistas.com/10.1177/13548565241258956},
 volume = {0},
 year = {2024i}
}

@article{doi:10.1177/1356336X19869734,
 abstract = {The purpose of the present study was to examine and compare Singaporean physical education teachers’ attitudes towards information and communication technologies in physical education across different demographic groups that included gender, age, teaching experience, and school level. A total of 422 Singaporean full-time physical education teachers (mean age = 38.47 years, standard deviation = 8.31) completed the Physical Education Teachers’ Subjective Theories Questionnaire to assess their perspectives towards the integration of information and communication technologies into physical education teaching practice. Mann–Whitney U and Kruskal–Wallis H tests were conducted to examine the differences in participants’ attitudes across different demographic groups. Results revealed that attitudes towards information and communication technologies significantly differed between teachers of different gender, age, and teaching experience. However, no significant difference was found in attitudes towards information and communication technologies among teachers of different school levels. The findings of this study can inform policy-makers and stakeholders with an interest in promoting the integration of information and communication technologies in physical education.},
 author = {Nien Xiang Tou and Ying Hwa Kee and Koon Teck Koh and Martin Camiré and Jia Yi Chow},
 doi = {10.1177/1356336X19869734},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1356336X19869734},
 journal = {European Physical Education Review},
 number = {2},
 pages = {481–494},
 title = {Singapore teachers’ attitudes towards the use of information and communication technologies in physical education},
 url = {https://doi-org.crai.referencistas.com/10.1177/1356336X19869734},
 volume = {26},
 year = {2020q}
}

@article{doi:10.1177/1356389019827035,
 abstract = {Although the literature on evaluation has theorized about the distinction between internal and external evaluation, hardly any research has compared them empirically. This article examines whether the lessons of internal evaluations differed from those of external evaluations in the case of international development aid. It analyzes internal evaluations of the Asian Development Bank for nearly 1000 sovereign interventions across 38 countries in the Asia-Pacific during 1996–2016, using computational text analysis or text mining techniques. The results show that internal evaluations focused more on micro- and meso-level characteristics, while external evaluations laid more emphasis on meso- and macro-level constructs, such as dimensions of policy and the institutional environment in the recipient country, or its level and rate of economic growth. The article concludes that internal and external evaluations can be combined to create a multilevel evaluation framework that integrates micro-, meso-, and macro-level lessons to facilitate better learning.},
 author = {Nihit Goyal and Michael Howlett},
 doi = {10.1177/1356389019827035},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1356389019827035},
 journal = {Evaluation},
 number = {3},
 pages = {366–380},
 title = {Combining internal and external evaluations within a multilevel evaluation framework: Computational text analysis of lessons from the Asian Development Bank},
 url = {https://doi-org.crai.referencistas.com/10.1177/1356389019827035},
 volume = {25},
 year = {2019h}
}

@article{doi:10.1177/1356389020980479,
 abstract = {Central government guidance seeks to ensure and enhance the quality of practice and decision-making across – and sometimes beyond – government. The Magenta Book, published by HM Treasury, is the key UK Government resource on policy evaluation, setting out central government guidance on how to evaluate policies, projects and programmes. The UK Centre for the Evaluation of Complexity Across the Nexus was invited to contribute its expertise to the UK Government’s 2020 update of the Magenta Book by developing an accompanying guide on policy evaluation and ‘complexity’. A small multidisciplinary team worked together to produce a set of guidance, going through multiple stages of work and drawing on a variety of sources including academic and practitioner literature and experts and stakeholders in the fields of evaluation, policy and complexity. It also drew on Centre for the Evaluation of Complexity Across the Nexus’ own work developing and testing evaluation methods for dealing with complexity in evaluation. The resulting Magenta Book 2020 Supplementary Guide: Handling Complexity in Policy Evaluation explores the implications of complexity for policy and evaluation and how evaluation can help to navigate complexity. This article, designed primarily for practitioners who might be interested in this guidance and how it was developed, describes the processes involved, particularly related to the interdisciplinary dialogue and consultation with other key stakeholders that this involved. It also briefly outlines the content and key messages in the guidance, with reflections on the experiences of the authors in developing the guide – including the challenges and insights that arose during the process, particularly around the challenges of communicating complexity to a broad audience of readers.},
 author = {Martha Bicket and Dione Hills and Helen Wilkinson and Alexandra Penn},
 doi = {10.1177/1356389020980479},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1356389020980479},
 journal = {Evaluation},
 number = {1},
 pages = {18–31},
 title = {Don’t panic: Bringing complexity thinking to UK Government evaluation guidance},
 url = {https://doi-org.crai.referencistas.com/10.1177/1356389020980479},
 volume = {27},
 year = {2021b}
}

@article{doi:10.1177/13567667231180994,
 abstract = {The purpose of this research is to examine one of the most effective approaches for locating niche tourism attractions that varies by people, using a methodology that combines statistical analysis, deep learning visual image detection, and text mining. Using 30,013 posts with the hashtag #Seoul in English, the analysis focused on the Instagram posts’ time, dominant color, image visual content, and hashtag to identify niche tourism attractions. The analysis result shows that Instagram posts hashtag #Seoul that depicted “young women” and was uploaded in the evening with warm colors such as orange, yellow, and green received more “likes” than other postings. Furthermore, deep learning and text mining analysis were used to identify and forecast the actual image with the most likes in each sectoral domain, as classified by topic modeling, such as “young, woman, outdoor” and “table, plate, indoor.” Through these findings, this study identified niche hotspots of tourism attractions based on those destination image attributes in Instagram photos, which contributes to the popularity of Instagram postings. The methods and results will be particularly useful to marketers and researchers looking to uncover specialized tourism themes and combine popularity measurement with visual image analysis.},
 author = {Ho Young Yoon and Seung-Chul Yoo},
 doi = {10.1177/13567667231180994},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/13567667231180994},
 journal = {Journal of Vacation Marketing},
 number = {4},
 pages = {874–889},
 title = {Finding tourism niche on image-based social media: Integrating computational methods},
 url = {https://doi-org.crai.referencistas.com/10.1177/13567667231180994},
 volume = {30},
 year = {2024t}
}

@article{doi:10.1177/1360780418763824,
 abstract = {Personalisation of media content is not a new phenomenon. Now, however, by configuring our search results and data feeds, algorithms that ‘learn’ from our digital footprint are determining what we see and hear. Pariser calls this the ‘Filter Bubble Effect’. Yet, despite concerns that this effect is a threat to deliberative democracy, we are told there is relatively little evidence to substantiate its existence. This article draws on a case study to argue that this is because the existing research looks for technical effects while neglecting our social lives. If we follow Foucault’s reasoning that systems of thought are also technologies, then we can see that material technologies (or what Foucault called ‘technologies of production’) and immaterial technologies (ideas formed in discourse) can co-constitute filter bubbles. Borrowing language from computing and science and technology studies, this leads to a redefinition of filter bubbles as socio-technical recursion. This case study illustrates just one potential combination of such material and immaterial technologies (namely, search engines and ideas that are encountered and formed during an individual’s social life within their culture and class) that can create socio-technical recursion. The article concludes by arguing the advantage of conceptualising filter bubbles in this way is that it offers us a theoretical foundation for breaking out of this recursion by simultaneously challenging the mediums and messages that sustain them.},
 author = {Huw C Davies},
 doi = {10.1177/1360780418763824},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1360780418763824},
 journal = {Sociological Research Online},
 number = {3},
 pages = {637–654},
 title = {Redefining Filter Bubbles as (Escapable) Socio-Technical Recursion},
 url = {https://doi-org.crai.referencistas.com/10.1177/1360780418763824},
 volume = {23},
 year = {2018g}
}

@article{doi:10.1177/1362361310363281,
 abstract = {We present results obtained with new instrumental methods for the acoustic analysis of prosody to evaluate prosody production by children with Autism Spectrum Disorder (ASD) and Typical Development (TD). Two tasks elicit focal stress — one in a vocal imitation paradigm, the other in a picture-description paradigm; a third task also uses a vocal imitation paradigm, and requires repeating stress patterns of two-syllable nonsense words. The instrumental methods differentiated significantly between the ASD and TD groups in all but the focal stress imitation task. The methods also showed smaller differences in the two vocal imitation tasks than in the picture-description task, as was predicted. In fact, in the nonsense word stress repetition task, the instrumental methods showed better performance for the ASD group. The methods also revealed that the acoustic features that predict auditory-perceptual judgment are not the same as those that differentiate between groups. Specifically, a key difference between the groups appears to be a difference in the balance between the various prosodic cues, such as pitch, amplitude, and duration, and not necessarily a difference in the strength or clarity with which prosodic contrasts are expressed.},
 author = {Jan P.H. Van Santen and Emily T. Prud’hommeaux and Lois M. Black and Margaret Mitchell},
 doi = {10.1177/1362361310363281},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1362361310363281},
 journal = {Autism},
 note = {PMID:20591942},
 number = {3},
 pages = {215–236},
 title = {Computational prosodic markers for autism},
 url = {https://doi-org.crai.referencistas.com/10.1177/1362361310363281},
 volume = {14},
 year = {2010t}
}

@article{doi:10.1177/1362361316677957,
 abstract = {Individuals with autism spectrum disorder demonstrate narrative (i.e. storytelling) difficulties which can significantly impact their ability to form and maintain social relationships. However, existing research has not comprehensively documented these impairments in more open-ended, emotionally evocative situations common to daily interactions. Computational linguistic measures offer a promising complement to traditional hand-coding methods of narrative analysis and in this study were applied together with hand coding of narratives elicited with emotionally salient scenes from the Thematic Apperception Test. In total, 19 individuals with autism spectrum disorder and 14 typically developing controls were asked to tell stories about six images from the Thematic Apperception Test. Both structural and qualitative aspects of narrative were assessed using a hand-coding system and Latent Semantic Analysis, an automated computational measure of semantic similarity. Individuals with autism spectrum disorder demonstrated significant difficulties with the use of complex syntax to integrate their narratives and problems explaining characters’ intentions. These and other key narrative skills were strongly related to narrative competence scores derived from Latent Semantic Analysis, which also distinguished the autism spectrum disorder group from controls. Together, results underscore key narrative impairments in autism spectrum disorder and support the promise of Latent Semantic Analysis as a valuable tool for the quantitative assessment of complex language abilities.},
 author = {Michelle Lee and Gary E Martin and Abigail Hogan and Deanna Hano and Peter C Gordon and Molly Losh},
 doi = {10.1177/1362361316677957},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1362361316677957},
 journal = {Autism},
 note = {PMID:28095705},
 number = {3},
 pages = {335–344},
 title = {What’s the story? A computational analysis of narrative competence in autism},
 url = {https://doi-org.crai.referencistas.com/10.1177/1362361316677957},
 volume = {22},
 year = {2018l}
}

@article{doi:10.1177/1365480219835324,
 abstract = {Making is a movement present in the United States which fosters creativity and invention through the creation and sharing of products. This case study of one Western Pennsylvania school district’s integration of Making into its lower and upper secondary schools shows how the investment in space and equipment, guided by visionary leadership, can bring about innovation. Our findings after year 1 of this 2-year project indicate that Elizabeth Forward School District has been successful in fostering an atmosphere where students and teachers are given permission to fail, thereby allowing for experimentation, exploration, and the integration of girls into this normally male-dominated field.},
 author = {Keith Trahan and Stephanie Maietta Romero and Renata de Almeida Ramos and Jeffrey Zollars and Cynthia Tananis},
 doi = {10.1177/1365480219835324},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1365480219835324},
 journal = {Improving Schools},
 number = {2},
 pages = {144–157},
 title = {Making success: What does large-scale integration of making into a middle and high school look like?},
 url = {https://doi-org.crai.referencistas.com/10.1177/1365480219835324},
 volume = {22},
 year = {2019s}
}

@article{doi:10.1177/13670069050090020501,
 abstract = {It has been noted by many researchers that young bilingual children pass through a stage of early mixing which extends approximately until the age of 2;6 and ends abruptly. Research on bilingual first language acquisition has clearly excluded the possibility to explain mixed utterances as the result of a fused lexical or grammatical system. However, the actual debate on the reasons for early mixing still continues. Two main approaches have dominated the field of language mixing in adults: One assumes that adult’s codeswitching is constrained by grammatical principles, suggesting that a third grammar is responsible for the grammaticality of mixed utterances (e.g., the Equivalence Constraint formulated by Poplack (1980), or the Functional Head Constraint formulated by Belazi, Rubin & Toribio (1994) among others). Since child grammar is supposed to be organized differently from adult grammar, the grammatical “ingredients” of the principles which constrain codeswitching are absent in early child language (Köppe & Meisel, 1995). It therefore follows that early mixing in young bilingual children is not to be considered as constrained by any grammatical principles. The other approach argues that codeswitching in adults is not regulated by external grammatical principles, but that the only constraints which govern codeswitching are those required by the two languages involved (MacSwan, 2000). We will show that this assumption holds for child language as well. Our analysis of mixing opens the perspective that child grammar can be considered to be organized in the same way as adult grammar. Furthermore, we will argue that early mixing is related to developing performance abilities, in the present paper to the readiness on the bilingual child’s part to speak the language(s).},
 author = {Katja Francesca Cantone and Natascha Müller},
 doi = {10.1177/13670069050090020501},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/13670069050090020501},
 journal = {International Journal of Bilingualism},
 number = {2},
 pages = {205–225},
 title = {Codeswitching at the interface of language-specific lexicons and the computational system},
 url = {https://doi-org.crai.referencistas.com/10.1177/13670069050090020501},
 volume = {9},
 year = {2005d}
}

@article{doi:10.1177/13675494231164874,
 abstract = {Algorithmic media have adopted and adapted divinatory practices and vernaculars of prediction, prophecy, probability, fortune-telling and forecasting – suggesting a possible link between artificial intelligence and pre-scientific modes of speculation. Statistical thinking and magical thinking, too, can be recognised as closely correlated epistemological systems for governing societies and ways of life. In fact, primitive astrological practices of looking up at the stars may represent one of the earliest statistical projects involving sophisticated calculations and data sets. Such pattern-making techniques could even be considered precursory to machine learning. As a point of departure for exploring these eclectic relationships between stars and data, magic and machines, I use a media archaeological methodology to question the historical roles of both astrological and computational divination in mediating methods of control, surveillance and knowledge production across transforming societal contexts. This methodology is especially relevant for examining historical narratives in the field of cultural studies as it makes apparent the hyper-connectedness between objects, cultural representation and sites of hegemonic contention. My findings reveal relationships between celestial pattern recognition and efforts to exert control over and manipulate the natural environment and its populations, the historical impact of meteorological and climatological practices for predicting and influencing future events with artificial intelligence, and links between statistics and algorithmic data biases. This article suggests a speculative genealogy of astrology and artificial intelligence, as well as a genealogy of the theological, scientific and machinic unconscious.},
 author = {Leona Nikolić},
 doi = {10.1177/13675494231164874},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/13675494231164874},
 journal = {European Journal of Cultural Studies},
 number = {2},
 pages = {131–146},
 title = {ECS-Ecrea Early Career Scholar Prize winner - An astrological genealogy of artificial intelligence: From ‘pseudo-sciences’ of divination to sciences of prediction},
 url = {https://doi-org.crai.referencistas.com/10.1177/13675494231164874},
 volume = {26},
 year = {2023l}
}

@article{doi:10.1177/1368430204046139,
 author = {Tatsuya Kameda and R. Scott Tindale},
 doi = {10.1177/1368430204046139},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1368430204046139},
 journal = {Group Processes & Intergroup Relations},
 number = {4},
 pages = {299–304},
 title = {Evolutionary/Adaptive Thinking as a Meta-theory for Systematic Group                 Research: An Extended ‘Fungus-eater’ Approach},
 url = {https://doi-org.crai.referencistas.com/10.1177/1368430204046139},
 volume = {7},
 year = {2004j}
}

@article{doi:10.1177/1368430220937354,
 abstract = {The social identity approach suggests that group prototypical individuals have greater influence over fellow group members. This effect has been well-studied offline. Here, we use a novel method of assessing prototypicality in naturally occurring data to test whether this effect can be replicated in online communities. In Study 1a (N = 53,049 Reddit users), we train a linguistic measure of prototypicality for two social groups: libertarians and entrepreneurs. We then validate this measure further to ensure it is not driven by demographics (Study 1b: N = 882) or local accommodation (Study 1c: N = 1,684 Silk Road users). In Study 2 (N = 8,259), we correlate this measure of prototypicality with social network indicators of social influence. In line with the social identity approach, individuals who are more prototypical generate more responses from others. Implications for testing sociopsychological theories with naturally occurring data using computational approaches are discussed.},
 author = {Alicia Cork and Richard Everson and Mark Levine and Miriam Koschate},
 doi = {10.1177/1368430220937354},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1368430220937354},
 journal = {Group Processes & Intergroup Relations},
 number = {6},
 pages = {808–826},
 title = {Using computational techniques to study social influence online},
 url = {https://doi-org.crai.referencistas.com/10.1177/1368430220937354},
 volume = {23},
 year = {2020c}
}

@article{doi:10.1177/13684310241234966,
 author = {Bryan S. Turner},
 doi = {10.1177/13684310241234966},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/13684310241234966},
 journal = {European Journal of Social Theory},
 number = {2},
 pages = {372–379},
 title = {Afterword: Thinking futures},
 url = {https://doi-org.crai.referencistas.com/10.1177/13684310241234966},
 volume = {27},
 year = {2024r}
}

@article{doi:10.1177/13694332211029732,
 abstract = {Fire is a critical risk in reinforced concrete (RC) structures and appropriate structural resistance against it has to be ensured. In this contribution, an approach using corotational layered beam finite elements is employed in which the cross-section temperature is derived from a low-cost closed form model, as opposed to the more commonly used fully computational thermal analysis. The effect of geometrical and material nonlinearities (constitutive behavior fitted to experimental data for concrete and steel), material degradation as a function of temperature rise, and the contributions of thermal, transient, and creep strains are incorporated in the structural analysis. The computational results are favorably compared to experimental data from the literature for an RC beam and for a larger RC frame. Taking benefit of the layered beam formulation offering local insight into the cross-sectional and material behavior, the relationship between the structural degradation and data extracted from the cross-sectional behavior is successfully established. Noteworthy originalities of the contribution are the use of ultimate strain and its evolution as a function of temperature for both materials and the explanation of the observed structural response in fire conditions from cross-sectional data.},
 author = {Batoma Sosso and Fabian M Paz Gutierrez and Péter Z Berke},
 doi = {10.1177/13694332211029732},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/13694332211029732},
 journal = {Advances in Structural Engineering},
 number = {15},
 pages = {3488–3506},
 title = {Computational analysis of reinforced concrete structures subjected to fire using a multilayered finite element formulation},
 url = {https://doi-org.crai.referencistas.com/10.1177/13694332211029732},
 volume = {24},
 year = {2021p}
}

@article{doi:10.1177/1420326X11420456,
 abstract = {Detailed information of transient exhaled air dispersion and recirculation in the breathing zone can be obtained using computational fluid dynamics (CFD) to generate detailed numerical models and obtain the necessary information. In this study, interaction of free convection flow around human body with respiration flow of breathing and vertical personalized flow from personalized ventilation (PV) system was simulated using a commercial CFD package. Impact of breathing process on personal exposure effectiveness εp was evaluated for different operating and environmental conditions. Re-inhaled exposure index εRI for exhaled CO2 was used to assess the amount of exhaled air re-inhaled due to the interaction between personalized and exhaled airflows. Another objective of this study was to consider the risk of airborne infection transmission, caused by undesirable transport and dispersion of exhaled pathogens to surrounding air when infected individual uses PV. Results show that calculation of personal exposure effectiveness would be sufficiently accurate to give proper information about the protection of occupant with a PV system also without the breathing simulation included. The operating mode of a PV proved as the main factor for dispersion of exhaled air and its transport to the background room air, resulting in an increased risk of airborne infection transmission.},
 author = {Mitja Mazej and Vincenc Butala},
 doi = {10.1177/1420326X11420456},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1420326X11420456},
 journal = {Indoor and Built Environment},
 number = {6},
 pages = {749–771},
 title = {Investigation in the Characteristics of the Personal Ventilation Using Computational Fluid Dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1177/1420326X11420456},
 volume = {21},
 year = {2012o}
}

@article{doi:10.1177/1420326X14532933,
 abstract = {Ultraviolet germicidal irradiation (UVGI) has been shown to be an effective technology for reducing the airborne bioburden in indoor environments and is already advocated as a potential infection control measure for healthcare settings. However, much of the understanding of UVGI performance is based on experimental studies or numerical simulation in mechanically ventilated environments. This study considers the application of an upper-room UVGI system in a naturally ventilated multi-bed hospital ward. A computational fluid dynamics model is used to simulate a Nightingale-type hospital ward with wind-driven cross-ventilation and three wall-mounted UVGI fixtures. A parametric study considering 50 different fixture configurations and three ventilation rates was carried out using a design of experiments approach. Each configuration was assessed by calculating the UV dose distribution over the ward and at each bed. Results show that dose is influenced by the location of the fixtures and the ventilation regime. Thermal effects are likely to be important at low ventilation rates and may reduce UV effectiveness. A metamodel-based numerical optimisation was applied at a ventilation rate of 6 air changes per hour. In this case, the optimum result is achieved when UVGI fixtures are mounted on the leeward wall at their lowest mounting height.},
 author = {C. A. Gilkeson and C. J. Noakes and M. A. I. Khan},
 doi = {10.1177/1420326X14532933},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1420326X14532933},
 journal = {Indoor and Built Environment},
 number = {3},
 pages = {449–466},
 title = {Computational fluid dynamics modelling and optimisation of an upper-room ultraviolet germicidal irradiation system in a naturally ventilated hospital ward},
 url = {https://doi-org.crai.referencistas.com/10.1177/1420326X14532933},
 volume = {23},
 year = {2014e}
}

@article{doi:10.1177/1420326X17698532,
 abstract = {The fabric air dispersion system (FADS) is a ventilation terminal made of special polymer fabric. The porous structure of the fabric causes complex flow motion. Due to its advantages over the conventional ventilation system, i.e. ducts and diffusers, the FADS has been widely favoured by architects and researchers. In computational fluid dynamics (CFD) simulation the FADS is usually simplified into a free opening with an area equal to all pores and perforations, called the free area (FA) method in this present work. However, the effectiveness of this simplified method has not been validated. The present work took a half cylindrical FADS without orifices as an example and employed the FA method to simulate the airflow properties inside a chamber under isothermal and non-isothermal conditions. The simulated distributions of air velocity and temperature were compared with those by the direct description (DD) method. Meanwhile, the uniformity of air velocity distribution close to the FADS was validated against test data and the flow visualization using the dry ice as a smoking material. Results demonstrate that the FA method is effective and easy to implement, and performs as well as the DD method in predicting the distribution of airflow generated by the FADS without orifices.},
 author = {Fu-Jiang Chen and Qin-Yu Wu and Dan-Dan Huang and Yun Zhang and Wang Lu and Meng-Meng Chen},
 doi = {10.1177/1420326X17698532},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1420326X17698532},
 journal = {Indoor and Built Environment},
 number = {7},
 pages = {969–982},
 title = {Validation of the free area method for modelling fabric air dispersion system without orifices in computational fluid dynamics simulation},
 url = {https://doi-org.crai.referencistas.com/10.1177/1420326X17698532},
 volume = {27},
 year = {2018c}
}

@article{doi:10.1177/1420326X17718053,
 abstract = {To design a comfortable aircraft cabin environment, designers conventionally follow an iterative guess-and-correction procedure to determine the air-supply parameters. The conventional method has an extremely low efficiency but does not guarantee an optimal design. This investigation proposed an inverse design method based on a proper orthogonal decomposition of the thermo-flow data provided by full computational fluid dynamics simulations. The orthogonal spatial modes of the thermo-flow fields and corresponding coefficients were firstly extracted. Then, a thermo-flow field was expressed into a linear combination of the spatial modes with their coefficients. The coefficients for each spatial mode are functions of air-supply parameters, which can be interpolated. With a quick map of the cause–effect relationship between the air-supply parameters and the exhibited thermo-flow fields, the optimal air-supply parameters were determined from specific design targets. By setting the percentage of dissatisfied and the predicted mean vote as design targets, the proposed method was implemented for inverse determination of air-supply parameters in two aircraft cabins. The results show that the inverse design using computational fluid dynamics-based proper orthogonal decomposition method is viable. Most of computing time lies in the construction of data samples of thermo-flow fields, while the proper orthogonal decomposition analysis and data interpolation is efficient.},
 author = {Jihong Wang and Tengfei (Tim) Zhang and Hongbiao Zhou and Shugang Wang},
 doi = {10.1177/1420326X17718053},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1420326X17718053},
 journal = {Indoor and Built Environment},
 number = {10},
 pages = {1379–1391},
 title = {Inverse design of aircraft cabin environment using computational fluid dynamics-based proper orthogonal decomposition method},
 url = {https://doi-org.crai.referencistas.com/10.1177/1420326X17718053},
 volume = {27},
 year = {2018q}
}

@article{doi:10.1177/1420326X17745943,
 abstract = {Previous studies on inter-unit dispersion around multi-storey buildings focused mostly on an isolated building. Considering that the presence of an upstream building(s) would significantly modify the airflow pattern around a downstream building, this study intends to investigate the influence of such changed airflow patterns on inter-unit dispersion characteristics around a multi-storey building due to wind effect. Computational fluid dynamics (CFD) method in the framework of Reynolds-averaged Navier-stokes modelling was employed to predict the coupled outdoor and indoor airflow field, and the tracer gas technique was used to simulate the dispersion of infectious agents between units. Based on the predicted concentration field, a mass conservation based parameter, namely re-entry ratio, was used to evaluate quantitatively the inter-unit dispersion possibilities and thus assess risks along different routes. The presence of upstream building(s) could disrupt the strong impingement of approaching flows but brings a more complex and irregular airflow pattern around the downstream multi-storey buildings, leading to a more scattered distribution of re-entry ratio values among different units and uncertain dispersion routes. Generally, the tracer gas concentration in most units was lower than those in an isolated building, although very high concentrations were found in some specific areas.},
 author = {Y. W. Dai and C. M. Mak and Z. T. Ai},
 doi = {10.1177/1420326X17745943},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1420326X17745943},
 journal = {Indoor and Built Environment},
 number = {2},
 pages = {217–234},
 title = {Computational fluid dynamics simulation of wind-driven inter-unit dispersion around multi-storey buildings: Upstream building effect},
 url = {https://doi-org.crai.referencistas.com/10.1177/1420326X17745943},
 volume = {28},
 year = {2019e}
}

@article{doi:10.1177/1420326X19856041,
 abstract = {To evaluate the dispersion of a heavy gas, such as sulphur hexafluoride, with a low Froude number in a built environment, an experimental and numerical simulation study was conducted. The experiment was carried out using seven different injection inlet configurations in an experimental chamber. The release rate was found to have a great effect on the concentration in the lower part of the chamber. The obstacle in the middle of the chamber could cause a non-uniform distribution of concentration, particularly due to variations in locations and angles of the release outlets. Additionally, numerical simulations were carried out to evaluate four turbulence models: the standard k-ε model, the realizable k-ε model, the re-normalization group (RNG) k-ε model and the shear stress transport (SST) k-ω model. Four indicators were used to evaluate the turbulent model performance. In general, the SST k-ω model performed the best, with geometric mean bias (MG) = 0.968 and geometric variance (VG) = 1.09 at 0.055 m height, and with MG = 0.384 and VG = 2.80 at 0.6 m height. The standard k-ε model was the next best in performance, followed by the realizable k-ε and the RNG k-ε model.},
 author = {Ying Zhang and Longtao Wang and Angui Li and Pengfei Tao},
 doi = {10.1177/1420326X19856041},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1420326X19856041},
 journal = {Indoor and Built Environment},
 number = {5},
 pages = {656–670},
 title = {Performance evaluation by computational fluid dynamics modelling of the heavy gas dispersion with a low Froude number in a built environment},
 url = {https://doi-org.crai.referencistas.com/10.1177/1420326X19856041},
 volume = {29},
 year = {2020t}
}

@article{doi:10.1177/1420326X19884667,
 abstract = {Ventilation is of primary importance for the creation of healthy and comfortable indoor environments and it has a significant impact on the building energy heating and cooling demand. The aim of this study is to assess the application of time-periodic supply velocities to enhance mixing in mixing ventilation cases to reduce heating and cooling energy demands. This paper presents computational fluid dynamics (CFD) simulations of a generic mixing ventilation case, in which the time-averaged velocities and pollutant concentrations from a reference case with constant supply velocities were compared with those obtained from a case with time-periodic supply velocities (sine function). The unsteady Reynolds-averaged Navier-Stokes (URANS) CFD simulations indicate that the use of time-periodic supply velocities can reduce high pollutant concentrations in stagnant regions, reduces the overall time-averaged pollutant concentrations and increases contaminant removal effectiveness with about 20%. The influence of the period of the sine function was assessed and the results showed that for the periods tested, the differences are negligible. Finally, the URANS approach was compared with the large eddy simulations (LES) approach, indicating that URANS leads to very similar results (NMSE < 3.2%) as LES and can thus be regarded as a suitable approach for this study.},
 author = {T. van Hooff and B. Blocken},
 doi = {10.1177/1420326X19884667},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1420326X19884667},
 journal = {Indoor and Built Environment},
 number = {4},
 pages = {603–620},
 title = {Mixing ventilation driven by two oppositely located supply jets with a time-periodic supply velocity: A numerical analysis using computational fluid dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1177/1420326X19884667},
 volume = {29},
 year = {2020g}
}

@article{doi:10.1177/1420326X20923132,
 abstract = {Organic solvents frequently lead to substantial occupational health issues in secondary industries. This study examined the xylene diffusion process in painting workshops as a case study to develop a control of indoor air quality in manufacturing workshops. Three-dimensional simulations of xylene emission were built based on a ventilated chamber test to provide the source term for the mathematical model. An exponential relationship was established between xylene emissions rates and time. Numerical results obtained using the emissions rate model were more consistent with experimental data than those from constant emission rate. Owing to the property that is denser than air; therefore, given the coupled influence of molecular diffusion, gravity and environmental turbulence, the xylene concentration at a height of 0.75–2.5 m is high, and it could possibly exceed the Chinese standard, GBZ 2.1–2019 permissible concentration time weighted average (PC-TWA) of 50 mg/m3 in human-occupied zones. At the height of the human breathing zone (1.1 m), the aggregated concentration may even exceed the PC-TWA at 450 s by 100 times. Considering that the diffusion of xylene in a painting workshop represents heavy-gas pollutant diffusion problems, this study can be extended to predict pollutant concentration distributions in other secondary industrial workshops.},
 author = {Aihua Liu and Xiaofei Huang and Zhi Yuan and Jing Wan and Yijie Zhuang},
 doi = {10.1177/1420326X20923132},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1420326X20923132},
 journal = {Indoor and Built Environment},
 number = {7},
 pages = {906–923},
 title = {Implementing an emissions-rate model in computational fluid dynamics simulations of contaminant diffusion processes: A case study with xylene in painting workshops},
 url = {https://doi-org.crai.referencistas.com/10.1177/1420326X20923132},
 volume = {30},
 year = {2021l}
}

@article{doi:10.1177/1420326X221092613,
 abstract = {The establishment of a healthy indoor environment requires the accurate evaluation of an individual’s exposure to pollutants. The concentration of indoor chemical pollutants is a representative indicator for such evaluation and is generally measured on-site. Moreover, material flow analysis (MFA), using macroscopic statistical data, is a reasonable method for objectively evaluating pollution on a wide scale; however, no effective strategy exists for the prediction of indoor air pollution, nor for the assessment of an individual’s exposure from social stock data. Accordingly, we developed a novel integration method comprising MFA and computational fluid dynamics (CFD) with a computer-simulated person (CSP) to establish a framework for evaluating indoor pollutant concentration and individual exposure of residents. We focused on diethyl-hexyl phthalate (DEHP) and first estimated the amount of DEHP-containing product accumulation in Japan by MFA. Second, we conducted a thorough survey and measurement of DEHP emission rates. Using these results as boundary conditions for indoor CFD with CSP, the individual exposure of a resident, in a standard residential house, was quantitatively evaluated. The total daily exposure per unit of body weight was estimated to be more than 100 (μg/kg/d) in the worst-case scenario which was considered the upper limit for exposure in this analysis.},
 author = {Ryota Muta and Sung-Jun Yoo and Hyuntae Kim and Toru Matsumoto and Kazuhide Ito},
 doi = {10.1177/1420326X221092613},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1420326X221092613},
 journal = {Indoor and Built Environment},
 number = {9},
 pages = {2291–2311},
 title = {Multiscale analysis of material flow and computational fluid dynamics for predicting individual diethyl-hexyl phthalate exposure concentration in indoors},
 url = {https://doi-org.crai.referencistas.com/10.1177/1420326X221092613},
 volume = {31},
 year = {2022q}
}

@article{doi:10.1177/1420326X9800700505,
 abstract = {Computational fluid dynamics (CFD) was used in a study of the air flow char acteristics in the occupied building zone. Correlation equations between the mean air speeds and the percentage dissatisfied with the macroscopic flow numbers were derived. Ten macroscopic flow numbers including the total ventilation rate, the air change rate, ventilation rate, air diffusion performance index, modified jet momentum number, two new flow numbers and three expressions of jet momentum ratio were investigated. A total number of 25 numerical experiments under isothermal flow conditions with four different geometrical arrangements (labelled A-D) were performed. The correlation equations derived from CFD were compared with those obtained from experi mental studies. It is found that the jet momentum ratio RM2 gives the equation with the best correlation coefficient and so is recommended for ventilation design.},
 author = {W.K. Chow and L.T. Wong},
 doi = {10.1177/1420326X9800700505},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1420326X9800700505},
 journal = {Indoor and Built Environment},
 number = {5–6},
 pages = {276–288},
 title = {Equations for a Ventilation Design Derived from Computational Fluid Dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1177/1420326X9800700505},
 volume = {7},
 year = {1998a}
}

@article{doi:10.1177/1460458216661862,
 abstract = {This article describes a methodology to recognize emotional states through an electroencephalography signals analysis, developed with the premise of reducing the computational burden that is associated with it, implementing a strategy that reduces the amount of data that must be processed by establishing a relationship between electrodes and Brodmann regions, so as to discard electrodes that do not provide relevant information to the identification process. Also some design suggestions to carry out a pattern recognition process by low computational complexity neural networks and support vector machines are presented, which obtain up to a 90.2% mean recognition rate.},
 author = {Adrian Rodriguez Aguiñaga and Miguel Angel Lopez Ramirez},
 doi = {10.1177/1460458216661862},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1460458216661862},
 journal = {Health Informatics Journal},
 note = {PMID:27644256},
 number = {2},
 pages = {146–170},
 title = {Emotional states recognition, implementing a low computational complexity strategy},
 url = {https://doi-org.crai.referencistas.com/10.1177/1460458216661862},
 volume = {24},
 year = {2018a}
}

@article{doi:10.1177/1460458219896673,
 abstract = {This study aims to explore the use of Instagram by the Centers for Disease Control and Prevention, one of the representative public health authorities in the United States. For this aim, all of the photos uploaded on the Centers for Disease Control and Prevention Instagram account were crawled and the content of them were analyzed using Microsoft Azure Cognitive Services. Also, engagement was measured by the sum of numbers of likes and comments to each photo, and sentiment analysis of comments was conducted. Results suggest that the photos that can be categorized into “text” and “people” took the largest share in the Centers for Disease Control and Prevention Instagram photos. And it was found that the Centers for Disease Control and Prevention’s major way of delivering messages on Instagram was to imprint key messages that call for actions for better health on photos and to provide the source of complementary information on text component of each post. It was also found that photos with more and bigger human faces had lower level of engagement than the others, and happiness and neutral emotions expressed on the faces in photos were negatively associated with engagement. The features whose high value would make the photos look splendid and gaudy were negatively correlated with engagement, but sharpness was positively correlated.},
 author = {Yunhwan Kim and Jang Hyun Kim},
 doi = {10.1177/1460458219896673},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1460458219896673},
 journal = {Health Informatics Journal},
 note = {PMID:31969051},
 number = {3},
 pages = {2159–2180},
 title = {Using photos for public health communication: A computational analysis of the Centers for Disease Control and Prevention Instagram photos and public responses},
 url = {https://doi-org.crai.referencistas.com/10.1177/1460458219896673},
 volume = {26},
 year = {2020k}
}

@article{doi:10.1177/1460458220984205,
 abstract = {Liver cancer kills approximately 800 thousand people annually worldwide, and its most common subtype is hepatocellular carcinoma (HCC), which usually affects people with cirrhosis. Predicting survival of patients with HCC remains an important challenge, especially because technologies needed for this scope are not available in all hospitals. In this context, machine learning applied to medical records can be a fast, low-cost tool to predict survival and detect the most predictive features from health records. In this study, we analyzed medical data of 165 patients with HCC: we employed computational intelligence to predict their survival, and to detect the most relevant clinical factors able to discriminate survived from deceased cases. Afterwards, we compared our data mining results with those obtained through statistical tests and scientific literature findings. Our analysis revealed that blood levels of alkaline-phosphatase (ALP), alpha-fetoprotein (AFP), and hemoglobin are the most effective prognostic factors in this dataset. We found literature supporting association of these three factors with hepatoma, even though only AFP has been used in a prognostic index. Our results suggest that ALP and hemoglobin can be candidates for future HCC prognostic indexes, and that physicians could focus on ALP, AFP, and hemoglobin when studying HCC records.},
 author = {Davide Chicco and Luca Oneto},
 doi = {10.1177/1460458220984205},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1460458220984205},
 journal = {Health Informatics Journal},
 note = {PMID:33504243},
 number = {1},
 pages = {1460458220984205},
 title = {Computational intelligence identifies alkaline phosphatase (ALP), alpha-fetoprotein (AFP), and hemoglobin levels as most predictive survival factors for hepatocellular carcinoma},
 url = {https://doi-org.crai.referencistas.com/10.1177/1460458220984205},
 volume = {27},
 year = {2021d}
}

@article{doi:10.1177/1461444812450686,
 abstract = {The digital world need not solely be conceived in Western, elite terms, but instead can and should be re-envisioned as a space that empowers the values, priorities, and ontologies held by global users from the ‘margins’, within the developing world. This paper asks us to re-consider the ontologies of the digital world in light of the cultural beliefs, languages, and value systems of emerging web users. I argue that as we design new media technologies and develop projects we must think about local ontologies and practices rather than singular Western-created representations of knowledge. I present several examples that think past rigid, hierarchical classifications, opening up the codes of new media to better listen to diverse community and cultural voices.},
 author = {Ramesh Srinivasan},
 doi = {10.1177/1461444812450686},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1461444812450686},
 journal = {New Media & Society},
 number = {2},
 pages = {203–223},
 title = {Re-thinking the cultural codes of new media: The question concerning ontology},
 url = {https://doi-org.crai.referencistas.com/10.1177/1461444812450686},
 volume = {15},
 year = {2013r}
}

@article{doi:10.1177/1461444812465137,
 abstract = {This article advances a sociological approach to computational journalism. By “computational journalism” the article refers to the increasingly ubiquitous forms of algorithmic, social scientific, and mathematical forms of newswork adopted by many 21st-century newsrooms and touted by many educational institutions as “the future of news.” By “sociological approach,” the article endorses a research model that brackets, at least temporarily, many of the current industry concerns with the practical usability of newsroom analysis. The bulk of the article outlines a series of six lenses through which such an approach to computational journalism might be carried out. Four of these lenses are drawn from Schudson’s classic typology of the sociology of news—economic, political, cultural, and organizational approaches. In addition, the author adds Bordieuean field approaches and technological lenses to the mix. In each instance, the author discusses how particular approaches might need to be modified in order to study computational journalism in the digital age.},
 author = {CW Anderson},
 doi = {10.1177/1461444812465137},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1461444812465137},
 journal = {New Media & Society},
 number = {7},
 pages = {1005–1021},
 title = {Towards a sociology of computational and algorithmic journalism},
 url = {https://doi-org.crai.referencistas.com/10.1177/1461444812465137},
 volume = {15},
 year = {2013c}
}

@article{doi:10.1177/1461444813518183,
 abstract = {Through an investigation of patterns of use of the location-based social network Foursquare derived from an extensive ethnographic survey of users, this paper focuses on the orientation of users towards location-based social media and mobile computational devices. Utilising Heidegger’s notions of mood and attunement to the world, the paper argues that the towards-which of the user, that is the mood of the user in a phenomenological sense, is critical to their experience of using location-based social media and the revealing of place that emerges from that usage. A contrast between a technological and a poetic or computational revealing of place can then be established based on the phenomenological orientation of user to device, application and world. The emphasis on orientation and attunement has implications for application design and research on user experience.},
 author = {Leighton Evans},
 doi = {10.1177/1461444813518183},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1461444813518183},
 journal = {New Media & Society},
 number = {6},
 pages = {845–860},
 title = {Being-towards the social: Mood and orientation to location-based social media, computational things and applications},
 url = {https://doi-org.crai.referencistas.com/10.1177/1461444813518183},
 volume = {17},
 year = {2015f}
}

@article{doi:10.1177/1461444815624182,
 abstract = {This article examines the articulation of computational journalism, focusing on how the meaning of the computational is discursively constructed and mobilized as a specific constellation of intelligibility within news organizations. Relying on the concept of articulation developed in cultural studies, the article asks what, exactly, is meant by the computational in the context of journalism? Drawing on interviews with key managerial staff, editors and developers at Scandinavian news organizations, three broad claims about the linkage between the computational and journalism emerged. These articulations include the notion that machines don’t have instincts, that democracy can never be personalized and finally that the computational is something to think with, rather than simulate. The argument is made that what can and cannot be calculated is not merely a technical question, it is also a deeply social, cultural, political and economic one. Thus, the computational emerges as an important organising framework and discursive order for thinking and talking about journalism in the digital age.},
 author = {Taina Bucher},
 doi = {10.1177/1461444815624182},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1461444815624182},
 journal = {New Media & Society},
 number = {6},
 pages = {918–933},
 title = {‘Machines don’t have instincts’: Articulating the computational in journalism},
 url = {https://doi-org.crai.referencistas.com/10.1177/1461444815624182},
 volume = {19},
 year = {2017c}
}

@article{doi:10.1177/1461444818783102,
 abstract = {The British government is claiming digital skills will deliver economic growth to the country and social mobility to young people: its ministers call it ‘a pipeline to prosperity’. While declaring this pipeline, the government assumes the needs of the economy and young people’s needs are (or should be) synchronised. We challenge this assumption and the policy it sustains with data from questionnaires, workshops and interviews with 50 young people from communities in South Wales (including a former mining town and a deprived inner city area) about digital technology’s role in their everyday life. We use a new typography to compare the reality of their socially and economically structured lives to the governmental policy discourse that makes them responsible for their country’s future economic success. To explain these young people’s creative and transgressive use of technology, we also make an empirically grounded contribution to the ongoing theoretical debates about structure and agency.},
 author = {Huw C Davies and Rebecca Eynon},
 doi = {10.1177/1461444818783102},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1461444818783102},
 journal = {New Media & Society},
 number = {11},
 pages = {3961–3979},
 title = {Is digital upskilling the next generation our ‘pipeline to prosperity’?},
 url = {https://doi-org.crai.referencistas.com/10.1177/1461444818783102},
 volume = {20},
 year = {2018d}
}

@article{doi:10.1177/1461444819856916,
 abstract = {News organizations have adapted in various ways to a digital media environment dominated by algorithmic gatekeepers such as search engines and social networks. This article dissects a campaign to actively shape that environment led by professional fact-checking organizations. We trace the development of the Share the Facts “widget,” a device designed to give fact-checks greater purchase in algorithmically governed media networks by driving adoption of a new data standard called ClaimReview. We show how “structured journalism” gave journalists a language for the social and technical challenges involved, and how this infrastructural technology mediates between fact-checkers, audiences, and platform companies. We argue that this standard-setting initiative exhibits both promotional and disciplining facets, offering greater distribution and impact to journalists while also defining their work in specific ways. Crucially, in this case, this disciplining influence reflects internal professional-institutional agendas in an emerging subfield of journalism as much as the demands of platform companies.},
 author = {Lucas Graves and CW Anderson},
 doi = {10.1177/1461444819856916},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1461444819856916},
 journal = {New Media & Society},
 number = {2},
 pages = {342–360},
 title = {Discipline and promote: Building infrastructure and managing algorithms in a “structured journalism” project by professional fact-checking groups},
 url = {https://doi-org.crai.referencistas.com/10.1177/1461444819856916},
 volume = {22},
 year = {2020f}
}

@article{doi:10.1177/1461444819884373,
 abstract = {We analyze information and communication technology in education initiatives in two South American countries: Bolivia and Uruguay. Utilizing qualitative data collection and analysis methods, we construct a comparative case study to trace the path of how national discourses—in response to the idea of globalization and initiatives promoting computers in education—were translated into policy goals, strategic implementation plans, and teaching and learning practices and outcomes. We document the role of the selected information and communication technology’s materiality in terms of portability and connectivity along this translation path. Our findings point to the importance of considering national discourse, often overlooked in information and communication technology in education studies, when examining initiative success and failure, and of conceptualizing materiality as more than merely the infrastructural foundation upon which information and communication technology in education initiatives are built. Understanding the role that materiality plays in interaction with national discourse may be especially important in guiding successful information and communication technology in education initiatives in developing countries, where financial resources are limited.},
 author = {Caroline Stratton and Diane E Bailey and Paul M Leonardi},
 doi = {10.1177/1461444819884373},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1461444819884373},
 journal = {New Media & Society},
 number = {12},
 pages = {2083–2107},
 title = {Translating national discourse into teaching and learning outcomes: Portability and connectivity in developing countries’ ICT in education (ICT4E) initiatives},
 url = {https://doi-org.crai.referencistas.com/10.1177/1461444819884373},
 volume = {22},
 year = {2020q}
}

@article{doi:10.1177/1461444820923674,
 abstract = {Although socializing is a powerful driver of youth engagement online, platforms struggle to leverage social engagement to promote learning. We seek to understand this dynamic using a multi-stage analysis of over 14,000 comments on Scratch, an online platform designed to support learning about programming. First, we inductively develop the concept of “participatory debugging”—a practice in which users learn through the process of collaborative technical troubleshooting. Second, we use a content analysis to establish how common the practice is on Scratch. Third, we conduct a qualitative analysis of user activity over time and identify three factors that serve as social antecedents of participatory debugging: (1) sustained community, (2) identifiable problems, and (3) what we call “topic porousness” to describe conversations that are able to span multiple topics. We integrate these findings in a framework that highlights a productive tension between the desire to promote learning and the interest-driven sub-communities that drive user engagement in many new media environments.},
 author = {Samantha Shorey and Benjamin Mako Hill and Samuel Woolley},
 doi = {10.1177/1461444820923674},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1461444820923674},
 journal = {New Media & Society},
 number = {8},
 pages = {2327–2344},
 title = {From hanging out to figuring it out: Socializing online as a pathway to computational thinking},
 url = {https://doi-org.crai.referencistas.com/10.1177/1461444820923674},
 volume = {23},
 year = {2021l}
}

@article{doi:10.1177/1461444821994491,
 abstract = {Online user comments (UCs) as the most popular type of online audience participation nowadays form a popular and important field of research. The widespread examination of UCs across different disciplines leads to a variety of terms and constructs and thus a missing clarity about the discussed topics. With this computational scoping review, we uncovered six relevant, overarching topics and their development in the field. Due to the combination of an automatic text analysis via structural topic modeling and a qualitative evaluation, we were able to describe the current state of UC research and found an inherently interdisciplinary body of literature. We observed an inter- and intradisciplinary fragmentation and call for a systematization of the used terms, constructs, and examined topics.},
 author = {Max Schindler and Emese Domahidi},
 doi = {10.1177/1461444821994491},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1461444821994491},
 journal = {New Media & Society},
 number = {8},
 pages = {2474–2492},
 title = {The growing field of interdisciplinary research on user comments: A computational scoping review},
 url = {https://doi-org.crai.referencistas.com/10.1177/1461444821994491},
 volume = {23},
 year = {2021k}
}

@article{doi:10.1177/14614448221079626,
 abstract = {With growing concerns over children’s data agencies, researchers have begun to draw attention to children’s and young people’s privacy practices in social media environments. However, little is known about the experiences of pre-service teachers who play a key role in educating future generations. This study aimed to address this gap by exploring Finnish pre-service teachers’ conceptions and experiences of data agency in social media environments. Drawing from in-depth interviews of pre-service teachers (N = 14), the analysis revealed that pre-service teachers construct their data agency in terms of social frames and shared social norms, and they also recognize the lack of understanding regarding wider socio-technical systems within which data agencies are situated. This research argues that without a sophisticated understanding of algorithmic governance and commercial use of data, it is unlikely that these future teachers would be prepared to facilitate children’s and youth’s agentive actions in a data-driven society.},
 author = {Henriikka Vartiainen and Lotta Pellas and Juho Kahila and Teemu Valtonen and Matti Tedre},
 doi = {10.1177/14614448221079626},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14614448221079626},
 journal = {New Media & Society},
 number = {4},
 pages = {1871–1890},
 title = {Pre-service teachers’ insights on data agency},
 url = {https://doi-org.crai.referencistas.com/10.1177/14614448221079626},
 volume = {26},
 year = {2024s}
}

@article{doi:10.1177/14614448221122212,
 abstract = {Digital trace data and computational methods are increasingly being used by researchers to study mental health phenomena (i.e. psychopathology and well-being) in social media. Computer-assisted mental health research is not simply a continuation of previous studies, but rather raises ethical, conceptual and methodological issues that are critical to behavioural science but have not yet been systematically explored. Based on a systematic review of n = 147 studies, we reveal a multidisciplinary field of research that has grown immensely since 2010, spanning the humanities, social sciences, and engineering. We find that a substantial majority of studies in our sample lack a standardized form of ethical consideration, focus on specific constructs and have a rather narrow focus on specific social media platforms. Based on our findings, we discuss how computational elements have influenced mental health research, highlight academic gaps and suggest promising directions for future studies.},
 author = {Max Schindler and Emese Domahidi},
 doi = {10.1177/14614448221122212},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14614448221122212},
 journal = {New Media & Society},
 number = {10},
 pages = {2781–2799},
 title = {The computational turn in online mental health research: A systematic review},
 url = {https://doi-org.crai.referencistas.com/10.1177/14614448221122212},
 volume = {25},
 year = {2023q}
}

@article{doi:10.1177/14614448231164409,
 abstract = {Increasingly, influencers are employed to market not only products but also ideas and beliefs. The far right has recognized the strategic potential of influencer communication to tap into new target groups and mobilize supporters. This paper provides insights into the little-explored field of far-right influencers. We conceptualize them as individual actors characterized by far-right ideology, positioned as political influencers, actively advocating for their ideological aims. Employing a multi-layered computational approach to explore communication practices and networking structures of 243 German-speaking far-right influencers on Telegram, we derive a typology and observe the emergence of a functionally differentiated influencer collective. In this collective, each community has specific functions and characteristics that emphasize different ideological aspects, mobilization modes, and influencer practices. Despite the decentralized organization, we find high efficiency in information dissemination. The results corroborate the assumed potential of far-right influencers as disseminators of ideological content who can be particularly persuasive through their role as parasocial opinion leaders.},
 author = {Sophia Rothut and Heidi Schulze and Julian Hohner and Diana Rieger},
 doi = {10.1177/14614448231164409},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14614448231164409},
 journal = {New Media & Society},
 number = {0},
 pages = {14614448231164408},
 title = {Ambassadors of ideology: A conceptualization and computational investigation of far-right influencers, their networking structures, and communication practices},
 url = {https://doi-org.crai.referencistas.com/10.1177/14614448231164409},
 volume = {0},
 year = {2023r}
}

@article{doi:10.1177/14614448231210268,
 abstract = {Contemporary artificial intelligence and algorithmic processes address deep-seated existential challenges and modes of desire. In so doing, they produce computational systems of imagination, an “algorithmic as if” that enables the expression, transformation, and seeming overcoming of existential limitations via technological means. This article elaborates the character of the “algorithmic as if” by focusing on Deep Nostalgia, an online tool that turns personal photographs of the deceased into looped animations which smile, blink, and move, promising to overcome mortality by technologically “resurrecting the dead.” Performing a close-reading of Deep Nostalgia’s technological processes and the public discourse around its 2021 launch, the article highlights its combination of computational learning, forms of visual representation (photography, video, and animation), and distinctive realignments of temporal experience. Together, these frame the “algorithmic as if” as a magical and affective space for realizing impossible longings that are also reflexive encounters with the “limit-situation” of human mortality.},
 author = {Sara Kopelman and Paul Frosh},
 doi = {10.1177/14614448231210268},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14614448231210268},
 journal = {New Media & Society},
 number = {0},
 pages = {14614448231210268},
 title = {The “algorithmic as if”: Computational resurrection and the animation of the dead in Deep Nostalgia},
 url = {https://doi-org.crai.referencistas.com/10.1177/14614448231210268},
 volume = {0},
 year = {2023j}
}

@article{doi:10.1177/1461445606069331,
 author = {Esmat Babaii},
 doi = {10.1177/1461445606069331},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1461445606069331},
 journal = {Discourse Studies},
 number = {6},
 pages = {849–852},
 title = {Book Review: Computational and Quantitative Studies},
 url = {https://doi-org.crai.referencistas.com/10.1177/1461445606069331},
 volume = {8},
 year = {2006a}
}

@article{doi:10.1177/1461445619866985,
 abstract = {In past studies, the few quantitative approaches to discourse structure were mostly confined to the presentation of the frequency of discourse relations. However, quantitative approaches should take into account both hierarchical and relational layers in the discourse structure. This study considers these factors and addresses the issue of how discourse relations and discourse units are related. It draws upon the available corpora of discourse structure (rhetorical structure theory-discourse treebank (RST-DT)) from a new perspective. Since an RST tree can be converted into a syntactic dependency tree, the data extracted from the RST-DT can be useful for calculating the discourse distance in much the same way as syntactic dependency distance is calculated. Discourse distance is also applicable to measuring the depth of the human processing of discourse. Furthermore, the data derived from the RST-DT are also easily converted into network data. This study finds that discourse structure has its discourse distance minimum and each type of RST relations has its range of discourse distance. The frequency distribution of discourse data basically follows the power law on several levels, while a network approach reveals how discourse units are arranged spatially in regular patterns. The two methods are mutually complementary in revealing the interaction between discourse relations and discourse units in a comprehensive manner, as well as in revealing how people process and comprehend discourse dynamically. Accordingly, we propose merging the two methods so as to yield a computational model for assessing discourse complexity and comprehension.},
 author = {Kun Sun and Wenxin Xiong},
 doi = {10.1177/1461445619866985},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1461445619866985},
 journal = {Discourse Studies},
 number = {6},
 pages = {690–712},
 title = {A computational model for measuring discourse complexity},
 url = {https://doi-org.crai.referencistas.com/10.1177/1461445619866985},
 volume = {21},
 year = {2019o}
}

@article{doi:10.1177/1463499605061672,
 author = {J. David Granger},
 doi = {10.1177/1463499605061672},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1463499605061672},
 journal = {Anthropological Theory},
 number = {4},
 pages = {583–584},
 title = {Book Review: Neuropolitics: Thinking, Culture, Speed. Out of Bounds},
 url = {https://doi-org.crai.referencistas.com/10.1177/1463499605061672},
 volume = {5},
 year = {2005d}
}

@article{doi:10.1177/1463949119846541,
 abstract = {This article presents steps that can be used to expose children to concrete experiences of practicing coding using unplugged activities, including using directional words or directional arrows, using sequential words, combining both directional and sequential words, and connecting with grids.},
 author = {Joohi Lee},
 doi = {10.1177/1463949119846541},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1463949119846541},
 journal = {Contemporary Issues in Early Childhood},
 number = {3},
 pages = {266–269},
 title = {Coding in early childhood},
 url = {https://doi-org.crai.referencistas.com/10.1177/1463949119846541},
 volume = {21},
 year = {2020o}
}

@article{doi:10.1177/14639491211033663,
 abstract = {Algorithms are the essence of computational thinking, which refers to a set of problem-solving processes that help children become logical thinkers in this increasingly digital society. It is important for teachers of young children to carefully plan and implement algorithm design tasks that involve repeated step-by-step procedures to build strong foundational computational thinking skills. In this article, the authors present algorithm tasks, including following a recipe, creating a treasure map, modeling how to perform a task, and sharing a routine, which can be easily integrated in the daily activities in early childhood classrooms. Fostering young children’s aptitude for algorithm-specific thinking-and-doing processes creates a foundation for logical thinking.},
 author = {Joohi Lee and Candace Joswick and Kathryn Pole and Robin Jocius},
 doi = {10.1177/14639491211033663},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14639491211033663},
 journal = {Contemporary Issues in Early Childhood},
 number = {2},
 pages = {198–202},
 title = {Algorithm design for young children},
 url = {https://doi-org.crai.referencistas.com/10.1177/14639491211033663},
 volume = {23},
 year = {2022i}
}

@article{doi:10.1177/1464420712470356,
 abstract = {Advanced non-linear dynamics, finite element computational methods and tools are utilized in order to assess the blast wave mitigation potential of the fluid–structure interaction phenomena involving rigid and deformable structures. The employed computational methods and tools are verified and validated by first demonstrating that they can quite accurately reproduce analytical solutions for a couple of well-defined blast wave propagation and interaction problems. Then the methods/tools are used to investigate the fluid–structure interaction phenomena involving deformable structures while accounting for both the interaction of the incident blast wave with the structure and for the structure-motion induced blast wave (at the back-face of the structure). To assess the role of the structure deformability, i.e. the role of the shock waves generated within the structure, the results obtained are compared with their rigid structure counterparts. This comparison established that no additional structure-deformability-related blast-mitigation effects are observed in the case of fully supported blast wave loading while, under exponentially decaying blast wave loading, such effects are observed but only under conditions when the shock wave propagation time within the structure is comparable with the incident wave decay time.},
 author = {M Grujicic and JS Snipes and N Chandrasekharan},
 doi = {10.1177/1464420712470356},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1464420712470356},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part L: Journal of Materials: Design and Applications},
 number = {2},
 pages = {124–142},
 title = {Computational analysis of fluid–structure interaction based blast-mitigation effects},
 url = {https://doi-org.crai.referencistas.com/10.1177/1464420712470356},
 volume = {227},
 year = {2013k}
}

@article{doi:10.1177/1464420716644472,
 author = {M Grujicic and S Ramaswami and JS Snipes and P Dudt},
 doi = {10.1177/1464420716644472},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1464420716644472},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part L: Journal of Materials: Design and Applications},
 number = {3},
 pages = {337–367},
 title = {RETRACTED: Potential improvement in helmet blast-protection via the use of a polyurea external coating: Combined experimental/computational analyses},
 url = {https://doi-org.crai.referencistas.com/10.1177/1464420716644472},
 volume = {234},
 year = {2020i}
}

@article{doi:10.1177/1464420716673670,
 abstract = {The extent of inter-material mixing and the formation of intermetallic compounds play a critical role in the structural integrity and mechanical properties of the joints in the case of dissimilar metal friction stir welding. In general, there is a critical volume fraction of the intermetallic compounds in the mix zone of the friction stir welding-joint at which the mechanical properties of the joint are maximized. That is, insufficient inter-material mixing and the accompanying sub-critical volume fraction of the intermetallic compounds results in insufficient inter-material bonding and inferior joint strength. Conversely, super-critical volume fraction of the intermetallic compounds typically gives rise to the joint embrittlement. To address the problem of the effect of the friction stir welding process parameters on the extent of intermetallic compound formation, a multi-physics computational framework has been developed and applied to the case of dissimilar metal friction stir welding involving commercially pure (CP) aluminum and AISI 1005 low-carbon steel. The multi-physics framework comprises the following main modules: (a) finite-element-based friction stir welding-process modeling; (b) quantum-mechanics, atomistic and CALPHAD-type continuum material thermodynamics analyses of the intermetallic compound-nucleation process; (c) a continuum-type analysis of multi-component diffusion-controlled growth of the intermetallic compounds; and (d) Kolmogorov–Johnson–Mehl–Avrami type analysis of the evolution of the intermetallic compound volume fraction within the friction stir welding joint as a function of the friction stir welding process parameters. The results obtained revealed that: (i) the extent and the spatial distribution of the intermetallic compounds is a sensitive function of the friction stir welding-process parameters; and (ii) among the six potential Al-Fe intermetallic compounds, FeAl and Fe3Al are associated with the largest volume fractions and, hence, play a key role in both attaining the required joint strength and in the potential loss of the joint fracture toughness.},
 author = {M Grujicic and JS Snipes and S Ramaswami and R Galgalikar and C-F Yen and BA Cheeseman},
 doi = {10.1177/1464420716673670},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1464420716673670},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part L: Journal of Materials: Design and Applications},
 number = {6},
 pages = {1080–1100},
 title = {Computational analysis of the intermetallic formation during the dissimilar metal aluminum-to-steel friction stir welding process},
 url = {https://doi-org.crai.referencistas.com/10.1177/1464420716673670},
 volume = {233},
 year = {2019i}
}

@article{doi:10.1177/1464420717707227,
 abstract = {This work focuses on the analysis of the micro and macroscopic mechanical response of particle-reinforced composites. A particular attention is paid to the influence of two fundamental design parameters, i.e. the particles shape and their volume fraction (up to very high values ranging from 0 to almost 0.8), on the overall mechanical response of the structure as well as on the resulting elastic symmetry of the material. The strain energy-based homogenisation technique of periodic media is here applied to a 2D finite element model of a representative volume element of the composite. Different algorithms are developed to generate, with a good level of accuracy, the real microstructure of the composite material characterised by circular as well as polygonal particles. Moreover, for each studied configuration, a link between the geometrical parameters of the microstructure (particles shape, size, distribution, and volume fraction) and the size of the representative volume element is also provided in order to properly describe the constitutive behaviour of the composite at the macroscopic scale. The numerical results are compared with analytical models taken from the literature to prove on the one hand the limitations of the analytical approaches and on the other hand the effectiveness of the proposed numerical models.},
 author = {Timothée Gentieu and Anita Catapano and Julien Jumel and James Broughton},
 doi = {10.1177/1464420717707227},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1464420717707227},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part L: Journal of Materials: Design and Applications},
 number = {6},
 pages = {1101–1116},
 title = {Computational modelling of particulate-reinforced materials up to high volume fractions: Linear elastic homogenisation},
 url = {https://doi-org.crai.referencistas.com/10.1177/1464420717707227},
 volume = {233},
 year = {2019d}
}

@article{doi:10.1177/1464420720961426,
 abstract = {In this paper, an outline of mean-field homogenization and fracture toughness of boron nitride nanotube-reinforced aluminium and epoxy composites have been presented. The meso-scale material modelling has been achieved using Mori-Tanaka and double inclusion homogenization schemes. To examine the effectiveness of Mori-Tanaka and double inclusion schemes, a comparison has been drawn with the rule of mixture. Meso-scale numerical results of boron nitride nanotube-reinforced aluminium and epoxy composites are presented for various design parameters such as aspect ratio and volume fraction of boron nitride nanotubes. The validation studies of the developed transversely isotropic composites model have been documented with the experiments performed and existing literature. Finally, macro-scale fracture toughness simulations incorporating the transversely isotropic properties have been performed using the domain integral method. It is observed that aluminium and epoxy with elastoplastic and viscoelastic nature, respectively, have shown a significant effect on the fracture properties of the composites.},
 author = {Srikant Padmanabhan and Ankit Gupta and Gaurav Arora and Himanshu Pathak and Ramesh G Burela and Anant S Bhatnagar},
 doi = {10.1177/1464420720961426},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1464420720961426},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part L: Journal of Materials: Design and Applications},
 number = {2},
 pages = {293–308},
 title = {Meso–macro-scale computational analysis of boron nitride nanotube-reinforced aluminium and epoxy nanocomposites: A case study on crack propagation},
 url = {https://doi-org.crai.referencistas.com/10.1177/1464420720961426},
 volume = {235},
 year = {2021m}
}

@article{doi:10.1177/1464884913486393,
 abstract = {This article investigates a rapidly expanding branch of journalism innovation in online news media. The umbrella term computational exploration in journalism (CEJ), embraces the multifaceted development of algorithms, data, and social science methods in reporting and storytelling. CEJ typically involves the journalistic co-creation of quantitative news projects that transcend geographical, disciplinary, and linguistic boundaries. Drawing on extensive empirical data, this article provides a conceptual overview of the field by identifying three main pathways of computational exploration in journalism: the newsroom approach, the academic approach, and the entrepreneurial approach. Implications for changing journalistic practice are discussed, and the theorizing is summed up in a triplex proposition about changing mindset processes coming out of CEJ. The study indicates that the computational exploration not only leads to innovative uses of the technology, but also to innovative ways for journalists to think and behave; journalism innovation leads to innovation journalism.},
 author = {Astrid Gynnild},
 doi = {10.1177/1464884913486393},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1464884913486393},
 journal = {Journalism},
 number = {6},
 pages = {713–730},
 title = {Journalism innovation leads to innovation journalism: The impact of computational exploration on changing mindsets},
 url = {https://doi-org.crai.referencistas.com/10.1177/1464884913486393},
 volume = {15},
 year = {2014g}
}

@article{doi:10.1177/1464884914545729,
 abstract = {Research on ‘digital’ journalism has focused largely on online news, with comparatively less interest in the longer-term implications of software and computational technologies. Drawing upon a 6-year study of the Toronto Star, this article provides an account of TOPS, an in-house web content management system which served as the backbone of thestar.com for 6 years. For some, TOPS was a successful software innovation, while for others, a strategic digital ‘property’. But for most journalists, it was slow, deficient in functionality, aesthetically unappealing and cumbersome. Although several organizational factors can explain TOPS’ obstinacy, I argue for particular attention to the complex ontology of software. Based on an outline of this ontology, I suggest software be taken seriously as an object of journalism, which implies acknowledging its partial autonomy from human use or authorization, accounting for its ability to mutate indefinitely and analysing its capacity to encourage forms of ‘computational thinking’.},
 author = {Scott Rodgers},
 doi = {10.1177/1464884914545729},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1464884914545729},
 journal = {Journalism},
 number = {1},
 pages = {10–26},
 title = {Foreign objects? Web content management systems, journalistic cultures and the ontology of software},
 url = {https://doi-org.crai.referencistas.com/10.1177/1464884914545729},
 volume = {16},
 year = {2015s}
}

@article{doi:10.1177/1464884915612681,
 abstract = {Journalism education has tended to respond slowly to developments in digital journalism, such as data journalism, despite or because of close links with the industry. This article examines the obstacles to innovation in journalism education with particular reference to data journalism, drawing on the literature, a review of stakeholders and course documents, and the author’s reflections on developing a data journalism module as part of a new MA programme. It highlights the complexities linked to the particular demands of data journalism, and identifies critical issues around student satisfaction; reputation and job/career outcomes; relevance, currency and appeal; programme management; and coherence. Rather than holding it back, more specialized socialization could assist journalism education to innovate effectively, the author suggests.},
 author = {Jonathan Hewett},
 doi = {10.1177/1464884915612681},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1464884915612681},
 journal = {Journalism},
 number = {1},
 pages = {119–137},
 title = {Learning to teach data journalism: Innovation, influence and constraints},
 url = {https://doi-org.crai.referencistas.com/10.1177/1464884915612681},
 volume = {17},
 year = {2016j}
}

@article{doi:10.1177/1464884916657517,
 abstract = {This article presents a study of what affects professional knowledge creation when journalism students have their periods of internship in legacy, yet highly digitized newsrooms. A total of 16 Norwegian j-students are interviewed and 30 internship reports analysed in order to detect the different actors – both humans and non-humans – that matter when students learn through practice in such newsrooms. Through this analysis, this article aims at understanding some of the tensions between legacy and digital culture that many newsrooms today are marked by and how these tensions affect professional knowledge creations for the journalists of tomorrow. The study is framed by sociomaterial perspectives on learning and journalistic practice and argues that two types of materiality are especially important for j-students’ professional knowledge creation during periods of internship: the structure and layout of the newsroom and the software applications in use at the newsroom.},
 author = {Steen Steensen},
 doi = {10.1177/1464884916657517},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1464884916657517},
 journal = {Journalism},
 number = {4},
 pages = {464–480},
 title = {What is the matter with newsroom culture? A sociomaterial analysis of professional knowledge creation in the newsroom},
 url = {https://doi-org.crai.referencistas.com/10.1177/1464884916657517},
 volume = {19},
 year = {2018q}
}

@article{doi:10.1177/1464884917693864,
 abstract = {This article outlines a general epistemological framework of data journalism in the devolved nations of the United Kingdom. By using an original model based on three conceptual lenses – materiality, performativity and reflexivity – this study examines the development of this form of journalism, the challenges it faces and its particularities in the context of Scotland, Wales and Northern Ireland. This research, therefore, offers unique insights from semi-structured interviews with data journalists and data editors based at, or working as freelancers for, the mainstream news organisations of these regions. The results suggest that data journalism in these devolved nations displays a distinctive character just as much as it reinforces the norms and rituals of the legacy organisations that pioneered this practice. While various models of data exploitation are tested, regional data journalists creatively circumvent generalised organisational struggles to lay the groundwork for their trade and professional community.},
 author = {Eddy Borges-Rey},
 doi = {10.1177/1464884917693864},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1464884917693864},
 journal = {Journalism},
 number = {7},
 pages = {915–932},
 title = {Towards an epistemology of data journalism in the devolved nations of the United Kingdom: Changes and continuities in materiality, performativity and reflexivity},
 url = {https://doi-org.crai.referencistas.com/10.1177/1464884917693864},
 volume = {21},
 year = {2020e}
}

@article{doi:10.1177/14648849211010582,
 abstract = {This paper presents work surrounding INJECT, a newsroom innovation offering digital tools to support journalists. Research showing increasing time and resource pressure on journalists has led to concerns about the demise of investigative reporting and the ability of today’s journalists to interrogate information adequately. Some digital innovations (e.g. tools facilitating robot journalism) have been viewed with suspicion by newsrooms. This paper reports on a research project that seeks to create an innovative tool to support the creative capabilities of time and resource poor journalists. The INJECT project used the advanced information discovery capabilities of digitisation to help journalists find new angles on stories and this paper analyses the extent to which such initiatives might harness digital innovation to benefit both the quality and range of reporting and thereby enhance creativity. It examines the potential of an information processing model of creativity derived from the INJECT tool to assist and support journalists, exploring the theoretical impact as well as the practical implications reported from the newsroom.},
 author = {Suzanne Franks and Rebecca Wells and Neil Maiden and Konstantinos Zachos},
 doi = {10.1177/14648849211010582},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14648849211010582},
 journal = {Journalism},
 number = {9},
 pages = {1881–1899},
 title = {Using computational tools to support journalists’ creativity},
 url = {https://doi-org.crai.referencistas.com/10.1177/14648849211010582},
 volume = {23},
 year = {2022g}
}

@article{doi:10.1177/14648849211023872,
 abstract = {Software studies is a research field that focuses on the social and cultural implications of the software. They are grounded by interdisciplinarity, borrowing to digital humanities, cultural studies, or new media studies. Their application domains are as heterogeneous as software can be, from interfaces to new digital mediatic forms. This paper examines the relevance of software studies for journalism studies in the context of automated news production, where technological artifacts can also be understood as being shaped by professional values and cultural practices. It also explores methods nourished by software studies’ theories to tackle news automation through a process-oriented approach. The cursor is placed on the materiality of the upstream components of automated news production: the data that feed the systems and the process that will make the news. Automated news production appears as a remixed editorial process nourished by previous editorial experiences that will be standardized through a some-how imitation game where technological and human mediation interplay. This paper addresses the issue of transparency with the flattening of the processes at work, relying on theories and methodological tools based on software studies.},
 author = {Laurence Dierickx},
 doi = {10.1177/14648849211023872},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14648849211023872},
 journal = {Journalism},
 number = {3},
 pages = {654–670},
 title = {News automation, materialities, and the remix of an editorial process},
 url = {https://doi-org.crai.referencistas.com/10.1177/14648849211023872},
 volume = {24},
 year = {2023d}
}

@article{doi:10.1177/14648849241231040,
 abstract = {Following the civil war in Syria in 2011, migration from Syria in massive numbers has caused various political, demographic, societal, and cultural challenges in Turkey. The focus of this research is on how the issue of naturalization of Syrians as Turkish citizens has been presented in Turkish newspapers and to what extent the dominant perspective on the issue has shifted over time. For our computational textual analysis of Turkish newspaper materials published between 2012 and 2019, we compiled a corpus consisting of 15,276 news items from four newspapers featuring the co-occurrence of the words “Syria*” (Suriye*) and “citizen*” (vatandaş*) in the main body of their text. The newspapers include mainstream and pro-government (Milliyet with liberal tendencies, Sabah with center-right tendencies), and left-wing (Sol, Evrensel) perspectives. A two-step textual analysis was conducted on the corpus. First, dictionary-based topic modeling was used to chart which broader topics Turkish citizenship of Syrians is predominantly linked to the data, and how the predominance of topics shifted over time. Second, a collocation analysis of the word “refugee” (mülteci) yields a more fine-grained picture of the specific words and expressions that tend to appear often when Syrian refugees are discussed. In both steps, an aggregated general analysis was complemented with separate newspaper-specific analyses, thus allowing for the comparison of different newspapers. Findings show that Syrians in Turkey predominantly feature in the context of diplomatic and societal topics. While mainstream news sources approach the issue less critically, left-wing newspapers show more sensitivity to migrant rights in Turkey.},
 author = {Elçin Istif Inci and Dirk Speelman},
 doi = {10.1177/14648849241231040},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14648849241231040},
 journal = {Journalism},
 number = {0},
 pages = {14648849241231040},
 title = {Syrians in Turkey and their naturalization as Turkish citizens: A computational text analysis of newspaper data},
 url = {https://doi-org.crai.referencistas.com/10.1177/14648849241231040},
 volume = {0},
 year = {2024f}
}

@article{doi:10.1177/14648849241279579,
 abstract = {This article sheds light on the emerging forms of cultural capital that media practitioners need to acquire to work with automated news, as in Bourdieu’s understanding of unique abilities that include, among others, journalistic expertise and technical know-how. To uncover these new skills, we carried out 30 interviews with editorial staff, executives and technologists working at 23 media organisations based in Europe, North America and Australia. We show that these new forms of cultural capital are essentially two-fold: on the one hand, they involve taking a “structured journalism” approach so as to think of what an ideal story may look like, and then by breaking it down into smaller predictable elements that can be reusable across many versions of that same story; on the other hand, they also call for knowing how to embed a media organisation’s standards and practices into code for automated news. Overall this study argues that a new type of cultural capital emerges, as it is associated with the production of automated news. We call it the distinct-abstract capital, whereby journalism is thought of both as a one-off endeavour and as a process that can be deconstructed in an abstract way close to computer programming.},
 author = {Samuel Danzon-Chambaud and Alessio Cornia},
 doi = {10.1177/14648849241279579},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14648849241279579},
 journal = {Journalism},
 number = {0},
 pages = {14648849241279580},
 title = {The cultural capital you need to work with automated news: Not only “your beautiful piece of work”, but also “patterns that emerge”},
 url = {https://doi-org.crai.referencistas.com/10.1177/14648849241279579},
 volume = {0},
 year = {2024a}
}

@article{doi:10.1177/1466138117725340,
 abstract = {This article argues the advance of computational methods for analyzing, visualizing and disseminating social scientific data can provide substantial tools for ethnographers operating within the broadly realist ‘normal-scientific tradition’ (NST). While computation does not remove the fundamental challenges of method and measurement that are central to social research, new technologies provide resources for leveraging what NST researchers see as ethnography’s strengths (e.g. the production of in situ observations of people over time) while addressing what NST researchers see as ethnography’s weaknesses (e.g. questions of sample size, generalizability and analytical transparency). Specifically, we argue computational tools can help: (1) scale ethnography, (2) improve transparency, (3) allow basic replications, and (4) ultimately address fundamental concerns about internal and external validity. We explore these issues by illustrating the utility of three forms of ethnographic visualization enabled by computational advances – ethnographic heatmaps (ethnoarrays), a combination of participant observation data with techniques from social network analysis (SNA), and text mining. In doing so, we speak to the potential uses and challenges of nascent ‘computational ethnography.’},
 author = {Corey M. Abramson and Jacqueline Joslyn and Katharine A. Rendle and Sarah B. Garrett and Daniel Dohan},
 doi = {10.1177/1466138117725340},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1466138117725340},
 journal = {Ethnography},
 number = {2},
 pages = {254–284},
 title = {The promises of computational ethnography: Improving transparency, replicability, and validity for realist approaches to ethnographic analysis},
 url = {https://doi-org.crai.referencistas.com/10.1177/1466138117725340},
 volume = {19},
 year = {2018a}
}

@article{doi:10.1177/1468087410396145,
 abstract = {The purpose of this study is to analyse the differences in the combustion process, and pollutants formation (especially soot) due to top-hat and boot injection-rate shapes at one specific high-load point of a single-cylinder small-bore diesel engine using multidimensional engine simulations. The simulations are performed using a response interactive flamelet model with detailed chemical kinetics. A detailed chemistry-based soot model is used for the prediction of soot emissions. The heights of the first and second stages of the boot shape are varied to observe the effect of the injected fuel mass distribution. In addition, results of boot shapes are compared with a trapezoidal (top-hat) shape. A detailed analysis of soot formation and oxidation is also presented for some selected rate shapes. Through computational analysis it is shown that the boot shapes have the potential to decrease combustion-generated noise and to lower emissions at the investigated load point compared to the top-hat shape. Variations in the temporal distribution of the injected fuel mass show that a lower height of the first stage and a higher height of the second stage of the boot shape result in a relatively slower rise of heat release (lower combustion-generated noise) in the early part of combustion and enhanced soot oxidation as a result of higher spray momentum near the end of injection.},
 author = {V Luckhchoura and N Peters and R Diwakar},
 doi = {10.1177/1468087410396145},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1468087410396145},
 journal = {International Journal of Engine Research},
 number = {2},
 pages = {145–168},
 title = {Computational analysis of injection-rate shapes in a small-bore direct-injection diesel engine},
 url = {https://doi-org.crai.referencistas.com/10.1177/1468087410396145},
 volume = {12},
 year = {2011k}
}

@article{doi:10.1177/1468087412438796,
 abstract = {The mixture formation and combustion process of a hydrogen direct-injection internal combustion engine is computed using a modified version of a commercial three-dimensional computational fluid dynamics code. The aim of the work is the evaluation of hydrogen laminar flame speed correlations and turbulent flame speed closures with respect to combustion of premixed and stratified mixtures at various levels of air-to-fuel equivalence ratio. Heat-release rates derived from in-cylinder pressure traces are used for the validation of the combustion simulations. A turbulent combustion model with closures for a turbulent flame speed is investigated. The value of the computed heat-release rates mainly depends on the quality of laminar burning velocities and standard of turbulence quantities provided to the combustion model. Combustion simulations performed with experimentally derived laminar flame speed data give better results than those using laminar flame speeds obtained from a kinetic scheme. However, experimental data of hydrogen laminar flame speeds found in the literature are limited regarding the range of pressures, temperatures and air-to-fuel equivalence ratios, and do not comply with the demand of high-pressure engine-relevant conditions.},
 author = {Udo Gerke and Konstantinos Boulouchos},
 doi = {10.1177/1468087412438796},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1468087412438796},
 journal = {International Journal of Engine Research},
 number = {5},
 pages = {464–481},
 title = {Three-dimensional computational fluid dynamics simulation of hydrogen engines using a turbulent flame speed closure combustion model},
 url = {https://doi-org.crai.referencistas.com/10.1177/1468087412438796},
 volume = {13},
 year = {2012g}
}

@article{doi:10.1177/1468087413488882,
 abstract = {To improve the fuel efficiency of automobile internal combustion engines, the friction reduction of each engine moving part is important. Recently, carbon films have been attracting much interest as surface-coating materials due to its excellent properties such as low friction and good wear resistance. In this study, the low-friction mechanisms of carbon films were analyzed using molecular dynamics simulations and density functional theory calculations. Molecular dynamics simulation results showed that the termination of OH groups on the surface of the diamond substantially reduced the friction coefficient from 0.07 to 0.01. This reduction was achieved because termination of OH groups weakened the covalent interaction between Fe and C atoms, which was indicated by density functional theory calculations. Additionally, based on the concept of terminating the OH groups on the surface of diamond-like carbon films, we carried out the reciprocating friction experiment between hydrogen-free diamond-like carbon surface and glycerin, which contains large number of OH groups. The friction coefficient of the glycerin was 0.028, much lower than that of the base oil, which was 0.075. The experiments confirmed that OH groups on the surface of hydrogen-free diamond-like carbon films greatly improved the friction properties of its films.},
 author = {Yusuke Morita and Satoshi Jinno and Motoichi Murakami and Nozomu Hatakeyama and Akira Miyamoto},
 doi = {10.1177/1468087413488882},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1468087413488882},
 journal = {International Journal of Engine Research},
 number = {4},
 pages = {399–405},
 title = {A computational chemistry approach for friction reduction of automotive engines},
 url = {https://doi-org.crai.referencistas.com/10.1177/1468087413488882},
 volume = {15},
 year = {2014p}
}

@article{doi:10.1177/1468087413500297,
 abstract = {A detailed understanding of the air–fuel mixing process in gasoline direct injection engines is necessary to avoid soot formation that might result from charge inhomogeneities or liquid fuel impingement on the cylinder walls. Within this context, the use of multidimensional models might be helpful to better understand how spray evolution in cylinder charge motions and combustion chamber design affects the mixture quality at spark-timing. In this work, the authors developed and applied a computational fluid dynamics methodology to simulate gas exchange and air–fuel mixture formation in gasoline direct injection engines. To this end, a suitable set of spray submodels was implemented into an open-source code to properly describe the evolution of gasoline jets emerging from multihole atomizers. Furthermore, the complete liquid film dynamics was also considered. For a proper assessment of the approach, a gasoline direct injection engine running at full load was simulated and effects of spray targeting and engine speed were studied. A detailed postprocessing of the computed data of liquid film mass, homogeneity index and equivalence ratio distributions was performed and correlated with experimental data of particulate emissions. Satisfactory results were achieved, proving the effectiveness of the proposed methodology in predicting the effects of injection system and operating conditions on soot formation.},
 author = {Tommaso Lucchini and Gianluca D’ Errico and Angelo Onorati and Giovanni Bonandrini and Luca Venturoli and Rita Di Gioia},
 doi = {10.1177/1468087413500297},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1468087413500297},
 journal = {International Journal of Engine Research},
 number = {5},
 pages = {581–596},
 title = {Development and application of a computational fluid dynamics methodology to predict fuel–air mixing and sources of soot formation in gasoline direct injection engines},
 url = {https://doi-org.crai.referencistas.com/10.1177/1468087413500297},
 volume = {15},
 year = {2014k}
}

@article{doi:10.1177/1468087414560592,
 abstract = {In order to satisfy emission standards and CO2 targets, spark-ignition engines are designed to operate with high dilution rates, compression ratios and boost levels, thus increasing the propensity for unstable combustion. Therefore it is important to address cycle-to-cycle variability (CCV) in complete engine simulators in order to support the design of viable architectures and control strategies. This work concerns the development, validation and application to a multi-cylinder spark-ignition engine of a physics-based one-dimensional combustion model able to render CCV. Its basis relies on the analysis of Large-Eddy Simulation (LES) of flow in a single-cylinder engine used to extract information relating physics to cyclic fluctuations. A one-dimensional CCV model is derived, accounting for variability related to in-cylinder aerodynamics, turbulence and mixture composition. A detailed spark-ignition model is developed, and the resulting model captures the strongly non-linear interactions between flow and combustion, starting from spark ignition and covering laminar/turbulent transition and wrinkling of the flame surface. A first validation is presented against dedicated experimental data from a single-cylinder engine. Detailed comparisons between measurements and predictions are reported on a set of parametric variations around a reference point to assess the physical bases of the model. The resulting model is applied to the simulation of the operating map of a multi-cylinder turbocharged engine. It is found able to reproduce CCV without the need to perform specific LES of that engine, highlighting a certain level of generality of the developed model.},
 author = {Stéphane Richard and Alessio Dulbecco and Christian Angelberger and Karine Truffin},
 doi = {10.1177/1468087414560592},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1468087414560592},
 journal = {International Journal of Engine Research},
 number = {3},
 pages = {379–402},
 title = {Invited Review: Development of a one-dimensional computational fluid dynamics modeling approach to predict cycle-to-cycle variability in spark-ignition engines based on physical understanding acquired from large-eddy simulation},
 url = {https://doi-org.crai.referencistas.com/10.1177/1468087414560592},
 volume = {16},
 year = {2015q}
}

@article{doi:10.1177/1468087414561276,
 abstract = {The design and optimization stages of combustion systems for modern heavy-duty diesel engines must be supported by reliable computational fluid dynamics tools for the definition of the chamber geometry and injection strategy. To be fully predictive in terms of in-cylinder thermodynamics and flame structure, the employed combustion models must account for complex chemistry and turbulence–kinetics interactions. Within this context, the authors have implemented into an open-source code a model based on the multiple representative interactive flamelets approach and applied it to diesel combustion simulations. New numerical techniques were integrated in the proposed multiple representative interactive flamelets model in order to speed up the central processing unit time in integrating chemistry and the β-probability density function of the chemical species to compute composition in the computational fluid dynamics domain. A parallel validation was performed both with constant-volume and heavy-duty diesel engine experiments, selecting similar operating conditions. In such way, both flame structure and heat release rate predictions are analyzed and the model capabilities with respect to its setup and mesh structure are assessed.},
 author = {Gianluca D’Errico and Tommaso Lucchini and Angelo Onorati and Gilles Hardy},
 doi = {10.1177/1468087414561276},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1468087414561276},
 journal = {International Journal of Engine Research},
 number = {1},
 pages = {112–124},
 title = {Computational fluid dynamics modeling of combustion in heavy-duty diesel engines},
 url = {https://doi-org.crai.referencistas.com/10.1177/1468087414561276},
 volume = {16},
 year = {2015e}
}

@article{doi:10.1177/1468087414566513,
 abstract = {The heterogeneous nature of diesel combustion adds many complexities that make understanding the combustion process difficult. Many researchers have made great efforts in diagnostics, prediction, and control capabilities. In this work, a computationally efficient thermodynamic-based model (15 ms on 2010 dual core processor) has been created that predicts the combustion trajectory (path through the ϕ–T plane) with the goal of bridging the gap between typical off-line engine prediction simulations and on-line real-time engine control strategies. The ϕ–T plane is often used to help illustrate the soot and NOx formation behavior during diesel combustion. The experimental engine operating conditions shown illustrate how exhaust gas recirculation influences the combustion trajectory at different timings—that is, showing the typical soot–NOx trade-off for diesel engines and the defeat of this trade-off when low-temperature combustion is obtained. The major insight gained is that the low-temperature combustion trajectory looks similar to a conventional one with just subtle differences that keep it from moving into the soot formation region. Additionally, the traditional conceptual explanations for diesel combustion are explored relative to how they are illustrated by the combustion trajectory, especially the transition from premixed to mixing-controlled combustion. Understanding that behavior in this context aids in explaining the different observations for the low-temperature combustion modes. The fact that these observations are made using this simplified modeling approach is promising for future use of this type of thermodynamic-based models in real-time engine control.},
 author = {Joshua A Bittle and Timothy J Jacobs},
 doi = {10.1177/1468087414566513},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1468087414566513},
 journal = {International Journal of Engine Research},
 number = {2},
 pages = {246–258},
 title = {A computationally efficient combustion trajectory prediction model developed for real-time diesel combustion control},
 url = {https://doi-org.crai.referencistas.com/10.1177/1468087414566513},
 volume = {17},
 year = {2016c}
}

@article{doi:10.1177/1468087416679570,
 abstract = {Combustion control and optimization is of great importance to meet future emission standards in diesel engines: increase in break mean effective pressure at high loads and extension of the operating range of advanced combustion modes seem to be the most promising solutions to reduce fuel consumption and pollutant emissions at the same time. Within this context, detailed computational fluid dynamics tools are required to predict the different involved phenomena such as fuel–air mixing, unsteady diffusion combustion and formation of noxious species. Detailed kinetics, consistent spray models and high quality grids are necessary to perform predictive simulations which can be used either for design or diagnostic purposes. In this work, the authors present a comprehensive approach which was developed using an open-source computational fluid dynamics code. To minimize the pre-processing time and preserve results’ accuracy, algorithms for automatic mesh generation of spray-oriented grids were developed and successfully applied to different combustion chamber geometries. The Lagrangian approach was used to describe the spray evolution while the combustion process is modeled employing detailed chemistry and, eventually, considering turbulence–chemistry interaction. The proposed computational fluid dynamics methodology was first assessed considering inert and reacting experiments in a constant-volume vessel, where operating conditions typical of heavy-duty diesel engines were reproduced. Afterward, engine simulations were performed considering two different load points and two piston bowl geometries, respectively. Experimental validation was carried out by comparing computed and experimental data of in-cylinder pressure, heat release rate and pollutant emissions (NOx, CO and soot).},
 author = {Tommaso Lucchini and Augusto Della Torre and Gianluca D’Errico and Angelo Onorati and Noud Maes and Lambertus MT Somers and Gilles Hardy},
 doi = {10.1177/1468087416679570},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1468087416679570},
 journal = {International Journal of Engine Research},
 number = {1–2},
 pages = {26–38},
 title = {A comprehensive methodology for computational fluid dynamics combustion modeling of industrial diesel engines},
 url = {https://doi-org.crai.referencistas.com/10.1177/1468087416679570},
 volume = {18},
 year = {2017i}
}

@article{doi:10.1177/1468087417701280,
 abstract = {Mixing and combustion of engine combustion network Spray A after end of injection are modeled using highly resolved multidimensional numerical simulations to explore the physics underlying recent experimental observations of combustion recession. Reacting spray simulations are performed using a traditional Lagrangian–Eulerian coupled formulation for two-phase mixture transport with a Reynolds-averaged Navier–Stokes approach using the open-source computational fluid dynamics code OpenFOAM. Chemical kinetics models for n-dodecane by Cai et al. and Yao et al. are deployed to evaluate the impact of mechanism formulation and low-temperature chemistry on predictions of combustion recession behavior. Simulations with the Cai mechanism show that under standard Spray A conditions, the end-of-injection transient induces second-stage ignition in distinct regions near the nozzle that are initially spatially separated from the lifted diffusion flame, but then rapidly merge with flame. By contrast, the Yao mechanism fails to predict sufficient low-temperature chemistry in mixtures upstream of the diffusion flame during the end-of-injection transient and does not predict combustion recession for the same conditions. The effects of the shape and duration of the end-of-injection transient on the entrainment wave near the nozzle, the likelihood of combustion recession, and the spatiotemporal development of mixing and chemistry in near-nozzle mixtures are also investigated. With a more rapid ramp-down injection profile (ramp-down duration < 400 µs), a weaker combustion recession occurs earlier in time after the start of ramp-down. For extremely fast ramp-down (ramp-down duration = 0), the entrainment flux varies rapidly near the nozzle and over-leaning of the mixture completely suppresses combustion recession. For a slower ramp-down profile with respect to the standard Spray A condition, complete combustion recession back toward the nozzle is observed and combustion recession occurred later in time. Simulations qualitatively agreed with the past experimental and modeling observations of combustion recession with different end-of-injection transients.},
 author = {Dorrin Jarrahbashi and Sayop Kim and Benjamin W Knox and Caroline L Genzale},
 doi = {10.1177/1468087417701280},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1468087417701280},
 journal = {International Journal of Engine Research},
 number = {10},
 pages = {1088–1110},
 title = {Computational analysis of end-of-injection transients and combustion recession},
 url = {https://doi-org.crai.referencistas.com/10.1177/1468087417701280},
 volume = {18},
 year = {2017i}
}

@article{doi:10.1177/1468087417731438,
 abstract = {With detailed chemical kinetics being employed in combustion simulations, its major computational challenge is the time-intensive nature of chemical kinetics integration due to the large number of chemical species and wide range of chemical timescales involved. In this work, an extended tabulated dynamic chemistry approach with dynamic pruning method is carried out to simulate complex spray combustion for non-premixed combustion process. The thought of extended tabulated dynamic chemistry approach with dynamic pruning is achieved by selecting the optimum acceleration method as well as its error tolerances at different combustion stages depending on combustion characteristics involving the low-temperature combustion. The present method is applied to realistically complex combustion systems involving spray flame of n-heptane fuel and non-premixed combustion engine. Computation efficiency of the proposed method is compared with the results using different accelerating methods, including dynamical adaptive chemistry, in situ adaptive tabulation, and coupled method of tabulated dynamical adaptive chemistry. The results show that transient computational cost will decrease for low-temperature combustion by reducing ambient oxygen concentration clearly in spray flame. Meanwhile, very low computational efficiency is presented once the autoignition occurs, especially at the initial oxygen concentration of 21%. Based on the feature, extended tabulated dynamic chemistry approach with dynamic pruning with different dynamic adaptive chemistry error tolerances is proposed to improve computational efficiency. Extended tabulated dynamic chemistry approach with dynamic pruning with larger error tolerance improves around two times for decreased amplitude of transient computational cost at high-temperature combustion stage, and at the same time, the computational accuracy is also improved by comparing the important intermediate species obtained by direct integration. For applications in diesel engine, the results show that extended tabulated dynamic chemistry approach with dynamic pruning can accurately capture the first-stage ignition feature that determines the high-temperature combustion stage. In addition, extended tabulated dynamic chemistry approach with dynamic pruning with the smaller in situ adaptive tabulation error tolerance of 0.001 only used at the high-temperature combustion stage significantly improves the performance on diesel engine simulation with a larger chemistry mechanism. The present method further significantly improves computational efficiency with an overall speedup factor of 10 with high-accuracy compared with result using direct integration.},
 author = {Lei Zhou and Wanhui Zhao and Haiqiao Wei},
 doi = {10.1177/1468087417731438},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1468087417731438},
 journal = {International Journal of Engine Research},
 number = {8},
 pages = {839–853},
 title = {Effect of improved accelerating method on efficient chemistry calculations in diesel engine},
 url = {https://doi-org.crai.referencistas.com/10.1177/1468087417731438},
 volume = {19},
 year = {2018t}
}

@article{doi:10.1177/1468087418767451,
 abstract = {Advanced combustion concepts, like homogeneous charge compression ignition, are limited by their narrow operating range, which stems from a lack of control over the heat release process. This study explores a new advanced combustion mode, called thermally stratified compression ignition, which uses a direct water injection event to control the heat release process in low-temperature combustion. A three-dimensional computational fluid dynamics model coupled with detailed chemical kinetics is used to better understand the effects of direct water injection on thermal stratification in the cylinder and the resulting heat release process. Previous results showed that increasing the injection pressure results in a significantly broader temperature distribution due to increased evaporative cooling. In this way, direct water injection can control low-temperature combustion heat release and extend significantly the operable load range. In this study, simulations were performed over a range of start of injection timings in order to determine its effect on thermal stratification and heat release. The results show that for both low and high injection pressures advancing the start of water injection results in increased thermal stratification and reduced peak pressure and heat release rate for injections occurring after −60 °CAD. Before −60 °CAD, advancing the water injection has a varied effect on thermal stratification and heat release depending on the injection pressure and mass of the injected water.},
 author = {Mozhgan Rahimi Boldaji and Aimilios Sofianopoulos and Sotirios Mamalis and Benjamin Lawler},
 doi = {10.1177/1468087418767451},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1468087418767451},
 journal = {International Journal of Engine Research},
 number = {5},
 pages = {555–569},
 title = {Computational fluid dynamics investigations of the effect of water injection timing on thermal stratification and heat release in thermally stratified compression ignition combustion},
 url = {https://doi-org.crai.referencistas.com/10.1177/1468087418767451},
 volume = {20},
 year = {2019p}
}

@article{doi:10.1177/1468087418808949,
 abstract = {Past research has shown that multidimensional computational fluid dynamics modeling in combination with a genetic algorithm method is an effective approach for optimizing internal combustion engine design. However, optimization studies performed with a detailed computational fluid dynamics model are time intensive, which limits the practical application of this approach. This study addresses this issue by using a machine learning approach called Gaussian process regression in combination with computational fluid dynamics modeling to reduce the computational optimization time. An approach was proposed where the Gaussian process regression model could be used instead of the computational fluid dynamics model to predict the outputs of the genetic algorithm optimization. In this approach, for every nth generation of the genetic algorithm, the data from the previous n − 1 generations was used to train the Gaussian process regression model. The approach was tested on an engine optimization study with five input parameters. When the genetic algorithm was run solely with computational fluid dynamics, the optimization took 50 days to complete. In comparison with the computational fluid dynamics and Gaussian process regression approach, the computational time was reduced by 62%, and the optimization was completed in 19 days using the same amount of computational resources. Additional parametric studies were performed to investigate the impact of genetic algorithm + Gaussian process regression parameters. Results showed that either reducing the initial dataset size or relaxing the error criterion resulted in increased Gaussian process regression evaluations within the genetic algorithm. However, relaxing the error criterion was found to impact the model predictions negatively. The initial dataset size was found to have a negligible impact on the final optimum design. Finally, the potential of machine learning in further improving the optimization process was explored by using the Gaussian process regression model to check for the robustness of the designs to operating parameter variations during the optimization. The genetic algorithm was repeated with the modified procedure and it was shown that adding the stability check resulted in a different, more reliable and stable optimum solution.},
 author = {Chaitanya Kavuri and Sage L Kokjohn},
 doi = {10.1177/1468087418808949},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1468087418808949},
 journal = {International Journal of Engine Research},
 number = {7},
 pages = {1251–1270},
 title = {Exploring the potential of machine learning in reducing the computational time/expense and improving the reliability of engine optimization studies},
 url = {https://doi-org.crai.referencistas.com/10.1177/1468087418808949},
 volume = {21},
 year = {2020h}
}

@article{doi:10.1177/1468087419856493,
 abstract = {Cycle-to-cycle variations are important to consider in the development of spark-ignition engines to further increase fuel conversion efficiency. Direct numerical simulation and large eddy simulation can predict the stochastics of flows and therefore cycle-to-cycle variations. However, the computational costs are too high for engineering purposes if detailed chemistry is applied. Detailed chemistry can predict the fuels’ tendency to auto-ignite for different octane ratings as well as locally changing thermodynamic and chemical conditions which is a prerequisite for the analysis of knocking combustion. In this work, the joint use of unsteady Reynolds-averaged Navier–Stokes simulations for the analysis of the average engine cycle and the spark-ignition stochastic reactor model for the analysis of cycle-to-cycle variations is proposed. Thanks to the stochastic approach for the modeling of mixing and heat transfer, the spark-ignition stochastic reactor model can mimic the randomness of turbulent flows that is missing in the Reynolds-averaged Navier–Stokes modeling framework. The capability to predict cycle-to-cycle variations by the spark-ignition stochastic reactor model is extended by imposing two probability density functions. The probability density function for the scalar mixing time constant introduces a variation in the turbulent mixing time that is extracted from the unsteady Reynolds-averaged Navier–Stokes simulations and leads to variations in the overall mixing process. The probability density function for the inflammation time accounts for the delay or advancement of the early flame development. The combination of unsteady Reynolds-averaged Navier–Stokes and spark-ignition stochastic reactor model enables one to predict cycle-to-cycle variations using detailed chemistry in a fraction of computational time needed for a single large eddy simulation cycle.},
 author = {Corinna Netzer and Michal Pasternak and Lars Seidel and Frédéric Ravet and Fabian Mauss},
 doi = {10.1177/1468087419856493},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1468087419856493},
 journal = {International Journal of Engine Research},
 number = {4},
 pages = {649–663},
 title = {Computationally efficient prediction of cycle-to-cycle variations in spark-ignition engines},
 url = {https://doi-org.crai.referencistas.com/10.1177/1468087419856493},
 volume = {21},
 year = {2020i}
}

@article{doi:10.1177/1468087419868020,
 abstract = {A detailed prediction of injection and air–fuel mixing is fundamental in modern direct injection, spark-ignition engines to guarantee a stable and efficient combustion process and to minimize pollutant formation. Within this context, computational fluid dynamics simulations nowadays represent a powerful tool to understand the in-cylinder evolution of spray and air–fuel charge. To guarantee the accuracy of the adopted multidimensional spray sub-models, it is mandatory to validate the computed results against available experimental data under well-defined operating conditions. To this end, in this work, the authors proposed the calibration and validation of a comprehensive set of spray sub-models by means of the simulation of the Spray G experiment, available in the context of the engine combustion network. For a suitable validation of the proposed numerical setup in addition to the baseline condition, gasoline direct injection operating points typical of early injection with homogeneous operation, late injection with high ambient density and flash boiling with enhanced fuel evaporation were also simulated. Numerical computations were validated against a wide set of available experimental data by means of an accurate post-processing analysis taking into account axial liquid and vapor penetrations, gas-phase velocity between spray plumes, droplet size, plume liquid velocity, direction and mass distribution. Satisfactory results were achieved with the proposed setup, which is able to predict gasoline spray evolution under different operating conditions.},
 author = {Davide Paredi and Tommaso Lucchini and Gianluca D’Errico and Angelo Onorati and Lyle Pickett and Joshua Lacey},
 doi = {10.1177/1468087419868020},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1468087419868020},
 journal = {International Journal of Engine Research},
 number = {1},
 pages = {199–216},
 title = {Validation of a comprehensive computational fluid dynamics methodology to predict the direct injection process of gasoline sprays using Spray G experimental data},
 url = {https://doi-org.crai.referencistas.com/10.1177/1468087419868020},
 volume = {21},
 year = {2020o}
}

@article{doi:10.1177/1468087419890418,
 abstract = {In-cylinder water injection is a promising approach for reducing NOx and soot emissions from internal combustion engines. It allows one to use a higher compression ratio by reducing engine knock; hence, higher fuel economy and power output can be achieved. However, water injection can also affect engine combustion and emission characteristics if water injection and injector parameters are not properly set. Majority of the previous studies on the water injection are done through experiments. Therefore, subtle aspects of water injection such as in-cylinder interaction of water sprays, spatial distribution of water vapor, and effect on flame propagation are not clearly understood and rarely reported in literature due to experimental limitations. Thus, in the present article, a computational fluid dynamics investigation is carried out to analyze the effects of direct water injection under various injector configurations on water evaporation, combustion, performance, and emission characteristics of a gasoline direct injection engine. The emphasis is given to analyze in-cylinder water spray interactions, flame propagation, water spray droplet size distribution, and water vapor spatial distribution inside the engine cylinder. For the study, the water-to-fuel ratio is varied from 0 to 1. Various water injector configurations using nozzle hole diameters of 0.14, 0.179, and 0.205 mm, along with nozzle holes of 4, 5, 6, and 7, are considered for comparison in addition to the case of no_water. Computational fluid dynamics models used in this study are validated with the available data in literature. From the results, it is found that the emission and performance characteristics of the engine are highly dependent on water evaporation characteristics. Also, the water-to-fuel ratio of 0.6 with 6 number of nozzle holes and the nozzle diameter of 0.14 mm results in the highest indicated mean effective pressure and the lowest NOx, soot, and CO emissions compared to other cases considered.},
 author = {Ankit A Raut and J M Mallikarjuna},
 doi = {10.1177/1468087419890418},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1468087419890418},
 journal = {International Journal of Engine Research},
 number = {8},
 pages = {1520–1540},
 title = {Effects of direct water injection and injector configurations on performance and emission characteristics of a gasoline direct injection engine: A computational fluid dynamics analysis},
 url = {https://doi-org.crai.referencistas.com/10.1177/1468087419890418},
 volume = {21},
 year = {2020n}
}

@article{doi:10.1177/1468087420910348,
 abstract = {Fuels based on admixtures of methane/natural gas and hydrogen are a promising way to reduce CO2 emissions of spark ignition engines and increase their efficiency. A lot of work was conducted experimentally, whereas only limited numerical work is available in the context of three-dimensional modelling of the full engine cycle. This work addresses this fact by proposing a reactive computational fluid dynamics modelling framework to consider the effects of hydrogen addition on the combustion process. Part I of this two-part study focuses on the modelling and crucial considerations in order to predict the mean cycle based on the G-equation combustion model using the Reynolds-averaged Navier–Stokes equations. There, the effect of increased burning speed was globally captured by increasing the flame speed coefficient A, appearing in the considered flame speed closure. The proposed simplified modelling of the early flame stage proved to be robust for the conducted hydrogen variation from 0 to 50 vol% H2 for stoichiometric and lean operation. Scope of this work, Part II, are cyclic fluctuations and the hydrogen influence thereon using large eddy simulation and the proposed modelling framework. The model is probed towards its capabilities to predict the fluctuation of the combustion process for 0 and 50 vol% H2 and correlations influencing the observed peak pressure of the individual cycle are presented. It is shown that the considered approach is capable to reproduce the cyclic fluctuations of the combustion process under the influence of hydrogen addition as well as lean operation. The importance of the early flame phase with respect to arising fluctuations is highlighted as well as the contribution of the resolved scales in terms of the flame front wrinkling.},
 author = {Jann Koch and Christian Schürch and Yuri M Wright and Konstantinos Boulouchos},
 doi = {10.1177/1468087420910348},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1468087420910348},
 journal = {International Journal of Engine Research},
 number = {6},
 pages = {2054–2068},
 title = {Reactive computational fluid dynamics modelling methane–hydrogen admixtures in internal combustion engines part II: Large eddy simulation},
 url = {https://doi-org.crai.referencistas.com/10.1177/1468087420910348},
 volume = {22},
 year = {2021h}
}

@article{doi:10.1177/1468087420916380,
 abstract = {The effects of hydrogen addition to internal combustion engines operated by natural gas/methane has been widely demonstrated experimentally in the literature. Already small hydrogen contents in the fuel show promising benefits with respect to increased engine efficiency, lower CO2 emissions, extended lean operating limits and a higher exhaust gas recirculation tolerance while maintaining the knock resistance of methane. In this article, the influence of hydrogen addition to methane on a spark ignited single cylinder engine is investigated. This article proposes a modelling approach to consider hydrogen addition within three-dimensional reactive computational fluid dynamics in order to establish a framework to gain further insights into the involved processes. Experiments have been performed on a single-cylinder spark-ignition engine situated at a test bed and cater as reference data for validating the proposed reactive computational fluid dynamics modelling approach based around the G-Equation combustion model. Within the course of the first part, crucial aspects relevant to the modelling of the mean engine cycle are highlighted. In this article, a simplified early combustion phase model which considers the transition towards a fully developed turbulent flame following ignition is introduced, along with a second submodel considering combined effects of the walls. The sensitivity of the combustion process towards the modelling approach is presented. The submodels were calibrated for a reference operating point, and a sweep in hydrogen content in the fuel as well as stoichiometric and lean operation has been considered. It is shown that the flame speed coefficient A appearing in the used turbulent flame speed closure, weighting the influence of the turbulent fluctuating speed , has to be adjusted for different hydrogen contents. The introduced submodels allowed for significant improvement of the in-cylinder pressure and heat release rate evolution throughout all considered operating conditions.},
 author = {Jann Koch and Christian Schürch and Yuri M Wright and Konstantinos Boulouchos},
 doi = {10.1177/1468087420916380},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1468087420916380},
 journal = {International Journal of Engine Research},
 number = {5},
 pages = {1525–1539},
 title = {Reactive computational fluid dynamics modelling of methane–hydrogen admixtures in internal combustion engines: Part I – RANS},
 url = {https://doi-org.crai.referencistas.com/10.1177/1468087420916380},
 volume = {22},
 year = {2021i}
}

@article{doi:10.1177/1468087420931730,
 abstract = {One of the major limitations of reactivity controlled compression ignition is higher unburned hydrocarbon and carbon monoxide emissions and lower thermal efficiency at part load operating conditions. In the present study, a combined numerical approach using a commercial three-dimensional computational fluid dynamics code CONVERGE along with artificial neural network and genetic algorithm is presented to address the above limitation. A production light-duty diesel engine is modified to run in reactivity controlled compression ignition by replacing an existing mechanical fuel injection system with a flexible electronic port fuel injection and common rail direct injection systems. The injection schedules of port fuel injection and direct injection injectors are controlled using National Instruments port and direct injection driver modules. Upon validation of combustion and emission parameters, parametric investigations are carried out to establish the effects of direct-injected diesel fuel timing start of injection (SOI), premixed fuel ratio and intake charge temperature on the engine performance and emissions in reactivity controlled compression ignition. The results obtained show that the start of injection timing and intake charge temperature significantly influence combustion phasing, while the premixed fuel ratio controls mixture reactivity and combustion quality. By utilizing the data generated with the validated computational fluid dynamics models, the artificial neural network models are trained to predict the engine exhaust emissions and efficiency. The artificial neural network models for gross indicated efficiency and oxides of nitrogen (NOx) are then coupled with genetic algorithm to maximize gross indicated efficiency while keeping the NOx and soot emissions within Euro VI emission limits. By optimizing the start of injection timing, premixed fuel ratio and intake charge temperature simultaneously using the artificial neural network models coupled with genetic algorithm, 19% improvement in gross indicated efficiency, 60% and 64% reduction in hydrocarbon and carbon monoxide emissions, respectively, are obtained in reactivity controlled compression ignition compared to the baseline case.},
 author = {Avilash Jain and Anand Krishnasamy and Pradeep V},
 doi = {10.1177/1468087420931730},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1468087420931730},
 journal = {International Journal of Engine Research},
 number = {7},
 pages = {2213–2232},
 title = {Computational optimization of reactivity controlled compression ignition combustion to achieve high efficiency and clean combustion},
 url = {https://doi-org.crai.referencistas.com/10.1177/1468087420931730},
 volume = {22},
 year = {2021k}
}

@article{doi:10.1177/1468087420963982,
 abstract = {Developing a profound understanding of the combustion characteristics of the cold-start phase of a Direct Injection Spark Ignition (DISI) engine is critical to meeting increasingly stringent emissions regulations. Computational Fluid Dynamics (CFD) modeling of gasoline DISI combustion under normal operating conditions has been discussed in detail using both the detailed chemistry approach and flamelet models (e.g. the G-Equation). However, there has been little discussion regarding the capability of the existing models to capture DISI combustion under cold-start conditions. Accurate predictions of cold-start behavior involves the efficient use of multiple models - spray modeling to capture the split injection strategies, models to capture the wall-film interactions, ignition modeling to capture the effects of retarded spark timings, combustion modeling to accurately capture the flame front propagation, and turbulence modeling to capture the effects of decaying turbulent kinetic energy. The retarded spark timing helps to generate high heat flux in the exhaust for the faster catalyst light-off during cold-start. However, the adverse effect is a reduced turbulent flame speed due to decaying turbulent kinetic energy. Accordingly, developing an understanding of the turbulence-chemistry interactions is imperative for accurate modeling of combustion under cold-start conditions. In the present work, combustion characteristics during the cold-start, fast-idle phase is modeled using the G-Equation flamelet model and the RANS turbulence model. The challenges associated with capturing the turbulent-chemistry interactions are explained by tracking the flame front travel along the Borghi-Peters regime diagram. In this study, a modified version of the G-Equation combustion model for capturing cold-start flame travel is presented.},
 author = {Arun C Ravindran and Sage L Kokjohn and Benjamin Petersen},
 doi = {10.1177/1468087420963982},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1468087420963982},
 journal = {International Journal of Engine Research},
 number = {9},
 pages = {2786–2802},
 title = {Improving computational fluid dynamics modeling of Direct Injection Spark Ignition cold-start},
 url = {https://doi-org.crai.referencistas.com/10.1177/1468087420963982},
 volume = {22},
 year = {2021p}
}

@article{doi:10.1177/14680874211023466,
 abstract = {In recent years, the use of machine learning-based surrogate models for computational fluid dynamics (CFD) simulations has emerged as a promising technique for reducing the computational cost associated with engine design optimization. However, such methods still suffer from drawbacks. One main disadvantage is that the default machine learning (ML) hyperparameters are often severely suboptimal for a given problem. This has often been addressed by manually trying out different hyperparameter settings, but this solution is ineffective in case of a high-dimensional hyperparameter space. Besides this problem, the amount of data needed for training is also not known a priori. In response to these issues that need to be addressed, the present work describes and validates an automated active learning approach, AutoML-GA, for surrogate-based optimization of internal combustion engines. In this approach, a Bayesian optimization technique is used to find the best machine learning hyperparameters based on an initial dataset obtained from a small number of CFD simulations. Subsequently, a genetic algorithm is employed to locate the design optimum on the ML surrogate surface. In the vicinity of the design optimum, the solution is refined by repeatedly running CFD simulations at the projected optima and adding the newly obtained data to the training dataset. It is demonstrated that AutoML-GA leads to a better optimum with a lower number of CFD simulations, compared to the use of default hyperparameters. The proposed framework offers the advantage of being a more hands-off approach that can be readily utilized by researchers and engineers in industry who do not have extensive machine learning expertise.},
 author = {Opeoluwa Owoyele and Pinaki Pal and Alvaro Vidal Torreira and Daniel Probst and Matthew Shaxted and Michael Wilde and Peter Kelly Senecal},
 doi = {10.1177/14680874211023466},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14680874211023466},
 journal = {International Journal of Engine Research},
 number = {9},
 pages = {1586–1601},
 title = {Application of an automated machine learning-genetic algorithm (AutoML-GA) coupled with computational fluid dynamics simulations for rapid engine design optimization},
 url = {https://doi-org.crai.referencistas.com/10.1177/14680874211023466},
 volume = {23},
 year = {2022k}
}

@article{doi:10.1177/14680874211031370,
 abstract = {Partial fuel stratification (PFS) is a low temperature combustion strategy that can alleviate high heat release rates of traditional low temperature combustion strategies by introducing compositional stratification in the combustion chamber using a split fuel injection strategy. In this study, a three-dimensional computational fluid dynamics (CFD) model with large eddy simulations and reduced detailed chemistry was used to model partial fuel stratification at three different stratified conditions. The double direct injection strategy injects 80% of the total fuel mass at −300 CAD aTDC and the remaining 20% of the fuel mass is injected at three different timings of −160, −50, −35 CAD to create low, medium, and high levels of compositional stratification, respectively. The PFS simulations were validated using experiments performed at Sandia National Laboratories on a single-cylinder research engine that operates on RD5-87, a research-grade E10 gasoline. The objective of this study is to compare the performance of three different reduced chemical kinetic mechanisms, namely SKM1, SKM2, and SKM3, at the three compositional stratification levels and identify the most suitable mechanism to reproduce the experimental data. Zero-dimensional chemical kinetic simulations were also performed to further understand differences in performance of the three reduced chemical kinetic mechanisms to explain variations in CFD derived heat release profiles. The modeling results indicate that SKM3 is the most suitable mechanism for partial fuel stratification modeling of research-grade gasoline. The results also show that the autoignition event progresses from the richer to the leaner compositional regions in the combustion chamber. Notably, the leaner regions that have less mass per unit volume, can contribute disproportionately more toward heat release as there are more cells at leaner equivalence ratio ranges. Overall, this study illuminates the underlying compositional stratification phenomena that control the heat release process in PFS combustion.},
 author = {Gaurav Guleria and Dario Lopez-Pintor and John E Dec and Dimitris Assanis},
 doi = {10.1177/14680874211031370},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14680874211031370},
 journal = {International Journal of Engine Research},
 number = {10},
 pages = {1658–1677},
 title = {A comparative study of gasoline skeletal mechanisms under partial fuel stratification conditions using large eddy simulations},
 url = {https://doi-org.crai.referencistas.com/10.1177/14680874211031370},
 volume = {23},
 year = {2022d}
}

@article{doi:10.1177/14680874221083371,
 abstract = {Group hole nozzles (GHN) are injector tips with clusters of small holes used in place of a single large hole. In this work, GHN with two holes per cluster and a diverging angle of 15° between the holes were studied using experiments and computational fluid dynamics (CFD) and comparisons were made with conventional diesel injectors. First, experiments were performed in a high temperature pressure vessel (HTPV) to understand the free spray behavior. HTPV experiments revealed that the diverging GHN had slower penetration with a more dispersed spray and a shorter lift-off length relative to the conventional injector. The GHN also exhibited higher natural luminosity compared to the conventional injector, which is indicative of increased soot formation with the GHN. CFD model validated with HTPV experimental data was used to study the sensitivity to the angle between the holes and the hole count per cluster. Results showed that soot formation increased as the divergence angle increased from 0° to 15°. Furthermore, as the divergence angle increased, increasing the hole count per cluster increased the soot formation significantly. Following the HTPV study, the injectors were tested on a single-cylinder heavy-duty diesel engine at a high-load operating condition. Results showed that the diverging GHN injector produced a lower mixing-controlled combustion rate compared to the conventional injector. The slower burn rates resulted in lower NOx and higher soot emissions for the diverging GHN injector. CFD simulations of the engine experiments predicted the slower burn rate seen with the diverging GHN injector. In-cylinder visualization of the CFD results showed that slower penetration with the diverging GHN results in reduced mixing upon surface interaction, and an inability to utilize the fresh air in the outer regions of the combustion chamber which resulted in richer local equivalence ratios and higher soot emissions compared to the conventional injector.},
 author = {Chaitanya Kavuri and Chad Koci and Jon Anders and Kenth Svensson and Russ Fitzgerald and Glen Martin and Ryan Zellers and Sage Kokjohn and Adam Dempsey},
 doi = {10.1177/14680874221083371},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14680874221083371},
 journal = {International Journal of Engine Research},
 number = {3},
 pages = {769–792},
 title = {Experimental and computational study comparing conventional diesel injectors and diverging group hole nozzle injectors in a high temperature pressure vessel and a heavy-duty diesel engine},
 url = {https://doi-org.crai.referencistas.com/10.1177/14680874221083371},
 volume = {24},
 year = {2023f}
}

@article{doi:10.1177/14680874221125538,
 abstract = {Predictive modeling of pre-chamber combustion engines relies primarily on the correct description of laminar and turbulent flame speeds. For engineering applications, the correlations of the flame speeds with physical variables involve empirical constants that are valid for a limited range of operating conditions. The current work aims at assessing the significance of laminar flame speed prediction in the simulation of ultra-lean pre-chamber engine combustion operated with methane. Gülder’s empirical correlation for laminar flame speed was chosen as a reference and was further modified for equivalence ratio, pressure and temperature ranges beyond what it was originally derived for, in order to confirm the original hypothesis; the pressure and temperature dependence were adopted as a power-law correlation. Based on the computational results using the skeletal reaction mechanism, the correlation was modified better represent the flame speeds at ultra-lean engine conditions, using GRI 3.0 as a reference. The modified correlation for methane was implemented in CONVERGE, a three-dimensional computational fluid dynamics (CFD) solver, and the results were validated against the experimental data. In all cases, the original formulation of Peters’s turbulent flame speed correlation was used and was found to have insignificant effect on the conditions under study, confirming the importance of the accurate determination of the laminar flame speed that dominates over any turbulence corrections for high Karlovitz number effects. The flame topology was also investigated to provide insights into the observed pressure behavior among the tested cases. Finally, the relevant turbulent combustion regimes encountered in the pre-chamber combustion engine conditions were examined in the Borghi-Peters diagram, further confirming the findings of the study.},
 author = {Ghufran Alkhamis and Mickael Silva and Emre Cenker and Hong G Im},
 doi = {10.1177/14680874221125538},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14680874221125538},
 journal = {International Journal of Engine Research},
 number = {6},
 pages = {2538–2551},
 title = {A computational assessment of flame speed correlation in an ultra-lean pre-chamber engine},
 url = {https://doi-org.crai.referencistas.com/10.1177/14680874221125538},
 volume = {24},
 year = {2023b}
}

@article{doi:10.1177/14680874221149321,
 abstract = {Ducted fuel injection (DFI) is an innovative method that curtails or prevents soot formation in direct-injection compression-ignition engines. DFI uses a simple duct, positioned outside each injector hole, facilitating the fuel/charge gas mixing before ignition. This reduces the equivalence ratio below two, in the autoignition zone, which in turn decreases soot formation. But this method also reduces fuel-conversion efficiency. This study investigates the effects of DFI on in-cylinder heat transfer. Experiments with conventional diesel combustion (CDC) and DFI were performed at four different dilution levels. Computational fluid dynamics (CFD) simulations were carried out at conditions matching those of the experiments, and the simulations were validated by the experimental data. The CFD simulations enabled to examine of in-cylinder heat release and temperature distributions. The heat transfer to the piston, head, and cylinder was investigated. The results show that DFI increased the heat transfer to the walls compared to CDC under the same conditions. This could help explain why DFI has been observed to reduce fuel-conversion efficiency by approximately 1% (absolute) relative to CDC under certain conditions. The efficiency loss typically decreases with dilution, such that DFI can improve fuel-conversion efficiencies relative to CDC at higher dilution levels.},
 author = {Ramazan Şener and Christopher W Nilsen and Drummond E Biles and Charles J Mueller},
 doi = {10.1177/14680874221149321},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14680874221149321},
 journal = {International Journal of Engine Research},
 number = {8},
 pages = {3328–3341},
 title = {A Computational Investigation of Engine Heat Transfer with Ducted Fuel Injection},
 url = {https://doi-org.crai.referencistas.com/10.1177/14680874221149321},
 volume = {24},
 year = {2023p}
}

@article{doi:10.1177/14687984231212723,
 abstract = {This article uses a comparative case study to examine how a methodological approach called interaction geography provides alternative ways to animate space, movement, and affect within the context of early childhood education. We take animation to incorporate the methods for representing space, movement, and affect; the social-material environment which animates the people and things we study; and the lively, energetic talk-in-interaction that takes place as people and things move. Our first case uses interaction geography to animate what we call gestural energies and choreographies between a teacher, students, and materials in a bilingual kindergarten classroom activity. Our second case uses interaction geography to animate a young child’s excitement for learning and teaching through movement in a cultural heritage museum. Together, our analysis demonstrates how interaction geography provides alternative ways to conceptualize the multimodal nature of literacy practices and contributes to a recent turn to affect in literacy research. We discuss how this work has implications not only for literacy researchers, teachers, and teacher educators, but also for architects, administrators, and researchers concerned with the physical design of literacy spaces.},
 author = {Ben Rydal Shapiro and Deborah Silvis},
 doi = {10.1177/14687984231212723},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14687984231212723},
 journal = {Journal of Early Childhood Literacy},
 number = {0},
 pages = {14687984231212724},
 title = {Animated movements, animating methods: An interaction geography approach to space and affect in early childhood education},
 url = {https://doi-org.crai.referencistas.com/10.1177/14687984231212723},
 volume = {0},
 year = {2023q}
}

@article{doi:10.1177/14690667231164096,
 abstract = {Applied sciences have increased focus on omics studies which merge data science with analytical tools. These studies often result in large amounts of data produced and the objective is to generate meaningful interpretations from them. This can sometimes mean combining and integrating different datasets through data fusion techniques. The most strategic course of action when dealing with products of unknown profile is to use exploratory approaches. For omics, this means using untargeted analytical methods and exploratory data analysis techniques. The current study aimed to perform data fusion on untargeted multimodal (negative and positive mode) liquid chromatography–high-resolution mass spectrometry data using multiple factor analysis. The data fusion results were interpreted using agglomerative hierarchical clustering on biplot projections. The study reduced the thousands of spectral signals processed to less than a hundred features (a primary parameter combination of retention time and mass-to-charge ratios, RT_m/z). The correlations between cluster members (samples and features from) were calculated and the top 10% highly correlated features were identified for each cluster. These features were then tentatively identified using secondary parameters (drift time, ion mobility constant and collision cross-section values) from the ion mobility spectra. These ion mobility (secondary) parameters can be used for future studies in wine chemical analysis and added to the growing list of annotated chemical signals in applied sciences.},
 author = {Mpho Mafata and Maria Stander and Keabetswe Masike and Astrid Buica},
 doi = {10.1177/14690667231164096},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14690667231164096},
 journal = {European Journal of Mass Spectrometry},
 note = {PMID:36942424},
 number = {2},
 pages = {111–122},
 title = {Exploratory data fusion of untargeted multimodal LC-HRMS with annotation by LCMS-TOF-ion mobility: White wine case study},
 url = {https://doi-org.crai.referencistas.com/10.1177/14690667231164096},
 volume = {29},
 year = {2023o}
}

@article{doi:10.1177/14703572241239515,
 abstract = {This article’s aim is to explore the possibilities and limitations of some quantitative tools for the analysis of images. It shows how three different families of tools – manual annotation, eye-tracking and signifier analysis – can help to better understand the meaning effects of figurative and plastic (abstract) elements in pictures. Advantages and disadvantages for each family of tools are discussed. The author also suggests some theoretical implications of the adoption of quantitative tools in the humanities and especially within semiotic disciplines.},
 author = {Dario Compagno},
 doi = {10.1177/14703572241239515},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14703572241239515},
 journal = {Visual Communication},
 number = {3},
 pages = {452–472},
 title = {Computational approaches to the figurative and plastic dimensions of images},
 url = {https://doi-org.crai.referencistas.com/10.1177/14703572241239515},
 volume = {23},
 year = {2024f}
}

@article{doi:10.1177/14703572241247836,
 abstract = {Images are today at the centre of multiple social and technological tensions as a consequence of the adoption of digital coding, of the massive diffusion of social networks and of the algorithmic processing to which they are subject, resulting in new opportunities for developing analytical inquiries and meaning-producing social actions. In this introduction, the authors intend to reconstruct the broad context that makes images one of the most important resources of the digital era and to focus on some of the research tracks that characterize it. In the first part, they begin by focusing on the relationship between images and the digital, which they retrace in accordance with the selection of three key moments: the transition from ontology to the epistemology of digital media; the opening, by social networks and portable devices, of a field for the computational study of contemporary cultures; and, finally, the analytical potential arising from the encounter between digital archives and computer algorithms. In the second part, they present the three axes around which this issue is structured: archives, identity and algorithms. They first of all discuss the concept of the archive, by presenting four different understandings it has come to bear in conjunction with digital encoding – the archive as heritage, resource, effect and as database. They go on to address the relation between images and identities, arguing that social platforms and visual apps are a new domain for identity experimentation and social aggregation. Finally, they discuss the issue of algorithms and more generally of the new computational economy that associates large amounts of data with their mobilization as operational images.},
 author = {Enzo D’Armenio and Maria Giulia Dondero},
 doi = {10.1177/14703572241247836},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14703572241247836},
 journal = {Visual Communication},
 number = {3},
 pages = {405–425},
 title = {Introduction: hyper-visuality: images in the era of social platforms, digital archives and computational economies},
 url = {https://doi-org.crai.referencistas.com/10.1177/14703572241247836},
 volume = {23},
 year = {2024d}
}

@article{doi:10.1177/1470412920964905,
 abstract = {This article proposes thinking of media archaeology as an operating table upon which historical, material and technological interconnections between fashion and film are made. By exploring how early cinema and digital film can be coupled to textile as technology, more specifically through the mechanisms of the sewing machine and the Jacquard loom, it extends the historical span from the mid-1890s, with the invention of cinema as projection, to the early 1800s, when computational thinking was successfully implemented as weaving technique. Instead of focusing on film and fashion as means of visual representation, the author relies on the concept of inscription for a better understanding of both cinema (as recording of light and movement) and textile (with its various thread techniques of weaving, stitching, knitting, etc.).},
 author = {Wanda Strauven},
 doi = {10.1177/1470412920964905},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1470412920964905},
 journal = {Journal of Visual Culture},
 number = {3},
 pages = {362–377},
 title = {Sewing machines and weaving looms: a media archaeological encounter between fashion and film},
 url = {https://doi-org.crai.referencistas.com/10.1177/1470412920964905},
 volume = {19},
 year = {2020v}
}

@article{doi:10.1177/1470412920966015,
 abstract = {This article functions as the introduction to the Themed Issue on Archaeologies of Fashion Film. The text introduces fashion film as a genre and as an historically dynamic form of audiovisual expression that we approach through fashion history, media archaeology and new film history. While introducing key concepts and approaches, the authors propose a form of ‘parallax historiography’, a term emerging from Thomas Elsaesser’s work, that links different time periods from early cinema to recent digital platforms, even ‘post-cinema’. The introduction makes references to the contributions in this issue that address historical conditions of emergence, marginal voices in the historical record and unexcavated archival materials; and the issue shows how they all contain feedback loops or recursive traits that resonate in contemporary practice where infrastructures of platforms and data frame the moving image.},
 author = {Caroline Evans and Jussi Parikka},
 doi = {10.1177/1470412920966015},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1470412920966015},
 journal = {Journal of Visual Culture},
 number = {3},
 pages = {323–339},
 title = {Introduction: touch, click and motion: archaeologies of fashion film after digital culture},
 url = {https://doi-org.crai.referencistas.com/10.1177/1470412920966015},
 volume = {19},
 year = {2020h}
}

@article{doi:10.1177/14705931241277159,
 author = {Stephen Dunne},
 doi = {10.1177/14705931241277159},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14705931241277159},
 journal = {Marketing Theory},
 number = {0},
 pages = {14705931241277160},
 title = {Book Review: Miles Coleman: Influential machines: The rhetoric of computational performance},
 url = {https://doi-org.crai.referencistas.com/10.1177/14705931241277159},
 volume = {0},
 year = {2024h}
}

@article{doi:10.1177/147366919903200203,
 doi = {10.1177/147366919903200203},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/147366919903200203},
 journal = {Reports of the International Commission on Radiation Units and Measurements},
 number = {2},
 pages = {1–5},
 title = {Tissue Substitutes, Phantoms and Computational Modelling in Medical Ultrasound: Part One-Background 1. Introduction},
 url = {https://doi-org.crai.referencistas.com/10.1177/147366919903200203},
 volume = {os-32},
 year = {2000t}
}

@article{doi:10.1177/1473871617752910,
 abstract = {Current web-based visualizations are designed for single computers and cannot make use of additional devices on the client side, even if today’s users often have access to several, such as a tablet, a smartphone, and a smartwatch. We present a framework for ad hoc computational clusters that leverage these local devices for visualization computations. Furthermore, we present an instantiating JavaScript toolkit called VisHive for constructing web-based visualization applications that can transparently connect multiple devices—called cells—into such ad hoc clusters—called a hive—for local computation. Hives are formed either using a matchmaking service or through manual configuration. Cells are organized into a master–slave architecture, where the master provides the visual interface to the user and controls the slaves and the slaves perform computation. VisHive is built entirely using current web technologies, runs in the native browser of each cell, and requires no specific software to be downloaded on the involved devices. We demonstrate VisHive using four distributed examples: a text analytics visualization, a database query for exploratory visualization, a density-based spatial clustering of applications with noise clustering running on multiple nodes, and a principal component analysis implementation.},
 author = {Zhe Cui and Shivalik Sen and Sriram Karthik Badam and Niklas Elmqvist},
 doi = {10.1177/1473871617752910},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1473871617752910},
 journal = {Information Visualization},
 number = {2},
 pages = {195–210},
 title = {VisHive: Supporting web-based visualization through ad hoc computational clusters of mobile devices},
 url = {https://doi-org.crai.referencistas.com/10.1177/1473871617752910},
 volume = {18},
 year = {2019d}
}

@article{doi:10.1177/14738716221137908,
 abstract = {This article presents a novel management and information visualization system proposal based on the tesseract, the 4D-hypercube. The concept comprises metaphors that mimic the tesseract geometrical properties using interaction and information visualization techniques, made possible by modern computer systems and human capabilities such as spatial cognition. The discussion compares the Hypercube and the traditional desktop metaphor systems. An operational prototype is also available for reader testing. Finally, a preliminary assessment with 31 participants revealed that 81.05% “agree” or “totally agree” that the proposed concepts offer real gains compared to the desktop metaphor.},
 author = {Alessandro Rego de Lima and Diana Carneiro Machado de Carvalho and Tânia de Jesus Vilela da Rocha},
 doi = {10.1177/14738716221137908},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14738716221137908},
 journal = {Information Visualization},
 number = {2},
 pages = {87–99},
 title = {HyperCube4x: A viewport management system proposal},
 url = {https://doi-org.crai.referencistas.com/10.1177/14738716221137908},
 volume = {22},
 year = {2023p}
}

@article{doi:10.1177/1474022220961750,
 abstract = {This essay reflects on the role of place for humanities practices and contributes to emerging discussions on infrastructure for the humanities and socio-material conditions of scholarly knowledge production. I provide a theoretical framework for studying venues for humanities work drawing on the phenomenological approach to the concepts of place and space, the pedagogical perspective on learning spaces in higher education, and epistemological studies of scientific places. Next, I analyse the landscape for the reconfiguration of humanities venues and present arguments for engaging with space by referring to the functioning of digital humanities. This essay shows that place is an extremely important resource, seeing as it is endowed with the power to drive new practices, institutionalize a community, and consolidate a discipline. Therefore, humanists should reflect critically on the ‘architecture of the humanities’ and engage in making their own spaces that determine practices, communication, and well-being.},
 author = {Urszula Pawlicka-Deger},
 doi = {10.1177/1474022220961750},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1474022220961750},
 journal = {Arts and Humanities in Higher Education},
 number = {3},
 pages = {320–338},
 title = {Place matters: Thinking about spaces for humanities practices},
 url = {https://doi-org.crai.referencistas.com/10.1177/1474022220961750},
 volume = {20},
 year = {2021l}
}

@article{doi:10.1177/1474474017704204,
 abstract = {The notion that the Earth has entered a new epoch characterized by the ubiquity of anthropogenic change presents the social sciences with something of a paradox, namely, that the point at which we recognize our species to be a geologic force is also the moment where our assumed metaphysical privilege becomes untenable. Cultural geography continues to navigate this paradox in conceptually innovative ways through its engagements with materialist philosophies, more-than-human thinking and experimental modes of ontological enquiry. Drawing upon the philosophy of Gilbert Simondon, this article contributes to these timely debates by articulating the paradox of the Anthropocene in relation to technological processes. Simondon’s philosophy precedes the identification of the Anthropocene epoch by a number of decades, yet his insistence upon situating technology within an immanent field of material processes resonates with contemporary geographical concerns in a number of important ways. More specifically, Simondon’s conceptual vocabulary provides a means of framing our entanglements with technological processes without assuming a metaphysical distinction between human beings and the forces of nature. In this article, I show how Simondon’s concepts of individuation and transduction intersect with this technological problematic through his far-reaching critique of the ‘hylomorphic’ distinction between matter and form. Inspired by Simondon’s original account of the genesis of a clay brick, the article unfolds these conceptual challenges through two contrasting empirical encounters with 3D printing technologies. In doing so, my intention is to lend an affective consistency to Simondon’s problematic, and to do so in a way that captures the kinds of material mutations expressive of a particular technological moment.},
 author = {Tom Roberts},
 doi = {10.1177/1474474017704204},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1474474017704204},
 journal = {cultural geographies},
 number = {4},
 pages = {539–554},
 title = {Thinking technology for the Anthropocene: encountering 3D printing through the philosophy of Gilbert Simondon},
 url = {https://doi-org.crai.referencistas.com/10.1177/1474474017704204},
 volume = {24},
 year = {2017o}
}

@article{doi:10.1177/147470490300100114,
 author = {Earl Hunt},
 doi = {10.1177/147470490300100114},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/147470490300100114},
 journal = {Evolutionary Psychology},
 number = {1},
 pages = {147470490300100130},
 title = {Book Review: Adaptive Thinking: Rationality in the Real World},
 url = {https://doi-org.crai.referencistas.com/10.1177/147470490300100114},
 volume = {1},
 year = {2003j}
}

@article{doi:10.1177/147470490800600413,
 abstract = {In this paper I explore the possibility that recursion is not part of the cognitive repertoire of non-human primates such as chimpanzees due to limited working memory capacity. Multiple lines of data, from nut cracking to the velocity and duration of cognitive development, imply that chimpanzees have a short-term memory size that limits working memory to dealing with two, or at most three, concepts at a time. If so, as a species they lack the cognitive capacity for recursive thinking to be integrated into systems of social organization and communication. If this limited working memory capacity is projected back to a common ancestor for Pan and Homo, it follows that early hominid ancestors would have had limited working memory capacity. Hence we should find evidence for expansion of working memory capacity during hominid evolution reflected in changes in the products of conceptually framed activities such as stone tool production. Data on the artifacts made by our hominid ancestors support this expansion hypothesis for hominid working memory, thereby leading to qualitative differences between Pan and Homo.},
 author = {Dwight W. Read},
 doi = {10.1177/147470490800600413},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/147470490800600413},
 journal = {Evolutionary Psychology},
 number = {4},
 pages = {147470490800600420},
 title = {Working Memory: A Cognitive Limit to Non-Human Primate Recursive Thinking Prior to Hominid Evolution},
 url = {https://doi-org.crai.referencistas.com/10.1177/147470490800600413},
 volume = {6},
 year = {2008r}
}

@article{doi:10.1177/147470491301100512,
 author = {Austin John Jeffery and Todd K. Shackelford},
 doi = {10.1177/147470491301100512},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/147470491301100512},
 journal = {Evolutionary Psychology},
 number = {5},
 pages = {1077–1083},
 title = {Book Review: Pumping Dust},
 url = {https://doi-org.crai.referencistas.com/10.1177/147470491301100512},
 volume = {11},
 year = {2013g}
}

@article{doi:10.1177/14749041211007496,
 abstract = {Drawing upon the growing datafication of contemporary schooling, our purpose in this article is to use topological thinking as an analytical device to better understand the professionals and practices within emergent data infrastructures. We address this by attending to an influential national (and subnational) data infrastructure of school monitoring in the United States, managed by the federal Department of Education, known as EDFacts. Informed by policy documents relating to EDFacts, as well as by various related software platforms and portals, we explore the whom and how of datafication, and expose the increasing presence and influence of otherwise ‘hidden’ technology mediators, or ‘shadow professionals’. In particular, we argue that the increasing dependency of EDFacts on data has necessitated the introduction of new professional roles associated with optimising the flow of data, and thus stabilising and normalising the topological space of the infrastructure. We conclude by suggesting that EDFacts encourages teaching professionals and shadow professionals alike to engage in acts of data submission; that is, providing data to EDFacts and, at the same time, positioning themselves as wholly responsive to the infrastructure and its datafied renderings of schooling.},
 author = {Steven Lewis and Sigrid Hartong},
 doi = {10.1177/14749041211007496},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14749041211007496},
 journal = {European Educational Research Journal},
 number = {6},
 pages = {946–960},
 title = {New shadow professionals and infrastructures around the datafied school: Topological thinking as an analytical device},
 url = {https://doi-org.crai.referencistas.com/10.1177/14749041211007496},
 volume = {21},
 year = {2022k}
}

@article{doi:10.1177/14749041221121477,
 abstract = {Narratives on innovation in education are spreading fast and both national and local educational administrations have been recently promoting innovation policies and programmes in many different European contexts. Academic literature analysing the potential benefits of innovation in education has expanded accordingly, with some international organizations increasingly commissioning research aiming to study the impacts of educational innovation, especially on learning outcomes. Interestingly, less attention has been given to analysing how these policies and programmes are translated into different practices at the school level. Drawing on a policy enactment framework, this paper aims to analyse the ways in which schools interpret top-down policy text and prescription on innovation and enact innovation in education. To do so, we focus on the case of Xarxes per al Canvi (XC), an educational innovation programme launched by the educational administration of the city of Barcelona in 2017 that aims to create school networks in order to stimulate knowledge sharing and innovation. Findings show how schools make sense of the innovation policy diversely. Policy enactment outcomes appear to be context-sensitive, with schools enacting its precepts in different ways, especially to serve their needs in increasingly competitive local education markets.},
 author = {Edgar Quilabert and Mauro C Moschetti},
 doi = {10.1177/14749041221121477},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14749041221121477},
 journal = {European Educational Research Journal},
 number = {1},
 pages = {87–107},
 title = {‘Most Likely You Go Your Way (and I’ll Go Mine)’: School-level enactment of an educational innovation policy in Barcelona},
 url = {https://doi-org.crai.referencistas.com/10.1177/14749041221121477},
 volume = {23},
 year = {2024o}
}

@article{doi:10.1177/147509020421800103,
 abstract = {This paper has investigated the risk of atmospheric exposure during oil spills. Air levels of volatile components (VCs) arising from oil spills under some hypothetical scenarios have been studied using a computational model. The computational results indicate that overexposure to benzene may exist under general conditions. A table summarizing the exposure risks under various conditions is presented which may be useful in exposure risk analysis during oil spill response. The exposure to other volatile hydrocarbon components is negligible. Exposure to sulphur components occurs, but the duration is very short.},
 author = {T Zhou and K V Wong},
 doi = {10.1177/147509020421800103},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/147509020421800103},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part M: Journal of Engineering for the Maritime Environment},
 number = {1},
 pages = {23–30},
 title = {Studying atmospheric exposure risks during oil spills using a localized computational model},
 url = {https://doi-org.crai.referencistas.com/10.1177/147509020421800103},
 volume = {218},
 year = {2004t}
}

@article{doi:10.1177/1475090215626462,
 abstract = {This article describes the evaluation of the wave profile of submarine at surface condition and deck flooding which occurred by the wave making pattern at the bow. Movement of ships and submarines on the free surface of calm water creates the surface wave. Because of the difference in the bow shape and freeboard height, the wave making system in ships and submarines is different. Rounded or elliptical bow shape of submarines generates a high bow wave which causes deck and bow wetness. This is because of the fact that in submarines, this situation arises a small freeboard. In submarines, Deck wetness (because of deck flooding) is a very important subject that has some remarkable consequences, such as increase in resistance and added weigh. The focus of this article is on the added frictional resistance in the deck wetness condition. The bow wave profile, deck wetness and added resistance are studied in several Froude numbers by computational fluid dynamics method. This analysis is performed for a bare hull model at two different drafts by Flow Vision (V.2.3) software based on computational fluid dynamics method and solving the Reynolds-averaged Navier–Stokes equations.},
 author = {Mohammad Moonesun and Mehran Javadi and Seyyed Hossein Mousavizadegan and Hosein Dalayeli and Yuri Mikhailovich Korol and Ataollah Gharachahi},
 doi = {10.1177/1475090215626462},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1475090215626462},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part M: Journal of Engineering for the Maritime Environment},
 number = {1},
 pages = {128–136},
 title = {Computational fluid dynamics analysis on the added resistance of submarine due to Deck wetness at surface condition},
 url = {https://doi-org.crai.referencistas.com/10.1177/1475090215626462},
 volume = {231},
 year = {2017o}
}

@article{doi:10.1177/1475090217734685,
 abstract = {Understanding the manoeuvring performance of a ship requires accurate predictions of the hydrodynamic forces and moments on the ship. In the present study, the hydrodynamic forces and moments on a manoeuvring container ship at various rudder and drift angles are numerically predicted by solving the unsteady Reynolds-averaged Navier–Stokes equations. The effects of dynamic sinkage and trim on the hydrodynamic forces are first investigated together with a grid dependency study to estimate the numerical error and uncertainty caused by grid discretization, and with a validation study combining the experimental data. The results show that the effect of dynamic sinkage and trim is non-negligible, since including it improves the hydrodynamic force predictions and reduces the numerical error and uncertainty, and the averaged error and uncertainty are smaller than the other computational fluid dynamics results where sinkage and trim were fixed with given values from model tests. Therefore, it is included in the subsequent systematic simulations regarding the influence of rudder and drift angles. The computed forces, moments and rudder coefficients at different rudder and drift angles on the container ship are compared with the benchmark model test data. From the computations, all the predicted quantities are in satisfactory agreement with the experimental data. The details of the flow filed and hydrodynamic forces, such as pressure distributions, transverse force distributions along the hull, velocity contours, streamlines and wave patterns are presented and discussed, and a deep insight into the physical mechanism in the hydrodynamic forces on a manoeuvring ship is obtained.},
 author = {Yi Liu and Lu Zou and Zao-Jian Zou},
 doi = {10.1177/1475090217734685},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1475090217734685},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part M: Journal of Engineering for the Maritime Environment},
 number = {1},
 pages = {251–266},
 title = {Computational fluid dynamics prediction of hydrodynamic forces on a manoeuvring ship including effects of dynamic sinkage and trim},
 url = {https://doi-org.crai.referencistas.com/10.1177/1475090217734685},
 volume = {233},
 year = {2019l}
}

@article{doi:10.1177/1475090218763199,
 abstract = {Determining and understanding the performance characteristics of marine propellers by experiments is quite a complex and costly task. Numerical predictions using computational fluid dynamics simulations could be a valuable alternative provided that the laminar-to-turbulent transition flow effects are fundamentally understood with the suitable numerical models developed. Experience suggests that the use of classical turbulent flow models may lead to high discrepancies especially at low rotational speeds where the effects of fluid flow transition from the laminar to the turbulent state may influence the predicted propeller’s performance. This article proposes a complete and detailed procedure for the computational fluid dynamics simulation of non-cavitating flow over marine propellers using the “k–kl–ω” transition-sensitive turbulence model. Results are evaluated by “ANSYS FLUENT 16” for the “INSEAN E779A” propeller. Comparisons against the fully turbulent standard “k–ε” model and against experiments show improved agreement in way of flow transition zones at lower rotational speeds, that is, at low Reynolds numbers.},
 author = {Mohamed M Helal and Tamer M Ahmed and Adel A Banawan and Mohamed A Kotb},
 doi = {10.1177/1475090218763199},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1475090218763199},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part M: Journal of Engineering for the Maritime Environment},
 number = {2},
 pages = {515–527},
 title = {Numerical prediction of the performance of marine propellers using computational fluid dynamics simulation with transition-sensitive turbulence model},
 url = {https://doi-org.crai.referencistas.com/10.1177/1475090218763199},
 volume = {233},
 year = {2019g}
}

@article{doi:10.1177/1475090218776143,
 abstract = {Most seabeds are unexplored and rich in mineral deposits, making offshore mining a promising activity. However, offshore operation brings in great challenges from technical equipment to physical space. For instance, an offshore agitated vessel is supposed to stabilize the solids concentration from the underwater mining and make little impact on the stability of the platform or ship. For this reason, we proposed a novel offshore agitated vessel. The whole system based on the arrangement of the mineral processing platform and the slurry mix flow rate is obtained from the previous design stage. Large-scale unsteady computational fluid dynamics simulations are performed to calculate its effectiveness. The simulation model equipped with two pitched blade turbines and inlets/outlets is investigated. A classical Eulerian multiphase model and a modification of the standard k-ε eddy-viscosity turbulence model are adopted to simulate the dense solid–liquid suspension dynamics. Computational fluid dynamics results were found to be in satisfactory agreement with the theoretical predictions. The agitated system obtained was found to be effective to stabilize the solid particle concentration. In order to achieve a higher concentration at outlets and lower power consumption, further improvement was made and validated by computational fluid dynamics simulations. The proposed offshore mechanical agitated vessel could be equipped on offshore mining.},
 author = {D Yang and XQ Lv and YL Xiong},
 doi = {10.1177/1475090218776143},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1475090218776143},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part M: Journal of Engineering for the Maritime Environment},
 number = {2},
 pages = {622–631},
 title = {A computational fluid dynamics study on the solid mineral particles-laden flow in a novel offshore agitated vessel},
 url = {https://doi-org.crai.referencistas.com/10.1177/1475090218776143},
 volume = {233},
 year = {2019t}
}

@article{doi:10.1177/1475090219892368,
 abstract = {In this article, noise spectrum of marine propellers is investigated in uniform flow under non-cavitating and cavitating conditions. New results are presented for this research field. Hydrodynamic performance of both non-cavitating and cavitating marine propellers is first analyzed by viscous and potential based flow solvers. In viscous solver, sheet cavitation on propeller blades is simulated with Schnerr–Sauer cavitation model based on Rayleigh Plesset equation using volume of fluid approach. Numerical hydrodynamic results based on viscous solver is compared with potential solver and then validated with experimental data of benchmark David Taylor Model Basin 4119 model propeller. Later, noise spectrum of model propellers is predicted by a hybrid method which combines Reynolds-averaged Navier Stokes and Ffowcs Williams Hawkings equations. Computed noise spectrum is compared with other numerical studies in the literature for the selected model propeller. In addition, hydrodynamic and hydroacoustic pressures are compared in near field to show reliability of numerical solution. Effects of blade number on hydrodynamic performance and noise spectrum are also investigated. Numerical results indicated that as blade number increases, propeller noise level decreases for different loading conditions due to decreased blade loading (circulation) per blade. However, propeller efficiency increases as blade number decreases.},
 author = {Savas Sezen and Sakir Bal},
 doi = {10.1177/1475090219892368},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1475090219892368},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part M: Journal of Engineering for the Maritime Environment},
 number = {2},
 pages = {374–387},
 title = {A computational investigation of noise spectrum due to cavitating and non-cavitating propellers},
 url = {https://doi-org.crai.referencistas.com/10.1177/1475090219892368},
 volume = {234},
 year = {2020q}
}

@article{doi:10.1177/14750902221127755,
 abstract = {All over the world, the oil and gas industries are the most important source of energy. The likelihood of subsea gas and oil leakage is increasing, which can cause threats and harmful impacts on the marine environment and potentially catastrophic events such as fires, explosives, and the loss of structural integrity of subsea infrastructure. Also, Physical models have usually been used to predict sea currents, but they are unstable to disturbances and hence incorrect over long periods of time. Machine learning approaches are more resistant than the physical models that have usually been used to predict sea currents. Also, Physical models have usually been used to predict sea currents. Nonetheless, they are not stable to disturbances and thus are not correct for long periods of time. Machine learning approaches are more resistant than the physical models that have usually been used to predict sea currents. Machine learning approaches are more resistant to change and perturbation. Therefore, the goal of this research is to assess the potential of hazards of the gas plume from subsea pipeline rupture till reaching the sea surface by changing the influence parameters on gas plume to assist petroleum companies in developing risk assessment strategies by assessing and simulating subsea gas release in order to contain the leakage by developing coupling models one for machine learning code which can predict the upcoming water current speed by using Multiple Linear Regression algorithm and hooked it by UDF to a second model which implements Computational Fluid Dynamics (CFD) model to study subsea gas release under current effects. Engebretsen’s Rotvoll experiment data is being used to validate the numerical computational fluid dynamics model. The rising time and fountain height and horizontal migration for gas release are the essential factors to be considered while evaluating the gas dispersion through our study by changing the influencing parameters such as leakage hole sizes, water current speeds, gas velocity, and water depths in the presence of water current in all cases. Also, applied our simulation to real case parameters for one of the Egyptian Petroleum Companies. These findings might aid in evaluating the hazards and response planning in the event of subsea gas leakage.},
 author = {Ahmed M Ellethy and Ahmed S Shehata and Ali I Shehata and Ahmed Mehanna},
 doi = {10.1177/14750902221127755},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14750902221127755},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part M: Journal of Engineering for the Maritime Environment},
 number = {3},
 pages = {764–787},
 title = {Modeling and assessment of accidental subsea gas leakage using a coupled computational fluid dynamics and machine learning approaches},
 url = {https://doi-org.crai.referencistas.com/10.1177/14750902221127755},
 volume = {237},
 year = {2023e}
}

@article{doi:10.1177/14750902221149308,
 abstract = {In order to perform increasingly complex underwater tasks, we have designed a hydraulic underwater non-spherical wrist manipulator with a binocular vision system and a six-dimensional force sensor mounted behind the end-effector, which can improve the path accuracy and automation capabilities of the system. The system lays a foundation for the high precision and autonomous deep-sea operation and can improve its intelligence level. In this paper, we focus on solving the singularity avoidance problem of the hydraulic underwater manipulator for accurate control, and propose a computationally efficient singularity avoidance method to improve the path accuracy of the manipulator with the non-spherical wrist. Firstly, the singular configurations of the manipulator are analyzed. Secondly, the Jacobian is decomposed into sub-block matrices to obtain the singular factors according to the singular configurations. Thirdly, the singularities are avoided by the approximate damped reciprocal method. Theoretical analysis shows that the computation costs of the proposed method are only one-third to one-half of that of the traditional methods. The simulation results prove that the proposed method can largely improve the path accuracy of the manipulator with less computation costs, which shows that our method would have a certain significance in improving the precision and real-time response of deep-sea operations.},
 author = {Tiantian Yu and Lifu Gao and Daqing Wang and Weibin Guo and Yue Zhang},
 doi = {10.1177/14750902221149308},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14750902221149308},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part M: Journal of Engineering for the Maritime Environment},
 number = {3},
 pages = {615–624},
 title = {A computation effective singularity avoidance method for the underwater manipulator with a non-spherical wrist},
 url = {https://doi-org.crai.referencistas.com/10.1177/14750902221149308},
 volume = {237},
 year = {2023s}
}

@article{doi:10.1177/1475472X16680447,
 abstract = {The self-noise of a controlled-diffusion airfoil is computed with several numerical techniques based on the acoustic analogy and involving different degrees of approximation. The flow solution is obtained through an incompressible large eddy simulation. The acoustic field as described by Lighthill’s analogy is computed with a finite element method applied to the exact airfoil geometry, and this solution is compared with results based on a half-plane Green’s function. This problem behaves as a classical trailing-edge noise problem for a wide range of frequencies; however, other mechanisms of sound production become significant at high frequencies. The results highlight the relative strengths and weaknesses of quadrupole- and dipole-based formulations of the acoustic analogy based on incompressible Computational Fluid Dynamics (CFD) results when applied to wall-bounded turbulent flows.},
 author = {P Martínez-Lera and J Christophe and C Schram},
 doi = {10.1177/1475472X16680447},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1475472X16680447},
 journal = {International Journal of Aeroacoustics},
 number = {1–2},
 pages = {44–64},
 title = {Computation of the self-noise of a controlled-diffusion airfoil based on the acoustic analogy},
 url = {https://doi-org.crai.referencistas.com/10.1177/1475472X16680447},
 volume = {16},
 year = {2017j}
}

@article{doi:10.1177/1475472X17718724,
 abstract = {An 18% scale semispan model is used as a platform for examining the efficacy of microphone array processing using synthetic data from numerical simulations. Two hybrid Reynolds-Averaged-Navier-Stokes/Large-Eddy-Simulation (RANS/LES) codes coupled with Ffowcs Williams–Hawkings solvers are used to calculate 97 microphone signals at the locations of an array employed in the NASA Langley Research Center 14 × 22 tunnel. Conventional, DAMAS, and CLEAN-SC array processing is applied in an identical fashion to the experimental and computational results for three different configurations involving deploying and retracting the main landing gear and a part-span flap. Despite the short time records of the numerical signals, the beamform maps are able to isolate the noise sources, and the appearance of the DAMAS synthetic array maps is generally better than those from the experimental data. The experimental CLEAN-SC maps are similar in quality to those from the simulations indicating that CLEAN-SC may have less sensitivity to background noise. The spectrum obtained from DAMAS processing of synthetic array data is nearly identical to the spectrum of the center microphone of the array, indicating that for this problem array processing of synthetic data does not improve spectral comparisons with experiment. However, the beamform maps do provide an additional means of comparison that can reveal differences that cannot be ascertained from spectra alone.},
 author = {David P Lockard and William M Humphreys and Mehdi R Khorrami and Ehab Fares and Damiano Casalino and Patricio A Ravetta},
 doi = {10.1177/1475472X17718724},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1475472X17718724},
 journal = {International Journal of Aeroacoustics},
 number = {4–5},
 pages = {358–381},
 title = {Comparison of computational and experimental microphone array results for an 18% scale aircraft model},
 url = {https://doi-org.crai.referencistas.com/10.1177/1475472X17718724},
 volume = {16},
 year = {2017m}
}

@article{doi:10.1177/1475472X19834522,
 author = {KBMQ Zaman and I Milanovic and AF Fagan and CJ Miller},
 doi = {10.1177/1475472X19834522},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1475472X19834522},
 journal = {International Journal of Aeroacoustics},
 number = {2–3},
 pages = {189–206},
 title = {Experimental and computational study of tones occurring with a coaxial nozzle},
 url = {https://doi-org.crai.referencistas.com/10.1177/1475472X19834522},
 volume = {18},
 year = {2019t}
}

@article{doi:10.1177/1475472X221079557,
 abstract = {Broadband noise due to the turbulence-aerofoil interaction, which is also called the leading edge noise, is one of the major noise sources of aircraft (including the engine). To study the noise properties numerically is a popular approach with the increasing power of computers. Conventional approaches of using body-fitted grids at the boundaries would be convoluted due to the complex geometries, which can constrain the efficiency of parametric studies. A promising approach to tackle this issue is to use the immersed boundary method (IBM). Among various IBM variants, the volume penalization (VP) approach employs a masking function to identify the immersed solid boundary, and continuous forcing terms are added to the original flow governing equations to account for the boundary conditions. It is, therefore, efficient and easy to implement into the existing computational aeroacoustics solvers. In this work, the VP-based IBM is used to simulate the leading edge noise by combining with the advanced synthetic turbulence method. The simulations are conducted for both the isolated aerofoils and cascade, and the results are compared with the well-validated body-fitted grid solutions. The viscosity effect is also highlighted by comparing the results obtained by solving both Euler and Navier–Stokes equations.},
 author = {Wei Ying and Ryu Fattah and Sinforiano Cantos and Siyang Zhong and Tatiana Kozubskaya},
 doi = {10.1177/1475472X221079557},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1475472X221079557},
 journal = {International Journal of Aeroacoustics},
 number = {1–2},
 pages = {74–94},
 title = {Computational aeroacoustics of aerofoil leading edge noise using the volume penalization-based immersed boundary methods},
 url = {https://doi-org.crai.referencistas.com/10.1177/1475472X221079557},
 volume = {21},
 year = {2022s}
}

@article{doi:10.1177/1475725716659252,
 abstract = {Computational thinking is an approach to problem solving that is typically employed by computer programmers. The advantage of this approach is that solutions can be generated through algorithms that can be implemented as computer code. Although computational thinking has historically been a skill that is exclusively taught within computer science, there has been a more recent movement to introduce these skills within other disciplines. Psychology is an excellent example of a discipline that would benefit from computational thinking skills because of the nature of questions that are typically asked within the discipline. However, there has not been a formal curriculum proposed to teach computational thinking within psychology and the behavioural sciences. I will argue that computational thinking is a fundamental skill that can easily be introduced to psychology students throughout their undergraduate education. This would provide students with the skills necessary to become successful researchers, and would also provide a practical and marketable skill to all psychology graduates.},
 author = {Nicole D. Anderson},
 doi = {10.1177/1475725716659252},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1475725716659252},
 journal = {Psychology Learning & Teaching},
 number = {3},
 pages = {226–234},
 title = {A Call for Computational Thinking in Undergraduate Psychology},
 url = {https://doi-org.crai.referencistas.com/10.1177/1475725716659252},
 volume = {15},
 year = {2016a}
}

@article{doi:10.1177/1475725716673002,
 doi = {10.1177/1475725716673002},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1475725716673002},
 journal = {Psychology Learning & Teaching},
 number = {3},
 pages = {386–426},
 title = {Abstracts of the 5th Vancouver International Conference on the Teaching of Psychology},
 url = {https://doi-org.crai.referencistas.com/10.1177/1475725716673002},
 volume = {15},
 year = {2016t}
}

@article{doi:10.1177/1475725716673004,
 author = {Steve Charlton and Karen Ryder and Jacqui Taylor},
 doi = {10.1177/1475725716673004},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1475725716673004},
 journal = {Psychology Learning & Teaching},
 number = {3},
 pages = {211–213},
 title = {Editorial},
 url = {https://doi-org.crai.referencistas.com/10.1177/1475725716673004},
 volume = {15},
 year = {2016b}
}

@article{doi:10.1177/1475921707081975,
 abstract = {Corrosion greatly affects the integrity of many engineering structures, such as bridges, pipelines, nuclear reactors, and aircraft. This study provides an overview of the computational intelligence methods developed for the corrosion damage assessment of aerospace materials and structures. Specifically, cellular automata modeling of corrosion pit initiation and growth, wavelet based image processing methods for corrosion damage assessment, and artificial neural networks (ANNs) for material loss and residual strength predictions. In addition, ANN based prediction of life due to corrosion-fatigue conditions are considered and presented. Results obtained from selected computational intelligence methods are compared to the existing alternate solutions and experimental data. The results presented illustrate the feasibility of computational intelligence methods for modeling and assessing the corrosion health of aging aircraft structures and materials.},
 author = {Ramana M. Pidaparti},
 doi = {10.1177/1475921707081975},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1475921707081975},
 journal = {Structural Health Monitoring},
 number = {3},
 pages = {245–259},
 title = {Structural Corrosion Health Assessment using Computational Intelligence Methods},
 url = {https://doi-org.crai.referencistas.com/10.1177/1475921707081975},
 volume = {6},
 year = {2007p}
}

@article{doi:10.1177/1475921713493344,
 abstract = {The integration of structural health monitoring into life-cycle management strategies can help facilitating a reliable operation of wind turbines and reducing the life-cycle costs significantly. This article presents a life-cycle management framework for online monitoring and performance assessment of wind turbines, enabling optimum maintenance and inspection planning at minimum associated life-cycle costs. Incorporating continuously updated monitoring data (i.e. structural, environmental, and operational data), the framework allows capturing and understanding the actual wind turbine condition and, hence, reduces uncertainty in structural responses as well as load effects acting on the structure. As will be shown in this article, the framework integrates a variety of heterogeneous hardware and software components, including sensors and data acquisition units, server systems, Internet-enabled user interfaces as well as finite element models for system identification, and a multiagent system for self-detecting sensor malfunctions. To validate its capabilities and to demonstrate its practicability, the framework is deployed for continuous monitoring and life-cycle management of a 500-kW wind turbine. Remote life-cycle analyses of the monitored wind turbine are conducted, and case studies are presented investigating both the structural performance and the operational efficiency of the wind turbine.},
 author = {Kay Smarsly and Dietrich Hartmann and Kincho H Law},
 doi = {10.1177/1475921713493344},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1475921713493344},
 journal = {Structural Health Monitoring},
 number = {4},
 pages = {359–376},
 title = {A computational framework for life-cycle management of wind turbines incorporating structural health monitoring},
 url = {https://doi-org.crai.referencistas.com/10.1177/1475921713493344},
 volume = {12},
 year = {2013p}
}

@article{doi:10.1177/1475921717691018,
 abstract = {This article proposes a Rayleigh’s quotient–based damage detection algorithm. It aims at efficiently revealing nascent structural changes on a given structure with the capability to differentiate between an actual damage and a change in operational conditions. The first three damage detection levels are targeted: existence, location, and severity. The proposed algorithm is analytically developed from the dynamics theory and the virtual energy principle. Some computational techniques are proposed for carrying out computations, including discretization, integration, derivation, and suitable optimization methods. Field implementation strategies are also considered for the purpose of online damage monitoring. In order to prove the efficiency of this strategy, one experimental and three numerical case studies were conducted. The proposed algorithm successfully detected the damage in all simulated cases and estimated the damage severity with acceptable accuracy. The conclusion is that the proposed algorithm was able to efficiently detect damage appearance in a range of structures for various damage levels and locations, and under different operational conditions.},
 author = {Wilfried Njomo Wandji},
 doi = {10.1177/1475921717691018},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1475921717691018},
 journal = {Structural Health Monitoring},
 number = {2},
 pages = {285–297},
 title = {Rayleigh’s quotient–based damage detection algorithm: Theoretical concepts, computational techniques, and field implementation strategies},
 url = {https://doi-org.crai.referencistas.com/10.1177/1475921717691018},
 volume = {17},
 year = {2018l}
}

@article{doi:10.1177/1476718X19860557,
 abstract = {Research on early childhood robotics education often focuses narrowly on teaching young children STEM (science, technology, engineering, and mathematics) concepts and skills. In this qualitative case study, our research team examined what happened when we worked with young children (age 7) and combined the technologies of robotics education with an inquiry approach, that is, an opportunity for students to collaboratively identify a problem arising from their own lived experiences and build a robot to solve it. We found that the process of children’s problem identification was dialogic, not only with peers and teachers but also with materials, as they defined and refined problems based on interactions with peers and objects. As this study was conducted at an economically disadvantaged public school in the Southern United States, we argue that early childhood robotics education has a great potential to engage young children in STEM learning in a personally meaningful manner and that an instructional approach fostering children’s inquiry and project-based learning through their problem finding and problem posing is effective in making STEM accessible to students from diverse backgrounds.},
 author = {Shara Cherniak and Kyunghwa Lee and Eunji Cho and Sung Eun Jung},
 doi = {10.1177/1476718X19860557},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1476718X19860557},
 journal = {Journal of Early Childhood Research},
 number = {4},
 pages = {347–360},
 title = {Child-identified problems and their robotic solutions},
 url = {https://doi-org.crai.referencistas.com/10.1177/1476718X19860557},
 volume = {17},
 year = {2019f}
}

@article{doi:10.1177/1476718X231175464,
 abstract = {Computational thinking (CT) has emerged as an important method in the United States for helping children learn to solve complex problems and develop skills necessary for coding and other computer science-related endeavors. Research has revealed that CT can be encouraged with children as young as 3–4 years of age. While some preschools and schools are incorporating CT into their curriculum for young children, ages 0–8 years, it is important to understand how environments outside of schools are using CT with young children, particularly given that, in the United States, a large percent of young children, ages 0–5 years, are not in formal school settings. This study provides insight into this area through 20 interviews with educators in libraries and museums to understand how they incorporate CT into their work with young children, ages 0–8 years, and their families. The interviews reveal that library and museum educators are using a variety of developmentally-appropriate approaches, such as play, experimentation, and narrative, to design and offer a diverse array of engaging, hands-on CT activities that allow young children to practice CT in child-centered, meaningful ways.},
 author = {Kathleen Campana and J Elizabeth Mills},
 doi = {10.1177/1476718X231175464},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1476718X231175464},
 journal = {Journal of Early Childhood Research},
 number = {3},
 pages = {369–383},
 title = {Playing, tinkering, and problem solving: Understanding early computational thinking in libraries and museums},
 url = {https://doi-org.crai.referencistas.com/10.1177/1476718X231175464},
 volume = {21},
 year = {2023c}
}

@article{doi:10.1177/1477878520981303,
 abstract = {It is widely recognised among educational theorists, educators and policy makers alike, that critical thinking should claim a superordinate place in our system of educational objectives. In the philosophical literature on this topic, critical thinking is often conceptualised as the educational cognate of rationality, which in turn is analysed as being comprised of the relevant skills and abilities to assess reasons and evidence, together with the intellectual dispositions to actively use these proficiencies in practice. The resulting picture is in many respects normative and idealised, following the style of philosophical theorising commonplace in the tradition of analytic philosophy of education. In contrast, certain recent empirical findings related to the rational performance of actual human beings seem to cast doubts on the extent to which we can expect people to fulfil these idealised normative standards of rationality. After introducing the relevant philosophical theories and psychological results, I ruminate on the implications these ideas have on our pedagogical views pertaining to critical thinking education.},
 author = {Henri Pettersson},
 doi = {10.1177/1477878520981303},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1477878520981303},
 journal = {Theory and Research in Education},
 number = {3},
 pages = {322–338},
 title = {De-idealising the educational ideal of critical thinking},
 url = {https://doi-org.crai.referencistas.com/10.1177/1477878520981303},
 volume = {18},
 year = {2020p}
}

@article{doi:10.1177/1478077115625516,
 abstract = {Since the publication in 1948 of Norbert Wiener’s Cybernetics, this thought model has exerted a profound influence in contemporary knowledge. Such influence has been decisive for a paradigm shift in the profession of architecture and particularly for the rise of a computational perspective in architectural design. This article explores the link between the cybernetic paradigm and the conception of architectural objects as performative, responsive, intelligent, and sentient artifacts—the visions of buildings that have been central to the development of digital architecture since its early stages. This connection shows that the dominant visions of design problems associated with the development of a computational perspective in architecture have not been exclusively the result of the introduction of computer pragmatics in architectural design. On the contrary, following such scholars as Bruno Latour and Katherine Hayles, these developments must be considered as the result of a particular feedback process that includes technical aspects as well as the definition of design problems around an informational ontology and epistemology. The understanding of the intellectual foundations of digital architecture is crucial not only to promote a critical regard of its productions but to imagine scenarios for a viable cybernetic practice of computer-mediated architectural design.},
 author = {Camilo Andrés Cifuentes Quin},
 doi = {10.1177/1478077115625516},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1478077115625516},
 journal = {International Journal of Architectural Computing},
 number = {1},
 pages = {16–29},
 title = {The cybernetic imagination of computational architecture},
 url = {https://doi-org.crai.referencistas.com/10.1177/1478077115625516},
 volume = {14},
 year = {2016p}
}

@article{doi:10.1177/1478077116638924,
 abstract = {In the current computer-aided architectural design education, students do not necessarily need to be taught to use more digital tools, but need to be introduced to the possibilities of designing their own digital tools. Designing the original tools with the assistance of educational-use environment for computational fluid dynamics programming improves the capability of the students to estimate the flows around architectures based on the mathematical background and the actual program. The authors have developed Educational Library for Fluid as an educational-use environment for computational fluid dynamics programming and ran the workshop where the students majoring in computer-aided architectural design tried to design computational fluid dynamics tools with Educational Library for Fluid. In this article, the details of Educational Library for Fluid and the results of the workshop are being reported. In addition, the key points of computational fluid dynamics education that the authors learned through the experience of the workshop are shared.},
 author = {Akito Nakano and John N Bohn and Akira Wakita},
 doi = {10.1177/1478077116638924},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1478077116638924},
 journal = {International Journal of Architectural Computing},
 number = {2},
 pages = {119–130},
 title = {Development of educational-use computational fluid dynamics programming environment and workshop},
 url = {https://doi-org.crai.referencistas.com/10.1177/1478077116638924},
 volume = {14},
 year = {2016o}
}

@article{doi:10.1177/1478077116638945,
 abstract = {Generative processes and generative design approaches are topics of continuing interest and debate within the realms of architectural design and related fields. While they are often held up as giving designers the opportunity (the freedom) to explore far greater numbers of options/alternatives than would otherwise be possible, questions also arise regarding the limitations of such approaches on the design spaces explored, in comparison with more conventional, human-centric design processes. This article addresses the controversy with a specific focus on parametric-associative modelling and genetic programming methods of generative design. These represent two established contenders within the pool of procedural design approaches gaining increasingly wide acceptance in architectural computational research, education and practice. The two methods are compared and contrasted to highlight important differences in freedoms and limitations they afford, with respect to each other and to ‘manual’ design. We conclude that these methods may be combined with an appropriate balance of automation and human intervention to obtain ‘optimal’ design freedom, and we suggest steps towards finding that balance.},
 author = {Andre Chaszar and Sam Conrad Joyce},
 doi = {10.1177/1478077116638945},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1478077116638945},
 journal = {International Journal of Architectural Computing},
 number = {2},
 pages = {167–181},
 title = {Generating freedom: Questions of flexibility in digital design and architectural computation},
 url = {https://doi-org.crai.referencistas.com/10.1177/1478077116638945},
 volume = {14},
 year = {2016c}
}

@article{doi:10.1177/1478077116663351,
 abstract = {This article presents the results from exploring the impact of using a parametric design tool on designers’ behavior in terms of using design patterns in the early conceptual development stage of designing. It is based on an empirical cognitive study in which eight architectural designers were asked to complete two architectural design tasks with similar complexity, respectively, in a parametric design environment and a geometric modeling environment. The protocol analysis method was employed to study the designers’ behavior. In order to explore the development of design patterns in the empirical data, Markov model analysis is utilized. Through Markov models analysis of the parametric design environment and geometric modeling environment results, it was found that there are some significantly different design patterns being used when designing in a parametric design environment compared to designing in a geometric modeling environment. The article articulates these differences and draws conclusions from these results.},
 author = {Rongrong Yu and John S Gero},
 doi = {10.1177/1478077116663351},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1478077116663351},
 journal = {International Journal of Architectural Computing},
 number = {3},
 pages = {289–302},
 title = {An empirical basis for the use of design patterns by architects in parametric design},
 url = {https://doi-org.crai.referencistas.com/10.1177/1478077116663351},
 volume = {14},
 year = {2016s}
}

@article{doi:10.1177/1478077117734660,
 abstract = {Parquet Deformation is an architectural studio exercise introduced by William Huff in 1960s. It aims to improve students’ reasoning of spatiotemporal variation by utilizing sequential shapeshifting of patterns. This article examines the outcomes of this educational research from a perspective of design computing with a purpose to remark its pedagogical significance. A multilayered reading about the exercise will reveal its historical, theoretical, and artistic backgrounds. Then the common structural elements and different construction approaches are explained along with a novel design and analysis method. The proposed method embeds variations of two-dimensional pattern deformations on a third dimension. It enables various analyses such as the measurement of regularity and locating the attractor points. This study is expected to exemplify how computational thinking and new digital tools change the way designers would approach to such systematic compositions.},
 author = {Tuğrul Yazar},
 doi = {10.1177/1478077117734660},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1478077117734660},
 journal = {International Journal of Architectural Computing},
 number = {4},
 pages = {250–267},
 title = {Revisiting Parquet Deformations from a computational perspective: A novel method for design and analysis},
 url = {https://doi-org.crai.referencistas.com/10.1177/1478077117734660},
 volume = {15},
 year = {2017x}
}

@article{doi:10.1177/1478077117735019,
 abstract = {This study sheds light on a holistic understanding of muqarnas with its historical, philosophical and conceptual backgrounds on one hand and formal, structural and algorithmic principles on the other hand. The vault-like Islamic architectural element, muqarnas, is generally considered to be a non-structural decorative element. Various compositional approaches have been proposed to reveal the inner logic of these complex geometric elements. Each of these approaches uses different techniques such as measuring, unit-based decoding or three-dimensional interpretation of two-dimensional patterns. However, the reflections of the inner logic onto different contexts, such as the usage of different initial geometries, materials or performative concerns, were neglected. In this study, we offer a new schema to approach the performative aspects of muqarnas tectonics. This schema contains new sets of elements, properties and relations deriving partly from previous approaches and partly from the technique of folding. Thus, this study first reviews the previous approaches to analyse the geometric and constructional principles of muqarnas. Second, it explains the proposed scheme through a series of algorithmic form-finding experiments. In these experiments, we question whether ‘fold’, as one of the performative techniques of making three-dimensional forms, contributes to the analysis of muqarnas in both a conceptual and computational sense. We argue that encoding vault-like systems via geometric and algorithmic relations based on the logic of the ‘fold’ provides informative and intuitive feedback for form-finding, specifically in the earlier phases of design. While focusing on the performative potential of a specific fold operation, we introduced the concept of bifurcation to describe the generative characteristics of folding technique and the way of subdividing the form with respect to redistribution of the forces. Thus, in this decoding process, the bifurcated fold explains not only to demystify the formal logic of muqarnas but also to generate new forms without losing contextual conditions.},
 author = {Sema Alaçam and Orkan Zeynel Güzelci and Ethem Gürer and Saadet Zeynep Bacınoğlu},
 doi = {10.1177/1478077117735019},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1478077117735019},
 journal = {International Journal of Architectural Computing},
 number = {4},
 pages = {285–303},
 title = {Reconnoitring computational potentials of the vault-like forms: Thinking aloud on muqarnas tectonics},
 url = {https://doi-org.crai.referencistas.com/10.1177/1478077117735019},
 volume = {15},
 year = {2017b}
}

@article{doi:10.1177/1478077118798395,
 abstract = {This article is motivated by the fact that in Cape Town, South Africa, approximately 7.5 million people live in informal settlements and focuses on potential upgrading strategies for such sites. To this end, we developed a computational method for rapid urban design prototyping. The corresponding planning tool generates urban layouts including street network, blocks, parcels and buildings based on an urban designer’s specific requirements. It can be used to scale and replicate a developed urban planning concept to fit different sites. To facilitate the layout generation process computationally, we developed a new data structure to represent street networks, land parcellation, and the relationship between the two. We also introduced a nested parcellation strategy to reduce the number of irregular shapes generated due to algorithmic limitations. Network analysis methods are applied to control the distribution of buildings in the communities so that preferred neighborhood relationships can be considered in the design process. Finally, we demonstrate how to compare designs based on various urban analysis measures and discuss the limitations that arise when we apply our method in practice, especially when dealing with more complex urban design scenarios.},
 author = {Yufan Miao and Reinhard Koenig and Katja Knecht and Kateryna Konieva and Peter Buš and Mei-Chih Chang},
 doi = {10.1177/1478077118798395},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1478077118798395},
 journal = {International Journal of Architectural Computing},
 number = {3},
 pages = {212–226},
 title = {Computational urban design prototyping: Interactive planning synthesis methods—a case study in Cape Town},
 url = {https://doi-org.crai.referencistas.com/10.1177/1478077118798395},
 volume = {16},
 year = {2018l}
}

@article{doi:10.1177/1478077120919850,
 abstract = {Computational design affords agency: the ability to orchestrate the material, spatial, and technical architectural system. In this specific case, it occurs through enhanced, authored means to facilitate making and performance—typically driven by concerns of structural optimization, material use, and responsivity to environmental factors—of an atmospheric rather than social nature. At issue is the positioning of this particular manner of agency solely with the architect auteur. This abruptly halts—at the moment in which fabrication commences—the ability to amend, redefine, or newly introduce fundamentally transformational constituents and their interrelationships and, most importantly, to explore the possibility for extraordinary outcomes. When the architecture becomes a functional, social, and cultural entity, in the hands of the idealized abled-bodied user, agency—especially for one of an otherly body or mind—is long gone. Even an empathetic auteur may not be able to access the motivations of the differently-abled body and neuro-divergent mind, effectively locking the constraints of the design process, which creates an exclusionary system to those beyond the purview of said auteur. It can therefore be deduced that the mechanisms or authors of a conventional computational design process cannot eradicate the exclusionary reality of an architectural system. Agency is critical, yet a more expansive terminology for agent and agency is needed. The burden to conceive of capacities that will always be highly temporal, social, unpredictable, and purposefully unknown must be shifted far from the scope of the traditional directors of the architectural system. Agency, and who it is conferred upon, must function in a manner that dissolves the distinctions between the design, the action of designing, the author of design, and those subjected to it.},
 author = {Sean Ahlquist},
 doi = {10.1177/1478077120919850},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1478077120919850},
 journal = {International Journal of Architectural Computing},
 number = {2},
 pages = {174–193},
 title = {Negotiating human engagement and the fixity of computational design: Toward a performative design space for the differently-abled bodymind},
 url = {https://doi-org.crai.referencistas.com/10.1177/1478077120919850},
 volume = {18},
 year = {2020a}
}

@article{doi:10.1177/1478077120942193,
 abstract = {In the fields of architecture and urban design, there has always been a delay in the impact of industrial revolution technologies, and in the case of less industrialised countries, the delay has been even bigger. This article starts with a review of the history of computer-aided architectural design in Brazil and then describes the state of the field in some Latin American countries. Finally, we discuss the ‘ideal computer curriculum’ for architects in the Fourth Industrial Revolution.},
 author = {Gabriela Celani},
 doi = {10.1177/1478077120942193},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1478077120942193},
 journal = {International Journal of Architectural Computing},
 number = {4},
 pages = {320–334},
 title = {Shortcut to the Fourth Industrial Revolution: The case of Latin America},
 url = {https://doi-org.crai.referencistas.com/10.1177/1478077120942193},
 volume = {18},
 year = {2020b}
}

@article{doi:10.1177/1478077120947990,
 abstract = {Building with additive manufacturing is an increasingly relevant research topic in the field of Construction 4.0, where designers are seeking higher levels of automation, complexity and precision compared to conventional construction methods. As an answer to the increasing problem of scarcity of resources, the presented research exploits the potential of Fused Deposition Modelling in the production of a lightweight load-responsive cellular lattice structure at the architectural scale. The article offers an extensive insight into the computational processes involved in the design, engineering, analysis, optimization and fabrication of a material-efficient, fully 3D printed, lattice structure. Material, structure and manufacturing features are integrated within the design development in a comprehensive computational workflow. The article presents methods and results while discussing the project as a material-efficient approach to complex structures.},
 author = {Roberto Naboni and Anja Kunic and Luca Breseghello},
 doi = {10.1177/1478077120947990},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1478077120947990},
 journal = {International Journal of Architectural Computing},
 number = {4},
 pages = {404–423},
 title = {Computational design, engineering and manufacturing of a material-efficient 3D printed lattice structure},
 url = {https://doi-org.crai.referencistas.com/10.1177/1478077120947990},
 volume = {18},
 year = {2020m}
}

@article{doi:10.1177/1478077120947996,
 abstract = {Life Cycle Assessment (LCA) has been widely adopted to identify the Global Warming Potential (GWP) in the construction industry and determine its high environmental impact through Greenhouse Gas (GHG) emissions, energy and resource consumptions. The consideration of LCA in the early stages of design is becoming increasingly important as a means to avoid costly changes at later stages of the project. However, typical LCA-based tools demand very detailed information about structural and material systems and thus become too laborious for designers in the conceptual stages, where such specifications are still loosely defined. In response, this paper presents a workflow for LCA-based evaluation where the selection of the construction system and material is kept open to compare the impacts of alternative design variants. We achieve this through a strict division into support and infill systems and a simplified visualization of a schematic floor layout using a shoebox approach, inspired from the energy modelling domain. The shoeboxes in our case are repeatable modules within a schematic floor plan layout, whose enclosures are defined by parametric 2D surfaces representing total ratios of permanent supports versus infill components. Thus, the assembly of modular surface enclosures simplifies the LCA evaluation process by avoiding the need to accurately specify the physical properties of each building component across the floor plan. The presented workflow facilitates the selection of alternative structural systems and materials for their comparison, and outputs the Global Warming Potential (GWP) in the form of an intuitive visualization output. The workflow for simplified evaluation is illustrated through a case study that compares the GWP for selected combinations of material choice and construction systems.},
 author = {Michael Budig and Oliver Heckmann and Markus Hudert and Amanda Qi Boon Ng and Zack Xuereb Conti and Clement Jun Hao Lork},
 doi = {10.1177/1478077120947996},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1478077120947996},
 journal = {International Journal of Architectural Computing},
 number = {1},
 pages = {6–22},
 title = {Computational screening-LCA tools for early design stages},
 url = {https://doi-org.crai.referencistas.com/10.1177/1478077120947996},
 volume = {19},
 year = {2021d}
}

@article{doi:10.1177/1478077120949033,
 abstract = {This paper documents a computational approach to the design, fabrication, and assembly of customizable space structures built entirely out of flat-cut interlocking elements without the need of nodes, fasteners, cement, or glue. Following a Research by Design (RbD) methodology, we establish a framework comprising geometric and parametric modeling, structural analysis, and digital fabrication stages to examine the following research question: how might the modularity of a construction kit be combined with the plasticity of parametric descriptions to facilitate the design and fabrication of flat-cut space structures? We find that an adaptive joint design that resolves local deformations at the node and element levels can facilitate the construction of flat-cut space structures by making modular components responsive to local geometric, material, and mechanical demands. The research centers on the design and construction of an architecture-scale installation based on the Weaire-Phelan structure—an aperiodic space-filling geometric structure that approximates the geometry of foam—entirely out of flat-cut interlocking elements. Documenting the process in technical detail, as well as some limitations, the paper contributes to recent efforts to develop digital materials suitable for architectural applications. In addition, it contributes to extend the formal and architectural possibilities of flat-cut space structure design by facilitating “bottom-up” design explorations in concert with the structure’s tectonic resolution.},
 author = {Jingyang Liu and Yi-Chin Lee and Daniel Cardoso Llach},
 doi = {10.1177/1478077120949033},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1478077120949033},
 journal = {International Journal of Architectural Computing},
 number = {1},
 pages = {37–49},
 title = {Computational design and fabrication of highly customizable architectural space frames: Making a flat-cut Weaire-Phelan structure},
 url = {https://doi-org.crai.referencistas.com/10.1177/1478077120949033},
 volume = {19},
 year = {2021g}
}

@article{doi:10.1177/1478077120958165,
 abstract = {The business model of architecture has been accused of being technologically deficient. Through case studies, this paper investigates alternatives to the status quo of computation in architecture, with a focus on design-oriented service providers. It first examines the discourse on computation within the field, then describes the experiments of a relevant design firm before discussing implications for the wider industry. In the process, the paper articulates the gap between theoretical computing applications and the realities of the architecture business. It addresses difficult technological adaptations within the industry with concrete use cases of integrated computing expertise, highlighting opportunities, benefits, and, more importantly, the need for a computational workforce.},
 author = {Zichu Will Wang},
 doi = {10.1177/1478077120958165},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1478077120958165},
 journal = {International Journal of Architectural Computing},
 number = {1},
 pages = {104–115},
 title = {Real design practice, real design computation},
 url = {https://doi-org.crai.referencistas.com/10.1177/1478077120958165},
 volume = {19},
 year = {2021q}
}

@article{doi:10.1177/14780771211025142,
 abstract = {This article introduces a methodology to implement Data-driven Thinking in the context of urban design. We present the results of a case study based on a 7-day workshop with 10 participants with landscape design and architecture background. The goal of the workshop was to expose participants to Data-driven Thinking through experimental design, multi-sensor data collection, data analysis, visualization, and insight generation. We evaluate their learning experience in designing an experimental setup, collecting real-time immediate environmental and physiological body reactions data. Our results from the workshop show that participants increased their knowledge about measuring, visualizing and understanding data of the surrounding built environment.},
 author = {Bige Tunçer and Francisco Benita},
 doi = {10.1177/14780771211025142},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14780771211025142},
 journal = {International Journal of Architectural Computing},
 number = {2},
 pages = {316–333},
 title = {Data-driven thinking for measuring the human experience in the built environment},
 url = {https://doi-org.crai.referencistas.com/10.1177/14780771211025142},
 volume = {20},
 year = {2022q}
}

@article{doi:10.1177/14780771211025311,
 abstract = {Digital fabrication and its cultivated spaces promise to break disciplinary boundaries and enable access to its technologies and computation for the broader public. This paper examines the trope of “access” in digital fabrication, design, and craft, and illustrates how it unfolds in these spaces and practices. An equitable future is one that builds on and creates space for multiple bodies, knowledges, and skills; allows perceptual interaction and corporeal engagement with people, materials, and tools; and employs technologies accessible to broad groups of society. By conducting comparative and transnational ethnographic studies at digital fabrication and crafting sites, and performing craft-centered computational design studies, we offer a critical description of what access looks like in an equitable future that includes digital fabrication. The study highlights the need to examine universal conceptions and study how they are operationalized in broader narratives and design pedagogy traditions.},
 author = {Vernelle AA Noel and Yana Boeva and Hayri Dortdivanlioglu},
 doi = {10.1177/14780771211025311},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14780771211025311},
 journal = {International Journal of Architectural Computing},
 number = {4},
 pages = {496–511},
 title = {The question of access: Toward an equitable future of computational design},
 url = {https://doi-org.crai.referencistas.com/10.1177/14780771211025311},
 volume = {19},
 year = {2021n}
}

@article{doi:10.1177/14780771211040169,
 abstract = {In the architecture, engineering and construction (AEC) industry, waste is oft framed as an economic problem typically addressed in a building’s construction and demolition phase. Yet, architectural design decision-making can significantly determine construction waste outcomes. Following the logic of zero waste, this research addresses waste minimisation ‘at the source’. By resituating the problem of construction waste within the architectural design process, the research explores waste as a data and informational problem in a design system. Accordingly, this article outlines the creation of an integrated computational design decision support waste tool that employs a novel data structure combining HTML-scraped material data and historic building information modelling (BIM) data to generate waste evaluations in a browser-based 3D modelling platform. Designing an accessible construction waste tool for use by architects and designers aims to heighten awareness of the waste implications of design decisions towards challenging the systems of consumption and production that generate construction and demolition waste.},
 author = {Matthias H Haeusler and Nicole Gardner and Daniel K Yu and Claire Oh and Blair Huang},
 doi = {10.1177/14780771211040169},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14780771211040169},
 journal = {International Journal of Architectural Computing},
 number = {4},
 pages = {594–611},
 title = {(Computationally) designing out waste: Developing a computational design workflow for minimising construction and demolition waste in early-stage architectural design},
 url = {https://doi-org.crai.referencistas.com/10.1177/14780771211040169},
 volume = {19},
 year = {2021e}
}

@article{doi:10.1177/14780771211070006,
 abstract = {If we understand architecture as a three-part system formed by the building, its image, or drawings and images describing buildings, and the critical discourse around architecture, then the texts or ways of speaking about architecture play a key role in understanding the field and its development. By analysing a corpus of around 4.6 million words from texts written between 2005 and 2020 that form a part of critical discourse in computational architecture (understood as the result of the intense digitalization of the field), this paper aims to map ways of speaking about computational architecture. This contributes to architectural theory and might help gain a better understanding of the evolution of the digitalization of construction in general. Findings show that computational architecture is surrounded by a specific way of speaking, hybridized with words from fields such as biology, neuroscience, arts and humanities, and engineering. While some topics such as ‘sustainability’ or ‘biology’ come up consistently in the discourse, others, such as ‘people’ or ‘human’, have periods when they are more and less popular. After highlighting open research questions, the paper concludes by presenting a map of periodic and recurring topics in ways of speaking about computational architecture over the last 15 years, thus tracking and documenting long-term trends, and illuminating patterns in the broader field of digital construction.},
 author = {Anca-Simona Horvath},
 doi = {10.1177/14780771211070006},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14780771211070006},
 journal = {International Journal of Architectural Computing},
 number = {2},
 pages = {150–175},
 title = {How we talk(ed) about it: Ways of speaking about computational architecture},
 url = {https://doi-org.crai.referencistas.com/10.1177/14780771211070006},
 volume = {20},
 year = {2022h}
}

@article{doi:10.1177/14780771221097683,
 abstract = {Self-learning is receiving great attention internationally in different fields, along with the best utilization of different computational applications or methods. This paper introduces a novel computational approach for supporting Architectural Design Education (ADE) in its early stages; a computational implementation through MATLAB has been developed to conduct the proposed processes. As a scope, spaces’ furnishing design has been selected to demonstrate the proposed computational approach and implementation, while office workspaces have been selected as a representative case. However, the proposed approach provides and enhances ADE through three main concepts: (a) generating design alternatives for different cases of furnishing spaces, (b) providing accurate and flexible evaluations to students’/designers’ works with different levels, and (c) tracking students based on their defaults and relevant sensitive modifications. Different applications of the proposed approach have been generated, analyzed, and validated.},
 author = {Randa M.A. Mahmoud and Amr M.A. Youssef},
 doi = {10.1177/14780771221097683},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14780771221097683},
 journal = {International Journal of Architectural Computing},
 number = {2},
 pages = {346–377},
 title = {A computational framework for supporting architectural education of spaces’ furnishing design},
 url = {https://doi-org.crai.referencistas.com/10.1177/14780771221097683},
 volume = {20},
 year = {2022h}
}

@article{doi:10.1177/14780771221097685,
 abstract = {Computer-based design and fabrication systems in architecture contain modes of operation and preferences that often constrain tectonic possibilities in design and construction. These predispositions neglect architecture’s cultural and material dimensions, resulting in universalizing tectonics that erase nuances of place, culture, and expression in design. How may we celebrate local tectonic languages while also revisiting them through computer-based systems in architecture? The project examined here highlights novel possibilities for cultural expression and craftsmanship through computational design methods, retaining the expressive potential of a local craft while de-familiarizing its cultural context. I analyze how shape grammars and digital fabrication methods deployed in design, de-familiarizes the craft of wire-bending in costuming in the Trinidad Carnival. I present and apply new rules for the craft’s computational description based on material tests and an architectural application to expand discourses on critical regionalism. I adopt Tabbarah’s term “computational regionalism” to describe this process and elaborate it as a five-step sequence. Computational regionalism employs computational methods to translate local craft knowledge and tectonic languages into new interpretations and poetics of construction. Its process of creative de-familiarization raises critical questions about the local and the universal.},
 author = {Vernelle AA Noel},
 doi = {10.1177/14780771221097685},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14780771221097685},
 journal = {International Journal of Architectural Computing},
 number = {2},
 pages = {277–296},
 title = {Computational regionalism: De-familiarization of tectonics in the wire-bending craft},
 url = {https://doi-org.crai.referencistas.com/10.1177/14780771221097685},
 volume = {20},
 year = {2022n}
}

@article{doi:10.1177/14780771221100102,
 abstract = {Through the recent technological developments within the fourth industrial revolution, artificial intelligence (AI) studies have had a huge impact on various disciplines such as social sciences, information communication technologies (ICTs), architecture, engineering, and construction (AEC). Regarding decision-making and forecasting systems in particular, AI and machine learning (ML) technologies have provided an opportunity to improve the mutual relationships between machines and humans. When the connection between ML and architecture is considered, it is possible to claim that there is no parallel acceleration as in other disciplines. In this study, and considering the latest breakthroughs, we focus on revealing what ML and architecture have in common. Our focal point is to reveal common points by classifying and analyzing current literature through describing the potential of ML in architecture. Studies conducted using ML techniques and subsets of AI technologies were used in this paper, and the resulting data were interpreted using the bibliometric analysis method. In order to discuss the state-of-the-art research articles which have been published between 2014 and 2020, main subjects, subsets, and keywords were refined through the search engines. The statistical figures were demonstrated as huge datasets, and the results were clearly delineated through Sankey diagrams. Thanks to bibliometric analyses of the current literature of WOS (Web of Science), CUMINCAD (Cumulative Index about publications in Computer Aided Architectural Design supported by the sibling associations ACADIA, CAADRIA, eCAADe, SIGraDi, ASCAAD, and CAAD futures), predictable data have been presented allowing recommendations for possible future studies for researchers.},
 author = {Gizem Özerol and Semra Arslan Selçuk},
 doi = {10.1177/14780771221100102},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14780771221100102},
 journal = {International Journal of Architectural Computing},
 number = {1},
 pages = {23–41},
 title = {Machine learning in the discipline of architecture: A review on the research trends between 2014 and 2020},
 url = {https://doi-org.crai.referencistas.com/10.1177/14780771221100102},
 volume = {21},
 year = {2023p}
}

@article{doi:10.1177/14780771221120576,
 abstract = {The space layout problem encompasses challenges that rely on a diverse range of contexts regarding urban planning and architectural design, during the traditional design phases which require immense effort and time for the evaluation of the spatial elements’ characteristic needs. In order to eliminate the burden of considering all multidimensional design aspects at the same time, this research presents a three-bodied computational method for locating the spaces of the given architectural design program in a project site, according to the defined list of design objectives and criteria. Besides the determination of the layout according to the requirements of the spatial elements, this research proposes an integration of the space syntax theory’s analytical compounds in terms of Justified Graph Analysis and Integration Values as the fitness criteria for the multi-objective evolutionary optimization in the computational model. To satisfy the integrity levels of each various characterized element within site organization, that are implied inherently by the architectural design program and generate a sustainable space network layout for the project site.},
 author = {Selen Cicek and Gozde Damla Turhan},
 doi = {10.1177/14780771221120576},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14780771221120576},
 journal = {International Journal of Architectural Computing},
 number = {3},
 pages = {610–629},
 title = {Computational generation of a spatial layout through syntactical evaluation and multi-objective evolutionary optimization},
 url = {https://doi-org.crai.referencistas.com/10.1177/14780771221120576},
 volume = {20},
 year = {2022c}
}

@article{doi:10.1177/14780771221121031,
 abstract = {This paper describes a method to tailor computational design algorithms to evaluate human-centric outputs in architectural projects by presenting a case study that evaluates teamwork affordance for primary care clinics. We argue that computational design assessment techniques fall short of evaluating those human experiences that stem from multiple interactions between individuals and the surrounding environment. This research suggests that future computational design algorithms could benefit from incorporating scenario-based methods developed in the field of evidence-based design, in which studies are concerned about improving qualitative goals. Through this case study, we describe the process of prototyping a computational design algorithm based on the Functional Scenario Analysis approach, applying the algorithm to evaluate case studies, visualizing findings, and extracting design strategies based on the analysis results. This method offers a new vision of how computational design can benefit informed decision-making processes and generate design alternatives aligned with a project’s design goals.},
 author = {Raha M Rastegar and Sara Saghafi Moghaddam and Ramtin Haghnazar and Craig Zimring},
 doi = {10.1177/14780771221121031},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14780771221121031},
 journal = {International Journal of Architectural Computing},
 number = {3},
 pages = {567–586},
 title = {From evidence to assessment: Developing a scenario-based computational design algorithm to support informed decision-making in primary care clinic design workflow},
 url = {https://doi-org.crai.referencistas.com/10.1177/14780771221121031},
 volume = {20},
 year = {2022n}
}

@article{doi:10.1177/14780771221121829,
 abstract = {In the fourth industrial revolution, programming promises to be a fundamental subject like mathematics, science, languages ​​or the arts. Architects design more than buildings developing innovative methods and they are among the pioneers in visual programming development. However, after more than 10 years of visual programming in architecture, despite the fast-learning curve, visual programming presents considerable limitations to solve complex problems. To overcome limitations, the authors propose to associate the advantages of visual and textual languages in Python. The article addresses an ongoing research study to implement Computational Methods in Architectural Education. The authors began by describing the general goal of this project, and of this article in particular. This article focuses on the implementation of two disciplines ‘Computation for Architecture in Python’ I and II. The first discipline uses programming based on the construction of functions in the imperative language, implemented in the text editor, in visual programming, using Grasshopper methods. The second discipline, which is under development, intends to teach object-oriented programming. The results of the first discipline are encouraging; despite reported difficulties in programming fundamentals, such as lists, loops and recursion. The development of the second discipline, in object-oriented programming, deals with the concepts of classes and objects, and more abstract principles such abstraction, inheritance, polymorphism or encapsulation. This paradigm allows building robust programs, but requires a more in-depth syntax. The article reports this ongoing research on this new paradigm of object-oriented language, expanding the application of a hybrid visual-textual language in Architecture.},
 author = {Goncalo Castro Henriques and Pedro Maciel Xavier and Victor de Luca Silva and Luca Rédua Bispo and João Victor Fraga},
 doi = {10.1177/14780771221121829},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14780771221121829},
 journal = {International Journal of Architectural Computing},
 number = {3},
 pages = {673–687},
 title = {Computation for Architecture, hybrid visual and textual language: Research developments and considerations about the implementation of structural imperative and object-oriented paradigms},
 url = {https://doi-org.crai.referencistas.com/10.1177/14780771221121829},
 volume = {20},
 year = {2022i}
}

@article{doi:10.1177/14780771221139911,
 abstract = {In a problem-based, digital-intensive learning environment, the increased proliferation of computational tools used for architectural design has led to a fundamental transformation in architectural studios. Many studies have shown that this has significantly led to the change in cognition of design environments in academia. Design decisions are made through a recursive process that is cyclically refined by allowing constant feedback and testing. This paper represents an observational study with an aim to understand the impact of digital mediums on design processes and design outcomes focusing on associative modeling using VPL. It contextualizes the difference, the associative modeling system as a parametric subset brings to design thinking when used as a medium to explore architectural design. It analyzes specific attributes of associative modeling, otherwise native to computational thinking, that contribute to the legibility of the design process. The paper demonstrates how associative modeling allows the design process to be examined and edited at any stage during and even after algorithmic development, bringing in flexibility. It is argued that digital design tool affordances enable students to develop multilayered and more structured design logic that augments cognition bringing more legibility to the design thinking process.},
 author = {Dhanashree Sardeshpande and Vasudha Gokhale},
 doi = {10.1177/14780771221139911},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14780771221139911},
 journal = {International Journal of Architectural Computing},
 number = {4},
 pages = {728–741},
 title = {“Legibility” a product of obligatory processes in parametric architectural design: A study of implications of associative modeling on design thinking in a parametric architectural design studio},
 url = {https://doi-org.crai.referencistas.com/10.1177/14780771221139911},
 volume = {20},
 year = {2022t}
}

@article{doi:10.1177/14780771221148778,
 abstract = {Evolutionary design (ED) is a strategy that makes use of computational power to couple generative techniques with evaluation methods, to put forward designs that are better with each iteration. In this research, we present a representation scheme for solving spatial layout problems that is simple to implement as well as extend. The mechanisms for evaluation and mutation are defined and also shown to be extendable. Ultimately, the topic explored here is the ways in which ED and computation can enhance our design thinking and how computers can provide the background to new design processes and workflows.},
 author = {Ruwan Fernando and Ruby Michael},
 doi = {10.1177/14780771221148778},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14780771221148778},
 journal = {International Journal of Architectural Computing},
 number = {4},
 pages = {679–694},
 title = {Solving planning problems with evolutionary computation},
 url = {https://doi-org.crai.referencistas.com/10.1177/14780771221148778},
 volume = {21},
 year = {2023g}
}

@article{doi:10.1177/14780771231170272,
 abstract = {This paper examines the prevalence of bias in artificial intelligence text-to-image models utilized in the architecture and design disciplines. The rapid pace of advancements in machine learning technologies, particularly in text-to-image generators, has significantly increased over the past year, making these tools more accessible to the design community. Accordingly, this paper aims to critically document and analyze the collective, computational, and cognitive biases that designers may encounter when working with these tools at this time. The paper delves into three hierarchical levels of operation and investigates the possible biases present at each level. Starting with the training data for large language models (LLM), the paper explores how these models may create biases privileging English-language users and perspectives. The paper subsequently investigates the digital materiality of models and how their weights generate specific aesthetic results. Finally, the report concludes by examining user biases through their prompt and image selections and the potential for platforms to perpetuate these biases through the application of user data during training.},
 author = {Andrew Kudless},
 doi = {10.1177/14780771231170272},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14780771231170272},
 journal = {International Journal of Architectural Computing},
 number = {2},
 pages = {256–279},
 title = {Hierarchies of bias in artificial intelligence architecture: Collective, computational, and cognitive},
 url = {https://doi-org.crai.referencistas.com/10.1177/14780771231170272},
 volume = {21},
 year = {2023l}
}

@article{doi:10.1177/14780771231180258,
 abstract = {Urban building energy modelling (UBEM) is a prevalent research method to examine the multi-scale building to urban renovation in mitigating global energy-related carbon emissions. However, only a few studies delineate a complete workflow from generation to application using UBEM. In particular, to facilitate the designing of sustainable built environments, existing research needs to emphasize the integration of multi-scale energy performance evaluation within the design development process for architects and urban planners. The key challenges lie in the need for integrated datasets and incompatibility between software tools required for designing, modelling, and evaluation. This paper presents a comprehensive methodological framework to investigate applicable urban decarbonization strategies. A case study of Sheffield in the UK demonstrates the development of an automated and standardized computational workflow. This data-driven workflow aims to evaluate energy demand and supply scenarios at an urban scale to access the potential of decarbonizing built environments. The workflow is designed to be adaptable to various scales of urban regions, given a suitable geographic information system (GIS) dataset.},
 author = {Hang Xu and Tsung-Hsien Wang},
 doi = {10.1177/14780771231180258},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14780771231180258},
 journal = {International Journal of Architectural Computing},
 number = {3},
 pages = {516–535},
 title = {A generative computational workflow to develop actionable renovation strategies for renewable built environments: A case study of Sheffield},
 url = {https://doi-org.crai.referencistas.com/10.1177/14780771231180258},
 volume = {21},
 year = {2023t}
}

@article{doi:10.1177/14780771241260850,
 abstract = {This article describes generative algorithms and Digital Fabrication techniques with organic materials to create complex 3D objects for industrial design, sculpture, and architecture. Experimental artistic production using these algorithms concluded with a solution based on programmable meshes, which use identifiers to control the topological characteristics of vertices during the modeling process. On the other hand, the hybridization of analog and digital techniques was explored through fabrication. Comparing artistic production and hybrid techniques with generative AI, we will discuss topics of Computational Creativity in art, industrial design, and architecture. The programmable meshes solution, combined with hybrid fabrication processes, enables an incredible variety of complex forms, stimulates artistic creativity, and provides flexible feedback to bypass some Digital Fabrication issues. Our findings also elucidate the importance of original technology development and cultural identity in fostering creative and culturally inclusive technologies for art and education.},
 author = {Umberto Luigi Roncoroni and Veronica Crousse de Vallongue and Octavio Centurion Bolaños},
 doi = {10.1177/14780771241260850},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14780771241260850},
 journal = {International Journal of Architectural Computing},
 number = {0},
 pages = {14780771241260850},
 title = {Computational creativity issues in generative design and digital fabrication of complex 3D meshes},
 url = {https://doi-org.crai.referencistas.com/10.1177/14780771241260850},
 volume = {0},
 year = {2024k}
}

@article{doi:10.1177/14780771241260855,
 abstract = {Scientific knowledge, ideally neutral and impartial, is inevitably shaped by geographical, economic, and cultural contexts. This research contends that overcoming the constraints of human and economic scarcity, inertia, and limited funding access demands development of collaborative research networks. To this end, four university laboratories from Brazil and Chile - UFRJ, UBB, UTFSM and UAI - have united to advance robotics applied to architecture. The methodology begins with an analysis of Industry 4.0, Fab Lab implementations, and robotics in architecture in the region. They identify key research aspects by mapping each laboratory’s activities and technologies to pinpoint expertise and potential collaborative areas. The authors propose a summary table comparing the labs and a chronological overview to track regional robotics advancement. To raise awareness among peers, the initiative involves joint actions such as courses, workshops and technical visits.1 Recognising the scarcity of robotic units and the inapplicability of procedures from wealthier contexts, the authors draw on their lab experience to propose guidelines for implementing robotics research units in academia within the region. This includes technological alternatives, installation considerations, and detailed configuration reviews. The deployment of additional robotic units is a means to foster collaboration and bolster the network. Anticipated outcomes encompass increased critical mass, collaborative research initiatives, faculty and student exchange across institutions, higher publication rates, knowledge acquisition, and improved access to global funding agencies. In summary, the Southern Creative Robotics network initiative aims to catalyse the creative integration of digital manufacturing and robotics technology within design in the Ibero-American context.},
 author = {Goncalo Castro Henriques and Andres Martin Passaro and Rodrigo Garcia Alvarado and Luis Felipe González Böhme and Francisco Javier Quitral Zapata and Sergio Araya},
 doi = {10.1177/14780771241260855},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14780771241260855},
 journal = {International Journal of Architectural Computing},
 number = {0},
 pages = {14780771241260856},
 title = {Southern creative robotics – A collaborative framework to implement computational design, fabrication and robotics},
 url = {https://doi-org.crai.referencistas.com/10.1177/14780771241260855},
 volume = {0},
 year = {2024d}
}

@article{doi:10.1177/14780771241270265,
 abstract = {The study presents unique teaching approach for generating a parametric modular facade fabrication for first-year students in visual design module using a computational visual coding method based on Truchet Tiles. The method followed four main phases: exploring tiling, testing connectivity, fabrication, and assembly. Shape grammar was considered during the design principles implementation to respond to climatic behavior. Space-filling shapes method known as the “Generalized Abeille Tiles” was followed to ensure tiling connectivity. The experimentation involves manual folding techniques without any software. A survey was conducted among first-year and fourth-year students post-introduction to computational software. The results showed a wide range of alternative geometries with different self-shading patterns generated. It highlighted that facade fabrication aided students in comprehending computational logic, easing the understanding of computational software complexity. This pilot study emphasizes the effectiveness of teaching computational concepts in early curricula.},
 author = {Deena EL-Mahdy},
 doi = {10.1177/14780771241270265},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14780771241270265},
 journal = {International Journal of Architectural Computing},
 number = {0},
 pages = {14780771241270264},
 title = {From theory to practice: Truchet tile as a computational visual coding approach for facade visualization},
 url = {https://doi-org.crai.referencistas.com/10.1177/14780771241270265},
 volume = {0},
 year = {2024c}
}

@article{doi:10.1177/14780771241279347,
 abstract = {Hybrid girihs refer to Islamic geometric patterns that include various stars/rosettes in their final pattern. In this paper, we first identified historical hybrid girihs and then categorized them based on symmetry groups and the number of stars/rosettes folds. In the next step, we analyzed the existing hybrid girihs to identify the generative parameters and present a method for generating historical and novel systematic and non-systematic hybrid girihs. The proposed method of this paper is a computational and parametric approach based on the symmetry groups theory. Its general steps include generating the minimal essential information (template motif) within the fundamental region, applying appropriate symmetry operations on the content of the fundamental region to create the content of the unit girih, and replicating the content of the unit girih in a suitable network according to the symmetry group to create the whole pattern. Our method is used to generate hybrid girih for adorning surfaces in digital spaces and for constructing facade modules (adorned with Islamic geometric patterns) and interior decorative partitions and furniture in physical spaces according to the aesthetic judgment of users.},
 author = {Ali Azizi Naserabad and Abdulhamid Ghanbaran},
 doi = {10.1177/14780771241279347},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14780771241279347},
 journal = {International Journal of Architectural Computing},
 number = {0},
 pages = {14780771241279348},
 title = {Computational approach in presentation a parametric method to construct hybrid girihs (hybrid Islamic geometric patterns)},
 url = {https://doi-org.crai.referencistas.com/10.1177/14780771241279347},
 volume = {0},
 year = {2024c}
}

@article{doi:10.1177/14780771241279350,
 abstract = {This paper introduces a computational aesthetics framework utilizing computer vision (CV) and artificial neural networks (ANN) to predict the aesthetic preferences of groups of people for architecture. It relies on part-to-whole theories from aesthetics and cognitive psychology. A survey of a group of people on preferences of images is held to record an average hedonic response (AHR). CV algorithms MSER and SAM recognize parts in images. Birkhoff’s aesthetic measure formula is adapted by employing the number of parts and their connections. These quantities are used as input layers of an ANN, and the AHR is the target output. The ANN evaluates images to output a predicted hedonic response (PHR), which is tested as a criterion in parametric design space navigation and in mapping the latent space of GANs. We conclude that such a framework is a heuristic method for better understanding the design and latent spaces and exploring designs.},
 author = {Victor Sardenberg and Igor Guatelli and Mirco Becker},
 doi = {10.1177/14780771241279350},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14780771241279350},
 journal = {International Journal of Architectural Computing},
 number = {0},
 pages = {14780771241279350},
 title = {A computational framework for aesthetic preferences in architecture using computer vision and artificial neural networks},
 url = {https://doi-org.crai.referencistas.com/10.1177/14780771241279350},
 volume = {0},
 year = {2024o}
}

@article{doi:10.1177/1478210319894785,
 abstract = {Programming and computational thinking have emerged as compulsory skills in elementary school education. In 2018, Sweden has integrated programming in mathematics education with the rationale that it fosters problem solving and logical thinking skills and motivates students to learn mathematics. We investigated how teachers introduce programming in mathematics education in a Swedish primary school using an explorative case study. We followed four mathematics teachers during the first semester in which programming was mandatory. They taught second-, sixth- and ninth-grade students. Our contributions are threefold: we provide an account of how programming is taught in mathematics education; we discuss how teachers reflect on the challenge of teaching programming in mathematics; and we report on students’ understanding of programming and their view on the relationship between programming and mathematics.},
 author = {Henrik Stigberg and Susanne Stigberg},
 doi = {10.1177/1478210319894785},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1478210319894785},
 journal = {Policy Futures in Education},
 number = {4},
 pages = {483–496},
 title = {Teaching programming and mathematics in practice: A case study from a Swedish primary school},
 url = {https://doi-org.crai.referencistas.com/10.1177/1478210319894785},
 volume = {18},
 year = {2020u}
}

@article{doi:10.1177/14782103211049913,
 abstract = {Information technology and computer science represent one area of science, technology, engineering, and mathematics (STEM) that have experienced significant growth in recent years. As such, federal policy has urged schools to embed new types of STEM courses into the curriculum. As one very prominent example, computer science (CS)-focused courses are a growing branch of career and technical education (CTE)—that is, CS-CTE. While previous research has examined coursetaking patterns and subsequent outcomes of CS-CTE courses for the general student population, little is known about how participation in these courses may benefit students with learning disabilities (SWLDs). From a pedagogical perspective, CS-CTE courses, and CTE courses in general, may be uniquely positioned to improve schooling outcomes for SWLDs. Using data from the nationally representative High School Longitudinal Study of 2009, we explored characteristics of CS-CTE participants, how CS-CTE may promote the development of key STEM attitudes (e.g., identity, self-efficacy, and utility), and how any relationships may differ by learning disability status. Using a double propensity score matching estimations, we found CS-CTE participation related to positive development of STEM identity and STEM self-efficacy for students without learning disabilities. For the SWLD population, CS-CTE participation was associated with growth in STEM self-efficacy and STEM utility. Policy implications discuss the gap in computer science employment between SWLDs and non-SWLDs, the prevalence of state-related computer science policies, and how to continue to promote STEM identity growth.},
 author = {Jay S Plasman and Michael Gottfried and Jennifer Freeman and Shaun Dougherty},
 doi = {10.1177/14782103211049913},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14782103211049913},
 journal = {Policy Futures in Education},
 number = {0},
 pages = {14782103211049912},
 title = {Promoting persistence: Can computer science career and technical education courses support educational advancement for students with learning disabilities?},
 url = {https://doi-org.crai.referencistas.com/10.1177/14782103211049913},
 volume = {0},
 year = {2022m}
}

@article{doi:10.1177/14782103221078401,
 abstract = {Emerging technological advancements can play an essential role in overcoming challenges caused by the COVID-19 pandemic. As a promising educational technology field, Learning Analytics (LA) tools or systems can offer solutions to COVID-19 pandemic-related needs, obstacles, and expectations in higher education. In the current study, we systematically reviewed 20 papers to better understand the responses of LA tools to the online learning challenges that higher education students, instructors, and institutions faced during the pandemic. In addition, we attempted to provide key cases in which LA has been effectively deployed for various purposes during the pandemic in the higher education context. We found out several prominent challenges for stakeholders. Accordingly, learners needed of timely support and interaction, and experienced difficulty of time management. Instructors lacked pedagogical knowledge for online teaching. In particular, individual and collaborative assessment have been a challenge for them. Institutions have not been ready for a digital transformation and online teaching. In response to these challenges, LA tools have been deployed for the following opportunities: monitoring, planning online learning process, fostering learners’ engagement and motivation, facilitating assessment process; increasing interaction, improving retention, being easy to use. Understanding these promises can also give insight into future higher education policies.},
 author = {Ismail Celik and Egle Gedrimiene and Anni Silvola and Hanni Muukkonen},
 doi = {10.1177/14782103221078401},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14782103221078401},
 journal = {Policy Futures in Education},
 number = {4},
 pages = {387–404},
 title = {Response of learning analytics to the online education challenges during pandemic: Opportunities and key examples in higher education},
 url = {https://doi-org.crai.referencistas.com/10.1177/14782103221078401},
 volume = {21},
 year = {2023d}
}

@article{doi:10.1177/14782103231177107,
 abstract = {To aid in the development of a globally competitive workforce, federal policymakers have expressed the priority of preparing students and adults with disabilities to succeed in science, technology, engineering, and mathematics (STEM) fields. Yet, no research has examined the extent to which information-processing, literacy, numeracy, and problem-solving skills in technologically rich environments may associate with having a STEM degree for various disability populations. This study analyzed the United States nationally representative data from the Programme for the International Assessment of Adult Competencies (PIAAC) to examine associations between adult skills and having a STEM degree for people with and without disabilities. No direct associations were found between adult skills and having a STEM degree for people with learning disabilities or for people without disabilities. These groups’ information processing, literacy, numeracy, and problem-solving skills were not determining factors in STEM degree attainment. However, findings suggest a significant association between problem-solving skills and having a STEM degree for people with visual and/or hearing impairments. Policy implications are discussed.},
 author = {J Jacob Kirksey and Kristin Mansell and Teresa Lansford},
 doi = {10.1177/14782103231177107},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14782103231177107},
 journal = {Policy Futures in Education},
 number = {3},
 pages = {427–453},
 title = {Literacy, numeracy, and problem-solving skills of adults with disabilities in STEM fields},
 url = {https://doi-org.crai.referencistas.com/10.1177/14782103231177107},
 volume = {22},
 year = {2024j}
}

@article{doi:10.1177/14782103231178069,
 abstract = {While a small number of school districts across the United States are well into the process of implementing system-wide computer science education (CSed), most districts are only just getting started. But what does it look like to “get started” on CSed for a whole district? This manuscript presents a single case study of a district’s process of initiating their CS instructional initiative, highlighting a distinct set of instructional leadership practices and the institutional conditions they were responding to. Early implementation research around CSed shows that in some districts, leadership practices are less often the focus of early activities. This study sheds light on what such leadership practices can look like in the early stages of a district’s CSed initiative. Our analysis, based on qualitative data collected longitudinally over 18 months of the district’s work, identified eight intertwined leadership practices that aimed to support instructional coherence, and in our findings, we share a narrative of the district’s initiation of its CS initiative around them. The case begins with the (1) initial leadership team formation and details how that team engaged in (2) content-specific instructional capacity building for its members and (3) sensemaking of ideas around CS with their relationship to existing district activities. It moves on to the team’s (4) development of an instructional vision and an (5) associated implementation strategy, which fed into processes of (6) sensegiving to foster buy-in among teachers, and providing encouragement to engage in (7) instructional piloting. Finally, leaders engaged in (8) landscape analysis activities in order to understand existing district resources and teacher perceptions related to CS. Throughout the case, we highlight the motivations behind these practices, what resources they drew on, intersections, and dependencies among them. We close our analysis exploring a number of tensions and unintended consequences associated with these leadership activities.},
 author = {Rafi Santo and Leigh Ann DeLyser and June Ahn},
 doi = {10.1177/14782103231178069},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/14782103231178069},
 journal = {Policy Futures in Education},
 number = {0},
 pages = {14782103231178068},
 title = {Booting the system: Leadership practices for initiating and infrastructuring district-wide computer science instructional programs},
 url = {https://doi-org.crai.referencistas.com/10.1177/14782103231178069},
 volume = {0},
 year = {2023l}
}

@article{doi:10.1177/1478422X241262625,
 abstract = {Cathodic protection (CP) is widely used to mitigate corrosion and protect the substrate. However, sacrificial anodes are often undersized or improperly positioned. This study systematically investigates how anode design factors affect the CP effectiveness through computational modelling and double-objective optimisation techniques around the optimum location and dimension of anodes. State-of-the-art algorithms including the Monte-Carlo, Nelder–Mead, co-ordinate search, constrained optimisation by linear approximation (COBYLA) and bound optimisation by quadratic approximation (BOBYQA) are employed to optimise anode locations and genetic algorithm is utilised for optimising anode dimension and the objective is maximising the current output and minimising the cost. In the current study, the BOBYQA technique proved efficient in reaching the optimal response at the appropriate time. The findings indicate that in double-objective (cost and potential) optimisation of 14.5 kg anode, minimising anode radius (from 6.5 cm to 5.4 cm) and optimising the position boosts the current output by up to 15.5%. Refined anode geometries increase the average structure potentials (806 → 822 mV) by over 15 mV, translating to extended service lifetimes. In the comparison of single (cost) and double (cost and potential)-objective optimisation, about 15% more current reaches the structure and causes about 2% less anode mass loss for the same anode dimensions. Also, the results show the 7.7 kg anode performs better than the 14.5 kg anode and so output current and the percentage of current that reaches the structure of the 7.7 kg and 14.5 kg are 0.66 A and 0.46 A and 66% and 60%, respectively.},
 author = {Mohammad Javad Shirshahi and Seyed Farshid Chini and Peyman Taheri and Abraham Mansouri},
 doi = {10.1177/1478422X241262625},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1478422X241262625},
 journal = {Corrosion Engineering, Science and Technology},
 number = {0},
 pages = {1478422X241262625},
 title = {Computational optimisation and modelling of sacrificial anode placement and dimension for maximising the corrosion prevention of screw piles},
 url = {https://doi-org.crai.referencistas.com/10.1177/1478422X241262625},
 volume = {0},
 year = {2024q}
}

@article{doi:10.1177/1525107115623505,
 abstract = {This article explores the conditions and assumptions under which it is possible to use National Incident-Based Reporting System (NIBRS) in lifetime crime computations, particularly for nonfatal violent crimes. We describe methods for using NIBRS to study lifetime risk for a variety of crimes and show how researchers and policy makers can apply these methods using readily available software such as Microsoft Excel. Finally, we demonstrate in two different studies how NIBRS can be used to estimate lifetime risk at the state and national levels. In doing so, we introduce the concept of the “average person” in each age–sex–race grouping to calculate the risk of victimization for this hypothetical person only.},
 author = {Yoshio Akiyama and James J. Nolan and Karen G. Weiss and Stacia Gilliard-Matthews},
 doi = {10.1177/1525107115623505},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1525107115623505},
 journal = {Justice Research and Policy},
 number = {2},
 pages = {129–146},
 title = {Lifetime Likelihood Computations With NIBRS},
 url = {https://doi-org.crai.referencistas.com/10.1177/1525107115623505},
 volume = {16},
 year = {2015a}
}

@article{doi:10.1177/1526602815615821,
 author = {Barry J. Doyle and Karol Miller and David E. Newby and Peter R. Hoskins},
 doi = {10.1177/1526602815615821},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1526602815615821},
 journal = {Journal of Endovascular Therapy},
 note = {PMID:26763259},
 number = {1},
 pages = {121–124},
 title = {Commentary: Computational Biomechanics–Based Rupture Prediction of Abdominal Aortic Aneurysms},
 url = {https://doi-org.crai.referencistas.com/10.1177/1526602815615821},
 volume = {23},
 year = {2016c}
}

@article{doi:10.1177/1526602819834713,
 abstract = {Purpose: To use computational simulations to compare the hemodynamic characteristics of a classic bifurcated stent-graft to an equally long endograft design with “dog bone”–shaped limbs (DB), which have large diameter proximal and distal ends and significant narrowing at the midsection to accommodate aneurysms with an extremely narrow bifurcation. Materials and Methods: A 3-dimensional model was constructed using commercially available validated software. Inlet and outlet diameters were 28 and 14 mm, respectively. The total length of both models was kept constant to 180 mm, but the main body of the DB model was 20 mm shorter than the bifurcated endograft. The iliac limbs of the DB model had a 9-mm stenosis over a 30-mm segmental length in the midsection. Flow was quantified by time-averaged wall shear stress, oscillatory shear index (OSI), and relative residence time (RRT). The displacement forces in newtons (N) and maximum wall shear stress (WSS) in pascals (Pa) were compared during a cardiac cycle at 3 segments (main body, bifurcation, and iliac limbs) of both models with computational fluid dynamics analysis. Results: The DB accommodation was associated with higher forces at the main body (range 3.15–4.9 Ν) compared with the classic configuration (1.56–2.34 N). On the contrary, the forces at the bifurcation (3.81–5.98 vs 3.76–5.54 N) and at the iliac limbs (0.34–0.85 vs 0.49–0.74 N) were comparable for both models. Accordingly, maximum WSS was detected at the iliac sites for both models throughout the cardiac cycle. The highest values were detected at peak systole and equaled 26.6 and 12 Pa for the DB and bifurcated configurations, respectively. The narrow segments in the DB model displayed high stress values but low OSI and very low RRT. Conclusion: The DB accommodation seems to correlate with higher displacement forces at the main body and higher stresses at the iliac limbs. Consequently, regular imaging follow-up of the DB design deems necessary to delineate its mid- and long-term clinical performance.},
 author = {Efstratios Georgakarakos and Antonios Xenakis and George S. Georgiadis},
 doi = {10.1177/1526602819834713},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1526602819834713},
 journal = {Journal of Endovascular Therapy},
 note = {PMID:30898071},
 number = {2},
 pages = {250–257},
 title = {Computational Comparison Between a Classic Bifurcated Endograft and a Customized Model With “Dog Bone”–Shaped Limbs},
 url = {https://doi-org.crai.referencistas.com/10.1177/1526602819834713},
 volume = {26},
 year = {2019k}
}

@article{doi:10.1177/1532708616634734,
 abstract = {This article presents a “top ten+ list” of lessons learned from the ontological turn. Across various terms and insistences, the article surveys theories of social change, the subject and agency, the canons constructed, the methodologies materialized, the “more and other than reflexivity” researcher subjectivity endorsed, and the edges of both policy analysis and quantitative research that already instantiate such lessons. It concludes with a meditation on how to think this latest turn within something other than a temporality of successor regimes, end-isms, and apocalyptic breaks.},
 author = {Patti Lather},
 doi = {10.1177/1532708616634734},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1532708616634734},
 journal = {Cultural Studies ↔ Critical Methodologies},
 number = {2},
 pages = {125–131},
 title = {Top Ten+ List: (Re)Thinking Ontology in (Post)Qualitative Research},
 url = {https://doi-org.crai.referencistas.com/10.1177/1532708616634734},
 volume = {16},
 year = {2016j}
}

@article{doi:10.1177/1532708616655759,
 author = {Elizabeth de Freitas and Ezekiel Dixon-Román and Patti Lather},
 doi = {10.1177/1532708616655759},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1532708616655759},
 journal = {Cultural Studies ↔ Critical Methodologies},
 number = {5},
 pages = {431–434},
 title = {Alternative Ontologies of Number: Rethinking the Quantitative in Computational Culture},
 url = {https://doi-org.crai.referencistas.com/10.1177/1532708616655759},
 volume = {16},
 year = {2016d}
}

@article{doi:10.1177/1532708616655765,
 abstract = {This article reflects on how the ingression of computation in culture has not only transformed media into algorithmic devices but has also, more importantly, led to the automation of the most precious faculty of the human, namely, reasoning. This article problematizes the tout court refusal of algorithmic thinking as thinking and suggests that we are witnessing the advance of a dynamic form of automated reasoning, exposing the limits of the critical approach toward the calculative. The article points out that the alliance between algorithmic automation and the digital infrastructure of neoliberalism is not without significance, but attention must be paid to the specific posthuman form of cognitive capitalism.},
 author = {Luciana Parisi},
 doi = {10.1177/1532708616655765},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1532708616655765},
 journal = {Cultural Studies ↔ Critical Methodologies},
 number = {5},
 pages = {471–481},
 title = {Automated Thinking and the Limits of Reason},
 url = {https://doi-org.crai.referencistas.com/10.1177/1532708616655765},
 volume = {16},
 year = {2016i}
}

@article{doi:10.1177/1532708616655771,
 abstract = {Re-emphasizing our agenda for this special issue as marking the shift from epistemological to ontological concerns in social science inquiry, Patti Lather locates it in challenging the orthodoxies of both positivist and critical approaches to the calculative, computational thinking and the limits of reason. With a focus on an escape from psychometrics in education research, she grounds her remarks in the context of the wider terrain of the possibilities of quantification for cultural studies and (post)critical inquiry.},
 author = {Patti Lather},
 doi = {10.1177/1532708616655771},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1532708616655771},
 journal = {Cultural Studies ↔ Critical Methodologies},
 number = {5},
 pages = {502–505},
 title = {Post-Face: Cultural Studies of Numeracy},
 url = {https://doi-org.crai.referencistas.com/10.1177/1532708616655771},
 volume = {16},
 year = {2016n}
}

@article{doi:10.1177/1532708617750178,
 abstract = {This article reimagines the quantified self within the context of Black feminist technologies. Bringing computation and autoethnographic methods together using a methodology I call computational digital autoethnography, I harvest my social media data to create a corpus for analysis. I apply topic modeling to these data to uncover themes that are connected with broader societal issues affecting African American women. Applying a computational autoethnographic approach to a researcher’s own digitized data allows for yet another dimension of mixed-methods research. This radical intervention has the potential to transform the social sciences by bringing together two seemingly divergent methodological approaches in service to Black feminist ways of knowing.},
 author = {Nicole Marie Brown},
 doi = {10.1177/1532708617750178},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1532708617750178},
 journal = {Cultural Studies ↔ Critical Methodologies},
 number = {1},
 pages = {55–67},
 title = {Methodological Cyborg as Black Feminist Technology: Constructing the Social Self Using Computational Digital Autoethnography and Social Media},
 url = {https://doi-org.crai.referencistas.com/10.1177/1532708617750178},
 volume = {19},
 year = {2019b}
}

@article{doi:10.1177/1533033818762207,
 abstract = {Background: Diverse thermal ablative therapies are currently in use for the treatment of cancer. Commonly applied with the intent to cure, these ablative therapies are providing promising success rates similar to and often exceeding “gold standard” approaches. Cancer-curing prospects may be enhanced by deeper understanding of thermal effects on cancer cells and the hosting tissue, including the molecular mechanisms of cancer cell mutations, which enable resistance to therapy. Furthermore, thermal ablative therapies may benefit from recent developments in computer hardware and computation tools for planning, monitoring, visualization, and education. Methods: Recent discoveries in cancer cell resistance to destruction by apoptosis, autophagy, and necrosis are now providing an understanding of the strategies used by cancer cells to avoid destruction by immunologic surveillance. Further, these discoveries are now providing insight into the success of the diverse types of ablative therapies utilized in the clinical arena today and into how they directly and indirectly overcome many of the cancers’ defensive strategies. Additionally, the manner in which minimally invasive thermal therapy is enabled by imaging, which facilitates anatomical features reconstruction, insertion guidance of thermal probes, and strategic placement of thermal sensors, plays a critical role in the delivery of effective ablative treatment. Results: The thermal techniques discussed include radiofrequency, microwave, high-intensity focused ultrasound, laser, and cryosurgery. Also discussed is the development of thermal adjunctive therapies—the combination of drug and thermal treatments—which provide new and more effective combinatorial physical and molecular-based approaches for treating various cancers. Finally, advanced computational and planning tools are also discussed. Conclusion: This review lays out the various molecular adaptive mechanisms—the hallmarks of cancer—responsible for therapeutic resistance, on one hand, and how various ablative therapies, including both heating- and freezing-based strategies, overcome many of cancer’s defenses, on the other hand, thereby enhancing the potential for curative approaches for various cancers.},
 author = {John M. Baust and Yoed Rabin and Thomas J. Polascik and Kimberly L. Santucci and Kristi K. Snyder and Robert G. Van Buskirk and John G. Baust},
 doi = {10.1177/1533033818762207},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1533033818762207},
 journal = {Technology in Cancer Research & Treatment},
 note = {PMID:29566612},
 number = { },
 pages = {1533033818762207},
 title = {Defeating Cancers’ Adaptive Defensive Strategies Using Thermal Therapies: Examining Cancer’s Therapeutic Resistance, Ablative, and Computational Modeling Strategies as a means for Improving Therapeutic Outcome},
 url = {https://doi-org.crai.referencistas.com/10.1177/1533033818762207},
 volume = {17},
 year = {2018a}
}

@article{doi:10.1177/1533034618766792,
 abstract = {Interstitial photodynamic therapy has shown promising results in the treatment of locally advanced head and neck cancer. In this therapy, systemic administration of a light-sensitive drug is followed by insertion of multiple laser fibers to illuminate the tumor and its margins. Image-based pretreatment planning is employed in order to deliver a sufficient light dose to the complex locally advanced head-and-neck cancer anatomy, in order to meet clinical requirements. Unfortunately, the tumor may deform between pretreatment imaging for the purpose of planning and intraoperative imaging when the plan is executed. Tumor deformation may result from the mechanical forces applied by the light fibers and variation of the patient’s posture. Pretreatment planning is frequently done with the assistance of computed tomography or magnetic resonance imaging in an outpatient suite, while treatment monitoring and control typically uses ultrasound imaging due to considerations of costs and availability in the operation room. This article presents a computational method designed to bridge the gap between the 2 imaging events by taking a tumor geometry, reconstructed during preplanning, and by following the displacement of fiducial markers, which are initially placed during the preplanning procedure. The deformed tumor shape is predicted by solving an inverse problem, seeking for the forces that would have resulted in the corresponding fiducial marker displacements. The computational method is studied on spheres of variable sizes and demonstrated on computed tomography reconstructed locally advanced head and neck cancer model. Results of this study demonstrate an average error of less than 1 mm in predicting the deformed tumor shape, where 1 mm is typically the order of uncertainty in distance measurements using magnetic resonance imaging or computed tomography imaging and high-quality ultrasound imaging. This study further demonstrates that the deformed shape can be calculated in a few seconds, making the proposed method clinically relevant.},
 author = {Ye Han and Emily Oakley and Gal Shafirstein and Yoed Rabin and Levent Burak Kara},
 doi = {10.1177/1533034618766792},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1533034618766792},
 journal = {Technology in Cancer Research & Treatment},
 note = {PMID:29658392},
 number = { },
 pages = {1533034618766792},
 title = {Reconstruction of a Deformed Tumor Based on Fiducial Marker Registration: A Computational Feasibility Study},
 url = {https://doi-org.crai.referencistas.com/10.1177/1533034618766792},
 volume = {17},
 year = {2018j}
}

@article{doi:10.1177/153450849201700205,
 abstract = {Traditionally, students have demonstrated substantial difficulties in the area of problem solving. Their strategies appear to be limited as they look for key words or rely on gimmicks to assist in the solution of word problems. The process is mindless, inefficient, and ineffective. This area of weakness is especially prevalent in the mildly handicapped population. The importance of problem-solving skills for these students is critical, as they engage not only in mathematical decisions but as they make functional decisions in their daily lives. A sequential hierarchy is presented that establishes a process for students to follow when confronted with problem-solving issues. The step-by-step process enhances student understanding and control. Problem solving becomes planned and deliberate. Implementing the SOLVE strategies enables the teacher to assess and teach the skills and improves students’ success rates.},
 author = {Brian E. Enright and Susan A. Beattie},
 doi = {10.1177/153450849201700205},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/153450849201700205},
 journal = {Diagnostique},
 number = {2},
 pages = {137–144},
 title = {Assessing Critical Thinking in Mathematics},
 url = {https://doi-org.crai.referencistas.com/10.1177/153450849201700205},
 volume = {17},
 year = {1992h}
}

@article{doi:10.1177/1535370216636722,
 abstract = {Children with sickle cell anemia (SCA) have a high incidence of strokes, and transcranial Doppler (TCD) identifies at-risk patients by measuring blood velocities in large intracerebral arteries; time-averaged mean velocities greater than 200 cm/s confer high stroke risk and warrant therapeutic intervention with blood transfusions. Our objective was to use computational fluid dynamics to alter fluid and artery wall properties, to simulate scenarios causative of significantly elevated arterial blood velocities. Two-dimensional simulations were created and increasing percent stenoses were created in silico, with their locations varied among middle cerebral artery (MCA), internal carotid artery (ICA), and anterior cerebral artery (ACA). Stenoses placed in the MCA, ICA, or ACA generated local increases in velocity, but not sufficient to reach magnitudes > 200 cm/s, even up to 75% stenosis. Three-dimensional reconstructions of the MCA, ICA, and ACA from children with SCA were generated from magnetic resonance angiograms. Using finite element method, blood flow was simulated with realistic velocity waveforms to the ICA inlet. Three-dimensional reconstructions revealed an uneven, internal arterial wall surface in children with SCA and higher mean velocities in the MCA up to 145 cm/s compared to non-SCA reconstructions. There were also greater areas of flow recirculation and larger regions of low wall shear stress. Taken together, these bumps on the internal wall of the cerebral arteries could create local flow disturbances that, in aggregate, could elevate blood velocities in SCA. Identifying cellular causes of these microstructures as adhered blood cells or luminal narrowing due to endothelial hyperplasia induced by disturbed flow would provide new targets to treat children with SCA. The preliminary qualitative results provided here point out the critical role of 3D reconstruction of patient-specific vascular geometries and provide qualitative insight to complex interplay between vascular geometry and rheological properties possibly altered by SCA.},
 author = {Christian P Rivera and Alessandro Veneziani and Russell E Ware and Manu O Platt},
 doi = {10.1177/1535370216636722},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1535370216636722},
 journal = {Experimental Biology and Medicine},
 number = {7},
 pages = {755–765},
 title = {Original Research: Sickle cell anemia and pediatric strokes: Computational fluid dynamics analysis in the middle cerebral artery},
 url = {https://doi-org.crai.referencistas.com/10.1177/1535370216636722},
 volume = {241},
 year = {2016n}
}

@article{doi:10.1177/1535370216689828,
 abstract = {Antibiotic resistance has become a serious global concern, and the discovery of antimicrobial herbal constituents may provide valuable solutions to overcome the problem. In this study, the effects of therapies combining antibiotics and four medicinal herbs on methicillin-resistant Staphylococcus aureus (MRSA) were investigated. Specifically, the synergistic effects of Magnolia officinalis, Verbena officinalis, Momordica charantia, and Daphne genkwa in combination with oxacillin or gentamicin against methicillin-resistant (ATCC43300) and methicillin-susceptible (ATCC25923) S. aureus were examined. In vitro susceptibility and synergistic testing were performed to measure the minimum inhibitory concentration and fractional inhibitory concentration (FIC) index of the antibiotics and medicinal herbs against MRSA and methicillin-susceptible S. aureus. To identify the active constituents in producing these synergistic effects, in silico molecular docking was used to investigate the binding affinities of 139 constituents of the four herbs to the two common MRSA inhibitory targets, penicillin binding proteins 2a (PBP2a) and 4 (PBP4). The physicochemical and absorption, distribution, metabolism, and excretion properties and drug safety profiles of these compounds were also analyzed. D. genkwa extract potentiated the antibacterial effects of oxacillin against MRSA, as indicated by an FIC index value of 0.375. M. officinalis and V. officinalis produced partial synergistic effects when combined with oxacillin, whereas M. charantia was found to have no beneficial effects in inhibiting MRSA. Overall, tiliroside, pinoresinol, magnatriol B, and momorcharaside B were predicted to be PBP2a or PBP4 inhibitors with good drug-like properties. This study identifies compounds that deserve further investigation with the aim of developing therapeutic agents to modulate the effect of antibiotics on MRSA. Impact statement Antibiotic resistant is a well-known threat to global health and methicillin-resistant Staphylococcus aureus is one of the most significant ones. These resistant bacteria kill thousands of people every year and therefore a new effective antimicrobial treatment is necessary. This study identified the herbs and their associated bioactive ingredients that can potential the effects of current antibiotics. These herbs have long history of human usage in China and have well-defined monograph in the Chinese Pharmacopeia. These indicate their relatively high clinical safety and may have a quicker drug development process than that of a new novel antibiotic. Based on the results of this study, the authors will perform further in vitro and animal studies, aiming to accumulate significant data for the application of clinical trial.},
 author = {Chiu-Fai Kuok and Sai-On Hoi and Chi-Fai Hoi and Chi-Hong Chan and Io-Hong Fong and Cheong-Kei Ngok and Li-Rong Meng and Pedro Fong},
 doi = {10.1177/1535370216689828},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1535370216689828},
 journal = {Experimental Biology and Medicine},
 note = {PMID:28118725},
 number = {7},
 pages = {731–743},
 title = {Synergistic antibacterial effects of herbal extracts and antibiotics on methicillin-resistant Staphylococcus aureus: A computational and experimental study},
 url = {https://doi-org.crai.referencistas.com/10.1177/1535370216689828},
 volume = {242},
 year = {2017g}
}

@article{doi:10.1177/1535370221993422,
 abstract = {G protein-coupled receptors (GPCRs) comprise the most important superfamily of protein targets in current ligand discovery and drug development. GPCRs are integral membrane proteins that play key roles in various cellular signaling processes. Therefore, GPCR signaling pathways are closely associated with numerous diseases, including cancer and several neurological, immunological, and hematological disorders. Computer-aided drug design (CADD) can expedite the process of GPCR drug discovery and potentially reduce the actual cost of research and development. Increasing knowledge of biological structures, as well as improvements on computer power and algorithms, have led to unprecedented use of CADD for the discovery of novel GPCR modulators. Similarly, machine learning approaches are now widely applied in various fields of drug target research. This review briefly summarizes the application of rising CADD methodologies, as well as novel machine learning techniques, in GPCR structural studies and bioligand discovery in the past few years. Recent novel computational strategies and feasible workflows are updated, and representative cases addressing challenging issues on olfactory receptors, biased agonism, and drug-induced cardiotoxic effects are highlighted to provide insights into future GPCR drug discovery.},
 author = {Siyu Zhu and Meixian Wu and Ziwei Huang and Jing An},
 doi = {10.1177/1535370221993422},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1535370221993422},
 journal = {Experimental Biology and Medicine},
 note = {PMID:33641446},
 number = {9},
 pages = {1011–1024},
 title = {Trends in application of advancing computational approaches in GPCR ligand discovery},
 url = {https://doi-org.crai.referencistas.com/10.1177/1535370221993422},
 volume = {246},
 year = {2021t}
}

@article{doi:10.1177/1536504219883850,
 abstract = {Computational sociology leverages new tools and data sources to expand the scope and scale of sociological inquiry. It’s opening up an exciting frontier for sociologists of every stripe—from theorists and ethnographers to experimentalists and survey researchers. It expands the sociological imagination.},
 author = {James Evans and Jacob G. Foster},
 doi = {10.1177/1536504219883850},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1536504219883850},
 journal = {Contexts},
 number = {4},
 pages = {10–15},
 title = {Computation and the Sociological Imagination},
 url = {https://doi-org.crai.referencistas.com/10.1177/1536504219883850},
 volume = {18},
 year = {2019e}
}

@article{doi:10.1177/1536867X1201100401,
 abstract = {This article describes a new command, gformula, that is an implementation of the g-computation procedure. It is used to estimate the causal effect of time-varying exposures on an outcome in the presence of time-varying confounders that are themselves also affected by the exposures. The procedure also addresses the related problem of estimating direct and indirect effects when the causal effect of the exposures on an outcome is mediated by intermediate variables, and in particular when confounders of the mediator–outcome relationships are themselves affected by the exposures. A brief overview of the theory and a description of the command and its options are given, and illustrations using two simulated examples are provided.},
 author = {Rhian M. Daniel and Bianca L. De Stavola and Simon N. Cousens},
 doi = {10.1177/1536867X1201100401},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1536867X1201100401},
 journal = {The Stata Journal},
 number = {4},
 pages = {479–517},
 title = {Gformula: Estimating Causal Effects in the Presence of Time-Varying                     Confounding or Mediation using the G-Computation Formula},
 url = {https://doi-org.crai.referencistas.com/10.1177/1536867X1201100401},
 volume = {11},
 year = {2011d}
}

@article{doi:10.1177/1536867X1301300102,
 abstract = {Thought experiments based on simulation can be used to explain the impact of the chosen study design, statistical analysis strategy, or the sensitivity of results to fellow researchers. In this article, we demonstrate with two examples how to implement quantitative thought experiments in Stata. The first example uses a large-sample approach to study the impact on the estimated effect size of dichotomizing an exposure variable at different values. The second example uses simulations of datasets of realistic size to illustrate the necessity of using sampling fractions as inverse probability weights in statistical analysis for protection against bias in a complex sampling design. We also give a brief outline of the general steps needed for implementing quantitative thought experiments in Stata. We demonstrate how Stata provides programming facilities for conveniently implementing such thought experiments, with the advantage of saving researchers time, speculation, and debate as well as improving communication in interdisciplinary research groups.},
 author = {Theresa Wimberley and Erik Parner and Henrik Støvring},
 doi = {10.1177/1536867X1301300102},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1536867X1301300102},
 journal = {The Stata Journal},
 number = {1},
 pages = {3–20},
 title = {Stata as a Numerical Tool for Scientific Thought Experiments: A Tutorial with Worked Examples},
 url = {https://doi-org.crai.referencistas.com/10.1177/1536867X1301300102},
 volume = {13},
 year = {2013s}
}

@article{doi:10.1177/1536867X1501500104,
 abstract = {Maxima is a free and open-source computer algebra system that can perform symbolic computations such as solving equations, determining derivatives of functions, obtaining Taylor series, and manipulating algebraic expressions. In this article, I present the Maxima Bridge System, which is a collection of software programs that allows Stata to interface with Maxima so that Maxima can be used for symbolic computation to transfer data from Stata to Maxima and to retrieve results from Maxima. The cooperation between Stata and Maxima provides an environment for statistical analysis in which symbolic computation can be easily used together with all the facilities supplied by Stata. In this environment, symbolic computation algorithms can be used to manage the complexity of algebra and calculus, whereas numerical computation can be used when speed matters. I also discuss the software architecture of the Maxima Bridge System, and I present several examples to illustrate how to develop new Stata commands that exploit the capabilities of Maxima.},
 author = {Giovanni L. Lo Magno},
 doi = {10.1177/1536867X1501500104},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1536867X1501500104},
 journal = {The Stata Journal},
 number = {1},
 pages = {45–76},
 title = {More Power through Symbolic Computation: Extending Stata by using the Maxima Computer algebra system},
 url = {https://doi-org.crai.referencistas.com/10.1177/1536867X1501500104},
 volume = {15},
 year = {2015g}
}

@article{doi:10.1177/1538574413503561,
 abstract = {Objectives: To evaluate hemodynamic changes during aneurysmal dilatation in chronic type B aortic dissections compared to hemodynamic parameters in the healthy aorta with the use of computational fluid dynamics (CFD). Methods: True lumen (TL)/false lumen (FL) dimensional changes, changes in total pressure (TP), and wall shear stress (WSS) were evaluated at follow-up (FU) compared to initial examination (IE) with transient CFD simulation with geometries derived from clinical image data and inflow boundary conditions from magnetic resonance images. The TL/FL pressure gradient between ascending and descending aorta (DAo) and maximum WSS at the site of largest dilatation was compared to values for the healthy aorta. Results: Hemodynamic changes at site of largest FL dilatation included 77% WSS reduction and 69% TP reduction. Compared to the healthy aorta, pressure gradient between ascending and DAo was a factor of 1.4 higher in the TL and a factor of 1.5 in the FL and increased at FU (1.6 and 1.7, respectively). Maximum WSS at the site of largest dilatation was a factor of 3 lower than that for the healthy aorta at IE and decreased by more than a factor of 2 at FU. Conclusions: The FL dilatation at FU favorably reduced TP. In contrast, unfavorable increase in pressure gradient between ascending and DAo was observed with higher values than in the healthy aorta. Maximum WSS was reduced at the site of largest dilation compared to healthy aorta.},
 author = {Christof Karmonik and Matthias Müller-Eschner and Sasan Partovi and Philipp Geisbüsch and Maria-Katharina Ganten and Jean Bismuth and Mark G. Davies and Dittmar Böckler and Matthias Loebe and Alan B. Lumsden et al.},
 doi = {10.1177/1538574413503561},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1538574413503561},
 journal = {Vascular and Endovascular Surgery},
 note = {PMID:24048257},
 number = {8},
 pages = {625–631},
 title = {Computational Fluid Dynamics Investigation of Chronic Aortic Dissection Hemodynamics Versus Normal Aorta},
 url = {https://doi-org.crai.referencistas.com/10.1177/1538574413503561},
 volume = {47},
 year = {2013k}
}

@article{doi:10.1177/1538574419867531,
 abstract = {Background-Aim: Limited data exist concerning the fluid dynamic changes induced by endovascular aortic repair with fenestrated and chimney graft modalities in pararenal aneurysms. We aimed to investigate and compare the wall shear stress (WSS) and flow dynamics for the branch vessels before and after endovascular aortic repair with fenestrated and chimney techniques. Methods: Modeling was done for patient specific pararenal aortic aneurysms employing fenestrated and chimney grafts (Materialise Mimics 10.0) before and after the endovascular procedure, using computed tomography scans of patients. Surface and spatial grids were created using the ANSYS CFD meshing software 2019 R2. Assessment of blood flow, streamlines, and WSS before and after aneurysm repair was performed. Results: The endovascular repair with chimney grafts leaded to a 43% to 53% reduction in perfusion in renal arteries. In fenestrated reconstruction, we observed a 15% reduced perfusion in both renal arteries. In both cases, we observed a decrease in the recirculation phenomena of the aorta after endovascular repair. Concerning the grafts of the renal arteries, we observed in both the transverse and longitudinal axes low WSS regions with simultaneous recirculation of the flow 1 cm distal to the ostium sites in both aortic graft models. High WSS regions appeared in the sites of ostium. Conclusions: We observed reduced renal perfusion in chimney grafts compared to fenestrated grafts, probably caused by the long and kinked characteristics of these devices.},
 author = {Konstantinos G. Moulakakis and John Kakisis and Eleni Gonidaki and Andreas M. Lazaris and Sokrates Tsangaris and George Geroulakos and Christos Manopoulos},
 doi = {10.1177/1538574419867531},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1538574419867531},
 journal = {Vascular and Endovascular Surgery},
 note = {PMID:31382837},
 number = {7},
 pages = {572–582},
 title = {Comparison of Fluid Dynamics Variations Between Chimney and Fenestrated Endografts for Pararenal Aneurysms Repair: A Patient Specific Computational Study as Motivation for Clinical Decision-Making},
 url = {https://doi-org.crai.referencistas.com/10.1177/1538574419867531},
 volume = {53},
 year = {2019n}
}

@article{doi:10.1177/154193120004400129,
 abstract = {Although the use of the psychological construct of situational awareness (SA) assists researchers in creating a flight environment that is safer and more predictable, its true potential remains untapped until a valid means of predicting SA a priori becomes available. Previous work proposed a computational model of SA (CSA) that sought to fill that void. The current line of research is aimed at validating that model. The results show that the model accurately predicted SA in a piloted simulation.},
 author = {Mark D. Burdick and R. Jay Shively},
 doi = {10.1177/154193120004400129},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/154193120004400129},
 journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
 number = {1},
 pages = {109–112},
 title = {Evaluation of a Computational Model of Situational Awareness},
 url = {https://doi-org.crai.referencistas.com/10.1177/154193120004400129},
 volume = {44},
 year = {2000c}
}

@article{doi:10.1177/154193120004402124,
 abstract = {The information processing approach traditionally has been the theoretical foundation of mental workload. Computational neurocognitive models are emerging approaches to understanding how the brain performs cognitive functions. Computational complexity refers to the many possibilities and ambiguities intrinsic in the environmental stimuli. These models agree that the brain has limited computational power. Utility and implications of the computational approaches to the understanding of mental workload, especially that of higher-level activities such as strategic control of dynamic multiple-task performance and situation awareness will be explored.},
 author = {Pamela S. Tsang},
 doi = {10.1177/154193120004402124},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/154193120004402124},
 journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
 number = {21},
 pages = {3-468-3–471},
 title = {Mental Workload Beyond Computational Complexity},
 url = {https://doi-org.crai.referencistas.com/10.1177/154193120004402124},
 volume = {44},
 year = {2000o}
}

@article{doi:10.1177/154193120104502607,
 abstract = {Modeling an intelligent adversary has provided great challenges to simulated training realism. Traditional approaches to modeling have relied on rule-based and analytical decision-making models in an attempt to optimize the decision making of an intelligent computer-generated adversary. In order to promote realistic transfer of training in the realm of command and control, the trainee must experience realistic decision-making behavior in the enemy. This means that the enemy must make realistic decisions based on environmental constraints, goals, and intent. The enemy’s decisions must then be reflected by the simulated agents. The Recognition-Primed Decision (RPD) Model is a descriptive model of expert decision making in real-world settings. We have several related projects where the goal is to translate the conceptual RPD Model into a computer model that can simulate realistic expert decision making. In attempting this feat, we have discovered many valuable lessons about modeling cognition and decision making, and about the assumptions and mechanisms underlying the RPD Model. The purpose of this paper is to report those findings.},
 author = {Robert J. B. Hutton and Walter Warwick and Terry Stanard and Patricia L. McDermott and Stacey McIlwaine},
 doi = {10.1177/154193120104502607},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/154193120104502607},
 journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
 number = {26},
 pages = {1833–1837},
 title = {Computational Model of Recognition-Primed Decisions (RPD): Improving Realism in Computer-Generated Forces (CGF)},
 url = {https://doi-org.crai.referencistas.com/10.1177/154193120104502607},
 volume = {45},
 year = {2001h}
}

@article{doi:10.1177/154193120204600358,
 abstract = {Our work with the Argus Prime (Schoelles & Gray, 2001) simulated task environment has uncovered a variety of strategies that subjects use, at least sometimes, during target acquisition. However, it is difficult to determine how well subjects implement these strategies and, if implemented, how much these strategies contribute to overall performance. Recently, we have adopted Byrne and Kirlik’s (2002) cognitive-ecological approach to determine what strategies work best in different task environments. In the work reported here, we took one computational cognitive model and, holding all else constant, swapped in and out alternative strategies for target acquisition. We then ran each of these simulated human users ten times through each of four interface conditions.},
 author = {Wayne D. Gray and Michael J. Schoelles and Christopher W. Myers},
 doi = {10.1177/154193120204600358},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/154193120204600358},
 journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
 number = {3},
 pages = {492–496},
 title = {Computational Cognitive Models Iso Ecologically Optimal Strategies},
 url = {https://doi-org.crai.referencistas.com/10.1177/154193120204600358},
 volume = {46},
 year = {2002h}
}

@article{doi:10.1177/154193120204601730,
 abstract = {A computational model of attention and situation awareness (SA) was developed and used to predict pilot errors in the task of taxiing from runway to terminal. The model incorporates a low-level perception/attention module and a higher-level belief-updating module. Attentional scanning is controlled by bottom-up and top-down processes, with the effectiveness of top-down guidance varying as a function of SA. Information sampled by the low-level module is fed forward to the higher-level module for consolidation within a working memory representation of the pilot’s situation, with the quality of this representation reflecting the pilot’s level of SA. The model was validated by comparing its predictions to the behavior of pilots performing a taxiway simulation. Results indicate that the model successfully predicts the improved performance associated with display augmentations, and provides construct validity regarding the effects of visibility, distraction, and degraded information quality.},
 author = {Jason S. McCarley and Christopher D. Wickens and Juliana Goh and William J. Horrey},
 doi = {10.1177/154193120204601730},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/154193120204601730},
 journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
 number = {17},
 pages = {1669–1673},
 title = {A Computational Model of Attention/Situation Awareness},
 url = {https://doi-org.crai.referencistas.com/10.1177/154193120204601730},
 volume = {46},
 year = {2002o}
}

@article{doi:10.1177/154193120404802313,
 abstract = {This paper describes a computational model of optical flow and demonstrates how it can be used to estimate the navigation velocity of the user’s viewpoint during a virtual reality (VR) simulation. The model takes as input a VR simulation video sequence and outputs the estimated average navigation velocity. When this estimated velocity is integrated into the published cybersickness dose value model (CSDV: So, 1999), the severity level of the cybersickness associated with the VR simulation can be predicted. Results of two simulation studies indicated that the average navigation velocities in lateral and vertical axes can be estimated to within 10% error. The simulations used a virtual environment (VE) with a known single depth. Discussion of an extension of the computational model to accommodate VEs with unknown multiple depths is included. The reported model is consistent with the biological neuro-pathway of the perception of visual motion velocities in humans. The benefits associated with this consistency are discussed.},
 author = {T.T. Ji Jennifer and W.K. Lor Felix and H.Y. So Richard},
 doi = {10.1177/154193120404802313},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/154193120404802313},
 journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
 number = {23},
 pages = {2667–2671},
 title = {Integrating a Computational Model of Optical Flow into the Cybersickness Dose Value Prediction Model},
 url = {https://doi-org.crai.referencistas.com/10.1177/154193120404802313},
 volume = {48},
 year = {2004j}
}

@article{doi:10.1177/154193120504900513,
 abstract = {Despite the importance of software interfaces, trade-offs must be made during development. Value Focused Thinking (VFT) provides an objective methodology that is well suited for handling multi-objective problems such as interface design issues. Facilitated sessions are held with the intended users to elicit what is of importance to the user. These values are characterized and put to a common scale, allowing measurement of their contribution to the overall objective. The resulting hierarchy can be used to guide software development or compare different interface solutions. In this study, a VFT hierarchy was developed with military intelligence analysts to identify what values were desired in an interface for software intended to address a facet of data overload. The prototype interface was evaluated against the hierarchy with these stated values. The process and the hierarchy itself provided insight on where resources should be expended to provide the greatest benefit in the final product.},
 author = {Janet E. Miller and Chris McGee},
 doi = {10.1177/154193120504900513},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/154193120504900513},
 journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
 number = {5},
 pages = {696–700},
 title = {Revealing What’s Important to the User: Value-Focused Thinking},
 url = {https://doi-org.crai.referencistas.com/10.1177/154193120504900513},
 volume = {49},
 year = {2005l}
}

@article{doi:10.1177/154193120705101206,
 abstract = {The objective of this study was to assess the use of a computational cognitive model for describing human performance with an adaptively automated system, with and without advance cueing of control mode transitions. A dual-task piloting simulation was developed to collect human performance data under auditory cueing or no cueing of automated or manual control. GOMSL models for simulating user behavior were constructed based on a theory of increased memory transactions at mode transitions. The models were applied to the same task simulation and scenarios performed by the humans. Comparison of results on human and model output demonstrated the model to be generally descriptive of performance; however, it was not accurate in predicting timing of memory use in preparing for manual control. Interestingly, the human data didn’t reveal differences between cued and no cue trials. A refined GOMSL model was developed by modifying assumptions on the timing and manner of memory use, and considering human parallel processing in dual-task performance. Results revealed the refined model to be more plausible for representing behavior. Computational cognitive modeling appears to be a viable approach to represent operator performance in adaptive systems.},
 author = {Sang-Hwan Kim and David B. Kaber and Carlene M. Perry},
 doi = {10.1177/154193120705101206},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/154193120705101206},
 journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
 number = {12},
 pages = {802–806},
 title = {Computational GOMSL Modeling towards Understanding Cognitive Strategy in Dual-Task Performance with Automation},
 url = {https://doi-org.crai.referencistas.com/10.1177/154193120705101206},
 volume = {51},
 year = {2007f}
}

@article{doi:10.1177/154193120805201603,
 abstract = {Link analysis (LA) method is one of major user interface analysis methods in HCI. However, it seemed that it has several limitations, including negligence of high-frequency, low-link elements in a control interface and underestimation of the total moving distance in real task settings. To improve LA in these limitations, this study developed an algorithm based on the concept of optimizing the total moving distance and the outcome of the algorithm can be quantitatively validated. The findings along with the difficulties faced in this study might provide beneficial reference as well as challenges worth for future studying.},
 author = {Cheng-Jhe Lin and Wanyan Yu and Changxu Wu},
 doi = {10.1177/154193120805201603},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/154193120805201603},
 journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
 number = {16},
 pages = {1112–1116},
 title = {Improving Link Analysis Method in User Interface Design Using a New Computational Optimization Algorithm},
 url = {https://doi-org.crai.referencistas.com/10.1177/154193120805201603},
 volume = {52},
 year = {2008i}
}

@article{doi:10.1177/154193120905300444,
 abstract = {This laboratory experiment shows how time constraint and event severity affect convergent and divergent thinking processes among professional emergency response personnel addressing a simulated emergency situation. Increasing time constraint resulted in fewer options considered, fewer recommendations made, fewer decisions taken, and less favorable decision outcomes. Increasing event severity had a more ambiguous effect, suggesting the need for further study on the effect of this factor alone and in conjunction with time constraint.},
 author = {Yao Hu and David Mendonça},
 doi = {10.1177/154193120905300444},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/154193120905300444},
 journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
 number = {4},
 pages = {374–378},
 title = {Divergent and Convergent Thinking in Emergency Response Organizations},
 url = {https://doi-org.crai.referencistas.com/10.1177/154193120905300444},
 volume = {53},
 year = {2009f}
}

@article{doi:10.1177/1541931218621073,
 abstract = {We demonstrate a set of software tools designed to facilitate computational cognitive modeling of multitasking performance. The Modifiable Multitasking Environment (ModME) offers a flexible, browser-based platform for creating multitasking experiments. Simplified Interfacing for Modeling Cognition–JavaScript (SIMCog-JS) provides communication between the browser-based experiments in ModME and the Java implementation of the ACT-R cognitive architecture. The baseline configuration of these software packages enables an ACT-R model to perform pilot-like multitasking in the modified Multi-Attribute Task Battery, which is implemented as the baseline task available in ModME. We show how this combination facilitates the development of models for assessing multitasking workload. In this demonstration, we will explain the software packages and allow attendees to interact with system elements, particularly the ModME graphical user interfaces. All software is available open source for attendees to try themselves.},
 author = {Leslie M. Blaha and Leif Carlsen and Tim Halverson and Brad Reynolds},
 doi = {10.1177/1541931218621073},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1541931218621073},
 journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
 number = {1},
 pages = {316–320},
 title = {Interfacing the Modifiable Multitasking Environment with ACT-R for Computational Cognitive Modeling of Complex Tasks},
 url = {https://doi-org.crai.referencistas.com/10.1177/1541931218621073},
 volume = {62},
 year = {2018b}
}

@article{doi:10.1177/1541931218621276,
 abstract = {Hospitals are plagued with a multitude of logistical challenges amplified by a time-sensitive and high intensity environment. These conditions have resulted in burnout among both doctors and nurses as they work tirelessly to provide critical care to patients in need. We propose a new machine-learning-powered matching mechanism that manages the surgeon-nurse-patient assignment process in an efficient way that saves time and energy for hospitals, enabling them to focus almost entirely on delivering effective care. Through this design, we show how incorporating artificial intelligence into management systems enables teams of all sizes to meaningfully coordinate in highly chaotic and complex environments.},
 author = {Lorenzo Barberis Canonico and Nathan J. McNeese and Marissa L. Shuffler},
 doi = {10.1177/1541931218621276},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1541931218621276},
 journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
 number = {1},
 pages = {1202–1206},
 title = {Stable Teamwork Marriages in Healthcare: Applying Machine Learning to Surgeon-Nurse-Patient Matching},
 url = {https://doi-org.crai.referencistas.com/10.1177/1541931218621276},
 volume = {62},
 year = {2018b}
}

@article{doi:10.1177/154193127401800524,
 abstract = {The purpose of the 427M Improvement Program is to replace and update the computer, communications, and display facility used to support the North American Air Defense Command (NORAD) and the Aerospace Defense Command in accomplishing their assigned missions, through 1980. The interface between the human operators and the several complete computer systems of the Space Computational Center (SCC) and NORAD is the Interactive Display Console. The SCC/NCS (NORAD Computer System) Graphic Display Console must meet specific Air Force readability, look-angle, accessibility, and maintenance requirements, as well as the restrictive space, weight, and power requirements. Both the SCC and NCS display presentations require extensive use of display vectors to construct maps and backgrounds in addition to presenting the data. The problems of legibility require evaluating the advantages of stroke-generation techniques over the raster-scan technology in producing straight, clearly defined vectors. Also, the system requires a highly reliable, modularly expandable, high-throughput system which would apply to both the SCC and NCS man/machine interface requirements. To effectively maintain the display console hardware, it should be identical for both the SCC and NCS segments; however, the segments must be addressable for different functional operations and displayed information. Thus the operational parameters were assigned to software programs. Functions of the SCC include space catalog maintenance, space sensor system status, space weapons support, system control and support, programming support, and personnel subsystem support. To perform these functions within the allowable time constraints, a highly automated and highly interactive computer system is needed. The Human Factors role in console definition, man/computer interface considerations, and facility layout problems will be discussed in the paper.},
 author = {John H. Van Arsdel},
 doi = {10.1177/154193127401800524},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/154193127401800524},
 journal = {Proceedings of the Human Factors Society Annual Meeting},
 number = {5},
 pages = {634–640},
 title = {Human Factors Role in Equipment Definition and Facility Layout for the Space Computational Center and the Norad Computer System Improvement Program},
 url = {https://doi-org.crai.referencistas.com/10.1177/154193127401800524},
 volume = {18},
 year = {1974t}
}

@article{doi:10.1177/154405910308200317,
 abstract = {There has been limited use of ceramic materials for all-ceramic posterior bridges. Major reasons are the low strength, the strength scatter, and the time-dependent strength decrease of ceramics due to slow crack growth. The objective of this study was to predict the long-term failure probability and loading capability of all-ceramic bridges (Empress 1, Empress 2, In-Ceram Alumina, and ZrO2) by computational techniques. The lifetimes of different bridge model designs were predicted by means of the NASA post-processor CARES. Bridges made of zirconia showed a very high mechanical long-term reliability. Empress I and InCeram Alumina seem to be insufficient as posterior bridge materials based on this prediction. The lifetime of the all-ceramic bridges can be significantly increased by improving the design in the connector area. We conclude that computational techniques can help to judge a ceramic material and a specific ceramic bridge design with respect to mechanical reliability before clinical use.},
 author = {H Fischer and M Weber and R Marx},
 doi = {10.1177/154405910308200317},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/154405910308200317},
 journal = {Journal of Dental Research},
 note = {PMID:12598556},
 number = {3},
 pages = {238–242},
 title = {Lifetime Prediction of All-ceramic Bridges by Computational Methods},
 url = {https://doi-org.crai.referencistas.com/10.1177/154405910308200317},
 volume = {82},
 year = {2003h}
}

@article{doi:10.1177/1545968311410940,
 abstract = {This chapter outlines the basic computational, anatomical, and physiological (CAP) principles underlying upper-limb actions, such as reaching for a cup and grasping it or picking up a key, inserting it into a lock, and turning it.},
 author = {Scott H. Frey and Leonardo Fogassi and Scott Grafton and Nathalie Picard and John C. Rothwell and Nicolas Schweighofer and Maurizio Corbetta and Susan M. Fitzpatrick},
 doi = {10.1177/1545968311410940},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1545968311410940},
 journal = {Neurorehabilitation and Neural Repair},
 note = {PMID:21613534},
 number = {5_suppl},
 pages = {6S-20S},
 title = {Neurological Principles and Rehabilitation of Action Disorders: Computation, Anatomy, and Physiology (CAP) Model},
 url = {https://doi-org.crai.referencistas.com/10.1177/1545968311410940},
 volume = {25},
 year = {2011g}
}

@article{doi:10.1177/1548051817750537,
 abstract = {Leaders have been classified as having charismatic, ideological, or pragmatic (CIP) leadership styles, each characterized by distinct patterns in cognition and interaction. Although each CIP style has been shown to facilitate certain aspects of the creative process for followers, questions remain regarding the impact of leadership style on overall follower creative performance. One factor likely to influence this relationship is leader distance, composed of the physical distance, perceived social distance, and perceived task interaction among leaders and followers. Past research has also emphasized the role of leaders’ mental models as they relate to follower performance. Less understood, however, is how the mental models of followers may affect this process. Using the CIP model of leadership, this study explores leader distance and leader–follower mental model congruence on follower creative performance. Results indicated that while leadership style does not directly influence follower creativity, it interacts with leader distance to shape creative outcomes. Results further indicated that while general mental model congruence is not predictive, alignment on specific mental model dimensions contributes to enhanced creative performance among followers. Implications and future research directions are discussed.},
 author = {Jennifer A. Griffith and Carter Gibson and Kelsey Medeiros and Alexandra MacDougall and Jay Hardy and Michael D. Mumford},
 doi = {10.1177/1548051817750537},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1548051817750537},
 journal = {Journal of Leadership & Organizational Studies},
 number = {2},
 pages = {153–170},
 title = {Are You Thinking What I’m Thinking?: The Influence of Leader Style, Distance, and Leader–Follower Mental Model Congruence on Creative Performance},
 url = {https://doi-org.crai.referencistas.com/10.1177/1548051817750537},
 volume = {25},
 year = {2018h}
}

@article{doi:10.1177/1548512911430967,
 abstract = {A chaff cloud is an electronic countermeasure to radio frequency emitters. The cloud immerses a protected entity in a multitude of false targets by reradiating incident electromagnetic energy from millions of thin aluminized fibers, foil strips, or elements printed with conductive ink. The elements are cut to form resonant structures to match the principle threat frequencies, making them an effective reradiator. Empirically testing chaff clouds is technically challenging, costly, and time consuming. With an increased emphasis on chaff research comes the need to characterize cloud radar cross-section performance to expand existing knowledge and explore development opportunities for future technologies. This paper describes the computational methodology and results for analyzing standard dipole chaff clouds and Koch snowflakes as a possible new chaff element.},
 author = {Ioannis Xenidis and Jason P Dauby and Maung H Myat and David W Scholfield},
 doi = {10.1177/1548512911430967},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1548512911430967},
 journal = {The Journal of Defense Modeling and Simulation},
 number = {4},
 pages = {361–368},
 title = {A computational approach to modeling chaff clouds consisting of fractal particles},
 url = {https://doi-org.crai.referencistas.com/10.1177/1548512911430967},
 volume = {9},
 year = {2012t}
}

@article{doi:10.1177/1548512916679038,
 abstract = {Climate change has the potential to displace large populations in many parts of the developed and developing world. Understanding why, how, and when environmental migrants decide to move is critical to successful strategic planning within organizations tasked with helping the affected groups, and mitigating their systemic impacts. One way to support planning is through the employment of computational modeling techniques. Models can provide a window into possible futures, allowing planners and decision makers to test different scenarios in order to understand what might happen. While modeling is a powerful tool, it presents both opportunities and challenges. This paper builds a foundation for the broader community of model consumers and developers by: providing an overview of pertinent climate-induced migration research, describing some different types of models and how to select the most relevant one(s), highlighting three perspectives on obtaining data to use in said model(s), and the consequences associated with each. It concludes with two case studies based on recent research that illustrate what can happen when ambitious modeling efforts are undertaken without sufficient planning, oversight, and interdisciplinary collaboration. We hope that the broader community can learn from our experiences and apply this knowledge to their own modeling research efforts.},
 author = {Charlotte Till and Jamie Haverkamp and Devin White and Budhendra Bhaduri},
 doi = {10.1177/1548512916679038},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1548512916679038},
 journal = {The Journal of Defense Modeling and Simulation},
 number = {4},
 pages = {415–435},
 title = {Understanding climate-induced migration through computational modeling: A critical overview with guidance for future efforts},
 url = {https://doi-org.crai.referencistas.com/10.1177/1548512916679038},
 volume = {15},
 year = {2018q}
}

@article{doi:10.1177/1548512916681085,
 abstract = {Representing causal social science knowledge in models is difficult: much of the best knowledge is qualitative and ambiguously conditional, unlike the knowledge in “physics models.” This paper describes a stream of RAND research that began with qualitative models providing a structured depiction of casual factors creating effects. That has subsequently been extended to an unusual kind of uncertainty sensitive computational modeling that enables exploratory reasoning and analysis. We illustrate the approach with applications to counterterrorism, detection of terrorists, and nuclear crises. We believe that the approach will complement other approaches that can reflect social science phenomena [see other papers in this special issue of JDMS] and that the approach has broad potential within and beyond the national security domain. We also believe that it has the potential to inform empirical work—encouraging a transition from the step-by-step empirical testing of simple discrete hypotheses to the testing and refinement of more comprehensive causal models.},
 author = {Paul K Davis and Angela O’Mahony},
 doi = {10.1177/1548512916681085},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1548512916681085},
 journal = {The Journal of Defense Modeling and Simulation},
 number = {1},
 pages = {57–78},
 title = {Representing qualitative social science in computational models to aid reasoning under uncertainty: National security examples},
 url = {https://doi-org.crai.referencistas.com/10.1177/1548512916681085},
 volume = {14},
 year = {2017c}
}

@article{doi:10.1177/1548512916681672,
 abstract = {a. In 1973, the Department of Defense (DoD) created the Office of Net Assessment (ONA) with a charter and unique approach to strategic analysis. This approach questioned the suitability of systems analysis to assess long-term, dynamic competition between complex military organizations, and turned to more qualitative methods as analytic alternatives. Developments in computing technology and modeling methods over the last two decades, most notably agent-based modeling (ABM), provide new opportunities to address the central analytic questions that motivated the original development of net assessment as a distinctive practice of strategic analysis. By employing ABM to simulate and analyze the behavior of strategic, adaptive, boundedly rational actors, which have previously frustrated mathematical analysis, a new generation of computational models can provide opportunities to add rigor to net assessment.},
 author = {Aaron B Frank},
 doi = {10.1177/1548512916681672},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1548512916681672},
 journal = {The Journal of Defense Modeling and Simulation},
 number = {1},
 pages = {79–94},
 title = {Toward computational net assessment},
 url = {https://doi-org.crai.referencistas.com/10.1177/1548512916681672},
 volume = {14},
 year = {2017f}
}

@article{doi:10.1177/15485129211073612,
 abstract = {Insurgency conflicts pose significant challenges to societies globally. The increase of insurgency conflicts creates a need to understand how insurgencies arise, and to identify societal drivers of insurgencies or effective strategies to counter them. In this paper, we analyze the contributions of computational modeling methods for the analysis of insurgent conflicts. We formalize a specific literature-based analysis framework using the identified key factors and drivers, which enables the evaluation of specific models in this domain. Through a systematic literature search, we identify 64 computational models to apply our framework. We highlight the development and contributions of various methodologies through an in-depth analysis of 13 high-quality models. The evaluation of these computational models revealed promising directions and future topics to design specific simulation models for all identified factors. In addition, our analysis revealed specific pitfalls concerning validity issues for each of the modeling methods.},
 author = {Koen van der Zwet and Ana I Barros and Tom M van Engers and Peter M A Sloot},
 doi = {10.1177/15485129211073612},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/15485129211073612},
 journal = {The Journal of Defense Modeling and Simulation},
 number = {3},
 pages = {333–350},
 title = {Promises and pitfalls of computational modelling for insurgency conflicts},
 url = {https://doi-org.crai.referencistas.com/10.1177/15485129211073612},
 volume = {20},
 year = {2023x}
}

@article{doi:10.1177/155005940904000308,
 abstract = {The aims of this study are to clarify whether patients with obstructive sleep apnea syndrome (OSAS) have a decline in verbally or visually-based cognitive abilities and whether the possible decline is related to particular sleep depth changes. In addition, the effect of continuous positive airway pressure (CPAP) on the possible changes is investigated. Fifteen OSAS patients and 15 healthy controls joined two full-night polysomnographies, including a computational measure of deep sleep percentage (DS%) bilaterally from the frontal, central and occipital channels, and a neuropsychological assessment. After a 6-month CPAP the patients underwent one more full-night polysomnography with computational DS% analysis and a neuropsychological assessment. At the baseline, the OSAS patients had poorer performance in the Picture Completion, in the Digit Symbol and in copying the Rey-Osterrieth Complex Figure Test (ROCFT) compared to the controls. The patients also showed reduced DS% in all 6 electrographic (EEG) channels compared to controls. The patients had an inter-hemispheric difference showing less deep sleep in the right hemisphere than in the left hemisphere both frontopolarly and centrally, while the controls showed this inter-hemispheric difference only frontopolarly. After CPAP the patients still had poorer performance in the Picture Completion and in the ROCFT. The patients continued to show reduced DS% in all 3 channels of the right hemisphere and occipitally in the left hemisphere, also the inter-hemispheric difference frontopolarly and centrally remained. OSAS patients have mild visually based cognitive dysfunction and reduced amount of deep sleep in the right hemisphere even after CPAP.},
 author = {Tiia Saunamäki and Mervi Jehkonen and Eero Huupponen and Olli Polo and Sari-Leena Himanen},
 doi = {10.1177/155005940904000308},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/155005940904000308},
 journal = {Clinical EEG and Neuroscience},
 note = {PMID:19715178},
 number = {3},
 pages = {162–167},
 title = {Visual Dysfunction and Computational Sleep Depth Changes in Obstructive Sleep Apnea Syndrome},
 url = {https://doi-org.crai.referencistas.com/10.1177/155005940904000308},
 volume = {40},
 year = {2009o}
}

@article{doi:10.1177/15501329221113508,
 abstract = {Cognitive radios are expected to play an important role in capturing the constantly growing traffic interest on remote networks. To improve the usage of the radio range, a cognitive radio hub detects the weather, evaluates the open-air qualities, and then makes certain decisions and distributes the executives’ space assets. The cognitive radio works in tandem with artificial intelligence and artificial intelligence methodologies to provide a flexible and intelligent allocation for continuous production cycles. The purpose is to provide a single source of information in the form of a survey research to enable academics better understand how artificial intelligence methodologies, such as fuzzy logics, genetic algorithms, and artificial neural networks, are used to various cognitive radio systems. The various artificial intelligence approaches used in cognitive radio engines to improve cognition capabilities in cognitive radio networks are examined in this study. Computerized reasoning approaches, such as fuzzy logic, evolutionary algorithms, and artificial neural networks, are used in the writing audit. This topic also covers cognitive radio network implementation and the typical learning challenges that arise in cognitive radio systems.},
 author = {Ahmed Alkhayyat and Firas Abedi and Ashish Bagwari and Pooja Joshi and Haider Mahmood Jawad and Sarmad Nozad Mahmood and Yousif K Yousif},
 doi = {10.1177/15501329221113508},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/15501329221113508},
 journal = {International Journal of Distributed Sensor Networks},
 number = {7},
 pages = {15501329221113508},
 title = {Fuzzy logic, genetic algorithms, and artificial neural networks applied to cognitive radio networks: A review},
 url = {https://doi-org.crai.referencistas.com/10.1177/15501329221113508},
 volume = {18},
 year = {2022a}
}

@article{doi:10.1177/1550147716666666,
 abstract = {The serious geological hazards occurred frequently in the last few years. They have inflicted heavy casualties and property losses. Hence, it is necessary to design a geological information service system to analyze and evaluate geological hazards. With the development of computer and Internet service model, it is now possible to obtain rich data and process the data with some advanced computing techniques under network environment. Then, some technologies, including cyber-physical system, Internet of Things, and cloud computing, have been used in geological information management. Furthermore, the concept of cyber-physical-social-thinking as a broader vision of the Internet of Things was presented through the fusion of those advanced computing technologies. Motivated by it, in this article, a novel modeling and computing method for geological information service system is developed in consideration of the complex data processing requirement of geological service under dynamic environment. Specifically, some key techniques of modeling the information service system and computing geological data via cyber-physical system and Internet of Things are analyzed. Moreover, to show the efficiency of proposed method, two application cases are provided during the cyber-physical-social-thinking modeling and computing for geological information service system.},
 author = {Yueqin Zhu and Yongjie Tan and Ruixin Li and Xiong Luo},
 doi = {10.1177/1550147716666666},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1550147716666666},
 journal = {International Journal of Distributed Sensor Networks},
 number = {11},
 pages = {1550147716666666},
 title = {Cyber-physical-social-thinking modeling and computing for geological information service system},
 url = {https://doi-org.crai.referencistas.com/10.1177/1550147716666666},
 volume = {12},
 year = {2016t}
}

@article{doi:10.1177/1550147720912397,
 abstract = {For large-scale wireless sensor networks, the nonlinear localization problem where only neighboring distances are available to each individual sensor nodes have been attracting great research attention. In general, distributed algorithms for this problem are likely to suffer from the failures that localizations are trapped in local minima. Focusing on this issue, this article considers a fully distributed algorithm by introducing a novel mechanism, where each individual node is allowed to computationally interact with a random subset of its neighbors, for helping localizations escape from local minima. Theoretical analyses reveal that with the proposed algorithm, any local minimum of the localization will be unstable, and the global optimum would finally be achieved with probability 1 after enough time of iterations. Numerical simulations are given as well to demonstrate the effectiveness of the algorithm.},
 author = {Xiaochu Wang and Ting Sun and Changhao Sun and Junqi Wang},
 doi = {10.1177/1550147720912397},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1550147720912397},
 journal = {International Journal of Distributed Sensor Networks},
 number = {3},
 pages = {1550147720912397},
 title = {Distributed networked localization using neighboring distances only through a computational topology control approach},
 url = {https://doi-org.crai.referencistas.com/10.1177/1550147720912397},
 volume = {16},
 year = {2020s}
}

@article{doi:10.1177/1555343411416442,
 abstract = {The objectives of this study were to investigate the effects of advance auditory cuing of control mode changes in an adaptively automated system on human performance and to explain cognitive behaviors at mode changes by using a computational cognitive model. A dual-task piloting simulation, involving tracking and tactical decision making, was developed to collect human performance data with auditory cuing or no cuing of mode transitions in the tactical task. Computational GOMS (goal, operators, methods, and selection) language models were coded to simulate user behavior on the basis of expectation of increased memory transactions (between long-term and working stores) at mode transitions. The models were applied to the same task simulation and scenarios performed by the humans. Human performance data did not reveal differences between cued and no-cue trials possibly because of distraction from the tracking (secondary loading) task. Comparison of results for human and model output demonstrated the model to be descriptive of the pattern of human performance across conditions but not accurate in predicting timing of memory use in preparing for manual control. A refined GOMS language model was coded on the basis of a modified assumption that memory stores are used on an ad hoc basis after high-workload mode transitions and with consideration of human parallel processing in dual-task performance. Results revealed the refined model to have greater plausibility for representing actual behavior. The manner of operator use of memory stores for controlling an adaptive system provides insight into the impact of cuing of mode transitions and a basis for future systems design.},
 author = {David B. Kaber and Sang-Hwan Kim},
 doi = {10.1177/1555343411416442},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1555343411416442},
 journal = {Journal of Cognitive Engineering and Decision Making},
 number = {3},
 pages = {309–331},
 title = {Understanding Cognitive Strategy With Adaptive Automation in Dual-Task Performance Using Computational Cognitive Models},
 url = {https://doi-org.crai.referencistas.com/10.1177/1555343411416442},
 volume = {5},
 year = {2011h}
}

@article{doi:10.1177/15553434231156417,
 abstract = {During crisis situations, teams are more prone to coordination breakdowns that are characterized by a temporary, diminished ability to function effectively as a team. However, team research currently lacks robust approaches for identifying transitions from effective team functioning to coordination breakdowns. With the current study, we aimed to develop such robust approaches, and to deepen our understanding of how team coordination dynamics across various physiological signals reflect coordination breakdowns. Consequently, we used audiovisual data from four-person teams involved in a stressful collaborative game task to manually identify coordination breakdowns. Next, we set out to computationally identify coordination breakdowns by applying continuous measures of team coordination (windowed synchronization coefficient and multidimensional recurrence quantification analysis) to photoplethysmogram and electrodermal activity data obtained during the task, and identifying transitions therein with change point and nonlinear prediction algorithms. We found that our computational coordination breakdown identification approaches can identify up to 96% of the manually identified coordination breakdowns although our results also show that the precision of our approaches falls far behind. Our findings contribute theoretically and methodologically to the systematic investigation of coordination breakdowns, which may ultimately facilitate the support of teams in responding to and mitigating negative consequences of crisis situations.},
 author = {Kyana H. J. van Eijndhoven and Travis J. Wiltshire and Elwira A. Hałgas and Josette M. P. Gevers},
 doi = {10.1177/15553434231156417},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/15553434231156417},
 journal = {Journal of Cognitive Engineering and Decision Making},
 number = {3},
 pages = {256–278},
 title = {A Computational Approach to Examining Team Coordination Breakdowns During Crisis Situations},
 url = {https://doi-org.crai.referencistas.com/10.1177/15553434231156417},
 volume = {17},
 year = {2023e}
}

@article{doi:10.1177/1555412008317312,
 abstract = {This article provides an overview of computer software and instructional strategies intended to engage young people in making computer games, to achieve a variety of educational goals. It briefly describes the most popular of such programs and compares their key features, including the kinds of games that can be created with the software, the types of communities and resources that are associated with each program, claims made for learning outcomes resulting from use of the software, and the results of empirical research (if any) on the application and outcomes of the software in formal or informal educational settings. A key finding is that existing software and educational applications stress the goal of teaching users about computer programming and place little or no emphasis on teaching concepts related to game design. It concludes by discussing the potential value of explicit attention to “design thinking” as goal of game making in education.},
 author = {Elisabeth R. Hayes and Ivan Alex Games},
 doi = {10.1177/1555412008317312},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1555412008317312},
 journal = {Games and Culture},
 number = {3–4},
 pages = {309–332},
 title = {Making Computer Games and Design Thinking: A Review of Current Software and Strategies},
 url = {https://doi-org.crai.referencistas.com/10.1177/1555412008317312},
 volume = {3},
 year = {2008c}
}

@article{doi:10.1177/1555412013513349,
 abstract = {Multiple recent works have emphasized the contribution that nondigital game scholarship can make to the study of games and gameplay. Warhammer 40,000 is the market dominator of the nondigital tabletop wargame genre. In this article, we perform a ludological analysis of the process of preparing, or drafting, an army for a competitive Warhammer 40,000 tournament. We find that there are four interrelated categories of resources that influence this fundamentally playful process. Our results indicate that this process of preparation constitutes a core component of the appeal of Warhammer 40,000. This emphasizes the importance of understanding the diverse activities that go into gameplay that often exceed the computer game “client” or board of play. We suggest the category of engagement pastime to encapsulate these extended, ongoing elements of Warhammer 40,000’s appeal, which we define as a collection of interlinked and associated activities that serve to occupy one’s time and thoughts pleasantly.},
 author = {Marcus Carter and Martin Gibbs and Mitchell Harrop},
 doi = {10.1177/1555412013513349},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1555412013513349},
 journal = {Games and Culture},
 number = {2},
 pages = {122–147},
 title = {Drafting an Army: The Playful Pastime of Warhammer 40,000},
 url = {https://doi-org.crai.referencistas.com/10.1177/1555412013513349},
 volume = {9},
 year = {2014f}
}

@article{doi:10.1177/15554120221149532,
 abstract = {This paper introduces a model for analyzing the aesthetic value of game rules. Drawing on Nguyen’s theory of agential aesthetics, this paper argues that the aesthetic value of a rule is related to the ways in which it contains modalities of agency. Using football’s (soccer) offside rule as a case study, this article provides a way of thinking about the aesthetics of game rules.},
 author = {Miguel Sicart},
 doi = {10.1177/15554120221149532},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/15554120221149532},
 journal = {Games and Culture},
 number = {7},
 pages = {889–906},
 title = {The Beautiful Rule: Thinking the Aesthetics of Game Rules},
 url = {https://doi-org.crai.referencistas.com/10.1177/15554120221149532},
 volume = {18},
 year = {2023n}
}

@article{doi:10.1177/15554120231211376,
 abstract = {This article analyzes the configuration of fear generated by the computational monster in computer games. We view the monster as a computational entity, which we approach through our theory of game-play coupled with the concepts of loss aversion and endowment effect. Of particular interest is player perception of the threat posed by monsters as they perturb the experience of progression and the sensation of control within the game. We scrutinize this aspect from a situational as well as an existential perspective. Furthermore, we advance an analytical scheme of the threat of the computational monster, which is radically different from the traditional academic approach with its emphasis on the representation of monsters. Overall, we argue that the threat players perceive when facing monsters in computer games springs more from the computational nature of monsters—how they upset progression and the feeling of control—and less from the representation of the monster(s).},
 author = {Lasse Juel Larsen and Bo Kampmann Walther},
 doi = {10.1177/15554120231211376},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/15554120231211376},
 journal = {Games and Culture},
 number = {0},
 pages = {15554120231211376},
 title = {Fear of Monsters: Toward an Understanding of the Threat of the Computational Monster Read Through the Theoretical Lens  of Game-Play},
 url = {https://doi-org.crai.referencistas.com/10.1177/15554120231211376},
 volume = {0},
 year = {2023f}
}

@article{doi:10.1177/1556264616661611,
 abstract = {Widespread availability of electronic health records coupled with sophisticated statistical methods offer great potential for a variety of applications for health and disease surveillance, developing predictive models and advancing decision support for clinicians. However, use of “big data” mining and discovery techniques has also raised ethical issues such as how to balance privacy and autonomy with the wider public benefits of data sharing. Furthermore, electronic data are being increasingly used to identify individual characteristics, which can be useful for clinical prediction and management, but were not previously disclosed to a clinician. This process in computer parlance is called electronic phenotyping, and has a number of ethical implications. Using the Belmont Report’s principles of respect for persons, beneficence, and justice as a framework, we examined the ethical issues posed by electronic phenotyping. Ethical issues identified include the ability of the patient to consent for the use of their information, the ability to suppress pediatric information, ensuring that the potential benefits justify the risks of harm to patients, and acknowledging that the clinician’s biases or stereotypes, conscious or unintended, may become a factor in the therapeutic interaction. We illustrate these issues with two vignettes, using the person characteristic of gender minority status (i.e., transgender identity) and health history characteristic of substance abuse. Data mining has the potential to uncover patient characteristics previously obscured, which can provide clinicians with beneficial clinical information. Hence, ethical guidelines must be updated to ensure that electronic phenotyping supports the principles of respect for persons, beneficence, and justice.},
 author = {Kenrick D. Cato and Walter Bockting and Elaine Larson},
 doi = {10.1177/1556264616661611},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1556264616661611},
 journal = {Journal of Empirical Research on Human Research Ethics},
 note = {PMID:27534587},
 number = {3},
 pages = {214–219},
 title = {Did I Tell You That? Ethical Issues Related to Using Computational Methods to Discover Non-Disclosed Patient Characteristics},
 url = {https://doi-org.crai.referencistas.com/10.1177/1556264616661611},
 volume = {11},
 year = {2016a}
}

@article{doi:10.1177/15586898211020862,
 abstract = {In a rapidly changing public health crisis such as COVID-19, researchers need innovative approaches that can effectively link qualitative approaches and computational methods. In this article, computational and qualitative methods are used to analyze survey data collected in March 2020 (n = 2,270) to explore the content of persuasive messages and their relationship with self-reported health behavior—that is, social distancing. Results suggest that persuasive messages, based on participants’ perspectives, vary by gender and race and are associated with self-reported health behavior. This article illustrates how qualitative analysis and structural topic modeling can be used in synergy in a public health study to understand the public’s perception and behavior related to science issues. Implications for health communication and future research are discussed.},
 author = {Pauline Ho and Kaiping Chen and Anqi Shao and Luye Bao and Angela Ai and Adati Tarfa and Dominique Brossard and Lori Brown and Markus Brauer},
 doi = {10.1177/15586898211020862},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/15586898211020862},
 journal = {Journal of Mixed Methods Research},
 number = {3},
 pages = {374–397},
 title = {A Mixed Methods Study of Public Perception of Social Distancing: Integrating Qualitative and Computational Analyses for Text Data},
 url = {https://doi-org.crai.referencistas.com/10.1177/15586898211020862},
 volume = {15},
 year = {2021k}
}

@article{doi:10.1177/155892500800300102,
 abstract = {In this paper, we present results from a computational fluid dynamics (CFD) model for the mixing process used to disperse synthetic fibers in wet-lay process. We used CFD software, FLUENT, together with the MIXSIM user interface to accurately model the impeller geometry. A multiple reference frame (MRF) model and standard k-e turbulence model were used to model the problem. After obtaining a converged solution for the mixing tank with water, a discrete phase model was constructed by injecting spherical particles into the flow. A mixing tank with baffles and a centrally located impeller was used in experiments. PET fibers (1.5 denier, 6.35, 12.7, and 38.7 mm) at a concentration of 0.01% were mixed in water for the study. In regions behind the baffles, where the model predicted higher concentration of particles, experimental results showed a 34% higher concentration relative to the region in the high turbulence zone near the center. Instantaneous sheets were formed by rapidly dipping and removing a flat wire mesh strainer into the tank at two different locations to assess the state of dispersion in the tank. The sheets were transferred onto a blotting paper and examined under a microscope to count defects. Results show that the number of rope defects was 43% higher in sheets drawn from the region behind the baffles than in the sheets drawn from regions near the center of the tank. Changing baffles from a rectangular to a triangular cross section decreased the number of rope defects, but increased the number of log defects in the sample sheets at the same location. The CFD model can be used to optimize mixing tank design for wet lay fiber dispersion. The model provides further insight into the mixing process by predicting the effect of changes in design parameters on dispersion quality.},
 author = {Melur K. Ramasubramanian and Donald A. Shiffler and Amit Jayachandran},
 doi = {10.1177/155892500800300102},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/155892500800300102},
 journal = {Journal of Engineered Fibers and Fabrics},
 number = {1},
 pages = {155892500800300100},
 title = {A Computational Fluid Dynamics Modeling and Experimental Study of the Mixing Process for the Dispersion of the Synthetic Fibers in Wet-Lay Forming},
 url = {https://doi-org.crai.referencistas.com/10.1177/155892500800300102},
 volume = {3},
 year = {2008o}
}

@article{doi:10.1177/1558925020973542,
 abstract = {This paper investigates on a computational simulation of Twaron® fabric against ballistic impact. It proposed a hybrid shell element model considering the strain-rate-sensitive failure criterion. This model innovatively provided a resolution of the yarn level to better capture the unique properties of the woven fabric, such as yarn crimp, sliding contact between yarns, stress transmission on yarns and yarn broken. The fabric is modeled using a hybrid shell element analysis approach aim of reducing the complexity and computational expense while ensuring accuracy. The response characteristics of fabric under high velocity ballistic impact are studied by applying a 3D finite element program DYNA3D in this paper and the experimental investigation had been taken by Shim et al. According to the computational and experimental results, transverse deflection distribution and stress transmission of fabrics are presented. The ballistic limit, energy absorption, remaining velocity are calculated by simulation models and compared with the experimental results. This approach is also validated by comparing it against a 2D uniform shell model and a 3D interlacing shell model. The results show that the hybrid model can accurately reflect the buckling and fluctuation behavior of fabrics and has a relatively few computational consumption at the same time.},
 author = {Xiuyang Qian and Yushan Zhou and Liya Cai and Feng Pei and Xu Li},
 doi = {10.1177/1558925020973542},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1558925020973542},
 journal = {Journal of Engineered Fibers and Fabrics},
 number = { },
 pages = {1558925020973542},
 title = {Computational simulation of the ballistic impact of fabrics using hybrid shell element},
 url = {https://doi-org.crai.referencistas.com/10.1177/1558925020973542},
 volume = {15},
 year = {2020p}
}

@article{doi:10.1177/15589250221121843,
 abstract = {Conventional single card air yarn units create flow vortices in the carding box that affect the fiber structure and thus the yarn performance. Key parameters for airflow characteristics include geometry and spinning parameters (Such as the length of the nozzle, the width of the inlet and outlet, the speed and diameter of the felting roller). This paper proposes to improve the transport channel to reduce eddy currents in the upflow region and to analyze airflow fields using fluid knowledge. This paper examines three designs: a bypass circular channel entrance for air supply, rounded corners at the bottom right in the conveyor channel and combining both modifications. According to the analysis, a nozzle inlet/outlet ratio between 0.5 and 1 gives the lowest swirl and the highest mass flow. Considering the yarn characteristics and the system production efficiency, it is ideal when the barbed roll speed is 8000 rpm and the diameter is about 30 mm. It is possible to dissipate the vortices for three methods.},
 author = {HaiFeng Fang and HanLin Sun and Qun Liu and Rui Liu and MingQiang Wang},
 doi = {10.1177/15589250221121843},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/15589250221121843},
 journal = {Journal of Engineered Fibers and Fabrics},
 number = { },
 pages = {15589250221121844},
 title = {Optimization analysis of combing box for air yarn based on computational fluid dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1177/15589250221121843},
 volume = {17},
 year = {2022d}
}

@article{doi:10.1177/15910199221097766,
 abstract = {Backgrounds Hemodynamics plays an important role in the natural history of the process of rupture and recurrence of intracranial aneurysms. This study aimed to investigate the role of hemodynamics for recurrence in a vertebral artery dissecting aneurysm (VADA). Methods A patient with a ruptured VADA firstly treated by low-profile visualized intraluminal support (LVIS)-assisted coiling, and was implanted with a Pipeline Embolization Device (PED) after aneurysm recurrence. Finite element analysis and computational fluid dynamics simulations were conducted in 6 serial imaging procedures, and the calculated hemodynamics was correlated with aneurysm recurrence. Results Wall shear stress (WSS) was not effectively suppressed, resulting in aneurysm recurrence with initial entry tear to occur above the protuberance after 7 months of LVIS stent-assisted coiling. With the implantation of PED, WSS, inflow stream and velocity at the aneurysm neck significantly decreased. During the 3-month follow-up after PED deployment, there was significant shrinkage of the sac and the blood flow in the sac was reduced considerably. The 27-month follow-up after PED deployment indicated the aneurysm was stable. Conclusions The present case study suggests that insufficient suppression of high WSS and high inflow velocity at the neck of the parent artery, especially near the posterior inferior cerebellar artery, might be associated with aneurysm recurrence.},
 author = {Linhui Chen and Xiaochang Leng and Chaobo Zheng and Yejie Shan and Ming Wang and Xiang Bao and Jiong Wu and Rong Zou and Xiaobo Liu and Shanhu Xu et al.},
 doi = {10.1177/15910199221097766},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/15910199221097766},
 journal = {Interventional Neuroradiology},
 note = {PMID:35484808},
 number = {4},
 pages = {442–449},
 title = {Computational fluid dynamics (CFD) analysis in a ruptured vertebral artery dissecting aneurysm implanted by Pipeline when recurrent after LVIS-assisted coiling treatment: Case report and review of the literatures},
 url = {https://doi-org.crai.referencistas.com/10.1177/15910199221097766},
 volume = {29},
 year = {2023b}
}

@article{doi:10.1177/1687814015606307,
 abstract = {Any hydraulic reaction turbine is installed with a draft tube that impacts widely the entire turbine performance, on which its functions are as follows: drive the flux in appropriate manner after it releases its energy to the runner; recover the suction head by a suction effect; and improve the dynamic energy in the runner outlet. All these functions are strongly linked to the geometric definition of the draft tube. This article proposes a geometric parametrization and analysis of a Francis turbine draft tube. Based on the parametric definition, geometric changes in the draft tube are proposed and the turbine performance is modeled by computational fluid dynamics; the boundary conditions are set by measurements performed in a hydroelectric power plant. This modeling allows us to see the influence of the draft tube shape on the entire turbine performance. The numerical analysis is based on the steady-state solution of the turbine component flows for different guide vanes opening and multiple modified draft tubes. The computational fluid dynamics predictions are validated using hydroelectric plant measurements. The prediction of the turbine performance is successful and it is linked to the draft tube geometric features; therefore, it is possible to obtain a draft tube parameter value that results in a desired turbine performance.},
 author = {JB Sosa and G Urquiza and JC García and LL Castro},
 doi = {10.1177/1687814015606307},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1687814015606307},
 journal = {Advances in Mechanical Engineering},
 number = {10},
 pages = {1687814015606307},
 title = {Computational fluid dynamics simulation and geometric design of hydraulic turbine draft tube},
 url = {https://doi-org.crai.referencistas.com/10.1177/1687814015606307},
 volume = {7},
 year = {2015n}
}

@article{doi:10.1177/1687814015620570,
 abstract = {Hybrid-driven underwater glider is a new kind of unmanned platform for water quality monitoring. It has advantages such as high controllability and maneuverability, low cost, easy operation, and ability to carry multiple sensors. This article develops a hybrid-driven underwater glider, PETRELII, and integrates a water quality monitoring sensor. Considering stability and economy, an optimal layout scheme is selected from four candidates by simulation using computational fluid dynamics method. Trials were carried out in Danjiangkou Reservoir—important headwaters of the Middle Route of the South-to-North Water Diversion Project. In the trials, a monitoring strategy with polygonal mixed-motion was adopted to make full use of the advantages of the unmanned platform. The measuring data, including temperature, dissolved oxygen, conductivity, pH, turbidity, chlorophyll, and ammonia nitrogen, are obtained. These data validate the practicability of the theoretical layout obtained using computational fluid dynamics method and the practical performance of PETRELII with sensor.},
 author = {Chao Li and Yanhui Wang and Zhiliang Wu},
 doi = {10.1177/1687814015620570},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1687814015620570},
 journal = {Advances in Mechanical Engineering},
 number = {12},
 pages = {1687814015620570},
 title = {Stability and economy analysis based on computational fluid dynamics and field testing of hybrid-driven underwater glider with the water quality sensor in Danjiangkou Reservoir},
 url = {https://doi-org.crai.referencistas.com/10.1177/1687814015620570},
 volume = {7},
 year = {2015g}
}

@article{doi:10.1177/1687814016632680,
 abstract = {In this article, a typical submersible well pump was investigated to study the effects of inter-stage leakage on the inner flow field and external characteristics. The whole flow field of the model pump with different seal clearances was simulated by computational fluid dynamics software. The inter-stage clearance leakage calculated by numerical simulation was compared with the values obtained by the empirical formula. The pressure values were recorded with the arrangement of monitor points in the inter-stage clearance, inlet region, and outlet region, which aimed to study the effects of the change of inter-stage clearance pressure on the pump performance. Meanwhile, a comparative analysis of numerical simulation and performance test of the pump was conducted further. The results showed that the leakage at the small clearance is close to the value calculated using empirical formula. But when the clearance is large, the discrepancy between the simulation result and empirical value inclines. Comparing the numerical results of three kinds of clearance leakage, we found that the clearance leakage could not be ignored in the simulation since it has a large effect on the prediction of pump efficiency and head.},
 author = {Xiongfa Gao and Weidong Shi and Ling Zhou and Desheng Zhang and Qihua Zhang},
 doi = {10.1177/1687814016632680},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1687814016632680},
 journal = {Advances in Mechanical Engineering},
 number = {2},
 pages = {1687814016632680},
 title = {Computational fluid dynamics and experimental study of inter-stage seal clearance on submersible well pump},
 url = {https://doi-org.crai.referencistas.com/10.1177/1687814016632680},
 volume = {8},
 year = {2016e}
}

@article{doi:10.1177/1687814016642955,
 abstract = {The three-dimensional time-domain computational fluid dynamics approach is employed to calculate and analyze the sound attenuation behavior of water-filled perforated pipe silencers. Transmission loss predictions from the time-domain computational fluid dynamics approach and the frequency-domain finite element method agree well with each other for the straight-through and cross-flow perforated pipe silencers without flow. Then, the time-domain computational fluid dynamics approach is used to investigate the effects of flow velocity, diameter, and porosity of orifices on the sound attenuation behavior of the silencers. The numerical predictions demonstrate that the flow increases the transmission loss, especially at high frequencies. Based on the above analysis, partially plugged straight-through perforated pipe silencer is proposed to improve the sound attenuation performance by increasing the flow velocity through the orifices. In order to eliminate the pass frequency of the perforated pipe silencers and improve the sound attenuation performance in mid- to high-frequency range, a folded straight-through perforated pipe silencer is designed and its sound attenuation behavior is analyzed numerically using the time-domain computational fluid dynamics approach.},
 author = {Xu Zhou and Zhenlin Ji},
 doi = {10.1177/1687814016642955},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1687814016642955},
 journal = {Advances in Mechanical Engineering},
 number = {4},
 pages = {1687814016642955},
 title = {Sound attenuation analysis of water-filled perforated pipe silencers using three-dimensional time-domain computational fluid dynamics approach},
 url = {https://doi-org.crai.referencistas.com/10.1177/1687814016642955},
 volume = {8},
 year = {2016t}
}

@article{doi:10.1177/1687814016682653,
 abstract = {For designing bolted connections in machinery applications, it is necessary to estimate the stiffness of the threaded connection. This work provides a new method for computing the stiffness of engaged screw in bolted connections according to the load distribution in screw thread. Finite element analysis is performed by building the three-dimensional model of threaded connection. A set of tensile tests are exerted to validate the accuracy of the suggested model of threaded connection. A good agreement is obtained when the analytical results are compared with finite element analysis results, experimental data, and Yamamoto method. Results reveal that the ultimate strength of thread connections is obviously lower than that of thread material. In addition, the results of calculation and finite element analysis indicated that increasing Young’s modulus of material and the engaged length or decreasing thread pitch could increase the stiffness of the thread portion of a bolt and nut.},
 author = {Dongmei Zhang and Shiqiao Gao and Xiao Xu},
 doi = {10.1177/1687814016682653},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1687814016682653},
 journal = {Advances in Mechanical Engineering},
 number = {12},
 pages = {1687814016682653},
 title = {A new computational method for threaded connection stiffness},
 url = {https://doi-org.crai.referencistas.com/10.1177/1687814016682653},
 volume = {8},
 year = {2016s}
}

@article{doi:10.1177/1687814016685224,
 abstract = {A micromechanical model was established based on the fluid dynamics theory. This model could be used to calculate several kinds of data when the asphalt pavement under the influence of traffic loading is in water-saturated condition. The results showed that the maximum pressure inside the effective pore was located at the junction between exit slits and the pore wall. There was a positive correlation between the pressure and the vehicle speed. Therefore, the repeated traffic loading could cause emulsification, shift and even peeling of the asphalt membrane. Moreover, the bigger size of the exit slit is, the higher velocity of the fluid has. The high velocity flow keeps scouring both the exit slit and the lower boundary of pore wall. It will cause a bigger slit. Pressure distribution inside the effective pore is related to the number of the exit slit which connect with the pore. More exit slits means bigger pressure inside the effective pore. In addition, if asphalt membranes at exit slits have micro-cracking, the cumulative damage could appear easily and asphalt membranes could be peeled easily. Finally, a test was conducted so as to obtain the bonding strength and adhesion strength between asphalt and aggregate. Then, we can get accurate damage form and position during the scour process by comparing the numerical simulation results with experiment results.},
 author = {Xuedong Guo and Mingzhi Sun and Wenting Dai},
 doi = {10.1177/1687814016685224},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1687814016685224},
 journal = {Advances in Mechanical Engineering},
 number = {2},
 pages = {1687814016685224},
 title = {Analysis of effective pore pressure in asphalt pavement based on computational fluid dynamics calculation},
 url = {https://doi-org.crai.referencistas.com/10.1177/1687814016685224},
 volume = {9},
 year = {2017i}
}

@article{doi:10.1177/1687814017713702,
 abstract = {Flow coefficient is an important performance index associated with the energy efficiency of a valve, and an effective method to evaluate valve flow coefficient is necessary for valve industry. However, theoretical estimation often results in poor accuracy, while experimental measurements involve significant costs in time and equipment. In this article, a computational fluid dynamics method is proposed to achieve simple and accurate evaluation of valve flow coefficient. For each valve, a computational fluid dynamics model is established containing a valve section, an upstream section, and a downstream section. A grid-adaptation strategy is then applied to improve the accuracy of simulation. To calculate flow coefficient, the most important issue is to determine the net pressure loss induced by valve (ΔPv). Herein, the overall pressure drop (ΔPo) is obtained first, and the pipe-induced pressure drop (ΔPp) is estimated by linear fitting. Then, ΔPv is calculated as the difference between ΔPo and ΔPp. To ensure accurate estimation of the pressure losses, a length of 26 times of pipe diameter is preferred for the upstream section. The experiments demonstrated that the presented method can accurately predict flow coefficient for various types of valves and thus has great potential to be widely used in the valve industry.},
 author = {Xiao-Ming Zhou and Zhi-Kun Wang and Yi-Fang Zhang},
 doi = {10.1177/1687814017713702},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1687814017713702},
 journal = {Advances in Mechanical Engineering},
 number = {7},
 pages = {1687814017713702},
 title = {A simple method for high-precision evaluation of valve flow coefficient by computational fluid dynamics simulation},
 url = {https://doi-org.crai.referencistas.com/10.1177/1687814017713702},
 volume = {9},
 year = {2017t}
}

@article{doi:10.1177/1687814017734109,
 abstract = {Mesh partitioning is significant to the efficiency of parallel computational fluid dynamics simulations. The most time-consuming parts of parallel computational fluid dynamics simulations are iteratively solving linear systems derived from partial differential equation discretizations. This article aims at mesh partitioning for better iterative convergence feature of this procedure. For typical computational fluid dynamics simulations in which partial differential equations are discretized and solved after the mesh is partitioned, numerical information of the linear systems is not available yet during mesh partitioning. We propose to construct approximations for matrix elements and theoretically find out that for finite-volume-based problems, the face area can approximate the corresponding matrix element well. A mesh partitioning scheme using the matrix value approximations for better iterative convergence behavior is implemented and numerically testified. The results show that our method can capture the most important factor influencing the matrix values and achieve partitions with good performance throughout the simulations with non-uniform meshes. The novel partitioning strategy is general and easy to implement in various partitioning packages.},
 author = {Miao Wang and Xinhai Xu and Xiaoguang Ren and Chao Li and Juan Chen and Xuejun Yang},
 doi = {10.1177/1687814017734109},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1687814017734109},
 journal = {Advances in Mechanical Engineering},
 number = {11},
 pages = {1687814017734109},
 title = {Mesh partitioning using matrix value approximations for parallel computational fluid dynamics simulations},
 url = {https://doi-org.crai.referencistas.com/10.1177/1687814017734109},
 volume = {9},
 year = {2017q}
}

@article{doi:10.1177/1687814018757261,
 abstract = {A novel adaptive fuzzy control approach with low computational costs is addressed for the longitudinal non-affine model of a generic hypersonic vehicle subject to parametric uncertainties, capable of guaranteeing that altitude and velocity tracking errors exhibit prescribed performance. A new performance function is devised to impose preselected bounds on the transient and steady-state tracking error performance, and then a transformed error is constructed, which converts the original “constrained” system into an equivalent “unconstrained” one for the simplicity of control designs. Furthermore, a simplified fuzzy controller is exploited for the altitude subsystem, eliminating the issue of “explosion of complexity” that is associated with back-stepping control. For the velocity subsystem, a prescribed performance-based proportional–integral controller is utilized. The special contributions of the proposed control strategy are that it can guarantee altitude and velocity tracking errors with desired transient and steady-state performance, and meanwhile the computational load is quite low. Finally, numerical simulation results are presented to verify the efficiency of the design.},
 author = {Xiangwei Bu and Yu Xiao},
 doi = {10.1177/1687814018757261},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1687814018757261},
 journal = {Advances in Mechanical Engineering},
 number = {2},
 pages = {1687814018757261},
 title = {Prescribed performance-based low-computational cost fuzzy control of a hypersonic vehicle using non-affine models},
 url = {https://doi-org.crai.referencistas.com/10.1177/1687814018757261},
 volume = {10},
 year = {2018c}
}

@article{doi:10.1177/1687814018780029,
 abstract = {The objective of this research was to investigate the effects of material models, element types, and boundary conditions on the consistency of finite element analysis. Two cantilever beams were used; one made of stainless steel SUS301 3/4H and the other made of polymer polyoxymethylene. The load–deflection curves of the two cantilever beams obtained by experiments were compared to those obtained by finite element analysis, where the material models—including bilinear, trilinear, and multi-linear—were used. Four element types—beam, plane stress, shell, and solid—were also employed with the material models to obtain the simulated load–deflection curves of the cantilever beams. It was found that bilinear material models had the stiffest behavior due to their overestimated yield strength. In addition, by applying a finite displacement to simulate the grip of the cantilever beams, the discrepancy between the simulated permanent set and the experimental set could be reduced from 80% to 5%. To sum up, both the selection of the material model and the setup of the boundary conditions are critical for obtaining good agreement between the finite element analysis results and the experimental data.},
 author = {Wei-chen Lee and Chen-hao Zhang},
 doi = {10.1177/1687814018780029},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1687814018780029},
 journal = {Advances in Mechanical Engineering},
 number = {6},
 pages = {1687814018780029},
 title = {Computational consistency of the material models and boundary conditions for finite element analyses on cantilever beams},
 url = {https://doi-org.crai.referencistas.com/10.1177/1687814018780029},
 volume = {10},
 year = {2018o}
}

@article{doi:10.1177/1687814019881039,
 abstract = {This article describes an efficient algorithm based on residual power series to approximate the solution of a class of partial differential equations of time-fractional Fokker–Planck model. The fractional derivative is assumed in the Caputo sense. The proposed algorithm gives the solution in a form of rapidly convergent fractional power series with easily computable coefficients. It does not require linearization, discretization, or small perturbation. To test simplicity, potentiality, and practical usefulness of the proposed algorithm, illustrative examples are provided. The approximate solutions of time-fractional Fokker–Planck equations are obtained by the residual power series method are compared with those obtained by other existing methods. The present results and graphics reveal the ability of residual power series method to deal with a wide range of partial fractional differential equations emerging in the modeling of physical phenomena of science and engineering.},
 author = {Asad Freihet and Shatha Hasan and Mohammad Alaroud and Mohammed Al-Smadi and Rokiah Rozita Ahmad and Ummul Khair Salma Din},
 doi = {10.1177/1687814019881039},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1687814019881039},
 journal = {Advances in Mechanical Engineering},
 number = {10},
 pages = {1687814019881039},
 title = {Toward computational algorithm for time-fractional Fokker–Planck models},
 url = {https://doi-org.crai.referencistas.com/10.1177/1687814019881039},
 volume = {11},
 year = {2019d}
}

@article{doi:10.1177/1687814019885263,
 abstract = {Skin deformation caused by contact with an object is transduced into nerve signals by tactile mechanoreceptors, allowing humans to perceive tactile information. Previous research has revealed that the mechanical state associated with finger skin deformation at mechanoreceptor locations in a finite element model is correlated with the experimentally measured responses of slowly adapting type I mechanoreceptors. However, these findings were obtained under static contact conditions. Therefore, in this study, we calculated the von Mises stress at slowly adapting type I and rapidly adapting type I mechanoreceptor locations during dynamic scanning of a textured surface using a finite element model of the human finger. We then estimated the hypothetical responses of the mechanoreceptors and compared the estimated results with the nerve firing of the receptors in previous neurophysiological experiments. These comparisons demonstrated that the temporal history of von Mises stress at mechanoreceptor locations was more strongly correlated with the “number of” impulses (R2 = 0.93 for slowly adapting type I and R2 = 0.90 for rapidly adapting type I) than the impulse “rate” (R2 = 0.58 for slowly adapting type I and R2 = 0.53 for rapidly adapting type I). Our findings suggest that the temporal history of von Mises stress can be used to roughly estimate the number of impulses of mechanoreceptors during scanning of a textured surface.},
 author = {Toru Hamasaki and Masami Iwamoto},
 doi = {10.1177/1687814019885263},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1687814019885263},
 journal = {Advances in Mechanical Engineering},
 number = {11},
 pages = {1687814019885263},
 title = {Computational analysis of the relationship between mechanical state and mechanoreceptor responses during scanning of a textured surface},
 url = {https://doi-org.crai.referencistas.com/10.1177/1687814019885263},
 volume = {11},
 year = {2019e}
}

@article{doi:10.1177/1687814020937951,
 abstract = {In the conveying process of a solid–liquid two-phase medium, the wear of the flow passage components is unavoidable. In this study, the solid–liquid two-phase flow in a centrifugal pump was numerically simulated by computational fluid dynamics–discrete element method coupling. For particle diameters up to 3 mm, the particle–particle and particle–wall interactions were considered in the simulation. Two-phase performance and wear experiments for different flow rates and particle concentrations were conducted. The wear experiment was carried out for 48 h at each mass concentration. In these experiments, a paint film method was used to display the wear position, and the wall thickness of the flow passage was measured using an ultrasonic thickness gauge. The results show that the instantaneous wear rate of the impeller, volute, and wear plate in the pump changed periodically with the impeller rotation. The volute wall wear was related to the particle movement. With the increase in the particle mass concentration, the wear rate increased. However, the rate of increase of the wear rate decreased because the particles moved to the wall in the volute to form a particle layer. Increasing the concentration did not linearly increase the effect of the particles on the wall.},
 author = {Yi Li and Xiaodong Zeng and Wenshuai Lv and Zhaohui He},
 doi = {10.1177/1687814020937951},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1687814020937951},
 journal = {Advances in Mechanical Engineering},
 number = {7},
 pages = {1687814020937951},
 title = {Centrifugal pump wear for solid–liquid two-phase flows based on computational fluid dynamics–discrete element method coupling},
 url = {https://doi-org.crai.referencistas.com/10.1177/1687814020937951},
 volume = {12},
 year = {2020j}
}

@article{doi:10.1177/16878140211009009,
 abstract = {Wind energy extraction is one of the fastest developing engineering branches today. Number of installed wind turbines is constantly increasing. Appropriate solutions for urban environments are quiet, structurally simple and affordable small-scale vertical-axis wind turbines (VAWTs). Due to small efficiency, particularly in low and variable winds, main topic here is development of optimal flow concentrator that locally augments wind velocity, facilitates turbine start and increases generated power. Conceptual design was performed by combining finite volume method and artificial intelligence (AI). Smaller set of computational results (velocity profiles induced by existence of different concentrators in flow field) was used for creation, training and validation of several artificial neural networks. Multi-objective optimization of concentrator geometric parameters was realized through coupling of generated neural networks with genetic algorithm. Final solution from the acquired Pareto set is studied in more detail. Resulting computed velocity field is illustrated. Aerodynamic performances of small-scale VAWT with and without optimal flow concentrator are estimated and compared. The performed research demonstrates that, with use of flow concentrator, average increase in wind speed of 20%–25% can be expected. It also proves that contemporary AI techniques can significantly facilitate and accelerate design processes in the field of wind engineering.},
 author = {Jelena Svorcan and Ognjen Peković and Aleksandar Simonović and Dragoljub Tanović and Mohammad Sakib Hasan},
 doi = {10.1177/16878140211009009},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/16878140211009009},
 journal = {Advances in Mechanical Engineering},
 number = {3},
 pages = {16878140211009008},
 title = {Design of optimal flow concentrator for vertical-axis wind turbines using computational fluid dynamics, artificial neural networks and genetic algorithm},
 url = {https://doi-org.crai.referencistas.com/10.1177/16878140211009009},
 volume = {13},
 year = {2021q}
}

@article{doi:10.1177/1687814021994402,
 abstract = {To predict and minimize machining distortion in the manufacturing process, bulk residual stresses in aeronautical components with distinct geometries were investigated via experimental mechanics and numerical simulation. The residual stress state was appropriately simplified according to geometric/processing feathers and deformation patterns of the investigated parts. In each case study, an optimal experimental method was selected to reconstruct the concerned stress tensor. Thereafter, qualitative comparison and validation were performed using cross-method verification and/or numerical simulation. Additionally, the spatial resolution and distribution characteristics of the residual stress were analyzed and discussed in detail. The results revealed that thermal and mechanical nonuniformity caused by material processing is the main source of bulk residual stress in the investigated components. Furthermore, the effectiveness of the contour method on the measurement of different geometric components was verified by numerical simulation. Combining the accurate measurement of the characteristic plane and the appropriate numerical simulation of the global stress field, an engineering-oriented approach for full-field stress evaluation was proposed. This research can provide valuable engineering guidance and suggestions for stress evaluation and distortion analysis prior to manufacturing of integral structures.},
 author = {Zheng Zhang and Yinfei Yang and Liang Li and Jinxing Kong},
 doi = {10.1177/1687814021994402},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1687814021994402},
 journal = {Advances in Mechanical Engineering},
 number = {2},
 pages = {1687814021994402},
 title = {Experimental and computational modeling of bulk residual stress for aeronautical components with distinct geometries},
 url = {https://doi-org.crai.referencistas.com/10.1177/1687814021994402},
 volume = {13},
 year = {2021s}
}

@article{doi:10.1177/1724600820903317,
 abstract = {Cancer is a complex disease characterized by a wide array of mutually interacting components constituting the tumor microenvironment (connective tissue, vascular system, immune cells), many of which are targeted therapeutically. In particular, immune checkpoint inhibitors have recently become an established part of the treatment of cancer. Despite great promise, only a portion of the patients display durable response. Current research efforts are concentrated on the determination of tumor-specific biomarkers predictive of response, such as tumor mutational burden, microsatellite instability, and neo-antigen presentation. However, it is clear that several additional characteristics pertaining to the tumor microenvironment play a critical role in the effectiveness of immunotherapy. Here we comment on the computational methods that are used for the analysis of the tumor microenvironment components from transcriptomic data, discuss the critical needs, and foresee potential evolutions in the field.},
 author = {Marco Bolis and Arianna Vallerga and Maddalena Fratelli},
 doi = {10.1177/1724600820903317},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1724600820903317},
 journal = {The International Journal of Biological Markers},
 note = {PMID:32079462},
 number = {1_suppl},
 pages = {20–22},
 title = {Computational deconvolution of transcriptomic data for the study of tumor-infiltrating immune cells},
 url = {https://doi-org.crai.referencistas.com/10.1177/1724600820903317},
 volume = {35},
 year = {2020b}
}

@article{doi:10.1177/1729881418766190,
 abstract = {Accurate calculation of canopy aerodynamic parameters is a prerequisite for precise modeling of a parafoil airdrop system. This investigation analyses the aerodynamic performance of the canopy in airdrop testing combining the leading-edge incision and the trailing-edge deflection. Aerodynamic parameters of the canopy are obtained using the computational fluid dynamic simulations, and then, the output data are used to estimate the deflection and incision factors. The estimated lift and drag coefficients instead of the traditional parameters based on lifting-line theory are incorporated into the eight degrees of freedom dynamic model of an airdrop system and make some simulations. The effectiveness of the proposed method for calculating aerodynamic coefficients is verified by actual airdrop testing.},
 author = {Wannan Wu and Qinglin Sun and Shuzhen Luo and Mingwei Sun and Zengqiang Chen and Hao Sun},
 doi = {10.1177/1729881418766190},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1729881418766190},
 journal = {International Journal of Advanced Robotic Systems},
 number = {2},
 pages = {1729881418766190},
 title = {Accurate calculation of aerodynamic coefficients of parafoil airdrop system based on computational fluid dynamic},
 url = {https://doi-org.crai.referencistas.com/10.1177/1729881418766190},
 volume = {15},
 year = {2018q}
}

@article{doi:10.1177/1729881418820166,
 abstract = {Since many off-the-shelf motor drives are supplied with complete control capability in the current, velocity and position loop, the robot model in the navigation control architecture can be oriented either to kinematics, interfaced with the velocity loop, or to dynamics, with the motor-current loop. Moreover, no constraints are imposed by a caster on the mobility of differential-driving mobile robots. Hence, a reduced model, containing only the platform, is sufficient for navigation control based only on the robot kinematics. However, if the multibody system model is used for navigation control based on the robot dynamics, to cope with the demands of high-speed manoeuvres and/or heavy-load operations, then the caster kinematics, especially the knowledge of the steering angle, is required to calculate the inertia matrix and the terms of Coriolis and centrifugal forces. While this angle can be measured by means of dedicated encoders to be installed for casters, the computation technique based on the existing tachometers, already mounted on the motor shafts for the servo control of the two driving wheels, is proved to be sufficient. Both a thorough kinematics model and a multibody dynamics model, including the platform and all different wheels, are formulated here for differential-driving mobile robots. Computational methods based on velocity compatibility and rigid body twists are proposed to estimate the steering angle. Simulation results of the differential-driving mobile robot moving on a smooth trajectory show the feasibility of the steering-angle computational scheme, which obviates the need of installing caster encoders. Moreover, a performance comparison on system modelling is implemented via simulation, between the differential-driving mobile robot model with and without caster dynamics. This further validates the importance of the dynamic effects of casters on the whole system model. Therefore, the multibody modelling approach for casters with the steering-angle computation technique can facilitate the navigation control architecture under dynamics conditions.},
 author = {Xing Wu and Jorge Angeles and Ting Zou and Haining Xiao and Wei Li and Peihuang Lou},
 doi = {10.1177/1729881418820166},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1729881418820166},
 journal = {International Journal of Advanced Robotic Systems},
 number = {6},
 pages = {1729881418820166},
 title = {Steering-angle computation for the multibody modelling of differential-driving mobile robots with a caster},
 url = {https://doi-org.crai.referencistas.com/10.1177/1729881418820166},
 volume = {15},
 year = {2018s}
}

@article{doi:10.1177/1729881418820425,
 abstract = {The development of skills related to computer programming and robotics and the introduction of computational thinking principles in high schools are worldwide trends today. An effective way of initiating young students in this world consists in proposing them stimulating challenges. This work presents a robotic platform that has been successfully used to develop a competition (called Drone Challenge) in which students had to program the navigation system for a simulated unmanned aerial vehicle (or drone). Both the competition and the supporting platform are described in detail. In particular, the article provides a deep technical description of the main components of the platform, namely the drone simulator and the navigation development framework. The results of the survey conducted after the challenge point to the suitability of the working platform deployed.},
 author = {Aurelio Bermúdez and Rafael Casado and Guillermo Fernández and María Guijarro and Pablo Olivas},
 doi = {10.1177/1729881418820425},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1729881418820425},
 journal = {International Journal of Advanced Robotic Systems},
 number = {1},
 pages = {1729881418820425},
 title = {Drone challenge: A platform for promoting programming and robotics skills in K-12 education},
 url = {https://doi-org.crai.referencistas.com/10.1177/1729881418820425},
 volume = {16},
 year = {2019d}
}

@article{doi:10.1177/1741659020917434,
 abstract = {This article introduces the concept of Artificial Intelligence (AI) to a criminological audience. After a general review of the phenomenon (including brief explanations of important cognate fields such as ‘machine learning’, ‘deep learning’, and ‘reinforcement learning’), the paper then turns to the potential application of AI by criminals, including what we term here ‘crimes with AI’, ‘crimes against AI’, and ‘crimes by AI’. In these sections, our aim is to highlight AI’s potential as a criminogenic phenomenon, both in terms of scaling up existing crimes and facilitating new digital transgressions. In the third part of the article, we turn our attention to the main ways the AI paradigm is transforming policing, surveillance, and criminal justice practices via diffuse monitoring modalities based on prediction and prevention. Throughout the paper, we deploy an array of programmatic examples which, collectively, we hope will serve as a useful AI primer for criminologists interested in the ‘tech-crime nexus’.},
 author = {Keith J Hayward and Matthijs M Maas},
 doi = {10.1177/1741659020917434},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1741659020917434},
 journal = {Crime, Media, Culture},
 number = {2},
 pages = {209–233},
 title = {Artificial intelligence and crime: A primer for criminologists},
 url = {https://doi-org.crai.referencistas.com/10.1177/1741659020917434},
 volume = {17},
 year = {2021i}
}

@article{doi:10.1177/1744259117750495,
 abstract = {A combined experimental-computational approach is used for the analysis of hygrothermal performance of a brick wall provided with interior thermal insulation system. A 2D laboratory experiment is performed to determine temperature and moisture fields in a characteristic segment of the envelope over a sufficiently long period including cold winter months. Then, a computational model of moisture and heat transport is developed, using an integral two-phase balance equation capable of distinguishing between the particular phases of water and an enthalpy-based heat balance equation. A 2D computational representation of the experiment is used for model calibration and identification of unknown parameters, resulting in a very good agreement of experimental and calculated fields, with R2 between 0.9687 and 0.9888. The calibrated model is subsequently used for a long-term hygrothermal assessment of the studied detail to demonstrate the functionality of the interior thermal insulation system, as well as the applicability of the developed model.},
 author = {Václav Kočí and Jan Kočí and Jiří Maděra and Zbyšek Pavlík and Xianglin Gu and Weiping Zhang and Robert Černý},
 doi = {10.1177/1744259117750495},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1744259117750495},
 journal = {Journal of Building Physics},
 number = {6},
 pages = {497–520},
 title = {Thermal and hygric assessment of an inside-insulated brick wall: 2D critical experiment and computational analysis},
 url = {https://doi-org.crai.referencistas.com/10.1177/1744259117750495},
 volume = {41},
 year = {2018e}
}

@article{doi:10.1177/1744259118771314,
 abstract = {In developed countries, presence at home varies between 60% and 90% of the day, sleeping supposes 30%. Therefore, it is essential to ensure good indoor air quality that enhances health and benefits rest and recovery. In this context, it is necessary to achieve a balance between energy efficiency and air distribution parameters; thus, the influence exerted by the furniture of a bedroom on the air exchange efficiency, in the breathing zone during sleep, is assessed in this study. Computational fluid dynamics techniques, experimentally validated by the tracer gas (SF6) concentration decay method, are used to analyze 52 case studies corresponding to the same space, but varying both the number and the arrangement of the furniture inside. It is concluded that, in order to achieve a significant improvement in the air exchange efficiency, the number of elements included in the bedroom is not relevant, but the position of them. The highest increase in the ventilation efficiency in breathing zone is observed when the furniture is located avoiding the airflow obstruction in the area near the inlet and creating an unfilled volume of air in the area close to the outlet.},
 author = {Susana Hormigos-Jimenez and Miguel Ángel Padilla-Marcos and Alberto Meiss and Roberto Alonso Gonzalez-Lezcano and Jesús Feijó-Muñoz},
 doi = {10.1177/1744259118771314},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1744259118771314},
 journal = {Journal of Building Physics},
 number = {4},
 pages = {458–483},
 title = {Assessment of the ventilation efficiency in the breathing zone during sleep through computational fluid dynamics techniques},
 url = {https://doi-org.crai.referencistas.com/10.1177/1744259118771314},
 volume = {42},
 year = {2019i}
}

@article{doi:10.1177/1744259120901840,
 abstract = {Building energy simulations coupled with computational fluid dynamics tools have emerged, recently, as an accurate and effective tool to improve the estimation of energy requirements and thermal comfort in buildings. Building modelers and researchers usually implement this coupling in the boundary conditions of both tools (e.g. surface temperature, ambient temperature, and conductive and convective fluxes). This work reviews how the building energy simulation–computational fluid dynamics coupling has evolved since its first implementation to the present day. Moreover, this article also summarizes and discusses the research studies in which the building energy simulation–computational fluid dynamics coupling has been used to analyze building systems, building components, and building urban configurations. Implementing a building energy simulation–computational fluid dynamics coupling brings a series of benefits when compared with the conventional building energy simulation methodology, a building energy simulation–computational fluid dynamics coupling provides an improvement that ranges between 10% and 50% for estimating the building energy requirements. Moreover, the computation time to implement computational fluid dynamics with information obtained from the building energy simulation could be reduced by as well.},
 author = {Martin Rodríguez-Vázquez and Iván Hernández-Pérez and Jesus Xamán and Yvonne Chávez and Miguel Gijón-Rivera and Juan M Belman-Flores},
 doi = {10.1177/1744259120901840},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1744259120901840},
 journal = {Journal of Building Physics},
 number = {2},
 pages = {137–180},
 title = {Coupling building energy simulation and computational fluid dynamics: An overview},
 url = {https://doi-org.crai.referencistas.com/10.1177/1744259120901840},
 volume = {44},
 year = {2020s}
}

@article{doi:10.1177/17442591241252417,
 abstract = {Solar chimneys are among the most common methods for natural ventilation of buildings. Computational Fluid Dynamics (CFD) has been widely applied in research and design of solar chimneys. Most of the previous studies are based on 2D CFD models which ignore the effects of the width (the third dimension) of the air channel. In addition, the applicability of 2D models for the solar chimneys with a low width-to-gap ratio is still questionable. This study investigates both 2D and 3D CFD models for window-sized vertical solar chimneys. The 2D model is applied to the domain comprising the central plane of the channel gap (G) and height (H) while the 3D model is applied to the domain consisting of the channel gap, height, and width (W). The flow fields, flow rates and thermal efficiencies computed with the 2D and 3D models at different heights, gaps, and widths are compared. The results show that W/G is the most crucial factor. The effects of the side walls are significant at a low W/G but gradually diminishes as W/G increases. At W/G = 15, the side wall effects are confined to a region of about 2.6% W. Particularly, for W/G>8, the differences between the 2D and 3D flow rates and thermal efficiencies are within ±5%. These findings offer a reference for researchers and engineers to select between 2D and 3D CFD models for a specific solar chimney.},
 author = {Y Quoc Nguyen and Trieu N Huynh and Khoa Le-Cao},
 doi = {10.1177/17442591241252417},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/17442591241252417},
 journal = {Journal of Building Physics},
 number = {1},
 pages = {100–126},
 title = {Comparison of the induced flow obtained with 2D and 3D CFD simulations of a solar chimney at different width-to-gap ratios},
 url = {https://doi-org.crai.referencistas.com/10.1177/17442591241252417},
 volume = {48},
 year = {2024l}
}

@article{doi:10.1177/1745691612454304,
 abstract = {A widely advocated idea in education is that people learn better when the flow of experience is under their control (i.e., learning is self-directed). However, the reasons why volitional control might result in superior acquisition and the limits to such advantages remain poorly understood. In this article, we review the issue from both a cognitive and computational perspective. On the cognitive side, self-directed learning allows individuals to focus effort on useful information they do not yet possess, can expose information that is inaccessible via passive observation, and may enhance the encoding and retention of materials. On the computational side, the development of efficient “active learning” algorithms that can select their own training data is an emerging research topic in machine learning. This review argues that recent advances in these related fields may offer a fresh theoretical perspective on how people gather information to support their own learning.},
 author = {Todd M. Gureckis and Douglas B. Markant},
 doi = {10.1177/1745691612454304},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1745691612454304},
 journal = {Perspectives on Psychological Science},
 note = {PMID:26168504},
 number = {5},
 pages = {464–481},
 title = {Self-Directed Learning: A Cognitive and Computational Perspective},
 url = {https://doi-org.crai.referencistas.com/10.1177/1745691612454304},
 volume = {7},
 year = {2012l}
}

@article{doi:10.1177/1745691619898795,
 abstract = {The ironic effect of thought suppression refers to the phenomenon in which individuals trying to rid their mind of a target thought ironically experience greater levels of occurrence and accessibility of the thought compared with individuals who deliberately concentrate on the thought (Wegner, 1994, doi:10.1037/0033-295X.101.1.34). Ironic effects occurring after thought suppression, also known as rebound effects, were consistently detected by previous meta-analyses. However, ironic effects that occur during thought suppression, also known as immediate enhancement effects, were found to be largely absent. In this meta-analysis, we test Wegner’s original proposition that detection of immediate enhancement effects depends on the cognitive load experienced by individuals when enacting thought suppression. Given that thought suppression is an effortful cognitive process, we propose that the introduction of additional cognitive load would compete for the allocation of existing cognitive resources and impair capacity for thought suppression. Studies (k = 31) consistent with Wegner’s original thought-suppression paradigm were analyzed. Consistent with our predictions, rebound effects were observed regardless of cognitive load, whereas immediate enhancement effects were observed only in the presence of cognitive load. We discuss implications in light of ironic-process theory and suggest future thought-suppression research.},
 author = {Deming (Adam) Wang and Martin S. Hagger and Nikos L. D. Chatzisarantis},
 doi = {10.1177/1745691619898795},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1745691619898795},
 journal = {Perspectives on Psychological Science},
 note = {PMID:32286932},
 number = {3},
 pages = {778–793},
 title = {Ironic Effects of Thought Suppression: A Meta-Analysis},
 url = {https://doi-org.crai.referencistas.com/10.1177/1745691619898795},
 volume = {15},
 year = {2020r}
}

@article{doi:10.1177/1745691620966794,
 abstract = {Discussions about the replicability of psychological studies have primarily focused on improving research methods and practices, with less attention paid to the role of well-specified theories in facilitating the production of reliable empirical results. The field is currently in need of clearly articulated steps to theory specification and development, particularly regarding frameworks that may generalize across different fields of psychology. Here we focus on two approaches to theory specification and development that are typically associated with distinct research traditions: computational modeling and construct validation. We outline the points of convergence and divergence between them to illuminate the anatomy of a scientific theory in psychology—what a well-specified theory should contain and how it should be interrogated and revised through iterative theory-development processes. We propose how these two approaches can be used in complementary ways to increase the quality of explanations and the precision of predictions offered by psychological theories.},
 author = {Ivan Grahek and Mark Schaller and Jennifer L. Tackett},
 doi = {10.1177/1745691620966794},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1745691620966794},
 journal = {Perspectives on Psychological Science},
 note = {PMID:33404380},
 number = {4},
 pages = {803–815},
 title = {Anatomy of a Psychological Theory: Integrating Construct-Validation and Computational-Modeling Methods to Advance Theorizing},
 url = {https://doi-org.crai.referencistas.com/10.1177/1745691620966794},
 volume = {16},
 year = {2021h}
}

@article{doi:10.1177/1745691620970585,
 abstract = {Psychology endeavors to develop theories of human capacities and behaviors on the basis of a variety of methodologies and dependent measures. We argue that one of the most divisive factors in psychological science is whether researchers choose to use computational modeling of theories (over and above data) during the scientific-inference process. Modeling is undervalued yet holds promise for advancing psychological science. The inherent demands of computational modeling guide us toward better science by forcing us to conceptually analyze, specify, and formalize intuitions that otherwise remain unexamined—what we dub open theory. Constraining our inference process through modeling enables us to build explanatory and predictive theories. Here, we present scientific inference in psychology as a path function in which each step shapes the next. Computational modeling can constrain these steps, thus advancing scientific inference over and above the stewardship of experimental practice (e.g., preregistration). If psychology continues to eschew computational modeling, we predict more replicability crises and persistent failure at coherent theory building. This is because without formal modeling we lack open and transparent theorizing. We also explain how to formalize, specify, and implement a computational model, emphasizing that the advantages of modeling can be achieved by anyone with benefit to all.},
 author = {Olivia Guest and Andrea E. Martin},
 doi = {10.1177/1745691620970585},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1745691620970585},
 journal = {Perspectives on Psychological Science},
 note = {PMID:33482070},
 number = {4},
 pages = {789–802},
 title = {How Computational Modeling Can Force Theory Building in Psychological Science},
 url = {https://doi-org.crai.referencistas.com/10.1177/1745691620970585},
 volume = {16},
 year = {2021l}
}

@article{doi:10.1177/17456916221091833,
 abstract = {Scientific discovery is a driving force for progress involving creative problem-solving processes to further our understanding of the world. The process of scientific discovery has historically been intensive and time-consuming; however, advances in computational power and algorithms have provided an efficient route to make new discoveries. Complex tools using artificial intelligence (AI) can efficiently analyze data as well as generate new hypotheses and theories. Along with AI becoming increasingly prevalent in our daily lives and the services we access, its application to different scientific domains is becoming more widespread. For example, AI has been used for the early detection of medical conditions, identifying treatments and vaccines (e.g., against COVID-19), and predicting protein structure. The application of AI in psychological science has started to become popular. AI can assist in new discoveries both as a tool that allows more freedom to scientists to generate new theories and by making creative discoveries autonomously. Conversely, psychological concepts such as heuristics have refined and improved artificial systems. With such powerful systems, however, there are key ethical and practical issues to consider. This article addresses the current and future directions of computational scientific discovery generally and its applications in psychological science more specifically.},
 author = {Laura K. Bartlett and Angelo Pirrone and Noman Javed and Fernand Gobet},
 doi = {10.1177/17456916221091833},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/17456916221091833},
 journal = {Perspectives on Psychological Science},
 note = {PMID:35943820},
 number = {1},
 pages = {178–189},
 title = {Computational Scientific Discovery in Psychology},
 url = {https://doi-org.crai.referencistas.com/10.1177/17456916221091833},
 volume = {18},
 year = {2023b}
}

@article{doi:10.1177/1746847719831398,
 abstract = {It is well known that, despite his close engagement with cinema, Gilles Deleuze was less concerned with animated film, being somewhat dismissive of its capabilities. In recent years, however, a number of attempts have been made – most notably by William Schaffer, Thomas Lamarre and Dan Torre – to construct Deleuzian positions in animation theory. This article outlines some of these approaches, whilst engaging critically with Torre’s writings. In particular, it foregrounds Torre’s neglect of the post-structural, political dimension of Deleuzian thought through an examination of the concepts of faciality, the close-up, and relation as they occur in Deleuzian and Deleuzo-Guattarian philosophy. This is in part facilitated through a comparison of Stuart Blackton’s Humorous Phases of Funny Faces (1906) – a work directly addressed by Torre, and Emile Cohl’s Fantasmagorie (1908) – a work which he largely passes by. It is claimed here that, despite a number of apparent similarities, the animations of Cohl and Blackton express a radically divergent series of ontological commitments. Cohl offers the audience an experience of chaotic, mutable, relational complexity that revels in its incoherence, whilst Blackton presents a series of more straightforward set pieces, dwelling for the most part upon object-centric representational form. The tension between representation and becoming that occurs between these works is employed to facilitate a critical engagement with Torre’s process-cognitivism. It is suggested that Torre’s work, though exceptional in its pedagogic value, is likewise expressive of this tension, and that in its effort firstly to combine a series of process-philosophical and cognitivist ideas, and secondly to unpack the radical ideas of Deleuze through the more conservative philosophy of Nicholas Rescher, it runs the risk of falling back into a quasi-Kantian philosophy of generality and representation.},
 author = {Spencer Roberts},
 doi = {10.1177/1746847719831398},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1746847719831398},
 journal = {Animation},
 number = {1},
 pages = {5–21},
 title = {(In)Animate Semiotics: Virtuality and Deleuzian Illusion(s) of Life},
 url = {https://doi-org.crai.referencistas.com/10.1177/1746847719831398},
 volume = {14},
 year = {2019n}
}

@article{doi:10.1177/1747021818768502,
 abstract = {Current theory has divided memory into multiple systems, resulting in a fractionated account of human behaviour. By an alternative perspective, memory is a single system. However, debate over the details of different single-system theories has overshadowed the converging agreement among them, slowing the reunification of memory. Evidence in favour of dividing memory often takes the form of dissociations observed in amnesia, where amnesic patients are impaired on some memory tasks but not others. The dissociations are taken as evidence for separate explicit and implicit memory systems. We argue against this perspective. We simulate two key dissociations between classification and recognition in a computational model of memory, A Theory of Nonanalytic Association. We assume that amnesia reflects a quantitative difference in the quality of encoding. We also present empirical evidence that replicates the dissociations in healthy participants, simulating amnesic behaviour by reducing study time. In both analyses, we successfully reproduce the dissociations. We integrate our computational and empirical successes with the success of alternative models and manipulations and argue that our demonstrations, taken in concert with similar demonstrations with similar models, provide converging evidence for a more general set of single-system analyses that support the conclusion that a wide variety of memory phenomena can be explained by a unified and coherent set of principles.},
 author = {Evan T Curtis and Randall K Jamieson},
 doi = {10.1177/1747021818768502},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1747021818768502},
 journal = {Quarterly Journal of Experimental Psychology},
 note = {PMID:29554833},
 number = {4},
 pages = {798–817},
 title = {Computational and empirical simulations of selective memory impairments: Converging evidence for a single-system account of memory dissociations},
 url = {https://doi-org.crai.referencistas.com/10.1177/1747021818768502},
 volume = {72},
 year = {2019b}
}

@article{doi:10.1177/17470218211053309,
 abstract = {Two experiments were run to determine how presentation modality and duration influence children’s arithmetic performance and strategy selection. Third and fourth graders were asked to find estimates for two-digit addition problems (e.g., 52 + 39). Children were tested in three conditions: (1) time-unlimited visual, (2) time-limited visual, or (3) time-limited auditory conditions. Moreover, we assessed children’s working-memory updating and arithmetic fluency. Children were told which strategy to use on each problem to assess arithmetic performance while executing strategies, in Experiment 1, and were asked to choose the best strategy of three available strategies to assess strategy selection, in Experiment 2. Presentation modality influenced strategy execution (i.e., children were faster and more accurate in problems under visual than auditory conditions) but only in children with low updating abilities. In contrast, presentation modality had no effect on children’s strategy selection. Presentation duration had an effect on both strategy execution and strategy selection with time-limited presentation leading to a decline in children’s performance. Interestingly, specifically in children with low updating abilities, time-limited presentation led to poorer performance. Hence, efficient updating seemed to compensate for detrimental effects of auditory in comparison to visual and time-limited in comparison to time-unlimited presentation. These findings have important implications for determining conditions under which children execute strategies most efficiently and select the best strategy on each problem most often, as well as for understanding mechanisms underlying strategic behaviour.},
 author = {Svenja Hammerstein and Sebastian Poloczek and Patrick Lösche and Patrick Lemaire and Gerhard Büttner},
 doi = {10.1177/17470218211053309},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/17470218211053309},
 journal = {Quarterly Journal of Experimental Psychology},
 note = {PMID:34609216},
 number = {8},
 pages = {1448–1463},
 title = {Effects of presentation modality and duration on children’s strategy use: A study in computational estimation},
 url = {https://doi-org.crai.referencistas.com/10.1177/17470218211053309},
 volume = {75},
 year = {2022d}
}

@article{doi:10.1177/17479541221136238,
 abstract = {The sports statistics community is rapidly evolving. R is an open-source software in constant development that has gained a lot of popularity within statistical communities and sports analytics. This work presents a systematic review of the available R CRAN sport packages following PRISMA guidelines. We consider all packages active as of 18 February 2021. A total of 81 sport R packages created since 2006 were detected. Of these, 35.9% were authored by an American national, 69.1% include a dataset, 43.2% provide vignettes, and 65.4% have been updated at least once. The sport with the highest representation is basketball (n = 14, 17.3%), followed by soccer (n = 12, 14.8%) and packages related to physical activity (n = 11, 13.6%). There are no sports packages directed solely for the female gender, while 59.3% of packages are focused on professional athletes. Fifty packages (61.7%) are related to the sports performance analysis category, and web scraping (n = 43, 53.1%) was the main functionality in the collected packages. The creation of new R packages in the area of sports could help solve questions and tasks that still remain a challenge in this field, while continuing to help to improve the level of statistical education and computational thinking skills.},
 author = {Martí Casals and José Fernández and Victor Martínez and Michael Lopez and Klaus Langohr and Jordi Cortés},
 doi = {10.1177/17479541221136238},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/17479541221136238},
 journal = {International Journal of Sports Science & Coaching},
 number = {2},
 pages = {621–629},
 title = {A systematic review of sport-related packages within the R CRAN repository},
 url = {https://doi-org.crai.referencistas.com/10.1177/17479541221136238},
 volume = {18},
 year = {2023e}
}

@article{doi:10.1177/1748006X15595875,
 abstract = {System availability is a key performance measure in the process industry. It ensures continuous operation of facilities to meet production targets, personnel safety and environmental sustainability. Process machinery condition assessment, early fault detection and its management are vital elements to ensure overall system availability. These elements can be explored and managed effectively by extracting hidden knowledge from machinery vibration information to improve plant availability and safe operations. This article describes a decision tree–based computational intelligence model using machinery vibration data to detect machinery faults, their severity, and suggests appropriate action to avoid unscheduled failures. Vibration data for this work were collected using a machinery simulator and real-world machine to show the applicability of the proposed model. Later, the data were analyzed to detect faults using decision tree–based model that was developed in MATLAB. Fault detection classification accuracies of 98% during training and 93% during testing showed excellent performance of the proposed model. The model also revealed that the proposed formulation has capability of detecting faults correctly in the range of 98%−99%. The results showed that the proposed decision tree–based model is effective in evaluating the condition of process machinery and predicting unscheduled equipment breakdowns with better accuracy and with reduced human effort.},
 author = {Qadeer Ahmed and Fatai A Anifowose and Faisal Khan},
 doi = {10.1177/1748006X15595875},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1748006X15595875},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part O: Journal of Risk and Reliability},
 number = {6},
 pages = {612–626},
 title = {System availability enhancement using computational intelligence–based decision tree predictive model},
 url = {https://doi-org.crai.referencistas.com/10.1177/1748006X15595875},
 volume = {229},
 year = {2015a}
}

@article{doi:10.1177/1748301818762527,
 author = {Changhui Yang and Ruixia Yang and Tingting Xu and Yinxia Li},
 doi = {10.1177/1748301818762527},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1748301818762527},
 journal = {Journal of Algorithms & Computational Technology},
 number = {2},
 pages = {177–184},
 title = {Computational model of enterprise cooperative technology innovation risk based on nerve network},
 url = {https://doi-org.crai.referencistas.com/10.1177/1748301818762527},
 volume = {12},
 year = {2018t}
}

@article{doi:10.1177/17483026221084030,
 abstract = {A method for the solution of the three-dimensional structural dynamics equations with large strains using a finite volume technique is presented. The proposed solution procedure is second order accurate in space and employs a second-order accurate dual time-stepping scheme. The momentum conservation equations are written in terms of the Piola-Kirchhoff stresses. The stress tensor is related to the Lagrangian strain tensor through the St. Venant-Kirchhoff constitutive relationship. The structural solver presented is verified through two test cases. The first test case is a three-dimensional cantilever beam subject to a gravitational load that is verified using theory and two-dimensional simulations reported in literature. The second test case is a three-dimensional highly deformable cantilever plate subject to a gravitational load. The results of this case are verified through a comparison with the modal response calculated by commercially available software. The focus of the current effort is the development and verification of the structural dynamics portion of a future fully coupled monolithic fluid-thermal-structure interaction code package.},
 author = {Joseph S Graff and Roger L Davis and John P Clark},
 doi = {10.1177/17483026221084030},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/17483026221084030},
 journal = {Journal of Algorithms & Computational Technology},
 number = { },
 pages = {17483026221084030},
 title = {Computational structural dynamics general solution procedure using finite volumes},
 url = {https://doi-org.crai.referencistas.com/10.1177/17483026221084030},
 volume = {16},
 year = {2022h}
}

@article{doi:10.1177/17504813231177280,
 abstract = {Facebook remains the most important platform where social media editors package and try to ‘sell’ media outlets’ online news articles to audiences. In one of the first studies of its kind, we assess how this practice was effectuated during the first year of the COVID-19 pandemic. We use computational analysis to determine the polarity, subjectivity and use of some linguistics features in the status messages of 140,359 Facebook posts of 17 mainstream and alternative news titles from Flanders (Belgium) between March 2020 and 2021. Among other things, we find that status messages score considerably higher than headlines in terms of polarity and subjectivity, and that they, along with the use of question and interrogation marks, peaked in the first months of the pandemic. We contextualise our findings within existing scholarship and wider trends in increasingly digitised and globalised media societies.},
 author = {Jonathan Hendrickx and Annelien Van Remoortere and Michael Opgenhaffen},
 doi = {10.1177/17504813231177280},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/17504813231177280},
 journal = {Discourse & Communication},
 number = {6},
 pages = {701–720},
 title = {News packaging during a pandemic: A computational analysis of news diffusion via Facebook},
 url = {https://doi-org.crai.referencistas.com/10.1177/17504813231177280},
 volume = {17},
 year = {2023j}
}

@article{doi:10.1177/1753944718765734,
 abstract = {Background The aim of this study was to evaluate the rheolytic effects of stenting a mid-shaft/distal left main coronary artery (LMCA) lesion with and without ostial coverage. Stenting of the LMCA has emerged as a valid alternative in place of traditional coronary bypass graft surgery. However, in case of mid-shaft/distal lesion, there is no consensus regarding the extension of the strut coverage up to the ostium or to stent only the culprit lesion. Methods We reconstructed a left main-left descending coronary artery (LM-LCA)-left circumflex (LCX) bifurcation after analysing 100 consecutive patients (mean age 71.4 ± 9.3, 49 males) with LM mid-shaft/distal disease. The mean diameter of proximal LM, left anterior descending (LAD) and LCX, evaluated with quantitative coronary angiography (QCA) was 4.62 ± 0.86 mm, 3.31 ± 0.92 mm, and 2.74 ± 0.93 mm, respectively. For the stent simulation, a third-generation, everolimus-eluting stent was virtually reconstructed. Results After virtual stenting, the net area averaged wall shear stress (WSS) of the model and the WSS at the LCA-LCX bifurcation resulted higher when the stent covered the culprit mid-shaft lesion only compared with the extension of the stent covering the ostium (3.68 versus 2.06 Pa, p = 0.01 and 3.97 versus 1.98 Pa, p < 0.001, respectively. Similarly, the static pressure and the Reynolds number were significantly higher after stent implantation covering up the ostium. At the ostium, the flow resulted more laminar when stenting only the mid-shaft lesion than including the ostium. Conclusions Although these findings cannot be translated directly into real practice our brief study suggests that stenting lesion 1:1 or extending the stent to cover the LM ostium impacts differently the rheolytic properties of LMCA bifurcation with potential insights for restenosis or thrombosis.},
 author = {Gianluca Rigatelli and Marco Zuin and Fabio Dell’Avvocata and Thach Nguyen},
 doi = {10.1177/1753944718765734},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1753944718765734},
 journal = {Therapeutic Advances in Cardiovascular Disease},
 note = {PMID:29589515},
 number = {6},
 pages = {161–168},
 title = {Rheolytic effects of left main mid-shaft/distal stenting: a computational flow dynamic analysis},
 url = {https://doi-org.crai.referencistas.com/10.1177/1753944718765734},
 volume = {12},
 year = {2018q}
}

@article{doi:10.1177/1754073909103589,
 abstract = {Based on the belief that computational modeling (thinking in terms of representation and computations) can help to clarify controversial issues in emotion theory, this article examines emotional experience from the perspective of the Computational Belief–Desire Theory of Emotion (CBDTE), a computational explication of the belief–desire theory of emotion. It is argued that CBDTE provides plausible answers to central explanatory challenges posed by emotional experience, including: the phenomenal quality, intensity and object-directedness of emotional experience, the function of emotional experience and its relation to cognition and motivation, and the relation between emotional experience and emotion. In addition, CBDTE avoids most objections that have been raised against cognitive theories of emotion. A remaining objection, that beliefs are not necessary for the emotions covered by CBDTE, is rejected as empirically unsupported.},
 author = {Rainer Reisenzein},
 doi = {10.1177/1754073909103589},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1754073909103589},
 journal = {Emotion Review},
 number = {3},
 pages = {214–222},
 title = {Emotional Experience in the Computational Belief–Desire Theory of Emotion},
 url = {https://doi-org.crai.referencistas.com/10.1177/1754073909103589},
 volume = {1},
 year = {2009j}
}

@article{doi:10.1177/1754073912439777,
 abstract = {This comment discusses work by Aylett and Paiva (2012) which describes a synthetic approach to building a virtual world inhabited by synthetic characters where the user can experience subjective culture, that is, the experience of social reality, and learn how to empathetically communicate with people in other cultures. It provides a computational theory for integrating recent findings on emotion and cultural sensitivities into an interactive drama played by interacting characters with varying personalities. The FAtiMA-PSI, the implementation of their theory, has advanced a critical step towards basing affective intelligence on a firmer ground. Future challenges include building an empathic agent on the simulated physiology level that can be recognized as an independent ego with its own personality and presence.},
 author = {Toyoaki Nishida},
 doi = {10.1177/1754073912439777},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1754073912439777},
 journal = {Emotion Review},
 number = {3},
 pages = {269–270},
 title = {Building Empathic Agents? Comment on “Computational Modelling of Culture and Affect” by Aylett and Paiva},
 url = {https://doi-org.crai.referencistas.com/10.1177/1754073912439777},
 volume = {4},
 year = {2012q}
}

@article{doi:10.1177/1754073919898526,
 abstract = {We provide a short review on the recent and near-future developments of computational processing of emotion in the voice, highlighting (a) self-learning of representations moving continuously away from traditional expert-crafted or brute-forced feature representations to end-to-end learning, (b) a movement towards the coupling of analysis and synthesis of emotional voices to foster better mutual understanding, (c) weakly supervised learning at a large scale, (d) transfer learning from related domains such as speech recognition or cross-modal transfer learning, and (e) reinforced learning through interactive applications at a large scale. For each of these trends, we shortly explain their implications and potential use such as for interpretation in psychological studies and usage in digital health and digital psychology applications. We also discuss further potential development.},
 author = {Dagmar M. Schuller and Björn W. Schuller},
 doi = {10.1177/1754073919898526},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1754073919898526},
 journal = {Emotion Review},
 number = {1},
 pages = {44–50},
 title = {A Review on Five Recent and Near-Future Developments in Computational Processing of Emotion in the Human Voice},
 url = {https://doi-org.crai.referencistas.com/10.1177/1754073919898526},
 volume = {13},
 year = {2021n}
}

@article{doi:10.1177/1754337112439171,
 abstract = {Computer methods can assist in understanding the behaviour of the individual components of a helmet, beyond merely the headform output as is usually done in a laboratory environment or for test-house certification purposes. This design study uses a method that we have previously used to analyse the effects of helmet liner material properties. While the helmet liner is of vital importance for energy absorption, other design modifications can also serve to improve its performance. The equestrian helmet model previously developed and analysed by the authors was used in this study. The helmet shell and geometric factors, such as a gap between the liner and shell, ventilation holes and ridges on the helmet liner were studied to observe their influence on helmet performance. By studying helmet design variations in terms of different variables other than headform linear acceleration, it is possible to determine which helmet configurations perform better, why they perform the way they do and how efficiently they perform. This can assist the product design and optimization process by suggesting models which would optimize cost, weight and helmet size.},
 author = {Manuel A Forero Rueda and Michael D Gilchrist},
 doi = {10.1177/1754337112439171},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1754337112439171},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part P: Journal of Sports Engineering and Technology},
 number = {3–4},
 pages = {208–219},
 title = {Computational analysis and design of components of protective helmets},
 url = {https://doi-org.crai.referencistas.com/10.1177/1754337112439171},
 volume = {226},
 year = {2012i}
}

@article{doi:10.1177/1754337119853485,
 abstract = {Aerodynamics research in cycling has underpinned innovative bicycle design, new refined riding positions and optimised rider apparel. There has been a rise in the level of aerodynamics research focused on cycling since the turn of the millennium, enabled by significant increases in computational power and the availability of software/hardware. However, cycling research has not yet fully embraced para-cycling, with limited studies conducted on the aerodynamic performance of hand-cyclists and other para-cyclists. Wind tunnel experiments and computational fluid dynamics simulations were conducted in this research for the analysis of hand-cycling aerodynamics, focused on competitive H1–H4 category hand-cyclists. A quarter-scale representative geometry of a hand-cyclist was used in high-speed wind tunnel experiments. The accuracy of the simulations performed with the three-dimensional Reynolds-averaged Navier–Stokes equations was found to be dependent on the turbulence model choice and near-wall grid resolution. Computational fluid dynamics simulations predicted the magnitude of the drag and lateral forces to an accuracy of 2.5% using the shear stress transport turbulence model. This study also presents the impact of wheel diameter and disc wheels on hand-cycling aerodynamics via computational fluid dynamics simulations, providing a deeper understanding of the aerodynamic characteristics unique to the hand-cycling discipline in the sport of competitive cycling. Drag reductions of up to 8.9% were found when utilising 20-inch diameter spoked wheels, opposed to the 26-inch wheels. Variations in wheel diameter between the front and rear wheels were found to have a significant impact on the CDA in part through altering the pitch angle of the hand-cycle.},
 author = {Paul Mannion and Yasin Toparlar and Bert Blocken and Magdalena Hajdukiewicz and Thomas Andrianne and Eoghan Clifford},
 doi = {10.1177/1754337119853485},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1754337119853485},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part P: Journal of Sports Engineering and Technology},
 number = {4},
 pages = {286–300},
 title = {Computational fluid dynamics analysis of hand-cycle aerodynamics with static wheels: Sensitivity analyses and impact of wheel selection},
 url = {https://doi-org.crai.referencistas.com/10.1177/1754337119853485},
 volume = {235},
 year = {2021j}
}

@article{doi:10.1177/1756829316646640,
 abstract = {In this study, a computational fluid dynamics analysis was performed on bio-inspired micro aerial vehicle scale rigid flapping wings. The computational fluid dynamics analysis used a compressible unsteady Reynolds Averaged Navier–Stokes solver with low-Mach number preconditioning to study the complex, highly vortical, three-dimensional flow of low aspect ratio flapping wings at micro aerial vehicle-scale Reynolds numbers. The wing was flapped at a constant 5 Hz flap frequency at a mean chord reference Reynolds number of 25,000. The flapping and pitching kinematics were set to match those of a previous experimental study resulting in a constant flap stroke of 107° at translational pitching angles of 40°, 50°, and 60°. The force and flowfield measurements of the previous flapping-wing experiment were used for the validation of the 3D computational fluid dynamics model. The objectives of this effort were to understand the unsteady aerodynamic mechanisms and their relation to force production and aerodynamic efficiency on a rigid wing undergoing an insect-type flapping motion with passive pitching kinematics. Overall, the computational fluid dynamics results showed good agreement with the measured experimental force data. Additionally, the computational fluid dynamics simulation was able to adequately predict the process of leading edge vortex formation and shedding observed during experimentation. A vorticity summation approach used to calculate the strength of the leading edge vortex from the experimental measurements and from the computational fluid dynamics predicted flowfields showed comparable results. The computational fluid dynamics results were utilized to further analyze the differences in the flowfield and leading edge vortex formation for the three pitch angles tested as well as the instantaneous aerodynamic loads and aerodynamic power.},
 author = {James Lankford and David Mayo and Inderjit Chopra},
 doi = {10.1177/1756829316646640},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1756829316646640},
 journal = {International Journal of Micro Air Vehicles},
 number = {2},
 pages = {64–78},
 title = {Computational investigation of insect-based flapping wings for micro air vehicle applications},
 url = {https://doi-org.crai.referencistas.com/10.1177/1756829316646640},
 volume = {8},
 year = {2016i}
}

@article{doi:10.1177/1756829319833686,
 author = {Chao Huo and Peng Lv and Anbang Sun},
 doi = {10.1177/1756829319833686},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1756829319833686},
 journal = {International Journal of Micro Air Vehicles},
 number = { },
 pages = {1756829319833686},
 title = {Computational study on the aerodynamics of a long-shrouded contra-rotating rotor in hover},
 url = {https://doi-org.crai.referencistas.com/10.1177/1756829319833686},
 volume = {11},
 year = {2019k}
}

@article{doi:10.1177/18344909211010240,
 abstract = {Recent advances in artificial intelligence have brought attention to computational thinking (CT) in school education worldwide. However, little is known about the development of the literacy of CT in children, mainly because of the lack of proper psychometric assessments. We developed the first psychometrically validated assessment on the literacy of CT of children in Chinese elementary schools, coined as the Computational Thinking Assessment for Chinese Elementary Students (CTA-CES). Items were constructed to reflect key aspects of CT such as abstraction, algorithm thinking, decomposition, evaluation, and pattern recognition. To examine the test reliability and validity, we recruited two samples of 280 third- to sixth-grade students in total. Cronbach’s alpha provided evidence for the reliability of the test scores, item response theory analyses demonstrated psychometric appropriateness, whereas construct validity was verified by convergent validity, and criterion-related validity was confirmed by correlations between the CTA-CES and measures related to CT, namely reasoning, spatial ability, and verbal ability. In addition, an fMRI study further demonstrated similar neural activation patterns when students conducted the CTA-CES and programming tasks. Taken together, the CTA-CES is the first reliable and valid instrument for measuring the literacy of CT for Chinese children, and may be applicable to children worldwide.},
 author = {Yan Li and Shan Xu and Jia Liu},
 doi = {10.1177/18344909211010240},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/18344909211010240},
 journal = {Journal of Pacific Rim Psychology},
 number = { },
 pages = {18344909211010240},
 title = {Development and Validation of Computational Thinking Assessment of Chinese Elementary School Students},
 url = {https://doi-org.crai.referencistas.com/10.1177/18344909211010240},
 volume = {15},
 year = {2021n}
}

@article{doi:10.1177/18344909211038105,
 abstract = {This special issue raises two thematic questions: (1) How will AI change learning in the future and what role will human beings play in the interaction with machine learning, and (2), What can we learn from the articles in this special issue for future research? These questions are reflected in the frame of the recent discussion of human and machine learning. AI for learning provides many applications and multimodal channels for supporting people in cognitive and non-cognitive task domains. The articles in this special issue evidence that agency, engagement, self-efficacy, and collaboration are needed in learning and working with intelligent tools and environments. The importance of social elements is also clear in the articles. The articles also point out that the teacher’s role in digital pedagogy primarily involves facilitating and coaching. AI in learning has a high potential, but it also has many limitations. Many worries are linked with ethical issues, such as biases in algorithms, privacy, transparency, and data ownership. This special issue also highlights the concepts of explainability and explicability in the context of human learning. We need much more research and research-based discussion for making AI more trustworthy for users in learning environments and to prevent misconceptions.},
 author = {Hannele Niemi},
 doi = {10.1177/18344909211038105},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/18344909211038105},
 journal = {Journal of Pacific Rim Psychology},
 number = { },
 pages = {18344909211038104},
 title = {AI in learning: Preparing grounds for future learning},
 url = {https://doi-org.crai.referencistas.com/10.1177/18344909211038105},
 volume = {15},
 year = {2021j}
}

@article{doi:10.1177/18479804211062316,
 abstract = {This review deals with different aspects of hydrophobicity at nanostructured surfaces. Theoretical and geometric effects as well as those of surface feature geometry on hydrophobicity are explored in this article. This review includes surface modification methods used to change surface hydrophobicity and effect on adhesion of cells as nano substrate. A small chapter is devoted to hydrophobicity at self-assembled monolayers as a special type of nanostructured surface. To the different models describing hydrophobicity is devoted one up to dated chapter. Calculation methods including quantum, density functional theory, and molecular modeling bring novel perspectives to the study of hydrophobicity at nanostructured surfaces.},
 author = {Jan Macko and Natalia Podrojková and Renata Oriňaková and Andrej Oriňak},
 doi = {10.1177/18479804211062316},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/18479804211062316},
 journal = {Nanomaterials and Nanotechnology},
 number = { },
 pages = {18479804211062316},
 title = {New insights into hydrophobicity at nanostructured surfaces: Experiments and computational models},
 url = {https://doi-org.crai.referencistas.com/10.1177/18479804211062316},
 volume = {12},
 year = {2022l}
}

@article{doi:10.1177/1932202X221119499,
 abstract = {Using the test scores of more than 1,000,000 students who participated in the Advanced Placement Computer Science (AP CS) exams from 1997 to 2020, this study examined the direction and magnitude of the trends in gender disparity in participation and top achievement in advanced exams. The findings indicated that the male-to-female ratio (MFR) among AP Computer Science (CS) exam participants declined from 4.87 to 2.26 between 1997 and 2020. Similarly, the MFR among top scorers (students who scored 5 out of 5) in any type of AP CS exams declined rapidly, in favor of female students, from 8.00 to 2.14 during the same period. Possible implications of these findings for educators, particularly for AP CS teachers and school counselors, were also discussed in the context of the underrepresentation of females in computing fields.},
 author = {A. Kadir Bahar and Erdogan Kaya and Xiaolu Zhang},
 doi = {10.1177/1932202X221119499},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1932202X221119499},
 journal = {Journal of Advanced Academics},
 number = {4},
 pages = {574–603},
 title = {Gender Disparities in AP Computer Science Exams: Analysis of Trends in Participation and top Achievement},
 url = {https://doi-org.crai.referencistas.com/10.1177/1932202X221119499},
 volume = {33},
 year = {2022a}
}

@article{doi:10.1177/1932202X231218487,
 abstract = {This study explores the direction and magnitude of racial disparities on three advanced placement (AP) computer science (CS) exams, namely AP CS Principles, AP CS A, and AP CS AB, based on the test scores of more than one million students who have taken AP CS exams between 1997 and 2020. Using Mann–Kendall test and Sen’s slope procedures we found that the number of Black, Hispanic, and Native American students in AP CS exams have increased steadily and significantly over years, yet they are still far from reaching parity. Further, our findings suggest that the racial disparities among top achievers are very wide. The results provide educators and researchers support for identifying and quantifying the racial disparities in advanced academic programs and may inform the development of policies, practices, and programs to reduce racial disparities in pre-college CS education and further.},
 author = {Kadir Bahar and Erdogan Kaya and Xiaolu Zhang and Eter Mjavanadze},
 doi = {10.1177/1932202X231218487},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1932202X231218487},
 journal = {Journal of Advanced Academics},
 number = {3–4},
 pages = {240–270},
 title = {Quo Vadis Racial Disparities? Trend Analysis of the Participation and Top Achievement in Advanced Placement Computer Science Exams},
 url = {https://doi-org.crai.referencistas.com/10.1177/1932202X231218487},
 volume = {34},
 year = {2023a}
}

@article{doi:10.1177/1932202X241230589,
 abstract = {An issue arising in emergency distance education procedures, such as the response to the COVID-19 pandemic, is a lack of appropriate high-quality content and course activities for high ability students suitable for distance education. In this study, online CAD-based learning experiences structured with the Tinkercad Circuits Platform designed for gifted students were investigated based on the opinions of students regarding the distance education activity and the evaluations of the students’ scientific writing skills. This study used a single case holistic design with 10 gifted 6th-grade students at Usak Science and Art Center in Türkiye. All of the students stated that they had not previously encountered an activity such as the “smart air conditioning system”. The students also stated that although the activity was carried out in the form of distance education, it was positive and fun to be application-based, and that it was fun to research, discuss, design, and code with Tinkercad to look for a solution to the given problem. The evaluation of students’ products showed participants’ high level of proficiency in activities requiring advanced problem-solving skills, including planning the solution to the problem, creating an alternative plan for the solution, realizing, and evaluating the design.},
 author = {Cengiz Tüysüz and Nurettin Can Bodur and Ilker Ugulu},
 doi = {10.1177/1932202X241230589},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1932202X241230589},
 journal = {Journal of Advanced Academics},
 number = {2},
 pages = {329–356},
 title = {Tinkercad Circuits Platform-Based Learning Experiences of Gifted Students in the Emergency Distance Education Process},
 url = {https://doi-org.crai.referencistas.com/10.1177/1932202X241230589},
 volume = {35},
 year = {2024v}
}

@article{doi:10.1177/1932202X241244881,
 abstract = {Promoting the education of talented and gifted students is a crucial aspect of establishing a strong society focused on scientific knowledge. This praxis article delves into the implementation of strategies in the Syrian Arab Republic to support and identify gifted students, focusing on initiatives such as the Syrian Scientific Olympiad, the National Centre for Distinguished Students, and Academic Programs for Distinguished Students. Data collection methods utilized in this study shed light on the outcomes of these initiatives and measures taken to promote gifted education in Syria. The analysis of this data provides insights into the impact of these programs on gifted students in crisis-stricken regions. The study emphasizes the significant role played by the Distinction and Creativity Agency in nurturing exceptional talents and fostering personal growth. Overall, the support provided to gifted students contributes to cognitive advancement, psychological well-being, and skill development, enhancing their overall well-being and paving the way for successful futures. The government’s commitment to supporting gifted education in the Syrian Arab Republic reflects its dedication to promoting talent and creativity in the Arab region.},
 author = {Marwan Al-Raeei and Chadi Azmeh and Hala AlDakak},
 doi = {10.1177/1932202X241244881},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1932202X241244881},
 journal = {Journal of Advanced Academics},
 number = {0},
 pages = {1932202X241244881},
 title = {Enriching Minds: The Gifted Education Landscape in the Syrian Arab Republic},
 url = {https://doi-org.crai.referencistas.com/10.1177/1932202X241244881},
 volume = {0},
 year = {2024b}
}

@article{doi:10.1177/193229681200600623,
 abstract = {Objective: Set-inversion-based prandial insulin delivery is a new model-based bolus advisor for postprandial glucose control in type 1 diabetes mellitus (T1DM). It automatically coordinates the values of basal-bolus insulin to be infused during the postprandial period so as to achieve some predefined control objectives. However, the method requires an excessive computation time to compute the solution set of feasible insulin profiles, which impedes its integration into an insulin pump. In this work, a new algorithm is presented, which reduces computation time significantly and enables the integration of this new bolus advisor into current processing features of smart insulin pumps. Methods: A new strategy was implemented that focused on finding the combined basal-bolus solution of interest rather than an extensive search of the feasible set of solutions. Analysis of interval simulations, inclusion of physiological assumptions, and search domain contractions were used. Data from six real patients with T1DM were used to compare the performance between the optimized and the conventional computations. Results: In all cases, the optimized version yielded the basal-bolus combination recommended by the conventional method and in only 0.032% of the computation time. Simulations show that the mean number of iterations for the optimized computation requires approximately 3.59 s at 20 MHz processing power, in line with current features of smart pumps. Conclusions: A computationally efficient method for basal-bolus coordination in postprandial glucose control has been presented and tested. The results indicate that an embedded algorithm within smart insulin pumps is now feasible. Nonetheless, we acknowledge that a clinical trial will be needed in order to justify this claim.},
 author = {Fabian León-Vargas and Remei Calm and Jorge Bondia and Josep Vehí},
 doi = {10.1177/193229681200600623},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/193229681200600623},
 journal = {Journal of Diabetes Science and Technology},
 note = {PMID:23294789},
 number = {6},
 pages = {1420–1428},
 title = {Improving the Computational Effort of Set-Inversion-Based Prandial Insulin Delivery for Its Integration in Insulin Pumps},
 url = {https://doi-org.crai.referencistas.com/10.1177/193229681200600623},
 volume = {6},
 year = {2012k}
}

@article{doi:10.1177/1934578X0800300306,
 abstract = {On the basis of theoretical methods using DFT/B3LYP/6-31G* geometries and HF/6-311+G** GIAO NMR predictions, the 13C NMR spectroscopic data of a coumarin from Zanthoxylum rhoifolium was reassigned. The structure was revised to be 4-methoxy-3-(3-methylbut-2-enyl)-2H-chromen-2-one (4) and not 3-methoxy-4-(3-methylbut-2-enyl)-2H-chromen-2-one (2), as assumed previously.},
 author = {Augusto Rivera and Jaime Rios-Motta},
 doi = {10.1177/1934578X0800300306},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1934578X0800300306},
 journal = {Natural Product Communications},
 number = {3},
 pages = {1934578X0800300306},
 title = {Revised Structure by Computational Methods for a Coumarin Isolated from Zanthoxylum Rhoifolium (Rutaceae)},
 url = {https://doi-org.crai.referencistas.com/10.1177/1934578X0800300306},
 volume = {3},
 year = {2008q}
}

@article{doi:10.1177/1934578X1801300620,
 abstract = {Diphyllin-7′-O-β-D-glucoside, a podophyllotoxin derivative, was isolated from the roots of Dysosma versipellis. The 1H and 13C NMR spectra acquired at 300 K exhibited doubling of signals, suggesting the existence of two rotamers in solution. Variable-temperature 1H NMR experiments indicated a dynamic exchange process between the two rotamers. DFT calculations verified nearly equal energies for the two rotamers about the single bond C7′-O7.},
 author = {Shi-Wen Zhou and Fei Jiang and Yang Yu and Xiu-Yong Huang and Yong-Heng Wang and Shi-Lin Luo},
 doi = {10.1177/1934578X1801300620},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1934578X1801300620},
 journal = {Natural Product Communications},
 number = {6},
 pages = {1934578X1801300620},
 title = {Atropisomerism of a Podophyllotoxin Derivative: Experimental and Computational Study},
 url = {https://doi-org.crai.referencistas.com/10.1177/1934578X1801300620},
 volume = {13},
 year = {2018t}
}

@article{doi:10.1177/1934578X221096966,
 abstract = {Jiedu Huoxue Decoction (JHD), a recommended traditional prescription for patients with severe COVID-19, has appeared in the treatment protocols in China. Based on bioinformatics and computational chemistry methods, including molecular docking, molecular dynamics (MD) simulation, and Molecular Mechanics Generalized Born Surface Area (MM/GBSA) calculation, we aimed to reveal the mechanism of JHD in treating severe COVID-19. The compounds in JHD were obtained and screened on TCMSP, SwissADME, and ADMETLab platforms. The compound targets were obtained from TCMSP and STITCH, while COVID-19 targets were obtained from Genecards and NCBI. The protein-protein interaction network was constructed by using STRING. Gene Ontology (GO) and KEGG enrichment were performed with ClueGO and R language. AutoDock vina was employed for molecular docking. 100 ns MD simulation of the optimal docking complex was carried out with AmberTools 20. A total of 84 compounds and 29 potential targets of JHD for COVID-19 were collected. The key phytochemicals included quercetin, luteolin, β-sitosterol, puerarin, stigmasterol, kaempferol, and wogonin, which could regulate the immune system. The hub genes included IL6, IL10, VEGFA, IL1B, CCL2, HMOX1, DPP4, and ACE2. ACE2 and DPP4 were related to SARS-CoV-2 entering cells. GO and KEGG analysis showed that JHD could intervene in cytokine storm and endothelial proliferation and migration related to thrombosis. The molecular docking, 100 ns MD simulation, and MM/GBSA calculation confirmed that targets enriched in the COVID-19 pathway had high affinities with related compounds, and the conformations of the puerarin-ACE2, quercetin-EGFR, luteolin-EGFR, and quercetin-IL1B complexes were stable. In a word, JHD could treat COVID-19 by intervening in cytokine storm, thrombosis, and the entry of SARS-CoV-2, while regulating the immune system. These mechanisms were consistent with JHD’s therapeutic concept of “detoxification” and “promoting blood circulation and removing blood stasis” in treating COVID-19. The research provides a theoretical basis for the development and application of JHD.},
 author = {Ying Liu and Han Yan and Hui-bin Jia and Li Pan and Jia-zheng Liu and Ya-wen Zhang and Jing Wang and Dao-gang Qin and Lei Ma and Ting Wang},
 doi = {10.1177/1934578X221096966},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1934578X221096966},
 journal = {Natural Product Communications},
 number = {4},
 pages = {1934578X221096966},
 title = {Jiedu Huoxue Decoction for Cytokine Storm and Thrombosis in Severe COVID-19: A Combined Bioinformatics and Computational Chemistry Approach},
 url = {https://doi-org.crai.referencistas.com/10.1177/1934578X221096966},
 volume = {17},
 year = {2022j}
}

@article{doi:10.1177/1934578X241237911,
 abstract = {Objectives The current study was conducted to evaluate the antibacterial potential of leaf and fruit extracts of Zanthoxylum armatum against two pathogenic bacterial isolates, Staphylococcus aureus and Staphylococcus epidermidis. Methods Twelve commercially available antibiotics were tested S. aureus and S. epidermidis by antimicrobial susceptibility test (AST). Qualitative analysis of phytochemicals was performed to evaluate the presence of certain secondary metabolites. The activity of Z. armatum extracts against S. aureus and S. epidermidis was measured as a maximum zone of inhibition exhibited by each leaf and fruit extract. An in-silico study was conducted on flavonoids and alkaloids to show their binding affinity with the PBP2a receptor protein of S. aureus and TcaR of S. epidermidis. Results The AST revealed that S. aureus was resistant to Penicillin, Ampicillin, Clindamycin, Vancomycin, Rifampicin, Novobiocin, and Oxacillin, whereas S. epidermidis was resistant to Streptomycin, Oxacillin, Tetracycline, and Novobiocin. Qualitative analysis of phytochemicals resulted in the presence of Saponins, fixed oils, flavonoids, alkaloids, starch, and fatty acids in both leaf and fruit extracts. The maximum zone of inhibition against S. aureus was produced by methanolic leaf extracts of Z. armatum and chloroform fruit extracts. For S. epidermidis, the best activity was exhibited by benzene leaf extracts and methanolic fruit extracts. An in-silico study showed that flavonoids Nitidine and Nevadensin exhibited binding affinity with the PBP2a receptor protein higher than selected antibiotics, ie, Penicillin, Chloramphenicol, and Oxacillin. TcaR of S. epidermidis interacted with Tambuletin, followed by Nitidine and Kaempferol. Conclusion After in vitro testing, in silico analysis advised extracting and purifying the bioactive components from Z. armatum extracts that showed significant interaction with bacterial virulence proteins for use as natural antibiotics against antibiotic-resistant bacteria.},
 author = {Mamuna Mukhtar and Haris Ahmed Khan and Shumaila Naz},
 doi = {10.1177/1934578X241237911},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1934578X241237911},
 journal = {Natural Product Communications},
 number = {3},
 pages = {1934578X241237911},
 title = {Antibacterial Profiling of Zanthoxylum armatum Extracts: A Comprehensive Computational and Experimental Study},
 url = {https://doi-org.crai.referencistas.com/10.1177/1934578X241237911},
 volume = {19},
 year = {2024m}
}

@article{doi:10.1177/1937586720959766,
 abstract = {Objectives: This study proposes a computational model to evaluate patient room design layout and features that contribute to patient stability and mitigate the risk of fall. Background: While common fall risk assessment tools in nursing have an acceptable level of sensitivity and specificity, they focus on intrinsic factors and medications, making risk assessment limited in terms of how the physical environment contributes to fall risk. Methods: We use literature to inform a computational model (algorithm) to define the relationship between these factors and the risk of fall. We use a trajectory optimization approach for patient motion prediction. Results: Based on available data, the algorithm includes static factors of lighting, flooring, supportive objects, and bathroom doors and dynamic factors of patient movement. This preliminary model was tested using four room designs as examples of typical room configurations. Results show the capabilities of the proposed model to identify the risk associated with different room layouts and features. Conclusions: This innovative approach to room design evaluation and resulting estimation of patient fall risk show promise as a proactive evidence-based tool to evaluate the relationship of potential fall risk and room design. The development of the model highlights the challenge of heterogeneity in factors and reporting found in the studies of patient falls, which hinder our understanding of the role of the built environment in mitigating risk. A more comprehensive investigation comparing the model with actual patient falls data is needed to further refine model development.},
 author = {Roya Sabbagh Novin and Ellen Taylor and Tucker Hermans and Andrew Merryweather},
 doi = {10.1177/1937586720959766},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1937586720959766},
 journal = {HERD: Health Environments Research & Design Journal},
 note = {PMID:32969295},
 number = {2},
 pages = {350–367},
 title = {Development of a Novel Computational Model for Evaluating Fall Risk in Patient Room Design},
 url = {https://doi-org.crai.referencistas.com/10.1177/1937586720959766},
 volume = {14},
 year = {2021m}
}

@article{doi:10.1177/19401612231204535,
 abstract = {This study examines how the news framing of immigration influences the public’s feelings toward immigrants and their preference for immigration policy in the United States. Unlike prior experimental research that documents the respondents’ immediate reactions to several hand-crafted news frames, this study provides strong empirical evidence for the association between the respondents’ real-world news exposure and their opinion change over time. Combining a computational media content analysis and a two-wave panel survey, the research demonstrates that while exposure to certain frames in the mainstream media would directly lead to public support for a stricter immigration policy, partisan media tend to affect public opinion indirectly by influencing their feelings toward immigrants in opposite directions.},
 author = {Lei Guo and Chris Chao Su and Hsuan-Ting Chen},
 doi = {10.1177/19401612231204535},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/19401612231204535},
 journal = {The International Journal of Press/Politics},
 number = {0},
 pages = {19401612231204536},
 title = {Do News Frames Really Have Some Influence in the Real World? A Computational Analysis of Cumulative Framing Effects on Emotions and Opinions About Immigration},
 url = {https://doi-org.crai.referencistas.com/10.1177/19401612231204535},
 volume = {0},
 year = {2023j}
}

@article{doi:10.1177/1945892420902005,
 abstract = {Background Endoscopic medial maxillectomy (EMM) is a workhorse for multiple sinonasal conditions. To reduce its burden on the sinonasal physiology, several modified EMM (M-EMM) have been proposed. Objective: In order to provide a theoretical basis for EMM and its modifications, this study introduces a computational fluid dynamics (CFD) model, based on a time-resolved direct numerical simulation, describing EMM and assessing the role of the M-EMM in preserving the overall fluid dynamics of the sinonasal cavities. Methods A normal sinonasal CT scan was converted into a geometrical model and used as a reference; 2 anatomies were then created by virtual surgery, mimicking EMM and M-EMM, with the latter sparing the anterior portion of inferior turbinate and medial maxillary sinus wall. The airflow was simulated in the models via the OpenFOAM CFD software and compared in terms of flow rate, mean and fluctuating velocity, vorticity, and turbulent structures. Results The analysis shows that EMM induces a massive flow rate increase in the operated side, which becomes less obvious in the M-EMM model. In contrast to M-EMM, EMM induces higher velocity fields that reach the maxillary sinus. Velocity and vorticity fluctuations are negligible in the baseline model, but become increasingly evident and widespread in the M-EMM and EMM models. Conclusions A significant disruption of the nasal fluid dynamics is observed in EMM, while M-EMM minimizes variations and reduces interference with nasal air conditioning. Our analysis provides insights into the pathophysiology of radical sinus surgery and provides a theoretical basis for the ability of M-EMM to reduce the temporary surgery-related changes on both healthy and operated sides.},
 author = {Alberto M. Saibene and Giovanni Felisati and Carlotta Pipolo and Antonio Mario Bulfamante and Maurizio Quadrio and Vanessa Covello},
 doi = {10.1177/1945892420902005},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1945892420902005},
 journal = {American Journal of Rhinology & Allergy},
 note = {PMID:31992048},
 number = {3},
 pages = {409–416},
 title = {Partial Preservation of the Inferior Turbinate in Endoscopic Medial Maxillectomy: A Computational Fluid Dynamics Study},
 url = {https://doi-org.crai.referencistas.com/10.1177/1945892420902005},
 volume = {34},
 year = {2020o}
}

@article{doi:10.1177/1945892420950157,
 abstract = {Background Past studies reported a low correlation between rhinomanometry and computational fluid dynamics (CFD), but the source of the discrepancy was unclear. Low correlation or lack of correlation has also been reported between subjective and objective measures of nasal patency. Objective: This study investigates (1) the correlation and agreement between nasal resistance derived from CFD (RCFD) and rhinomanometry (RRMN), and (2) the correlation between objective and subjective measures of nasal patency. Methods Twenty-five patients with nasal obstruction underwent anterior rhinomanometry before and after mucosal decongestion with oxymetazoline. Subjective nasal patency was assessed with a 0-10 visual analog scale (VAS). CFD simulations were performed based on computed tomography scans obtained after mucosal decongestion. To validate the CFD methods, nasal resistance was measured in vitro (REXPERIMENT) by performing pressure-flow experiments in anatomically accurate plastic nasal replicas from 6 individuals. Results Mucosal decongestion was associated with a reduction in bilateral nasal resistance (0.34 ± 0.23 Pa.s/ml to 0.19 ± 0.24 Pa.s/ml, p = 0.003) and improved sensation of nasal airflow (bilateral VAS decreased from 5.2 ± 1.9 to 2.6 ± 1.9, p < 0.001). A statistically significant correlation was found between VAS in the most obstructed cavity and unilateral airflow before and after mucosal decongestion (r = −0.42, p = 0.003). Excellent correlation was found between RCFD and REXPERIMENT (r = 0.96, p < 0.001) with good agreement between the numerical and in vitro values (RCFD/REXPERIMENT = 0.93 ± 0.08). A weak correlation was found between RCFD and RRMN (r = 0.41, p = 0.003) with CFD underpredicting nasal resistance derived from rhinomanometry (RCFD/RRMN = 0.65 ± 0.63). A stronger correlation was found when unilateral airflow at a pressure drop of 75 Pa was used to compare CFD with rhinomanometry (r = 0.76, p < 0.001). Conclusion CFD and rhinomanometry are moderately correlated, but CFD underpredicts nasal resistance measured in vivo due in part to the assumption of rigid nasal walls. Our results confirm previous reports that subjective nasal patency correlates better with unilateral than with bilateral measurements and in the context of an intervention.},
 author = {Giancarlo B. Cherobin and Richard L. Voegels and Fábio R. Pinna and Eloisa M. M. S. Gebrim and Ryan S. Bailey and Guilherme J. M. Garcia},
 doi = {10.1177/1945892420950157},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1945892420950157},
 journal = {American Journal of Rhinology & Allergy},
 note = {PMID:32806938},
 number = {2},
 pages = {245–255},
 title = {Rhinomanometry Versus Computational Fluid Dynamics: Correlated, but Different Techniques},
 url = {https://doi-org.crai.referencistas.com/10.1177/1945892420950157},
 volume = {35},
 year = {2021b}
}

@article{doi:10.1177/19458924221137982,
 abstract = {Background Nasal adhesions (NAs) are a known complication of nasal airway surgery. Even minor NAs can lead to significant postoperative nasal airway obstruction (NAO). Division of such NAs often provides much greater relief than anticipated. Objective We examine the impact of NAs at various anatomical sites on nasal airflow and mucosal cooling using computational fluid dynamics (CFD) and multiple test subjects. Methods CT scans of healthy adult subjects were used to construct three-dimensional nasal airway computational models. A single virtual 2.5 mm diameter NA was placed at one of five sites commonly seen following NAO surgery within each nasal cavity bilaterally, resulting in 10 NA models and 1 NA-free control for each subject. CFD analysis was performed on each NA model and compared with the subject’s NA-free control model. Results 4 subjects were recruited to create 44 computational models. The NAs caused the airflow streamlines to separate, leading to a statistically significant increase in mucosal temperature immediately downstream to the NAs (wake region). Changes in the mucosal temperature in the wake region of the NAs were most prominent in anteriorly located NAs with a mean increase of 1.62 °C for the anterior inferior turbinate NAs (P < .001) and 0.63 °C for the internal valve NAs (P < .001). Conclusion NAs result in marked disruption to airflow patterns and reduced mucosal cooling on critical surfaces, particularly in the wake region. Reduced wake region mucosal cooling may be a contributing factor to the exaggerated perception of nasal obstruction experienced by patients with NAs.},
 author = {Praween Senanayake and Patrick Warfield-McAlpine and Hana Salati and Kimberley Bradshaw and Eugene Wong and Kiao Inthavong and Narinder Singh},
 doi = {10.1177/19458924221137982},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/19458924221137982},
 journal = {American Journal of Rhinology & Allergy},
 note = {PMID:36373577},
 number = {3},
 pages = {273–283},
 title = {The Impact of Adhesions on Nasal Airflow: A Quantitative Analysis Using Computational Fluid Dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1177/19458924221137982},
 volume = {37},
 year = {2023q}
}

@article{doi:10.1177/194675671000200204,
 author = {Anvar Idiatullin},
 doi = {10.1177/194675671000200204},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/194675671000200204},
 journal = {World Futures Review},
 number = {2},
 pages = {16–22},
 title = {About the Philosophy for Future Thinking},
 url = {https://doi-org.crai.referencistas.com/10.1177/194675671000200204},
 volume = {2},
 year = {2010l}
}

@article{doi:10.1177/1948550613492345,
 abstract = {We analyze data from nearly 2 million text messages (tweets) across over 16,000 users on Twitter to examine differences between Christians and atheists in natural language. Analyses reveal that Christians use more positive emotion words and less negative emotion words than atheists. Moreover, two independent paths predict differences in expressions of happiness: frequency of words related to an intuitive (vs. analytic) thinking style and frequency of words related to social relationships. These findings provide the first evidence that the relationship between religion and happiness is partially mediated by thinking style. This research also provides support for previous laboratory studies and self-report data, suggesting that social connection partially mediates the relationship between religiosity and happiness. Implications for theory and the future of social science using computational methods to analyze social media are discussed.},
 author = {Ryan S. Ritter and Jesse Lee Preston and Ivan Hernandez},
 doi = {10.1177/1948550613492345},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1948550613492345},
 journal = {Social Psychological and Personality Science},
 number = {2},
 pages = {243–249},
 title = {Happy Tweets: Christians Are Happier, More Socially Connected, and Less Analytical Than Atheists on Twitter},
 url = {https://doi-org.crai.referencistas.com/10.1177/1948550613492345},
 volume = {5},
 year = {2014k}
}

@article{doi:10.1177/1971400917740362,
 abstract = {Brain atlases have a wide range of use from education to research to clinical applications. Mathematical methods as well as computational methods and tools play a major role in the process of brain atlas building and developing atlas-based applications. Computational methods and tools cover three areas: dedicated editors for brain model creation, brain navigators supporting multiple platforms, and atlas-assisted specific applications. Mathematical methods in atlas building and developing atlas-aided applications deal with problems in image segmentation, geometric body modelling, physical modelling, atlas-to-scan registration, visualisation, interaction and virtual reality. Here I overview computational and mathematical methods in atlas building and developing atlas-assisted applications, and share my contribution to and experience in this field.},
 author = {Wieslaw L Nowinski},
 doi = {10.1177/1971400917740362},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1971400917740362},
 journal = {The Neuroradiology Journal},
 note = {PMID:29096578},
 number = {6},
 pages = {520–534},
 title = {Computational and mathematical methods in brain atlasing},
 url = {https://doi-org.crai.referencistas.com/10.1177/1971400917740362},
 volume = {30},
 year = {2017o}
}

@article{doi:10.1177/1971400918759812,
 abstract = {Background Investigators use phase-contrast magnetic resonance (PC-MR) and computational fluid dynamics (CFD) to assess cerebrospinal fluid dynamics. We compared qualitative and quantitative results from the two methods. Methods Four volunteers were imaged with a heavily T2-weighted volume gradient echo scan of the brain and cervical spine at 3T and with PC-MR. Velocities were calculated from PC-MR for each phase in the cardiac cycle. Mean pressure gradients in the PC-MR acquisition through the cardiac cycle were calculated with the Navier-Stokes equations. Volumetric MR images of the brain and upper spine were segmented and converted to meshes. Models of the subarachnoid space were created from volume images with the Vascular Modeling Toolkit. CFD simulations were performed with a previously verified flow solver. The flow patterns, velocities and pressures were compared in PC-MR and CFD flow images. Results PC-MR images consistently revealed more inhomogeneous flow patterns than CFD, especially in the anterolateral subarachnoid space where spinal nerve roots are located. On average, peak systolic and diastolic velocities in PC-MR exceeded those in CFD by 31% and 41%, respectively. On average, systolic and diastolic pressure gradients calculated from PC-MR exceeded those of CFD by 11% and 39%, respectively. Conclusions PC-MR shows local flow disturbances that are not evident in typical CFD. The velocities and pressure gradients calculated from PC-MR are systematically larger than those calculated from CFD.},
 author = {Erika Kristina Lindstrøm and Jakob Schreiner and Geir Andre Ringstad and Victor Haughton and Per Kristian Eide and Kent-Andre Mardal},
 doi = {10.1177/1971400918759812},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/1971400918759812},
 journal = {The Neuroradiology Journal},
 note = {PMID:29464985},
 number = {3},
 pages = {292–298},
 title = {Comparison of phase-contrast MR and flow simulations for the study of CSF dynamics in the cervical spine},
 url = {https://doi-org.crai.referencistas.com/10.1177/1971400918759812},
 volume = {31},
 year = {2018n}
}

@article{doi:10.1177/2041386612450455,
 abstract = {Computational modeling has long been advocated as an important tool in the scientist’s tool shed. They are common in physical and biological sciences, but are very rare in organizational psychology. This paper describes the role computational models might play in informing theory and science in organizational psychology. After describing the major advantages of computational models, architectures, examples, and resources are described. This is followed by a comprehensive review of where computational models have been applied and where they might be profitably applied in each of the major domains of organizational psychology.},
 author = {Justin M. Weinhardt and Jeffrey B. Vancouver},
 doi = {10.1177/2041386612450455},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2041386612450455},
 journal = {Organizational Psychology Review},
 number = {4},
 pages = {267–292},
 title = {Computational models and organizational psychology: Opportunities abound},
 url = {https://doi-org.crai.referencistas.com/10.1177/2041386612450455},
 volume = {2},
 year = {2012p}
}

@article{doi:10.1177/2041386614547955,
 abstract = {Emergent phenomena—those that manifest bottom-up from the psychological characteristics, perceptions, and interactions among individuals—are a fundamental dynamic process in multilevel theory, but have been treated in a very limited way in the research literature. In particular, treatments are largely assumed (rather than observed directly), retrospective, and static. This paper describes a research paradigm designed to examine directly the dynamics of micro-meso—individual, dyad, and team—emergent phenomena. We identify, describe, and illustrate the sequence of theoretical, measurement, computational, data analytic, and systematic research activities that are necessary to operationalize and utilize the paradigm. We illustrate the paradigm development process using our research, focused on learning and team knowledge emergence, and highlight key design principles that can be applied to examine other emergent phenomena in teams. We conclude with a discussion of contributions, strengths and limitations, and generalization of the approach to other emergent phenomena in teams.},
 author = {Steve W. J. Kozlowski and Georgia T. Chao and James A. Grand and Michael T. Braun and Goran Kuljanin},
 doi = {10.1177/2041386614547955},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2041386614547955},
 journal = {Organizational Psychology Review},
 number = {1},
 pages = {3–33},
 title = {Capturing the multilevel dynamics of emergence: Computational modeling, simulation, and virtual experimentation},
 url = {https://doi-org.crai.referencistas.com/10.1177/2041386614547955},
 volume = {6},
 year = {2016d}
}

@article{doi:10.1177/2041419619889071,
 abstract = {Soft armour consisting of multi-layered high-performance fabrics are a popular choice for personal protection. Extensive work done in the last few decades suggests that shear thickening fluids improve the impact resistance of woven fabrics. Shear thickening fluid–impregnated fabrics have been proven as an ideal candidate for producing comfortable, high-performance soft body armour. However, the mechanism of defeating a projectile using a shear thickening fluid–impregnated multi-layered fabric is not fully understood and can be considered as a gap in the research done on the improvement of soft armour. Even though considerable progress has been achieved on dry fabrics, limited studies have been performed on shear thickening fluid–impregnated fabrics. The knowledge of simulation of multi-layered fabric armour is not well developed. The complexity in creating the geometry of the yarns, incorporating friction between yarns and initial pre-tension between yarns due to weaving patterns make the numerical modelling a complex process. In addition, the existing knowledge in this area is widely dispersed in the published literature and requires synthesis to enhance the development of shear thickening fluid–impregnated fabrics. Therefore, this article aims to provide a comprehensive review of the current methods of modelling shear thickening fluid–impregnated fabrics with a critical analysis of the techniques used. The review is preceded by an overview of shear thickening behaviour and related mechanisms, followed by a discussion of innovative approaches in numerical modelling of fabrics. A novel state-of-the-art means of modelling shear thickening fluid–impregnated fabrics is proposed in conclusion of the review of current methods. A short case study is also presented using the proposed approach of modelling.},
 author = {Dakshitha Weerasinghe and Damith Mohotti and Jeremy Anderson},
 doi = {10.1177/2041419619889071},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2041419619889071},
 journal = {International Journal of Protective Structures},
 number = {3},
 pages = {340–378},
 title = {Incorporation of shear thickening fluid effects into computational modelling of woven fabrics subjected to impact loading: A review},
 url = {https://doi-org.crai.referencistas.com/10.1177/2041419619889071},
 volume = {11},
 year = {2020t}
}

@article{doi:10.1177/20414196221085720,
 abstract = {Numerical analysis is increasingly used for batch modelling runs, with each individual model possessing a unique combination of input parameters sampled from a range of potential values. Whilst such an approach can help to develop a comprehensive understanding of the inherent unpredictability and variability of explosive events, or populate training/validation data sets for machine learning approaches, the associated computational expense is relatively high. Furthermore, any given model may share a number of common solution steps with other models in the batch, and simulating all models from birth to termination may result in large amounts of repetition. This paper presents a new branching algorithm that ensures calculation steps are only computed once by identifying when the parameter fields of each model in the batch becomes unique. This enables informed data mapping to take place, leading to a reduction in the required computation time. The branching algorithm is explained using a conceptual walk-through for a batch of 9 models, featuring a blast load acting on a structural panel in 2D. By eliminating repeat steps, approximately 50% of the run time can be saved. This is followed by the development and use of the algorithm in 3D for a practical application involving 20 complex containment structure models. In this instance, a ∼20% reduction in computational costs is achieved.},
 author = {Adam A Dennis and Danny J Smyl and Chris G Stirling and Samuel E Rigby},
 doi = {10.1177/20414196221085720},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/20414196221085720},
 journal = {International Journal of Protective Structures},
 number = {2},
 pages = {135–167},
 title = {A branching algorithm to reduce computational time of batch models: Application for blast analyses},
 url = {https://doi-org.crai.referencistas.com/10.1177/20414196221085720},
 volume = {14},
 year = {2023g}
}

@article{doi:10.1177/20420986211041277,
 abstract = {Introduction: Tuberculosis is a major respiratory disease globally with a higher prevalence in Asian and African countries than rest of the world. With a larger population of tuberculosis patients anticipated to be co-infected with COVID-19 infection, an ongoing pandemic, identifying, preventing and managing drug–drug interactions is inevitable for maximizing patient benefits for the current repurposed COVID-19 and antitubercular drugs. Methods: We assessed the potential drug–drug interactions between repurposed COVID-19 drugs and antitubercular drugs using the drug interaction checker of IBM Micromedex®. Extensive computational studies were performed at a molecular level to validate and understand the drug–drug interactions found from the Micromedex drug interaction checker database at a molecular level. The integrated knowledge derived from Micromedex and computational data was collated and curated for predicting potential drug–drug interactions between repurposed COVID-19 and antitubercular drugs. Results: A total of 91 potential drug–drug interactions along with their severity and level of documentation were identified from Micromedex between repurposed COVID-19 drugs and antitubercular drugs. We identified 47 pharmacodynamic, 42 pharmacokinetic and 2 unknown DDIs. The majority of our molecular modelling results were in line with drug–drug interaction data obtained from the drug information software. QT prolongation was identified as the most common type of pharmacodynamic drug–drug interaction, whereas drug–drug interactions associated with cytochrome P450 3A4 (CYP3A4) and P-glycoprotein (P-gp) inhibition and induction were identified as the frequent pharmacokinetic drug–drug interactions. The results suggest antitubercular drugs, particularly rifampin and second-line agents, warrant high alert and monitoring while prescribing with the repurposed COVID-19 drugs. Conclusion: Predicting these potential drug–drug interactions, particularly related to CYP3A4, P-gp and the human Ether-à-go-go-Related Gene proteins, could be used in clinical settings for screening and management of drug–drug interactions for delivering safer chemotherapeutic tuberculosis and COVID-19 care. The current study provides an initial propulsion for further well-designed pharmacokinetic-pharmacodynamic-based drug–drug interaction studies. Plain Language Summary Introduction: Tuberculosis is a major respiratory disease globally with a higher prevalence in Asian and African countries than rest of the world. With a larger population of tuberculosis patients predicted to be infected with COVID-19 during this period, there is a higher risk for the occurrence of medication interactions between the medicines used for COVID-19 and tuberculosis. Hence, identifying and managing these interactions is vital to ensure the safety of patients undergoing COVID-19 and tuberculosis treatment simultaneously. Methods: We studied the major medication interactions that could likely happen between the various medicines that are currently given for COVID-19 and tuberculosis treatment using the medication interaction checker of a drug information software (Micromedex®). In addition, thorough molecular modelling was done to confirm and understand the interactions found from the medication interaction checker database using specific docking software. Molecular docking is a method that predicts the preferred orientation of one medicine molecule to a second molecule, when bound to each other to form a stable complex. Knowledge of the preferred orientation may be used to determine the strength of association or binding affinity between two medicines using scoring functions to determine the extent of the interactions between medicines. The combined knowledge from Micromedex and molecular modelling data was used to properly predict the potential medicine interactions between currently used COVID-19 and antitubercular medicines. Results: We found a total of 91 medication interactions from Micromedex. Majority of our molecular modelling findings matched with the interaction information obtained from the drug information software. QT prolongation, an abnormal heartbeat, was identified as one of the most common interactions. Our findings suggest that antitubercular medicines, mainly rifampin and second-line agents, suggest high alert and scrutiny while prescribing with the repurposed COVID-19 medicines. Conclusion: Our current study highlights the need for further well-designed studies confirming the current information for recommending safe prescribing in patients with both infections.},
 author = {Levin Thomas and Sumit Raosaheb Birangal and Rajdeep Ray and Sonal Sekhar Miraj and Murali Munisamy and Muralidhar Varma and Chidananda Sanju S.V. and Mithu Banerjee and Gautham G. Shenoy and Mahadev Rao},
 doi = {10.1177/20420986211041277},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/20420986211041277},
 journal = {Therapeutic Advances in Drug Safety},
 note = {PMID:34471515},
 number = { },
 pages = {20420986211041276},
 title = {Prediction of potential drug interactions between repurposed COVID-19 and antitubercular drugs: an integrational approach of drug information software and computational techniques data},
 url = {https://doi-org.crai.referencistas.com/10.1177/20420986211041277},
 volume = {12},
 year = {2021r}
}

@article{doi:10.1177/2042753017731356,
 author = {Frances Tracy and Patrick Carmichael},
 doi = {10.1177/2042753017731356},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2042753017731356},
 journal = {E-Learning and Digital Media},
 number = {3},
 pages = {164–182},
 title = {Disrupting the dissertation: Linked data, enhanced publication and algorithmic culture},
 url = {https://doi-org.crai.referencistas.com/10.1177/2042753017731356},
 volume = {14},
 year = {2017r}
}

@article{doi:10.1177/2042753017731357,
 author = {Jennifer Jenson and Milena Droumeva},
 doi = {10.1177/2042753017731357},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2042753017731357},
 journal = {E-Learning and Digital Media},
 number = {4},
 pages = {212–225},
 title = {Revisiting the media generation: Youth media use and computational literacy instruction},
 url = {https://doi-org.crai.referencistas.com/10.1177/2042753017731357},
 volume = {14},
 year = {2017k}
}

@article{doi:10.1177/2042753017752583,
 abstract = {Technology is an all-encompassing aspect of life in the 21st century. Its existence has implications on how communication occurs, education is shaped, knowledge is spread, and ideas are formulated. There is a significant shift taking place in society as we become more accustomed to existing in a digital world. Digital natives, young people who have been born into a virtual reality, view the world differently, have a ‘digital footprint,’ process info-graphics speedily, but lack basic capacity for interpersonal interactions. They also present neurological differences from those who were exposed to digital technologies later in life. However, regardless of human capacity for technological understanding, digital technologies adversely impact our shared humanity and the ubiquitous nature of these technologies is quite frightening. As a high school English teacher, I experience the impact of digital technologies on learning and language expression first-hand through my work with digital natives. My concern is that because of student dependence on the rapid influx of digital technologies, they will not possess certain imperative faculties of the mind including the ability to embrace mystery, wonderment, and inquiry. There is also concern for the potential loss of creativity. The research that follows attempts to evaluate the impact – both positive and negative – on the domain of language expression including reading, writing, and faculties for imagination and critical thinking. Through thorough examination of neuroscience, trends in reading and writing, usage of electronic communications, social media and politics, levels of digital literacy, primary observations of high school students in a tech-dependent classroom, the evaluations that follow form a basis for theoretical assumptions about technology’s impact on language expression and education.},
 author = {Andrea E Cladis},
 doi = {10.1177/2042753017752583},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2042753017752583},
 journal = {E-Learning and Digital Media},
 number = {5},
 pages = {341–364},
 title = {A shifting paradigm: An evaluation of the pervasive effects of digital technologies on language expression, creativity, critical thinking, political discourse, and interactive processes of human communications},
 url = {https://doi-org.crai.referencistas.com/10.1177/2042753017752583},
 volume = {17},
 year = {2020b}
}

@article{doi:10.1177/2042753018757757,
 author = {Enrico Gandolfi},
 doi = {10.1177/2042753018757757},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2042753018757757},
 journal = {E-Learning and Digital Media},
 number = {3},
 pages = {128–145},
 title = {You have got a (different) friend in me: Asymmetrical roles in gaming as potential ambassadors of computational and cooperative thinking},
 url = {https://doi-org.crai.referencistas.com/10.1177/2042753018757757},
 volume = {15},
 year = {2018e}
}

@article{doi:10.1177/2042753020980119,
 abstract = {Set in English Language Arts, this article takes up recent trends in literacy toward investigating ontological notions of digital texts. Two teacher educators recently implemented a series of readings and activities in their methods courses designed to help preservice teachers sophisticate their conceptions of texts beyond autonomous, neutral collections of information by considering digital age ideas such as software theory, textual ideology, and the algorithmic bias of the Internet. The authors review recent scholarship surrounding the integration of computational thinking and the humanities before illustrating a theoretical framework that combines software-driven interpretation and critical media literacy. Descriptions and applications of course texts and exercises precede a discussion on typological methodology. Through the analysis of semester-long writing reflections and course interactions, a typology of preservice teachers is then presented, illustrating three archetypes: Strategists, Hawkeyes, and Improvers. These archetypes are taken up to analyze the ways in which a range of teacher candidates considered ontological notions of digital texts to analyze instructional techniques, to sharpen their critical lenses, or to gain greater understanding of ELA as a discipline (or some combination of all three). This work demonstrates that as teacher educators and teacher candidates increasingly consider software-powered literacies, interrogations of who we are, who we are becoming, and what it all means requires attention to, and explicit practice with, the dark side of digital texts.},
 author = {Rick Marlatt and Mark A Sulzer},
 doi = {10.1177/2042753020980119},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2042753020980119},
 journal = {E-Learning and Digital Media},
 number = {3},
 pages = {226–250},
 title = {Illuminating the dark side: A typology for preservice ELA teachers engaging in ideologies of digital texts},
 url = {https://doi-org.crai.referencistas.com/10.1177/2042753020980119},
 volume = {18},
 year = {2021n}
}

@article{doi:10.1177/20427530211022964,
 abstract = {This article explores the use of modding as a formal tool for learning history. The article examines data from a formal analysis of Europa Universalis IV (EUIV), a survey of 331 EUIV forum participants and a case study of 18 university participants. Significant quantitative survey data indicated that 45% (149/331) of participants had modified EUIV, and of the 125 participants who responded with comments about modding, a significant number (86/125 responses or 68.8%) explained how they had learnt about history, geography or other subjects through the modding process. Closer analysis of survey and case study responses and mods reveals the variety of ways participants learnt and critiqued history through the modding process. The article discusses the data and the pedagogical affordance of modding in a few steps. First, the article briefly explores the evidence that indicates modding is popular within the EUIV gaming community. In this instance, it examines whether given the popularity of gaming practice, modding might also be seen as a new casual form of engagement with games. Second, the article reviews the modding process in EUIV and examines how both playing and creating mods may be beneficial for learning history. Modding is examined in terms of its pedagogical importance and the unique educational opportunities it may offer that are not otherwise accessible through other forms of game-based learning. Finally, the article explores how and what the case study participants learnt when they were tasked with creating and implementing playable mods to demonstrate their understanding of history. Overall, the article considers the growing importance of mods, how learners can create and represent history using mods and how mods can provide a platform for learners to develop their own critique and analysis of official history.},
 author = {Rhett Loban},
 doi = {10.1177/20427530211022964},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/20427530211022964},
 journal = {E-Learning and Digital Media},
 number = {6},
 pages = {530–556},
 title = {Modding Europa Universalis IV: An informal gaming practice transposed into a formal learning setting},
 url = {https://doi-org.crai.referencistas.com/10.1177/20427530211022964},
 volume = {18},
 year = {2021o}
}

@article{doi:10.1177/20427530211030642,
 abstract = {In this industrial age, skills required in most jobs are 21st-century skills. The current study aimed to investigate whether there is a relationship between implementing project-based collaborative learning using PowerPoint and improving students’ 21st-century skills from the students’ perspectives. It also examines whether there is a significant relationship between students’ attitudes toward learning collaboratively using PowerPoint to improve their 21st-century skills and their major. The participants of the study were 75 female students enrolled in an Educational Technology and Means course at Najran University. The findings revealed that there is a significant and positive relationship between implementing a project-based collaborative learning approach using PowerPoint and improving the students’ 21st-century skills, r (74) = 0.74 and p < 0.05. Additionally, the findings demonstrated that 21st-century skills improved the most through “actively collaborating with others” (M = 4.6, SD = 0.56). Additionally, there was no significant difference in students’ attitudes toward learning collaboratively using PowerPoint to improve their 21st-century skills in terms of human studies or scientific studies majors, t (37) = 1.97 and p > 0.05. The findings demonstrate that more research is required on the role of higher education in developing meaningful technology-based strategies to improve students’ 21st-century skills in learning environments.},
 author = {Hanan Aifan},
 doi = {10.1177/20427530211030642},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/20427530211030642},
 journal = {E-Learning and Digital Media},
 number = {3},
 pages = {258–273},
 title = {Implementing a project-based collaborative learning approach using PowerPoint to improve students’ 21st- century skills},
 url = {https://doi-org.crai.referencistas.com/10.1177/20427530211030642},
 volume = {19},
 year = {2022a}
}

@article{doi:10.1177/20427530221117331,
 abstract = {This study investigates pre-service computer science (CS) teachers’ perspectives on the factors affecting their programming abilities, concerns about their future professional lives, and pedagogical suggestions for effective programming teaching. The participants of the study were twenty-eight pre-service CS teachers studying at eighteen different universities in Türkiye. The data was gathered with a questionnaire consisting of open-ended questions. Results showed that inherent programming challenges, personal factors, and quality of undergraduate education affected pre-service CS teachers’ abilities in programming teaching. They were concerned about their professional lives relating to programming teaching, keeping up with technological innovations, pedagogical concerns, and teaching practice. They also suggested pedagogical approaches that could be used in programming teaching under two categories, namely basic programming teaching, and teaching principles and techniques. The study contributes to the development of programming teaching by shedding light on the current perspectives of pre-service CS teachers on programming education.},
 author = {Seyfullah Gökoğlu and Servet Kilic},
 doi = {10.1177/20427530221117331},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/20427530221117331},
 journal = {E-Learning and Digital Media},
 number = {5},
 pages = {498–518},
 title = {Programming learning and teaching of pre-service computer science teachers: Challenges, concerns, and solutions},
 url = {https://doi-org.crai.referencistas.com/10.1177/20427530221117331},
 volume = {20},
 year = {2023i}
}

@article{doi:10.1177/20427530241239406,
 abstract = {The pedagogy, andragogy, and heutagogy (PAH) continuum have been widely endorsed within higher education and online learning over the years. Thus, the evaluation of research productivity has become relevant due to the emerging development of the PAH continuum as a theoretical framework. This paper aims to explore the research productivity within the PAH continuum by performing a bibliometric performance analysis. An iterative search strategy was performed to identify PAH authors and then investigate their research productivity over a 5-year period (2017–2022) using the SciVal analytics instrument by Elsevier. Drawing upon the results from multiple bibliometric analyses, the study explores publication productivity specifically in relation to the PAH continuum. This study revealed that distance education, learning, teaching, professional development, and lifelong learning have been the most prominent research areas within the PAH continuum. Within that, related topics were identified, including online learning, mobile learning, e-learning, and educational technology.},
 author = {Zulyar Kavashev},
 doi = {10.1177/20427530241239406},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/20427530241239406},
 journal = {E-Learning and Digital Media},
 number = {0},
 pages = {20427530241239410},
 title = {A bibliometric performance analysis of publication productivity within pedagogy, andragogy, and heutagogy continuum: Outcomes of SciVal analytics},
 url = {https://doi-org.crai.referencistas.com/10.1177/20427530241239406},
 volume = {0},
 year = {2024i}
}

@article{doi:10.1177/2043820618808662,
 author = {Harlan Morehouse},
 doi = {10.1177/2043820618808662},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2043820618808662},
 journal = {Dialogues in Human Geography},
 number = {1},
 pages = {110–112},
 title = {On the political and speculative promises of Gabrys’ Program Earth},
 url = {https://doi-org.crai.referencistas.com/10.1177/2043820618808662},
 volume = {9},
 year = {2019n}
}

@article{doi:10.1177/2043820618808664,
 author = {James Ash},
 doi = {10.1177/2043820618808664},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2043820618808664},
 journal = {Dialogues in Human Geography},
 number = {1},
 pages = {115–117},
 title = {For a techno-geography of sensing objects},
 url = {https://doi-org.crai.referencistas.com/10.1177/2043820618808664},
 volume = {9},
 year = {2019c}
}

@article{doi:10.1177/2043820618808665,
 author = {Andrés Luque-Ayala},
 doi = {10.1177/2043820618808665},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2043820618808665},
 journal = {Dialogues in Human Geography},
 number = {1},
 pages = {117–120},
 title = {Rethinking the material politics of the city through ‘Interoperable streams of data’},
 url = {https://doi-org.crai.referencistas.com/10.1177/2043820618808665},
 volume = {9},
 year = {2019i}
}

@article{doi:10.1177/2043820618808668,
 author = {Jennifer Gabrys},
 doi = {10.1177/2043820618808668},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2043820618808668},
 journal = {Dialogues in Human Geography},
 number = {1},
 pages = {121–124},
 title = {Sensors experiencing environments, environments becoming computational},
 url = {https://doi-org.crai.referencistas.com/10.1177/2043820618808668},
 volume = {9},
 year = {2019i}
}

@article{doi:10.1177/20438206221075714,
 abstract = {This intervention advances glitches as epistemological vectors for apprehending and engaging the significance of digitally-mediated spatialities that appear nonperformative against normative scripts of urban computational paradigms. Drawing on two strands of contemporary thinking about glitches as systemic design features of digital systems and as generative fissures within them, we mobilize a queer orientation that stays with the generative tensions of urban spatialities that present as idiosyncratic and as interrupting. We mobilize this epistemological approach through illustrative U.S. based examples of seemingly abandoned shared e-bikes, performatively ‘ugly’ homes, and wilful property dilapidation wrought through the registers of desire and aesthetics. In so doing, we show how glitch empistemologies render visible how the technocapitalist manufacturing of normative spatial desires for particular kinds of urban sociospatialities and aesthetic visual signatures are both secured and interrupted on digitally-mediated and -mediatized terrains. Glitch epistemologies establish the significance of small-scale disorientations in digital urban mediations, engaging these nonperformativities and non-computes as unexceptional openings onto everyday possibilities for politics in computational cities.},
 author = {Agnieszka Leszczynski and Sarah Elwood},
 doi = {10.1177/20438206221075714},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/20438206221075714},
 journal = {Dialogues in Human Geography},
 number = {3},
 pages = {361–378},
 title = {Glitch epistemologies for computational cities},
 url = {https://doi-org.crai.referencistas.com/10.1177/20438206221075714},
 volume = {12},
 year = {2022i}
}

@article{doi:10.1177/20438206231179477,
 abstract = {This commentary lays out a framework for building on early critical cartographic and critical geographic information system work to develop a critical approach for the computational future of geographical thought and praxis. Computation – as a highly representational and structural form which combines speech and action – is a very particular way of building worlds. As computation becomes more prevalent in critiques of deep fakes, GeoAI, and platform geographies, geographers have also developed the foundations for a critical computational approach with an explicitly spatial or geographical focus that combines both theory and practice. Yet, while many of the technological affordances of spatial computation are relatively novel, the critiques raised by social, political, economic, and cultural geographers shadow debates that emerged two decades ago between cartographers and geospatial scientists about the power and praxis of mapping as it becomes translated into a digital era. This commentary argues that by returning to these debates, as well as critique by Black, queer, and Indigenous computing seen in other disciplines, geographers find themselves in a moment of opportunity to deeply influence the future of computation via a situated, critical geographical thought and praxis.},
 author = {Clancy Wilmott},
 doi = {10.1177/20438206231179477},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/20438206231179477},
 journal = {Dialogues in Human Geography},
 number = {2},
 pages = {332–336},
 title = {Critical computation on a geographical register},
 url = {https://doi-org.crai.referencistas.com/10.1177/20438206231179477},
 volume = {14},
 year = {2024t}
}

@article{doi:10.1177/20438206231189586,
 abstract = {The accelerated post-Covid expansion of online worlds presents an unprecedented move of people into real-time interactive digital spaces. What does this change mean for the future of geography as a discipline? At this critical juncture, there is potential to rethink the position of the digital in geographical thought and praxis – to move beyond apparently common-sense categorizations of real and virtual, representation and reality. This commentary considers the implications of the contemporary push toward ‘metaversal’ thinking for geographical theory as well as the significance of virtual world-making for geographical theorizations of digital space and place. I suggest that key thinkers on space and media geographies must be re-evaluated and applied to this new wave of digital development. What is the significance of recent debates around emerging spaces like the metaverse, augmented reality, and virtual reality, understood not as happenings with distinct real and virtual counterparts, but as geographical – spatial – phenomena?},
 author = {Emma Fraser},
 doi = {10.1177/20438206231189586},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/20438206231189586},
 journal = {Dialogues in Human Geography},
 number = {2},
 pages = {347–351},
 title = {The future of digital space: Gaming, virtual reality, and metaversal thinking},
 url = {https://doi-org.crai.referencistas.com/10.1177/20438206231189586},
 volume = {14},
 year = {2024f}
}

@article{doi:10.1177/20438206241264631,
 abstract = {The question of geography’s future has recurred throughout the history of geographical thought, and responses to it often presume a linear trajectory from the past and present to a possible future. Yet one of the major contributions that geographers have made to understanding spatio-temporality is reconceiving both space and time as plural, fluid, and co-constituted through multiple space–time trajectories simultaneously. Amidst the ongoing crises of the present, this article opens the current special issue with a call to pluralize geography’s futures by diversifying the voices speaking in the name of ‘geography’ and broadening the horizon of possibilities for the futures of geographical thought and praxis. We have assembled the contributions in this collection with the aim of raising important theoretical, methodological, and empirical questions about how geography’s past and present shape the conditions of possibility for its potential futures. In doing so, we seek to demonstrate how the worlding of geography’s futures is fundamentally a matter of transforming its disciplinary reproduction in the here-and-now.},
 author = {Reuben Rose-Redwood and CindyAnn Rose-Redwood and Elia Apostolopoulou and Tyler Blackman and Han Cheng and Anindita Datta and Sharon Dias and Federico Ferretti and Wil Patrick and James Riding et al.},
 doi = {10.1177/20438206241264631},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/20438206241264631},
 journal = {Dialogues in Human Geography},
 number = {2},
 pages = {177–191},
 title = {Re-imagining the futures of geographical thought and praxis},
 url = {https://doi-org.crai.referencistas.com/10.1177/20438206241264631},
 volume = {14},
 year = {2024p}
}

@article{doi:10.1177/20438869231178035,
 abstract = {Drawing on the story of Aginic and its educational analytics platform edPortal, this teaching case study examines how applying agile methods and design thinking to analytics has helped unlock significant value for Aginic, its clients and the education sector overall. It describes key factors driving the successful integration of agile values and design approaches, allowing students to gain a deep understanding on how such integration can facilitate the development of innovative data analytics products.},
 author = {Edgar Brea and Ida Someh and Emma Freya and Brett Thebault and Shazia Sadiq},
 doi = {10.1177/20438869231178035},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/20438869231178035},
 journal = {Journal of Information Technology Teaching Cases},
 number = {2},
 pages = {227–240},
 title = {Integrating design thinking and agile approaches in analytics development: The case of Aginic},
 url = {https://doi-org.crai.referencistas.com/10.1177/20438869231178035},
 volume = {14},
 year = {2024b}
}

@article{doi:10.1177/20531680231197456,
 abstract = {Temporality is an important aspect of political discourse. Politicians and policymakers attempt to construct the past and the future to gain power, legitimize their policies, claim success for themselves and blame others. To make computational analysis of temporality more accessible, we develop a new methodology using a semisupervised machine-learning algorithm called Latent Semantic Scaling. Only with a set of common verbs in the past perfect and future tense as seed words, the algorithm estimates the temporality of all other words. We demonstrate that it can identify temporal orientation of English and German sentences from election manifestos around 60–70% accurately, which is comparable to the results from a recent study based on supervised machine-learning algorithms. We also apply it to Twitter posts by German political parties to reveal temporal orientation of policy issues.},
 author = {Kohei Watanabe and Marius Sältzer},
 doi = {10.1177/20531680231197456},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/20531680231197456},
 journal = {Research & Politics},
 number = {3},
 pages = {20531680231197456},
 title = {Semantic temporality analysis: A computational approach to time in English and German texts},
 url = {https://doi-org.crai.referencistas.com/10.1177/20531680231197456},
 volume = {10},
 year = {2023q}
}

@article{doi:10.1177/2053951715602908,
 abstract = {Social scientists and computer scientist are divided by small differences in perspective and not by any significant disciplinary divide. In the field of text analysis, several such differences are noted: social scientists often use unsupervised models to explore corpora, whereas many computer scientists employ supervised models to train data; social scientists hold to more conventional causal notions than do most computer scientists, and often favor intense exploitation of existing algorithms, whereas computer scientists focus more on developing new models; and computer scientists tend to trust human judgment more than social scientists do. These differences have implications that potentially can improve the practice of social science.},
 author = {Paul DiMaggio},
 doi = {10.1177/2053951715602908},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2053951715602908},
 journal = {Big Data & Society},
 number = {2},
 pages = {2053951715602908},
 title = {Adapting computational text analysis to social science (and vice versa)},
 url = {https://doi-org.crai.referencistas.com/10.1177/2053951715602908},
 volume = {2},
 year = {2015d}
}

@article{doi:10.1177/2053951715613809,
 abstract = {We describe some of the ways that the field of content analysis is being transformed in an Era of Big Data. We argue that content analysis, from its beginning, has been concerned with extracting the main meanings of a text and mapping those meanings onto the space of a textual corpus. In contrast, we suggest that the emergence of new styles of text mining tools is creating an opportunity to develop a different kind of content analysis that we describe as a computational hermeneutics. Here the goal is to go beyond a mapping of the main meaning of a text to mimic the kinds of questions and concerns that have traditionally been the focus of a hermeneutically grounded close reading, a reading that focuses on what Kenneth Burke described as the poetic meanings of a text. We illustrate this approach by referring to our own work concerning the rhetorical character of US National Security Strategy documents.},
 author = {John W Mohr and Robin Wagner-Pacifici and Ronald L Breiger},
 doi = {10.1177/2053951715613809},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2053951715613809},
 journal = {Big Data & Society},
 number = {2},
 pages = {2053951715613809},
 title = {Toward a computational hermeneutics},
 url = {https://doi-org.crai.referencistas.com/10.1177/2053951715613809},
 volume = {2},
 year = {2015p}
}

@article{doi:10.1177/2053951715617783,
 abstract = {Coupled with the ‘smart city’, the idea of the ‘smart school’ is emerging in imaginings of the future of education. Various commercial, governmental and civil society organizations now envisage education as a highly coded, software-mediated and data-driven social institution. Such spaces are to be governed through computational processes written in computer code and tracked through big data. In an original analysis of developments from commercial, governmental and civil society sectors, the article examines two interrelated dimensions of an emerging smart schools imaginary: (1) the constant flows of digital data that smart schools depend on and the mobilization of analytics that enable student data to be used to anticipate and shape their behaviours; and (2) the ways that young people are educated to become ‘computational operatives’ who must ‘learn to code’ in order to become ‘smart citizens’ in the governance of the smart city. These developments constitute an emerging educational space fabricated from intersecting standards, technologies, discourses and social actors, all infused with the aspirations of technical experts to govern the city at a distance through both monitoring young people as ‘data objects’ and schooling them as active ‘computational citizens’ with the responsibility to compute the future of the city.},
 author = {Ben Williamson},
 doi = {10.1177/2053951715617783},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2053951715617783},
 journal = {Big Data & Society},
 number = {2},
 pages = {2053951715617783},
 title = {Educating the smart city: Schooling smart citizens through computational urbanism},
 url = {https://doi-org.crai.referencistas.com/10.1177/2053951715617783},
 volume = {2},
 year = {2015s}
}

@article{doi:10.1177/2053951716670190,
 abstract = {Mapping a public discourse with the tools of computational text analysis comes with many contingencies in the areas of corpus curation, data processing and analysis, and visualisation. However, the complexity of algorithmic assemblies and the beauty of resulting images give the impression of ‘objectivity’. Instead of concealing uncertainties and artefacts in order to tell a coherent and all-encompassing story, retaining the variety of alternative assemblies may actually strengthen the method. By utilising the mobility of digital devices, we could create mutable mobiles that allow access to our laboratories and enable challenging rearrangements and interpretations.},
 author = {Daniel Marciniak},
 doi = {10.1177/2053951716670190},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2053951716670190},
 journal = {Big Data & Society},
 number = {2},
 pages = {2053951716670190},
 title = {Computational text analysis: Thoughts on the contingencies of an evolving method},
 url = {https://doi-org.crai.referencistas.com/10.1177/2053951716670190},
 volume = {3},
 year = {2016p}
}

@article{doi:10.1177/2053951718768831,
 abstract = {This paper argues that analyses of the ways in which Big Data has been enacted in other academic disciplines can provide us with concepts that will help understand the application of Big Data to social questions. We use examples drawn from our Science and Technology Studies (STS) analyses of -omic biology and high energy physics to demonstrate the utility of three theoretical concepts: (i) primary and secondary inscriptions, (ii) crafted and found data, and (iii) the locus of legitimate interpretation. These help us to show how the histories, organisational forms, and power dynamics of a field lead to different enactments of big data. The paper suggests that these concepts can be used to help us to understand the ways in which Big Data is being enacted in the domain of the social sciences, and to outline in general terms the ways in which this enactment might be different to that which we have observed in the ‘hard’ sciences. We contend that the locus of legitimate interpretation of Big Data biology and physics is tightly delineated, found within the disciplinary institutions and cultures of these disciplines. We suggest that when using Big Data to make knowledge claims about ‘the social’ the locus of legitimate interpretation is more diffuse, with knowledge claims that are treated as being credible made from other disciplines, or even by those outside academia entirely.},
 author = {Andrew Bartlett and Jamie Lewis and Luis Reyes-Galindo and Neil Stephens},
 doi = {10.1177/2053951718768831},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2053951718768831},
 journal = {Big Data & Society},
 number = {1},
 pages = {2053951718768831},
 title = {The locus of legitimate interpretation in Big Data sciences: Lessons for computational social science from -omic biology and high-energy physics},
 url = {https://doi-org.crai.referencistas.com/10.1177/2053951718768831},
 volume = {5},
 year = {2018d}
}

@article{doi:10.1177/2053951718811843,
 abstract = {This paper reviews the contemporary discussion on the epistemological and ontological effects of Big Data within social science, observing an increased focus on relationality and complexity, and a tendency to naturalize social phenomena. The epistemic limits of this emerging computational paradigm are outlined through a comparison with the discussions in the early days of digitalization, when digital technology was primarily seen through the lens of dematerialization, and as part of the larger processes of “postmodernity”. Since then, the online landscape has become increasingly centralized, and the “liquidity” of dematerialized technology has come to empower online platforms in shaping the conditions for human behavior. This contrast between the contemporary epistemological currents and the previous philosophical discussions brings to the fore contradictions within the study of digital social life: While qualitative change has become increasingly dominant, the focus has gone towards quantitative methods; while the platforms have become empowered to shape social behavior, the focus has gone from social context to naturalizing social patterns; while meaning is increasingly contested and fragmented, the role of hermeneutics has diminished; while platforms have become power hubs pursuing their interests through sophisticated data manipulation, the data they provide is increasingly trusted to hold the keys to understanding social life. These contradictions, we argue, are partially the result of a lack of philosophical discussion on the nature of social reality in the digital era; only from a firm metatheoretical perspective can we avoid forgetting the reality of the system under study as we are affected by the powerful social life of Big Data.},
 author = {Petter Törnberg and Anton Törnberg},
 doi = {10.1177/2053951718811843},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2053951718811843},
 journal = {Big Data & Society},
 number = {2},
 pages = {2053951718811843},
 title = {The limits of computation: A philosophical critique of contemporary Big Data research},
 url = {https://doi-org.crai.referencistas.com/10.1177/2053951718811843},
 volume = {5},
 year = {2018p}
}

@article{doi:10.1177/2053951720919964,
 author = {Florian Stalph},
 doi = {10.1177/2053951720919964},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2053951720919964},
 journal = {Big Data & Society},
 number = {1},
 pages = {2053951720919964},
 title = {Evolving data teams: Tensions between organisational structure and professional subculture},
 url = {https://doi-org.crai.referencistas.com/10.1177/2053951720919964},
 volume = {7},
 year = {2020p}
}

@article{doi:10.1177/2053951720949571,
 abstract = {This article introduces an interpretative approach to the analysis of situations in computational settings called situational analytics. I outline the theoretical and methodological underpinnings of this approach, which is still under development, and show how it can be used to surface situations from large data sets derived from online platforms such as YouTube. Situational analytics extends to computationally-mediated settings a qualitative methodology developed by Adele Clarke, Situational Analysis (2005), which uses data mapping to detect heterogeneous entities in fieldwork data to determine ‘what makes a difference’ in a situation. Situational analytics scales up this methodology to analyse situations latent in computational data sets with semi-automated methods of textual and visual analysis. I discuss how this approach deviates from recent analyses of situations in computational social science, and argue that Clarke’s framework renders tractable a fundamental methodological problem that arises in this area of research: while social researchers turn to computational settings in order to analyse social life, the social processes unfolding in these envirnoments are fundamentally affected by the computational architectures in which they occur. Situational analytics offers a way to address this problematic by making a heterogeneously composed situation – involving social, technical and media elements – the unit of computational analysis. To conclude, I show how situational analytics can be applied in a case study of YouTube videos featuring intelligent vehicles and discuss how situational analysis itself needs to be elaborated if we are to come to terms with computational transformations of the situational fabric of social life.},
 author = {Noortje Marres},
 doi = {10.1177/2053951720949571},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2053951720949571},
 journal = {Big Data & Society},
 number = {2},
 pages = {2053951720949571},
 title = {For a situational analytics: An interpretative methodology for the study of situations in computational settings},
 url = {https://doi-org.crai.referencistas.com/10.1177/2053951720949571},
 volume = {7},
 year = {2020l}
}

@article{doi:10.1177/20539517211047725,
 abstract = {The proliferation of digital data has been the impetus for the emergence of a new discipline for the study of social life: ‘computational social science’. Much research in this field is founded on the premise that society is a complex system with emergent structures that can be modeled or reconstructed through digital data. This paper suggests that computational social science serves practical and legitimizing functions for digital capitalism in much the same way that neoclassical economics does for neoliberalism. In recognition of this homology, this paper develops a critique of the complexity perspective of computational social science and argues for a heterodox computational social science founded on the meta-theory of critical realism that is critical, methodological pluralist, interpretative and explanative. This implies diverting computational social science’ computational methods and digital data so as to not be aimed at identifying invariant laws of social life, or optimizing state and corporate practices, but to instead be used as part of broader research strategies to identify contingent patterns, develop conjunctural explanations, and propose qualitatively different ways of organizing social life.},
 author = {Petter Törnberg and Justus Uitermark},
 doi = {10.1177/20539517211047725},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/20539517211047725},
 journal = {Big Data & Society},
 number = {2},
 pages = {20539517211047724},
 title = {For a heterodox computational social science},
 url = {https://doi-org.crai.referencistas.com/10.1177/20539517211047725},
 volume = {8},
 year = {2021u}
}

@article{doi:10.1177/20539517211062885,
 abstract = {The concept of ‘digital phenotyping’ was originally developed by researchers in the mental health field, but it has travelled to other disciplines and areas. This commentary draws upon our experiences of working in two scientific projects that are based at the University of Oxford’s Big Data Institute – The RADAR-AD project and The Minerva Initiative – which are developing algorithmic phenotyping technologies. We describe and analyse the concepts of digital biomarkers and computational phenotyping that underlie these projects, explain how they are linked to other research in digital phenotyping and compare and contrast some of their epistemological and ethical implications. In particular, we argue that the phenotyping paradigm in both projects is grounded on an assumption of ‘objectivity’ that is articulated in different ways depending on the role that is given to the computational/digital tools. Using the concept of ‘affordance’, we show how specific functionalities relate to potential uses and social implications of these technologies and argue that it is important to distinguish among them as the concept of digital phenotyping is increasingly being used with a variety of meanings.},
 author = {Federica Lucivero and Nina Hallowell},
 doi = {10.1177/20539517211062885},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/20539517211062885},
 journal = {Big Data & Society},
 number = {2},
 pages = {20539517211062884},
 title = {Digital/computational phenotyping: What are the differences in the science and the ethics?},
 url = {https://doi-org.crai.referencistas.com/10.1177/20539517211062885},
 volume = {8},
 year = {2021h}
}

@article{doi:10.1177/20539517211069892,
 abstract = {This commentary elaborates on the ideas and projects outlined in this special issue, from a specifically sociological perspective. Much recent work in sociology proposes ‘methods mashups’ of ethnography and digital data/computational tools in different and diverse ways. However, typically, these have taken the form of applying (with or without tweaks) the principles of ethnography to new domains and data types, as if ethnography itself is stable and immutable; that it has a universal set of methodological principles that unify ethnographic practice. Returning to anthropology (whence, arguably, ethnography originally came) is, therefore, a useful way to extend our methodological thinking to (re)consider what ethnography is and how it operates, and from there think more clearly about how it may be effectively combined with digital data/computational tools in an emerging ‘Computational Anthropology’.},
 author = {Phillip Brooker},
 doi = {10.1177/20539517211069892},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/20539517211069892},
 journal = {Big Data & Society},
 number = {1},
 pages = {20539517211069892},
 title = {Computational ethnography:  A view from sociology},
 url = {https://doi-org.crai.referencistas.com/10.1177/20539517211069892},
 volume = {9},
 year = {2022e}
}

@article{doi:10.1177/20539517221080146,
 abstract = {The size and variation in both meaning-making and populations that characterize much contemporary text data demand research processes that support both discovery, interpretation and measurement. We assess one dominant strategy within the social sciences that takes a computer-led approach to text analysis. The approach is coined computational grounded theory. This strategy, we argue, relies on a set of unwarranted assumptions, namely, that unsupervised models return natural clusters of meaning, that the researcher can understand text with limited immersion and that indirect validation is sufficient for ensuring unbiased and precise measurement. In response to this criticism, we develop a framework that is computer assisted. We argue that our reformulation of computational grounded theory better aligns with the principles within grounded theory, anthropological theory generation and ethnography.},
 author = {Hjalmar Bang Carlsen and Snorre Ralund},
 doi = {10.1177/20539517221080146},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/20539517221080146},
 journal = {Big Data & Society},
 number = {1},
 pages = {20539517221080144},
 title = {Computational grounded theory revisited: From computer-led to computer-assisted text analysis},
 url = {https://doi-org.crai.referencistas.com/10.1177/20539517221080146},
 volume = {9},
 year = {2022c}
}

@article{doi:10.1177/20539517231171051,
 abstract = {The social services sector, comprised of a constellation of programs meeting critical human needs, lacks the resources and infrastructure to implement data science tools. As the use of data science continues to expand, it has been accompanied by a rise in interest and commitment to using these tools for social good. This commentary examines overlooked, and under-researched limitations of data science applications in the social sector—the volume, quality, and context of the available data that currently exists in social service systems require unique considerations. We explore how the presence of small data within the social service contexts can result in extrapolation; if not properly considered, data science can negatively impact the organizations data scientists are trying to assist. We conclude by proposing three ways data scientists interested in working within the social services sector can enhance their contributions to the field: refining and leveraging available data, improving collaborations, and respecting data limitations.},
 author = {Geri Louise Dimas and Lauri Goldkind and Renata Konrad},
 doi = {10.1177/20539517231171051},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/20539517231171051},
 journal = {Big Data & Society},
 number = {1},
 pages = {20539517231171052},
 title = {Big ideas, small data: Opportunities and challenges for data science and the social services sector},
 url = {https://doi-org.crai.referencistas.com/10.1177/20539517231171051},
 volume = {10},
 year = {2023g}
}

@article{doi:10.1177/2055207619880671,
 author = {Adi Kuntsman and Esperanza Miyake and Sam Martin},
 doi = {10.1177/2055207619880671},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2055207619880671},
 journal = {DIGITAL HEALTH},
 note = {PMID:31636917},
 number = { },
 pages = {2055207619880671},
 title = {Re-thinking Digital Health: Data, Appisation and the (im)possibility of ‘Opting out’},
 url = {https://doi-org.crai.referencistas.com/10.1177/2055207619880671},
 volume = {5},
 year = {2019j}
}

@article{doi:10.1177/20552076221111941,
 abstract = {The prevalent availability of high-performance computing coupled with validated computerized simulation platforms as open-source packages have motivated progress in the development of realistic anthropomorphic computational models of the human anatomy. The main application of these advanced tools focused on imaging physics and computational internal/external radiation dosimetry research. This paper provides an updated review of state-of-the-art developments and recent advances in the design of sophisticated computational models of the human anatomy with a particular focus on their use in radiation dosimetry calculations. The consolidation of flexible and realistic computational models with biological data and accurate radiation transport modeling tools enables the capability to produce dosimetric data reflecting actual setup in clinical setting. These simulation methodologies and results are helpful resources for the medical physics and medical imaging communities and are expected to impact the fields of medical imaging and dosimetry calculations profoundly.},
 author = {Azadeh Akhavanallaf and Hadi Fayad and Yazdan Salimi and Antar Aly and Hassan Kharita and Huda Al Naemi and Habib Zaidi},
 doi = {10.1177/20552076221111941},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/20552076221111941},
 journal = {DIGITAL HEALTH},
 note = {PMID:35847523},
 number = { },
 pages = {20552076221111940},
 title = {An update on computational anthropomorphic anatomical models},
 url = {https://doi-org.crai.referencistas.com/10.1177/20552076221111941},
 volume = {8},
 year = {2022b}
}

@article{doi:10.1177/20552076231174786,
 abstract = {Objectives Deficits affecting hand motor skills negatively impact the quality of life of patients. The NeuroData Tracker platform has been developed for the objective and precise evaluation of hand motor deficits. We describe the design and development of the platform and analyse the technological feasibility and usability in a relevant clinical setting. Methods A software application was developed in Unity (C#) to obtain kinematic data from hand movement tracking by a portable device with two cameras and three infrared sensors (leap motion®). Four exercises were implemented: (a) wrist flexion-extension (b) finger-grip opening-closing (c) finger spread (d) fist opening-closing. The most representative kinematic parameters were selected for each exercise. A script in Python was integrated in the platform to transform real-time kinematic data into relevant information for the clinician. The application was tested in a pilot study comparing the data provided by the tool from ten healthy subjects without any motor impairment and ten patients diagnosed with a stroke with mild to moderate hand motor deficit. Results The NeuroData Tracker allowed the parameterization of kinematics of hand movement and the issuance of a report with the results. The comparison of the data obtained suggests the feasibility of the tool for detecting differences between patients and healthy subjects. Conclusions This new platform based on optical motion capturing provides objective measurement of hand movement allowing quantification of motor deficits. These findings require further validation of the tool in larger trials to verify its usefulness in the clinical setting.},
 author = {David López and Laura Casado-Fernández and Fernando Fernández and Blanca Fuentes and Blanca Larraga-García and Jorge Rodríguez-Pardo and David Hernández and Elisa Alonso and Exuperio Díez-Tejedor and Álvaro Gutiérrez et al.},
 doi = {10.1177/20552076231174786},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/20552076231174786},
 journal = {DIGITAL HEALTH},
 number = { },
 pages = {20552076231174784},
 title = {Neurodata Tracker: Software for computational assessment of hand motor skills based on optical motion capture in a virtual environment},
 url = {https://doi-org.crai.referencistas.com/10.1177/20552076231174786},
 volume = {9},
 year = {2023k}
}

@article{doi:10.1177/20552076231186513,
 abstract = {Objective Healthcare systems require transformation to meet societal challenges and projected health demands. Digital and computational tools and approaches are fundamental to this transformation, and hospitals have a key role to play in their development and implementation. This paper reports on a study with the objective of exploring the challenges encountered by hospital leaders and innovators as they implement a strategy to become a data-driven hospital organisation. In doing so, this paper provides guidance to future leaders and innovators seeking to build computational and digital capabilities in complex clinical settings. Methods Interviews were undertaken with 42 participants associated with a large public hospital organisation within England’s National Health Service. Using the concept of institutional readiness as an analytical framework, the paper explores participants’ perspectives on the organisation’s capacity to support the development of, and benefit from, digital and computational approaches. Results Participants’ accounts reveal a range of specific institutional readiness criteria relating to organisational vision, technical capability, organisational agility, and talent and skills that, when met, enhance the organisations’ capacity to support the development and implementation of digital and computational tools. Participant accounts also reveal challenges relating to these criteria, such as unrealistic expectations and the necessary prioritisation of clinical work in resource-constrained settings. Conclusions The paper identifies a general set of institutional readiness criteria that can guide future hospital leaders and innovators aiming to improve their organisation’s digital and computational capability. The paper also illustrates the challenges of pursuing digital and computational innovation in resource-constrained hospital environments.},
 author = {John Gardner and Daniel Herron and Nick McNally and Bryan Williams},
 doi = {10.1177/20552076231186513},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/20552076231186513},
 journal = {DIGITAL HEALTH},
 note = {PMID:36644660},
 number = { },
 pages = {20552076231186510},
 title = {Advancing the digital and computational capabilities of healthcare providers: A qualitative study of a hospital organisation in the NHS},
 url = {https://doi-org.crai.referencistas.com/10.1177/20552076231186513},
 volume = {9},
 year = {2023h}
}

@article{doi:10.1177/20552076241237392,
 abstract = {Objective Digital pathology (DP) is moving into Danish pathology departments at high pace. Conventionally, biomedical laboratory scientists (BLS) and technicians have prepared tissue sections for light microscopy, but workflow alterations are required for the new digital era with whole slide imaging (WSI); digitally assisted image analysis (DAIA) and artificial intelligence (AI). We aim to explore the role of BLS in DP and assess a potential need for professional development. Methods We investigated the roles of BLS in the new digital era through qualitative interviews at Danish Pathology Departments in 2019/2020 before DP implementation (supported by a questionnaire); and in 2022 after DP implementation. Additionally, senior lecturers from three Danish University Colleges reported on how DP was integrated into the 2023 bachelor’s degree educational curricula for BLS students. Results At some Danish pathology departments, BLS were involved in the implementation process of DP and their greatest concerns were lack of physical laboratory requirements (69%) and implementation strategies (63%). BLS were generally positive towards working with DP, however, some expressed concern about extended working hours for scanning. Work-task transfers from pathologists were generally greeted positively from both management and pathologists; however, at follow-up interviews after DP implementation, job transfers had not been effectuated. At Danish university colleges, DP had been integrated systematically in the curricula for BLS students, especially WSI. Conclusion Involving BLS in DP implementation and development may benefit the process, as BLS have a hands-on workflow perspective with a focus on quality assurance. Several new work opportunities for BLS may occur with DP including WSI, DAIA and AI, and therefore new qualifications are warranted, which must be considered in future undergraduate programmes for BLS students or postgraduate programmes for BLS.},
 author = {Charlotte Lerbech Jensen and Lisbeth Koch Thomsen and Mette Zeuthen and Sys Johnsen and Rima El Jashi and Michael Friberg Bruun Nielsen and Line E Hemstra and Julie Smith},
 doi = {10.1177/20552076241237392},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/20552076241237392},
 journal = {DIGITAL HEALTH},
 number = { },
 pages = {20552076241237390},
 title = {Biomedical laboratory scientists and technicians in digital pathology – Is there a need for professional development?},
 url = {https://doi-org.crai.referencistas.com/10.1177/20552076241237392},
 volume = {10},
 year = {2024n}
}

@article{doi:10.1177/2056305119896057,
 abstract = {Conflicts involving caste issues, mainly concerning the lowest caste rights, pervade modern Indian society. Caste affiliation, being rigorously enforced by the society, is an official contemporary reality. Although caste identity is a major social discrimination, it also serves as a necessary condition for affirmative action like reservation policy. In this article, we perform an original and rigorous analysis of the discourse involving the theme “caste” in India newspapers. To this purpose, we have implemented a computational analysis over a big dataset of the 2016 and 2017 editions of three major Indian newspapers to determine the most salient themes associated with “caste” in the news. We have used an original mix of state-of-the-art algorithms, including those based on statistical distributions and two-layer neural networks, to detect the relevant topics in the news and characterize their linguistic context. We concluded that there is an excessive association between lower castes, victimization, and social unrest in the news that does not adequately cover the reports on other aspects of their life and personal identity, thus reinforcing conflict, while attenuating the vocality and agency of a large section of the population. From our conclusion, we propose a positive discrimination policy in the newsroom.},
 author = {António Filipe Fonseca and Sohhom Bandyopadhyay and Jorge Louçã and Jaison A. Manjaly},
 doi = {10.1177/2056305119896057},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2056305119896057},
 journal = {Social Media + Society},
 number = {4},
 pages = {2056305119896057},
 title = {Caste in the News: A Computational Analysis of Indian Newspapers},
 url = {https://doi-org.crai.referencistas.com/10.1177/2056305119896057},
 volume = {5},
 year = {2019e}
}

@article{doi:10.1177/20563051221150407,
 abstract = {To counter the fake news phenomenon, the scholarly community has attempted to debunk and prebunk disinformation. However, misinformation still constitutes a major challenge due to the variety of misleading techniques and their continuous updates which call for the exercise of critical thinking to build resilience. In this study we present two open access chatbots, the Fake News Immunity Chatbot and the Vaccinating News Chatbot, which combine Fallacy Theory and Human–Computer Interaction to inoculate citizens and communication gatekeepers against misinformation. These chatbots differ from existing tools both in function and form. First, they target misinformation and enhance the identification of fallacious arguments; and second, they are multiagent and leverage discourse theories of persuasion in their conversational design. After having described both their backend and their frontend design, we report on the evaluation of the user interface and impact on users’ critical thinking skills through a questionnaire, a crowdsourced survey, and a pilot qualitative experiment. The results shed light on the best practices to design user-friendly active inoculation tools and reveal that the two chatbots are perceived as increasing critical thinking skills in the current misinformation ecosystem.},
 author = {Elena Musi and Elinor Carmi and Chris Reed and Simeon Yates and Kay O’Halloran},
 doi = {10.1177/20563051221150407},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/20563051221150407},
 journal = {Social Media + Society},
 number = {1},
 pages = {20563051221150410},
 title = {Developing Misinformation Immunity: How to Reason-Check Fallacious News in a Human–Computer Interaction Environment},
 url = {https://doi-org.crai.referencistas.com/10.1177/20563051221150407},
 volume = {9},
 year = {2023h}
}

@article{doi:10.1177/20563051231196880,
 abstract = {Social media, in general, and Facebook in particular, have been clearly identified as important platforms for the dissemination of mis- and disinformation and related problematic content. However, the patterns and processes of such dissemination are still not sufficiently understood. We detail a novel computational methodology that focusses on the identification of high-profile vectors of “fake news” and other problematic information in public Facebook spaces. The method enables examination of networks of content sharing that emerge between public pages and groups, and external sources, and the study of longitudinal dynamics of these networks as interests and allegiances shift and new developments (such as the COVID-19 pandemic or the US presidential elections) drive the emergence or decline of dominant themes. Through a case study of content captured between 2016 and 2021, we demonstrate how this methodology allows the development of a new and more comprehensive picture of the overall impact of “fake news,” in all its forms, on contemporary societies.},
 author = {Daniel Angus and Axel Bruns and Edward Hurcombe and Stephen Harrington and Xue Ying (Jane) Tan},
 doi = {10.1177/20563051231196880},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/20563051231196880},
 journal = {Social Media + Society},
 number = {3},
 pages = {20563051231196880},
 title = {Computational Communication Methods for Examining Problematic News-Sharing Practices on Facebook at Scale},
 url = {https://doi-org.crai.referencistas.com/10.1177/20563051231196880},
 volume = {9},
 year = {2023b}
}

@article{doi:10.1177/20570473231185996,
 abstract = {In this dialogue, Phillip Howard introduces “computational propaganda” as an emerging communication tool in political communication and a perspective for investigating misinformation and disinformation. By articulating the concepts, patterns, and mechanisms of computational propaganda, Howard proposes a socio-technical framework for studying computational propaganda. He calls for mixed methods to undertake computational research alongside qualitative investigation, thus addressing the computational as well as the political. Howard emphasizes the battle against algorithm bias, manipulation, and misinformation, and he advocates building an International Panel on the Information Environment (IPIE), an international scientific collaboration, to respond to the challenges. In addition, Howard offers advice on further research in computational propaganda.},
 author = {Philip Howard and Fen Lin and Viktor Tuzov},
 doi = {10.1177/20570473231185996},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/20570473231185996},
 journal = {Communication and the Public},
 number = {2},
 pages = {47–53},
 title = {Computational propaganda: Concepts, methods, and challenges},
 url = {https://doi-org.crai.referencistas.com/10.1177/20570473231185996},
 volume = {8},
 year = {2023j}
}

@article{doi:10.1177/20570473241284759,
 abstract = {This article proposes and tests a reproducible framework for a computational method to measure social media-based deliberative discourse by analyzing commentary surrounding the Canadian convoy protests of COVID-19 vaccine mandates and restrictions. Employing a combination of analytic calculations, alongside tools such as Google Perspective and Linguistic Inquiry and Word Count (LIWC), this article assesses the quality of online deliberative discourse using established measures of deliberation including the variables rationality, interactivity, equality, and civility. We propose computational approaches to measuring these variables, and work toward validating our approach by observing correlations between an established computational measure of online deliberation-cognitive complexity. This computational approach is tested using Twitter and Reddit commentary related to the convoy protests that took place in Ottawa, Canada, during February 2022, which influenced the emergence of similar protests around the world. In addition to testing our proposed online deliberative discourse measurement framework, this case study provides insight into the deliberative characteristics of the Twitter and Reddit social media platforms.},
 author = {Stuart Duncan and Lauren Dwyer and Hanako Smith and Davis Vallesi and Frauke Zeller and Charles Davis},
 doi = {10.1177/20570473241284759},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/20570473241284759},
 journal = {Communication and the Public},
 number = {0},
 pages = {20570473241284760},
 title = {Toward a computational mixed methods framework to measure online deliberative discourse},
 url = {https://doi-org.crai.referencistas.com/10.1177/20570473241284759},
 volume = {0},
 year = {2024f}
}

@article{doi:10.1177/2057158517704398,
 abstract = {The aim of this study was to describe and compare perceptions of critical thinking, attitudes to and availability of research, research utilization and barriers to this among nursing students in Scandinavia and Indonesia. Data were collected at the beginning, middle and end of education from nursing students in Norway, Sweden (bachelor’s diploma) and Banda Aceh (bachelor’s diploma). Critical Thinking and Research Utilization Questionnaires were used along with the Barrier Scale. Descriptive analyses, comparisons between and within groups were performed. At the end of education, all samples exhibited positive attitudes to research and the main barrier was related to the setting. Scandinavian students reported higher critical thinking. Indonesian students perceived greater barriers on two Barrier subscales. No differences were found between the samples regarding research utilization. Significant changes over time varied among the samples except for the Norwegian sample. Indonesian students (diploma) exhibited most changes over time. Teachers must support nursing students to strengthen their critical thinking ability and develop professional competence.},
 author = {Bodil Wilde-Larsson and Ilyas Aiyub and Hasan Hermansyah and Reidun Hov and Sevald Høye and Margrethe Valen Gillund and Kari Kvigne and Abubakar Suwarni and Gun Nordström},
 doi = {10.1177/2057158517704398},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2057158517704398},
 journal = {Nordic Journal of Nursing Research},
 number = {1},
 pages = {28–37},
 title = {Critical thinking, research utilization and barriers to this among nursing students in Scandinavia and Indonesia},
 url = {https://doi-org.crai.referencistas.com/10.1177/2057158517704398},
 volume = {38},
 year = {2018r}
}

@article{doi:10.1177/2059204317741717,
 abstract = {The 10th annual International Conference of Students of Systematic Musicology (SysMus) took place on September 13–15, 2017, at Queen Mary University of London (UoL). The SysMus series has established itself as an international, student-run conference series aimed at introducing graduate students to networking and discussing their work in an academic conference environment. The term “Systematic Musicology,” first coined by Guido Adler (1885), nowadays covers a wide range of systematic or empirical approaches to theoretical, psychological, neuroscientific, ethnographic, and computational methodologies in music research. Presentations for SysMus17 focused on three central topics in relation to music: cognition and neuroscience, computation, and health and well-being. Each of these topics was the subject of workshops as well as keynotes by Prof. Lauren Stewart (Goldsmiths University of London and Music in the Brain Centre, Aarhus University), Prof. Elaine Chew and Dr. Marcus Pearce (both Queen Mary UoL), Dr. Daniel Müllensiefen (Goldsmiths UoL), and Prof. Aaron Williamon (Royal College of Music). Further presentations addressed issues relating to harmony and rhythm, musicians and performance, music and emotion, and sociology of music. This year’s conference brought together early-career researchers from the fields of musicology, psychology, and medicine, allowing them to socialize, share their work, and gain insight into interdisciplinary approaches to their subjects. SysMus17 was organized by students at Queen Mary’s Music Cognition Lab and was particularly marked by the series’ 10th anniversary, the live streaming of all presentations via social media, and a carbon-offsetting Green Initiative. The proceedings of SysMus17 will be available on demand from the conference website (www.sysmus17.qmul.ac.uk) and the videos will be made available for public access.},
 author = {Emma Allingham and Christopher Corcoran},
 doi = {10.1177/2059204317741717},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2059204317741717},
 journal = {Music & Science},
 number = { },
 pages = {2059204317741717},
 title = {Report on the 10th International Conference of Students of Systematic Musicology (SysMus17)},
 url = {https://doi-org.crai.referencistas.com/10.1177/2059204317741717},
 volume = {1},
 year = {2018c}
}

@article{doi:10.1177/20592043221085659,
 abstract = {Traditional harmonic analysis annotations can be represented in a computer model of a piece of music by plain text strings. But whenever automated processing like analysis, comparison or retrieval is intended, a formal definition is helpful. This should cover not only the syntactic structure, but also the semantics, i.e. the intended meaning, and thus adheres to the technique of mathematical remodelling of existing cultural phenomena. The resulting models can serve as a basis for automated processing, but also help to clarify the communication and discussion among humans substantially. This article proposes such a definition in four layers, which address different problems of encoding and communication: (a) relation of symbol sequences to staff positions, (b) combining functions, (c) chord roots, and (d) interval structure and voice leading. Only one of them is specific to functional (Riemannian) theory and can possibly be replaced to represent scale degree theory. The proposal is configurable to different interval specification methods and open to localisation. Syntax and semantics are defined by precise mathematical means, borrowed from computer science, and thus are unambiguously documented.},
 author = {Markus Lepper and Baltasar Trancòn y Widemann and Michael Oehler},
 doi = {10.1177/20592043221085659},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/20592043221085659},
 journal = {Music & Science},
 number = { },
 pages = {20592043221085660},
 title = {funCode—Versatile Syntax and Semantics for Functional Harmonic Analysis Labels},
 url = {https://doi-org.crai.referencistas.com/10.1177/20592043221085659},
 volume = {5},
 year = {2022m}
}

@article{doi:10.1177/2059799115622763,
 abstract = {Today’s world allows people to connect over larger distances and in shorter intervals than ever before, widely monitored by massive online data sources. Ongoing worldwide computerization has led to completely new opportunities for social scientists to conceive human interactions and relations in unknown precision and quantities. However, the large data sets require techniques that are more likely to be found in computer and natural sciences than in the established fields of social relations. In order to facilitate the participation of social scientists in an emerging interdisciplinary research branch of “computational social science,” we propose in this article the usage of the Python programming language. First, we carve out its capacity to handle “Big Data” in suitable formats. Second, we introduce programming libraries to analyze large networks and big text corpora, conduct simulations, and compare their performance to their counterparts in the R environment. Furthermore, we highlight practical tools implemented in Python for operational tasks like preparing presentations. Finally, we discuss how the process of writing code may help to exemplify theoretical concepts and could lead to empirical applications that gain a better understanding of the social processes initiated by the truly global connections of the Internet era.},
 author = {Raphael H. Heiberger and Jan R. Riebling},
 doi = {10.1177/2059799115622763},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2059799115622763},
 journal = {Methodological Innovations},
 number = { },
 pages = {2059799115622763},
 title = {Installing computational social science: Facing the challenges of new information and communication technologies in social science},
 url = {https://doi-org.crai.referencistas.com/10.1177/2059799115622763},
 volume = {9},
 year = {2016j}
}

@article{doi:10.1177/2096531120944929,
 abstract = {Purpose: This article aims to shed light on a latest education informatization policy blueprint in China, titled Education Informatization 2.0 Action Plan, which was promulgated by the Ministry of Education in China on April 18, 2018. Design/Approach/Methods: The study is an analytical policy review based on the policy documents, theoretical discussion, and development of practice. Findings: This new Chinese education informatization policy was driven by three factors: the promotion of education informatization 1.0 in China, the requirement of education modernization toward 2035, and the response to “Wisdom Education.” The framework for action can be summarized as “One Goal, Three Tasks, and Eight Actions.” The main features involve innovation-driven development rather than technology-driven development, committing to the expansion of digital educational resources rather than the digital presentation of textbooks, and aiming at improving teachers and students’ information literacy rather than the applied skills of information technology. The future vision of the plan involves building new models on talent cultivation, education service, and education governance. The new models on talent cultivation involve establishing “Wisdom Teaching” mode, learning mode, and intelligent learning environment supported by artificial intelligence technology. The new education service models entail building the admission and sharing mechanism of quality educational resources based on National Network for Education and the public service platform and system for educational resources by means of the cloud computing and artificial intelligence. The new education governance models involve achieving precise, flat, and humanized education governance. Originality/Value: This article entails expounding the motivation, framework for action, main features, and vision of the education informatization 2.0 in China, which will be helpful for learning and understanding the current background, stage, and future path of China’s education informatization.},
 author = {Shouxuan Yan (闫守轩) and Yun Yang (杨运)},
 doi = {10.1177/2096531120944929},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2096531120944929},
 journal = {ECNU Review of Education},
 number = {2},
 pages = {410–428},
 title = {Education Informatization 2.0 in China: Motivation, Framework, and Vision},
 url = {https://doi-org.crai.referencistas.com/10.1177/2096531120944929},
 volume = {4},
 year = {2021t}
}

@article{doi:10.1177/20965311221143798,
 abstract = {Purpose There is limited scholarship on artificial intelligence (AI) in higher education governance, despite the growing prevalence of AI-powered technologies in many fields, including education. However, as the technology is still nascent and has yet to reach its full potential, ideas and arguments abound, championing or cautioning against the use of these technologies. Design/Approach/Methods To fill this gap in research on policy networks and AI in British higher education, this article employs network ethnography and discourse analysis to study how ideas about AI-powered technologies in higher education circulate in policy networks in the United Kingdom. Findings The findings evidence a policy network showing signs of a heterarchy permeated by neoliberal rationales and populated by policy actors actively promoting artificial intelligence technologies to be used in education. Originality/Value This paper builds on existing research by looking at the university and not-for-profit sectors, in addition to the governmental and educational technology sectors. Using network ethnography, this article expands our understanding of the policy actors involved and critically analyzes ideas regarding the use of AI in education.},
 author = {Dániel Béla Gellai},
 doi = {10.1177/20965311221143798},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/20965311221143798},
 journal = {ECNU Review of Education},
 number = {4},
 pages = {568–596},
 title = {Enterprising Academics: Heterarchical Policy Networks for Artificial Intelligence in British Higher Education},
 url = {https://doi-org.crai.referencistas.com/10.1177/20965311221143798},
 volume = {6},
 year = {2023f}
}

@article{doi:10.1177/20965311231158393,
 abstract = {Purpose To analyze mathematics problem-solving (PS) procedures in Chinese (CH) and Canadian (CA) elementary mathematics textbooks that leverage computational thinking (CT) as a cognitive tool, which have evidently existed and been implemented. Design/Approach/Methods In this study, an analysis framework was developed to investigate the characteristics of CT tools for three PS steps—understand the problem, devise and conduct plans, and look back into textbooks—in four contexts: data practices, modeling and simulation practices, computational tools practices, and systemic thinking practices. Findings Our results demonstrate the tools (CT) employed in the PS process in CH and CA mathematics textbooks. The strong connections between the “look back” stage and CT tools were explored. During the “look back” stage, both countries required students to transfer their knowledge and perform generalization. In addition, CT is regarded as a basic skill analysis for students in mathematics education and has received significant attention at every stage of the PS process. Originality/Value This study brings a new perspective to CT research in education by regarding CT as a cognitive tool for students in mathematics PS.},
 author = {Yimei Zhang (张艺美) and Annie Savard},
 doi = {10.1177/20965311231158393},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/20965311231158393},
 journal = {ECNU Review of Education},
 number = {4},
 pages = {677–699},
 title = {Defining Computational Thinking as an Evident Tool in Problem-Solving: Comparative Research on Chinese and Canadian Mathematics Textbooks},
 url = {https://doi-org.crai.referencistas.com/10.1177/20965311231158393},
 volume = {6},
 year = {2023x}
}

@article{doi:10.1177/20965311231206148,
 abstract = {Highlights We live in a technology-driven age, so the education field must rationally consider chatbots powered by artificial intelligence (AI). Faced with increasing application of AI in education, teachers should be reflective educators, and students should be self-educators. AI chatbots will evolve into a new prosthesis before being institutionalized into the fabric of school education. Reflective educators, institutionalized educators, self-educators foreshadow the future rise of hybrid educators.},
 author = {Youchao Deng (邓友超)},
 doi = {10.1177/20965311231206148},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/20965311231206148},
 journal = {ECNU Review of Education},
 number = {3},
 pages = {677–680},
 title = {The Rise of Hybrid Educators},
 url = {https://doi-org.crai.referencistas.com/10.1177/20965311231206148},
 volume = {7},
 year = {2024f}
}

@article{doi:10.1177/2158244019832687,
 abstract = {Although a wealth of studies on companies’ environmental behavior exists, little is known about the factors with the greatest influence on the evolution of such behavior. Thus, employing empirical data on China and an agent-based simulation model, this study examines the evolution from defensive to preventive environmental behavior. The results show that community support is the most important factor in this process, followed by managers’ environmental awareness and companies’ financial ability. However, financial ability is the most significant factor in the evolution from preventive to enthusiastic environmental behavior, followed by managers’ environmental awareness and community support. Our identification of the most important factors can serve as a basis for decision makers to focus on improving the operational effectiveness of environmental policies.},
 author = {Yong Liu and Fei Li and Yunpeng Su},
 doi = {10.1177/2158244019832687},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2158244019832687},
 journal = {Sage Open},
 number = {1},
 pages = {2158244019832687},
 title = {Critical Factors Influencing the Evolution of Companies’ Environmental Behavior: An Agent-Based Computational Economic Approach},
 url = {https://doi-org.crai.referencistas.com/10.1177/2158244019832687},
 volume = {9},
 year = {2019j}
}

@article{doi:10.1177/21582440211016418,
 abstract = {Computational thinking (CT) is being recognized as a critical component of student success in the digital era. Many contend that integrating CT into core curricula is the surest method for providing all students with access to CT. However, the CT community lacks an agreed-upon conceptualization of CT that would facilitate this integration, and little effort has been made to critically analyze and synthesize research on CT/content integration (CTCI). Conflicting CT conceptualizations and little understanding of evidence-based strategies for CTCI could result in significant barriers to increasing students’ access to CT. To address these concerns, we analyzed 80 studies on CT education, focusing on both the CT conceptualizations guiding current CT education research and evidence-based strategies for CTCI. Our review highlights the code-centric nature of CT education and reveals significant gaps in our understanding of CTCI and CT professional development for teachers. Based on these findings, we propose an approach to operationalizing CT that promotes students’ participation in CT, present promising methods for infusing content with CT, and discuss future directions for CT education research.},
 author = {Vance Kite and Soonhye Park and Eric Wiebe},
 doi = {10.1177/21582440211016418},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/21582440211016418},
 journal = {Sage Open},
 number = {2},
 pages = {21582440211016416},
 title = {The Code-Centric Nature of Computational Thinking Education: A Review of Trends and Issues in Computational Thinking Education Research},
 url = {https://doi-org.crai.referencistas.com/10.1177/21582440211016418},
 volume = {11},
 year = {2021h}
}

@article{doi:10.1177/21582440221097916,
 abstract = {For 2 weeks in the summer of 2018, K-12 science, technology, engineering, and mathematics (STEM) teachers (n = 40) attended a professional development (PD) that included four sessions focused on computer science modeling with follow-up academic year sessions; however, overall, the teachers did not incorporate or utilize modeling means or how as the instructors intended. The purpose of the study is to examine why this occurred, and the authors looked at the teachers’ modeling discourse. Using two theories to connect to practice (terministic screens, and schema theory), the authors collected data via the surveys, interviews, and email reflections. The authors analyzed the results via coding to explore participants’ concept of models and the potential difficulties of implementing computer modeling in their classrooms. Findings show that the term model was interpreted differently by the PD’s faculty team and participants. Further, the authors found that the majority of presenters held differing theories of models than the participants. Participant concepts of models did improve slightly after the PD, but lingering model concepts caused confusion with the anticipated PD results. Conclusions include five general modeling concepts which are presented and explained. Implications are provided showcasing articulated keys for delivering PD that assists in eliminating discursive and theoretical issues. Included are considerations for STEM teacher educators, PD providers, and K-12 teachers. The main study limitations include mixed K-12 teaching participants, distance between participants, a self-selected population, and non-generalizable findings based on qualitative work. Future directions are outlined.},
 author = {Todd Reynolds and Andrea C. Burrows and Mike Borowczak},
 doi = {10.1177/21582440221097916},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/21582440221097916},
 journal = {Sage Open},
 number = {2},
 pages = {21582440221097916},
 title = {Confusion Over Models: Exploring Discourse in a STEM Professional Development},
 url = {https://doi-org.crai.referencistas.com/10.1177/21582440221097916},
 volume = {12},
 year = {2022o}
}

@article{doi:10.1177/21582440221099525,
 abstract = {Many countries and regions have reached a consensus to promote science, technology, engineering, and mathematics (STEM) education in the past decade. A body of studies have demonstrated that the design and organization of interdisciplinary teaching activities are important for the effective implementation of STEM education. So far, however, little attention has been paid to the taxonomy of teaching models in STEM education. This paper aims to propose a taxonomy based on the dimensions of learning outcomes (i.e., product-oriented and knowledge-oriented) and teaching process (i.e., forward teaching model and reverse teaching model) in STEM education. Through the intersection of the above two dimensions, four teaching models in STEM education have emerged, including project-based learning (PBL), reverse engineering (RE), scientific inquiry (SI), and troubleshooting/debugging (T/D). In addition, four cases are introduced to explain how these four teaching models operate in STEM education. The implications of this work for future research are also discussed. It is promising that the study will be valuable to enrich the current research, and shed light on the theory and practice of STEM education.},
 author = {Baichang Zhong and Xiaofan Liu and Liying Xia and Wang Sun},
 doi = {10.1177/21582440221099525},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/21582440221099525},
 journal = {Sage Open},
 number = {2},
 pages = {21582440221099524},
 title = {A Proposed Taxonomy of Teaching Models in STEM Education: Robotics as an Example},
 url = {https://doi-org.crai.referencistas.com/10.1177/21582440221099525},
 volume = {12},
 year = {2022t}
}

@article{doi:10.1177/21582440231175371,
 abstract = {Massive Open Online Courses have become a frequent platform for learners to acquire knowledge. This study aims to explore multiple factors influencing learner retention in MOOCs during the COVID-19 pandemic. To address this, we collected quantitative and qualitative data from questionnaires and qualitative data from interviews and then analyzed them through the Partial Least Square–Structural Equation Modeling to test 14 research hypotheses. The proposed research model and research hypotheses are empirically tested with 243 participants across the world. According to the results, support is found for all of the 14 research hypotheses. We confirmed 14 factors influencing learner retention in MOOCs. The result is beneficial for designers and manufacturers of MOOCs to improve the quality of the products and facilitate online or blended learning during this special time. It could also help students improve their learning experiences. Future research could examine influencing factors of learner retention in MOOCs with interdisciplinary cooperation.},
 author = {Zhonggen Yu and Liheng Yu},
 doi = {10.1177/21582440231175371},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/21582440231175371},
 journal = {Sage Open},
 number = {2},
 pages = {21582440231175372},
 title = {Examining Factors That Influence Learner Retention in MOOCs During the COVID-19 Pandemic Time},
 url = {https://doi-org.crai.referencistas.com/10.1177/21582440231175371},
 volume = {13},
 year = {2023t}
}

@article{doi:10.1177/21582440231179710,
 abstract = {This study investigated the effect of educational board games and the creative thinking spiral teaching strategy (CTSTS) on the learning outcomes of beginner-level Chinese language learners. Two dimensions were measured: learning outcome and writing ability. A total of 82 learners from one university in Taiwan participated in this study. Participants were non-randomly selected using convenience sampling. The participants were divided into the control group (40 participants) and experimental group (42 participants). In the control group, the teacher used the board game “Conveyance GO” as a teaching tool; in the experimental group, both Conveyance GO and CTSTS were used to teach the Chinese language. The results indicate no significant difference in learning outcomes between the groups. However, the experimental group exhibited a significantly larger improvement in writing abilities than did the control group, especially in terms of cohesion, coherence, and grammatical accuracy.},
 author = {Ju-May Wen and Hai-Dung Do and Eric Zhi-Feng Liu and Chun-Hung Lin and Shihping Kevin Huang},
 doi = {10.1177/21582440231179710},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/21582440231179710},
 journal = {Sage Open},
 number = {3},
 pages = {21582440231179710},
 title = {Strengthening Writing Ability Among Students Learning Chinese as a Second Language Through Creative Thinking Spiral Teaching Strategy},
 url = {https://doi-org.crai.referencistas.com/10.1177/21582440231179710},
 volume = {13},
 year = {2023r}
}

@article{doi:10.1177/21582440231205409,
 abstract = {The cultivation of computational thinking and programing education have gained prominence in K-12 education worldwide. Primary school teachers should be proficient in visual programing and using microcontrollers to teach programing courses. To cope with these trends, a learning activity was developed and implemented in Taiwan’s primary teacher education curriculum. The activity aimed to help preservice primary teachers learn about Scratch visual programing and micro:bit microcontroller boards by engaging in a physical computing project involving the design of an educational motion sensor game about energy. The results of the preliminary study found that the preservice primary teachers who participated in the activity were able to collaborate and develop motion sensor games suitable for primary school students. They also demonstrated significant improvements in their computational thinking concepts (t(10) = 3.13, p < .05) and energy knowledge test scores (t(10) = 2.74, p < .05). Furthermore, most participants expressed satisfaction with the activity, implying the activity’s feasibility for teacher education.},
 author = {Fu-Hsing Tsai},
 doi = {10.1177/21582440231205409},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/21582440231205409},
 journal = {Sage Open},
 number = {4},
 pages = {21582440231205410},
 title = {Using a Physical Computing Project to Prepare Preservice Primary Teachers for Teaching Programing},
 url = {https://doi-org.crai.referencistas.com/10.1177/21582440231205409},
 volume = {13},
 year = {2023o}
}

@article{doi:10.1177/21582440231215123,
 abstract = {Big data analytics (BDA) enhances knowledge and decision-making. Despite its importance, the connection between technical progress and political change is neglected in the administrative process. Most studies focus on e-government, e-governance, and how technology can improve existing operations of the bureaucracy. However, this article aims to explore the potential of BDA for public policy systems and provide a linkage for the transformation toward digital and smart governance using preferred reported items for systematic review and meta-analysis (PRISMA) approach to reveal the relevant documents and narrative review approach to interpret the application of BDA at each step of the public policy system. In addition, this study identifies several common public policy-related big data sources and techniques that could be used at the various stages of the public policy process. This study argues that BDA has the potential to be used for policy formulation in the four main phases—planning, design, service delivery, and evaluation. Most studies confirm its potential in the policy process for taxation, health, education, transportation, law, economy, and social system. This study reveals that it is also suitable for public policy execution stages, such as public supervision, public regulation, service delivery, and policy feedback. Previous studies have indicated that the application of BDA can transform traditional or manual governance systems into digital and smart governance. We contend that the policy cycle should be seen as a dynamic and iterative process characterized by continuous evolution. Though each step of transformation has its unique challenges in handling BDA and maintaining the Information and Communication Technology (ICT) infrastructure, it can ensure an accurate, prompt, and context-oriented public policy system. These insights provide a novel outlook on effectively managing the interplay between innovation and traditional approaches in the realm of public policy development.},
 author = {Md Altab Hossin and Jie Du and Lei Mu and Isaac Owusu Asante},
 doi = {10.1177/21582440231215123},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/21582440231215123},
 journal = {Sage Open},
 number = {4},
 pages = {21582440231215124},
 title = {Big Data-Driven Public Policy Decisions: Transformation Toward Smart Governance},
 url = {https://doi-org.crai.referencistas.com/10.1177/21582440231215123},
 volume = {13},
 year = {2023c}
}

@article{doi:10.1177/21582440231217715,
 abstract = {The capability of computer programming language logic is one of the basics of technical education. How to improve students “interest in program logic design and help overcome students” fears of coding has become vital for educators. Cultivating practical talents with information technology application and basic programming development will become one of the important topics in the department of information related science. The objective of this research is to improve the ability of learning basic programming courses by using Zuvio interactive software. Zuvio employs the mathematical logic of computational thinking to analyze problems and enhance learners’ interest in learning programming skills through a graphical interface tool with building blocks. It uses innovative interactive teaching to use peer and self-assessment to study the content of the course. Zuvio improves the design ability of different groups of class learning Python programming. In line with the innovative teaching policy of the schools and the current stage of the learner’s learning model, learning effectiveness can be achieved. The research results were analyzed by midterm and final experimental group scores, and the progress of the experimental group’s scores was examined through descriptive statistics. The average and standard deviation of the assessment were used to analyze the progress of the experimental group students in the programming course. In the classroom, assessment criteria were set up as the basis for peer assessment scoring. After the midterm and final exams, the teacher assessment and peer assessment scores were analyzed for cognitive differences, and possible learning differences were analyzed. The students’ professional ability was examined to see if it met the professional standards required by the course, and whether innovative teaching methods could improve the learning outcomes of learners with different professional backgrounds in Python programming.},
 author = {Tsung-Chih Hsiao and Ya-Hsueh Chuang and Chien-Yun Chang and Tzer-Long Chen and Hong-Bo Zhang and Jhih-Chung Chang},
 doi = {10.1177/21582440231217715},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/21582440231217715},
 journal = {Sage Open},
 number = {4},
 pages = {21582440231217716},
 title = {Combining Building Block Process With Computational Thinking Improves Learning Outcomes of Python Programming With Peer Assessment},
 url = {https://doi-org.crai.referencistas.com/10.1177/21582440231217715},
 volume = {13},
 year = {2023d}
}

@article{doi:10.1177/21582440241236242,
 abstract = {This study examines and explores the challenges, issues, and problems with their solutions regarding Science, Technology, Engineering, and Mathematics (STEM) education along with the awareness enjoyed by science teachers. The selection of papers for this study was done through systematic literature review techniques. The 11 themes were generated through the thematic analysis of the papers that is, STEM education and its awareness; its challenges, issues, and problems; its curriculum; inquiry and learning environment; technologies; learning and models of integration; a blended learning approach; students’ problem-solving abilities and skills; its implementation and evaluation; it’s research and current trends; and interdisciplinary collaboration and intervention. Policy and planning, infrastructure, and the state of society were found the main factors of the poor conditions of STEM education. STEM research studies suggest that a curriculum with a blended learning approach has to be integrated by interventions of models depicting technological and inquiry-based environments to bring awareness among teachers, which strengthens the students’ problem-solving abilities and skills.},
 author = {Zafarullah Sahito and Shahid Hussain Wassan},
 doi = {10.1177/21582440241236242},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/21582440241236242},
 journal = {Sage Open},
 number = {1},
 pages = {21582440241236240},
 title = {Literature Review on STEM Education and Its Awareness among Teachers: An Exploration of Issues and Problems with Their Solutions},
 url = {https://doi-org.crai.referencistas.com/10.1177/21582440241236242},
 volume = {14},
 year = {2024s}
}

@article{doi:10.1177/21582440241249897,
 abstract = {Problem-solving skills are an ability that must be cultivated to equip students with the skills needed to deal with today’s increasingly complex and volatile environment. Computational thinking represents a new paradigm in problem-solving skills. After Wing proposed Computational Thinking as problem-solving skills in 2006, other scholars investigated this topic; nevertheless, the link between Computational Thinking and problem-solving has not been clearly discussed in previous studies. To uncover evidence for the connection between Computational Thinking and problem-solving skills, we conduct a systematic literature review of 37 papers collected from Web of Science database. The results indicate that (a) problem-solving is discussed in the 37 articles in the context of Computational Thinking, (b) the most frequently employed Computational Thinking stages in problem-solving skills are decomposition, pattern recognition, abstraction, and algorithm, (c) Computational Thinking is closely linked to problem-solving, and (d) Computational Thinking and problem-solving stages serve the same functions in solving problems. The results of this study will encourage the development of education research, particularly in the application of CT as a problem-solving tool in various real-life scenarios.},
 author = {Ting-Ting Wu and Andik Asmara and Yueh-Min Huang and Intan Permata Hapsari},
 doi = {10.1177/21582440241249897},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/21582440241249897},
 journal = {Sage Open},
 number = {2},
 pages = {21582440241249896},
 title = {Identification of Problem-Solving Techniques in Computational Thinking Studies: Systematic Literature Review},
 url = {https://doi-org.crai.referencistas.com/10.1177/21582440241249897},
 volume = {14},
 year = {2024r}
}

@article{doi:10.1177/21582440241254595,
 abstract = {Uzbekistan has not adopted robotics education as the school curriculum yet. However, several robotics learning centers have introduced robotics education in an informal setting. This research paper aims to highlight the essence of robotics education in Uzbekistan investigating the perception of parents and children to their full potential and identifying impediments to the process of implementing robotics in Uzbekistan. This research study involves primary and secondary research methods. A systematic literature review was conducted to examine the reflection of robotics education among primary and secondary school children. Official statistical data was gathered to prove the scope of demographics. Primary data was collected through the survey among parents whose children attended robotics classes. Ultimately, the authors have used empirical evidence to provide recommendations and solutions on how to implement robotics education effectively in Uzbekistan. Much emphasis has not been put on robotics education in Uzbekistan, despite reforms in the field of STEM education. Moreover, the condition (including teachers, lesson materials, classrooms, computers, and robotics kits) to implement robotics classes as more developed countries are doing has not been created properly, which hampers the introduction of robotics at schools. In addition, the majority of parents are not fully aware of the authentic value of robotics education in children’s lives. Hence, Uzbek schools are lagging in the field of robotics.},
 author = {Indira Abdullaeva Yuldashevna and Karan Khurana},
 doi = {10.1177/21582440241254595},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/21582440241254595},
 journal = {Sage Open},
 number = {2},
 pages = {21582440241254596},
 title = {The Impediments to the Process of Implementing Robotics in the School Education System in Uzbekistan},
 url = {https://doi-org.crai.referencistas.com/10.1177/21582440241254595},
 volume = {14},
 year = {2024x}
}

@article{doi:10.1177/21582440241260612,
 abstract = {Since the first phase of the lockdown in Malaysia, the cinema has been the place which best exemplifies the implementation of the control order in the venue-based sectors. After almost 2 years of physical distancing and lifestyle changes, the authority attempted to bring a new form of “normal life” to its residents. Such a decision witnessed more economic sectors were permitted to reopen, cinema among them. This study employed a mixed-method approach which aims to identify the emerging factors which inform about Malaysian audiences’ perception of cinema-going in the context of the COVID-19 pandemic. It examines the audiences with the help of the Theory of Planned Behavior (TPB) as this theory claims that people are more likely to act in a certain way if they feel certain behaviors will lead to specific results that are in keeping with their values. This study identified the attitude toward cinema reopening and readiness toward visiting cinema are able to significantly predict all the related factors of cinemagoers’ willingness in the post COVID-19 era. The results informed on the concerns of their family members, friends, and those with whom they have regular physical contact have become the agent of decision-making in terms of cinema-going.},
 author = {Wang Changsong and Low Jinghong and Aleena Tan Poh Ling and Muhammad Afiq Bin Sukiman and Lucyann Kerry},
 doi = {10.1177/21582440241260612},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/21582440241260612},
 journal = {Sage Open},
 number = {2},
 pages = {21582440241260612},
 title = {An Understanding of Malaysian Cinemagoers in the Post COVID-19 Era by Using a Computational Ontology},
 url = {https://doi-org.crai.referencistas.com/10.1177/21582440241260612},
 volume = {14},
 year = {2024c}
}

@article{doi:10.1177/2167702614562040,
 abstract = {Psychiatric disorders profoundly impair many aspects of decision making. Poor choices have negative consequences in the moment and make it very hard to navigate complex social environments. Computational neuroscience provides normative, neurobiologically informed descriptions of the components of decision making that serve as a platform for a principled exploration of dysfunctions. Here, we identify and discuss three classes of failure modes arising in these formalisms. They stem from abnormalities in the framing of problems or tasks, from the mechanisms of cognition used to solve the tasks, or from the historical data available from the environment.},
 author = {Quentin J. M. Huys and Marc Guitart-Masip and Raymond J. Dolan and Peter Dayan},
 doi = {10.1177/2167702614562040},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2167702614562040},
 journal = {Clinical Psychological Science},
 number = {3},
 pages = {400–421},
 title = {Decision-Theoretic Psychiatry},
 url = {https://doi-org.crai.referencistas.com/10.1177/2167702614562040},
 volume = {3},
 year = {2015e}
}

@article{doi:10.1177/2167702614565359,
 abstract = {Psychiatric research is in crisis. We highlight efforts to overcome current challenges by focusing on the emerging field of computational psychiatry, which might enable the field to move from a symptom-based description of mental illness to descriptors based on objective computational multidimensional functional variables. We survey recent efforts toward this goal and describe a set of methods that together form a toolbox to aid this research program. We identify four levels in computational psychiatry: (a) behavioral tasks that index various psychological processes, (b) computational models that identify the generative psychological processes, (c) parameter-estimation methods concerned with quantitatively fitting these models to subject behavior by focusing on hierarchical Bayesian estimation as a rich framework with many desirable properties, and (d) machine-learning clustering methods that identify clinically significant conditions and subgroups of individuals. As a proof of principle, we apply these methods to two different data sets. Finally, we highlight challenges for future research.},
 author = {Thomas V. Wiecki and Jeffrey Poland and Michael J. Frank},
 doi = {10.1177/2167702614565359},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2167702614565359},
 journal = {Clinical Psychological Science},
 number = {3},
 pages = {378–399},
 title = {Model-Based Cognitive Neuroscience Approaches to Computational Psychiatry: Clustering and Classification},
 url = {https://doi-org.crai.referencistas.com/10.1177/2167702614565359},
 volume = {3},
 year = {2015w}
}

@article{doi:10.1177/2167702614567350,
 author = {Tiago V. Maia},
 doi = {10.1177/2167702614567350},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2167702614567350},
 journal = {Clinical Psychological Science},
 number = {3},
 pages = {374–377},
 title = {Introduction to the Series on Computational Psychiatry},
 url = {https://doi-org.crai.referencistas.com/10.1177/2167702614567350},
 volume = {3},
 year = {2015j}
}

@article{doi:10.1177/2167702614567351,
 author = {Alan E. Kazdin},
 doi = {10.1177/2167702614567351},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2167702614567351},
 journal = {Clinical Psychological Science},
 number = {3},
 pages = {372–373},
 title = {Editor’s Introduction to the Special Series: Computational Psychiatry},
 url = {https://doi-org.crai.referencistas.com/10.1177/2167702614567351},
 volume = {3},
 year = {2015h}
}

@article{doi:10.1177/2167702615578130,
 abstract = {Impairments in cognitive processes have been theorized to play a critical role in rumination, a well-established risk factor for depression. In this review, we outline central theories that present cognitive impairments as causal contributors to ruminative thinking and review relevant findings from cross-sectional and prospective studies. We then focus on experimental evidence gathered within the paradigm of cognitive bias modification (CBM). Although CBM has generated considerable interest in relation to anxiety and depression, it has only recently emerged in the field of rumination. After considering the purpose and possible advantages of CBM procedures, we review CBM work related to rumination and discuss key limitations and implications within this developing area of research. Among our recommendations, we outline ways to contrast and integrate cognitive theories of rumination, as well as to obtain stronger bias modification procedures.},
 author = {Nilly Mor and Shimrit Daches},
 doi = {10.1177/2167702615578130},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2167702615578130},
 journal = {Clinical Psychological Science},
 number = {4},
 pages = {574–592},
 title = {Ruminative Thinking: Lessons Learned From Cognitive Training},
 url = {https://doi-org.crai.referencistas.com/10.1177/2167702615578130},
 volume = {3},
 year = {2015m}
}

@article{doi:10.1177/2167702616654688,
 abstract = {Calculating math problems from memory may seem unrelated to everyday processing of emotions, but they have more in common than one might think. Prior research highlights the importance of the dorsolateral prefrontal cortex (dlPFC) in executive control, intentional emotion regulation, and experience of dysfunctional mood and anxiety. Although it has been hypothesized that emotion regulation may be related to “cold” (i.e., not emotion-related) executive control, this assertion has not been tested. We address this gap by providing evidence that greater dlPFC activity during cold executive control is associated with increased use of cognitive reappraisal to regulate emotions in everyday life. We then demonstrate that in the presence of increased life stress, increased dlPFC activity is associated with lower mood and anxiety symptoms and clinical diagnoses. Collectively, our results encourage ongoing efforts to understand prefrontal executive control as a possible intervention target for improving emotion regulation in mood and anxiety disorders.},
 author = {Matthew A. Scult and Annchen R. Knodt and Johnna R. Swartz and Bartholomew D. Brigidi and Ahmad R. Hariri},
 doi = {10.1177/2167702616654688},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2167702616654688},
 journal = {Clinical Psychological Science},
 note = {PMID:28191365},
 number = {1},
 pages = {150–157},
 title = {Thinking and Feeling: Individual Differences in Habitual Emotion Regulation and Stress-Related Mood Are Associated With Prefrontal Executive Control},
 url = {https://doi-org.crai.referencistas.com/10.1177/2167702616654688},
 volume = {5},
 year = {2017l}
}

@article{doi:10.1177/21677026211022013,
 abstract = {Suicide, a leading cause of death, is a complex and a hard-to-predict human tragedy. In this article, we introduce a comprehensive outlook on the emerging movement to integrate computational linguistics (CL) in suicide prevention research and practice. Focusing mainly on the state-of-the-art deep neural network models, in this “travel guide” article, we describe, in a relatively plain language, how CL methodologies could facilitate early detection of suicide risk. Major potential contributions of CL methodologies (e.g., word embeddings, interpretational frameworks) for deepening that theoretical understanding of suicide behaviors and promoting the personalized approach in psychological assessment are presented as well. We also discuss principal ethical and methodological obstacles in CL suicide prevention, such as the difficulty to maintain people’s privacy/safety or interpret the “black box” of prediction algorithms. Ethical guidelines and practical methodological recommendations addressing these obstacles are provided for future researchers and clinicians.},
 author = {Yaakov Ophir and Refael Tikochinski and Anat Brunstein Klomek and Roi Reichart},
 doi = {10.1177/21677026211022013},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/21677026211022013},
 journal = {Clinical Psychological Science},
 number = {2},
 pages = {212–235},
 title = {The Hitchhiker’s Guide to Computational Linguistics in Suicide Prevention},
 url = {https://doi-org.crai.referencistas.com/10.1177/21677026211022013},
 volume = {10},
 year = {2022j}
}

@article{doi:10.1177/2167702621998344,
 abstract = {Evaluation (the process attributing value to outcomes) underlies “hot” aspects of cognition, such as emotion, affect, and motivation. In several psychopathologies, such as depression and addiction, impairments in evaluation are critical. Contemporary theories highlight the reference-dependent nature of evaluation, whereby outcomes are evaluated relative to their context. Surprisingly, reference-dependent evaluation remains to be explored in the context of psychopathology. We offer a computational theory of how impaired reference-dependent evaluation might underlie mental illness. The theory proposes that evaluation derives from comparing an outcome against a reference point parameter and by weighting any discrepancy by an uncertainty parameter. Maladaptive evaluation is proposed to occur when these parameters do not reflect the true context statistics. Depending on which parameter is altered, different forms of maladaptive evaluation emerge, each associated with specific clinical conditions. This model highlights how the concept of reference-dependent evaluation can elucidate several clinical conditions, including perfectionism, depression, and addiction.},
 author = {Francesco Rigoli and Cristina Martinelli and Giovanni Pezzulo},
 doi = {10.1177/2167702621998344},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2167702621998344},
 journal = {Clinical Psychological Science},
 number = {6},
 pages = {1021–1034},
 title = {The Half-Empty/Full Glass in Mental Health: A Reference-Dependent Computational Model of Evaluation in Psychopathology},
 url = {https://doi-org.crai.referencistas.com/10.1177/2167702621998344},
 volume = {9},
 year = {2021o}
}

@article{doi:10.1177/21695067231192245,
 abstract = {We provide a transdisciplinary viewpoint on creating artificial social intelligence for human-agent teaming. We discuss theoretical, methodological, and technological insights, drawn from different disciplines, to more fully illuminate how cross-disciplinary research can inform research design and development. We unite ideas spanning human factors, cognitive and computer science, and organizational behavior. Grounding our ideas in real world challenges for human-AI teaming, and via a series of questions designed to facilitate synthesis across disciplines, we illustrate how transdisciplinary team science more effectively asks and answers complex questions on human-agent teaming. Our objective is to contribute to research and development in the field of human-AI and human-robot teaming by emphasizing a more human-centered perspective on AI.},
 author = {Stephen M. Fiore and Matthew Johnson and Paul Robertson and Pablo Diego-Rosell and Adam Fouse},
 doi = {10.1177/21695067231192245},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/21695067231192245},
 journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
 number = {1},
 pages = {419–424},
 title = {Transdisciplinary Team Science: Transcending Disciplines to Understand Artificial Social Intelligence in Human-Agent Teaming},
 url = {https://doi-org.crai.referencistas.com/10.1177/21695067231192245},
 volume = {67},
 year = {2023h}
}

@article{doi:10.1177/21925682221081224,
 abstract = {Study Design Mechanical simulations. Objective Inadequate calibration of annuli negatively affects the computational accuracy of finite element (FE) models. Specifically, the definition of annulus average radius (AR) does not have uniformity standards. Differences between the elastic moduli in the different layers and parts of the annulus were not fully calibrated when a linear elastic material is used to define its material properties. This study aims to optimize the computational accuracy of the FE model by calibrating the annulus. Methods We calibrated the annulus AR and elastic modulus in our anterior-constructed lumbar model by eliminating the difference between the computed range of motion and that measured by in vitro studies under a flexion-extension loading condition. Multi-indicator validation was performed by comparing the computed indicators with those measured in in vitro studies. The computation time required for the different models has also been recorded to evaluate the computational efficiency. Results The difference between computed and measured ROMs was less than 1% when the annulus AR and elastic modulus were calibrated. In the model validation process, all the indicators computed by the calibrated FE model were within ±1 standard deviation of the average values obtained from in vitro studies. The maximum difference between the computed and measured values was less than 10% under nearly all loading conditions. There is no apparent variation tendency for the computational time associated with different models. Conclusion The FE model with calibrated annulus AR and regional elastic modulus has higher computational accuracy and can be used in subsequent mechanical studies.},
 author = {Chen Xu and Zhipeng Xi and Zhongxin Fang and Xiaoyu Zhang and Nan Wang and Jingchi Li and Yang Liu},
 doi = {10.1177/21925682221081224},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/21925682221081224},
 journal = {Global Spine Journal},
 note = {PMID:35293827},
 number = {8},
 pages = {2310–2318},
 title = {Annulus Calibration Increases the Computational Accuracy of the Lumbar Finite Element Model},
 url = {https://doi-org.crai.referencistas.com/10.1177/21925682221081224},
 volume = {13},
 year = {2023t}
}

@article{doi:10.1177/23259671221084970,
 abstract = {Background: Recent studies on lateral knee anatomy have reported the presence of a true ligament structure, the anterolateral ligament (ALL), in the anterolateral region of the knee joint. However, its biomechanical effects have not been fully elucidated. Purpose: To investigate, by using computer simulation, the association between the ALL and anterior cruciate ligament (ACL) under dynamic loading conditions. Study Design: Descriptive laboratory study; Level of evidence, 5. Methods: The authors combined medical imaging from 5 healthy participants with motion capture to create participant-specific knee models that simulated the entire 12 degrees of freedom of tibiofemoral (TF) and patellofemoral (PF) joint behaviors. These dynamic computational models were validated using electromyographic data, muscle activation data, and data from previous experimental studies. Forces exerted on the ALL with ACL deficiency and on the ACL with ALL deficiency, as well as TF and PF contact forces with deficiencies of the ACL, ALL, and the entire ligament structure, were evaluated under gait and squat loading. A single gait cycle and squat cycle were divided into 11 time points (periods 0.0-1.0). Simulated ligament forces and contact forces were compared using nonparametric repeated-measures Friedman tests. Results: Force exerted on the ALL significantly increased with ACL deficiency under both gait- and squat-loading conditions. With ACL deficiency, the mean force on the ALL increased by 129.7% under gait loading in the 0.4 period (P < .05) and increased by 189% under high flexion during the entire cycle of squat loading (P < .05). A similar trend of significantly increased force on the ACL was observed with ALL deficiency. Contact forces on the TF and PF joints with deficiencies of the ACL, ALL, and entire ligament structure showed a complicated pattern. However, contact force exerted on TF and PF joints with respect to deficiencies of ACL and ALL significantly increased under both gait- and squat-loading conditions. Conclusion: The results of this computer simulation study indicate that the ACL and the ALL of the lateral knee joint act as secondary stabilizers to each other under dynamic load conditions.},
 author = {Kyoung-Tak Kang and Yong-Gon Koh and Kyoung-Mi Park and Chong-Hyuk Choi and Min Jung and Hyunik Cho and Sung-Hwan Kim},
 doi = {10.1177/23259671221084970},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/23259671221084970},
 journal = {Orthopaedic Journal of Sports Medicine},
 note = {PMID:35400144},
 number = {4},
 pages = {23259671221084970},
 title = {Effects of the Anterolateral Ligament and Anterior Cruciate Ligament on Knee Joint Mechanics: A Biomechanical Study Using Computational Modeling},
 url = {https://doi-org.crai.referencistas.com/10.1177/23259671221084970},
 volume = {10},
 year = {2022j}
}

@article{doi:10.1177/2327857922111020,
 abstract = {Hand sanitization by healthcare staff remains one of the most effective ways for controlling infection in healthcare settings. However, predicting faithful adherence to Hand Hygiene Compliance (HHC) is difficult in complex environments such as inpatient hospital settings. The main challenge is understanding how different components of human and built systems interact to achieve specific goals such as HHC at the critical moments of care delivery. The aim of this explorative study was to evaluate how Human Factors derived visual salience cues and proximity-compatibility principles might be used in the design of healthcare spaces to support nurse moments of HHC through increased perceived behavioral control and intention. The investigative team used a Collaborative Computational Scenario Planning (CCSP) Model approach to determine the integrative effects of reinforcing and detracting operational and environmental factors on discrete moments of HHC behavior. Supervised Machine Learning analysis was conducted on data collected by a large academic medical center that included HHC observance in clinical staff spanning from 2017 to 2021 in two inpatient hospital units. The probabilistic outcomes of unit based HHC observance likelihood were used to compute Fuzzy Cognitive Model Edge Probabilities between Hand Hygiene (HH) cues and detected HHC at key moments. Hospital infection control experts were then engaged to identify the weight of various reinforcing and detracting operational and environmental factors contributing to HHC observance. Combining the quantitative and qualitative methods, allowed the team to then develop integrative CCSP models which facilitated predictive insight into the development of targeted environmental improvements that might contribute to HHC control and intention to support safer patient care.},
 author = {Lisa Sundahl Platt and Hui Cai and Arezoo Zeinali and Krystle Green and Jordan Harvey and Tanya Mcintosh},
 doi = {10.1177/2327857922111020},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2327857922111020},
 journal = {Proceedings of the International Symposium on Human Factors and Ergonomics in Health Care},
 number = {1},
 pages = {98–103},
 title = {Improving Hand Hygiene Compliance through Collaborative Computational Design},
 url = {https://doi-org.crai.referencistas.com/10.1177/2327857922111020},
 volume = {11},
 year = {2022o}
}

@article{doi:10.1177/2327857923121006,
 abstract = {From their common roots in Human Factors Engineering, Human-Centered Design and Cognitive Systems Engineering have drifted into distinct fields over the past three decades, each developing beneficial heuristics, design patterns, and evaluation methods for designing for individuals and teams, respectively. GeoHAI, a clinical decision support application for preventing hospital-acquired infection, has yielded positive results in early usability testing and is expected to test positively in supporting joint activity, which will be measured through the novel implementation of Joint Activity Monitoring . The design and implementation of this application provide a demonstration of the possibilities and necessities to unify the work of Human-Centered Design and Cognitive Systems Engineering when designing technologies that are usable and useful to individuals engaged in joint activity with machine counterparts and other people. We are calling this unified process Joint Activity Design, which supports designing for machines to be good team players.},
 author = {Aaron Cochran and Michael F. Rayo},
 doi = {10.1177/2327857923121006},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2327857923121006},
 journal = {Proceedings of the International Symposium on Human Factors and Ergonomics in Health Care},
 number = {1},
 pages = {19–23},
 title = {Toward Joint Activity Design: Augmenting User-Centered Design with Heuristics for Supporting Joint Activity},
 url = {https://doi-org.crai.referencistas.com/10.1177/2327857923121006},
 volume = {12},
 year = {2023e}
}

@article{doi:10.1177/2332858416674200,
 abstract = {Current reform efforts in science place a premium on student sense making and participation in the practices of science. Given the disparity between these activities and current teaching practices, effective means of professional development around such practices must be identified. We use a close examination of 106 science teachers participating in Research Experiences for Teachers (RET) to identify, through structural equation modeling, the essential features in supporting teacher learning from these experiences. Findings suggest that participation in RET shape science teacher practice and beliefs, which in turn influence practice. Essential features of RET include engaging teachers socially in the research context and in research projects that are personally relevant to them. The model suggests ways to maximize the professional development potential of RET intended to support engagement in disciplinary practices.},
 author = {Sherry A. Southerland and Ellen M. Granger and Roxanne Hughes and Patrick Enderle and Fengfeng Ke and Katrina Roseler and Yavuz Saka and Miray Tekkumru-Kisa},
 doi = {10.1177/2332858416674200},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2332858416674200},
 journal = {AERA Open},
 number = {4},
 pages = {2332858416674200},
 title = {Essential Aspects of Science Teacher Professional Development: Making Research Participation Instructionally Effective},
 url = {https://doi-org.crai.referencistas.com/10.1177/2332858416674200},
 volume = {2},
 year = {2016n}
}

@article{doi:10.1177/2332858419867653,
 abstract = {Utilizing a nationally representative sample of middle school students, this article focuses on whether students who report experiencing more inquiry-based instruction in science and mathematics classrooms have more positive attitudes toward these subjects. Results of multilevel, multivariate regression analyses revealed that, net of the inclusion of control variables for student, teacher, and school characteristics, a higher frequency of inquiry-based instruction is significantly associated with greater interest, perceptions of utility, and self-efficacy for science and mathematics. Furthermore, although there is some evidence indicating that compared with female students, male students’ perceptions of science utility are higher in relation to more inquiry-based instruction, overall, the weight of evidence clearly leans toward the conclusion that the attitudes of students from different gender and racial/ethnic backgrounds are similarly associated with greater exposure to inquiry-based instruction in both their science and mathematics classrooms.},
 author = {Catherine Riegle-Crumb and Karisma Morton and Ursula Nguyen and Nilanjana Dasgupta},
 doi = {10.1177/2332858419867653},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2332858419867653},
 journal = {AERA Open},
 number = {3},
 pages = {2332858419867653},
 title = {Inquiry-Based Instruction in Science and Mathematics in Middle School Classrooms: Examining Its Association With Students’ Attitudes by Gender and Race/Ethnicity},
 url = {https://doi-org.crai.referencistas.com/10.1177/2332858419867653},
 volume = {5},
 year = {2019k}
}

@article{doi:10.1177/23328584221081256,
 abstract = {Computational thinking (CT) is a set of cognitive skills that every child should acquire. K–12 classrooms are expected to provide students opportunities (tasks) to think computationally. We introduce a CT competency assessment for middle school students. The assessment design process started by establishing a cognitive model of CT domain mastery, in which three broad skill types were identified to represent CT competency. After multiple-choice item prototypes were written, pilot tested, and revised, 15 of them were finally selected to be administered to 564 students in two middle schools in the Midwestern United States. Using a cognitive diagnostic scoring model, mastery classifications for each student were determined that can be used diagnostically by teachers as a pretest and, perhaps in the future, to compare the outcomes of CT instructional programs. The results inform an initial understanding of typical learning progressions in CT at the middle school level.},
 author = {Tingxuan Li and Anne Traynor},
 doi = {10.1177/23328584221081256},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/23328584221081256},
 journal = {AERA Open},
 number = { },
 pages = {23328584221081256},
 title = {The Use of Cognitive Diagnostic Modeling in the Assessment of Computational Thinking},
 url = {https://doi-org.crai.referencistas.com/10.1177/23328584221081256},
 volume = {8},
 year = {2022m}
}

@article{doi:10.1177/2336825X1902700303,
 abstract = {This article brings Alexander Wendt’s (2015) ‘quantum social ontology’ into the realm of empirical International Relations (IR) research by coupling it with Q methodology. It shows how Wendt’s ontology and Q methodology share a central interest in complex agency and are inherently allied in terms of principles and purposes. The quantum view has catalysed conversation within IR and social sciences more broadly, but that debate has remained almost exclusively on a theoretical level. This article shows that there is potential for empirical research in this area regardless of whether one considers the quantum view to be an analogy or ontological reality. Q methodology’s grounding ideas align with quantum physics and the quantum social ontology, e.g. in the fashion it conceives of subjective states of mind and their measurement. Practical examples of Q methodological work are presented to illustrate the quantum concepts in a social scientific setting. The article argues for a broader study of political subjectivity within IR through a notion of personhood, which opens up vast potentialities for agency as well as for breaking free of determinism, and fixed notions of human nature as well as ostensibly fixed understandings of advantaged or disadvantaged subject positions.},
 author = {Pinja Lehtonen},
 doi = {10.1177/2336825X1902700303},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2336825X1902700303},
 journal = {New Perspectives},
 number = {3},
 pages = {37–61},
 title = {How Quantum Ontology and Q Methodology Can Revitalise Agency in IR},
 url = {https://doi-org.crai.referencistas.com/10.1177/2336825X1902700303},
 volume = {27},
 year = {2019l}
}

@article{doi:10.1177/2347631119886418,
 abstract = {Things we can do because of learning are called outcomes of learning. Outcome based education (OBE) was propounded by William Spady in the 90s to bring the focus of formal education to what the students learn rather than what they were taught. OBE is a system of education giving priority to ends, purpose, accomplishments, and results. All decisions about the curriculum, assessment, and instruction are driven by the exit learning outcomes the students should display at the end of a program or a course. This paper presents a method of writing outcomes for General higher education programs. Outcomes for a higher education program are defined at three levels as program outcomes (POs), program specific outcomes (PSOs), and course outcomes (COs). The most important aspect of an outcome is that it should be observable and measurable. These are best written in a well-defined framework of taxonomy of learning. Bloom’s taxonomy of learning identifies three domains of learning: Cognitive, affective and psychomotor. Revised Bloom taxonomy of cognitive domain has two dimensions cognitive levels and knowledge categories. It is proposed that CO statements be written within a well-defined structure: Action, knowledge elements, conditions, and criteria. Tagging COs with POs, PSOs, cognitive levels and the number of classroom hours associated facilitates the computation of attainment of COs, POs, and PSOs.},
 author = {N. J. Rao},
 doi = {10.1177/2347631119886418},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2347631119886418},
 journal = {Higher Education for the Future},
 number = {1},
 pages = {5–21},
 title = {Outcome-based Education: An Outline},
 url = {https://doi-org.crai.referencistas.com/10.1177/2347631119886418},
 volume = {7},
 year = {2020p}
}

@article{doi:10.1177/2347631120970177,
 abstract = {In this article, Fishbone-based advanced computational thinking (FACT) pedagogy is proposed by fusing fishbone pedagogy and computational thinking pedagogy for enhancing teaching-learning process while teaching engineering and science courses, for engineering and science students respectively. The proposed FACT pedagogy has been implemented using the concept of X-ray machine in biomedical instrumentation course and biomolecules, in biochemistry course. Using fishbone approach, various components of X-ray machine in biomedical course and the components of biomolecules in biochemistry course are visually explained as ribs and riblets of a fishbone diagram, without coining the keywords X-ray and biomolecules in an engineering institution and science institution respectively. Finally, the targeted concept is arrived and explained. Similarly, the same concepts of X-ray and biomolecules are coined among students and they are asked to divide or decompose the concepts into sub-concepts separately. To implement and evaluate the proposed pedagogy, an engineering institution and a science institution have been selected and evaluation results have been published in this article. In this pedagogical approach, the same complex concept is taught as a backward thinking by the teacher using fishbone pedagogy and forward thinking by the students using computational thinking pedagogy. This combined approach helps students to understand any complex concept in science courses. Also, it helps the teachers to easily convey and embed the same among the student community while teaching science courses.},
 author = {B. Gopinath and R. Santhi},
 doi = {10.1177/2347631120970177},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2347631120970177},
 journal = {Higher Education for the Future},
 number = {1},
 pages = {108–122},
 title = {Development and Evaluation of Fishbone-Based Advanced Computational Thinking (FACT) Pedagogy: A Teacher-Student Collaborative Learning Environment in Engineering and Science Education},
 url = {https://doi-org.crai.referencistas.com/10.1177/2347631120970177},
 volume = {8},
 year = {2021e}
}

@article{doi:10.1177/23476311231183204,
 abstract = {Integration of computational data science (CDS) into the university curriculum offers several advantages for students, faculty and the institution. This article discusses the benefits to students of introducing CDS into the university curriculum with a focus on developing skills in cheminformatics, data analysis, structure–activity relationships, modelling and simulation. Moreover, CDS can enable students to engage with complex chemical and toxicological data in new and dynamic ways, helping them to develop a more nuanced understanding of the potential hazards and risks associated with different chemicals and substances. On the other hand, it can foster greater collaboration between students and faculty and with external partners in industry and government. This can lead to the development of more effective and efficient toxicological testing methods and tools to screen chemicals for potential hazards and aid the development of environmentally friendly chemicals. Overall, the integration of CDS into the university curriculum will help prepare the next generation of scientists giving them a competitive edge to make considerable contributions to green chemistry, designing safer chemicals and non-animal testing methods. It will enable them to tackle modern challenges facing society including identifying safer and more sustainable chemicals and predicting the health and environmental impacts of novel chemical substances.},
 author = {N. Renu and K. Sunil},
 doi = {10.1177/23476311231183204},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/23476311231183204},
 journal = {Higher Education for the Future},
 number = {2},
 pages = {183–195},
 title = {Integrating Computational Data Science in University Curriculum for the New Generation of Scientists},
 url = {https://doi-org.crai.referencistas.com/10.1177/23476311231183204},
 volume = {10},
 year = {2023q}
}

@article{doi:10.1177/23476311241285100,
 abstract = {Unlike other technologies that augment human physical skills and abilities, artificial intelligence (AI) technologies interact with human thinking skills nurtured through various educational processes. Hence, advances in these technologies challenge the education sector to reimagine the suitable intellectual formation of students in the AI age. It is observed that as soon as AI tools acquire certain cognitive skills, humans tend to delegate those skills to such tools, risking redundancy in their personal and professional lives. Therefore, anticipating an ever-increasing involvement of AI tools in various cognitive processes, educational systems must equip students with both AI-collaborative and AI-complementary thinking skills, making them resilient to the advancements of AI technologies. Furthermore, it demands that educators continually redefine the spectrum of thinking skills required for and imparted to students in the AI age. This article aims to develop a comprehensive taxonomy of educational objectives for AI-natives by reimagining thinking skills relevant in the AI age. It is intended to help educational policymakers worldwide reimagine the appropriate set of thinking skills relevant for students in the AI age. Additionally, the article provides important insights to restructure contemporary educational practices to meet the requirements of the holistic formation of the AI-natives.},
 author = {Varghese Panthalookaran},
 doi = {10.1177/23476311241285100},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/23476311241285100},
 journal = {Higher Education for the Future},
 number = {0},
 pages = {23476311241285100},
 title = {A Revised Taxonomy of Educational Objectives for  AI-Natives: Reimagining Thinking Skills in the Age of AI},
 url = {https://doi-org.crai.referencistas.com/10.1177/23476311241285100},
 volume = {0},
 year = {2024j}
}

@article{doi:10.1177/2372732218790017,
 abstract = {The Next Generation Science Standards (NGSS) for K-12 science education has outlined new standards that integrate science inquiry practices with scientific concepts and ideas. The challenge with implementing this framework has been determining how to provide students with authentic scientific experiences and real-time individualized scaffolding during inquiry, as well as reliably and validly assess students’ inquiry competencies. This article reviews current computer-based educational technologies, namely, educational data mining and natural language processing, and describes how these technologies have been used to automatically assess science inquiry practices aligned with NGSS practices. The second section describes the implementation of real-time adaptive, individualized scaffolds and instruction, based on automated inquiry assessment techniques. Finally, we aim to direct the attention of policy makers toward the use of technology to promote significant progress of nationwide inquiry-based learning, teaching, and assessment.},
 author = {Haiying Li and Janice Gobert and Art Graesser and Rachel Dickler},
 doi = {10.1177/2372732218790017},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2372732218790017},
 journal = {Policy Insights from the Behavioral and Brain Sciences},
 number = {2},
 pages = {171–178},
 title = {Advanced Educational Technology for Science Inquiry Assessment},
 url = {https://doi-org.crai.referencistas.com/10.1177/2372732218790017},
 volume = {5},
 year = {2018i}
}

@article{doi:10.1177/2378023119849803,
 abstract = {Reproducibility is fundamental to science, and an important component of reproducibility is computational reproducibility: the ability of a researcher to recreate the results of a published study using the original author’s raw data and code. Although most people agree that computational reproducibility is important, it is still difficult to achieve in practice. In this article, the authors describe their approach to enabling computational reproducibility for the 12 articles in this special issue of Socius about the Fragile Families Challenge. The approach draws on two tools commonly used by professional software engineers but not widely used by academic researchers: software containers (e.g., Docker) and cloud computing (e.g., Amazon Web Services). These tools made it possible to standardize the computing environment around each submission, which will ease computational reproducibility both today and in the future. Drawing on their successes and struggles, the authors conclude with recommendations to researchers and journals.},
 author = {David M. Liu and Matthew J. Salganik},
 doi = {10.1177/2378023119849803},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2378023119849803},
 journal = {Socius},
 number = { },
 pages = {2378023119849803},
 title = {Successes and Struggles with Computational Reproducibility: Lessons from the Fragile Families Challenge},
 url = {https://doi-org.crai.referencistas.com/10.1177/2378023119849803},
 volume = {5},
 year = {2019j}
}

@article{doi:10.1177/23813377231182093,
 abstract = {Although participatory culture promotes productive engagement in online spaces, members may respond in ways that constrain digital composition practices. This article provides theoretical and contextual background regarding a shift toward digital media production and programming-as-writing for youth. In addition, I describe the intersecting of computational thinking, computational participation, and participatory culture within Scratch, an online programming community. Findings from a descriptive case study focused on early adolescents as they engaged in digital media composition within an online programming community called Scratch are examined. Specifically, I discuss the contrast in the digital composition experiences of two participants as they leveraged participatory culture in the creation of digital media. This research highlights assumptions made regarding participatory cultures and the need to consider how to foster productive and sustainable digital literacy practices for youth in participatory cultures.},
 author = {Julia Hagge},
 doi = {10.1177/23813377231182093},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/23813377231182093},
 journal = {Literacy Research: Theory, Method, and Practice},
 number = {1},
 pages = {200–217},
 title = {Productivity Versus Sustainability: A Tale of Two Authors’ Engagement in Participatory Culture},
 url = {https://doi-org.crai.referencistas.com/10.1177/23813377231182093},
 volume = {72},
 year = {2023j}
}

@article{doi:10.1177/23977914241259338,
 abstract = {An electrically conducting and magnetically influenced three-dimensional flow of modified nanofluid between two rotating disks is investigated in this study. Nanometer-size particles of two distinct materials (Al2O3, Ag) suspended in water in the hybrid nanofluid are considered. The Joule heating effects, mixed convection, chemical factor, and exponential heating are considered in this study. The physical problem under these assumptions is transformed into a system of equations. To convert partial differential equations (PDEs) into systems of ordinary differential equations (ODEs), the suitable similarity variables are introduced. The reduced system is numerically solved using bvp4c, a well-known higher-order algorithm. The effects of significant variables on the profiles of velocity, temperature, and concentration are depicted graphically. We have chosen the pertinent parameters in the specific intervals; , and . The interactions between a number of significant factors, including skin friction and Nusselt and Sherwood numbers at the higher and lower disks, are shown in tables. The results demonstrate that the local skin fraction decreases as the mixed convection factor is increased, which physically increases the heat transmission rate between the two discs. Furthermore, as the magnetic field and radiation number rise, the rate of heat transfer at the lower and top discs decreases. The numerical results in the form of a table are compared with the available literature, and our results beat the previously published work. The results described here are desirable choices for thermal uses such as automotive cooling systems, solar thermal systems, engineering, medical areas, heat sinks, or current energy storage since they have demonstrated higher thermal characteristics and stability than simple nanofluids (NFs). In Table 6, a comparative numerical study is presented for the various of state variables with the available literature, where the present study is validated.},
 author = {Humaira Gul and Muhammad Ayaz and Amjid Rashid and Asad Ullah and Talib K. Ibrahim and Emad AA. Ismail and Fuad A Awwad},
 doi = {10.1177/23977914241259338},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/23977914241259338},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part N: Journal of Nanomaterials, Nanoengineering and Nanosystems},
 number = {0},
 pages = {23977914241259336},
 title = {Chemically reactive and mixed convective hybrid nanofluid flow between two parallel rotating disks with Joule heating: A thermal computational study},
 url = {https://doi-org.crai.referencistas.com/10.1177/23977914241259338},
 volume = {0},
 year = {2024d}
}

@article{doi:10.1177/2398212818810591,
 abstract = {Metacognition supports reflection upon and control of other cognitive processes. Despite metacognition occupying a central role in human psychology, its neural substrates remain underdetermined, partly due to study-specific differences in task domain and type of metacognitive judgement under study. It is also unclear how metacognition relates to other apparently similar abilities that depend on recursive thought such as theory of mind or mentalising. Now that neuroimaging studies of metacognition are more prevalent, we have an opportunity to characterise consistencies in neural substrates identified across different analysis types and domains. Here we used quantitative activation likelihood estimation methods to synthesise findings from 47 neuroimaging studies on metacognition, divided into categories based on the target of metacognitive evaluation (memory and decision-making), analysis type (judgement-related activation, confidence-related activation, and predictors of metacognitive sensitivity), and, for metamemory judgements, temporal focus (prospective and retrospective). A domain-general network, including medial and lateral prefrontal cortex, precuneus, and insula was associated with the level of confidence in self-performance in both decision-making and memory tasks. We found preferential engagement of right anterior dorsolateral prefrontal cortex in metadecision experiments and bilateral parahippocampal cortex in metamemory experiments. Results on metacognitive sensitivity were inconclusive, likely due to fewer studies reporting this contrast. Finally, by comparing our results to meta-analyses of mentalising, we obtain evidence for common engagement of the ventromedial and anterior dorsomedial prefrontal cortex in both metacognition and mentalising, suggesting that these regions may support second-order representations for thinking about the thoughts of oneself and others.},
 author = {Anthony G. Vaccaro and Stephen M. Fleming},
 doi = {10.1177/2398212818810591},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2398212818810591},
 journal = {Brain and Neuroscience Advances},
 note = {PMID:30542659},
 number = { },
 pages = {2398212818810591},
 title = {Thinking about thinking: A coordinate-based meta-analysis of neuroimaging studies of metacognitive judgements},
 url = {https://doi-org.crai.referencistas.com/10.1177/2398212818810591},
 volume = {2},
 year = {2018p}
}

@article{doi:10.1177/2399808319885210,
 doi = {10.1177/2399808319885210},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2399808319885210},
 journal = {Environment and Planning B: Urban Analytics and City Science},
 number = {9},
 pages = {1603–1604},
 title = {Winners of the Breheny Prize},
 url = {https://doi-org.crai.referencistas.com/10.1177/2399808319885210},
 volume = {46},
 year = {2019y}
}

@article{doi:10.1177/23998083221100550,
 abstract = {The current urban design computation is mostly centered on the professional designer while ignoring the plural dimension of urban design. In addition, available public participation computational tools focus mainly on information and idea sharing, leaving the public excluded in design generation because of their lack of design expertise. To address such an issue, this study develops Urban-GAN, a plural urban design computation system, to provide new technical support for design empowerment, allowing the public to generate their own designs. The sub-symbolic representation and artificial intelligence techniques of deep convolutional neural networks, case-based reasoning, and generative adversarial networks are used to acquire and embody design knowledge as the density function, and generate design schemes with this knowledge. The system consists of an urban form database and five process models through which the user with little design expertise can select urban form cases, generate designs similar to those cases, and make design decisions. The Urban-GAN is applied to hypothetical design experiments, which show that the user is able to apply the system to successfully generate distinctive designs following the urban form “styles” in Manhattan, Portland, and Shanghai. This study further extends the discussion about the plural urban design computation to general reflections on the goals and values in AI technique application in planning and design.},
 author = {Steven Jige Quan},
 doi = {10.1177/23998083221100550},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/23998083221100550},
 journal = {Environment and Planning B: Urban Analytics and City Science},
 number = {9},
 pages = {2500–2515},
 title = {Urban-GAN: An artificial intelligence-aided computation system for plural urban design},
 url = {https://doi-org.crai.referencistas.com/10.1177/23998083221100550},
 volume = {49},
 year = {2022n}
}

@article{doi:10.1177/2472555216682725,
 abstract = {Heterogeneity is a fundamental property of biological systems at all scales that must be addressed in a wide range of biomedical applications, including basic biomedical research, drug discovery, diagnostics, and the implementation of precision medicine. There are a number of published approaches to characterizing heterogeneity in cells in vitro and in tissue sections. However, there are no generally accepted approaches for the detection and quantitation of heterogeneity that can be applied in a relatively high-throughput workflow. This review and perspective emphasizes the experimental methods that capture multiplexed cell-level data, as well as the need for standard metrics of the spatial, temporal, and population components of heterogeneity. A recommendation is made for the adoption of a set of three heterogeneity indices that can be implemented in any high-throughput workflow to optimize the decision-making process. In addition, a pairwise mutual information method is suggested as an approach to characterizing the spatial features of heterogeneity, especially in tissue-based imaging. Furthermore, metrics for temporal heterogeneity are in the early stages of development. Example studies indicate that the analysis of functional phenotypic heterogeneity can be exploited to guide decisions in the interpretation of biomedical experiments, drug discovery, diagnostics, and the design of optimal therapeutic strategies for individual patients.},
 author = {Albert Gough and Andrew M. Stern and John Maier and Timothy Lezon and Tong-Ying Shun and Chakra Chennubhotla and Mark E. Schurdak and Steven A. Haney and D. Lansing Taylor},
 doi = {10.1177/2472555216682725},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2472555216682725},
 journal = {SLAS DISCOVERY: Advancing the Science of Drug Discovery},
 note = {PMID:28231035},
 number = {3},
 pages = {213–237},
 title = {Biologically Relevant Heterogeneity: Metrics and Practical Insights},
 url = {https://doi-org.crai.referencistas.com/10.1177/2472555216682725},
 volume = {22},
 year = {2017e}
}

@article{doi:10.1177/2472555220921132,
 abstract = {Genome-wide arrayed CRISPR screening is a powerful method for drug target identification as it enables exploration of the effect of individual gene perturbations using diverse highly multiplexed functional and phenotypic assays. Using high-content imaging, we can measure changes in biomarker expression, intracellular localization, and cell morphology. Here we present the computational pipeline we have developed to support the analysis and interpretation of arrayed CRISPR screens. This includes evaluating the quality of guide RNA libraries, performing image analysis, evaluating assay results quality, data processing, hit identification, ranking, visualization, and biological interpretation.},
 author = {Maria Luisa Guerriero and Adam Corrigan and Aurélie Bornot and Mike Firth and Patrick O’Shea and Douglas Ross-Thriepland and Samantha Peel},
 doi = {10.1177/2472555220921132},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2472555220921132},
 journal = {SLAS DISCOVERY: Advancing the Science of Drug Discovery},
 note = {PMID:32394775},
 number = {6},
 pages = {646–654},
 title = {Delivering Robust Candidates to the Drug Pipeline through Computational Analysis of Arrayed CRISPR Screens},
 url = {https://doi-org.crai.referencistas.com/10.1177/2472555220921132},
 volume = {25},
 year = {2020h}
}

@article{doi:10.1177/2514848619867608,
 abstract = {Over the past three years, the dams of Chelan County, Washington, its watershed and fish, the electrical grid and the laborers who maintain it, and cleared land with warehouses filled with computers, have all been enrolled as part of the decentralized digital infrastructure of Bitcoin. While popular accounts of the Bitcoin network correctly report the massive scale of energy it consumes and its potential environmental ramifications, in practice, the material geographies of Bitcoin are highly uneven and intertwined with specific infrastructural, ecological, and economic systems. In this article, we examine Bitcoin’s impacts on Chelan County, untangling the processes that occur as the distributed, digital infrastructure consumes the very real material resources of one place to produce digital goods used in another. In so doing, we examine not only the material costs of networks like Bitcoin, but also their historical ties to older processes of accumulation.},
 author = {Nick Lally and Kelly Kay and Jim Thatcher},
 doi = {10.1177/2514848619867608},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2514848619867608},
 journal = {Environment and Planning E: Nature and Space},
 number = {1},
 pages = {18–38},
 title = {Computational parasites and hydropower: A political ecology of Bitcoin mining on the Columbia River},
 url = {https://doi-org.crai.referencistas.com/10.1177/2514848619867608},
 volume = {5},
 year = {2022j}
}

@article{doi:10.1177/25152459241236149,
 abstract = {Recent developments in the causal-inference literature have renewed psychologists’ interest in how to improve causal conclusions based on observational data. A lot of the recent writing has focused on concerns of causal identification (under which conditions is it, in principle, possible to recover causal effects?); in this primer, we turn to causal estimation (how do researchers actually turn the data into an effect estimate?) and modern approaches to it that are commonly used in epidemiology. First, we explain how causal estimands can be defined rigorously with the help of the potential-outcomes framework, and we highlight four crucial assumptions necessary for causal inference to succeed (exchangeability, positivity, consistency, and noninterference). Next, we present three types of approaches to causal estimation and compare their strengths and weaknesses: propensity-score methods (in which the independent variable is modeled as a function of controls), g-computation methods (in which the dependent variable is modeled as a function of both controls and the independent variable), and doubly robust estimators (which combine models for both independent and dependent variables). A companion R Notebook is available at github.com/ArthurChatton/CausalCookbook. We hope that this nontechnical introduction not only helps psychologists and other social scientists expand their causal toolbox but also facilitates communication across disciplinary boundaries when it comes to causal inference, a research goal common to all fields of research.},
 author = {Arthur Chatton and Julia M. Rohrer},
 doi = {10.1177/25152459241236149},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/25152459241236149},
 journal = {Advances in Methods and Practices in Psychological Science},
 number = {1},
 pages = {25152459241236148},
 title = {The Causal Cookbook: Recipes for Propensity Scores, G-Computation, and Doubly Robust Standardization},
 url = {https://doi-org.crai.referencistas.com/10.1177/25152459241236149},
 volume = {7},
 year = {2024d}
}

@article{doi:10.1177/2515690X221082989,
 abstract = {Stroke-related numbness and weakness (SRNW) are resultant symptoms of post-stroke sufferers. Existing research has supported the use of Huangqi Guizhi Wuwu Tang (HGWT) particularly for SRNW; however, their mechanisms of action have not been fully elucidated. Therefore, this study aimed to investigate the mechanisms of action of HGWT components targeting SRNW-related proteins through a computational molecular docking approach. Target proteins associated with SRNW were identified through DrugBank database and Open Targets database. Chemical compounds from each herb of HGWT were identified from the Traditional Chinese Medicine Systems Pharmacology and Analysis Platform (TCMSP). Autodock Vina was utilized and the cut-off criterion applied for protein-ligand complexes was a binding affinity score of ≤ -9.5 kcal/mol; selected protein-ligand complexes were identified using 3D and 2D structural analyses. The protein targets PDE5A and ESR1 have highlighted interactions with compounds (BS040, DZ006, DZ058, DZ118, and HQ066) which are the key molecules in the management of SRNW. PDE5A have bioactivity with the amino acid residues (Val230, Asn252, Gln133 and Thr166) throughout PDE5A-cGMP-PKG pathways which involved reduction in myofilament responsiveness. ESR1 were predicted to be critical active with site residue (Leu346, Glu419 and Leu387) and its proteoglycans pathway involving CD44v3/CD44 that activates rho-associated protein kinase 1 (ROCK1) and ankyrin increasing vascular smooth muscle. In conclusion, HGWT may provide therapeutic benefits through strong interactions between herbal compounds and target proteins of PDE5A and ESR1. Further experimental studies are needed to unequivocally support this result which can be valuable to increase the quality of life of post-stroke patients. Keywords Herbal medicine, Complementary and alternative medicine, Natural product, Post-stroke, Computational analysis},
 author = {Sanghyun Lee and Andrew Hung and Hong Li and Angela Wei Hong Yang},
 doi = {10.1177/2515690X221082989},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/2515690X221082989},
 journal = {Journal of Evidence-Based Integrative Medicine},
 note = {PMID:35369720},
 number = { },
 pages = {2515690X221082989},
 title = {Mechanisms of Action of a Herbal Formula Huangqi Guizhi Wuwu Tang for the Management of Post-Stroke Related Numbness and Weakness: A Computational Molecular Docking Study},
 url = {https://doi-org.crai.referencistas.com/10.1177/2515690X221082989},
 volume = {27},
 year = {2022l}
}

@article{doi:10.1177/26349825231200607,
 abstract = {The quality of intradisciplinary thinking during research design is crucial not only for alleviating multifaceted problems – such as global environmental change – but potentially to avoid worsening them. This article builds on the emphasis placed on research framing by human, physical and critical physical geographers by drawing from ideas on iterative framing in transdisciplinarity studies – as related to collaboration between geography’s internal subdisciplines – as well as from Gilles Deleuze, especially in Difference and Repetition (Deleuze, 1994 (1968)). Here, research reframing is based on a critical engagement with the contrasts in subdisciplinary foci and research methodologies, which I argue is achieved through a consideration of contrasting axiologies and facilitated by subdisciplinary encounter. I explore this approach to intradisciplinary thinking using examples from the literature in geography and beyond. Given that collaboration across research methodologies is challenging, I demonstrate one example practical approach to such intradisciplinary thinking: mapping. Different map-making and map-using activities can reflect contrasting forms of data and understandings from the field, and can therefore act as a shared space of encounter for geographers of all kinds.},
 author = {Heather J Miles},
 doi = {10.1177/26349825231200607},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/26349825231200607},
 journal = {Environment and Planning F},
 number = {4},
 pages = {495–514},
 title = {Practising difference across geography: A transdisciplinary and Deleuzian approach to intradisciplinary thinking},
 url = {https://doi-org.crai.referencistas.com/10.1177/26349825231200607},
 volume = {2},
 year = {2023m}
}

@article{doi:10.1177/27325016221138749,
 abstract = {Background: Midvault reconstruction is an essential element of functional rhinoplasty. An improved understanding of airflow patterns after spreader graft (SG) or spreader flap (SF) techniques can inform surgical techniques based on individual anatomy. Objectives: The objective of this study was to compare the physiologic changes related to nasal function after midvault reconstruction with SF and SG. Methods: Soft tissue elevation (STE), SG, and SF were performed in sequence on 5 cadaveric specimens. Computational modeling was used to simulate airflow, heat transfer, and humidity in three-dimensional nasal airway reconstructions of each specimen. Results: Median bilateral airflow-rates (L/min) were similar for STE (29.4), SF (27.6), and SG (28.9), and were not statistically significant (STE vs SF: P = 1.0, power = 5%; STE vs SG: P = .31, power = 16%; SF vs SG: P = .42, power = 14%). Both SF and SG had increased unilateral airflow volume (L/min) through the more obstructed nasal passage (median: STE 10.3, SF 12.2, SG 12.7), but these differences were not significant (STE vs SF: P = .19, power = 24%; STE vs SG P = .19, power = 30%). Furthermore, SF and SG had decreased unilateral nasal resistance (Pa s/mL) on the more obstructed side (median: STE 0.085, SF 0.072, SG 0.062) (STE vs SF: P = .13, power = 23%; STE vs SG P = .13, power = 24%). For all 3 models, heat flux distribution was greater in the anterior portion of the nasal passage than the posterior portions. Conclusions: Differences in nasal airflow and resistance after SF and SG were not statistically significant, but both procedures resulted in higher airflow rates and decreased nasal resistance through the more obstructed nasal passage.},
 author = {Yash J. Avashia and Hannah L. Martin and Dennis O. Frank-Ito and Katrina Z. Hodges and Rose T. Trotta and Hang Li and Carolyn Lowry and Charles R. Woodard and Alexander C. Allori and Jeffrey R. Marcus},
 doi = {10.1177/27325016221138749},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/27325016221138749},
 journal = {FACE},
 number = {1},
 pages = {22–32},
 title = {Computational Analyses of Physiologic Effects After Midvault Repair Techniques in Rhinoplasty},
 url = {https://doi-org.crai.referencistas.com/10.1177/27325016221138749},
 volume = {4},
 year = {2023b}
}

@article{doi:10.1177/27523543241288818,
 abstract = {The integration of artificial intelligence (AI) in journalism has sparked complex ethical debates, particularly with the rise of generative AI systems. By now, AI permeates the entire news cycle, from information gathering to news dissemination, raising questions revolving around issues such as transparency, accountability, responsibility, bias, and diversity. Previous research showed that news organizations have slowly approached and adapted to ethical concerns regarding the use of AI, developing critical stances mainly due to rising AI power, growing audience skepticism, and mounting tensions within the industry between news publishers’ strategies and journalists’ anxieties. Consequently, ethical guidelines have started to emerge in news organizations, but their practical application remains challenging and under-studied, not only due to the opacity of AI algorithms, but also due to the difficulties of “embedding” journalistic values into AI systems. In the light of an intensifying discourse about ethical concerns in the news industry and growing efforts by governments and institutions such as the European Union to strengthen AI governance, journalism studies have started to explore the issue as well. However, research on AI ethics is still in its infancy, with significant gaps in understanding the practical enforcement of ethical guidelines within newsrooms, in particular when it comes to the design of AI systems. This essay critically discusses the way journalism (studies) approach ethical issues related to the use and the design of AI systems, given that the responsible use and design of AI systems in journalism is crucial given its integral role for democracy and society.},
 author = {Colin Porlezza and Aljosha Karim Schapals},
 doi = {10.1177/27523543241288818},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/27523543241288818},
 journal = {Emerging Media},
 number = {0},
 pages = {27523543241288816},
 title = {AI Ethics in Journalism (Studies): An Evolving Field Between Research and Practice},
 url = {https://doi-org.crai.referencistas.com/10.1177/27523543241288818},
 volume = {0},
 year = {2024p}
}

@article{doi:10.1177/27527263231181963,
 abstract = {We presented a design-based study within the context of a four-session Scratch programming activity among 23 fourth-grade students in Hong Kong. Inspired by the computational thinking (CT) strategies and the 5E instructional model, we investigated students’ mathematical learning of fractions in a Scratch (block-based programming) environment. Students developed CT concepts, practices, and perspectives by building a “fraction magic calculator” in groups. This study analyzed the lesson design, students’ drawings, interviews, and work expressing their mathematical understanding of fractions in Scratch applications. The learning tasks were designed to support the students’ fraction learning and utilized computational abstractions in the form of variables, functions, and iterations to formulate mathematical models in a programming context. Students’ artifacts and feedback showed they were interested in learning fractions in a programming learning context, contributing to exercising and improving their fraction concepts and CT. Ultimately, we emphasized the benefits of CT integrated into mathematics education, promoting students’ understanding of fraction concepts, a set of CT abilities (concepts, practices, perspectives), and learning motivation. Moreover, we suggested a set of non-cognitive skills (e.g., socializing, expressing) to enrich the CT perspectives in the framework and show the importance of developing coding communities to co-create digital artifacts among learners. Overall, we highlighted that mathematics teachers should apply and create learning tasks that promote computational thinking to forge mathematical ideas and thinking.},
 author = {Xiaoxuan Fang and Davy Tsz Kit Ng and Wing Tung Tam and Manwai Yuen},
 doi = {10.1177/27527263231181963},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/27527263231181963},
 journal = {Asian Journal for Mathematics Education},
 number = {2},
 pages = {220–239},
 title = {Integrating computational thinking into primary mathematics: A case study of fraction lessons with Scratch programming activities},
 url = {https://doi-org.crai.referencistas.com/10.1177/27527263231181963},
 volume = {2},
 year = {2023i}
}

@article{doi:10.1177/27527263231188638,
 abstract = {During the COVID-19 pandemic, researchers faced challenges conducting data collection. However, emerging communication technologies, such as web-conferencing platforms, provided an alternative mode of data collection. Among these platforms, Zoom stands out for its functional affordances and unique features that support data collection methods, including screen-capturing, digital photography, drawing, polling, and artifact-based interviews, which attempt to reveal a child’s viewpoint and ability. In this article, we explore the feasibility of using Zoom to conduct educational research by investigating the mathematical learning experience among 26 primary 1 and 2 students in Hong Kong on two topics—understanding time and addition and subtraction. Five mathematics teachers were recruited to participate in 2-month online workshops with assessments among three groups of students to understand the suitability for data collection. Based on teachers’ observations and students’ responses, our findings suggest that Zoom is a viable and effective tool for qualitative data collection. It offers a cost-effective method for researchers and educators to examine students’ mathematical performance in an online environment. We also investigate students’ perceptions of online assessment strategies, perceived usefulness of various Zoom functions, and explore the benefits and challenges teachers and students perceived throughout mathematical education using Zoom. However, several technical difficulties were encountered, such as the digital literacy gap and privacy concerns. Therefore, improving students’ digital literacy for remote learning and triangulation is important for online mathematical data collection and assessments.},
 author = {Davy Tsz Kit Ng and Xiaoxuan Fang},
 doi = {10.1177/27527263231188638},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/27527263231188638},
 journal = {Asian Journal for Mathematics Education},
 number = {3},
 pages = {274–298},
 title = {How to use Zoom to collect data in mathematics educational research: A case study in assessing students’ online mathematics learning},
 url = {https://doi-org.crai.referencistas.com/10.1177/27527263231188638},
 volume = {2},
 year = {2023n}
}

@article{doi:10.1177/27527263231217826,
 abstract = {Problem posing has long been recognized as a critically important teaching method and goal in the area of mathematics education. However, few studies have used problem posing to assess in-service teachers’ mathematical understanding. The present study investigated in-service teachers’ mathematical understanding of fraction division, which is often considered challenging content in elementary school, from three angles: computation, drawing, and problem posing. Two studies involving 66 and 193 primary and middle school teachers were conducted to reveal the in-service teachers’ mathematical understanding and whether drawing and problem posing affected each other. Although the in-service teachers rarely had the opportunity to pose mathematical problems in their daily teaching, they were able to pose mathematical problems in this study. In addition, problem-posing tasks were more useful in diagnosing the in-service teachers’ conceptual understanding than were computation or drawing. Thus, problem posing seems to have contributed to their conceptual understanding of fraction division on the drawing task.},
 author = {Yiling Yao and Suijun Jia and Jinfa Cai},
 doi = {10.1177/27527263231217826},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/27527263231217826},
 journal = {Asian Journal for Mathematics Education},
 number = {4},
 pages = {413–429},
 title = {Beyond computation: Assessing in-service mathematics teachers’ conceptual understanding of fraction division through problem posing},
 url = {https://doi-org.crai.referencistas.com/10.1177/27527263231217826},
 volume = {2},
 year = {2023t}
}

@article{doi:10.1177/27538699221128218,
 abstract = {Design research has much to contribute to and much to gain from the emerging field of possibility studies. In this short essay, I discuss these opportunities with respect to four topics: (1) processes of mediation and representation, (2) systems perspectives on creative work, (3) methodological options for investigation, and (4) educational challenges that should be addressed. Considering design research’s contributions to each of these topics raises interesting questions that possibility studies might address as it develops. Conversely, possibility studies is already raising issues that design research should also attend to.},
 author = {Nathan Crilly},
 doi = {10.1177/27538699221128218},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/27538699221128218},
 journal = {Possibility Studies & Society},
 number = {1–2},
 pages = {46–50},
 title = {Design research and the study of the possible},
 url = {https://doi-org.crai.referencistas.com/10.1177/27538699221128218},
 volume = {1},
 year = {2023c}
}

@article{doi:10.1177/27546330241245290,
 abstract = {Purpose A pilot study evaluated the feasibility of a curriculum that overtly teaches computer programming while covertly scaffolding social-communication skills for autistic children aged 8–12 years. Methods Participants were taught the Python programming language so they could program their own chatbots to greet a human user and discuss different topics, taking turns during the discussion, as though the chatbot were a human itself. The students were challenged with creating chatbots that pass the ‘Turing Test’, where a human evaluator would not be able to tell whether their chatbots were humans or computer programs. The curriculum included didactic instruction, peer-group discussion, homework and the chatbot project. Six autistic children participated in the six-session program. Feasibility was assessed using questionnaires and qualitative feedback. Results The curriculum is deemed feasible and desirable. There was no measurable change in social-communication skills immediately following the six-session program. Participants and their parents were highly interested in similar programs in the future, suggesting promising potential for further development and refinement. Conclusion A curriculum of programming a chatbot that also covertly scaffolds social communication is feasible for autistic children who are interested in computer programming.},
 author = {Sarah Halabieh and Meng-Chuan Lai and Hsiang-Yuan Lin and L. H. Shu},
 doi = {10.1177/27546330241245290},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/27546330241245290},
 journal = {Neurodiversity},
 number = { },
 pages = {27546330241245290},
 title = {Computer programming a chatbot to improve social-communication skills in autistic children: A feasibility study},
 url = {https://doi-org.crai.referencistas.com/10.1177/27546330241245290},
 volume = {2},
 year = {2024f}
}

@article{doi:10.1177/8756087913484920,
 abstract = {A computational model to design plastic food packaging is proposed. The model minimizes the cost of the multi-layer structure satisfying the specific product requirements, using a heuristic optimization algorithm. The product requirements are defined by the expected shelf life, the storage conditions, the water sorption isotherms of foods and the maximum allowable gain or loss of gases (O2, CO2, N2, etc.) and moisture for the packaged food. In order to assure the food shelf life, these product requirements should be fulfilled to estimate the maximum permeance values of the plastic package. The computational algorithm automatically generates different multi-layer film structures that satisfy the product requirements. This algorithm combines different polymeric materials taking into account the barrier properties and cost of each layer, the compatibility between layers, the maximum number of layers and the minimum and maximum film thickness for each layer. Temperature and relative humidity corrections for the permeance calculations are considered. Permeance calculations of several barrier films are compared with oxygen transmission rate (OTR) and water vapor transmission rate (WVTR) measurements. The optimization model algorithm is evaluated by means of standard numerical routines and numerical benchmarking.},
 author = {María del Pilar Noriega and Omar Estrada and Iván López},
 doi = {10.1177/8756087913484920},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/8756087913484920},
 journal = {Journal of Plastic Film & Sheeting},
 number = {1},
 pages = {48–76},
 title = {Computational model to design plastic multi-layer films for food packaging to assure a shelf life at the best cost},
 url = {https://doi-org.crai.referencistas.com/10.1177/8756087913484920},
 volume = {30},
 year = {2014n}
}

@article{doi:10.1177/875697280703800102,
 abstract = {Building upon prior research on enterprise centralization and knowledge dynamics, this paper uses computational methods to assess the behavior and project performance of different organization-al designs in varying environments. The results reinforce contingency theory and suggest particular characteristics of different project environments that make one form relatively more or less appropriate than another. Practically, the answers to the research questions have direct and immediate application to project/portfolio managers and senior executives. Theoretically, broad classes of organizations are generalized and prescribe a novel set of organizational design guides.},
 author = {John Dillard and Mark E. Nissen},
 doi = {10.1177/875697280703800102},
 eprint = {https://doi-org.crai.referencistas.com/10.1177/875697280703800102},
 journal = {Project Management Journal},
 number = {1},
 pages = {5–20},
 title = {Computational Modeling of Project Organizations under Stress},
 url = {https://doi-org.crai.referencistas.com/10.1177/875697280703800102},
 volume = {38},
 year = {2007h}
}

@article{doi:10.1179/030192309X12492910938050,
 abstract = {The selection of coals for pulverised coal injection usually consists of evaluating the carbonaceous matter. However, the reduction of permeability in the lower section of the blast furnace with high rates of pulverised coal injection can be associated with remaining ashes from the coal combustion process. The aim of this work is to evaluate the behaviour of coal ashes at high temperatures in relation to their chemical and mineralogical composition. These ashes were submitted to the following analysis: chemical (X-ray fluorescence), mineralogical (X-ray diffraction), fusibility (heating microscopy) and viscosity (rotational viscometer). The software FactSage was also used to evaluate the behaviour of coal ashes. It was observed that samples present different chemical and mineralogical compositions, reflecting in the fusibility and viscosity of ashes. Their proportions and relevant phases were determined by computational thermodynamics and also related to the experimental work.},
 author = {M. C. Bagatini and J. L. Klug and N. C. Heck and E. Osório and A. C. F. Vilela and R. da Cruz},
 doi = {10.1179/030192309X12492910938050},
 eprint = {https://doi-org.crai.referencistas.com/10.1179/030192309X12492910938050},
 journal = {Ironmaking & Steelmaking},
 number = {8},
 pages = {583–589},
 title = {Behaviour of coal ashes for pulverised coal injection at high temperatures in relation to their chemical and mineralogical composition – experimental and computational analysis},
 url = {https://doi-org.crai.referencistas.com/10.1179/030192309X12492910938050},
 volume = {36},
 year = {2009a}
}

@article{doi:10.1179/030192310X12731438631804,
 abstract = {All operations in process metallurgy involve complex phenomena comprising momentum, heat, and/or mass transport; iron- and steelmaking is not an exception. Transport phenomena, i.e. fluid flows, heat transfer and mass transfer, play a dominant role in process metallurgy since their respective laws govern the kinetics of the various physical phenomena occurring in ironmaking and in steelmaking. These phenomena include such events as three-phase reactions, entrainment of slag and gas in liquid steel, vacuum degassing, alloy melting and mixing, the movements and flotation of inclusions, melt temperature losses, residence times in a metallurgical reactor, erosion of refractory linings, etc. In all these aspects, the evolution in our techniques and abilities to model single and multiphase flows and their attendant heat and mass transfer processes has contributed significantly to our understanding and effectively operating these processes, to designing improvements, and to developing new processes. To be ignorant of these matters can doom a processing operation to the scrap heap of metallurgical failures. Computational fluid dynamics (CFD) and computational heat and mass transfer has been a very effective tool over the last three decades, for modelling iron- and steelmaking processes, starting from the blast furnace up to continuous casting and beyond. With the advent of commercial CFD packages and ever increasing computational power through parallel processing, CFD has now become the dominant approach for predicting various aspects in iron- and steelmaking processes. In Part 1 of this review paper, the applications of CFD in ironmaking processes are thoroughly reviewed, discussed and critiqued. In Part 2, fluid flows and CFD in steelmaking and steel casting processes are similarly reviewed and critiqued.},
 author = {K. Chattopadhyay and M. Isac and R. I. L. Guthrie},
 doi = {10.1179/030192310X12731438631804},
 eprint = {https://doi-org.crai.referencistas.com/10.1179/030192310X12731438631804},
 journal = {Ironmaking & Steelmaking},
 number = {8},
 pages = {554–561},
 title = {Applications of Computational Fluid Dynamics (CFD) in iron- and steelmaking: Part 1},
 url = {https://doi-org.crai.referencistas.com/10.1179/030192310X12731438631804},
 volume = {37},
 year = {2010d}
}

@article{doi:10.1179/030801803225010340,
 abstract = {Much debate has taken place on Joseph Needham’s question regarding ‘the failure of China and India to give rise to distinctively modern science while being ahead of Europe for fourteen previous centuries’. It is argued in this paper that while there is probably some truth in many of the sociocultural explanations that have been offered for the failure in India, they are in the final analysis not entirely convincing. The proposal in this paper is in two parts. The first is that the scientific revolution, which was part of a European miracle, was triggered in part by the advent of a variety of technologies from China and the new numeral system and other mathematical inventions from India - both via creative West Asian intermediaries. India had experienced a mathematical (more specifically algoristic or computational) revolution heralded by Ārya-bhata in the fifth century CE. The new computational power unleashed by this revolution combined with the classical Greek penchant for axiomatised modelmaking and a technology empowered experimental philosophy, in what appears to have been a very creative and uniquely European cultural fusion that led to the scientific (and later the industrial) revolution. The second part of the proposal is that there was an epistemological reason why the Indian mathematical revolution did not lead to a corresponding ‘distinctively modern’ scientific one. The Indic approach was basically not that of modelmakers but of ingenious algorisers, and showed a deep and studied distrust of axioms and physical models. This led to an attitude described here as ‘computational positivism’, which considers observation and computation as the only things that matter. In retrospect, that distrust appears not unjustified, especially in the light of twentieth century developments in quantum and classical mechanics and in logic; but it was historically expensive for India, as Europe achieved unreasonably and unexpectedly spectacular successes in science. To the Indians, it was Newton who was the extraordinary epistemological revolutionary, not Heisenberg or Gödel. In summary, Indian science could not move forward without the modelmaking and technology enabled experimental abilities that grew in the West, just as European modelmaking had earlier been unable to progress without the advent of powerful technologies and computational tools whose roots can be traced to China and India.},
 author = {Roddam Narasimha},
 doi = {10.1179/030801803225010340},
 eprint = {https://doi-org.crai.referencistas.com/10.1179/030801803225010340},
 journal = {Interdisciplinary Science Reviews},
 number = {1},
 pages = {54–66},
 title = {The Indian half of Needham’s question: some thoughts on axioms, models, algorithms, and computational positivism},
 url = {https://doi-org.crai.referencistas.com/10.1179/030801803225010340},
 volume = {28},
 year = {2003p}
}

@article{doi:10.1179/030801810X12772143410485,
 abstract = {This contribution is part of a special issue on History and Human Nature, comprising an essay by G.E.R. Lloyd and fifteen invited responses.},
 author = {Alan F Blackwell},
 doi = {10.1179/030801810X12772143410485},
 eprint = {https://doi-org.crai.referencistas.com/10.1179/030801810X12772143410485},
 journal = {Interdisciplinary Science Reviews},
 number = {3–4},
 pages = {387–403},
 title = {When Systemizers Meet Empathizers: Universalism and the Prosthetic Imagination},
 url = {https://doi-org.crai.referencistas.com/10.1179/030801810X12772143410485},
 volume = {35},
 year = {2010c}
}

@article{doi:10.1179/0308018812Z.0000000006,
 abstract = {In this study I examine the use of visualization within everyday research practices in computational physics. In doing so, I attempt to move from the well documented representational issues elicited by the concept of the image, to more microscale issues of the habitual structuring of the everyday that emerge when a specific example of science in the making is analysed. To this end, I focus on one specific example, of tracing a computational error through a fluid dynamics simulation of the ‘lock exchange’ experiment. This simulation is one small part of the research that goes on within one of Europe’s largest computational physics research groups, the Applied Modelling and Computation Group at Imperial College in London, where I am involved in ongoing ethnographic fieldwork research. Visualization is shown to play a central role, not just in daily routines of investigation and problem solving, but in the process of habituation through which scientists cultivate the dispositions through which everyday life gains its texture and form. Far from being a detachable representation of a part of a world, simulation is shown to come into being as a process within a world structured by the repetitions and improvizations that characterize research practice.},
 author = {Matt Spencer},
 doi = {10.1179/0308018812Z.0000000006},
 eprint = {https://doi-org.crai.referencistas.com/10.1179/0308018812Z.0000000006},
 journal = {Interdisciplinary Science Reviews},
 number = {1},
 pages = {86–100},
 title = {Image and Practice: Visualization in Computational Fluid Dynamics Research},
 url = {https://doi-org.crai.referencistas.com/10.1179/0308018812Z.0000000006},
 volume = {37},
 year = {2012p}
}

@article{doi:10.1179/0308018814Z.00000000082,
 abstract = {The emergent reflexive process, by which researchers in a computational humanities project work towards a viable organizational modality for interdisciplinary collaboration, is analyzed. Using the metaphor of decomposition, successful collaboration between computer scientists and humanities scholars can be seen to require a reflexive scrutiny — a decomposition — of the disciplinary research processes that are involved, thus allowing crucial differences with respect to typical ways of posing research questions, the role of data, and the rhythm of the research process to be highlighted. It is argued that the currently popular expectation towards data as a self-identical organizational unit tends to downplay the role of decomposition as practice and process. A European cyberinfrastructure initiative that tries to respect the specificities of scholarly practice in the humanities is critically assessed, reflecting in particular on the inherent tension between ‘mutual shaping’ of digital tools and their users on the one hand, and the policy interest in efficient, functionalist design principles on the other.},
 author = {Wolfgang Kaltenbrunner},
 doi = {10.1179/0308018814Z.00000000082},
 eprint = {https://doi-org.crai.referencistas.com/10.1179/0308018814Z.00000000082},
 journal = {Interdisciplinary Science Reviews},
 number = {2},
 pages = {143–161},
 title = {Decomposition as Practice and Process: Creating Boundary Objects in Computational Humanities},
 url = {https://doi-org.crai.referencistas.com/10.1179/0308018814Z.00000000082},
 volume = {39},
 year = {2014f}
}

@article{doi:10.1179/037195504225004706,
 abstract = {Most manganese used in the world is consumed as ferroalloys by the steelmaking industry. Submerged arc electric furnace smelting using the manganese-rich slag method is widely used to produce ferromanganese. This process has been modelled using the HSC computational thermodynamics package. It was assumed that higher manganese and iron oxides are reduced to MnO and FeO before entering the zone where molten slag and alloy form and equilibrate. The model predictions were compared to data from Thermit Alloys (P) Limited, an Indian ferroalloy smelter, and the agreement was found to be good. It was then used to examine the affects of changing the amount of carbon reductant and temperature on several performance indicators. The results of this modelling are discussed and it is concluded that the model is useful as an aid to understanding ferromanganese smelting.},
 author = {E. C. Vanderstaay and D. R. Swinbourne and M. Monteiro},
 doi = {10.1179/037195504225004706},
 eprint = {https://doi-org.crai.referencistas.com/10.1179/037195504225004706},
 journal = {Mineral Processing and Extractive Metallurgy},
 number = {1},
 pages = {38–44},
 title = {A computational thermodynamics model of submerged arc electric furnace ferromanganese smelting},
 url = {https://doi-org.crai.referencistas.com/10.1179/037195504225004706},
 volume = {113},
 year = {2004p}
}

@article{doi:10.1179/174327908X366897,
 abstract = {The key characteristics that distinguish soft systems from hard ones are spelt out. The writings of Michael Polanyi and Martin Heidegger, despite their very different worldviews, are used to give philosophical underpinning to these characteristics, especially the notion of practitioner involvement. In addition, the complementary nature of Polanyi’s ‘emergence’ and Heidegger’s ‘breakdowns’ is highlighted. In conclusion, a reflection is given on what constitutes a soft systems approach.},
 author = {W. P. S. Dias},
 doi = {10.1179/174327908X366897},
 eprint = {https://doi-org.crai.referencistas.com/10.1179/174327908X366897},
 journal = {Interdisciplinary Science Reviews},
 number = {3},
 pages = {202–213},
 title = {Philosophical underpinning for systems thinking},
 url = {https://doi-org.crai.referencistas.com/10.1179/174327908X366897},
 volume = {33},
 year = {2008f}
}

@article{doi:10.1179/174328405X58850,
 abstract = {Resistivity maps for Si wafers processed using BN solid sources and hydrogen injection were compared to convective flow patterns predicted computationally. The convective flow patterns were found to mirror the resistivity maps, with low velocity flow domains being associated with high resistivity regions on the wafer and high velocity flow with low resistivity regions. Subtle changes in both the flow patterns and resistivity maps as a function of location within the furnace were consistently reflected in both computed flow domains and measured resistitivies. Finally, axially eccentric placement of the wafer–source stack was found to promote convection between wafer–source pairs by as much as a factor of five.},
 author = {S. Shanmugasundaram and J. J. Biernacki and R. Subramanian},
 doi = {10.1179/174328405X58850},
 eprint = {https://doi-org.crai.referencistas.com/10.1179/174328405X58850},
 journal = {Materials Science and Technology},
 number = {9},
 pages = {1103–1110},
 title = {Variation in boron doping by planar diffusion – a comparative study of computational hydrodynamics and experimental observations},
 url = {https://doi-org.crai.referencistas.com/10.1179/174328405X58850},
 volume = {21},
 year = {2005o}
}

@article{doi:10.1179/1743284715Y.0000000079,
 abstract = {The mechanisms involved in the abnormal grain growth of the iron based oxide dispersion strengthened alloys are analysed in the present work. Its microstructural evolution takes place at high temperatures (0.9Tm) and is characterised by an initial submicrometre size microstructure and a strong < 110>|| rolling direction (RD) texture that evolves into a few extremely coarse grains (mm sizes) with < 112>||RD orientation. The analysis of the observed grain boundaries has been completed by molecular dynamics simulations. Microstructure evolution consists of an extended recovery process, followed by an abnormal grain growth stage, consequence of the orientation pinning mechanism and the proximity to a symmetric tilt boundary family between the < 110>||RD and < 112>||RD grains.},
 author = {G. Pimentel and I. Toda-Caraballo and C. Capdevila},
 doi = {10.1179/1743284715Y.0000000079},
 eprint = {https://doi-org.crai.referencistas.com/10.1179/1743284715Y.0000000079},
 journal = {Materials Science and Technology},
 number = {13},
 pages = {1618–1626},
 title = {Experimental and computational analysis of abnormal grain growth},
 url = {https://doi-org.crai.referencistas.com/10.1179/1743284715Y.0000000079},
 volume = {31},
 year = {2015o}
}

@article{doi:10.1179/1743285511Y.0000000031,
 abstract = {The major issue in stainless steelmaking is the difficulty of oxidising carbon from molten steel without also oxidising large proportions of expensive chromium. This can, however, be achieved by reducing the partial pressure of the gaseous product of carbon oxidation, carbon monoxide, by dilution with argon. Modern stainless steelmaking is dominated by duplex processes which prepare a high carbon melt in an electric arc furnace, and then decarburise it in an argon–oxygen decarburisation (AOD) converter. In this work, the thermodynamic basis of preferential carbon oxidation by dilution of oxygen with argon is discussed, together with a review of AOD practice. The AOD process was simulated using computational thermodynamics software to illustrate the way in which it can achieve very low carbon levels in the molten steel bath without excessive co-oxidation of chromium. The slag reduction stage using ferrosilicon additions was also modelled and shown to be able to recover almost all oxidised chromium from the slag, limited only by the accompanying increase in the silicon content of the steel. The models, although simple and easy to develop, correctly predicted all trends in output variables as input parameters were changed and often matched plant data very well. The models provide a valuable learning tool for those interested in pyrometallurgical processing in general, and stainless steelmaking in particular.},
 author = {D R Swinbourne and T S Kho and B Blanpain and S Arnout and D E Langberg},
 doi = {10.1179/1743285511Y.0000000031},
 eprint = {https://doi-org.crai.referencistas.com/10.1179/1743285511Y.0000000031},
 journal = {Mineral Processing and Extractive Metallurgy},
 number = {1},
 pages = {23–31},
 title = {Understanding stainless steelmaking through computational thermodynamics: Part 3 – AOD converting},
 url = {https://doi-org.crai.referencistas.com/10.1179/1743285511Y.0000000031},
 volume = {121},
 year = {2012t}
}

@article{doi:10.1179/1743285515Y.0000000019,
 abstract = {Ferrovanadium is essential for the production of many alloy steels. It is made by the aluminothermic reduction of vanadium oxides, together with scrap steel and burnt lime as a flux at very high temperatures. In this work, the theory of aluminothermic reduction is discussed, and then a computational thermodynamics model is described and its input parameters are discussed and justified. The model predicted very satisfactorily the composition of both the ferrovanadium and waste slag from a heat at the Windimurra smelter in Western Australia. Moreover, the modelling revealed that published data on the activity coefficient of VO1.5 in slag were likely to be seriously in error. It was shown that increasing the recovery of vanadium to the ferroalloy was accompanied by a rapid increase in its aluminium content, which is limited by commercial specifications. The limit on the silicon content of ferrovanadium can only be met by limiting the input of SiO2 to the furnace. The model provides a useful basis for assessing process improvements.},
 author = {D. R. Swinbourne and T. Richardson and F. Cabalteja},
 doi = {10.1179/1743285515Y.0000000019},
 eprint = {https://doi-org.crai.referencistas.com/10.1179/1743285515Y.0000000019},
 journal = {Mineral Processing and Extractive Metallurgy},
 number = {1},
 pages = {45–55},
 title = {Understanding ferrovanadium smelting through computational thermodynamics modelling},
 url = {https://doi-org.crai.referencistas.com/10.1179/1743285515Y.0000000019},
 volume = {125},
 year = {2016p}
}

@article{doi:10.1179/2042645313Y.0000000031,
 abstract = {In this study, solid Scots pine was surface densified in an open press using different moisture contents (9·6 and 12·4%), temperatures (150 and 200°C), press closing times (0·5 and 5 min), holding times (1 and 10 min) and compression ratios (6·7 and 25%). The characteristics of the formed density profiles were defined and their correlations to the process parameters and Brinell hardness and elastic recovery were analysed. Compression ratio and closing time had the strongest effect on the formation of the density profile, as well as the hardness and elastic recovery. The amount of localised deformation (peak area) did not have significant effect on the Brinell hardness or elastic recovery, whereas, the highest density achieved (peak density) and its location (peak distance) dominated the effect. Brinell hardness and elastic recovery were found to correlate well with each other.},
 author = {K Laine and T Antikainen and L Rautkari and M Hughes},
 doi = {10.1179/2042645313Y.0000000031},
 eprint = {https://doi-org.crai.referencistas.com/10.1179/2042645313Y.0000000031},
 journal = {International Wood Products Journal},
 number = {3},
 pages = {144–149},
 title = {Analysing density profile characteristics of surface densified solid wood using computational approach},
 url = {https://doi-org.crai.referencistas.com/10.1179/2042645313Y.0000000031},
 volume = {4},
 year = {2013j}
}

@article{doi:10.1179/isr.1989.14.3.225,
 abstract = {Specific genetic abnormalities (alterations in growth-related genes), either inherited or acquired during lifetimes, may deregulate normal cell growth by interfering with very basic cellular, biochemical and biophysical processes. In order to be able to prevent such unregulated growth of cells, which may lead to cancer, we will have to understand the basic principles of genome organisation better, the mechanisms of genetic information storage, and the molecular structure and functioning of the genes and gene families themselves. This review gives a short overview of the various hierarchical levels at which an attempt is made, using different methods, to collect genomic information relevant to cancer research. We will start with cytogenetic and genetic linkage maps at the lowest resolution, work with ordered clone and complementary DNA libraries and with nucleotide sequences at medium resolution, and investigate fine details of the atomic and electronic structures of the molecules involved at the highest resolution. Very different disciplines – biology, chemistry, physics, mathematics, genetics, artificial intelligence research and others – have to cooperate closely in approaching the above-mentioned ambitious goals. One of their common denominators is their intensive use of computers as tools in organising genomic information and in trying to convert it to biological knowledge.},
 author = {Sándor Suhai},
 doi = {10.1179/isr.1989.14.3.225},
 eprint = {https://doi-org.crai.referencistas.com/10.1179/isr.1989.14.3.225},
 journal = {Interdisciplinary Science Reviews},
 number = {3},
 pages = {225–232},
 title = {Computational Methods in Cancer Research The Hierarchy of Genomic Information},
 url = {https://doi-org.crai.referencistas.com/10.1179/isr.1989.14.3.225},
 volume = {14},
 year = {1989r}
}

@article{doi:10.1186/1744-8069-4-13,
 abstract = {Understanding differences in the genetic architecture of complex traits between the two sexes has significant implications for evolutionary studies and clinical diagnosis. However, our knowledge about sex-specific genetic architecture is limited largely because of a lack of analytical models that can detect and quantify the effects of sex on the complexity of quantitative genetic variation. Here, we derived a statistical model for mapping DNA sequence variants that contribute to sex-specific differences in allele frequencies, linkage disequilibria, and additive and dominance genetic effects due to haplotype diversity. This model allows a genome-wide search for functional haplotypes and the estimation and test of haplotype by sex interactions and sex-specific heritability. The model, validated by simulation studies, was used to detect sex-specific functional haplotypes that encode a pain sensitivity trait in humans. The model could have important implications for mapping complex trait genes and studying the detailed genetic architecture of sex-specific differences.},
 author = {Chenguang Wang and Yun Cheng and Tian Liu and Qin Li and Roger B Fillingim and Margaret R Wallace and Roland Staud and Lee Kaplan and Rongling Wu},
 doi = {10.1186/1744-8069-4-13},
 eprint = {https://doi-org.crai.referencistas.com/10.1186/1744-8069-4-13},
 journal = {Molecular Pain},
 note = {PMID:18416828},
 number = { },
 pages = {1744-8069-4–13},
 title = {A Computational Model for Sex-Specific Genetic Architecture of Complex Traits in Humans: Implications for Mapping Pain Sensitivity},
 url = {https://doi-org.crai.referencistas.com/10.1186/1744-8069-4-13},
 volume = {4},
 year = {2008s}
}

@article{doi:10.1191/030913298670827726,
 author = {A. Stewart Fotheringham},
 doi = {10.1191/030913298670827726},
 eprint = {https://doi-org.crai.referencistas.com/10.1191/030913298670827726},
 journal = {Progress in Human Geography},
 number = {2},
 pages = {283–292},
 title = {Trends in quantitative methods II: stressing the computational},
 url = {https://doi-org.crai.referencistas.com/10.1191/030913298670827726},
 volume = {22},
 year = {1998j}
}

@article{doi:10.1243/030932405X7809,
 abstract = {The main aim of the work was to investigate a simplified finite element simulation of the out-of-plane distortion caused by fusion butt welding. The thermal transient part of the simulation made use of a finite element analysis of the two-dimensional cross-section of the weld joint and the thermoelastic-plastic treatment was based on analytical algorithms describing transverse and longitudinal deformations, leading to predictions of transverse angular deformation and longitudinal contraction force. These results were then applied to a non-linear elastic finite element model to provide predictions of the final angular and overall deformations of the butt-welded plates. The validity of the simulation was investigated via full-scale tests on 4m x 1.4m x 5 mm steel plates, butt welded using a flux-cored Ar-CO2 metal-inert gas process. Thermography and thermocouple arrays were used to validate the thermal transient computations and out-of-plane deformations were measured using displacement transducers for transient deformations and a laser scanning system to measure the profiles of the whole plates before and after welding. The results of six full-scale tests are given and comparison with the simulations shows that the procedure provides good prediction of the angular and overall out-of-plane deformations. Prediction accuracy requires account to be taken of initial shape, gravity loading, and support conditions.},
 author = {D Camilleri and T Comlekci and T. G. F Gray},
 doi = {10.1243/030932405X7809},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/030932405X7809},
 journal = {The Journal of Strain Analysis for Engineering Design},
 number = {2},
 pages = {161–176},
 title = {Computational prediction of out-of-plane welding distortion and experimental investigation},
 url = {https://doi-org.crai.referencistas.com/10.1243/030932405X7809},
 volume = {40},
 year = {2005e}
}

@article{doi:10.1243/03093247JSA232,
 abstract = {This paper explores the use of both macroscale and microscale modelling for the analysis of extrusion of an AA2009 + 25%SiCp metal matrix composite (MMC). The performance of a micromechanical model, where the heterogeneous microstructure of the MMC is explicitly modelled, in predicting the tensile stress—strain behaviour of an AA2009 + 25% SiCp MMC is examined. A macroscale modelling approach is used to simulate extrusion of the MMC through two different die designs, where the MMC is modelled as a homogeneous continuum. Firstly, the extrusion results are used to compare the two die designs, to determine which is the more favourable. Secondly, the predicted macroscale plastic strain distributions and pressures are used with the micromechanical model to assess microscale stress states in the material during extrusion with a view to gaining insights into the risk of damage in the material. In this context, pressure is shown to be hugely important in controlling tensile stress magnitude and in reducing microscale damage risk, and essentially ensuring that extrusion can be achieved in practice. However, the results reveal that damage risk is not totally eliminated and that there may still be locations where the material may rupture.},
 author = {D. E Ilie and B. P O’Donnell and J. P McGarry and P. E McHugh},
 doi = {10.1243/03093247JSA232},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/03093247JSA232},
 journal = {The Journal of Strain Analysis for Engineering Design},
 number = {4},
 pages = {237–252},
 title = {Computational modelling of the extrusion of an Al-SiC metal matrix composite using macroscale and microscale methods},
 url = {https://doi-org.crai.referencistas.com/10.1243/03093247JSA232},
 volume = {42},
 year = {2007m}
}

@article{doi:10.1243/03093247V074303,
 abstract = {Abstract First results of an analysis of notch-bend pieces in plane strain by two-dimensional elastic-plastic finite-element computations are given. Comparisons are given of load-displacement records for three notch depths and a general non-dimensional presentation proposed. Relations between the values of surface and cracktip crack opening displacement are shown and comparison is made with a few results in plane stress, by Dugdale model and in tension. The trend of local stress and strain patterns is shown and stress triaxiality is evaluated approximately. It is concluded that nearly quantitative agreement can be reached for the general behaviour of notched pieces by the use of fairly simple two-dimensional models but many of the results could be improved by the use of better element meshes. For the study of local crack-tip deformations more detailed models or more powerful means of characterizing the crack tip are required.},
 author = {C E Turner and J S T Cheung},
 doi = {10.1243/03093247V074303},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/03093247V074303},
 journal = {Journal of Strain Analysis},
 number = {4},
 pages = {303–312},
 title = {Computation of post-yield behaviour in notch-bend and tension testpieces},
 url = {https://doi-org.crai.referencistas.com/10.1243/03093247V074303},
 volume = {7},
 year = {1972r}
}

@article{doi:10.1243/095440505X32661,
 abstract = {Abstract Hemming is a three-step sheet-folding process utilized in the production of automotive closures. It has a critical imact on the performance and perceived quality of assembled vehicles. Using a two-dimensional finite element model, this paper presents a design-of-experiments (DOE) study of the relationships between important hemming process parameters and hem quality for aluminium alloy AA 6111-T4PD flat surface-straight edge hemming. The quality measures include roll-in/roll-out of the hem edge as well as the maximum true strain on the exposed bent surface. The finite element (FE) model combines explicit and implicit procedures in simulating the three forming subprocesses (flanging, pre-hemming, and final hemming) along with the corresponding springback (unloading). The results show that the pre-hemming die angle and the flanging die radius have the greatest influence on hem edge roll-in/roll-out, while pre-strain and the flanging die radius impact the maximum surface strain significantly. The computational DOE results also provide the basis for process parameter selection to avoid hem surface cracking and particular insights for achieving acceptable formability.},
 author = {G Lin and K Iyer and S J Hu and W Cai and S P Marin},
 doi = {10.1243/095440505X32661},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/095440505X32661},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part B: Journal of Engineering Manufacture},
 number = {10},
 pages = {711–722},
 title = {A computational design-of-experiments study of hemming processes for automotive aluminium alloys},
 url = {https://doi-org.crai.referencistas.com/10.1243/095440505X32661},
 volume = {219},
 year = {2005m}
}

@article{doi:10.1243/09544054JEM539,
 abstract = {Abstract A number of modelling approaches, such as enterprise modelling, simulation modelling, and causal loop diagramming, have been used independently for designing systems of different kinds, such as machine systems, social systems, and production systems. Enterprise modelling has its roots in control system design. Simulation modelling has generally been used in re-engineering and optimizing production and service processes, whereas causal loop diagramming (systems thinking) has generally been used for systems designing in management and social sciences. This paper describes the combined use of these three modelling approaches which carry complementary concepts that can be used together for supporting management decisions in designing and changing complex manufacturing enterprises. A decision support framework is presented that is instrumented with combined use of these three modelling approaches and has been tested in production management-related decision making.},
 author = {K A Chatha and R H Weston},
 doi = {10.1243/09544054JEM539},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/09544054JEM539},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part B: Journal of Engineering Manufacture},
 number = {12},
 pages = {1969–1981},
 title = {Combined discrete event simulation and systems thinking-based framework for management decision support},
 url = {https://doi-org.crai.referencistas.com/10.1243/09544054JEM539},
 volume = {220},
 year = {2006c}
}

@article{doi:10.1243/095440605X31788,
 abstract = {Abstract This article presents a complementary experimental and computational investigation of the effect of viscosity and flowrate on the dynamics of drop formation in the dripping mode. In contrast to previous studies, numerical simulations are performed with two popular commercial computational fluid dynamics (CFD) packages, CFX and FLOW-3D, both of which employ the volume of fluid (VOF) method. Comparison with previously published experimental and computational data and new experimental results reported here highlight the capabilities and limitations of the aforementioned packages.},
 author = {O. B. Fawehinmi and P. H. Gaskell and P. K. Jimack and N Kapur and H. M. Thompson},
 doi = {10.1243/095440605X31788},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/095440605X31788},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
 number = {9},
 pages = {933–947},
 title = {A Combined Experimental and Computational Fluid Dynamics Analysis of the Dynamics of Drop Formation},
 url = {https://doi-org.crai.referencistas.com/10.1243/095440605X31788},
 volume = {219},
 year = {2005d}
}

@article{doi:10.1243/09544062JMES1575,
 abstract = {Abstract An Eulerian—Eulerian two-fluid model (TFM) integrating the kinetic theory for emulsion phase was used to simulate gas—solid fluidized beds. Validation of the model was investigated based on hydrodynamic parameters such as bed expansion ratio, H/H0, gas volume fraction profile, bubble behaviour, and motion of the particles. A good agreement was found between numerical results and experimental values. The model was used to study a bubbling fluidized bed (BFB) including the ring baffles. Predicted results show that the ring baffles have an important role in the flow pattern of the bed. Baffles increase the bed expansion height and particle velocities at axial locations on the top of the highest baffle as well as uniform distribution of gas volume fraction between the baffles area. In spite of increasing the dead zones in the bed, ring baffles cause the improvement of mixing and heat transfer in the bed. The present study provides a useful basis for further works on the effect of baffles in BFBs.},
 author = {S H Hosseini and R Rahimi and M Zivdar and A Samimi},
 doi = {10.1243/09544062JMES1575},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/09544062JMES1575},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
 number = {10},
 pages = {2281–2289},
 title = {The effect of ring baffles on the hydrodynamics of a gas—solid bubbling fluidized bed using computational fluid dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1243/09544062JMES1575},
 volume = {223},
 year = {2009h}
}

@article{doi:10.1243/09544062JMES2464,
 abstract = {A robust approach to computational kinematics intended to cope with algorithmic singularities is introduced in this article. The approach is based on the reduction of the original system of equations to a subsystem of bivariate equations, as opposed to the multivariate polynomial reduction leading to the characteristic univariate polynomial. The effectiveness of the approach is illustrated for the exact function-generation synthesis of planar, spherical, and spatial four-bar linkages. Some numerical examples are provided for the case of the spherical four-bar function generator with six precision points to show the benefits of the proposed method with respect to methods reported in the literature.},
 author = {L Gracia and J Angeles},
 doi = {10.1243/09544062JMES2464},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/09544062JMES2464},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
 number = {4},
 pages = {987–999},
 title = {Robustness to algorithmic singularities and sensitivity in computational kinematics},
 url = {https://doi-org.crai.referencistas.com/10.1243/09544062JMES2464},
 volume = {225},
 year = {2011f}
}

@article{doi:10.1243/09544062JMES2487,
 abstract = {The purpose of this article is to optimize the design of a pickup head that removes particles from road surface. A validated computational fluid dynamics model was proposed to evaluate the particle removal performance of the designed pickup head with different inclination angles. The gas-particle flow through the pickup head was modelled using the EulerianLagrangian approach. The realizable k model and the discrete particle model were adopted to simulate gas flow field and solid particle trajectories, respectively. The results indicate that the inclination angle of the rear edge wall and the pressure drop across the pickup head have great impact on the particle removal performance. Both the particle overall removal efficiency and the grade efficiency increase with the increment of inclination angle, and higher pressure drop can pick up more particles from the road surface, but it would induce unnecessary energy consumption. Therefore, it is necessary to design a pickup head with high removal efficiency and low pressure drop. Through simulation, the optimal angle should be 135 for the range of the inclination angle in this study, and pressure drop is about 2400Pa. Furthermore, more information can be acquired for pickup head design.},
 author = {B Wu and J Men and J Chen},
 doi = {10.1243/09544062JMES2487},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/09544062JMES2487},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
 number = {4},
 pages = {939–948},
 title = {Improving the design of a pickup head for particle removal using computational fluid dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1243/09544062JMES2487},
 volume = {225},
 year = {2011t}
}

@article{doi:10.1243/0954406981521583,
 abstract = {Abstract This paper presents some results obtained during the computational fluid dynamics (CFD) analysis of internal flows inside a hydraulic component, using a scaling technique applied to numerical pre- and post-processing. The main aim of the work is to demonstrate the reduction of computational work needed for a complete analysis of component behaviour over a wide range of operating conditions. This result is achieved through the adoption of a methodology aimed at giving the highest level of generality to a non-dimensional solution, thereby overcoming the two major limitations encountered in the use of CFD in fluid power design: computer resources and time. In the case study, the technique was applied to a hydraulic distributor and computations were performed with a commercial computational fluid dynamics code. The key factor of this technique is the evaluation, for a given distributor opening, of the Reynolds number of the flow in the metering region. Provided that this number is high enough to ensure that the discharge coefficient has reached its asymptotic value, the characterization of the flow by a single non-dimensional numerical run can be shown. The theoretical contents of the analysis of the re-scaling technique, which focuses on the engineering information necessary in component design, are described in detail. The bases for its subsequent application to actual cases are then outlined. Finally, a fairly close correlation between numerical results and experimental data is presented.},
 author = {M Borghi and G Cantore and M Milani and R Paoluzzi},
 doi = {10.1243/0954406981521583},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/0954406981521583},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
 number = {7},
 pages = {619–629},
 title = {Analysis of hydraulic components using computational fluid dynamics models},
 url = {https://doi-org.crai.referencistas.com/10.1243/0954406981521583},
 volume = {212},
 year = {1998b}
}

@article{doi:10.1243/0954406991522202,
 abstract = {Abstract The present paper discusses the basic principles of hydraulic turbines, with special emphasis on the use of computational fluid dynamics (CFD) as a tool which is being increasingly applied to gain insight into the complex three-dimensional (3D) phenomena occurring in these types of fluid machinery. The basic fluid mechanics is briefly treated for the three main types of hydraulic turbine: Pelton, Francis and axial turbines. From the vast number of applications where CFD has proven to be an important help to the design engineer, two examples have been chosen for a detailed discussion. The first example gives a comparison of experimental data and 3D Euler and 3D Navier-Stokes results for the flow in a Francis runner. The second example highlights the state-of-the-art of predicting the performance of an entire Francis turbine by means of numerical simulation.},
 author = {P Drtina and M Sallaberger},
 doi = {10.1243/0954406991522202},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/0954406991522202},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
 number = {1},
 pages = {85–102},
 title = {Hydraulic turbines—basic principles and state-of-the-art computational fluid dynamics applications},
 url = {https://doi-org.crai.referencistas.com/10.1243/0954406991522202},
 volume = {213},
 year = {1999f}
}

@article{doi:10.1243/0954406991522211,
 abstract = {Abstract Computational fluid dynamics (CFD) probably plays a greater part in the aerodynamic design of turbomachinery than it does in any other engineering application. For many years the design of a modern turbine or compressor has been unthinkable without the help of CFD and this dependence has increased as more of the flow becomes amenable to numerical prediction. The benefits of CFD range from shorter design cycles to better performance and reduced costs and weight. This paper presents a review of the main CFD methods in use, discusses their advantages and limitations and points out where further developments are required. The paper is concerned with the application of CFD and does not describe the numerical methods or turbulence modelling in any detail.},
 author = {J. D. Denton and W. N. Dawes},
 doi = {10.1243/0954406991522211},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/0954406991522211},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
 number = {2},
 pages = {107–124},
 title = {Computational fluid dynamics for turbomachinery design},
 url = {https://doi-org.crai.referencistas.com/10.1243/0954406991522211},
 volume = {213},
 year = {1998h}
}

@article{doi:10.1243/0954407011528581,
 abstract = {Abstract This study was carried out to assess the ability of a computational fluid dynamics (CFD) code to predict the scavenging flow in the transfer duct of a two-stroke cycle engine. A two-stroke cycle engine was modified to allow laser Doppler velocimetry (LDV) measurements to be made in one transfer duct. It was operated under motoring conditions at 500r/min with a delivery ratio of 0.7. Predictions were obtained from a dynamic CFD simulation of the flow within the cylinder, transfer duct and a portion of the exhaust duct. Boundary conditions for the CFD model were obtained from experimentally measured pressure-time histories in the crankcase and exhaust. A comparison of measured and predicted transfer duct axial velocities at various locations within the duct showed that the CFD model could replicate the general trend of the flow but not the details. From the LDV measurements and CFD predictions, velocity oscillations were observed between the end of crankcase blowdown and transfer port closing. A one-dimensional general engine simulation package was used to investigate the gas dynamic activity in the transfer duct. It was found that the observed oscillations were due to pressure wave reflections in the transfer duct. The general trend of the axial velocity profile in the transfer duct was well replicated by the one-dimensional simulation as were the exhaust and crankcase pressures.},
 author = {J. P. Creaven and R. G. Kenny and R Fleck and G Cunningham},
 doi = {10.1243/0954407011528581},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/0954407011528581},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering},
 number = {9},
 pages = {1017–1031},
 title = {A computational and experimental study of the scavenging flow in the transfer duct of a motored two-stroke cycle engine},
 url = {https://doi-org.crai.referencistas.com/10.1243/0954407011528581},
 volume = {215},
 year = {2001c}
}

@article{doi:10.1243/095440704774061165,
 abstract = {Abstract A study has been undertaken to assess the capability of incorporating different empirical approaches in a computational ftuid dynamics (CFD) environment for predicting boiling heat transfer. The application is for internal combustion (IC) engine cooling galleries and experimental validation work has been undertaken. Three different boiling heat transfer models are described, one based on the principle of superposition (Chen) and two based on the partial boiling method (Thom and Cipolla). Overall, the Thom partial boiling approach was found to be the most representative of the three considered. However, numerous issues were found to be evident whatever approach was adopted and these are discussed in the paper. The partial boiling model was found to be the most simple to incorporate in the CFD model.},
 author = {J G Hawley and M Wilson and N A F Campbell and G P Hammond and M J Leathard},
 doi = {10.1243/095440704774061165},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/095440704774061165},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering},
 number = {5},
 pages = {509–520},
 title = {Predicting boiling heat transfer using computational fluid dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1243/095440704774061165},
 volume = {218},
 year = {2004j}
}

@article{doi:10.1243/09544070JAUTO1063,
 abstract = {Abstract Since ballistic and blast survivability and off-road handling and stability of military vehicles, such as the high-mobility multi-purpose wheeled vehicle (HMMWV), are two critical vehicle performance aspects, they both (including the delicate balance between them) have to be considered when a new vehicle is being designed or an existing vehicle retrofitted (e.g. up-armoured). Finite-element-based transient non-linear dynamics and multi-body longitudinal dynamics computational analyses were employed, relatively, in the present work to address the following two specific aspects of the performance of an HMMWV: first, the ability of the vehicle to survive detonation of a landmine shallow buried into sand underneath the right wheel of the vehicle and, second, the ability of the vehicle to withstand a simple straight-line brake manoeuvre during off-road travel without compromising its stability and safety of its occupants. Within the first analysis, the kinematic and structural responses (including large-scale rotation and deformation, buckling, plastic yielding, failure initiation, fracture, and fragmentation) of the HMMWV to the detonation of a landmine were analysed computationally using the general-purpose transient non-linear dynamics analysis software ABAQUS/Explicit. The second analysis was carried out using Simpack, a general-purpose multi-body dynamics program, and the main purpose of this analysis was to address the vehicle stability during the off-road travel. The same sand model was used in both types of analysis. Finally, the computational results obtained are compared with general field-test observations and data in order to judge the physical soundness and fidelity of the present approach.},
 author = {M Grujicic and G Arakere and H Nallagatla and W C Bell and I Haque},
 doi = {10.1243/09544070JAUTO1063},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/09544070JAUTO1063},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering},
 number = {3},
 pages = {301–325},
 title = {Computational investigation of blast survivability and off-road performance of an up-armoured high-mobility multi-purpose wheeled vehicle},
 url = {https://doi-org.crai.referencistas.com/10.1243/09544070JAUTO1063},
 volume = {223},
 year = {2009b}
}

@article{doi:10.1243/09544070JAUTO1170,
 abstract = {Abstract A comprehensive finite-element-based computational investigation of the ability of different measures (e.g. up-armouring, seat cushion, or seat belt restraint system) to protect the occupant(s) of a prototypical high-mobility multi-purpose wheeled vehicle in the event of an anti-vehicle mine detonation under the vehicle’s front right wheel is carried out. While assessing the effectiveness of these protective measures, different injury criteria had to be defined and/or employed, their values calculated (during the initial response stage of the blast event), and compared with the limiting values corresponding to the critical levels of injury. The efficacies are compared both when these measures are implemented in isolation and in the presence of other protective measures with respect to specific injuries.},
 author = {M Grujicic and G Arakere and W C Bell and I Haque},
 doi = {10.1243/09544070JAUTO1170},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/09544070JAUTO1170},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering},
 number = {7},
 pages = {903–920},
 title = {Computational investigation of the effect of up-armouring on the reduction in occupant injury or fatality in a prototypical high-mobility multi-purpose wheeled vehicle subjected to mine blast},
 url = {https://doi-org.crai.referencistas.com/10.1243/09544070JAUTO1170},
 volume = {223},
 year = {2009d}
}

@article{doi:10.1243/09544070JAUTO1434,
 abstract = {Abstract In this paper, experiments were carried out on a direct-injection diesel engine using a common-rail system, in order to study the effects of multi-injection modes on the combustion characteristics and pollutant emissions. A soot model was proposed for the post-injection mode, namely the Hiroyasu—Kodota averaged-reaction-rate soot model, which took into account both the chemical kinetics reaction and the turbulent mixing motion of the spray jet. Through integrating the revised soot model into a computational fluid dynamics (CFD) code, the combustion process and pollutants formation of the tested engine were simulated. The in-cylinder gas pressure and combustion heat release rate showed satisfactory agreement with measurements. The experimental data demonstrated that the pilot-injection mode was one of the most effective measures for reducing combustion noise. Meanwhile an optimum split-injection mode consisting of an appropriate pilot-injection fuel quantity combined with an optimal pilot-injection—main-injection interval could be achieved to decrease the nitrogen oxide (NOx) emission while not causing the particulate matter (PM) emission to deteriorate very much. Two innovative concepts of an active thermo-atmosphere and a passive inert atmosphere were presented from numerical simulation to discuss the effect of the pilot-injection mode on the combustion behaviour of the main injection. Regarding the post-injection mode, its prominent advantage was to decrease significantly the PM emission without an NOx emission penalty. Furthermore, by CFD modelling of the soot formation process, it can be observed that the turbulent mixing motion caused by the post-injection spray played a vital role in the soot oxidization process.},
 author = {X-Y Shi and X-Q Qiao and J-M Ni and Y-Y Zheng and N-Y Ye},
 doi = {10.1243/09544070JAUTO1434},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/09544070JAUTO1434},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering},
 number = {9},
 pages = {1161–1176},
 title = {Study on the combustion and emission characteristics of a diesel engine with multi-injection modes based on experimental investigation and computational fluid dynamics modelling},
 url = {https://doi-org.crai.referencistas.com/10.1243/09544070JAUTO1434},
 volume = {224},
 year = {2010k}
}

@article{doi:10.1243/09544070JAUTO355,
 abstract = {Abstract In-cylinder air motion is one of the most important factors that control the degree of mixture preparation and thus is fundamental to improvements in the combustion process and overall engine performance. The major aim of this paper is to elucidate, through a predictive study, the main features of in-cylinder flow fields in a motored homogeneous charge compression ignition (HCCI) engine cylinder with variable negative valve overlapping (NVO). A commercial finite-volume computational fluid dynamics (CFD) package was used in the programme of simulation. The computational model was validated through a qualitative comparison between CFD results and the available experimental data. Thus one of the main developments presented in this study is the investigation of the intake process of the HCCI engine with various valve strategies, and it is perhaps the first time (to the current authors’ best knowledge) that a direct comparison has been made of the results obtained in the same HCCI NVO motored engine using modelling and experimental approaches. The comparison illustrated a fair agreement between both sets of results, with some differences. A parametric predictive study of the effects of variable valve timings on the in-cylinder air motion has then been carried out. Three different sets of valve timings have been applied to the intake and exhaust valves to generate NVO of 70, 90, and 110 degrees of crank angle (°CA). The NVO was controlled by adjusting the times of exhaust valves closing (EVC) and intake valves opening (IVO) while keeping the times of exhaust valves opening (EVO) and intake valves closing (IVC) unchanged. The predicted results show a noticeable modification of the strength and the global direction of the in-cylinder charge motion as a result of increasing the magnitude of NVO. Modifications of in-cylinder swirl and tumble motions obtained by applying higher degrees of NVO are expected to have a considerable effect on the air-fuel mixture preparation process as well as the actual in-cylinder conditions at the end of the compression stroke.},
 author = {A-F M Mahrous and M L Wyszynski and T Wilson and H-M Xu},
 doi = {10.1243/09544070JAUTO355},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/09544070JAUTO355},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering},
 number = {10},
 pages = {1295–1304},
 title = {Computational fluid dynamics simulation of in-cylinder flows in a motored homogeneous charge compression ignition engine cylinder with variable negative valve overlapping},
 url = {https://doi-org.crai.referencistas.com/10.1243/09544070JAUTO355},
 volume = {221},
 year = {2007m}
}

@article{doi:10.1243/09544070JAUTO450,
 abstract = {Abstract Experimental data from internal combustion (IC) engines suggests that the use of proprietary computational fluid dynamics (CFD) codes for the prediction of coolant-side heat transfer within IC engine coolant jackets often results in underprediction of the convective heat transfer coefficient. An experimental and computational study, based on a coolant gallery simulator rig designed specifically to reproduce realistic IC engine operating conditions, has been conducted to explore this issue. It is shown that the standard ‘wall function’ approach normally used in CFD models to model near-wall conditions does not adequately represent some features of the flow that are relevant in convective heat transfer. Alternative modelling approaches are explored to account for these shortcomings and an empirical approach is shown to be successful; however, the methodology is not easily transferable to other situations.},
 author = {K Robinson and M Wilson and M J Leathard and J G Hawley},
 doi = {10.1243/09544070JAUTO450},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/09544070JAUTO450},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering},
 number = {9},
 pages = {1147–1157},
 title = {Computational modelling of convective heat transfer in a simulated engine cooling gallery},
 url = {https://doi-org.crai.referencistas.com/10.1243/09544070JAUTO450},
 volume = {221},
 year = {2007k}
}

@article{doi:10.1243/09544070JAUTO755,
 abstract = {Abstract Increasing demand from the consumer for higher levels of refinement from their passenger vehicles has put considerable pressure on the automotive industry to produce ever quieter cars. In order to prevent the occurrence of many forms of brake noise, especially judder and drone, excessive heating of the brake disc must be avoided, while minimizing temperature variations across the rotor. In order for this to be achieved the brake rotor must be designed such that it ensures sufficient uniform heat dissipation and thermal capacity. In high-demand braking applications, vented discs consisting of two rubbing surfaces separated by straight radial vanes are normally employed, as they utilize a greater surface area to dissipate heat. Within this paper the effects of changing the geometry of the first row of pins on aerothermo-dynamic properties of a pin-vented brake rotor are investigated using computational fluid dynamics (CFD). The validated CFD model shows that decreasing the thickness of the first row of pins by 10 per cent improves the mass flowrate through the rotor by 14 per cent and the heat transfer rate by 6 per cent. The results obtained can be used for the design of brake discs which are efficient with respect to heat dissipation.},
 author = {E Palmer and R Mishra and J Fieldhouse},
 doi = {10.1243/09544070JAUTO755},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/09544070JAUTO755},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering},
 number = {7},
 pages = {1231–1245},
 title = {A computational fluid dynamic analysis on the effect of front row pin geometry on the aerothermodynamic properties of a pin-vented brake disc},
 url = {https://doi-org.crai.referencistas.com/10.1243/09544070JAUTO755},
 volume = {222},
 year = {2008n}
}

@article{doi:10.1243/09544070JAUTO760,
 abstract = {Abstract A three-dimensional computational fluid dynamics modelling was carried out to investigate effects of variable valve timing (VVT) and variable valve actuation (VVA) on gas exchange and fuel-air mixing processes in a diesel homogeneous charge compression ignition (HCCI) engine with early fuel injection. Four VVT or VVA strategies were conducted for this study: first, a negative valve overlap (NVO) strategy with fixed exhaust-valve-opening (EVO) and intake-valve-closing (IVC) timings but variable valve lifts, referred to as the NVO strategy; second, the NVO strategy with fixed valve profiles but variable EVO and IVC timings, referred to as the EVO strategy; third, the NVO strategy with fixed valve lifts and fixed EVO and IVC timings but variable exhaust-valve-closing (EVC) and intake-valve-opening timings, referred to as the EVC strategy; fourth, VVA with just variable valve lifts, referred to as the VMAX strategy. The results indicate that suitable NVO settings will enhance in-cylinder tumble and then increase turbulence intensity before compression end, although the increased NVO has a negative contribution to swirl ratio. It was found that reducing valve lifts alone is not an efficient way to retain the residual gas, but the function of reduced valve lifts will become significantly obvious by combining it with increasing NVO. For the effect of NVO on in-cylinder temperature, longer NVO not only will increase in-cylinder temperature because of the higher residual gas rate but also will improve the in-cylinder temperature homogeneity. On lowering the maximum valve lift or increasing the NVO, the unmixed region of the in-cylinder charge shrinks. The fuel-rich region expands because of the high intake velocity and enhanced turbulence intensity. This is beneficial to the forming of a global homogeneous charge. It has been noted from the current study that, as the droplet distribution may be influenced more by the in-cylinder air motion caused by NVO when the average droplet size is smaller, it is recommended that future studies explore the effects of VVT and VVA on diesel HCCI mixing and combustion with various advanced fuel injection strategies.},
 author = {Z -J Peng and M Jia},
 doi = {10.1243/09544070JAUTO760},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/09544070JAUTO760},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering},
 number = {6},
 pages = {1047–1064},
 title = {An investigation and evaluation of variable-valve-timing and variable-valve-actuation strategies in a diesel homogeneous charge compression ignition engine using three-dimensional computational fluid dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1243/09544070JAUTO760},
 volume = {222},
 year = {2008o}
}

@article{doi:10.1243/0954408011530253,
 abstract = {Abstract The optimization of thermal processes such as sterilization relies on the accuracy of relevant kinetic data for bacterial inactivation and quality evolution. It is also dependent on the geometry and heating mechanism involved in the process. In these processes or systems, profiles of temperature distribution, bacteria concentration and concentrations of vitamins C (ascorbic acid), B1 (thiamin) and B2 (riboflavin) in a can filled with cherry juice during thermal sterilization have been obtained through numerical simulations. Different heating medium temperatures of 121, 130 and 140°C were tested. In order to generate these profiles, the continuity, momentum and energy equations are solved numerically, together with those of bacteria and vitamins concentrations, using the computational fluid dynamics code PHOENICS, combined with reaction kinetics models. Natural convection that occurs during thermal sterilization of viscous liquid (concentrated cherry juice, 74 °Brix) in a cylindrical can heated from all sides has been studied in this work. The simulations show clearly the dependences of the concentration of live bacteria and different vitamins on both the temperature distribution and the flow pattern as sterilization proceeds. The results also show that the best sterilization temperature may not always be 121 °C, depending on the quality requirements imposed on individual food material of concern.},
 author = {A G Abdul Ghani and M M Farid and X D Chen and P Richards},
 doi = {10.1243/0954408011530253},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/0954408011530253},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part E: Journal of Process Mechanical Engineering},
 number = {1},
 pages = {9–17},
 title = {A computational fluid dynamics study on the effect of sterilization temperatures on bacteria deactivation and vitamin destruction},
 url = {https://doi-org.crai.referencistas.com/10.1243/0954408011530253},
 volume = {215},
 year = {2001f}
}

@article{doi:10.1243/09544080360562936,
 abstract = {Abstract In this study, a theoretical analysis of a heating and cooling cycle during sterilization of a three-dimensional pouch filled with carrot-orange soup was presented and analysed. Transient temperature, the shape of the slowest heating zone (SHZ) during heating and the slowest cooling zone (SCZ) during cooling were presented and studied. The simulation covered the whole heating and cooling cycles of 3600s and 1200s durations, respectively. The computational fluid dynamics (CFD) code PHOENICS was used for this purpose. Saturated steam at 121°C and water at 20°C were assumed to be the heating and cooling media, respectively. The partial differential equations describing the conservation of mass, momentum, and energy were solved numerically using the finite volume method. The liquid food used in the simulation has a temperature-dependent viscosity and density. At the end of heating, the SHZ was found to have settled into a region within 30–40 per cent of the pouch height above the bottom and at a distance approximately 20–30 per cent of the pouch length from its deepest end. In the cooling cycle, the slowest cooling zone (SCZ) was found to develop in the core of the pouch and gradually migrate toward the widest end. The vertical location of this slowest cooling zone was about 60–70 per cent of the pouch height. Experimental validation has been performed by measuring the temperature distribution in the pouch during heating and cooling, using thermocouples fixed at different locations. The predicted results were in good agreement with those obtained from the experiments.},
 author = {A G Ghani and M M Farid and X D Chen},
 doi = {10.1243/09544080360562936},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/09544080360562936},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part E: Journal of Process Mechanical Engineering},
 number = {1},
 pages = {1–9},
 title = {A computational and experimental study of heating and cooling cycles during thermal sterilization of liquid foods in pouches using CFD},
 url = {https://doi-org.crai.referencistas.com/10.1243/09544080360562936},
 volume = {217},
 year = {2003e}
}

@article{doi:10.1243/095440903769012902,
 abstract = {Abstract This work addresses crosswind stability exemplified for the German Railway Deutsche Bahn AG high-speed train ICE 2. The scope of the work is to describe the flow by means of computational fluid dynamics past the leading two cars of the train for yaw angles in the range 12.2–40.0°. Three track formations are utilized. The basic results are the set of independent aerodynamic coefficients for the lead and subsequent cars. The results are to some extent compared with experimental data for ICE 2 and also with data obtained for the Swedish high-speed train X2000. A numerical sensitivity study is undertaken to quantify differences in the above results dependent on the grid density and quality, turbulence model, numerical scheme, location of inlet and outlet boundaries, turbulence intensity and flow simulation software.},
 author = {B Diedrichs},
 doi = {10.1243/095440903769012902},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/095440903769012902},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part F: Journal of Rail and Rapid Transit},
 number = {3},
 pages = {203–226},
 title = {On computational fluid dynamics modelling of crosswind effects for high-speed rolling stock},
 url = {https://doi-org.crai.referencistas.com/10.1243/095440903769012902},
 volume = {217},
 year = {2003e}
}

@article{doi:10.1243/095441002321029026,
 abstract = {Abstract The results from two well-known and widely accepted codes, the Navier—Stokes solver FLUENT and the direct simulation Monte Carlo (DSMC) solver DS2G, have been analysed in order to fix the levels of the flow field rarefaction where the codes can work properly for the computation of aerodynamic forces and heat flux on a spacecraft during the re-entry. This subject has already been widely investigated; thus the purpose of the present work is to provide a further contribution. In order to make realistic computations, a probable path of a typical capsule, returning from an interplanetary mission to Earth, has been considered in the altitude range 50—120 km. Proper use of FLUENT was fixed at the free-stream Knudsen number Kn∞ < 7×10−5. Attempts have been made to increase this limit, but with no success. More specifically, a finer mesh as well as a slip velocity and temperature jump were considered. Physical conditions like the lack of isotropy of the pressure tensor and the failure of the classical phenomenological equations, both increasing with the rarefaction, are very probably the causes of the failure of FLU EN T. The basic principle of the DSMC solver is valid at each rarefaction level; a sensitivity analysis on the characteristic dimension of the cell, on the time step and on the number of simulated molecules verified that the restrictions on DS2G are imposed only by the capability of the computer. As neither experimental data nor numerical results are available at the present test conditions, the evaluation of the results relies just on qualitative considerations about the trends of experimental data, reported in the literature, of a sphere in a hypersonic transitional regime.},
 author = {G Zuppardi and D Paterna},
 doi = {10.1243/095441002321029026},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/095441002321029026},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {6},
 pages = {277–290},
 title = {Influence of rarefaction on the computation of aerodynamic parameters in hypersonic flow},
 url = {https://doi-org.crai.referencistas.com/10.1243/095441002321029026},
 volume = {216},
 year = {2002s}
}

@article{doi:10.1243/0954410041872861,
 abstract = {Abstract A survey of the main challenges in computational aeroelasticity is presented and recent ideas and developments are discussed. Advances over the past 25 years have to a large extent been paced by the required developments in computational fluid dynamics (CFD). The fluid-structure coupling problem remains of central importance and must be addressed in a rational manner in order to obtain accurate and reliable flutter solutions. In the direct Eulerian-Lagrangian computational scheme, a consistent and efficient fluid-structure coupling is obtained by modelling and integrating the fluid-structure system as a single dynamical system, without introducing normal or assumed modes, or an artificial “virtual surface” at the boundary. This computational approach effectively eliminates the phase integration errors associated with classical methods, where the fluid and the structure are integrated sequentially using different schemes. Numerical results are presented to contrast the efficacy of the various schemes in non-linear aeroelastic and aeroservoelastic calculations.},
 author = {O O Bendiksen},
 doi = {10.1243/0954410041872861},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/0954410041872861},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {3},
 pages = {157–177},
 title = {Modern developments in computational aeroelasticity},
 url = {https://doi-org.crai.referencistas.com/10.1243/0954410041872861},
 volume = {218},
 year = {2004a}
}

@article{doi:10.1243/095441005X30306,
 abstract = {Abstract Computational fluid dynamics (CFD) simulations of ship airwakes are discussed in this article. CFD is used to simulate the airwakes of landing helicopter assault (LHA) and landing platform dock-17 (LPD-17) classes of ships. The focus is on capturing the massively separated flow from sharp edges of blunt bodies, while ignoring the viscous effects. A parallel, finite-volume flow solver is used with unstructured grids on full-scale ship models for the CFD calculations. Both steady-state and time-accurate results are presented for a wind speed of 15.43 m/s (30 knot) and for six different wind-over-deck angles. The article also reviews other computational and experimental ship airwake research.},
 author = {N Sezer-Uzol and A Sharma and L N Long},
 doi = {10.1243/095441005X30306},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/095441005X30306},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {5},
 pages = {369–392},
 title = {Computational Fluid Dynamics Simulations of Ship Airwake},
 url = {https://doi-org.crai.referencistas.com/10.1243/095441005X30306},
 volume = {219},
 year = {2005q}
}

@article{doi:10.1243/09544100JAERO101,
 abstract = {Abstract Numerical simulation of three-dimensional dynamic stall (DS) of helicopter rotor blades has been undertaken using computational fluid dynamics (CFD) based on the Navier-Stokes equations. The CFD method has been carefully validated against experimental data before being used to study the topology of the three-dimensional DS vortex and the effects of yaw and rotation on its shape and trajectory. The three-dimensional unsteady viscous computations were found to give a wealth of results at the expense of significant amounts of CPU time. To alleviate this problem and to develop a faster model for the aerodynamic loads encountered during three-dimensional DS, a neural network (NN) was put forward. The NN was trained using both CFD and experimental data and was subsequently used as a method for interpolating DS loads between known states for which CFD data were available. The NN was found to work well for such interpolations, although its capability to extrapolate outside the training envelope was somehow limited. Nevertheless, the NN was found to be a very efficient technique for reconstructing three-dimensional DS and has demonstrated great potential as a method for reducing non-linear aerodynamics to a simple computational model.},
 author = {A Spentzos and G Barakos and K Badcock and B Richards},
 doi = {10.1243/09544100JAERO101},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/09544100JAERO101},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {6},
 pages = {605–618},
 title = {Modelling three-dimensional dynamic stall of helicopter blades using                     computational fluid dynamics and neural networks},
 url = {https://doi-org.crai.referencistas.com/10.1243/09544100JAERO101},
 volume = {220},
 year = {2006p}
}

@article{doi:10.1243/09544100JAERO349,
 abstract = {Abstract In this article, aerodynamic optimization of a near-sonic passenger plane was conducted, starting with a conventional tail configuration. The genetic algorithm (GA) combined with the Euler simulations were used to maximize the lift-to-drag ratio (L/ D) at Mach 0.95. First, the wing planform shape, twist, and thickness ratios were optimized. As a result, information about the aerodynamic design parameters was obtained and the optimal individual was designed with high inboard sweep angle, long chord length at the root, and less outboard aerofoil thickness. Then, the point of junction was optimized by applying the optimized wing and fuselage. Through the optimization process, the incidence angles of the wing were reduced, and the shapes of the root leading edge were then sharpened to reduce the interference drag. By its aerodynamic performance, the optimized wing/body configuration showed the aerodynamic feasibility of near-sonic flight.},
 author = {T Watanabe and K Matsushima and K Nakahashi},
 doi = {10.1243/09544100JAERO349},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/09544100JAERO349},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {7},
 pages = {1025–1035},
 title = {Aerodynamic shape optimization of a near-sonic passenger plane using computational fluid dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1243/09544100JAERO349},
 volume = {222},
 year = {2008r}
}

@article{doi:10.1243/09544100JAERO37,
 abstract = {Abstract This study focuses on a simulation strategy that will allow the performance characteristics of an isolated gas turbine engine component, resolved from a detailed, high-fidelity analysis, to be transferred to an engine system analysis carried out at a lower level of resolution. The technique described in this paper is called ‘de-coupled’ high-fidelity analysis and utilizes an object-oriented, zero-dimensional gas turbine modelling and performance simulation system and a three-dimensional computational fluid dynamics (CFD) component model. The technique involves the generation of a component characteristic map without the parallel or iterative execution of the non-dimensional cycle and the three-dimensional CFD model. Therefore, a faster high-fidelity engine performance simulation can be achieved at run time. This paper demonstrates the ‘de-coupled’ approach to component high-fidelity analysis by using a three-dimensional CFD intake model of a high by-pass ratio turbofan as a case study. The CFD model is based on the geometry of the intake of the CFM56-5B2 engine. The CFD-generated performance map can fully define the characteristics of the intake at several operating conditions and power settings, and is subsequently used to provide a more accurate, physics-based estimate of intake performance (i.e. pressure recovery) and hence, engine performance, replacing the default, empirical values within the non-dimensional cycle model. A detailed comparison between the baseline engine performance (empirical pressure recovery) and the engine performance obtained after using the CFD-generated map is presented in this paper. The analysis carried out by this study demonstrates relative changes in the simulated engine performance > 1 per cent.},
 author = {V Pachidis and P Pilidis and I Templalexis and J B Barbosa and N Nantua},
 doi = {10.1243/09544100JAERO37},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/09544100JAERO37},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {1},
 pages = {105–113},
 title = {A de-coupled approach to component high-fidelity analysis using computational fluid dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1243/09544100JAERO37},
 volume = {221},
 year = {2007q}
}

@article{doi:10.1243/09544100JAERO390,
 abstract = {Abstract This article describes a portion of the Propulsion Integration project that deals with the design and optimization of the flow control in the FOI-EIC-01 inlet. It uses both computational fluid dynamics (CFD) and wind tunnel testing in a way that both disciplines strongly interact with each other. Both the wind tunnel and CFD produce inputs for its counterpart and use outputs produced by their counterpart. The main chain of events during the design is the initial CFD analysis followed by the wind tunnel test and final CFD analysis.},
 author = {A Jirásek},
 doi = {10.1243/09544100JAERO390},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/09544100JAERO390},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {4},
 pages = {369–377},
 title = {Example of integrated computational fluid dynamics and experimental studies: Design of flow control in the FOI-EIC-01 inlet},
 url = {https://doi-org.crai.referencistas.com/10.1243/09544100JAERO390},
 volume = {223},
 year = {2009i}
}

@article{doi:10.1243/09544100JAERO399,
 abstract = {Abstract A joint effort between the Georgia Tech Research Institute and the Army Research Lab successfully used a combination of numerical and experimental results to demonstrate the performance of a guidance actuator for a supersonic projectile. The use of computational and experimental approaches greatly enhanced the understanding of how the actuators worked as well enabled the program to be completed for a lower cost than if either the modelling or the experiments had been neglected. Wind tunnel experiments were used with computational fluid dynamics results to provide aerodynamic coefficients for six-degree of freedom (6-DOF) simulations. The 6-DOF simulations were used to predict the performance of the projectile in the range, thus ensuring that good range data were acquired and reducing the necessary number of set-up rounds. It was found that there were cases where experimental methods were necessary, although the modelling provided the researchers with a greater detail of flow interactions and provided forces that were difficult to measure.},
 author = {K C Massey and S I Silton},
 doi = {10.1243/09544100JAERO399},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/09544100JAERO399},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {4},
 pages = {341–355},
 title = {Combining experimental data, computational fluid dynamics, and six-degree of freedom simulation to develop a guidance actuator for a supersonic projectile},
 url = {https://doi-org.crai.referencistas.com/10.1243/09544100JAERO399},
 volume = {223},
 year = {2009m}
}

@article{doi:10.1243/09544100JAERO424,
 abstract = {Abstract A proposed modification of the test section in the Boeing/AFOSR Mach 6 Quiet Tunnel is evaluated using CFD. The new design incorporates a section of increased diameter with the intention of enabling the tunnel to start in the presence of larger blunt models. Cone models with fixed base diameter (and hence fixed blockage ratio) are selected for this study. Cone half-angles from 15° to 75° are examined to ascertain the effect of the strength of the test model shock wave on the tunnel startup. The unsteady, laminar, compressible Navier—Stokes equations are solved. The resulting flowfields are examined to see what affect the shocks and shear layers would have on the quiet test section flow. This study indicates that cone angles ≤20° allow the tunnel to start.},
 author = {H Naiman and D D Knight and S P Schneider},
 doi = {10.1243/09544100JAERO424},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/09544100JAERO424},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {4},
 pages = {407–413},
 title = {Computational redesign of the test section for the Boeing/AFOSR Mach 6 Quiet Tunnel},
 url = {https://doi-org.crai.referencistas.com/10.1243/09544100JAERO424},
 volume = {223},
 year = {2009p}
}

@article{doi:10.1243/09544100JAERO569,
 abstract = {Abstract A computational investigation of the interaction of the vectored primary jet from the main engine with the subsonic flow around a tailless unmanned air vehicle (UAV) configuration, with a cropped diamond wing planform, has been carried out. An initial study has been completed using Reynolds-averaged Navier—Stokes methods in conjunction with the shear-stress transport k—ω model. Various sets of simulations were performed on the basis of quantifying the thrust vectoring (TV) jet entrainment effects by means of aerodynamic forces and moments when key factors are combined, such as angles of thrust velocity deflection at different jet Mach number and angles of attack. It was found that reaction forces induced by the modified pressure on the integrated boundary of the body results in an enhancement of the TV effectiveness in producing the forces and moments required for the flight vehicle trim and manoeuvring. Those reaction forces were found to be independent of the angle of attack.},
 author = {A Buonanno and D Drikakis and C Papachristou and A Savvaris and C Vamvakoulas and C Warsop},
 doi = {10.1243/09544100JAERO569},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/09544100JAERO569},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
 number = {4},
 pages = {387–394},
 title = {Computational investigation of the DEMON unmanned air vehicle thrust vectoring system},
 url = {https://doi-org.crai.referencistas.com/10.1243/09544100JAERO569},
 volume = {224},
 year = {2010c}
}

@article{doi:10.1243/0954411021536351,
 abstract = {Abstract The development of intimal hyperplasia at arterial bypass graft anastomoses is a major factor responsible for graft failure. A revised surgical technique, involving the incorporation of a small section of vein (vein cuff) into the distal anastomosis of polytetrafluoroethylene (PTFE) grafts, alters the distribution of intimal hyperplasia and improves graft performance. Numerical and in vitro flow visualization experiments have been conducted to identify the flow behaviour in the cuffed bypass model and to determine whether the improved performance of the cuffed system can be accounted for by haemodynamic factors. The flowfield at the cuffed anastomosis is characterized by an expansive recirculation. Separation occurs at the graft heel, and at the cuff toe as the blood enters the recipient artery. Wall shear stresses in the vicinity of the cuff heel are low, but high shear stresses and large spatial gradients in the shearing force act for a time on the artery floor. In the conventional model, a less disturbed flow prevails while the gradients of shear stress on the floor are smaller. Aspects of the anastomotic haemodynamics are worsened when the cuff is employed. The superior patency rates of cuffed bypasses may not be explained purely on the basis of local haemodynamic factors.},
 author = {J S Cole and L D Wijesinghe and J K Watterson and D J A Scott},
 doi = {10.1243/0954411021536351},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/0954411021536351},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine},
 note = {PMID:12022420},
 number = {2},
 pages = {135–143},
 title = {Computational and experimental simulations of the haemodynamics at cuffed arterial bypass graft anastomoses},
 url = {https://doi-org.crai.referencistas.com/10.1243/0954411021536351},
 volume = {216},
 year = {2002b}
}

@article{doi:10.1243/09544119JEIM649,
 abstract = {Abstract The topics of verification and validation have increasingly been discussed in the field of computational biomechanics, and many recent articles have applied these concepts in an attempt to build credibility for models of complex biological systems. Verification and validation are evolving techniques that, if used improperly, can lead to false conclusions about a system under study. In basic science, these erroneous conclusions may lead to failure of a subsequent hypothesis, but they can have more profound effects if the model is designed to predict patient outcomes. While several authors have reviewed verification and validation as they pertain to traditional solid and fluid mechanics, it is the intent of this paper to present them in the context of computational biomechanics. Specifically, the task of model validation will be discussed, with a focus on current techniques. It is hoped that this review will encourage investigators to engage and adopt the verification and validation process in an effort to increase peer acceptance of computational biomechanics models.},
 author = {H B Henninger and S P Reese and A E Anderson and J A Weiss},
 doi = {10.1243/09544119JEIM649},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/09544119JEIM649},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine},
 note = {PMID:20839648},
 number = {7},
 pages = {801–812},
 title = {Validation of computational models in biomechanics},
 url = {https://doi-org.crai.referencistas.com/10.1243/09544119JEIM649},
 volume = {224},
 year = {2010g}
}

@article{doi:10.1243/09544119JEIM670,
 abstract = {Abstract This paper investigated the biomechanics of two clinical cases of bone fracture treatments. Both fractures were treated with the same locking compression plate but with different numbers of screws as well as different plate materials. The fracture treated with 12 screws (rigid fixation) failed at 7 weeks with the plate breaking; the fracture with six screws (flexible fixation) endured the entire healing process. It was hypothesized that the plate failure in the unsuccessful case was due to the material fatigue induced by stress concentration in the plate. As the two clinical cases had different fracture locations and different plate materials, finite element simulations were undertaken for each fractured bone fixed by both a rigid and a flexible method. This enabled comparisons to be made between the rigid and flexible fixation methods. The fatigue life was assessed for each fixation method. The results showed that the stress in the rigid fixation methods could be significantly higher than that in flexible fixation methods. The fatigue analyses showed that, with the stress level in flexible fixation (i.e. with fewer screws), the plate was able to endure 2000 days, and that the plate in rigid fixation could fail by fatigue fracture in 20 days. The paper concludes that the rigid fixation method resulted in serious stress concentrations in the plate, which induced fatigue failure. The flexible fixation gave sufficient stability and was better for fracture healing.},
 author = {G Chen and B Schmutz and M Wullschleger and M J Pearcy and M A Schuetz},
 doi = {10.1243/09544119JEIM670},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/09544119JEIM670},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine},
 note = {PMID:20225463},
 number = {1},
 pages = {119–126},
 title = {Computational investigations of mechanical failures of internal plate fixation},
 url = {https://doi-org.crai.referencistas.com/10.1243/09544119JEIM670},
 volume = {224},
 year = {2010d}
}

@article{doi:10.1243/0957650001538146,
 abstract = {Abstract This paper describes the flow analysis in a centrifugal compressor stage using a three-dimensional computational fluid dynamics (CFD) algorithm. The flow unsteadiness arising from the interaction between the impeller and the diffuser has also been analysed using an algorithm suitable for equal or multiple numbers of rotor and diffuser blades. The multi-block, structured grid CFD code TASCflow which provides an approximate solution to the Reynolds-averaged Navier-Stokes equations was used as a basis and algorithm development was undertaken to provide the required capability of modelling the unsteady interactions of the impeller and the diffuser. The centrifugal compressor stage under investigation consists of 28 radial blades (of which 14 are splitter blades) in the impeller and 27 vanes in the diffuser. The presented results and analysis are for off-design flow conditions where experimental results were available for comparison. The results obtained for the steady-state model show a reasonable agreement with the measurements. In general the unsteady flow field obtained shows a reasonable agreement with experimental data and exhibits considerable differences when compared with the steady-state results.},
 author = {A Koumoutsos and A Tourlidakis and R L Elder},
 doi = {10.1243/0957650001538146},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/0957650001538146},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {6},
 pages = {611–633},
 title = {Computational studies of unsteady flows in a centrifugal compressor stage},
 url = {https://doi-org.crai.referencistas.com/10.1243/0957650001538146},
 volume = {214},
 year = {2000g}
}

@article{doi:10.1243/0957650001538353,
 abstract = {Abstract This paper describes a study of the use of computational fluid dynamics (CFD) to investigate the performance of a precalciner vessel at a cement works, In this vessel, limestone, held in suspension, is calcined to calcium oxide and the endothermic reaction is supported by the combustion of coal. Results are presented from a CFD model that contains all the essential features of the precalciner as operated when burning coal. The model fully represents the reactions and fluid dynamics of the precalciner. Previously unidentified features are illustrated. Certain key features at points in the precalciner, where some limited measurements can be made, are compared with the parameters indicated by the computational model. The measurements are consistent with the results calculated by the model indicating fair validation. The CFD data show the following 1 The gases undergo distinct recirculation. 2 The coal particles entering at one inlet have significantly different trajectories and temperature histories from those entering at the second diametrically opposed inlet. 3 There is 90 per cent completion of coal combustion at the exit. 4 73 per cent limestone in the raw meal is calcined to calcined to calcium oxide at the exit from the precalciner. 5 The highest reaction rate of the raw meal is closer to one side of the vessel due to interaction with the gas flows. Future work is proposed which, firstly, will provide further validation of the results so far attained by selective measurements on the precalciner and, secondly, will model the combustion and aerodynamic behaviour of waste-derived fuels in the precalciner vessel, commencing with shredded car tyre chips.},
 author = {D Giddings and C N Eastwick and S J Pickering and K Simmons},
 doi = {10.1243/0957650001538353},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/0957650001538353},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {3},
 pages = {269–280},
 title = {Computational fluid dynamics applied to a cement precalciner},
 url = {https://doi-org.crai.referencistas.com/10.1243/0957650001538353},
 volume = {214},
 year = {2000j}
}

@article{doi:10.1243/095765005X31261,
 abstract = {Abstract A typical centrifugal impeller characterized by a low flow coefficient and cylindrical blades is redesigned by means of an intelligent automatic search program. The procedure consists of a feasible sequential quadratic programming algorithm (Fletcher, R. Practical Methods of optimization, 2000 (Wiley)) coupled to a lazy learning (LL) interpolator 1 to speed-up the process. The program is able to handle geometric constraints to reduce the computational effort devoted to the analysis of non-physical configurations. The objective function evaluator is an in-house developed structured computational fluid dynamics (CFD) code. The LL approx-imator is called each time the stored database can provide a sufficiently accurate performance estimate for a given geometry, thus reducing the effective CFD computations. The impeller is represented by 25 geometric parameters describing the vane in the meridional and s-0 planes, the blade thickness, and the leading edge shape. The optimization is carried out on the impeller design point maximizing the polytropic efficiency with nearly constant flow coefficient and polytropic head. The optimization is accomplished by maintaining unaltered those geometrical parameters which have to be kept fixed in order to make the impeller fit the original stage. The optimization, carried out on a cluster of 16 PCs, is self-learning and leads to a geometry presenting an increased design point efficiency. The program is completely general and can be applied to any component which can be described by a finite number of geometrical parameters and computed by any numerical instrument to provide performance indices. The work presented in this paper was done under the METHOD EC funded project for the implementation of new technologies for optimization of centrifugal compressors.},
 author = {F Martelli and S Pazzi and V Michelassi},
 doi = {10.1243/095765005X31261},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/095765005X31261},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {7},
 pages = {549–557},
 title = {Automatic computational fluid dynamics-based procedure for the optimization of a centrifugal impeller},
 url = {https://doi-org.crai.referencistas.com/10.1243/095765005X31261},
 volume = {219},
 year = {2005m}
}

@article{doi:10.1243/095765005X31306,
 abstract = {Abstract Two-dimensional (orthogonal) steady isothermal flows of a Bingham plastic between two plates, one moving and the other stationary, are discussed. This is done principally to examine and quantify the concept of cooling a smart clutch by throughflow. The fluid is modelled conventionally as an ideal Bingham plastic to verify the subsequent use of a computational fluid dynamics (CFD) package for similar flows in more complex situations but with a futuristic view to including heat transfer, electrical conductance, thermal and shear rate effects on fluid properties, and unsteady motion. The CFD (Fluent) package incorporates a user-defined subroutine facility which allows non-Newtonian constitutive models to be incorporated. Both radial and concentric geometries are considered. The two approaches (conventional analysis and CFD) are seen to complement one another.},
 author = {D J Ellam and R J Atkin and W A Bullough},
 doi = {10.1243/095765005X31306},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/095765005X31306},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {8},
 pages = {639–652},
 title = {Analysis of a smart clutch with cooling flow using two-dimensional bingham plastic analysis and computational fluid dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1243/095765005X31306},
 volume = {219},
 year = {2005i}
}

@article{doi:10.1243/0957650971537105,
 abstract = {Abstract Partial admission in the steam turbine is associated with strong unsteady flow effects on aerodynamic performance. This paper presents a first-of-its-kind computational study of the problem. The unsteady flow field in multiple blade passages and multiple blade rows is governed by the quasi three-dimensional unsteady Navier-Stokes equations, closed by a mixing-length turbulence model. The partial admission is introduced by blocking one segmental arc (or several segmental arcs) of the inlet guide vane of the first stage. The flow equations are solved by using a time-dependent finite volume method. The calculated unsteady force on rotor blades for a turbine stage at partial admission compares well with the corresponding experimental data. The present results show that a cyclic pumping and sucking phenomenon occurs in the rotor blade row of the first stage, resulting in large unsteady loading and marked mixing loss. For a single stage at a given admission rate, a blocking arrangement with two flow segments is shown to be much more detrimental than one arc of admission, because of the extra mixing loss. The results for a two-stage case, however, suggest that the decaying rate of circumferential non-uniformities could be far more important for performance. For this reason, an enhanced mixing loss in the first stage might be beneficial to the overall efficiency of a multistage turbine.},
 author = {L He},
 doi = {10.1243/0957650971537105},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/0957650971537105},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {3},
 pages = {197–205},
 title = {Computation of unsteady flow through steam turbine blade rows at partial admission},
 url = {https://doi-org.crai.referencistas.com/10.1243/0957650971537105},
 volume = {211},
 year = {1997i}
}

@article{doi:10.1243/0957650971537150,
 abstract = {Abstract Although three-dimensional Navier-Stokes computations are coming into use more and more, streamline curvature through-flow computations are still needed, especially for multistage compressors, and where codes which run in minutes rather than hours are preferred. These methods have been made more realistic by taking account of end-wall effects and spanwise mixing by four aerodynamic mechanisms: turbulent diffusion, turbulent convection by secondary flow, spanwise migration of aerofoil boundary layer fluid and spanwise convection of fluid in blade wakes. This paper describes the models adopted in the DRA streamline curvature method for axial compressor design and analysis. Previous papers are summarized briefly before describing the new part of the model—that accounting for aerofoil boundary layers and wakes. Other changes to the previously published annulus wall boundary layer model have been made to enable it to cater for separations and end bends. The resulting code is evaluated against a range of experimental and computational results.},
 author = {J Dunham},
 doi = {10.1243/0957650971537150},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/0957650971537150},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {3},
 pages = {243–251},
 title = {Modelling of spanwise mixing in compressor through-flow computations},
 url = {https://doi-org.crai.referencistas.com/10.1243/0957650971537150},
 volume = {211},
 year = {1997b}
}

@article{doi:10.1243/0957650991537590,
 abstract = {Abstract In this paper a three-dimensional computational methodology for multistage turbomachinery flows is developed, validated and applied in analysing the reduced mass flowrate operation of a four-stage turbine. The flow is modelled by the three-dimensional Favre-Reynolds-averaged Navier-Stokes equations using the Launder-Sharma near-wall k-ɛ turbulence closure, which are integrated using an implicit third-order upwind solver. The computation models the eight blade rows, including the tip clearance gaps of the rotors, using a 6 × 106 points grid, and computes the time-averaged flow using mixing planes between rows. Computational results are compared with measurements for various operating points. After this validation the method is used in a first attempt to analyse the three-dimensional flow at reduced-mass-flow operation.},
 author = {G. A. Gerolymos and C Hanisch},
 doi = {10.1243/0957650991537590},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/0957650991537590},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {4},
 pages = {243–261},
 title = {Multistage three-dimensional Navier-Stokes computation of off-design operation of a four-stage turbine},
 url = {https://doi-org.crai.referencistas.com/10.1243/0957650991537590},
 volume = {213},
 year = {1999j}
}

@article{doi:10.1243/09576509JPE342,
 abstract = {Abstract The paper presents the overall frame, principal steps, and some results of a numerical model of a power boiler furnace that uses pulverized coal, with tangential disposition of the burners. This model demonstrates the application potential of the computational fluid dynamics (CFD) technique and of the computational thermal analysis. Complex three-dimensional furnace geometry, two-phase turbulent flow, coal combustion, and heat transfer have been examined. Two numerical modelling approaches were employed in the investigation, both based on the Euler-Lagrangean two-phase flow concept and on the gas-phase semiempirical k-ɛ turbulence model. The first approach is based on a specially developed comprehensive model of processes in a pulverized coal furnace. In the second case, a commercial CFD code is used to create a three-dimensional furnace model. Some distinctive results concerning the performance of the boiler that was examined are presented graphically. On the basis of a comparison between the simulation predictions and available site measurements, a conclusion can be drawn that these two numerical codes provide realistic insight into furnace processes. Qualitative agreement indicates that the calculations are reasonable and validates the submodels employed.},
 author = {R. V. Filkoski and S. V. Belosevic and I. J. Petrovski and S. N. Oka and M. A. Sijercic},
 doi = {10.1243/09576509JPE342},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/09576509JPE342},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {3},
 pages = {399–409},
 title = {Computational fluid dynamics technique as a tool for description of the phenomena occurring in pulverized coal combustion systems},
 url = {https://doi-org.crai.referencistas.com/10.1243/09576509JPE342},
 volume = {221},
 year = {2007g}
}

@article{doi:10.1243/09576509JPE368,
 abstract = {Abstract A three-dimensional, multi-phase, non-isothermal computational fluid dynamics model of a proton exchange membrane fuel cell has been developed to simulate the hygro and thermal stresses in polymer membrane, which developed during the cell operation. The behaviour of the membrane during the operation of a unit cell has been studied and investigated. The model accounts for both gas and liquid phase in the same computational domain, and thus allows for the implementation of phase change inside the gas diffusion layers. The model includes the transport of gaseous species, liquid water, protons, energy, and water dissolved in the ion-conducting polymer. The new feature of the present model is to incorporate the effect of hygro and thermal stresses into actual three-dimensional, multi-phase, non-isothermal fuel cell model. In addition to hygro—thermal stresses, the model features an algorithm that allows for a more realistic representation of the local activation overpotentials, which leads to improved prediction of the local current density distribution in high accuracy, and therefore, high accuracy prediction of temperature distribution in the cell and then thermal stresses. This model also takes into account convection and diffusion of different species in the channels as well as in the porous gas diffusion layer, heat transfer in the solids as well as in the gases, and electrochemical reactions.},
 author = {M A R Sadiq Al-Baghdadi and H A K Shahad Al-Janabi},
 doi = {10.1243/09576509JPE368},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/09576509JPE368},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {7},
 pages = {941–953},
 title = {Prediction of hygro—thermal stress distribution in proton exchange membranes using a three-dimensional multi-phase computational fluid dynamics model},
 url = {https://doi-org.crai.referencistas.com/10.1243/09576509JPE368},
 volume = {221},
 year = {2007a}
}

@article{doi:10.1243/09576509JPE476,
 abstract = {Abstract A new inverse design method based on non-constant distribution of circulation and axial velocity along the radial direction is used to design a pump impeller and a stator. The radial distribution of axial velocity at the design flowrate is calculated when an empirical radial distribution of circulation is given. Computational fluid dynamics (CFD) modelling of the overall performance and the detailed flow field is performed using TASCflow software. A standard k—ɛ turbulence model combined with standard wall functions is used. A frozen rotor approach is employed to simulate the rotor—stator coupling flow field. The overall performances of the water-jet pump and the radial distribution of velocity components at the exit of the impeller are also measured. Good agreement of the overall performance, such as the pressure rise, the power, and the efficiency, between CFD and experiment is obtained. The detailed velocity fields from inviscid analysis, CFD, and experiment are compared and investigated.},
 author = {H Gao and W L Lin and Z H Du},
 doi = {10.1243/09576509JPE476},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/09576509JPE476},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {5},
 pages = {517–527},
 title = {An investigation of the flow and overall performance in a water-jet axial flow pump based on computational fluid dynamics and inverse design method},
 url = {https://doi-org.crai.referencistas.com/10.1243/09576509JPE476},
 volume = {222},
 year = {2008h}
}

@article{doi:10.1243/09576509JPE662,
 abstract = {Abstract A computational study has been conducted to analyse the performance of a centrifugal compressor with different types of diffusers under various levels of impeller—diffuser interactions. Vaneless (VLD), vaned (VD), low solidity vaned (LSVD), and partial vaned diffusers (PVD) are used for this purpose. The study is carried out using commercial software ANSYS CFX. The interaction level is varied by varying the radial gap between the impeller and diffuser by keeping the diffuser vane at three different radial locations. Numerical simulations have been conducted for four different flow coefficients. At design flow coefficient maximum efficiency occurs when the leading edge is at R3 (ratio of radius of the diffuser leading edge to the impeller tip radius) = 1.10 for all vane-type diffuser configurations. At below design flow coefficient higher stage efficiency occurs when the diffuser vanes are kept far away (R3 = 1.15) and at above design flow coefficient R3 = 1.05 gives better efficiency. The highest diffuser pressure recovery coefficient (Cp) is observed for VD at design flow coefficient. For VLD, the Cp value increases with flow coefficient. In the case of VD and LSVD configurations the exit flow from the impeller is disturbed when the diffuser vanes are closer, and these disturbances are more evident in the last 10 per cent of the impeller flow. In the case of the impeller with PVD the interaction effects are minimum.},
 author = {S Anish and N Sitaram},
 doi = {10.1243/09576509JPE662},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/09576509JPE662},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {2},
 pages = {167–178},
 title = {Computational investigation of impeller—diffuser interaction in a centrifugal compressor with different types of diffusers},
 url = {https://doi-org.crai.referencistas.com/10.1243/09576509JPE662},
 volume = {223},
 year = {2009c}
}

@article{doi:10.1243/09576509JPE697,
 abstract = {Abstract This article presents the computational flow analysis of an axial—centrifugal flow system used for transformer oil cooling in WAP-5 and WAP-7 electric locomotives of Indian Railways. The commercial computational fluid dynamics code ‘FLUENT’ was used for the numerical simulation of the flow system. A standard k—ε turbulence model was employed for turbulence modelling. The flow field through the impeller was analysed to investigate the effect of blade twists and to determine the forces on the blades. A modified impeller design is suggested whose diameter is less than the existing one, which helps in reducing the centrifugal forces. The flow field analysis of the modified impeller shows improvement in the flow through the impeller channels with better performance and reduction in forces on the blades.},
 author = {A Mittal and B K Gandhi and K M Singh},
 doi = {10.1243/09576509JPE697},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/09576509JPE697},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {8},
 pages = {981–989},
 title = {Improvement in the design of a centrifugal impeller for an oil cooling blower system using computational fluid dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1243/09576509JPE697},
 volume = {223},
 year = {2009o}
}

@article{doi:10.1243/09596518JSCE340,
 abstract = {Abstract This paper presents a computational procedure to evaluate the fault tolerance of a linear-constrained model predictive control (LCMPC) scheme for a given actuator fault configuration (AFC). Faults in actuators cause changes in the constraints related to control signals (inputs), which in turn modify the set of MPC feasible solutions. This fact may result in an empty set of admissible solutions for a given control objective. Therefore, the admissibility of the control law facing actuator faults can be determined by knowing the set of feasible solutions. One of the aims of this paper is to provide methods to compute this set and to evaluate the admissibility of the control law for a given AFC, once the control objective and the admissibility criteria have been established. In particular, the admissible solution set for the predictive control problem, including the effect of faults (either through reconfiguration or accommodation), is determined using an algorithm that is implemented using set computations based on zonotopes. Finally, the proposed method is tested on a real application consisting of a part of the Barcelona sewer network.},
 author = {C Ocampo-Martinez and P Guerra and V Puig and J Quevedo},
 doi = {10.1243/09596518JSCE340},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/09596518JSCE340},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part I: Journal of Systems and Control Engineering},
 number = {6},
 pages = {915–926},
 title = {Actuator fault-tolerance evaluation of linear constrained model predictive control using zonotope-based set computations},
 url = {https://doi-org.crai.referencistas.com/10.1243/09596518JSCE340},
 volume = {221},
 year = {2007n}
}

@article{doi:10.1243/09596518JSCE722,
 abstract = {Abstract A computational intelligence (CI)-based approach is presented for prognostics of machine conditions using morphological signal processing (MSP). The machine vibration signals are processed using MSP to extract a novel entropy-based health index (HI) characterizing the signal shape-size complexity for system prognostics. The progression of HI is predicted using CI techniques, namely, recursive neural network (RNN), adaptive neuro-fuzzy inference system (ANFIS), and support vector regression (SVR). Both single- and multi-step ahead predictions were evaluated through benchmark datasets of non-linear, non-stationary, and chaotic time series solutions of Mackey-Glass and Lorenz equations. The prognostic effectiveness of the CI techniques was illustrated using a vibration dataset of a helicopter drive-train system gearbox. For each CI predictor, both training datasets gave almost similar prediction performance. In training, the performance of ANFIS was the best, followed by SVR and RNN. In test, the best performance was obtained with SVR for both single- and multi-step ahead predictions. The results are helpful in understanding the relationship between the system conditions, the corresponding indicating feature, the level of degradation, and their progression.},
 author = {B Samanta and C Nataraj},
 doi = {10.1243/09596518JSCE722},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/09596518JSCE722},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part I: Journal of Systems and Control Engineering},
 number = {8},
 pages = {1095–1109},
 title = {Morphological signal processing and computational intelligence for engineering system prognostics},
 url = {https://doi-org.crai.referencistas.com/10.1243/09596518JSCE722},
 volume = {223},
 year = {2009r}
}

@article{doi:10.1243/13506501JET125,
 abstract = {Abstract The Boussinesq-Cerruti potential functions for the calculation of subsurface stresses and displacements in elastic half-spaces are presented in full and in clear formulation. They were expanded and optimized for fast computational analysis of subsurface stress and displacement fields, including fatigue life computations. A simple technique was developed to accelerate the computations by omitting the stress contribution of parts of the loaded boundary if the said contribution is lower than a predetermined limit. The error of this approximation was defined and formulated for the comparison of various solutions. The technique was applied in a computationally intensive problem involving a rolling-sliding-spinning elastohydrody-namic, elliptical, heavily loaded contact, and three-dimensional subsurface stress and displacement fields were calculated with hundreds of thousands of surface gridpoints and tens of thousands of subsurface gridpoints, followed by the computation of fatigue lives of the contacting solids. Results are presented for a smooth contact, but the method has been applied and is particularly useful for rough contacts as well. The results show a two to ten-fold acceleration of computations, which reduces the computational times by 50-95 per cent for a negligible-to-small loss of accuracy. In real terms, this means reduction of computer time from days to hours or from hours to minutes.},
 author = {George K Nikas},
 doi = {10.1243/13506501JET125},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/13506501JET125},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part J: Journal of Engineering Tribology},
 number = {1},
 pages = {19–28},
 title = {Boussinesq-Cerruti functions and a simple technique for substantial acceleration of subsurface stress computations in elastic half-spaces},
 url = {https://doi-org.crai.referencistas.com/10.1243/13506501JET125},
 volume = {220},
 year = {2006m}
}

@article{doi:10.1243/13506501JET319,
 abstract = {An analysis of a lubricated conformal contact is carried out to study the effect of surface texture on bearing friction and load carrying capacity using computational fluid dynamics. The work focuses on a journal bearing with several dimples. Two- and three-dimensional bearing geometries are considered. The full Navier—Stokes equations are solved under steady-state conditions with a multi-phase flow cavitation model. The coefficient of friction can be reduced if a texture of suitable geometry is introduced. This can be achieved either in the region of maximum hydrodynamic pressure for a bearing with high eccentricity ratio or just downstream of the maximum film for a bearing with low eccentricity ratio. An additional pressure build-up produced as a result of the surface texture has been shown at low eccentricity ratios.},
 author = {S Cupillard and S Glavatskih and M J Cervantes},
 doi = {10.1243/13506501JET319},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/13506501JET319},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part J: Journal of Engineering Tribology},
 number = {2},
 pages = {97–107},
 title = {Computational fluid dynamics analysis of a journal bearing with surface texturing},
 url = {https://doi-org.crai.referencistas.com/10.1243/13506501JET319},
 volume = {222},
 year = {2008e}
}

@article{doi:10.1243/1350650981542128,
 abstract = {Abstract The present paper reviews tribological phenomena in metal machining from computational aspects. Emphasis is laid on the interaction between the mechanical aspects of tribology and the characteristics of the cutting process. Firstly, the fundamentals of the mechanics and physics of cutting processes are outlined. This is extended to friction and lubrication at the tool-workpiece interface and tool wear. On the basis of this quantitative knowledge, the interaction of tribology with a cutting process is demonstrated by means of computer simulation including machinability of free-cutting steels, cutting fluid action and prediction of tool wear. Finally, concluding remarks are devoted to the interaction of tribology with wider aspects of cutting towards the goal of ‘better machining‘.},
 author = {K Maekawa},
 doi = {10.1243/1350650981542128},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/1350650981542128},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part J: Journal of Engineering Tribology},
 number = {4},
 pages = {307–318},
 title = {Computational aspects of tribology in metal machining},
 url = {https://doi-org.crai.referencistas.com/10.1243/1350650981542128},
 volume = {212},
 year = {1998m}
}

@article{doi:10.1243/1350650981542236,
 abstract = {Abstract This paper demonstrates the suitability of using computational fluid dynamics software for solving steady state hydrodynamic lubrication problems pertaining to slider bearings, step bearings, journal bearings and squeeze-film dampers under conditions of constant unidirectional or rotating loading. The relevance of the inertia and viscous terms which are neglected in the derivation of the Reynolds equation are briefly investigated for the above bearing and damper configurations and it is shown that the neglected viscous terms have negligible effect whereas the inertia effect predictions agree reasonably well with those reported in the literature.},
 author = {P. Y. P. Chen and E. J. Hahn},
 doi = {10.1243/1350650981542236},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/1350650981542236},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part J: Journal of Engineering Tribology},
 number = {6},
 pages = {427–436},
 title = {Use of computational fluid dynamics in hydrodynamic lubrication},
 url = {https://doi-org.crai.referencistas.com/10.1243/1350650981542236},
 volume = {212},
 year = {1998c}
}

@article{doi:10.1243/14644193JMBD84,
 abstract = {Abstract Experimental studies using human volunteers are limited to low acceleration impacts while whole cadavers, isolated cervical spine specimens, and impact dummies do not normally reflect the true human response. Computational modelling offers a cost effective and useful alternative to experimental methods to study the behaviour of the human head and neck and their response to impacts to gain insight into injury mechanisms. This article reports the approach used in the development of a detailed multi-body computational model that reproduces the head and cervical spine of an adult in the upright posture representing the natural lordosis of the neck with mid-sagittal symmetry. The model comprises simplified but accurate representations of the nine rigid bodies representing the head, seven cervical vertebrae of the neck, and the first thoracic vertebra, as well as the soft tissues, i.e. muscles, ligaments, and intervertebral discs. The rigid bodies are interconnected by non-linear viscoelastic intervertebral discs elements in flexion and extension, non-linear viscoelastic ligaments and supported through frictionless facet joints. Eighteen muscle groups and 69 individual muscle segments of the head and neck on each side of the body are also included in the model. Curving the muscle around the vertebrae and soft tissues of the neck during the motion of the neck is also modelled. Simulation is handled by the multi-body dynamic software MSC.visuaNastran4D. Muscle mechanics is handled by an external application, Virtual Muscle, in conjunction with MSC.visuaNastran4D that provides realistic muscle properties. Intervertebral discs are modelled as non-linear viscoelastic material in flexion and extension but represented by ‘bushing elements’ in Visual Nastran 4D, which allows stiffness and damping properties to be assigned to a joint with required number of degrees of freedom of the motion. Ligaments are modelled as non-linear viscoelastic spring-damper elements. As the model is constructed, the cervical spine motion segments are validated by comparing the segment response to published experimental data on the load-displacement behaviour for both small and large static loads. The response of the entire ligamentous cervical spine model to quasi-static flexion and extension loading is also compared to experimental data to validate the model before the effect of muscle stiffening is included. Moreover the moment-generating capacity of the neck muscle elements has been compared against in vivo experimental data. The main and coupled motions of the model segments are shown to be accurate and realistic, and the whole model is in good agreement with experimental findings from actual human cervical spine specimens. It has been shown that the model can predict the loads and deformations of the individual soft-tissue elements making the model suitable for injury analysis. The validation of the muscle elements shows the morphometric values, origins, and insertions selected to be reasonable. The muscles can be activated as required, providing a more realistic representation of the human head and neck. The curved musculature results in a more realistic representation of the change in muscle length during the head and neck motion.},
 author = {D W van Lopik and M Acar},
 doi = {10.1243/14644193JMBD84},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/14644193JMBD84},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part K: Journal of Multi-body Dynamics},
 number = {2},
 pages = {175–197},
 title = {Development of a multi-body computational model of human head and neck},
 url = {https://doi-org.crai.referencistas.com/10.1243/14644193JMBD84},
 volume = {221},
 year = {2007l}
}

@article{doi:10.1243/14644193JMBD89,
 abstract = {Abstract A multi-body computational model of the human head and neck was previously shown to be in good agreement with experimental findings from actual human cervical spine specimens. The model segments were tested in three directions of loading showing main and coupled motions to be accurate and realistic. The model’s ability to predict the dynamic response of the head and neck, when subjected to acceleration pulses representing frontal, lateral, and rear-end impacts, is verified using experimental data derived from sled acceleration tests with human volunteers for 15 g frontal and 7 g lateral impacts and from isolated cervical spine specimen tests for rear-end impacts. Response corridors based on sled acceleration tests with human volunteers for frontal and lateral impacts are used to evaluate the model and investigate the effect of muscle activation on the head-neck motion. Firstly, the impacts are simulated with both passive and active muscle behaviour. Secondly, the local loads in the soft-tissue elements during the frontal impact are analysed. For rear-end impact simulation experiments using ligamentous isolated cervical spine specimens are used to evaluate the model performance before investigating the effects of muscle tensioning. Good agreement with human volunteer response corridors resulting from frontal and lateral impacts, and isolated cervical spine specimen sled test rear-end impact experiments is demonstrated for the model, highlighting the important role the muscles of the neck play in the head-neck response to acceleration impacts. The model is shown to be able to predict the loads and deformations of the cervical spine components making it suitable for injury analysis.},
 author = {D W van Lopik and M Acar},
 doi = {10.1243/14644193JMBD89},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/14644193JMBD89},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part K: Journal of Multi-body Dynamics},
 number = {2},
 pages = {199–217},
 title = {Dynamic verification of a multi-body computational model of human head and neck for frontal, lateral, and rear impacts},
 url = {https://doi-org.crai.referencistas.com/10.1243/14644193JMBD89},
 volume = {221},
 year = {2007m}
}

@article{doi:10.1243/1468087001545164,
 abstract = {Abstract An overview over flamelet modelling for turbulent non-premixed combustion is given. A short review of previous contributions to simulations of direct injection (DI) diesel engine combustion using the representative interactive flamelet concept is presented. A surrogate fuel consisting of 70 per cent (liquid volume) n-decane and 30 per cent α-methyl-naphthalene is experimentally compared to real diesel fuel. The resemblance of their physical and chemical properties is shown to result in very similar combustion and pollutant formation for both fuels. In order to account for variations of the scalar dissipation rate within the computational domain, a method using multiple flamelets, called the Eulerian particle flamelet model, is used. A strategy is described for subdividing the computational domain and assigning the resulting subdomains to different flamelet histories represented by Eulerian marker particles. Experiments conducted with an Audi DI diesel engine and diesel fuel are compared to simulations using the surrogate fuel. The use of multiple flamelets, each having a different history, significantly improves the description of the ignition phase, leading to a better prediction of pressure, heat release and exhaust emissions of soot and NOx. The effect of the number of flamelet particles on the predictions is discussed.},
 author = {H Barths and C Hasse and N Peters},
 doi = {10.1243/1468087001545164},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/1468087001545164},
 journal = {International Journal of Engine Research},
 number = {3},
 pages = {249–267},
 title = {Computational fluid dynamics modelling of non-premixed combustion in direct injection diesel engines},
 url = {https://doi-org.crai.referencistas.com/10.1243/1468087001545164},
 volume = {1},
 year = {2000a}
}

@article{doi:10.1243/146808703321533268,
 abstract = {Abstract This study was carried out to assess the ability of a computational fluid dynamics (CFD) code to predict the scavenging flow in the cylinder of a two-stroke cycle engine. Predictions were obtained from a CFD simulation of the flow within the cylinder. Due to the apparent sym-metry of the engine port layout, only half of the cylinder volume was modelled. Boundary conditions for the CFD model were obtained from experimentally measured pressure-time histories in the crankcase and exhaust. The two-stroke cycle engine was modified to allow laser Doppler velocimetry (LDV) measurements to be made of the in-cylinder flow. The engine was operated under motoring conditions at 500 r/min with a delivery ratio of 0.7. Although the engine scavenge port layout was geometrically symmetrical, an asymmetrical flow field was identified in the cylinder. As a result of this, a direct comparison of the in-cylinder LDV measured and CFD computed results was not possible. However, LDV and CFD results for the in-cylinder flow are presented to help highlight the dissimilarity between the measured and predicted flow fields. Two-dimensional LDV measurements were made in the cylinder at the transfer ports for a portion of the cycle. A comparison of these LDV measurements with CFD predictions of the in-cylinder velocities at the same locations showed that the CFD model could replicate reasonably well the general trend of the flow. The measured cylinder averaged turbulent kinetic energy was compared with that of the CFD model. The qualitative trend of the overall turbulence generating capacity of the engine was well replicated by the CFD model.},
 author = {J. P. Creaven and R Fleck and R. G. Kenny and G Cunningham},
 doi = {10.1243/146808703321533268},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/146808703321533268},
 journal = {International Journal of Engine Research},
 number = {2},
 pages = {103–128},
 title = {Laser Doppler velocimetry measurements of flow within the cylinder of a motored tow-stroke cycle engine-comparison with some computational fluid dynamics predictions},
 url = {https://doi-org.crai.referencistas.com/10.1243/146808703321533268},
 volume = {4},
 year = {2003c}
}

@article{doi:10.1243/146808703322743921,
 abstract = {Abstract Fuel impingement and film formation are critical issues for the goal of reducing hydrocarbon emissions from automotive gasoline direct injection engines. This computational study compares fuel impingement from a simulated air-assisted spray with that of a simulated spray from a swirl atomizer. Impingement occurs on a flat plate in a low ambient pressure case that approximates an early-injection condition and in a high ambient pressure case that approximates a late-injection condition. The simulated results suggest that the air-assisted injector produces substantially less fuel film and impingement than the swirl atomizer because of faster vaporization of smaller droplets and slower penetration. Although the swirl atomizer produces more splashing, much of the splashed fuel mass returns to the plate. More detailed numerical studies of the air-assisted injector were also conducted to ascertain the effects of turning the vaporization model off, employing a very fine grid, having angled impingement and changing the injector design to increase the initial spray angle.},
 author = {D-L Chang and C-f F Lee},
 doi = {10.1243/146808703322743921},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/146808703322743921},
 journal = {International Journal of Engine Research},
 number = {4},
 pages = {331–345},
 title = {Computational studies of air-assisted spray impingement on a flat plate},
 url = {https://doi-org.crai.referencistas.com/10.1243/146808703322743921},
 volume = {4},
 year = {2003c}
}

@article{doi:10.1243/146808705X30486,
 abstract = {Abstract Velocity measurements were made in the catalyst system of a firing engine using a one-component laser Doppler velocimetry system. The 1.4 l engine was operated at 2000 r/min and 88 per cent full-load condition. Velocity pulsations were observed in one of the runners supplying the catalyst and downstream of the catalyst. The velocity pulsations measured downstream of the catalyst enabled the mean velocity profile to be found. Observations were compared with simulations obtained from a coupling of the Star-CD computational fluid dynamics code, which modelled the catalyst as a three-dimensional component, with the Ricardo WAVE one-dimensional engine-cycle simulation code. The velocities in the runners were predicted to fluctuate between –65 m/s and 240 m/s. The observed velocity showed a similar pulse shape but a smaller magnitude of reversed flow. The velocities downstream of the catalyst were predicted to fluctuate between –5 and 22 m/s. The observed velocities showed smaller amplitude pulsations and significantly lower magnitudes of reversed flow, consistent with the input runner observations. The coupled simulation was shown to give good qualitative agreement with measurements, with quantitative predictions being most accurate near the catalyst centre but less accurate at locations closer to the outer wall.},
 author = {S F Benjamin and W Disdale and Z Liu and C A Roberts and H Zhao},
 doi = {10.1243/146808705X30486},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/146808705X30486},
 journal = {International Journal of Engine Research},
 number = {1},
 pages = {29–40},
 title = {Velocity Predictions from a Coupled One-Dimensional/Three-Dimensional Computational Fluid Dynamics Simulation Compared with Measurements in the Catalyst System of a Firing Engine},
 url = {https://doi-org.crai.referencistas.com/10.1243/146808705X30486},
 volume = {7},
 year = {2006c}
}

@article{doi:10.1243/146808705X30503,
 abstract = {Abstract Modelling the premixed charge compression ignition (PCCI) engine requires a balanced approach that captures both fluid motion as well as low- and high-temperature fuel oxidation. A fully integrated computational fluid dynamics (CFD) and chemistry scheme (i.e. detailed chemical kinetics solved in every cell of the CFD grid) would be the ideal PCCI modelling approach, but is computationally very expensive. As a result, modelling assumptions are required in order to develop tools that are computationally efficient, yet maintain an acceptable degree of accuracy. Multi-zone models have been previously shown accurately to capture geometry-dependent processes in homogeneous charge compression ignition (HCCI) engines. In the presented work, KIVA-3V is fully coupled with a multi-zone model with detailed chemical kinetics. Computational efficiency is achieved by utilizing a low-resolution discretization to solve detailed chemical kinetics in the multi-zone model compared with a relatively high-resolution CFD solution. The multi-zone model communicates with KIVA-3V at each computational timestep, as in the ideal fully integrated case. The composition of the cells, however, is mapped back and forth between KTVA-3V and the multi-zone model, introducing significant computational time savings. The methodology uses a novel re-mapping technique that can account for both temperature and composition non-uniformities in the cylinder. Validation cases were developed by solving the detailed chemistry in every cell of a KIVA-3V grid. The new methodology shows very good agreement with the detailed solutions in terms of ignition timing, burn duration, and emissions.},
 author = {A Babajimopoulos and D N Assanis and D L Flowers and S M Aceves and R P Hessel},
 doi = {10.1243/146808705X30503},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/146808705X30503},
 journal = {International Journal of Engine Research},
 number = {5},
 pages = {497–512},
 title = {A fully coupled computational fluid dynamics and multi-zone model with detailed chemical kinetics for the simulation of premixed charge compression ignition engines},
 url = {https://doi-org.crai.referencistas.com/10.1243/146808705X30503},
 volume = {6},
 year = {2005a}
}

@article{doi:10.1243/146808705X30585,
 abstract = {Abstract We analyzed the interrelationships between mixture heterogeneity and reaction in a premixed charge compression ignition (PCCI) combustion, using large eddy simulation (LES) in conjunction with a reaction kinetics model. The aim of this analysis is to find the statistical characteristics of the mixture heterogeneity in a turbulent flowfield for moderating the PCCI combustion and for increasing an output limit, which is restricted by a severe knock. Several different initial conditions of heterogeneity of an air-fuel or air-fuel-EGR gas mixture were given at the intake valve closing time by a new method, which generated statistically reasonable turbulent fluctuations in both velocity and fuel mass fraction fields. The autoignition and combustion behaviours were analysed for several different sets of the r.m.s. and the length scale of the fluctuations in the fuel mass fraction. The analyses show that the combination of a larger r.m.s. value and a longer-length scale of the fluctuations in fuel mass fraction is effective to slow the combustion in a hot flame reaction phase and to avoid knocking. The analytical results also show that the heterogeneous distribution of an EGR gas has a considerable effect in making the combustion slower, even when a fuel-air mixture is homogeneous.},
 author = {K Saijyo and T Kojima and K Nishiwaki},
 doi = {10.1243/146808705X30585},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/146808705X30585},
 journal = {International Journal of Engine Research},
 number = {5},
 pages = {487–495},
 title = {Computational fluid dynamics analysis of the effect of mixture heterogeneity on combustion process in a premixed charge compression ignition engine},
 url = {https://doi-org.crai.referencistas.com/10.1243/146808705X30585},
 volume = {6},
 year = {2005r}
}

@article{doi:10.1243/14680874JER00308,
 abstract = {Abstract The objective of the present work is to study soot-EGR and soot-NOx tradeoff for premixed charge compression ignition (PCCI) combustion at a light-load (4 bar BMEP) operation of a medium-duty, large-bore, direct-injection diesel engine using multi-dimensional computational analysis. The simulations are performed using KIVA-3V computer code coupled with reduced chemical kinetics. Low (40 per cent) to heavy (70 per cent) EGR rates are applied to study the effects of reduced intake oxygen concentration on combustion and emissions. Model predictions of cylinder pressure and emissions are first validated against experimental data. Model-predicted temporal and spatial evolution of in-cylinder mixture in φ—T coordinates is then used to explain the fundamentals of PCCI combustion and emissions. Using computational analysis, it is shown that lower exhaust soot emissions for the lower EGR dilution cases (45 per cent) are due to higher soot oxidation rates, and lower exhaust soot emissions for very high EGR rates (70 per cent) are due to lower soot formation rates. The trend seen in NOx emissions are primarily attributable to the inert effect of EGR dilution and varied ignition delay caused by varied percentages of oxygen in the intake mixture.},
 author = {R Diwakar and S Singh},
 doi = {10.1243/14680874JER00308},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/14680874JER00308},
 journal = {International Journal of Engine Research},
 number = {3},
 pages = {195–214},
 title = {NOx and soot reduction in diesel engine premixed charge compression ignition combustion: a computational investigation},
 url = {https://doi-org.crai.referencistas.com/10.1243/14680874JER00308},
 volume = {9},
 year = {2008e}
}

@article{doi:10.1243/14680874JER537,
 abstract = {This paper presents a comprehensive investigation of a compression—ignition (CI) heavy-duty engine fuelled with diesel and gasoline-like fuels. A state-of-the-art engine computational fluid dynamics (CFD) tool was used to explore the influences of the physical and chemical properties of diesel and gasoline-like fuels (no. 91 gasoline and E10) on spray development, auto-ignition and combustion processes, and pollutant formation. The CFD simulation results were found to be consistent with available experimental measurements, both qualitatively and quantitatively. The results indicate that gasoline-like fuels are able to achieve better premixed charge in CI engines owing to their higher volatility and lower ignitability compared to diesel fuel, which promotes efficient, clean, and low temperature combustion. However, the high combustion pressure rise rate becomes problematic under some circumstances, especially for high octane number fuels, such as E10. Both previous experimental measurements and the present numerical results show that gasoline-like fuels have great potential to be used in future CI engines, but the injection strategies and injection system have to be optimized based on the fuel properties.},
 author = {Y Shi and Y Wang and R D Reitz},
 doi = {10.1243/14680874JER537},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/14680874JER537},
 journal = {International Journal of Engine Research},
 number = {5},
 pages = {355–373},
 title = {Computational Fluid Dynamic Modelling a Heavy-Duty Compression-Ignition Engine Fuelled with Diesel and Gasoline-Like Fuels},
 url = {https://doi-org.crai.referencistas.com/10.1243/14680874JER537},
 volume = {11},
 year = {2010o}
}

@article{doi:10.1243/JMES_JOUR_1982_024_041_02,
 abstract = {Multidimensional computational analysis of fluid flow is usually done by segmented iterative methods, as the equations sets generated are too large to permit simultaneous solution. Frequently the need arises to compute values for variables which must remain bounded for physical reasons. In two-phase computation, for example, the volume fraction is restricted to values between 0 and 1, but iterative procedures often return intermediate values which violate these bounds. It is fairly straightforward to prevent negative values, however no satisfactory method of imposing the upper limit has been published. A method of smoothly applying the limit in reversible fashion is outlined in this note.},
 author = {M. B. Carver},
 doi = {10.1243/JMES_JOUR_1982_024_041_02},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/JMES_JOUR_1982_024_041_02},
 journal = {Journal of Mechanical Engineering Science},
 number = {4},
 pages = {221–224},
 title = {A Method of Limiting Intermediate Values of Volume Fraction in Iterative Two-Fluid Computations},
 url = {https://doi-org.crai.referencistas.com/10.1243/JMES_JOUR_1982_024_041_02},
 volume = {24},
 year = {1982a}
}

@article{doi:10.1243/PIME_PROC_1976_190_049_02,
 abstract = {SYNOPSIS The effect upon computational cost and accuracy of various assumptions in diesel combustion models was investigated by comparing the performance of a range of models and by comparing the results of computation with experimental results.},
 author = {T. J. Williams and N. D. Whitehouse},
 doi = {10.1243/PIME_PROC_1976_190_049_02},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/PIME_PROC_1976_190_049_02},
 journal = {Proceedings of the Institution of Mechanical Engineers},
 number = {1},
 pages = {467–475},
 title = {Investigation into Some Aspects of the Computation of Diesel Engine Combustion},
 url = {https://doi-org.crai.referencistas.com/10.1243/PIME_PROC_1976_190_049_02},
 volume = {190},
 year = {1976r}
}

@article{doi:10.1243/PIME_PROC_1977_191_007_02,
 abstract = {Numerical techniques for the calculation of velocity and temperature distributions in heated ducts have proved accurate but expensive in computer time and capacity. It is worth investigating to what extent simplification is possible without loss of accuracy. Entry-length heat transfer to upward laminar flow with combined convection in a vertical tube is taken as typical. Comparison is made between measured values and, first, a full numerical solution for constant thermophysical properties (viscosity and thermal diffusivity), secondly, the same solution but allowing for their individual and combined variation with temperature and, thirdly, a solution which assumes a series of truncated versions of the fully developed temperature distribution to establish corresponding velocity profiles, allowing for temperature-dependent properties.},
 author = {M. W. Collins and P. H. G. Allen and O. Szpiro},
 doi = {10.1243/PIME_PROC_1977_191_007_02},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/PIME_PROC_1977_191_007_02},
 journal = {Proceedings of the Institution of Mechanical Engineers},
 number = {1},
 pages = {19–29},
 title = {Computational Methods for Entry Length Heat Transfer by Combined Laminar Convection in Vertical Tubes},
 url = {https://doi-org.crai.referencistas.com/10.1243/PIME_PROC_1977_191_007_02},
 volume = {191},
 year = {1977d}
}

@article{doi:10.1243/PIME_PROC_1991_205_004_02,
 abstract = {With the vastly increased capabilities of computers within the past decade the teaching of thermodynamics and fluid dynamics must evolve to encompass more treatment of numerical methods than before. Accepting that CFD will occupy a significant portion of undergraduate and postgraduate syllabuses in the 1990s, this paper looks at certain conceptual difficulties associated with conventional approaches in CFD and the impact of these on thermofluids education. An alternative approach to CFD which avoids complicated mathematics is described. It is argued that such an approach can promote a better understanding of fluid behavioural mechanisms and, thereby, should be of benefit to both research and teaching in thermofluids.},
 author = {J G L Aston},
 doi = {10.1243/PIME_PROC_1991_205_004_02},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/PIME_PROC_1991_205_004_02},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
 number = {1},
 pages = {11–23},
 title = {Computational Fluid Dynamics without Complex Mathematics: The Advantages for Thermofluids Education},
 url = {https://doi-org.crai.referencistas.com/10.1243/PIME_PROC_1991_205_004_02},
 volume = {205},
 year = {1991a}
}

@article{doi:10.1243/PIME_PROC_1991_205_195_02,
 abstract = {The well-known glove box arrangement used in the nuclear industry maintains a pressure depression within a contaminated volume while allowing passage of a small purge flow to the filtration plant. Conventionally, if a glove tears for example, an electromechanical control system restores the depression by opening and regulating an extra connection to the filtration duct. However, faster response and lower maintenance overheads result from replacing the conventional system with the fluidic vortex amplifier (VA) but coupled ventilation system and VA dynamics can be unstable. Designers needed a tool to predict oscillatory performance and, ideally, to advise on the strengths of influence of network parameters on stability. A system modelling approach is presented in which pipework dynamics, including steady state friction, are presented by transmission line modelling (TLM). A semi-empirical feedback model of linearized VA dynamics has been developed and shown to be substantially correct. The model’s restriction to small signals does not adversely affect investigation of perturbation growth or decay. Partial transmission line representation of the VA model with special measures for feedback allowed time-domain simulations of network perturbations. However, this approach is indirect and it was possible to convert the whole VA model to transmission lines and then reformulate the TLM scheme into a direct, quantitative stability analysis. Extension of the technique provided a means of assessing the sensitivity of the stability to simple network alterations. The method has been programmed on an 80386 PC and successfully used on a large ventilation system. Success in the present context of dynamic stability for fluidic nuclear ventilation networks should be repeatable in other areas since the approach is applicable to any TLM-amenable system, be it fluid, mechanical, electrical or mixed.},
 author = {R F Boucher and M P Kissane},
 doi = {10.1243/PIME_PROC_1991_205_195_02},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/PIME_PROC_1991_205_195_02},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part E: Journal of Process Mechanical Engineering},
 number = {1},
 pages = {39–43},
 title = {A Computational Modelling Approach to the Design of Stable Ventilation Systems},
 url = {https://doi-org.crai.referencistas.com/10.1243/PIME_PROC_1991_205_195_02},
 volume = {205},
 year = {1991b}
}

@article{doi:10.1243/PIME_PROC_1994_208_282_02,
 abstract = {The local blood flow in arteries, especially at bends and bifurcations, is correlated with the distribution of atherosclerotic lesions. The flow is three-dimensional, unsteady and difficult to measure in vivo. In this paper a numerical treatment of blood flow in general three-dimensional arterial bifurcations is presented. The flow is assumed to be laminar and incompressible, the blood non-Newtonian and the vessel wall rigid. The three-dimensional time-dependent Navier-Stokes equations are employed to describe the flow, and a newly developed computational fluid dynamics (CFD) code AST EC based on finite volume methods is used to solve the equations. A comprehensive range of code validations has been carried out. Good agreement between numerical predictions and in vitro model data is demonstrated, but the correlation with in vivo measurements is less satisfactory. Effects of the non-Newtonian viscosity have also been investigated. It is demonstrated that differences between Newtonian and non-Newtonian flows occur mainly in regions of flow separation. With the non-Newtonian fluid, the duration of flow separation is shorter and the reverse flow is weaker. Nevertheless, it does not have significant effects on the basic features of the flow field. As for the magnitude of wall shear stress, the effect of non-Newtonian viscosity might not be negligible.},
 author = {X Y Xu and M W Collins},
 doi = {10.1243/PIME_PROC_1994_208_282_02},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/PIME_PROC_1994_208_282_02},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine},
 number = {3},
 pages = {163–175},
 title = {Studies of Blood Flow in Arterial Bifurcations Using Computational Fluid Dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1243/PIME_PROC_1994_208_282_02},
 volume = {208},
 year = {1994t}
}

@article{doi:10.1243/PIME_PROC_1995_209_159_02,
 abstract = {The reasons for the recent growth of computational fluid dynamics (CFD) for industrial and environmental applications are briefly explained, and thence why the users and managers of CFD systems should understand the main underlying principles, the different options and future possibilities of this essential element in modern engineering design. The paper reviews in non-mathematical terms (a) current concepts of turbulence and the mechanisms that need to be modelled; (b) the three levels of computer code, classified according to their output level, their requirements for data and computational resources; (c) the way the codes are constructed and used; (d) how the results have to be interpreted and qualified for all practical applications; and (e) finally how CFD is developing, with better accuracy in specific areas and applications to more complex problems (with thermodynamics, chemistry, etc.) and even to flows where the turbulence is controlled interactively.},
 author = {J C R Hunt},
 doi = {10.1243/PIME_PROC_1995_209_159_02},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/PIME_PROC_1995_209_159_02},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
 number = {5},
 pages = {297–314},
 title = {Practical and Fundamental Developments in the Computational Modelling of Fluid Flows},
 url = {https://doi-org.crai.referencistas.com/10.1243/PIME_PROC_1995_209_159_02},
 volume = {209},
 year = {1995g}
}

@article{doi:10.1243/PIME_PROC_1995_209_412_02,
 abstract = {The use of computational fluid dynamics (CFD) techniques enables performance predictions of bearing designs to be made when the usual operating assumptions of the Reynolds equation Jail to hold. This paper addresses the application of a full three-dimensional thermohydrodynamic CFD approach to journal bearings. The journal/shaft may extend beyond the bearing length and the rotation effect is accounted for in the thermal transport process. A circumferentially uniform shaft surface temperature is not assumed. Cavitation modelling is based on averaged lubricant/vapour properties and does not set pressures directly, allowing sub-ambient pressures to be predicted. Lubricant inlet grooves are incorporated with conservation of mass and the possibility of backflow. The modelling is validated against published experimental work on fully circumferential, single inlet and two-inlet circular bore bearings. The predicted and experimental results are in general agreement, although the predicted cyclic variation of journal surface temperature is less than the experimental value. However, an assumption in the predictions was of a non-orbiting journal. The techniques developed may, in principle, be extended to the orbiting journal case providing a dynamic cavitation model can be formulated.},
 author = {P G Tucker and P S Keogh},
 doi = {10.1243/PIME_PROC_1995_209_412_02},
 eprint = {https://doi-org.crai.referencistas.com/10.1243/PIME_PROC_1995_209_412_02},
 journal = {Proceedings of the Institution of Mechanical Engineers, Part J: Journal of Engineering Tribology},
 number = {2},
 pages = {99–108},
 title = {A Generalized Computational Fluid Dynamics Approach for Journal Bearing Performance Prediction},
 url = {https://doi-org.crai.referencistas.com/10.1243/PIME_PROC_1995_209_412_02},
 volume = {209},
 year = {1995r}
}

@article{doi:10.1255/ejms.1019,
 abstract = {Twenty-two substituted 7-hydroxycoumarins were studied by negative ion electrospray ionization collision-induced dissociation (CID) mass spectrometry. Fragmentation pathways were also investigated by computation method using the B3LYP density functional theory. In general, the most important fragmentations of the 7-hydroxycoumarin [M – H]− ions were the elimination of CO2 and CO which agreed with the calculated energies of the proposed fragmentation reactions. In most cases, methyl group elimination was also favorable. Methyl group elimination occurred in three different ways, the most interesting being hydrogen rearrangement from a neighboring alkyl group to a ring carbon, which led to a benzyl radical formation. In some cases, CH2CO elimination was observed as well. Isomeric compounds gave rise to different CID spectra.},
 author = {Juri Timonen and Paula Aulaskari and Pipsa Hirva and Pirjo Vainiotalo},
 doi = {10.1255/ejms.1019},
 eprint = {https://doi-org.crai.referencistas.com/10.1255/ejms.1019},
 journal = {European Journal of Mass Spectrometry},
 number = {5},
 pages = {595–603},
 title = {Negative Ion Electrospray Ionization Mass Spectrometry and Computational Studies on Substituted 7-Hydroxycoumarins},
 url = {https://doi-org.crai.referencistas.com/10.1255/ejms.1019},
 volume = {15},
 year = {2009r}
}

@article{doi:10.1255/ejms.327,
 abstract = {The metastable dissociations of a series of simple inorganic gaseous ions of general formula (H,F,M)+, including NF3H+ (M = NF2), N2F2H+ (M = FN2), F2NOH2+ (M = FNOH) and FNOH+ (M = NO) have been investigated using classical dynamics in order to rationalise their mass-analysed ion kinetic energy (MIKE) spectra which, from earlier experimental studies, showed the systematic loss of neutral hydrofluoric acid, HF, accompanied by a fairly large release of translational energy (T). All the simulations were initiated in correspondence of the transition structures involved in the above decompositions and expanded on the related Hartree–Fock potential energy surfaces, calculated “on the fly” during the evolution of the trajectories according to the Dynamic Reaction Path methodology. The initial associated momenta were deduced by performing a standard kinetic analysis of the above dissociations taking into account the specific features of the MIKE experiments. For all the above ions, the resulting computed Ts are in reasonable agreement with the experimental values. In addition, from a more accurate analysis of the trajectories, it could be possible to appreciate qualitatively both the specific role of the internal degrees of freedom of the decomposing ions and the dynamic and energetic influence of the HF–M+ ion–neutral complex located between the dissociation products and the tight transition structure involved in the above decompositions.},
 author = {Massimiliano Aschi and Felice Grandinetti},
 doi = {10.1255/ejms.327},
 eprint = {https://doi-org.crai.referencistas.com/10.1255/ejms.327},
 journal = {European Journal of Mass Spectrometry},
 number = {1},
 pages = {31–37},
 title = {The Unimolecular Loss of HF by Simple Inorganic Ions: A Computational Dynamic Reaction Path Study},
 url = {https://doi-org.crai.referencistas.com/10.1255/ejms.327},
 volume = {6},
 year = {2000b}
}

@article{doi:10.1255/ejms.831,
 abstract = {In this lecture/paper we will discuss the somewhat complicated relationship between theory and experiment in molecular science in general and mass spectrometry in particular, illustrated by some recent literature examples.},
 author = {Einar Uggerud},
 doi = {10.1255/ejms.831},
 eprint = {https://doi-org.crai.referencistas.com/10.1255/ejms.831},
 journal = {European Journal of Mass Spectrometry},
 number = {1},
 pages = {97–100},
 title = {Theory and Experiment in Mass Spectrometry: A Perspective on the Relationship between Computational Modelling and Experiment in Gas-Phase Ion Chemistry},
 url = {https://doi-org.crai.referencistas.com/10.1255/ejms.831},
 volume = {13},
 year = {2007s}
}

@article{doi:10.1258/002367798780600070,
 abstract = {As part of a recent animal facility refurbishment, a cubicle containment system was designed to increase the amount of experimental space and also provide containment facilities to support the holding and use of specialized animal models. In order to achieve this, a series of computational fluid dynamic (CFD) studies was undertaken to evaluate the effects of different airflows and in order to optimize ventilation, a variety of exhaust/supply arrangements and animal loads was employed. These studies showed that air delivered via two, opposed, low level ducts, at a rate of 20 air changes per hour and exhausted high in the cubicle above the rack, was the optimal configuration resulting in minimal turbulence, stagnation and entrainment.},
 author = {G. Curry and H. C. Hughes and D. Loseby and S. Reynolds},
 doi = {10.1258/002367798780600070},
 eprint = {https://doi-org.crai.referencistas.com/10.1258/002367798780600070},
 journal = {Laboratory Animals},
 note = {PMID:9587893},
 number = {2},
 pages = {117–127},
 title = {Advances in cubicle design using computational fluid dynamics as a design tool},
 url = {https://doi-org.crai.referencistas.com/10.1258/002367798780600070},
 volume = {32},
 year = {1998c}
}

@article{doi:10.1258/hsmr.2008.008015,
 abstract = {One feature that characterizes the organization and delivery of health care is its inherent complexity. All too often, with so much information and so many activities involved, it is difficult for decision-makers to determine in an objective fashion an appropriate course of action. It would appear that a holistic rather than a reductionist approach would be advantageous. The aim of this paper is to review how formal systems thinking can aid decision-making in complex situations. Consideration is given as to how the use of a number of systems modelling methodologies can help in gaining an understanding of a complex decision situation. This in turn can enhance the possibility of a decision being made in a more rational, explicit and transparent fashion. The arguments and approaches are illustrated using examples taken from the public health arena.},
 author = {D G Cramp and E R Carson},
 doi = {10.1258/hsmr.2008.008015},
 eprint = {https://doi-org.crai.referencistas.com/10.1258/hsmr.2008.008015},
 journal = {Health Services Management Research},
 note = {PMID:19401500},
 number = {2},
 pages = {71–80},
 title = {Systems thinking, complexity and managerial decision-making: an analytical review},
 url = {https://doi-org.crai.referencistas.com/10.1258/hsmr.2008.008015},
 volume = {22},
 year = {2009e}
}

@article{doi:10.1258/jtt.2009.008103,
 author = {David Boddy},
 doi = {10.1258/jtt.2009.008103},
 eprint = {https://doi-org.crai.referencistas.com/10.1258/jtt.2009.008103},
 journal = {Journal of Telemedicine and Telecare},
 number = {8},
 pages = {423–424},
 title = {Computational Technology for Effective Healthcare: immediate steps and strategic directionsWilliamStead, Herbert SLin(eds)Washington, DC: National Academies Press, 2009ISBN 978-0-309-13050-9103 pages of text and xvi of preliminaries$31.50},
 url = {https://doi-org.crai.referencistas.com/10.1258/jtt.2009.008103},
 volume = {15},
 year = {2009b}
}

@article{doi:10.1260/0144-5987.31.2.221,
 abstract = {Minimum miscibility pressure is the least required pressure for complete mixing of gas and oil in the reservoir conditions. It is an important parameter in the processes of gas injection in a miscible manner and its precise determination is very vital in choosing the type of injecting gas and planning injecting equipment for increasing the recovery efficiency. The common method is determining MMP in slim tube or 1-D simulation of slim tube. Usually determining the minimum miscibility pressure via slim tube apparatus is an expensive and time-consuming test and to carry it out it is necessary to have a sample of reservoir oil and suggested injecting gas. Occasionally it is possible that for some unknown reasons despite spending much time and money it won’t bring up any result. As a result, for determining this parameter, finding another method which has a higher precision in addition to being swift and less expensive is very necessary. On the other hand, there are several simulation methods to determine minimum miscibility pressure. These methods are so fast rather than slim tube experiment and relatively precise. MMP can be estimated numerically using compositional simulation, method of characteristics (MOC), mixing-cell methods, intelligent methods, and empirical correlations. However, nowadays one dimensional (1-D) slim tube simulation based on compositional simulation is very common. In this paper a suggestive method is proposed for determining MMP. A mixing rule method coupled with artificial neural network model (ANN) based on a numerous experiment data. Accuracy and computational time of artificial neural network method were compared to common prior models and correlations. The results show although intelligent methods are so fast, 1-D slim tube simulation is still a proper method to determine MMP in high accuracy. Average absolute relative error for MMP value is 1.5% for 1-D slim tube simulation, while the number for ANN is 3.25%. However, ANN method is recommended for fast MMP estimation.},
 author = {M.J. Ameri and M. Shegheft Fard and M.R. Akbari and S.M. Zamanzadeh and E. Nasiri},
 doi = {10.1260/0144-5987.31.2.221},
 eprint = {https://doi-org.crai.referencistas.com/10.1260/0144-5987.31.2.221},
 journal = {Energy Exploration & Exploitation},
 number = {2},
 pages = {221–236},
 title = {A Comparison of Accuracy and Computational Time for Common and Artificial Methods in Predicting Minimum Miscibility Pressure},
 url = {https://doi-org.crai.referencistas.com/10.1260/0144-5987.31.2.221},
 volume = {31},
 year = {2013c}
}

@article{doi:10.1260/0266-3511.28.3-4.215,
 abstract = {Active bending introduces a new level of integration into the design of architectural structures, and opens up new complexities for the architectural design process. In particular, the introduction of material variation reconfigures the design space. Through the precise specification of their stiffness, it is possible to control and pre-calibrate the bending behaviour of a composite element. This material capacity challenges architecture’s existing methods for design, specification and prediction. In this paper, we demonstrate how architects might connect the designed nature of composites with the design of bending-active structures, through computational strategies. We report three built structures that develop architecturally oriented design methods for bending-active systems using composite materials. These projects demonstrate the application and limits of the introduction of advanced engineering simulation into an architectural workflow, and the extension of architecture’s existing physics-based approaches.},
 author = {Paul Nicholas and Martin Tamke},
 doi = {10.1260/0266-3511.28.3-4.215},
 eprint = {https://doi-org.crai.referencistas.com/10.1260/0266-3511.28.3-4.215},
 journal = {International Journal of Space Structures},
 number = {3–4},
 pages = {215–228},
 title = {Computational Strategies for the Architectural Design of Bending Active Structures},
 url = {https://doi-org.crai.referencistas.com/10.1260/0266-3511.28.3-4.215},
 volume = {28},
 year = {2013l}
}

@article{doi:10.1260/026635103322987968,
 abstract = {Based on the advanced computational plasticity and an artificial neural network (ANN) simulator, a new design strategy has been presented for large space structures with imperfections. Nonlinear system identification approach has also been greatly spread among the researchers and engineers in the past few years. The neural network simulators as a non-parametric system identification approach present a robust and efficient way to simulate the nonlinear behaviour of engineering systems. In the paper herein an artificial neural network (ANN) simulator, a general back error propagating perceptron, is use to simulate random imperfection for nonlinear dynamic analysis of large space structures. It is also desirable to search for a procedure for wind pressure calculation with accuracy and reliability. In this respect, attention is paid to the advanced computational fluid dynamics (CFD). The use of the advanced CFD analysis can help engineers to estimate the wind pressure for the design of large space structures with complex geometries. The characteristics of the new design method have been shown graphically using a full documented numerical example, which highlights the efficiency of the new simulation method. The purpose of this paper is to present a new design method, which takes into account the effects of imperfection on the resulting dynamic responses of large space structures under gravity, temperature and wind loadings.},
 author = {A. M. Horr and S. R. Asadsajadi and M. Safi},
 doi = {10.1260/026635103322987968},
 eprint = {https://doi-org.crai.referencistas.com/10.1260/026635103322987968},
 journal = {International Journal of Space Structures},
 number = {4},
 pages = {235–255},
 title = {Design of Large Space Structures with Imperfection Using ANN-Based Simulator},
 url = {https://doi-org.crai.referencistas.com/10.1260/026635103322987968},
 volume = {18},
 year = {2003g}
}

@article{doi:10.1260/0309524054797159,
 abstract = {A comprehensive computational study, in both steady and unsteady flow conditions, has been carried out to investigate the aerodynamic characteristics of the Risø-B1-18 airfoil equipped with variable trailing edge geometry as produced by a hinged flap. The function of such flaps should be to decrease fatigue-inducing oscillations on the blades. The computations were conducted using a 2D incompressible RANS solver with a k-w turbulence model under the assumption of a fully developed turbulent flow. The investigations were conducted at a Reynolds number of Re = 1.6 · 106. Calculations conducted on the baseline airfoil showed excellent agreement with measurements on the same airfoil with the same specified conditions. Furthermore, a more widespread comparison with an advanced potential theory code is presented. The influence of various key parameters, such as flap shape, flap size and oscillating frequencies, was investigated so that an optimum design can be suggested for application with wind turbine blades. It is concluded that a moderately curved flap with flap chord to airfoil curve ratio between 0.05 and 0.10 would be an optimum choice.},
 author = {Niels Troldborg},
 doi = {10.1260/0309524054797159},
 eprint = {https://doi-org.crai.referencistas.com/10.1260/0309524054797159},
 journal = {Wind Engineering},
 number = {2},
 pages = {89–113},
 title = {Computational Study of the Risø-B1-18 Airfoil with a Hinged Flap Providing Variable Trailing Edge Geometry},
 url = {https://doi-org.crai.referencistas.com/10.1260/0309524054797159},
 volume = {29},
 year = {2005r}
}

@article{doi:10.1260/0957-4565.45.4.9,
 abstract = {As train speed increases, the aerodynamic noise gradually exceeds the wheel-rail noise and becomes the main noise source of high-speed trains. The aerodynamic noise affects the interior environment of a train in two ways: turbulent wall pressure fluctuation (TWPF) and acoustic wall pressure fluctuation (AWPF). In this paper, a hybrid aerodynamic model for noise analysis of high-speed trains based on the computational fluid dynamics (CFD) method and the Lighthill’s acoustic analogy (LAA) algorithm, so-called CFD-LAA method, is established and solved by using the finite-element acoustic simulator of Actran software. The noise contributions of both the TWPF and the AWPF are examined and discussed. The method presented in this paper may be a valuable reference for revealing the generation mechanism of aerodynamic noises and evaluating their effects on a train interior noise.},
 author = {Yan-song Wang and Chao Tang and Chang-an Bai},
 doi = {10.1260/0957-4565.45.4.9},
 eprint = {https://doi-org.crai.referencistas.com/10.1260/0957-4565.45.4.9},
 journal = {Noise & Vibration Worldwide},
 number = {4},
 pages = {9–16},
 title = {Finite Element Analysis on Aerodynamic Noise of the High-Speed Train Head Based on Computational Fluid Dynamics and Acoustic Analogy Methods},
 url = {https://doi-org.crai.referencistas.com/10.1260/0957-4565.45.4.9},
 volume = {45},
 year = {2014t}
}

@article{doi:10.1260/1369-4332.18.7.1003,
 abstract = {Bat-inspired (BI) algorithm is a recent metaheuristic optimization technique that simulates echolocation behavior of bats in seeking a design space. Along the same line with almost all metaheuristics, this algorithm also entails a large number of time-consuming structural analyses in structural design optimization applications. This study is focused on improving computational efficiency of the BI algorithm in optimum structural design. The number of structural analyses required by BI algorithm in the course of design optimization is reduced considerably by incorporating an upper bound strategy (UBS) into the solution procedure. The performance of the resulting algorithm, i.e. UBS integrated BI algorithm (UBI), is evaluated in discrete sizing optimization of large-scale steel skeletal structures designed for minimum weight according to American Institute of Steel Construction-Allowable Stress Design provisions. The numerical results verify that the UBI results in a significant gain in the computational efficiency of the standard algorithm.},
 author = {O. Hasançebi and S. Kazemzadeh Azad},
 doi = {10.1260/1369-4332.18.7.1003},
 eprint = {https://doi-org.crai.referencistas.com/10.1260/1369-4332.18.7.1003},
 journal = {Advances in Structural Engineering},
 number = {7},
 pages = {1003–1015},
 title = {Improving Computational Efficiency of Bat-Inspired Algorithm in Optimal Structural Design},
 url = {https://doi-org.crai.referencistas.com/10.1260/1369-4332.18.7.1003},
 volume = {18},
 year = {2015e}
}

@article{doi:10.1260/1475-472X.10.2-3.117,
 abstract = {The theory of Green’s functions for the wave and Helmholtz equations is examined with particular attention to their use in aeroacoustics for the extrapolation of acoustic wavefields from numerical flow simulations. In a new synthesis that permits straightforward generalization of previously published results, spatial and temporal windowing functions are employed to provide equivalent-source expressions to account for both initial and boundary conditions. Detailed results describe the transformation of both source terms and Green’s functions to take account of uniform subsonic mean flow, and expressions are given for free-field Green’s functions, both with and without flow, in time, frequency and wavenumber domains. A worked example illustrates the non-uniqueness of the Green’s function for a simple one-dimensional bounded problem.},
 author = {C. L. Morfey and C. J. Powles and M. C. M. Wright},
 doi = {10.1260/1475-472X.10.2-3.117},
 eprint = {https://doi-org.crai.referencistas.com/10.1260/1475-472X.10.2-3.117},
 journal = {International Journal of Aeroacoustics},
 number = {2–3},
 pages = {117–159},
 title = {Green’s Functions in Computational Aeroacoustics},
 url = {https://doi-org.crai.referencistas.com/10.1260/1475-472X.10.2-3.117},
 volume = {10},
 year = {2011m}
}

@article{doi:10.1260/1475-472X.12.2.155,
 abstract = {Background literature review, methodology, results, and analysis are presented for a novel approach to approximating wind pressure on tall buildings for the application of generative design exploration and optimisation. The predictions are approximations of time-averaged computational fluid dynamics (CFD) data with the aim of maintaining simulation accuracy but with improved speed. This is achieved through the use of a back-propagation artificial neural network (ANN) with vertex-based shape features as input and pressure as output. The training set consists of 600 procedurally generated tall building models, and the test set of 10 real building models; for all models in both sets, a feature vector is calculated for every vertex. Over the test set, mean absolute errors against the basis CFD are 1.99–4.44% (σ:2. 10–5.09%) with an on-line process time of 14.72–809.98s (0.028s/sample). Studies are also included on feature sensitivity, training set size, and comparison of CFD against prediction times. Results indicate that prediction time is only dependent on the number of test model vertices, and is therefore invariant to basis CFD time.},
 author = {Samuel Wilkinson and Sean Hanna},
 doi = {10.1260/1475-472X.12.2.155},
 eprint = {https://doi-org.crai.referencistas.com/10.1260/1475-472X.12.2.155},
 journal = {International Journal of Architectural Computing},
 number = {2},
 pages = {155–177},
 title = {Approximating Computational Fluid Dynamics for Generative Tall Building Design},
 url = {https://doi-org.crai.referencistas.com/10.1260/1475-472X.12.2.155},
 volume = {12},
 year = {2014r}
}

@article{doi:10.1260/1475-472X.12.2.199,
 abstract = {Idea generation in the design production process often occurs within brainstorming sessions. Linking ideas is the key mechanism in the process of producing design. Through linking ideas, a graph-like knowledge is achieved in representing the individual memories, with the nodes and arcs being the ideas and the links between ideas respectively. The design process is similar to doing a jigsaw. Using the jigsaw metaphor, a cognitive study is applied to the design tool DIM (Dynamic Idea Map) to explore computational jigsaw mechanisms. Thereafter, a computational framework called Design Jigsaw is developed to support students in assembling vast ideas effectively and reveals the construction of meaning in the graph-like knowledge structure of design thinking},
 author = {Chia-Hui Nico Lo and Ih-Cheng Lai and Teng-Wen Chang},
 doi = {10.1260/1475-472X.12.2.199},
 eprint = {https://doi-org.crai.referencistas.com/10.1260/1475-472X.12.2.199},
 journal = {International Journal of Architectural Computing},
 number = {2},
 pages = {199–212},
 title = {Design Jigsaw: Exploring a Computational Approach to Assembling Ideas in the           Design Production Process},
 url = {https://doi-org.crai.referencistas.com/10.1260/1475-472X.12.2.199},
 volume = {12},
 year = {2014n}
}

@article{doi:10.1260/1475-472X.5.3.217,
 abstract = {This study aims to predict the overall aerodynamic performance, unsteady forces and the tonal noise radiation from a small axial flow fan. The computation is divided into two stages: (a) the computation of the unsteady flow field at design and off-design working conditions, and (b) detailed analysis of the acoustic field. A dual-time scheme for dealing with rotor-stator or wake-struts interactions in turbomachinery environments by introducing a pseudo-time to represent the time variation of source terms and the finite volume control method are applied for the unsteady solutions involving the whole flow passages of the axial flow fan, and the pressure rise against volume flow rate is obtained and shown to be in good agreement with experimental data. The interaction of the rotor wake and the downstream struts has been simulated by investigating the effects of source distribution over the whole blades. And the tonal noise is then predicted based on Lowson’s theory. The unsteady force is divided into thrust in the rotational axis direction and drag in the rotational direction for the rotor and struts. It is demonstrated that the unsteady forces acting on the rotor and struts are produced by the interaction between them, and has a close relationship with the rotor wake structure and the strut alignment. In the rotor near wake, both the viscous and potential flow is dominant not only to aerodynamic forces but also to interaction noise. The present study shows that the higher lean angle of struts can gain about 4 dB reduction of overall sound power level compared to the smallest one.},
 author = {Renjing Cao and Duck-Joo Lee},
 doi = {10.1260/1475-472X.5.3.217},
 eprint = {https://doi-org.crai.referencistas.com/10.1260/1475-472X.5.3.217},
 journal = {International Journal of Aeroacoustics},
 number = {3},
 pages = {217–232},
 title = {Computational Study of Aero- and Acoustic Performance of a Small Axial-Flow Fan},
 url = {https://doi-org.crai.referencistas.com/10.1260/1475-472X.5.3.217},
 volume = {5},
 year = {2006c}
}

@article{doi:10.1260/1478-0771.10.1.121,
 abstract = {This paper compares the use of scripting languages and visual programming languages for teaching computational design concepts to novice and advanced architecture students. Both systems are described and discussed in terms of the representation methods they use. With novice students better results were obtained with the visual programming language. However, the generative strategies used were restricted to parametric variation and the use of randomness. Scripting, on the other hand, was used by advanced students to implement rule-based generative systems. It is possible to conclude that visual languages can be very useful for making architecture students understand general programming concepts, but scripting languages are fundamental for implementing generative design systems. The paper also discusses the importance of the ability to shift between different representation methods, from more concrete to more abstract, as part of the architectural education.},
 author = {Gabriela Celani and Carlos Eduardo Verzola Vaz},
 doi = {10.1260/1478-0771.10.1.121},
 eprint = {https://doi-org.crai.referencistas.com/10.1260/1478-0771.10.1.121},
 journal = {International Journal of Architectural Computing},
 number = {1},
 pages = {121–137},
 title = {CAD Scripting and Visual Programming Languages for Implementing Computational Design Concepts: A Comparison from a Pedagogical Point of View},
 url = {https://doi-org.crai.referencistas.com/10.1260/1478-0771.10.1.121},
 volume = {10},
 year = {2012b}
}

@article{doi:10.1260/1478-0771.10.4.613,
 abstract = {This research intends to illustrate a nonlinear relationship that could be drawn between the fundamental processes in living systems and architectural design of responsive surface. The research focuses on deriving a set of parametric relationships from the phenomenon in cell biology and generating an architectural expression of a responsive façade system. The research methods primarily investigates the cell – to – cell connection in mammary epithelial cell system and review the evident relay of communication across the entire system of cells. This thorough investigation unfolds the logical parameters of the biological system that delineates the dynamic feedback mechanism and changes in the cell surface conditions initiated from the changes in the extracellular environment (ECM). The research findings of this complex mechanism are further translated though parametric modeling tool (in this case Generative Components) to model the causalities of the changes in cell environment and surface condition changes. In the next phase of our research we have explored the architectural utility of this hybridized model operating in a user defined controlled environ, and not just a mere response to biological stimulus.},
 author = {Florina Dutt and Subhajit Das},
 doi = {10.1260/1478-0771.10.4.613},
 eprint = {https://doi-org.crai.referencistas.com/10.1260/1478-0771.10.4.613},
 journal = {International Journal of Architectural Computing},
 number = {4},
 pages = {613–633},
 title = {Computational Design of a Bio Inspired Responsive Architectural Façade System},
 url = {https://doi-org.crai.referencistas.com/10.1260/1478-0771.10.4.613},
 volume = {10},
 year = {2012e}
}

@article{doi:10.1260/1478-0771.12.1.1,
 abstract = {The use of computational processes in architecture is a widespread practice which draws on a set of theories of computer science developed in the 60s and 70s. With the advent of computers, many of these methodologies were developed in research centres in the USA and the UK. Focussing on this period, this paper investigates the importance of the German Hochschule fur Gestaltung, Ulm (HfG) design school in the early stages of computation in design and architecture. Even though there were no computers in the school, it may be argued that its innovative pedagogy and distinguished faculty members launched analogical computational design methods that can be seen as the basis for further computational approaches in architecture. The paper draws on archive material, as well as at an original interview with Tomas Maldonado, to propose that the remarkable work pursued by Tomas Maldonado (the educational project), Max Bense (information aesthetics) and Horst Rittel (scientific methods) was fundamental in establishing HfG Ulm as the forerunner of computation in architecture.},
 author = {Isabel Clara Neves and João Rocha and José Pinto Duarte},
 doi = {10.1260/1478-0771.12.1.1},
 eprint = {https://doi-org.crai.referencistas.com/10.1260/1478-0771.12.1.1},
 journal = {International Journal of Architectural Computing},
 number = {1},
 pages = {1–25},
 title = {Computational Design Research in Architecture: The Legacy of the Hochschule für Gestaltung, Ulm},
 url = {https://doi-org.crai.referencistas.com/10.1260/1478-0771.12.1.1},
 volume = {12},
 year = {2014i}
}

@article{doi:10.1260/1478-0771.7.4.565,
 abstract = {In-Between Architecture Computation describes the evolution of the Computational Design approach at the AedasR&D Computational Design and Research group founded in 2004 at Aedas architects in London. The approach has transformed itself from an academic inspired thinking about computing media to a more flexible model of design heuristics and search algorithms that finally start to produce new hybrid design workflows in the industry while also swimming against the industry trend of super-integration software. Only if computing is not exclusively defined through architectural design intent or purely computing logic, does computational design explore new design thinking.},
 author = {Christian Derix},
 doi = {10.1260/1478-0771.7.4.565},
 eprint = {https://doi-org.crai.referencistas.com/10.1260/1478-0771.7.4.565},
 journal = {International Journal of Architectural Computing},
 number = {4},
 pages = {565–585},
 title = {In-Between Architecture Computation},
 url = {https://doi-org.crai.referencistas.com/10.1260/1478-0771.7.4.565},
 volume = {7},
 year = {2009e}
}

@article{doi:10.1260/1478-0771.7.4.i,
 author = {Gabriela Celani},
 doi = {10.1260/1478-0771.7.4.i},
 eprint = {https://doi-org.crai.referencistas.com/10.1260/1478-0771.7.4.i},
 journal = {International Journal of Architectural Computing},
 number = {4},
 pages = {i–vi},
 title = {Incorporating Computational Theories and Technologies in Architectural Design},
 url = {https://doi-org.crai.referencistas.com/10.1260/1478-0771.7.4.i},
 volume = {7},
 year = {2009e}
}

@article{doi:10.1260/1478-0771.8.1.1,
 abstract = {In recent decades, new methodologies have emerged in architectural design that exploit the computer as a design tool. This has generated a varied set of digital skills and a new type of architectural knowledge. However, up to now, a theoretical framework is missing that would allow for a comprehensive pedagogical agenda for the teaching of digital design in architecture. The present paper offers an attempt towards such a theoretical grounding based on the concept of computable functions. This approach results in an abstract and formal perspective on digital design that enables a grouping of contemporary digital design methods and an understanding of their logical relationship. On a theoretical level, it opens a path for the study of the mechanism that facilitates the transfer of concepts from various scientific disciplines into architecture.},
 author = {Toni Kotnik},
 doi = {10.1260/1478-0771.8.1.1},
 eprint = {https://doi-org.crai.referencistas.com/10.1260/1478-0771.8.1.1},
 journal = {International Journal of Architectural Computing},
 number = {1},
 pages = {1–16},
 title = {Digital Architectural Design as Exploration of Computable Functions},
 url = {https://doi-org.crai.referencistas.com/10.1260/1478-0771.8.1.1},
 volume = {8},
 year = {2010k}
}

@article{doi:10.1260/1478-0771.8.4.439,
 abstract = {This article discusses interaction between multimodal representations of architectural design knowledge, particularly focusing on relating explicit and implicit types of information. The aim of the presented research is to develop a computational environment that combines several modes of representation, including and integrating different forms of architectural design knowledge. Development of an interactive digital-models library and ontological model of architectural design factors are discussed, which are complementary in nature. In a time when BIM software is seen as embodiment of domain knowledge and the future medium of architectural design, this paper presents an interaction between ontological representation of architectural design knowledge and its embodiment in interactive models, thus focusing on the process of design and design space exploration. In the digital environments that we propose, representation of different formats of knowledge, such as visual, linguistic or numeric, are integrated with relational and procedural information, design rules, and characteristics. Interactive search and query based on contextual constraints, and parametric variation of the model based on the information received from ontology are the underlying drivers for design exploration and development.},
 author = {Ajla Aksamija and Ivanka Iordanova},
 doi = {10.1260/1478-0771.8.4.439},
 eprint = {https://doi-org.crai.referencistas.com/10.1260/1478-0771.8.4.439},
 journal = {International Journal of Architectural Computing},
 number = {4},
 pages = {439–460},
 title = {Computational Environments with Multimodal Representations of Architectural Design Knowledge},
 url = {https://doi-org.crai.referencistas.com/10.1260/1478-0771.8.4.439},
 volume = {8},
 year = {2010a}
}

@article{doi:10.1260/147807703771799166,
 abstract = {This paper describes a studio that explores interfaces for computationally enhanced artifacts and environments. The studio is designed as a traditional architectural design studio, fostering creative thinking and encouraging hands-on learning. It brings students from art, music, architecture, computer science, and engineering together into teams to design and build physical computing projects. The team’s unusual mix of knowledge and experience allows for creative solutions. As a result, the studio has become a test bed for new and interesting ideas.},
 author = {Ken Camarata and Mark D Gross and Ellen Yi-Luen Do},
 doi = {10.1260/147807703771799166},
 eprint = {https://doi-org.crai.referencistas.com/10.1260/147807703771799166},
 journal = {International Journal of Architectural Computing},
 number = {2},
 pages = {169–190},
 title = {A Physical Computing Studio: Exploring Computational Artifacts and Environments},
 url = {https://doi-org.crai.referencistas.com/10.1260/147807703771799166},
 volume = {1},
 year = {2003b}
}

@article{doi:10.1260/1478077041518665,
 abstract = {The article examines one little understood but ubiquitous form of divergent thinking achieved intermittently during the act of drawing or modeling. It is argued that this phenomenon, here called intermittent divergence, is rooted in a special kind of interaction between perception and imagination, and that this interaction has specific experiential requirements. Three requirements are defined. The resulting new theory then provides a framework for the critical analysis of conventional digital modeling and parametric modeling. Conventional modeling methods are shown to satisfy the requirements for intermittent divergence, while parametric modeling methods are shown to undermine them. The article concludes that parametric systems, as currently developed, could inhibit rather than augment this important route to creativity. Additionally, the article questions prevailing beliefs about the computer support of creativity, including the premise that sketching is an ideal creative medium and the premise that ambiguity in graphical depictions is key to the support of creativity. The theory offers an alternative view on these issues.},
 author = {Kyle W. Talbott},
 doi = {10.1260/1478077041518665},
 eprint = {https://doi-org.crai.referencistas.com/10.1260/1478077041518665},
 journal = {International Journal of Architectural Computing},
 number = {2},
 pages = {263–286},
 title = {Divergent Thinking in the Construction of Architectural Models},
 url = {https://doi-org.crai.referencistas.com/10.1260/1478077041518665},
 volume = {2},
 year = {2004p}
}

@article{doi:10.1260/147807705775377366,
 abstract = {In contrast to traditional models of design process fundamentally defined by the abstract manipulation of objects, this study recognizes that the resources available for rethinking architecture are to be found in a reformulation of its theory and practice. This reformation should be based on non-linear design processes in which dynamic emergence and invention take the place of a linear design process fixed on a particular object evolution. Advances in computation thinking and technology have stimulated the design and formulation of a large number of design software. Its elaboration supposes a new conceptualization of our discipline’s knowledge, in a body of principles and regulations, which commands the artifact’s design and its realization; therefore, it constitutes a preliminary datum for its comprehension, and thereby is of theoretical importance. Despite the continuous increment of power in computers and software capacities, the creative space of freedom defined by them acting as cognitive instruments remains almost unexplored. Therefore, we propose a change from a design knowledge based on objects to one focused on design as a network of processes. In addition, this study explores the concept of Distributed Cognition in order to redefine the use of digital tools in design process as Cognitive Instruments.},
 author = {Eduardo Lyon},
 doi = {10.1260/147807705775377366},
 eprint = {https://doi-org.crai.referencistas.com/10.1260/147807705775377366},
 journal = {International Journal of Architectural Computing},
 number = {3},
 pages = {317–333},
 title = {Autopoiesis and Digital Design Theory: CAD Systems as Cognitive Instruments},
 url = {https://doi-org.crai.referencistas.com/10.1260/147807705775377366},
 volume = {3},
 year = {2005m}
}

@article{doi:10.1260/147807706779398962,
 abstract = {In this paper, we present a generative design model for conceptual design in architecture. Based on this model we developed and implemented a compact, open-ended generative tool with a connected design evaluation database. Core concept of our generative approach is to achieve complex forms from a base primitive and create the form from the modified instances. Our tool is used in various levels of design studios, including graduate and undergraduate students. Designs from these experiments are evaluated in a qualitative framework.},
 author = {Burak Pak and Ozan Onder Ozener and Arzu Erdem},
 doi = {10.1260/147807706779398962},
 eprint = {https://doi-org.crai.referencistas.com/10.1260/147807706779398962},
 journal = {International Journal of Architectural Computing},
 number = {4},
 pages = {21–33},
 title = {Utilizing Customizable Generative Design Tools in Digital Design Studio: Xp-GEN Experimental Form Generator},
 url = {https://doi-org.crai.referencistas.com/10.1260/147807706779398962},
 volume = {4},
 year = {2006n}
}

@article{doi:10.1260/147807707783600771,
 abstract = {In historic design conventions geometry has traditionally promoted descriptive manifestations of form. Beyond the realm of geometry, the concept of performance which may inform such manifestations also carries important potential for design generation. This work explores the relation between geometry and performance from a computational-geometry perspective. It does so by revisiting certain analytical tools offered in most of today’s 3-D modelers which support the evaluation of any generated surface geometry specifically curvature and draft angle analysis. It is demonstrated that these tools can be reconstructed with added functionality assigning 3-D geometrical features informed by structural and environmental performance respectively. In the examples illustrated surface thickness (as a function of structural performance) is assigned to curvature values, and transparency (as a function of light performance) is assigned to light analysis values. In a broader scope this work promotes a methodology of performance-informed form generation by means of computational geometry. Vector and tensor math was exploited to reconstruct existing analytical tools adapted to function as design generators.},
 author = {Neri Oxman},
 doi = {10.1260/147807707783600771},
 eprint = {https://doi-org.crai.referencistas.com/10.1260/147807707783600771},
 journal = {International Journal of Architectural Computing},
 number = {4},
 pages = {663–684},
 title = {Get Real towards Performance-Driven Computational Geometry},
 url = {https://doi-org.crai.referencistas.com/10.1260/147807707783600771},
 volume = {5},
 year = {2007k}
}

@article{doi:10.1260/175682909789498279,
 abstract = {We seek to extend the literature on sinusoidal pure-plunge of 2D airfoils at high reduced frequency and low Reynolds number, by including effects of camber and nonzero mean incidence angle. We compare experimental results in a water tunnel using dye injection and 2D particle image velocimetry, with a set of computations in 2D – Immersed Boundary Method and unsteady Reynolds-Averaged Navier Stokes. The Re range is from 10,000 to 60,000, based on free stream velocity and airfoil chord, chosen to cover cases where transition in attached boundary layers would be of some importance, and where transition would only occur in the wake. Generally at high reduced frequency there is no Reynolds number effect. Mean angle of attack has significance, notionally, depending on whether it is below or above static stall. Computations were found to agree well with experimentally-derived velocity contours, vorticity contours and momentum in the wake. As found previously for the NACA0012, varying Strouhal number is found to control the topology of the wake, while varying reduced amplitude and reduced frequency together, but keeping Strouhal number constant, causes wake vortical structures to scale with the reduced amplitude of plunge. Flowfield periodicity – as evinced from comparison of instantaneous and time-averaged particle image velocimetry – is generally attained after two periods of oscillation from motion onset.},
 author = {Michael V. Ol and Mark Reeder and Daniel Fredberg and Gregory Z. McGowan and Ashok Gopalarathnam and Jack R. Edwards},
 doi = {10.1260/175682909789498279},
 eprint = {https://doi-org.crai.referencistas.com/10.1260/175682909789498279},
 journal = {International Journal of Micro Air Vehicles},
 number = {2},
 pages = {99–119},
 title = {Computation vs. Experiment for High-Frequency Low-Reynolds Number Airfoil Plunge},
 url = {https://doi-org.crai.referencistas.com/10.1260/175682909789498279},
 volume = {1},
 year = {2009o}
}

@article{doi:10.1260/1757-482X.2.1.33,
 abstract = {The two-dimensional Kelvin-Helmholtz instability which arises in two immiscible co-currently moving horizontal liquid layers is studied numerically. Two different methods for capturing the free interface movement are applied, namely the volume-of-fluid method and the level-set method. Both methods are very popular and available within commercial computational fluid dynamics tools. The wave is initialized using two different perturbations implemented into the model. As a case study, a system comprising a toluene layer and an aqueous layer is chosen. Numerical results obtained by both methods are in a good agreement with the linear stability theory for small wave amplitudes. The application of both methods is compared and their advantages and drawbacks are highlighted.},
 author = {Theodoros Atmakidis and Eugeny Y. Kenig},
 doi = {10.1260/1757-482X.2.1.33},
 eprint = {https://doi-org.crai.referencistas.com/10.1260/1757-482X.2.1.33},
 journal = {The Journal of Computational Multiphase Flows},
 number = {1},
 pages = {33–45},
 title = {A Study on the Kelvin-Helmholtz Instability Using Two Different Computational Fluid Dynamics Methods},
 url = {https://doi-org.crai.referencistas.com/10.1260/1757-482X.2.1.33},
 volume = {2},
 year = {2010b}
}

@article{doi:10.1260/1757-482X.3.3.147,
 abstract = {Accumulation of soot particles on the wall of a constricted pipe, through which JP4 liquid jet fuel flows, is investigated numerically, using a pipe with a bump on its wall. The effects of particle size, particle-to-fluid density ratio and carrier fluid flow Reynolds number, on particle accumulation, are investigated. When the particles’ size is relatively large, with particle-to-fluid density ratio greater than unity, and at relatively large Re number, the largest accumulation of particles occurs near the bump’s front wall where those particles are blocked by the protruding bump and are too massive to be carried away by the nearwall, low momentum carrier fluid. Conversely, when either the particles’ size or the carrier fluid Re number is relatively small, or the density ratio is less than unity, the largest accumulation of particles occurs in one of three regions: on the bump’s rear wall, near the center of the recirculation eddy, or at the flow detachment point. Particle-accumulation ratio on the bump’s wall decreases monotonically as the particle size decreases (with the mass concentration of particles at the pipe inlet held fixed).},
 author = {Inchul Kim and Uriel Goldberg},
 doi = {10.1260/1757-482X.3.3.147},
 eprint = {https://doi-org.crai.referencistas.com/10.1260/1757-482X.3.3.147},
 journal = {The Journal of Computational Multiphase Flows},
 number = {3},
 pages = {147–163},
 title = {Computational Investigation of Particle-Laden Jet Fuel Flow through a Constricted Pipe},
 url = {https://doi-org.crai.referencistas.com/10.1260/1757-482X.3.3.147},
 volume = {3},
 year = {2011h}
}

@article{doi:10.1260/1757-482X.5.4.265,
 abstract = {This paper presents the results of comparison of experimental and CFD studies of slug flow in a vertical 90° bend using validated models. For the experimental part, Electrical Capacitance Tomography (ECT), Wire Mesh Tomography (WMS), and high-speed videos were used to monitor an air-silicone oil mixture flowing in a vertical 90o bend. The ECT probes were mounted before the bend whilst the WMS was positioned either immediately upstream or immediately downstream of the bend. The downstream pipe was maintained horizontal whilst the upstream pipe was maintained vertical. The bend (R/D = 2.3) was made of transparent acrylic resin. Parallel to the experiments, simulations were carried out for same experiment set-up using the commercial software package Star-CD and Star-CCM+. The condition was simulated with the Volume of Fluid (VOF) model. The simulation predictions were validated against the experimental data. A reasonably good agreement was observed for the results of the experiment and CFD.},
 author = {M. Abdulkadir and V. Hernandez-Perez and S. Lo and I. S. Lowndes and B. J. Azzopardi},
 doi = {10.1260/1757-482X.5.4.265},
 eprint = {https://doi-org.crai.referencistas.com/10.1260/1757-482X.5.4.265},
 journal = {The Journal of Computational Multiphase Flows},
 number = {4},
 pages = {265–281},
 title = {Comparison of Experimental and Computational Fluid Dynamics (CFD) Studies of Slug Flow in a Vertical 90° Bend},
 url = {https://doi-org.crai.referencistas.com/10.1260/1757-482X.5.4.265},
 volume = {5},
 year = {2013a}
}

@article{doi:10.1260/1759-3131.1.3-4.223,
 abstract = {A numerical investigation has been carried out to obtain a non-dimensional grid size (grid size/ tsunami base width) for the near shore discretisation of computational domains for long wave modelling. A 1D domain has been considered in which, the flow has been solved by 1D shallow water equations with vertically integrated flow variables. The sensitivity study of the grid size was carried out in the 1D channel with an open boundary at one end and shelf boundary at the other end. The grid size was varied from 10 m to 1000 m and its effect on the computation of the tsunami run-up along the shoreline has been investigated. The non-dimensional grid size for the computation of run-up was optimised by comparing the non-dimensional run-up (tsunami run-up/initial tsunami height) and a threshold value of 5.0e-4 was obtained. Further, the study was extended to real scenario by adopting various grids for the shelf region of northern Tamil Nadu coast, south east coast of India in 2D and a suitable grid size was obtained.},
 author = {Manasa Ranjan Behera and K. Murali and V. Sundar},
 doi = {10.1260/1759-3131.1.3-4.223},
 eprint = {https://doi-org.crai.referencistas.com/10.1260/1759-3131.1.3-4.223},
 journal = {The International Journal of Ocean and Climate Systems},
 number = {3–4},
 pages = {223–237},
 title = {Identification of Suitable Grid Size for Accurate Computation of Run-up Height},
 url = {https://doi-org.crai.referencistas.com/10.1260/1759-3131.1.3-4.223},
 volume = {1},
 year = {2010b}
}

@article{doi:10.1366/0003702053946056,
 abstract = {The mathematical problem of converting a normal spectrum into the corresponding first- and second-derivative spectra is formulated as an integral equation of the first kind. Tikhonov regularization is then applied to solve the spectral conversion problem. The end result is a set of linear algebraic equations that takes in as input the original spectrum and produces as output the second-derivative spectrum, which is then integrated to yield the first-derivative spectrum. Noise amplification is kept under control by adjusting the regularization parameter (guided by generalized cross-validation) in the algebraic equations. The performance of this procedure is demonstrated by applying it to different types of spectral data taken from the literature.},
 author = {Y. Leong Yeow and Y. K. Leong},
 doi = {10.1366/0003702053946056},
 eprint = {https://doi-org.crai.referencistas.com/10.1366/0003702053946056},
 journal = {Applied Spectroscopy},
 note = {PMID:15969803},
 number = {5},
 pages = {584–592},
 title = {A General Computational Method for Converting Normal Spectra into Derivative Spectra},
 url = {https://doi-org.crai.referencistas.com/10.1366/0003702053946056},
 volume = {59},
 year = {2005s}
}

@article{doi:10.1366/000370278774331521,
 abstract = {Equations and a calculation procedure for modeling the operation of an electronic, adjustable-waveform and other types of high voltage spark sources are presented. The equations return time-dependent capacitor voltage and charging current and the calculation procedure returns complete break patterns. Comparisons between laboratory and computed results indicate the calculations predict experiment with accuracy in the 1 to 5% relative error range. This is sufficient to make the calculation procedure useful for characterization of research or production spark sources.},
 author = {Alexander Scheeline and R. J. Klueppel and David M. Coleman and John P. Walters},
 doi = {10.1366/000370278774331521},
 eprint = {https://doi-org.crai.referencistas.com/10.1366/000370278774331521},
 journal = {Applied Spectroscopy},
 number = {2},
 pages = {224–238},
 title = {Computational Procedures for Characterization of High Voltage Spark Sources},
 url = {https://doi-org.crai.referencistas.com/10.1366/000370278774331521},
 volume = {32},
 year = {1978p}
}

@article{doi:10.1366/13-07096,
 abstract = {A stable liquid/liquid optical waveguide (LLW) was formed using a sheath flow, where a 15% sodium chloride (NaCl) solution functioned as the core solution and water functioned as the cladding solution (15% NaCl/water LLW). The LLW was at least 200 mm in length. The concentration distributions of the liquid core and liquid cladding solutions in the LLW system were predicted by computational fluid dynamics (CFD) to validate the characteristics of the waveguide. The broadening of the region of the fluorescence of Rhodamine B excited by the guided light and the increase in the critical angle of the guided light with the increase in the contact time of the core and the cladding solutions were well explained by CFD calculations. However, no substantial leakage of the guided light was observed despite the considerably large change in the refractive index profile of the LLW; thus, a narrower and longer waveguide was realized.},
 author = {Junya Kamiyama and Soto Asanuma and Hiroyasu Murata and Yasuhiko Sugii and Hiroki Hotta and Kiichi Sato and Kin-Ichi Tsunoda},
 doi = {10.1366/13-07096},
 eprint = {https://doi-org.crai.referencistas.com/10.1366/13-07096},
 journal = {Applied Spectroscopy},
 note = {PMID:24359663},
 number = {12},
 pages = {1479–1484},
 title = {Characterization of Liquid-Core/Liquid-Cladding Optical Waveguides of a Sodium Chloride Solution/Water System by Computational Fluid Dynamics},
 url = {https://doi-org.crai.referencistas.com/10.1366/13-07096},
 volume = {67},
 year = {2013j}
}

@article{doi:10.1509/jmkr.43.2.303,
 author = {Naveen Donthu and Meryl P. Gardner and Sandeep Krishnamurthy and Stephaine Noble and Nicholas H. Lurie},
 doi = {10.1509/jmkr.43.2.303},
 eprint = {https://doi-org.crai.referencistas.com/10.1509/jmkr.43.2.303},
 journal = {Journal of Marketing Research},
 number = {2},
 pages = {303–306},
 title = {Book Review},
 url = {https://doi-org.crai.referencistas.com/10.1509/jmkr.43.2.303},
 volume = {43},
 year = {2006d}
}

@article{doi:10.1509/jmkr.46.1.81,
 abstract = {Consumers’ judgments of the magnitude of numerical differences are influenced by the ease of mental computations. The results from a set of experiments show that ease of computation can affect judgments of the magnitude of price differences, discount magnitudes, and brand choices. Participants seem to believe that it is easier to judge the size of a larger difference than that of a smaller difference. In the absence of appropriate corrective steps, this naive belief can lead to systematic biases in judgments. For example, when presented with two pairs of numbers, participants incorrectly judged the magnitude of the difference to be smaller for pairs with difficult computations (e.g., 4.97 – 3.96, an arithmetic difference of 1.01) than for pairs with easy computations (e.g., 5.00 – 4.00, an arithmetic difference of 1.00). The effect does not manifest when judgments do not entail mental computations or when participants are made aware that the ease or difficulty is caused by computational complexity. Furthermore, this effect is mitigated when participants’ prior experience is manipulated in a learning phase of the experiment. The results have implications for buyers and sellers and for understanding the role of metacognitive experiences in numerical judgments.},
 author = {Manoj Thomas and Vicki G. Morwitz},
 doi = {10.1509/jmkr.46.1.81},
 eprint = {https://doi-org.crai.referencistas.com/10.1509/jmkr.46.1.81},
 journal = {Journal of Marketing Research},
 number = {1},
 pages = {81–91},
 title = {The Ease-of-Computation Effect: The Interplay of Metacognitive Experiences and Naive Theories in Judgments of Price Differences},
 url = {https://doi-org.crai.referencistas.com/10.1509/jmkr.46.1.81},
 volume = {46},
 year = {2009r}
}

@article{doi:10.1515/jos-2016-0044,
 abstract = {There is a growing interest in data amongst small and medium enterprises (SMEs). This article looks at ways in which SMEs can combine their internal company data with open data, such as official statistics, and thereby enhance their business opportunities. Case studies are given as illustrations of the statistical and data-mining methods involved in such integrated data analytics. The article considers the barriers that prevent more SMEs from benefitting in this field and appraises some of the initiatives that are aimed at helping to overcome them. The discussion emphasizes the importance of bringing people together from the business, IT, and statistical worlds and suggests ways for statisticians to make a greater impact.},
 author = {Shirley Y. Coleman},
 doi = {10.1515/jos-2016-0044},
 eprint = {https://doi-org.crai.referencistas.com/10.1515/jos-2016-0044},
 journal = {Journal of Official Statistics},
 number = {4},
 pages = {849–865},
 title = {Data-Mining Opportunities for Small and Medium Enterprises with Official Statistics in the UK},
 url = {https://doi-org.crai.referencistas.com/10.1515/jos-2016-0044},
 volume = {32},
 year = {2016d}
}

@article{doi:10.1518/001872095778995571,
 abstract = {People who interact with graphs to perform arithmetic tasks typically employ quantitative features. However, they may employ the spatial metaphor under two alternative conditions: if they have been trained to apply visual arithmetic methods to traditional line and bar graphs, or if they interact with specially constructed and special-purpose computational graphics. Two experiments explored a visual arithmetic method used to train subjects to determine a mean by locating the spatial midpoint of a set of indicators (bars in a bar graph or points in a line graph). In Experiment 1 visual arithmetic subjects determined the mean of five indicators as quickly as they did the mean of two indicators; control subjects performed slower with five indicators. Both groups took more time to add the values of five indicators than to add two indicators. In Experiment 2, unlike control subjects, visual arithmetic subjects’ response times in mean trials were unaffected by changes in the y-axis scale. However, both groups were affected by the y-axis scale on addition trials. Neither group was influenced by changes in the distance between indicators. The discussion addresses the transfer of visual arithmetic training to tasks that differ from the one used during training, uses for visual arithmetic and computational graphics, and the role of parallel processing and task restructuring in visual arithmetic.},
 author = {Douglas J. Gillan},
 doi = {10.1518/001872095778995571},
 eprint = {https://doi-org.crai.referencistas.com/10.1518/001872095778995571},
 journal = {Human Factors},
 note = {PMID:8851778},
 number = {4},
 pages = {766–780},
 title = {Visual Arithmetic, Computational Graphics, and the Spatial Metaphor},
 url = {https://doi-org.crai.referencistas.com/10.1518/001872095778995571},
 volume = {37},
 year = {1995e}
}

@article{doi:10.1518/106480407X255206,
 abstract = {FEATURE AT A GLANCE: The earliest stage of the design process, known as conceptual design, is crucial to the overall effectiveness of the end product. Of particular importance is the search of the design space for ideas and concepts, which is restricted because of limitations in designers’ information-processing capabilities. One way to overcome this restriction is to implement computational intelligence to support and extend the search and exploration process. In this article, we report the outcome of a series of multidisciplinary workshops that identify seven key topics necessary for the future development of a generic user-centered computational environment to support conceptual design.},
 author = {Damien J. Williams and Jan M. Noyes},
 doi = {10.1518/106480407X255206},
 eprint = {https://doi-org.crai.referencistas.com/10.1518/106480407X255206},
 journal = {Ergonomics in Design},
 number = {4},
 pages = {12–16},
 title = {Development of a User-Centered Computational Environment for Conceptual Design},
 url = {https://doi-org.crai.referencistas.com/10.1518/106480407X255206},
 volume = {15},
 year = {2007r}
}

@article{doi:10.1518/155534309X441826,
 author = {Walter Warwick and Laurel Allender and John Yen},
 doi = {10.1518/155534309X441826},
 eprint = {https://doi-org.crai.referencistas.com/10.1518/155534309X441826},
 journal = {Journal of Cognitive Engineering and Decision Making},
 number = {2},
 pages = {93–96},
 title = {Editors’ Introduction to the Special Issue on Developing and Understanding Computational Models of Macrocognition},
 url = {https://doi-org.crai.referencistas.com/10.1518/155534309X441826},
 volume = {3},
 year = {2009s}
}

@article{doi:10.1518/hfes.45.1.47.27231,
 abstract = {This paper presents the VDM 2000, a computational model of target detection designed for use in military developmental test and evaluation settings. The model integrates research results from the fields of early vision, object recognition, and psychophysics. The VDM2000 is image based and provides a criterion-independent measure of target conspicuity, referred to as the vehicle metric (VM). A large data set of human responses to photographs of military vehicles in a field setting was used to validate the model. The VM adjusted by a single calibration parameter accounts for approximately 80% of the variance in the validation data. The primary application of this model is to predict detection of military targets in daylight with the unaided eye. The model also has application to target detection prediction using infrared night vision systems. The model has potential as a tool to evaluate the visual properties of more general task settings.},
 author = {Gary Witus and R. Darin Ellis},
 doi = {10.1518/hfes.45.1.47.27231},
 eprint = {https://doi-org.crai.referencistas.com/10.1518/hfes.45.1.47.27231},
 journal = {Human Factors},
 note = {PMID:12916581},
 number = {1},
 pages = {47–60},
 title = {Computational Modeling of Foveal Target Detection},
 url = {https://doi-org.crai.referencistas.com/10.1518/hfes.45.1.47.27231},
 volume = {45},
 year = {2003s}
}

@article{doi:10.15655/mw_2018_v9i1_49287,
 author = {Sreeram Gopalkrishnan},
 doi = {10.15655/mw_2018_v9i1_49287},
 eprint = {https://doi-org.crai.referencistas.com/10.15655/mw_2018_v9i1_49287},
 journal = {Media Watch},
 number = {1},
 pages = {79–88},
 title = {The Trump Campaign Computational Propaganda Challenge for the Indian Parliamentary Elections 2019},
 url = {https://doi-org.crai.referencistas.com/10.15655/mw_2018_v9i1_49287},
 volume = {9},
 year = {2018h}
}

@article{doi:10.1583/12-3820.1,
 abstract = {Purpose To evaluate the displacement forces acting on an aortic endograft when the iliac limbs are crossed (“ballerina” position). Methods An endograft model was computationally reconstructed based on data from a patient whose infrarenal aortic aneurysm had an endovascular stent-graft implanted with the iliac limbs crossed. Computational fluid dynamics analysis determined the maximum displacement force on the endograft and separately on the bifurcation and iliac limbs. Its analogue model was reconstructed for comparison, assuming the neck, main body, and total length constant but considering the iliac limbs to be deployed in the usual bifurcated mode. Calculations were repeated after developing “idealized” models of both the bifurcated and crossed-limbs endografts with straight main bodies and no neck angulation or curved iliac segments. Results The vector of the total force was directed anterocaudal for both the typical bifurcated and the crossed-limbs configurations, with the forces in the latter slightly reduced and the vertical component accounting for most of the force in both configurations. Idealized crossed-limbs and bifurcated configurations differed only in the force on the iliac limbs, but this difference disappeared in the realistic models. Conclusion Crossing of the iliac limbs can slightly affect the direction of the displacement forces. Although this configuration can exert larger forces on the limbs than in the bifurcated mode, this effect can be blunted by concomitant modifications in the geometry of the main body and other parts of the endograft, making its hemodynamic behavior resemble that of a typically positioned endograft.},
 author = {Efstratios Georgakarakos and Antonios Xenakis and Christos Manopoulos and George S. Georgiadis and Sokrates Tsangaris and Miltos K. Lazarides},
 doi = {10.1583/12-3820.1},
 eprint = {https://doi-org.crai.referencistas.com/10.1583/12-3820.1},
 journal = {Journal of Endovascular Therapy},
 note = {PMID:22891840},
 number = {4},
 pages = {549–557},
 title = {Modeling and Computational Analysis of the Hemodynamic Effects of                     Crossing the Limbs in an Aortic Endograft (“Ballerina”                     Position)},
 url = {https://doi-org.crai.referencistas.com/10.1583/12-3820.1},
 volume = {19},
 year = {2012d}
}

@article{doi:10.2190/2102-5G77-QL77-5506,
 abstract = {Despite the digital saturation of today’s youth across demographic groups, students of color and females remain severely underrepresented in computer science. Reporting on a sequential mixed methods study, this article explores the ways that high school computer science teachers can act as change agents to broaden the participation in computing for historically underrepresented students. Three high school case studies reveal a critical need for professional development and support to do this work. The subsequent part of the study focuses on the impact of a district-university intervention which trained 25 urban teachers to teach Advanced Placement computer science in their schools. The swift success of this intervention was evident from the following years’ dramatic increase in course offerings and enrollment of females, Latinos, and African Americans.},
 author = {Joanna Goode},
 doi = {10.2190/2102-5G77-QL77-5506},
 eprint = {https://doi-org.crai.referencistas.com/10.2190/2102-5G77-QL77-5506},
 journal = {Journal of Educational Computing Research},
 number = {1},
 pages = {65–88},
 title = {If You Build Teachers, Will Students Come? The Role of Teachers in Broadening Computer Science Learning for Urban Youth},
 url = {https://doi-org.crai.referencistas.com/10.2190/2102-5G77-QL77-5506},
 volume = {36},
 year = {2007i}
}

@article{doi:10.2190/8DB8-ELNU-FCDY-ENMR,
 abstract = {In order to analyze a free-associative discourse–internal monologues obtained by the method of thinking aloud–, two analyses are offered, descriptive and deductive. In the descriptive analysis, an Imagery Dictionary and a Regressive Imagery Dictionary, both in French, were used to evaluate the importance and unfolding of images and regressive thinking in three-hour long internal monologues obtained from eighteen French-speaking volunteers. The deductive analysis relies on Osgood’s Naturalness Principle: that congruent sentences conjoined by and evidence faster processing times than incongruent sentences conjoined by but. A positive relationship was hypothesized and confirmed between images and regressive thinking (considered as natural cognizings) on the one hand, and the use of markers of congruence on the other hand.},
 author = {Robert Hogenraad and Etienne Orianne},
 doi = {10.2190/8DB8-ELNU-FCDY-ENMR},
 eprint = {https://doi-org.crai.referencistas.com/10.2190/8DB8-ELNU-FCDY-ENMR},
 journal = {Imagination, Cognition and Personality},
 number = {2},
 pages = {127–145},
 title = {Imagery, Regressive Thinking, and Verbal Performance in Internal Monologue},
 url = {https://doi-org.crai.referencistas.com/10.2190/8DB8-ELNU-FCDY-ENMR},
 volume = {5},
 year = {1985i}
}

@article{doi:10.2190/8J11-0TY4-E6QP-JKRJ,
 author = {Gregg Solomon and Susan McLaughlin-Beltz},
 doi = {10.2190/8J11-0TY4-E6QP-JKRJ},
 eprint = {https://doi-org.crai.referencistas.com/10.2190/8J11-0TY4-E6QP-JKRJ},
 journal = {Imagination, Cognition and Personality},
 number = {1},
 pages = {81–88},
 title = {Book Reviews: Imagining the Impossible: Magical, Scientific, and Religious Thinking in Children, Patient-Based Approaches to Cognitive Neuroscience},
 url = {https://doi-org.crai.referencistas.com/10.2190/8J11-0TY4-E6QP-JKRJ},
 volume = {21},
 year = {2001p}
}

@article{doi:10.2190/BKML-B1QV-KDN4-8ULH,
 abstract = {This article reports on a year-long study of high school students learning computer programming. The study examined three issues: 1) what is the impact of programming on particular mathematical and reasoning abilities?; 2) what cognitive skills or abilities best predict programming ability?; and 3) what do students actually understand about programming after two years of high school study? The results showed that even after two years of study, many students had only a rudimentary understanding of programming. Consequently, it was not surprising to also find that programming experience (as opposed to expertise) does not appear to transfer to other domains which share analogous formal properties. The article concludes that we need to more closely study the pedagogy of programming and how expertise can be better attained before we prematurely go looking for significant and wide reaching transfer effects from programming.},
 author = {D. Midian Kurland and Roy D. Pea and Catherine Clement and Ronald Mawby},
 doi = {10.2190/BKML-B1QV-KDN4-8ULH},
 eprint = {https://doi-org.crai.referencistas.com/10.2190/BKML-B1QV-KDN4-8ULH},
 journal = {Journal of Educational Computing Research},
 number = {4},
 pages = {429–458},
 title = {A Study of the Development of Programming Ability and Thinking Skills in High School Students},
 url = {https://doi-org.crai.referencistas.com/10.2190/BKML-B1QV-KDN4-8ULH},
 volume = {2},
 year = {1986m}
}

@article{doi:10.2190/D5YQ-MMW6-V0FR-RNJQ,
 abstract = {In this article, we present a study focused on developing students’ understanding the ecology through participation in a technology-supported urban planning simulation—specifically, 11 high school students in Madison, Wisconsin acted as urban planners to redesign a local shopping street using a Geographic Information System (GIS) model. This experimental design was situated within the theory of pedagogical praxis, which suggests that modeling learning environments on authentic professional practices enables youth to develop a deeper understanding of important domains of inquiry (Shaffer, 2004). Results presented here suggest that through participation in the project students: a) developed an understanding of ecology; and b) developed this understanding through the urban planning practices and the features of the GIS model used during the project. Thus, we propose that this augmented by reality learning environment modeled on the professional practices of urban planners extends the theory of pedagogical praxis into the domain of ecology and offers a useful method for developing ecological understanding through participation in simulations that incorporate the authentic tools and practices of urban planning.},
 author = {Kelly L. Beckett and David Williamson Shaffer},
 doi = {10.2190/D5YQ-MMW6-V0FR-RNJQ},
 eprint = {https://doi-org.crai.referencistas.com/10.2190/D5YQ-MMW6-V0FR-RNJQ},
 journal = {Journal of Educational Computing Research},
 number = {1},
 pages = {31–52},
 title = {Augmented by Reality: The Pedagogical Praxis of Urban Planning as a Pathway to Ecological Thinking},
 url = {https://doi-org.crai.referencistas.com/10.2190/D5YQ-MMW6-V0FR-RNJQ},
 volume = {33},
 year = {2005c}
}

@article{doi:10.2190/EC.44.4.a,
 author = {P. G. Schrader and Kimberly A. Lawless},
 doi = {10.2190/EC.44.4.a},
 eprint = {https://doi-org.crai.referencistas.com/10.2190/EC.44.4.a},
 journal = {Journal of Educational Computing Research},
 number = {4},
 pages = {385–390},
 title = {Research on Immersive Environments and 21st Century Skills: an Introduction to the Special Issue},
 url = {https://doi-org.crai.referencistas.com/10.2190/EC.44.4.a},
 volume = {44},
 year = {2011p}
}

@article{doi:10.2190/EC.49.4.b,
 abstract = {The paper-and-pencil programming strategy (PPS) is a way of representing an idea logically by any representation that can be created using paper and pencil. It was developed for non-computer majors to improve their understanding and use of computational thinking and increase interest in learning computer science. A total of 110 non-majors in their sophomore year were assigned to either a Logo or a PPS course with attendance being 2 hours per week for 15 weeks. To measure the effectiveness of PPS, the Group Assessment of Logical Thinking and a self-assessment survey pre- and post-test were used. Findings indicated that PPS not only improved students’ overall logical thinking as much as did Logo programming learning, but also increased scores on one more subscale of logical thinking than did the Logo course. In addition, PPS significantly helped students understand the concept of computational thinking and increased their interest in learning computer science.},
 author = {Byeongsu Kim and Taehun Kim and Jonghoon Kim},
 doi = {10.2190/EC.49.4.b},
 eprint = {https://doi-org.crai.referencistas.com/10.2190/EC.49.4.b},
 journal = {Journal of Educational Computing Research},
 number = {4},
 pages = {437–459},
 title = {Paper-and-Pencil Programming Strategy toward Computational Thinking for Non-Majors: Design Your Solution},
 url = {https://doi-org.crai.referencistas.com/10.2190/EC.49.4.b},
 volume = {49},
 year = {2013l}
}

@article{doi:10.2190/EC.49.4.g,
 doi = {10.2190/EC.49.4.g},
 eprint = {https://doi-org.crai.referencistas.com/10.2190/EC.49.4.g},
 journal = {Journal of Educational Computing Research},
 number = {4},
 pages = {543–545},
 title = {Journal of Educational Computing Research Index—Contents of Volume 49, 2013},
 url = {https://doi-org.crai.referencistas.com/10.2190/EC.49.4.g},
 volume = {49},
 year = {2013y}
}

@article{doi:10.2190/EC.51.1.e,
 abstract = {Learning styles are increasingly being integrated into computational-enhanced earning environments and a great deal of recent research work is taking place in this area. The purpose of this study was to examine the impact of the computational experiment approach, learning styles, epistemic beliefs, and engagement with the inquiry process on the learning performance of pre-service engineering students. The study used the Felder-Silverman learning style model (FSLSM), in order to provide information for the relation of FSLSM with the learning environment in order to examine whether the strength of learning styles has an effect on the students’ learning performance in mismatched courses. Our objective was: a) to investigate whether students with a strong preference for a specific learning style have more difficulties in learning, if their learning style is not supported in the learning environment; b) if the methodology of the computational experiment has an impact on students independently of their learning style; and c) if the epistemological beliefs are related to different learning styles. The learning environment was based on the methodology of the computational experiment and applications were developed using the Easy Java Simulator software, while the inquiry based teaching and learning process was adopted. The questionnaire responses were gathered from 79 pre-service engineering students in a higher education institute in Greece. Results indicate that students with no preferred learning style have a better learning performance in mismatched courses.},
 author = {Sarantos Psycharis and Evanthia Botsari and George Chatzarakis},
 doi = {10.2190/EC.51.1.e},
 eprint = {https://doi-org.crai.referencistas.com/10.2190/EC.51.1.e},
 journal = {Journal of Educational Computing Research},
 number = {1},
 pages = {91–118},
 title = {Examining the Effects of Learning Styles, Epistemic Beliefs and the Computational Experiment Methodology on Learners’ Performance Using the Easy Java Simulator Tool in Stem Disciplines},
 url = {https://doi-org.crai.referencistas.com/10.2190/EC.51.1.e},
 volume = {51},
 year = {2014l}
}

@article{doi:10.2190/EC.51.2.c,
 abstract = {Scratch, a visual programming language, was used in many studies in computer science education. Most of them reported positive results by integrating Scratch into K-12 computer courses. However, the object-oriented concept, one of the important computational thinking skills, is not represented well in Scratch. Alice, another visual programming language, seems to have better illustration of the object-oriented concept for programming novices. To demonstrate effects of Alice and Scratch, we compared students’ responses to both visual programming languages, especially for students with low performances, in an introductory programming course. The relationships among learning engagement, learning anxiety, and learning playfulness were explored. The results could be referred to by computer science instructors to select proper visual programming language for corrective instruction.},
 author = {Chih-Kai Chang},
 doi = {10.2190/EC.51.2.c},
 eprint = {https://doi-org.crai.referencistas.com/10.2190/EC.51.2.c},
 journal = {Journal of Educational Computing Research},
 number = {2},
 pages = {185–204},
 title = {Effects of Using Alice and Scratch in an Introductory Programming Course for Corrective Instruction},
 url = {https://doi-org.crai.referencistas.com/10.2190/EC.51.2.c},
 volume = {51},
 year = {2014g}
}

@article{doi:10.2190/ECB0-MV0A-8AP9-N47Q,
 abstract = {In an effort to infuse problem-solving activities requiring higher-order thinking into inner-city classrooms, we investigated the use of microcomputers with a graphing application program to teach principles of the design and interpretation of graphs to a population of students with little or no prior knowledge of graphs or data analysis. This is an exploratory, naturalistic research study of students’ thoughts and behavior in this context. It differs from most earlier work in the field of computer-assisted graphing in three major ways: first, we focus broadly on a wide variety of types of graphs, used in several different subject matter contexts; second, we conduct research into problem solving processes in an unmodified whole-classroom environment; third, we supplement participant observations on student cognition with extensive behavioral sequence data gathered automatically and unobtrusively by the computer. We detail our attempts to develop, adapt, and apply quantitative methods of analysis of behavioral sequences which are potentially applicable to such data gathered for a large number of students.},
 author = {David F. Jackson and Carl F. Berger and Billie Jean Edwards},
 doi = {10.2190/ECB0-MV0A-8AP9-N47Q},
 eprint = {https://doi-org.crai.referencistas.com/10.2190/ECB0-MV0A-8AP9-N47Q},
 journal = {Journal of Educational Computing Research},
 number = {1},
 pages = {43–67},
 title = {Computer-Assisted Thinking Tools: Problem Solving in Graphical Data Analysis},
 url = {https://doi-org.crai.referencistas.com/10.2190/ECB0-MV0A-8AP9-N47Q},
 volume = {8},
 year = {1992i}
}

@article{doi:10.2190/IC.28.1.c,
 abstract = {Greater vividness of visual imagery correlated significantly with improved performance on a task in which 112 subjects generated and tested hypotheses about the rules of three-point perspective (3PP), but not with improvement on a similar task in which another 116 subjects applied previously known rules of two-point perspective (2PP). Given the 100-year history of research finding zero correlation between image vividness and visuospatial reasoning, the 2PP results are interpreted as support for Külpe’s thesis that the imageless thinker’s existing knowledge of imageless rules allows for the solution of spatial problems, without any construction of visual images. The 3PP results are interpreted as support for Kunzendorf and Reynold’s thesis that the vivid imager’s construction of test images from newly hypothesized rules allows for the refinement of those imageless rules, with less reliance on overt mistakes for corrective feedback.},
 author = {Robert G. Kunzendorf and Franz Buker},
 doi = {10.2190/IC.28.1.c},
 eprint = {https://doi-org.crai.referencistas.com/10.2190/IC.28.1.c},
 journal = {Imagination, Cognition and Personality},
 number = {1},
 pages = {5–35},
 title = {Should Visual Images Be Defined as Depictive Neural Patterns, or as Conscious Sensations Constructed from “Imageless” Visual Thoughts?},
 url = {https://doi-org.crai.referencistas.com/10.2190/IC.28.1.c},
 volume = {28},
 year = {2008m}
}

@article{doi:10.2190/IC.33.4.f,
 abstract = {In this factor-analytic study of imaging and thinking, measures of vivid visual and auditory imaging loaded onto a unitary “vivid imaging” factor, on which no measures of thinking loaded. This first factor-analytic finding, like previous findings of no correlation between vivid imaging and productive thinking, is consistent with Külpe’s classic argument and Kunzendorf’s contemporary argument that visually and auditorily imaged sensations are not building blocks for “spatial” and “temporal” thinking, respectively, but are sensory representations which vivid imagers construct from their imageless thoughts. Moving beyond individual differences in vivid imaging to individual differences in imageless thinking, this factor-analytic study found, in addition, that radically different measures of styles of thought load onto a unitary “heterarchical versus hierarchical thinking” factor. These differing measures of thinking styles instructed research participants to find embedded spatial and temporal patterns, to organize two unstructured data sets into matrices or outlines, and to rank their preferences for 12 competitive events—6 events defined by “how close, “how high,” or “how far” competitors can perform on a heterarchical measure, and 6 events defined by “how well synchronized,” “how long,” or “how fast” competitors can perform on a hierarchical measure. Engineering students in this research exhibited significantly more heterarchical thinking and significantly less hierarchical thinking; management students exhibited significantly less heterarchical thinking and significantly more hierarchical thinking; and male students and female students exhibited, on average, no significant differences in their modes of thinking. Based on these findings, the authors conclude that people’s “imageless thinking” can be characterized in terms of two orthogonal modes of thinking: heterarchical thinking and hierarchical thinking.},
 author = {Robert G. Kunzendorf and Nicole Dwyer and Michelle Foran and Alicia Pagan and Alesci Giuseppe and Brianna Milligan},
 doi = {10.2190/IC.33.4.f},
 eprint = {https://doi-org.crai.referencistas.com/10.2190/IC.33.4.f},
 journal = {Imagination, Cognition and Personality},
 number = {4},
 pages = {403–420},
 title = {Heterarchical versus Hierarchical Modes of Imageless Thinking},
 url = {https://doi-org.crai.referencistas.com/10.2190/IC.33.4.f},
 volume = {33},
 year = {2014j}
}

@article{doi:10.2190/J570-9MKA-7BHN-ULN9,
 abstract = {The constant emphasis on specialization produces university graduates who do not or cannot look at problems broadly. As a result, engineers, scientists and executives—indeed graduates in all fields—including the supposedly broad-based humanities—often cannot solve problems that require knowledge outside of their specializations. Or their narrowness causes them to commit embarrassing blunders that could be avoided if they took a broader view. The case of the British Westland Lysander P12 Ground Strafer aircraft illustrates the problem of narrow thinking. Very little direct information is available on this ingenious but obscure prototype airplane, but by examining many peripheral matters we can determine not only why the P12 was built but also how it was built. Further, we can also determine why it failed. Had the initial designers approached the problem in a broad way, and using information that was then available, they would have seen in advance that the project would fail. The case is instructive as an industrial problem, but it also demonstrates the value of global thinking methodology.},
 author = {John S. Harris},
 doi = {10.2190/J570-9MKA-7BHN-ULN9},
 eprint = {https://doi-org.crai.referencistas.com/10.2190/J570-9MKA-7BHN-ULN9},
 journal = {Journal of Technical Writing and Communication},
 number = {3},
 pages = {223–239},
 title = {Global Thinking, or the Utility of Trivia},
 url = {https://doi-org.crai.referencistas.com/10.2190/J570-9MKA-7BHN-ULN9},
 volume = {31},
 year = {2001j}
}

@article{doi:10.2190/L1TH-3V6M-2W5Q-8LTJ,
 abstract = {Bruner, Goodnow, and Austin’s (1956) research on concept development is re-examined from a connectionist perspective. A neural network was constructed which associates positive and negative instances of a concept with their corresponding attribute values. Two methods were used to help preserve the ecological validity of the input: 1) closely mapping the input to the actual visual stimuli; and 2) structuring the output layer based on Gagne’s (1962, 1985) work on human concept learning. This resulted in the addition of output units referred to as attribute context constraints. These units required the network to demonstrate the identification of attributes both relevant and irrelevant to the task of classification. Results suggest that the simultaneous learning of attributes guided the network in constructing a faster and more generalizable representation than when attribute constraints were absent. Results are discussed with respect to the advantages of computational approaches to studying learning.},
 author = {Michael Carbonaro},
 doi = {10.2190/L1TH-3V6M-2W5Q-8LTJ},
 eprint = {https://doi-org.crai.referencistas.com/10.2190/L1TH-3V6M-2W5Q-8LTJ},
 journal = {Journal of Educational Computing Research},
 number = {1},
 pages = {63–81},
 title = {Making a Connection between Computational Modeling and Educational Research},
 url = {https://doi-org.crai.referencistas.com/10.2190/L1TH-3V6M-2W5Q-8LTJ},
 volume = {28},
 year = {2003e}
}

@article{doi:10.2203/dose-response.09-030.Beam,
 abstract = {An essential part of toxicity and chemical screening is assessing the concentrated related effects of a test article. Most often this concentration-response is a nonlinear, necessitating sophisticated regression methodologies. The parameters derived from curve fitting are essential in determining a test article’s potency (EC50) and efficacy (Emax) and variations in model fit may lead to different conclusions about an article’s performance and safety. Previous approaches have leveraged advanced statistical and mathematical techniques to implement nonlinear least squares (NLS) for obtaining the parameters defining such a curve. These approaches, while mathematically rigorous, suffer from initial value sensitivity, computational intensity, and rely on complex and intricate computational and numerical techniques. However if there is a known mathematical model that can reliably predict the data, then nonlinear regression may be equally viewed as parameter optimization. In this context, one may utilize proven techniques from machine learning, such as evolutionary algorithms, which are robust, powerful, and require far less computational framework to optimize the defining parameters. In the current study we present a new method that uses such techniques, Evolutionary Algorithm Dose Response Modeling (EADRM), and demonstrate its effectiveness compared to more conventional methods on both real and simulated data.},
 author = {Andrew L. Beam and Alison A. Motsinger-Reif},
 doi = {10.2203/dose-response.09-030.Beam},
 eprint = {https://doi-org.crai.referencistas.com/10.2203/dose-response.09-030.Beam},
 journal = {Dose-Response},
 note = {PMID:22013401},
 number = {3},
 pages = {dose-response.09-030.Beam},
 title = {Optimization of Nonlinear Dose- and Concentration-Response Models Utilizing Evolutionary Computation},
 url = {https://doi-org.crai.referencistas.com/10.2203/dose-response.09-030.Beam},
 volume = {9},
 year = {2011a}
}

@article{doi:10.2304/ciec.2005.6.2.5,
 abstract = {Sixty-five Year 2 children with ages ranging from six to seven years participated in a teaching experiment to introduce functional thinking. The results show that young children are capable of generalising, can provide examples of relations and functions, can describe the inverse of such relationships and give valid reasons for how they found the inverse relationships. They also indicate that specific features of instruction assist this process, particularly abstracting underlying mathematical relationships, notably the materials used by the teacher and the children, the types of activities and the questions asked by the teacher. This leads to specific implications for the teaching of arithmetic in the early years.},
 author = {Elizabeth Warren and Tom Cooper},
 doi = {10.2304/ciec.2005.6.2.5},
 eprint = {https://doi-org.crai.referencistas.com/10.2304/ciec.2005.6.2.5},
 journal = {Contemporary Issues in Early Childhood},
 number = {2},
 pages = {150–162},
 title = {Introducing Functional Thinking in Year 2: A Case Study of Early Algebra Teaching},
 url = {https://doi-org.crai.referencistas.com/10.2304/ciec.2005.6.2.5},
 volume = {6},
 year = {2005r}
}

@article{doi:10.2304/elea.2013.10.3.324,
 author = {Daniel Araya},
 doi = {10.2304/elea.2013.10.3.324},
 eprint = {https://doi-org.crai.referencistas.com/10.2304/elea.2013.10.3.324},
 journal = {E-Learning and Digital Media},
 number = {3},
 pages = {324–327},
 title = {Thinking Forward: Conrad Wolfram on the Computational Knowledge Economy},
 url = {https://doi-org.crai.referencistas.com/10.2304/elea.2013.10.3.324},
 volume = {10},
 year = {2013e}
}

@article{doi:10.2304/pfie.2014.12.6.832,
 abstract = {Information and communications technology (ICT) is crucial in any contemporary society, especially if its online presence is to be widely significant, but, in a national context, it is important to investigate whether there is a compelling ICT ‘politic’ in the education sector in Turkey. This study specifically focuses on the ICT for Educational Development (ICT4ED) (Fatih) project, valued at US$8 billion, which is an embodiment of future educational reform in centralist Turkey. The study investigates the level of policy-making capacity within the scope of the Fatih project, which promises to fully integrate ICT into education in order to solve many educational issues, such as establishing entrepreneurialism in education, improving ICT sectors, exporting educational services to other nations for profit, and, ultimately, meeting the overarching purpose of Turkey becoming a competitive nation. The discourse of the government is to explore technological opportunities within education in order to create new ‘profitable’ avenues that conceive education as a ‘commodity’ and a ‘private good’, which is another feature of fragmented centralisation through public-private partnership(s) for both covert and overt privatisations. While the Fatih project defines the future as having new, ‘fully integrated ICT in education’, this study presents three main normative arguments — with regard to the project’s political execution, technical development and philosophical conception — to refute its objectives and highlight that it is destined to fail, and to suggest what urgent matters the ICT4ED politic should highlight in future education.},
 author = {Hüseyin Tolu},
 doi = {10.2304/pfie.2014.12.6.832},
 eprint = {https://doi-org.crai.referencistas.com/10.2304/pfie.2014.12.6.832},
 journal = {Policy Futures in Education},
 number = {6},
 pages = {832–849},
 title = {The Politics of the ICT4ED (Fatih) Project in Turkey},
 url = {https://doi-org.crai.referencistas.com/10.2304/pfie.2014.12.6.832},
 volume = {12},
 year = {2014s}
}

@article{doi:10.2304/plat.2005.5.2.153,
 abstract = {The teaching of statistics to psychology undergraduates has traditionally included formulae and methods for conducting statistical tests without the aid of a computer. Does this requirement to master the formulae encourage deeper understanding or merely provoke anxiety? The relationship between competence in calculation and statistical thinking was examined using questions from an exam set at the end of a compulsory first year module. In addition, scores on a premodule test and coursework grades were recorded, and used to control for prior ability and learning effort. The control variables jointly accounted for 26% of the variance in statistical thinking, whereas calculation accounted for only a further 2%. The contribution made by competence in calculation, though statistically significant, was not large.},
 author = {Rachel Seabrook},
 doi = {10.2304/plat.2005.5.2.153},
 eprint = {https://doi-org.crai.referencistas.com/10.2304/plat.2005.5.2.153},
 journal = {Psychology Learning & Teaching},
 number = {2},
 pages = {153–161},
 title = {Is the Teaching of Statistical Calculations Helpful to Students’ Statistical Thinking?},
 url = {https://doi-org.crai.referencistas.com/10.2304/plat.2005.5.2.153},
 volume = {5},
 year = {2006n}
}

@article{doi:10.2307/3556626,
 author = {John Freeman},
 doi = {10.2307/3556626},
 eprint = {https://doi-org.crai.referencistas.com/10.2307/3556626},
 journal = {Administrative Science Quarterly},
 number = {1},
 pages = {140–142},
 title = {Book Reviews},
 url = {https://doi-org.crai.referencistas.com/10.2307/3556626},
 volume = {48},
 year = {2003f}
}

@article{doi:10.2310/JIM.0b013e318224d8cc,
 abstract = {Today, there is an ever-increasing amount of biological and clinical data available that could be used to enhance a systems-based understanding of disease progression through innovative computational analysis. In this article, we review a selection of published research regarding computational methods, primarily from systems biology, which support translational research from the molecular level to the bedside, with a focus on applications in trauma and critical care. Trauma is the leading cause of mortality in Americans younger than 45 years, and its rapid progression offers both opportunities and challenges for computational analysis of trends in molecular patterns associated with outcomes and therapeutic interventions. This review presents methods and domain-specific examples that may inspire the development of new algorithms and computational methods that use both molecular and clinical data for diagnosis, prognosis, and therapy in disease progression.},
 author = {Mary F. McGuire and Madurai Sriram Iyengar and David W. Mercer},
 doi = {10.2310/JIM.0b013e318224d8cc},
 eprint = {https://doi-org.crai.referencistas.com/10.2310/JIM.0b013e318224d8cc},
 journal = {Journal of Investigative Medicine},
 number = {6},
 pages = {893–903},
 title = {Computational Approaches for Translational Clinical Research in Disease Progression},
 url = {https://doi-org.crai.referencistas.com/10.2310/JIM.0b013e318224d8cc},
 volume = {59},
 year = {2011m}
}

@article{doi:10.2466/pms.102.3.721-735,
 abstract = {This study tested the hypothesis that individual differences in generalized control perception for 43 undergraduate adults may be reflected in Spontaneous Eye Blink Rates during conversation in an interview. Control perception was assessed by means of Rotter’s internal-external Locus of Control questionnaires, while Spontaneous Eye Blink Rates were computed from filmed videos of interviews consisting of a series of questions which could presumably have triggered different mental states. Pearson correlations and linear regression analyses indicated that the individual differences in Spontaneous Eye Blink Rates did not differ significantly across different questions, but that Spontaneous Eye Blink Rates measured over the entire interview correlated positively and significantly with an internal Locus of Control (r = .26). This could be interpreted as modest but corroborative evidence that a personality trait reflecting control perception may have a biological component. The possible roles of dopamine neurotransmission and frontal cortex involvement in higher cognition and Locus of Control are discussed.},
 author = {Carolyn H. Declerck and Bert De Brabander and Christophe Boone},
 doi = {10.2466/pms.102.3.721-735},
 eprint = {https://doi-org.crai.referencistas.com/10.2466/pms.102.3.721-735},
 journal = {Perceptual and Motor Skills},
 note = {PMID:16916151},
 number = {3},
 pages = {721–735},
 title = {Spontaneous Eye Blink Rates Vary According to Individual Differences in Generalized Control Perception},
 url = {https://doi-org.crai.referencistas.com/10.2466/pms.102.3.721-735},
 volume = {102},
 year = {2006b}
}

@article{doi:10.2466/pms.1978.46.1.3,
 abstract = {This paper is a general introduction to the lens model, written primarily for individuals in a variety of applied areas (Clinical Psychology, Nursing, Public Administration, Clinical Pharmacology, and Education). First the background issues of Social Judgment Theory, bivariate and multivariate regression are discussed; then the actual experimental procedures and analyses are demonstrated. Finally there is a brief discussion of the application of the model to the study of conflict resolution and interpersonal learning.},
 author = {Don Beal and John S. Gillis and Tom Stewart},
 doi = {10.2466/pms.1978.46.1.3},
 eprint = {https://doi-org.crai.referencistas.com/10.2466/pms.1978.46.1.3},
 journal = {Perceptual and Motor Skills},
 number = {1},
 pages = {3–28},
 title = {The Lens Model: Computational Procedures and Applications},
 url = {https://doi-org.crai.referencistas.com/10.2466/pms.1978.46.1.3},
 volume = {46},
 year = {1978a}
}

@article{doi:10.2466/pms.1978.46.1.323,
 abstract = {The present study investigated whether for children with analytic-nonanalytic/impulsive-reflective cognitive styles the tendency to exhibit different types of computational errors in arithmetic are related. 198 students from Grades 3 and 6 were drawn from two school districts. Tests of cognitive style and a computation test were administered. Subjects’ types of errors and cognitive styles only tentatively confirmed a relationship between the dimensions of cognitive style and types of errors.},
 author = {Jon M. Engelhardt},
 doi = {10.2466/pms.1978.46.1.323},
 eprint = {https://doi-org.crai.referencistas.com/10.2466/pms.1978.46.1.323},
 journal = {Perceptual and Motor Skills},
 number = {1},
 pages = {323–330},
 title = {Cognitive Style and Children’s Computational Errors},
 url = {https://doi-org.crai.referencistas.com/10.2466/pms.1978.46.1.323},
 volume = {46},
 year = {1978d}
}

@article{doi:10.2466/pms.1978.47.2.363,
 abstract = {A variety of fluency-oriented divergent thinking tasks, evaluation-type problem-solving tasks, and the Coopersmith Self-esteem Inventory were administered to 192 black and Puerto Rican children in Grades 4, 5, and 6 from a low-income area of New York City. There was little relationship between fluency and self-esteem, and all children appeared to have great difficulty with tasks involving skills of evaluation. Results were discussed in terms of the effects of an evaluation-oriented environment.},
 author = {John C. Houtz and Robert H. Phillips},
 doi = {10.2466/pms.1978.47.2.363},
 eprint = {https://doi-org.crai.referencistas.com/10.2466/pms.1978.47.2.363},
 journal = {Perceptual and Motor Skills},
 number = {2},
 pages = {363–368},
 title = {Relation of Self-Esteem and Divergent Thinking among Black and Puerto Rican Children},
 url = {https://doi-org.crai.referencistas.com/10.2466/pms.1978.47.2.363},
 volume = {47},
 year = {1978l}
}

@article{doi:10.2466/pms.1985.60.1.203,
 abstract = {A representation for the visual recognition of skilled arm movements is proposed that lies within Mart and Vaina’s (1982) three-dimensional model representation for shape movements. Algorithms for segmenting arm movements into pieces are proposed. It is suggested that for a large class of arm movements recognition could be reliably achieved based only on the description of the hand shape and path in the body coordinates, without needing the detailed description of the variation of all the joint angles of the arm.},
 author = {Lucia Vaina and Youcef Bennour},
 doi = {10.2466/pms.1985.60.1.203},
 eprint = {https://doi-org.crai.referencistas.com/10.2466/pms.1985.60.1.203},
 journal = {Perceptual and Motor Skills},
 note = {PMID:3982933},
 number = {1},
 pages = {203–228},
 title = {A Computational Approach to Visual Recognition of Arm Movements},
 url = {https://doi-org.crai.referencistas.com/10.2466/pms.1985.60.1.203},
 volume = {60},
 year = {1985r}
}

@article{doi:10.2466/pr0.1963.12.2.595,
 abstract = {Five kinds of data were gathered on various groups of college females who differ in their approach to and success in solving problems. Ss high and low on intuitiveness and on success, as defined here, were studied in some detail Usual measures of intellectual capacity and standard personality measures discriminated only slightly between the groups, but more detailed study of the personality scales, interviews, and self-concept materials revealed these groups to have clearly distinguishable patterns of personality characteristics.},
 author = {Malcolm R. Westcott and Jane H. Ranzoni},
 doi = {10.2466/pr0.1963.12.2.595},
 eprint = {https://doi-org.crai.referencistas.com/10.2466/pr0.1963.12.2.595},
 journal = {Psychological Reports},
 number = {2},
 pages = {595-613E},
 title = {Correlates of Intuitive Thinking},
 url = {https://doi-org.crai.referencistas.com/10.2466/pr0.1963.12.2.595},
 volume = {12},
 year = {1963r}
}

@article{doi:10.2466/pr0.1966.18.2.335,
 abstract = {This study was designed to determine on which of 18 creativity factors a group of 60 intellectually bright students in regular fifth grade classes would differ from a group of intellectually normal Ss (matched on sex, race, socio-economic position, school, and classroom setting) when variables of possible influence were either not controlled or were covaried in different ways. Eight Guilford-type tests were used to assess factors of creativity. Without statistical control, the gifted exceeded significantly the normals on 8 of 14 verbal factors but on none of the non-verbal dimensions. When different variables were controlled singly and in combination, significant differences appeared on the dependent variables. Separate correlations for gifted and normal Ss indicated differences in the way intelligence and achievement related to creativity factors for the groups.},
 author = {Robert M. Smith and John T. Neisworth},
 doi = {10.2466/pr0.1966.18.2.335},
 eprint = {https://doi-org.crai.referencistas.com/10.2466/pr0.1966.18.2.335},
 journal = {Psychological Reports},
 number = {2},
 pages = {335–341},
 title = {Creative Thinking Abilities of Intellectually Superior Children in the Regular Grades},
 url = {https://doi-org.crai.referencistas.com/10.2466/pr0.1966.18.2.335},
 volume = {18},
 year = {1966o}
}

@article{doi:10.2466/pr0.94.3c.1393-1403,
 abstract = {Previous investigators have suggested that the personality variable tolerance for error is related to success in computational estimation. However, this suggestion has not been tested directly. This study examined the relationship between performance on a computational estimation test and scores on the NEO–Five Factor Inventory, a measure of the Big Five personality traits, including Openness, an index of tolerance for ambiguity. Other variables included SAT–I Verbal and Mathematics scores and self-rated mathematics ability. Participants were 65 college students. There was no significant relationship between the tolerance variable and computational estimation performance. There was a modest negative relationship between Agreeableness and estimation performance. The skepticism associated with the negative pole of the Agreeableness dimension may be important to pursue in further understanding of estimation ability.},
 author = {Thomas P. Hogan and Laurie A. Wyckoff and Paul Krebs and William Jones and Mark P. Fitzgerald},
 doi = {10.2466/pr0.94.3c.1393-1403},
 eprint = {https://doi-org.crai.referencistas.com/10.2466/pr0.94.3c.1393-1403},
 journal = {Psychological Reports},
 note = {PMID:15362423},
 number = {3_suppl},
 pages = {1393–1403},
 title = {Tolerance for Error and Computational Estimation Ability},
 url = {https://doi-org.crai.referencistas.com/10.2466/pr0.94.3c.1393-1403},
 volume = {94},
 year = {2004h}
}

@article{doi:10.2500/ajra.2010.24.3428,
 abstract = {Background Septal deviation is an extremely common anatomic variation in healthy adults. However, there are no standard criteria to determine when a deviated septum is clinically relevant. Presently, selection of patients for septoplasty is based on mostly clinical examination, which is prone to observer bias and may lead to unsuccessful treatment. The objective of this article is twofold. First, we investigate whether the location of a septal deviation within the nasal passages affects nasal resistance. Second, we test whether computer simulations are consistent with rhinomanometry studies in predicting that anterior septal deviations increase nasal resistance more than posterior deviations. Methods A three-dimensional computational model of a healthy nose was created from computed tomography scans. Geometry-deforming software was used to produce models with septal deviations. Computational fluid dynamics techniques were used to simulate nasal airflow and compute nasal resistance. Results Our results revealed that the posterior nasal cavity can accommodate significant septal deviations without a substantial increase in airway resistance. In contrast, a deviation in the nasal valve region more than doubled nasal resistance. These findings are in good agreement with the rhinomanometry literature and with the observation that patients with anterior septal deviations benefit the most from septoplasty. Conclusions In the model, anterior septal deviations increased nasal resistance more than posterior deviations. This suggests, in agreement with the literature, that other causes of nasal obstruction (dysfunction of the nasal valve, allergy, etc.) should be carefully considered in patients with posterior septal deviations because such deviations may not affect nasal resistance. This study illustrates how computational modeling and virtual manipulation of the nasal geometry are useful to investigate nasal physiology.},
 author = {Guilherme J.M. Garcia and John S. Rhee and Brent A. Senior and Julia S. Kimbell},
 doi = {10.2500/ajra.2010.24.3428},
 eprint = {https://doi-org.crai.referencistas.com/10.2500/ajra.2010.24.3428},
 journal = {American Journal of Rhinology & Allergy},
 note = {PMID:20109325},
 number = {1},
 pages = {e46–e53},
 title = {Septal Deviation and Nasal Resistance: An Investigation using Virtual Surgery and Computational Fluid Dynamics},
 url = {https://doi-org.crai.referencistas.com/10.2500/ajra.2010.24.3428},
 volume = {24},
 year = {2010e}
}

@article{doi:10.2500/ajra.2016.30.4266,
 abstract = {Background Maxillary antrostomy is commonly performed during endoscopic sinus surgery. Little is known about the association surrounding recalcitrant maxillary sinusitis, antrostomy size, and intranasal airflow changes. Furthermore, the interaction between sinus mucosa and airflow is poorly understood. This study used computational fluid dynamics (CFD) modeling to investigate postoperative airflow characteristics between diseased and nondiseased maxillary sinuses in subjects with recurrent disease. Methods A retrospective review of patients from a tertiary-level academic rhinology practice was performed. Seven subjects with endoscopic evidence of postoperative maxillary sinus disease that presented as chronic unilateral crusting at least 1 year after bilateral maxillary antrostomies were selected. A three-dimensional model of each subject’s sinonasal cavity was created from postoperative computed tomographies and used for CFD analysis. Results Although the variables investigated between diseased and nondiseased sides were not statistically significant, the diseased side in six subjects had a smaller antrostomy, and five of these subjects had both reduced nasal unilateral airflow and increased unilateral nasal resistance on the diseased side. The ratio of posterior wall shear stress (WSS) of the maxillary sinus to the total WSS was higher on the diseased side in six subjects. Results also showed strong correlations between antrostomy and CFD variables on the diseased side than on the nondiseased side. Conclusion This pilot study showed that the majority of the simulated sinonasal models exhibited common characteristics on the side with persistent disease, such as smaller antrostomy, reduced nasal airflow, increased nasal resistance, and increased posterior WSS. Although statistical significance was not established, this study provided preliminary insight into variables to consider in a larger cohort study.},
 author = {Kevin J. Choi and David W. Jang and Matthew D. Ellison and Dennis O. Frank-Ito},
 doi = {10.2500/ajra.2016.30.4266},
 eprint = {https://doi-org.crai.referencistas.com/10.2500/ajra.2016.30.4266},
 journal = {American Journal of Rhinology & Allergy},
 note = {PMID:26867527},
 number = {1},
 pages = {29–36},
 title = {Characterizing Airflow Profile in the Postoperative Maxillary Sinus by Using Computational Fluid Dynamics Modeling: A Pilot Study},
 url = {https://doi-org.crai.referencistas.com/10.2500/ajra.2016.30.4266},
 volume = {30},
 year = {2016d}
}

@article{doi:10.3102/00028312003002131,
 author = {Fred W. Ohnmacht},
 doi = {10.3102/00028312003002131},
 eprint = {https://doi-org.crai.referencistas.com/10.3102/00028312003002131},
 journal = {American Educational Research Journal},
 number = {2},
 pages = {131–138},
 title = {Achievement, Anxiety and Creative Thinking},
 url = {https://doi-org.crai.referencistas.com/10.3102/00028312003002131},
 volume = {3},
 year = {1966i}
}

@article{doi:10.3102/00028312026004499,
 abstract = {This study investigated teachers’ use of knowledge from research on children’s mathematical thinking and how their students’ achievement is influenced as a result. Twenty first grade teachers, assigned randomly to an experimental treatment, participated in a month-long workshop in which they studied a research-based analysis of children’s development of problem-solving skills in addition and subtraction. Other first grade teachers (n = 20) were assigned randomly to a control group. Although instructional practices were not prescribed, experimental teachers taught problem solving significantly more and number facts significantly less than did control teachers. Experimental teachers encouraged students to use a variety of problem-solving strategies, and they listened to processes their students used significantly more than did control teachers. Experimental teachers knew more about individual students’ problem-solving processes, and they believed that instruction should build on students’ existing knowledge more than did control teachers. Students in experimental classes exceeded students in control classes in number fact knowledge, problem solving, reported understanding, and reported confidence in their problem-solving abilities.},
 author = {Thomas P. Carpenter and Elizabeth Fennema and Penelope L. Peterson and Chi-Pang Chiang and Megan Loef},
 doi = {10.3102/00028312026004499},
 eprint = {https://doi-org.crai.referencistas.com/10.3102/00028312026004499},
 journal = {American Educational Research Journal},
 number = {4},
 pages = {499–531},
 title = {Using Knowledge of Children’s Mathematics Thinking in Classroom Teaching: An Experimental Study},
 url = {https://doi-org.crai.referencistas.com/10.3102/00028312026004499},
 volume = {26},
 year = {1989c}
}

@article{doi:10.3102/00028312028004737,
 abstract = {A framework for thinking about teacher empowerment is presented. Two important dimensions are used as a basis for highlighting some important similarities and differences emerging in the literature on empowerment. The first relates to the context in which the process occurs; some educators highlight the importance of the personal context (i.e., conversations with self), while others assume more of an outward perspective in their thinking about empowerment (i.e., conversations with settings). The second important dimension relates to the focus or agenda of the conversations. Here, a distinction is made between agendas that are more epistemological in nature and those that are more political. An illustration is provided of how this framework can be used to better understand the dynamics involved in collaborative work with teachers.},
 author = {Richard S. Prawat},
 doi = {10.3102/00028312028004737},
 eprint = {https://doi-org.crai.referencistas.com/10.3102/00028312028004737},
 journal = {American Educational Research Journal},
 number = {4},
 pages = {737–757},
 title = {Conversations With Self and Settings: A Framework for Thinking About Teacher Empowerment},
 url = {https://doi-org.crai.referencistas.com/10.3102/00028312028004737},
 volume = {28},
 year = {1991k}
}

@article{doi:10.3102/00028312037003747,
 abstract = {Current research and theory indicate that college students’ scientific and statistical reasoning skills are deficient, but can be improved through instruction. Accordingly, an innovative statistics course was developed for the undergraduate education curriculum at the University of Wisconsin—Madison. The course promoted the idea “that the purpose of statistics is to organize a useful argument from quantitative evidence based on a form of principled rhetoric” (Abelson, 1995, pp. xiii). Most instruction was anchored to mentored, small-group collaborative activities that simulated complex, real-life problem solving. In conjunction with the second offering, evidence of student growth was obtained from pre- and post-course interviews designed to assess students’ ability to reason with statistical evidence from everyday sources. Both quantitative and qualitative analyses indicated that students made meaningful gains in their ability to reason statistically. Analyses also pointed to specific conceptual confusions, some related to course design. Students’ reactions Io the course were variable.},
 author = {Sharon J. Derry and Joel R. Levin and Helen P. Osana and Melanie S. Jones and Michael Peterson},
 doi = {10.3102/00028312037003747},
 eprint = {https://doi-org.crai.referencistas.com/10.3102/00028312037003747},
 journal = {American Educational Research Journal},
 number = {3},
 pages = {747–773},
 title = {Fostering Students’ Statistical and Scientific Thinking: Lessons Learned                From an Innovative College Course},
 url = {https://doi-org.crai.referencistas.com/10.3102/00028312037003747},
 volume = {37},
 year = {2000e}
}

@article{doi:10.3102/0013189X014007011,
 abstract = {Contemporary beliefs about the impact of information-processing technology (IPT) on thinking are examined. Whereas some suggest that learning to program and other contacts with IPT will empower thinking, it is argued from both theory and evidence that typical contacts with IPT today do not meet certain conditions for significantly reshaping thought. Whereas others suggest that IPT will have a narrowing and dehumanizing influence, it is argued that the striking diversification of IPT now underway will eventually allow for many styles of involvement. In the long term, as this diversification spreads to nearly all aspects of society, thinking may change in certain basic ways as it has in response to literacy and print.},
 author = {D. N. PERKINS},
 doi = {10.3102/0013189X014007011},
 eprint = {https://doi-org.crai.referencistas.com/10.3102/0013189X014007011},
 journal = {Educational Researcher},
 number = {7},
 pages = {11–17},
 title = {The Fingertip Effect: How Information-Processing Technology Shapes Thinking},
 url = {https://doi-org.crai.referencistas.com/10.3102/0013189X014007011},
 volume = {14},
 year = {1985o}
}

@article{doi:10.3102/0013189X016001022,
 author = {Seymour Papert},
 doi = {10.3102/0013189X016001022},
 eprint = {https://doi-org.crai.referencistas.com/10.3102/0013189X016001022},
 journal = {Educational Researcher},
 number = {1},
 pages = {22–30},
 title = {Information Technology and Education: Computer Criticism vs. Technocentric Thinking},
 url = {https://doi-org.crai.referencistas.com/10.3102/0013189X016001022},
 volume = {16},
 year = {1987o}
}

@article{doi:10.3102/0013189X024008025,
 author = {Lorrie A. Shepard and Carribeth L. Bliem},
 doi = {10.3102/0013189X024008025},
 eprint = {https://doi-org.crai.referencistas.com/10.3102/0013189X024008025},
 journal = {Educational Researcher},
 number = {8},
 pages = {25–32},
 title = {Research news and Comment: Parents’ Thinking About Standardized Tests and Performance Assessments},
 url = {https://doi-org.crai.referencistas.com/10.3102/0013189X024008025},
 volume = {24},
 year = {1995n}
}

@article{doi:10.3102/0013189X027005006,
 author = {Elizabeth Fennema and Thomas P. Carpenter and Victoria R. Jacobs and Megan L. Franke and Linda W. Levi},
 doi = {10.3102/0013189X027005006},
 eprint = {https://doi-org.crai.referencistas.com/10.3102/0013189X027005006},
 journal = {Educational Researcher},
 number = {5},
 pages = {6–11},
 title = {A Longitudinal Study of Gender Differences in Young Children’s Mathematical Thinking},
 url = {https://doi-org.crai.referencistas.com/10.3102/0013189X027005006},
 volume = {27},
 year = {1998a}
}

@article{doi:10.3102/0013189X12463051,
 abstract = {Jeannette Wing’s influential article on computational thinking 6 years ago argued for adding this new competency to every child’s analytical ability as a vital ingredient of science, technology, engineering, and mathematics (STEM) learning. What is computational thinking? Why did this article resonate with so many and serve as a rallying cry for educators, education researchers, and policy makers? How have they interpreted Wing’s definition, and what advances have been made since Wing’s article was published? This article frames the current state of discourse on computational thinking in K–12 education by examining mostly recently published academic literature that uses Wing’s article as a springboard, identifies gaps in research, and articulates priorities for future inquiries.},
 author = {Shuchi Grover and Roy Pea},
 doi = {10.3102/0013189X12463051},
 eprint = {https://doi-org.crai.referencistas.com/10.3102/0013189X12463051},
 journal = {Educational Researcher},
 number = {1},
 pages = {38–43},
 title = {Computational Thinking in K–12: A Review of the State of the Field},
 url = {https://doi-org.crai.referencistas.com/10.3102/0013189X12463051},
 volume = {42},
 year = {2013g}
}

@article{doi:10.3102/0013189X19872497,
 abstract = {The Every Student Succeeds Act of 2015 mandates that English language proficiency (ELP) standards align with content standards. As the fast-growing population of English learners (ELs) is expected to meet college- and career-ready content standards, the purpose of this article is to highlight key issues in aligning ELP standards with content standards. The overarching question is how to align ELP standards with academically rigorous and language-intensive disciplinary practices of content standards while respecting and maintaining the nature of the discipline within each area. I begin by describing contributions and shortcomings of content standards and ELP standards. Next, I propose consideration of three components in aligning ELP standards with content standards: (a) norms of disciplinary practices across content areas, (b) developmental progressions of disciplinary practices across K–12 grade levels or bands and across content areas, and (c) language use across levels of English proficiency. For each component, the challenges in establishing alignment and potential trade-offs in addressing these challenges are discussed. Finally, I highlight how these challenges present opportunities for substantive collaboration between EL education and content areas to move these fields forward and ensure ELs achieve academically rigorous content standards while developing ELP.},
 author = {Okhee Lee},
 doi = {10.3102/0013189X19872497},
 eprint = {https://doi-org.crai.referencistas.com/10.3102/0013189X19872497},
 journal = {Educational Researcher},
 number = {8},
 pages = {534–542},
 title = {Aligning English Language Proficiency Standards With Content Standards: Shared Opportunity and Responsibility Across English Learner Education and Content Areas},
 url = {https://doi-org.crai.referencistas.com/10.3102/0013189X19872497},
 volume = {48},
 year = {2019k}
}

@article{doi:10.3102/0013189X20923708,
 abstract = {With the release of the consensus report English Learners in STEM Subjects: Transforming Classrooms, Schools, and Lives, we highlight foundational constructs and perspectives associated with STEM subjects and language with English learners (ELs) that frame the report. The purpose here is to elevate these constructs and perspectives for discussion among the broader education research community. First, we provide an overview of the unique contributions of the report to move the ELs and STEM fields forward. Second, we describe ELs in terms of their heterogeneity and the inconsistency of educational policies that affect their learning opportunities in STEM subjects. Third, we describe contemporary views on STEM subjects and language with ELs that indicate that instructional shifts across STEM subjects and language are mutually supportive. Fourth, we describe promising instructional strategies to promote STEM learning and language development with ELs. Lastly, we close the article by reimagining STEM education with ELs and offer potential next steps. These foundational constructs and perspectives on STEM subjects and language with ELs are critical because they provide the conceptual grounding for the design of the education system for ELs. The report could contribute to building a knowledge base for ELs in STEM subjects and language as education research, policy, and practice converge to reimagine what is possible to both support and challenge ELs to learn academically rigorous content standards that are expected of all students.},
 author = {Okhee Lee and Amy Stephens},
 doi = {10.3102/0013189X20923708},
 eprint = {https://doi-org.crai.referencistas.com/10.3102/0013189X20923708},
 journal = {Educational Researcher},
 number = {6},
 pages = {426–432},
 title = {English Learners in STEM Subjects: Contemporary Views on STEM Subjects and Language With English Learners},
 url = {https://doi-org.crai.referencistas.com/10.3102/0013189X20923708},
 volume = {49},
 year = {2020k}
}

@article{doi:10.3102/0013189X211057904,
 abstract = {Over the past decade, initiatives around the world have introduced computing into K–12 education under the umbrella of computational thinking. While initial implementations focused on skills and knowledge for college and career readiness, more recent framings include situated computational thinking (identity, participation, creative expression) and critical computational thinking (political and ethical impacts of computing, justice). This expansion reflects a revaluation of what it means for learners to be computationally-literate in the 21st century. We review the current landscape of K–12 computing education, discuss interactions between different framings of computational thinking, and consider how an encompassing framework of computational literacies clarifies the importance of computing for broader K–12 educational priorities as well as key unresolved issues.},
 author = {Yasmin B. Kafai and Chris Proctor},
 doi = {10.3102/0013189X211057904},
 eprint = {https://doi-org.crai.referencistas.com/10.3102/0013189X211057904},
 journal = {Educational Researcher},
 number = {2},
 pages = {146–151},
 title = {A Revaluation of Computational Thinking in K–12 Education: Moving Toward Computational Literacies},
 url = {https://doi-org.crai.referencistas.com/10.3102/0013189X211057904},
 volume = {51},
 year = {2022h}
}

@article{doi:10.3102/0013189X231206172,
 abstract = {This synthesis examines recent science education research on multilingual students’ experiences with language-rich science practices. Adopting a translanguaging lens, we explore how researchers’ language conceptualizations impact the science practices they study and the ways multilingual students are positioned. This analysis helps us understand the extent to which recent research is disrupting, or sustaining, minoritizing narratives about multilingual students and how they sensemake in science. Based on our findings, we suggest researchers: (1) reflect upon and expand their views of language, which will enable the field to develop more nuanced understandings of how language use across linguistic and multimodal resources permeates all science practices, and (2) consider how to expand multilingual students’ language repertoires for sensemaking while also valuing students’ existing language resources and practices.},
 author = {María González-Howard and Sage Andersen and Karina Méndez Pérez and Enrique Suárez},
 doi = {10.3102/0013189X231206172},
 eprint = {https://doi-org.crai.referencistas.com/10.3102/0013189X231206172},
 journal = {Educational Researcher},
 number = {9},
 pages = {570–579},
 title = {Language Views for Scientific Sensemaking Matter: A Synthesis of Research on Multilingual Students’ Experiences with Science Practices Through a Translanguaging Lens},
 url = {https://doi-org.crai.referencistas.com/10.3102/0013189X231206172},
 volume = {52},
 year = {2023j}
}

@article{doi:10.3102/00346543015005441,
 author = {Irving Lorge},
 doi = {10.3102/00346543015005441},
 eprint = {https://doi-org.crai.referencistas.com/10.3102/00346543015005441},
 journal = {Review of Educational Research},
 number = {5},
 pages = {441–446},
 title = {Chapter VIII: Computational Technics},
 url = {https://doi-org.crai.referencistas.com/10.3102/00346543015005441},
 volume = {15},
 year = {1945k}
}

@article{doi:10.3102/00346543018005485,
 author = {Nicholas A. Fattu},
 doi = {10.3102/00346543018005485},
 eprint = {https://doi-org.crai.referencistas.com/10.3102/00346543018005485},
 journal = {Review of Educational Research},
 number = {5},
 pages = {485–499},
 title = {Chapter VIII Computational Technics},
 url = {https://doi-org.crai.referencistas.com/10.3102/00346543018005485},
 volume = {18},
 year = {1948g}
}

@article{doi:10.3102/00346543056004381,
 abstract = {This paper examines reports of empirical research on Feuerstein’s “Instrumental Enrichment” (FIE) as a method of teaching thinking skills and asks what can be concluded from these reports with respect to the following: (a) the nature and statistical reliability of observed FIE effects and, for those effects that are statistically reliable, (b) the “amount” of FIE that appears to be required for these effects to appear.},
 author = {Joel M. Savell and Paul T. Twohig and Douglas L. Rachford},
 doi = {10.3102/00346543056004381},
 eprint = {https://doi-org.crai.referencistas.com/10.3102/00346543056004381},
 journal = {Review of Educational Research},
 number = {4},
 pages = {381–409},
 title = {Empirical Status of Feuerstein’s “Instrumental Enrichment” (FIE) Technique as a Method of Teaching Thinking Skills},
 url = {https://doi-org.crai.referencistas.com/10.3102/00346543056004381},
 volume = {56},
 year = {1986p}
}

@article{doi:10.3102/00346543067003271,
 abstract = {This is a review of research on thinking aloud in reading comprehension that considers thinking aloud as a method of inquiry, a mode of instruction, and a means for encouraging social interaction. As a method of inquiry, the analysis of verbal reports provided by readers thinking aloud revealed the flexible and goal-directed processing of expert readers. As a mode of instruction, thinking aloud was first employed by teachers who modeled their processing during reading, making overt the strategies they were using to comprehend text. Subsequently, instructional approaches were developed to engage students themselves in thinking aloud. Such endeavors revealed facilitation effects on text understanding. Current efforts to engage students in constructing meaning from text in collaborative discussions seem to indicate a new direction for thinking aloud research, one in which social interaction assumes increased importance.},
 author = {Linda Kucan and Isabel L. Beck},
 doi = {10.3102/00346543067003271},
 eprint = {https://doi-org.crai.referencistas.com/10.3102/00346543067003271},
 journal = {Review of Educational Research},
 number = {3},
 pages = {271–299},
 title = {Thinking Aloud and Reading Comprehension Research: Inquiry, Instruction, and Social Interaction},
 url = {https://doi-org.crai.referencistas.com/10.3102/00346543067003271},
 volume = {67},
 year = {1997i}
}

@article{doi:10.3102/0034654317710096,
 abstract = {Computational thinking (CT) uses concepts that are essential to computing and information science to solve problems, design and evaluate complex systems, and understand human reasoning and behavior. This way of thinking has important implications in computer sciences as well as in almost every other field. Therefore, we contend that CT should be taught in elementary schools and included in every university’s educational curriculum. Several studies that measure the impact of teaching programming, analytical thinking, and CT have been conducted. In this review, we analyze and discuss findings from these studies and highlight the importance of learning programming with a focus on the development of CT skills at a young age. We also describe the tools that are available to improve the teaching of CT and provide a state-of-the-art overview of how programming is being taught at schools and universities in Colombia and around the world.},
 author = {Francisco Buitrago Flórez and Rubby Casallas and Marcela Hernández and Alejandro Reyes and Silvia Restrepo and Giovanna Danies},
 doi = {10.3102/0034654317710096},
 eprint = {https://doi-org.crai.referencistas.com/10.3102/0034654317710096},
 journal = {Review of Educational Research},
 number = {4},
 pages = {834–860},
 title = {Changing a Generation’s Way of Thinking: Teaching Computational Thinking Through Programming},
 url = {https://doi-org.crai.referencistas.com/10.3102/0034654317710096},
 volume = {87},
 year = {2017b}
}

@article{doi:10.3102/00346543211070048,
 abstract = {Research–practice partnerships (RPPs) have grown rapidly in the last decade in the United States to challenge traditional notions of education research by emphasizing the importance of researchers and practitioners working together in a spirit of mutuality to develop research questions, collect data, implement interventions, and analyze and use findings. RPP scholarship in the United States has historically advocated for the need to pay more focused attention to issues of equity and justice. To address that need, this literature review examined how RPPs in the United States have addressed equity and justice in their work. Based on five dimensions of equity and justice that could be observed within the 149 examples of RPP work we reviewed, we identified 17 exemplar projects that explicitly and effectively forefront equity and justice in RPPs, what we call equity-focused. Implications suggest that researchers and practitioners who have initiated equity-orientated RPPs may reflect on the partnerships’ existing strengths, specifically related to the five interconnected features that characterize equity-focused RPPs, to sustain and advance equity and justice through RPPs.},
 author = {Amy Vetter and Beverly S. Faircloth and Kimberly K. Hewitt and Laura M. Gonzalez and Ye He and Marcia L. Rock},
 doi = {10.3102/00346543211070048},
 eprint = {https://doi-org.crai.referencistas.com/10.3102/00346543211070048},
 journal = {Review of Educational Research},
 number = {5},
 pages = {829–866},
 title = {Equity and Social Justice in Research Practice Partnerships in the United States},
 url = {https://doi-org.crai.referencistas.com/10.3102/00346543211070048},
 volume = {92},
 year = {2022q}
}

@article{doi:10.3102/00346543231216958,
 abstract = {Computer programming provides a framework for interdisciplinary learning in sciences, arts and languages. However, increasing integration of programming in K–12 shows that the block-based and text-based dichotomy of programming environments does not reflect the spectrum of their affordance. Hence, educators are confronted with a fundamental hurdle of matching programming environments with learners’ cognitive abilities and learning objectives. This study addresses this challenge by analyzing 111 articles evaluating the affordances of programming environments to identify both structural and theoretical models to support educators’ choice of programming environments. The following dimensions of programming environments were identified: connectivity mode, interface natural language, language inheritance, age appropriateness, cost of environment, output interface, input interface, and project types. For each of these dimensions, the synthesis of the literature ranged from examining its nature and effect on learning programming to the implications of choosing an environment and the critical gaps that future studies should address. The findings offer instructors useful parameters to compare and assess programming environments’ suitability and alignment with learning objectives.},
 author = {Ndudi Okechukwu Ezeamuzie and Mercy Noyenim Ezeamuzie},
 doi = {10.3102/00346543231216958},
 eprint = {https://doi-org.crai.referencistas.com/10.3102/00346543231216958},
 journal = {Review of Educational Research},
 number = {0},
 pages = {00346543231216958},
 title = {Multidimensional Framing of Environments Beyond Blocks and Texts in K–12 Programming},
 url = {https://doi-org.crai.referencistas.com/10.3102/00346543231216958},
 volume = {0},
 year = {2024d}
}

@article{doi:10.3102/00346543241241327,
 abstract = {Teaching coding and computational thinking is an emerging educational imperative, now embedded in compulsory curriculum in the United States, Finland, the UK, Germany, Belgium, the Netherlands, New Zealand, and Australia. This meta-synthesis of 49 studies critically reviews recent international research (2009–2022) of coding and computational thinking as core and integrated across the curriculum. It addresses four essential problems: (a) What are the key features of learning environments that successfully develop students’ coding and computational thinking? (b) What is the impact of student engagement in coding and computational thinking on learning outcomes across curriculum areas? (c) What pedagogical constraints are evident for coding and computational thinking, including across curriculum areas? and (d) Which conceptual frameworks support coding and computational thinking, and what has been marginalized or excluded? The review advances knowledge of coding and computational thinking—vital to guide and develop future AI-based solutions to real-world problems that challenge disciplinary boundaries.},
 author = {Kathy A. Mills and Jen Cope and Laura Scholes and Luke Rowe},
 doi = {10.3102/00346543241241327},
 eprint = {https://doi-org.crai.referencistas.com/10.3102/00346543241241327},
 journal = {Review of Educational Research},
 number = {0},
 pages = {00346543241241327},
 title = {Coding and Computational Thinking Across the Curriculum: A Review of Educational Outcomes},
 url = {https://doi-org.crai.referencistas.com/10.3102/00346543241241327},
 volume = {0},
 year = {2024k}
}

@article{doi:10.3102/0091732X16682474,
 doi = {10.3102/0091732X16682474},
 eprint = {https://doi-org.crai.referencistas.com/10.3102/0091732X16682474},
 journal = {Review of Research in Education},
 number = {1},
 pages = {886–903},
 title = {About the Contributors},
 url = {https://doi-org.crai.referencistas.com/10.3102/0091732X16682474},
 volume = {40},
 year = {2016t}
}

@article{doi:10.3102/01623737011004417,
 abstract = {This survey assessed state policymakers’ efforts to promote teaching for understanding and thinking in elementary schools. Data were provided by two rounds of interviews of curriculum specialists in state departments of education nationwide and a review of curriculum-related documents cited during the interviews. Results indicate that state guidelines promoting teaching for understanding and thinking are typically communicated through inservice programs, goals and objectives statements, and/or guidelines for local curriculum planners. These initiatives rarely include statewide tests. The report summarizes similarities and differences in policy initiatives across all 50 states, with closer attention to seven states that are especially active in promoting curriculum reforms (e.g., California). The report also discusses implications for state-level policymakers and others who are eager to reshape the elementary school curriculum.},
 author = {Donald J. Freeman},
 doi = {10.3102/01623737011004417},
 eprint = {https://doi-org.crai.referencistas.com/10.3102/01623737011004417},
 journal = {Educational Evaluation and Policy Analysis},
 number = {4},
 pages = {417–429},
 title = {State Guidelines Promoting Teaching for Understanding and Thinking in Elementary Schools: A 50-State Survey},
 url = {https://doi-org.crai.referencistas.com/10.3102/01623737011004417},
 volume = {11},
 year = {1989f}
}

@article{doi:10.3102/10769986029002257,
 author = {Gerald A Rosen},
 doi = {10.3102/10769986029002257},
 eprint = {https://doi-org.crai.referencistas.com/10.3102/10769986029002257},
 journal = {Journal of Educational and Behavioral Statistics},
 number = {2},
 pages = {257–259},
 title = {A Review of Score Reliability: Contemporary Thinking on Reliability Issues},
 url = {https://doi-org.crai.referencistas.com/10.3102/10769986029002257},
 volume = {29},
 year = {2004o}
}

@article{doi:10.3102/10769986031004437,
 abstract = {Simple slopes, regions of significance, and confidence bands are commonly used to evaluate interactions in multiple linear regression (MLR) models, and the use of these techniques has recently been extended to multilevel or hierarchical linear modeling (HLM) and latent curve analysis (LCA). However, conducting these tests and plotting the conditional relations is often a tedious and error-prone task. This article provides an overview of methods used to probe interaction effects and describes a unified collection of freely available online resources that researchers can use to obtain significance tests for simple slopes, compute regions of significance, and obtain confidence bands for simple slopes across the range of the moderator in the MLR, HLM, and LCA contexts. Plotting capabilities are also provided.},
 author = {Kristopher J. Preacher and Patrick J. Curran and Daniel J. Bauer},
 doi = {10.3102/10769986031004437},
 eprint = {https://doi-org.crai.referencistas.com/10.3102/10769986031004437},
 journal = {Journal of Educational and Behavioral Statistics},
 number = {4},
 pages = {437–448},
 title = {Computational Tools for Probing Interactions in Multiple Linear Regression, Multilevel Modeling, and Latent Curve Analysis},
 url = {https://doi-org.crai.referencistas.com/10.3102/10769986031004437},
 volume = {31},
 year = {2006o}
}

@article{doi:10.3102/10769986241268907,
 abstract = {Constructive matching and response elimination strategies are two primarily used cognitive strategies in Raven’s Advanced Progressive Matrices (APM), a valid measurement instrument of general intelligence. Identifying strategies is necessary for conducting studies on the relationship between cognitive strategy and other cognitive factors and for cognitive strategy training. However, the strategy identification method used in research is either subjective, or the information in the behavior data is not fully utilized, or it is limited by the size of the sample and cannot be widely used. Therefore, this study trained a convolutional neural network-based visual computational model (CVC) for cognitive strategy identification based on eye movement images. Focusing on the APM, the trained CVC can be used for strategy identification by learning and mining the pattern information in the eye movement images with predefined training labels from a psychometric model. An empirical study was conducted to illustrate the training and application of the CVC. Utilizing the trained CVC and a developed graphical user interface application, the primary finding of the study reveals a high level of agreement in strategy identification between the CVC and the psychometric model, as well as between the CVC and expert judgment. This implies that, akin to the psychometric model, the CVC can be used to identify the two cognitive strategies of constructive matching and response elimination. Overall, the proposed deep learning-based model follows the data-driven perspective and provides a new way of studying cognitive strategy in the APM by presenting objective and quantitative identification results.},
 author = {Zhimou Wang and Yaohui Liu and Peida Zhan},
 doi = {10.3102/10769986241268907},
 eprint = {https://doi-org.crai.referencistas.com/10.3102/10769986241268907},
 journal = {Journal of Educational and Behavioral Statistics},
 number = {0},
 pages = {10769986241268908},
 title = {Using a Deep Learning-Based Visual Computational Model to Identify Cognitive Strategies in Matrix Reasoning},
 url = {https://doi-org.crai.referencistas.com/10.3102/10769986241268907},
 volume = {0},
 year = {2024r}
}

@article{doi:10.3141/1679-15,
 abstract = {Emerging artificial intelligence techniques such as genetic algorithms (GAs) allow a more realistic representation and solution of difficult and combinatorial problems such as the dynamic traffic queue management problem. Computational experience in solving such complex large-scale problems by use of micro-GAs is described. In addition to providing evidence of the ability of micro-GAs to successfully identify optimal traffic management schemes, some micro-GA-associated computational issues that warrant attention are highlighted. Choosing a proper population size is a critical decision, and internal variability must be accounted for to assess the goodness of the optimization results properly. A simple rule for deciding the best population size for micro-GAs is proposed. Micro-GAs may converge to low-quality solutions, particularly with very small population sizes; convergence of micro-GAs by itself is not a sufficient indication of good performance. The size of the search space for some real-world systems can pose some difficulties to micro-GAs. Choosing between micro-GAs and regular GAs is a problem-dependent decision.},
 author = {Ghassan Abu-Lebdeh and Rahim F. Benekohal},
 doi = {10.3141/1679-15},
 eprint = {https://doi-org.crai.referencistas.com/10.3141/1679-15},
 journal = {Transportation Research Record},
 number = {1},
 pages = {112–118},
 title = {Computational Issues in Micro-Genetic Algorithms for Traffic Management},
 url = {https://doi-org.crai.referencistas.com/10.3141/1679-15},
 volume = {1679},
 year = {1999a}
}

@article{doi:10.3141/1777-07,
 abstract = {The practical feasibility is demonstrated of a traffic assignment method, dubbed area-spread assignment, that incorporates information on the spatial relationships between nodes, links, and zone boundaries. Tests of area-spread assignment were performed on networks from Fredericton, New Brunswick, and Racine, Wisconsin. To eliminate the effect of errors from other model steps in the tests, comparisons were made with a detailed network in Fredericton that contained all streets in the city and extremely small zones. The tests indicated that there is a substantial difference between area-spread assignment and a traditional assignment using centroid connectors. Area-spread assignment did a better job of replicating volumes on the detailed network, although computation times were much longer.},
 author = {Alan J. Horowitz},
 doi = {10.3141/1777-07},
 eprint = {https://doi-org.crai.referencistas.com/10.3141/1777-07},
 journal = {Transportation Research Record},
 number = {1},
 pages = {68–74},
 title = {Computational Issues in Increasing Spatial Precision of Traffic Assignments},
 url = {https://doi-org.crai.referencistas.com/10.3141/1777-07},
 volume = {1777},
 year = {2001l}
}

@article{doi:10.3141/1801-09,
 abstract = {A computational means to assess the conspicuity of highway features was developed, verified, and then applied to a sample construction work zone scene. This work was conceived as a balance between modeling the complex phenomena within the human visual system and the need for a simple applications-oriented tool for practitioners to derive a quantitative relative assessment of real-world construction work zones to rank choices in terms of conspicuity. The results indicate that the vision model–based tool can assess the relative conspicuity of individual elements of a roadway or roadside scene and is relatively straightforward in use. As such, it holds potential value in virtual prototyping of work-zone sight lines, colors, and placement of hazard warning cues, such as cones, markings, and reflective vests.},
 author = {Jay E. Barton and James A. Misener and Theodore E. Cohn},
 doi = {10.3141/1801-09},
 eprint = {https://doi-org.crai.referencistas.com/10.3141/1801-09},
 journal = {Transportation Research Record},
 number = {1},
 pages = {73–79},
 title = {Computational Vision Model to Assess Work-Zone Conspicuity},
 url = {https://doi-org.crai.referencistas.com/10.3141/1801-09},
 volume = {1801},
 year = {2002c}
}

@article{doi:10.3141/1836-08,
 abstract = {While information technology has facilitated the collection of neverbefore-seen quantities of data, these data have not always provided the information needed by transportation professionals to support sound decision making. Computational intelligence (CI) has great potential to support the needs of transportation professionals. CI is a result of synergy among information processing technologies such as artificial neural networks (ANNs), fuzzy sets, and genetic algorithms. As the number of CI applications to transportation problems grows, so does the need to evaluate these systems. The issue of validating and evaluating transportation CI applications is addressed. A case study that evaluates the effectiveness of two CI paradigms, case-based reasoning and ANNs, for estimating the benefits of real-time traffic diversion is presented. The case study illustrates the need for regarding validation and evaluation as a part of the development effort and the need for tuning the design parameters of CI paradigms.},
 author = {Adel W. Sadek and Gary Spring and Brian L. Smith},
 doi = {10.3141/1836-08},
 eprint = {https://doi-org.crai.referencistas.com/10.3141/1836-08},
 journal = {Transportation Research Record},
 number = {1},
 pages = {57–63},
 title = {Toward More Effective Transportation Applications of Computational Intelligence Paradigms},
 url = {https://doi-org.crai.referencistas.com/10.3141/1836-08},
 volume = {1836},
 year = {2003n}
}

@article{doi:10.3141/2113-04,
 abstract = {Concrete in transportation infrastructure is constantly subjected to the ingress of chloride, which could cause reinforcement corrosion and significant deterioration of concrete structures if not well controlled. Major types of reinforcement corrosion, such as pitting corrosion and concentration-cell corrosion, are usually initiated with localized chloride concentrations whose behavior is closely related to the transport characteristics of chloride ions in hydrated cement paste. However, unless studied at the molecular level, chloride transport behavior cannot be properly understood and controlled in view of the high heterogeneity of cement paste. The findings are presented from a research study recently conducted with a molecular dynamics (MD) approach to investigate the physicochemical nature of the various interactions between chloride ions and cement hydration products, which might significantly influence chloride transport in cement paste. Six hydrated compounds including portlandite, C-S-H phases (tobermorite and jennite), AFm phases (hydrocalumite and kuzelite), and the AFt phase (ettringite) were modeled and then validated by using the 35Cl nuclear magnetic resonance spectroscopy technique. On the basis of the MD-generated ionic trajectory, the macroscopic transport phenomenon of chloride species was evaluated in the vicinity of each different hydration product in terms of a computed effective diffusion coefficient.},
 author = {Tongyan Pan and Yajun Liu},
 doi = {10.3141/2113-04},
 eprint = {https://doi-org.crai.referencistas.com/10.3141/2113-04},
 journal = {Transportation Research Record},
 number = {1},
 pages = {31–40},
 title = {Computational Molecular Analysis of Chloride Transport in Hydrated Cement Paste},
 url = {https://doi-org.crai.referencistas.com/10.3141/2113-04},
 volume = {2113},
 year = {2009l}
}

@article{doi:10.3141/2156-04,
 abstract = {This research investigates route choice behavior in networks with risky travel times and real-time information. A stated preference survey is conducted. In it subjects use a PC-based interactive map to choose routes link by link in various scenarios. The scenarios include two types of maps: the first map presents a choice between one stochastic route and one deterministic route, and the second contains real-time information and an available detour. The first type of map measures the basic risk attitude of the subject. The second type allows for strategic planning and measures the effect of this opportunity on subjects’ choice behavior. Results from each subject are analyzed to determine whether the subject planned strategically for the en route information or simply selected fixed paths from origin to destination. The full data set is used to estimate several choice models with expected travel times and standard deviations as explanatory variables. Estimation results are used to assess whether models that incorporate strategic behavior more accurately reflect route choice than do simpler path-based models.},
 author = {Michael Razo and Song Gao},
 doi = {10.3141/2156-04},
 eprint = {https://doi-org.crai.referencistas.com/10.3141/2156-04},
 journal = {Transportation Research Record},
 number = {1},
 pages = {28–35},
 title = {Strategic Thinking and Risk Attitudes in Route Choice: Stated Preference Approach},
 url = {https://doi-org.crai.referencistas.com/10.3141/2156-04},
 volume = {2156},
 year = {2010n}
}

@article{doi:10.3141/2283-14,
 abstract = {This paper aims to investigate how the trade-offs involved in the construction and solution of restrictive master problems for the user equilibrium traffic assignment problem may affect the overall convergence performance of the solution algorithms. Three strategies used to update user equilibrium path sets in path-based algorithms are examined: an origin–destination (O-D)-based strategy that updates the path set for each single O-D pair just before path flows are updated, an origin-based strategy that updates path sets for all O-D pairs associated with the same origin at once, and a simultaneous strategy that updates the path sets for all O-D pairs together. Three recently developed path-based algorithms to update the path sets by using three strategies are implemented for solution of the restrictive master problems. The implementation strategies are tested on medium to large real networks, and their relative performance characteristics are compared. The results show that the construction and update of the restrictive master problems by the different strategies have significant impacts on convergence as well as solution noise. The computational time for convergence under the simultaneous and origin-based strategies is significantly less than that under the O-D-based strategy. In addition, the solution obtained by adoption of the simultaneous strategy exhibits less noise than the other two strategies. These results provide important theoretical and practical insights into solution algorithms for the traffic assignment problem.},
 author = {Amit Kumar and Srinivas Peeta and Yu (Marco) Nie},
 doi = {10.3141/2283-14},
 eprint = {https://doi-org.crai.referencistas.com/10.3141/2283-14},
 journal = {Transportation Research Record},
 number = {1},
 pages = {131–142},
 title = {Update Strategies for Restricted Master Problems for User Equilibrium Traffic Assignment Problem: Computational Study},
 url = {https://doi-org.crai.referencistas.com/10.3141/2283-14},
 volume = {2283},
 year = {2012n}
}

@article{doi:10.3141/2334-08,
 abstract = {The multiclass percentile user equilibrium (MCPUE) problem discussed in this paper assumes that travelers, subject to uncertainty in a transportation network, strive to minimize reserved travel time (also known as the travel time budget) to ensure a probability of arriving at their destinations on time. An MCPUE, defined as an extension of the Wardrop equilibrium in a probabilistic network, is achieved when no travelers, regardless of their preferred on-time arrival probability, can reduce their reserved travel time by unilaterally changing their routes. Efficient numerical procedures for computing the MCPUE are studied in this paper. Specifically, a proposed new gradient projection algorithm avoids path enumeration through a column generation procedure based on a reliable shortest path algorithm. Implementation details of inner iterations, which are critical to the overall efficiency of the algorithm, are also discussed. Numerical experiments are conducted to test the computational performance of the proposed algorithm.},
 author = {Xing Wu and Yu (Marco) Nie},
 doi = {10.3141/2334-08},
 eprint = {https://doi-org.crai.referencistas.com/10.3141/2334-08},
 journal = {Transportation Research Record},
 number = {1},
 pages = {75–83},
 title = {Solving the Multiclass Percentile User Equilibrium Traffic Assignment Problem: A Computational Study},
 url = {https://doi-org.crai.referencistas.com/10.3141/2334-08},
 volume = {2334},
 year = {2013t}
}

@article{doi:10.3141/2422-14,
 abstract = {The reliability of traffic model results is strictly connected to the quality of its calibration. A challenge arising in this context concerns the selection of the most influential input parameters. A model sensitivity analysis should be used with this aim. However, because of the limitations of time and computational resources, a proper sensitivity analysis is rarely performed in common practice. A recent study introduced a methodology based on Gaussian process metamodels for the sensitivity analysis of computationally expensive traffic simulation models. The main limitation was a dependence on model dimensionality. When the model has more than about 15 to 20 parameters, estimation of a Gaussian process metamodel (also known as a Kriging metamodel) may become problematic. In this paper, the Kriging-based approach is coupled with a recently developed approach, quasi-optimized trajectory-based elementary effects (quasi-OTEE), for the sensitivity analysis of computationally expensive models. The quasi-OTEE sensitivity analysis can be used to identify the whole subset of sensitive parameters of a high-dimensional model, and the Kriging-based sensitivity analysis can then be used to refine the analysis and to rank the different parameters of the subset in a more reliable way. Application of this new sequential sensitivity analysis method is illustrated with the Wiedemann-74 car-following model. Results show that the new method requires 40 times fewer model evaluations than a standard variance-based sensitivity analysis to identify the influential parameters and their ranks.},
 author = {Qiao Ge and Biagio Ciuffo and Monica Menendez},
 doi = {10.3141/2422-14},
 eprint = {https://doi-org.crai.referencistas.com/10.3141/2422-14},
 journal = {Transportation Research Record},
 number = {1},
 pages = {121–130},
 title = {Comprehensive Approach for the Sensitivity Analysis of High-Dimensional and Computationally Expensive Traffic Simulation Models},
 url = {https://doi-org.crai.referencistas.com/10.3141/2422-14},
 volume = {2422},
 year = {2014h}
}

@article{doi:10.3141/2563-18,
 abstract = {Integrated activity-based model (ABM) and dynamic traffic assignment (DTA) frameworks have emerged as promising tools to support transportation planning and operations, particularly in the context of novel technologies and data sources. This research proposes an approach to characterize the implementation of integrated ABM-DTA models and seeks to facilitate the interpretation and comparison of frameworks and, ultimately, the selection of appropriate tools. The importance of the dimensions considered in this characterization is illustrated through a detailed analysis of the computation of skims. Skims are the level of service (LOS) metric produced by DTA models, and the computation of skims may impact the performance and convergence of ABM-DTA applications. Numerical results from experiments on a regional ABM-DTA model in Austin, Texas, suggest that skims produced at relatively small time steps (10 to 30 min) may lead to a faster integrated model convergence. Finer time-grained skims are also observed to capture sharper temporal peaking patterns in the LOS. This work considers two skim computation methods; the analysis of the results suggests that simpler techniques are adequate, as the inherent variability of travel times from simulation overshadows any gain in precision from more complex methods. This study also uses promising techniques to visualize and analyze the model results, a challenging task in the context of highly disaggregate models and the subject of further research. The insights from this research effort can inform future research on the implementation of ABM-DTA methods and practical applications of existing frameworks.},
 author = {Natalia Ruiz Juri and Rachel M. James and Nan Jiang and Jennifer Duthie and Abdul R. Pinjari and Chandra R. Bhat},
 doi = {10.3141/2563-18},
 eprint = {https://doi-org.crai.referencistas.com/10.3141/2563-18},
 journal = {Transportation Research Record},
 number = {1},
 pages = {134–143},
 title = {Computation of Skims for Large-Scale Implementations of Integrated Activity-Based and Dynamic Traffic Assignment Models},
 url = {https://doi-org.crai.referencistas.com/10.3141/2563-18},
 volume = {2563},
 year = {2016j}
}

@article{doi:10.3181/0704-MR-97,
 abstract = {A new algorithm has been constructed for finding under- and overrepresented oligonucleotide motifs in the protein coding regions of genomes that have been normalized for G/C content, codon usage, and amino acid order. This Robins-Krasnitz algorithm has been employed to compare the oligonucleotide frequencies between many different prokaryotic genomes. Evidence is presented demonstrating that at least some of these sequence motifs are functionally important and selected for or against during the evolution of these prokaryotes. The applications of this method include the optimization of protein expression for synthetic genes in foreign organisms, identification of novel oligonucleotide signals used by the organism and the examination of evolutionary relationships not dependent upon different gene sequence trees.},
 author = {Harlan Robins and Michael Krasnitz and Arnold J. Levine},
 doi = {10.3181/0704-MR-97},
 eprint = {https://doi-org.crai.referencistas.com/10.3181/0704-MR-97},
 journal = {Experimental Biology and Medicine},
 number = {6},
 pages = {665–673},
 title = {The Computational Detection of Functional Nucleotide Sequence Motifs in the Coding Regions of Organisms},
 url = {https://doi-org.crai.referencistas.com/10.3181/0704-MR-97},
 volume = {233},
 year = {2008o}
}

@article{doi:10.3184/007967402103165360,
 abstract = {Recent advances in experimental and theoretical studies of liquid interfaces provide remarkable evidence for the unique properties of these systems. In this review we examine how these properties affect the thermodynamics and kinetics of chemical reactions which take place at the liquid/vapor interface and at the liquid/liquid interface. We demonstrate how the rapidly varying density and viscosity, the marked changes in polarity and the surface roughness manifest themselves in isomerization, electron transfer and photodissociation reactions.},
 author = {Ilan Benjamin},
 doi = {10.3184/007967402103165360},
 eprint = {https://doi-org.crai.referencistas.com/10.3184/007967402103165360},
 journal = {Progress in Reaction Kinetics and Mechanism},
 number = {2},
 pages = {87–126},
 title = {Chemical Reaction Dynamics at Liquid Interfaces: A Computational Approach},
 url = {https://doi-org.crai.referencistas.com/10.3184/007967402103165360},
 volume = {27},
 year = {2002b}
}

@article{doi:10.3184/146867816X14634937797102,
 abstract = {The reaction mechanism of neutral phenol with formaldehyde in aqueous solution was studied theoretically by using the GGA-PW91/DNP+COSMO method. In previous studies, it was found that the water molecules can mediate proton transfer from nitrogen to oxygen, but the results of this study show that water molecules can also catalyse proton transfer from carbon to oxygen. With the mediation of water, the energy barriers were greatly lowered. The calculated energy barriers indicate that the para-position of phenol has higher reactivity than the ortho-position. The reaction occurs in a concerted mechanism. However, the process exhibits asynchronous characteristics, particularly the C–C bond formation precedes the proton transfer and the proton transfer from water molecules to the carbonyl oxygen is ahead of proton abstraction from the benzene ring.},
 author = {Ming Cao and Taohong Li and Zhigang Wu and Jiankun Liang and Xiaoguang Xie and Guanben Du},
 doi = {10.3184/146867816X14634937797102},
 eprint = {https://doi-org.crai.referencistas.com/10.3184/146867816X14634937797102},
 journal = {Progress in Reaction Kinetics and Mechanism},
 number = {2},
 pages = {144–152},
 title = {A computational study on the reaction mechanism of neutral phenol with formaldehyde in aqueous solution},
 url = {https://doi-org.crai.referencistas.com/10.3184/146867816X14634937797102},
 volume = {41},
 year = {2016e}
}

@article{doi:10.3197/096327108X271941,
 abstract = {Is philosophy an appropriate means for inducing the ‘moral point of view’ with respect to nature? The moral point of view involves a feeling for the inner reality of others, a feeling which, it is argued, is induced more by processes of synergistic interaction than by the kind of rational deliberation that classically constituted philosophy. But how are we to engage synergistically with other-than-human life forms and systems? While synergy with animals presents no in-principle difficulty, synergy with larger life systems takes us into epistemological realms explored only in the margins of the Western tradition, such as in Goethe’s Romantic alternative to science. These ‘alternative’ epistemological realms are however the very province of the Daoist arts of China, and these arts accordingly furnish us with practices conducive to a moral consciousness of nature.},
 author = {Freya Mathews},
 doi = {10.3197/096327108X271941},
 eprint = {https://doi-org.crai.referencistas.com/10.3197/096327108X271941},
 journal = {Environmental Values},
 number = {1},
 pages = {41–65},
 title = {Thinking from within the Calyx of Nature},
 url = {https://doi-org.crai.referencistas.com/10.3197/096327108X271941},
 volume = {17},
 year = {2008n}
}

@article{doi:10.3233/AAC-170032,
 abstract = {Persuasion is an activity that involves one party trying to induce another party to believe something or to do something. It is an important and multifaceted human facility. Obviously, sales and marketing is heavily dependent on persuasion. But many other activities involve persuasion such as a doctor persuading a patient to drink less alcohol, a road safety expert persuading drivers to not text while driving, or an online safety expert persuading users of social media sites to not reveal too much personal information online. As computing becomes involved in every sphere of life, so too is persuasion a target for applying computer-based solutions. An automated persuasion system (APS) is a system that can engage in a dialogue with a user (the persuadee) in order to persuade the persuadee to do (or not do) some action or to believe (or not believe) something. To do this, an APS aims to use convincing arguments in order to persuade the persuadee. Computational persuasion is the study of formal models of dialogues involving arguments and counterarguments, of user models, and strategies, for APSs. A promising application area for computational persuasion is in behaviour change. Within healthcare organizations, government agencies, and non-governmental agencies, there is much interest in changing behaviour of particular groups of people away from actions that are harmful to themselves and/or to others around them.},
 author = {Anthony Hunter},
 doi = {10.3233/AAC-170032},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/AAC-170032},
 journal = {Argument & Computation},
 number = {1},
 pages = {15–40},
 title = {Towards a framework for computational persuasion with applications in behaviour change},
 url = {https://doi-org.crai.referencistas.com/10.3233/AAC-170032},
 volume = {9},
 year = {2018k}
}

@article{doi:10.3233/AAC-181003,
 abstract = {Critical thinking about international politics often involves reasoning about the beliefs, goals, appraisals, actions, and plans of actors such as countries, governments, politicians, etc. We analyzed arguments in interpretive reports about international politics, in order to develop a prototype argument diagramming tool for this domain, AVIZE (Argument Visualization and Evaluation). The purpose of AVIZE is to aid users in the construction and self-evaluation of real-world arguments in the domain of international politics. AVIZE provides a set of argument schemes as cognitive building blocks for constructing argument diagrams. Most of the schemes are related to concepts from the field of automated plan recognition in artificial intelligence. While some currently available argument diagramming tools provide schemes, they are not tailored to the domain of international politics. This paper describes the argument schemes for this domain and the design of the argument diagramming tool.},
 author = {Nancy L. Green and Michael Branon and Luke Roosje and Federico Cerutti and Richard Booth},
 doi = {10.3233/AAC-181003},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/AAC-181003},
 journal = {Argument & Computation},
 number = {1},
 pages = {41–53},
 title = {Argument schemes and visualization software for critical thinking about international politics},
 url = {https://doi-org.crai.referencistas.com/10.3233/AAC-181003},
 volume = {10},
 year = {2019g}
}

@article{doi:10.3233/AAC-190467,
 abstract = {In informal argument, an essential step is to ask what will “resonate” with a particular audience and hence persuade. Marketers, for example, may recommend a certain colour for a new soda can because it “pops” on Instagram; politicians may “fine-tune” diction for different social demographics. This paper engages the need to strategise for such resonance by offering a method for automating opposition analysis (OA), a technique from semiotics used in marketing and literary analysis to plot objects of interest on oppositional axes. Central to our computational approach is a reframing of texts as proxies for thought and opposition as the product of oscillation in thought in response to those proxies, a model to which the contextual similarity information contained in word embeddings is relevant. We illustrate our approach with an analysis of texts on gun control from ProCon.org, implementing a three-step method to: 1) identify relatively prominent signifiers; 2) rank possible opposition pairs on prominence and contextual similarity scores; and 3) derive plot values for proxies on opposition pair axes. The results are discussed in terms of strategies for informal argument that might be derived by those on each side of gun control.},
 author = {Cameron Shackell and Laurianne Sitbon},
 doi = {10.3233/AAC-190467},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/AAC-190467},
 journal = {Argument & Computation},
 number = {3},
 pages = {301–317},
 title = {Computational opposition analysis using word embeddings: A method for strategising resonant informal argument},
 url = {https://doi-org.crai.referencistas.com/10.3233/AAC-190467},
 volume = {10},
 year = {2019p}
}

@article{doi:10.3233/AAC-210555,
 abstract = {There are many benefits of using argumentation-based techniques in multi-agent systems, as clearly shown in the literature. Such benefits come not only from the expressiveness that argumentation-based techniques bring to agent communication but also from the reasoning and decision-making capabilities under conditions of conflicting and uncertain information that argumentation enables for autonomous agents. When developing multi-agent applications in which argumentation will be used to improve agent communication and reasoning, argumentation schemes (reasoning patterns for argumentation) are useful in addressing the requirements of the application domain in regards to argumentation (e.g., defining the scope in which argumentation will be used by agents in that particular application). In this work, we propose an argumentation framework that takes into account the particular structure of argumentation schemes at its core. This paper formally defines such a framework and experimentally evaluates its implementation for both argumentation-based reasoning and dialogues.},
 author = {Alison R. Panisson and Peter McBurney and Rafael H. Bordini and Fabrizio Macagno},
 doi = {10.3233/AAC-210555},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/AAC-210555},
 journal = {Argument & Computation},
 number = {3},
 pages = {357–395},
 title = {A computational model of argumentation schemes for multi-agent systems},
 url = {https://doi-org.crai.referencistas.com/10.3233/AAC-210555},
 volume = {12},
 year = {2021n}
}

@article{doi:10.3233/ADR-210039,
 abstract = {With the approval of aducanumab on the “Accelerated Approval Pathway” and the recognition of amyloid load as a surrogate marker, new successful therapeutic approaches will be driven by combination therapy as was the case in oncology after the launch of immune checkpoint inhibitors. However, the sheer number of therapeutic combinations substantially complicates the search for optimal combinations. Data-driven approaches based on large databases or electronic health records can identify optimal combinations and often using artificial intelligence or machine learning to crunch through many possible combinations but are limited to the pharmacology of existing marketed drugs and are highly dependent upon the quality of the training sets. Knowledge-driven in silico modeling approaches use multi-scale biophysically realistic models of neuroanatomy, physiology, and pathology and can be personalized with individual patient comedications, disease state, and genotypes to create ‘virtual twin patients’. Such models simulate effects on action potential dynamics of anatomically informed neuronal circuits driving functional clinical readouts. Informed by data-driven approaches this knowledge-driven modeling could systematically and quantitatively simulate all possible target combinations for a maximal synergistic effect on a clinically relevant functional outcome. This approach seamlessly integrates pharmacokinetic modeling of different therapeutic modalities. A crucial requirement to constrain the parameters is the access to preferably anonymized individual patient data from completed clinical trials with various selective compounds. We believe that the combination of data- and knowledge driven modeling could be a game changer to find a cure for this devastating disease that affects the most complex organ of the universe.},
 author = {Hugo Geerts and Piet van der Graaf},
 doi = {10.3233/ADR-210039},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/ADR-210039},
 journal = {Journal of Alzheimer’s Disease Reports},
 number = {1},
 pages = {815–826},
 title = {Computational Approaches for Supporting Combination Therapy in the Post-Aducanumab Era in Alzheimer’s Disease},
 url = {https://doi-org.crai.referencistas.com/10.3233/ADR-210039},
 volume = {5},
 year = {2021h}
}

@article{doi:10.3233/BME-240022,
 abstract = {BACKGROUND: Systemic-to-pulmonary shunt is a palliative procedure used to decrease pulmonary blood flow in congenital heart diseases. Shunt stenosis or occlusion has been reported to be associated with mortality; therefore, the management of thrombotic complications remains a challenge for most congenital cardiovascular surgeons. Despite its importance, the optimal method for shunt anastomosis remains unclear. OBJECTIVE: The study investigates the clinical benefits of the punch-out technique over conventional methods in the anastomosis process of Systemic-to-pulmonary shunt, focusing on its potential to reduce shunt-related complications. METHODS: Anastomotic models were created by two different surgeons employing both traditional slit and innovative punch-out techniques. Computational tomography was performed to construct three-dimensional models for computational fluid dynamics (CFD) analysis. We assessed the flow pattern, helicity, magnitude of wall shear stress, and its gradient. RESULTS: The anastomotic flow area was larger in the model using the punch-out technique than in the slit model. In CFD simulation, we found that using the punch-out technique decreases the likelihood of establishing a high wall shear stress distribution around the anastomosis line in the model. CONCLUSION: The punch-out technique emerges as a promising method in SPS anastomosis, offering a reproducible and less skill-dependent alternative that potentially diminishes the risk of shunt occlusion, thereby enhancing patient outcomes.},
 author = {Shiho Yamazaki and Ryosuke Kowatari and Tetsuya Yano and Hanae Sasaki and Kazuyuki Daitoku and Masahito Minakawa},
 doi = {10.3233/BME-240022},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/BME-240022},
 journal = {Bio-Medical Materials and Engineering},
 number = {5},
 pages = {425–437},
 title = {Evaluating the efficacy of the punch-out technique in systemic-to-pulmonary shunts: A computational fluid dynamics approach},
 url = {https://doi-org.crai.referencistas.com/10.3233/BME-240022},
 volume = {35},
 year = {2024n}
}

@article{doi:10.3233/CI-2009-0017,
 abstract = {We present Illuminator, a user-friendly web front end to computational models such as docking and 3D shape similarity calculations. Illuminator was specifically created to allow non-experts to design and submit molecules to computational chemistry programs. As such it provides a simple user interface allowing users to submit jobs starting from a 2D structure. The models provided are pre-optimized by computational chemists for each specific target. We provide an example of how Illuminator was used to prioritize the design of molecular substituents in the Anadys HCV Polymerase (NS5B) project. With 7500 submitted jobs in 1.5 years, Illuminator has allowed project teams at Anadys to accelerate the optimization of novel leads. It has also improved communication between project members and increased demand for computational drug discovery tools.},
 author = {Alberto Gobbi and Matthew Lardy and Sun Hee Kim and Frank Ruebsam and Martin Tran and Stephen E. Webber and Alan X. Xiang},
 doi = {10.3233/CI-2009-0017},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/CI-2009-0017},
 journal = {In Silico Biology},
 number = {1–2},
 pages = {83–93},
 title = {Illuminator: Increasing Synergies Between Medicinal And     Computational Chemists},
 url = {https://doi-org.crai.referencistas.com/10.3233/CI-2009-0017},
 volume = {11},
 year = {2012g}
}

@article{doi:10.3233/FI-2009-0064,
 abstract = {Visual Cryptography (VC) has drawn much attention for providing the service of secret communication. Basically, VC is the process of encoding a secret into several meaningless shares and later decoding the secret by superimposing all or some of the shares without any computation involved. VC has been adopted to support some practical applications, such as image authentication, visual authentication, image hiding, and digital watermarking. Unfortunately, in many applications, VC has been shown to suffer from the “cheating problem” in which the disclosed secret image may be altered by malicious insiders who are called “cheaters.” While ubiquitous computing has been well developed, it has recently occurred to people in both academia and industry that research could benefit more from computational VC by introducing light-weight computation costs in the decoding phase. In this paper, a simple scheme is proposed to conquer the cheating problem by facilitating the capability of share authentication. It is worthwhile to note that the proposed scheme can identify for certain whether cheating attacks have occurred or not, while other schemes that have the same objective frequently provide a vague answer. In addition, the proposed scheme effectively addresses the two main problems of VC, i.e., the inconvenience of meaningless share management and the challenge of achieving difficult alignment.},
 author = {Chin-Chen Chang and Tzung-Her Chen and Li-Jen Liu},
 doi = {10.3233/FI-2009-0064},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/FI-2009-0064},
 journal = {Fundamenta Informaticae},
 number = {1–2},
 pages = {27–42},
 title = {Preventing Cheating in Computational Visual Cryptography},
 url = {https://doi-org.crai.referencistas.com/10.3233/FI-2009-0064},
 volume = {92},
 year = {2009e}
}

@article{doi:10.3233/FI-2010-331,
 abstract = {Recent years have seen a wealth of computational methods applied to problems stemming from molecular biology. In particular, with the completion of many new full genome sequences, great advances have been made in studying the role of non-protein-coding parts of the genome, reshaping our understanding of the role of DNA sequences. Recent breakthroughs in experimental technologies allowing us to inspect the innards of cells on a genomic scale has provided us with unprecedented amounts of data, posing new computational challenges for scientists working to uncover the secrets of life. Due to the binary-like nature of the DNA code and switch-like behavior of many regulatory mechanisms, many of the questions that are currently in focus in biology are surprisingly related to problems that have been of long-term interest to computer scientists. In this review, we present a glimpse into the current state of research in computational methods applied to modeling the regulatory genome. Our aim is to cover current approaches to selected problems from molecular biology that we consider most interesting from the perspective of computer scientists as well as highlight new challenges that will most likely draw the attention of computational biologists in the coming years.},
 author = {Bartek Wilczyński and Torgeir R. Hvidsten},
 doi = {10.3233/FI-2010-331},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/FI-2010-331},
 journal = {Fundamenta Informaticae},
 number = {1–4},
 pages = {323–332},
 title = {A Computer Scientist’s Guide to the Regulatory Genome},
 url = {https://doi-org.crai.referencistas.com/10.3233/FI-2010-331},
 volume = {103},
 year = {2010s}
}

@article{doi:10.3233/FI-2011-534,
 abstract = {We show that the popular pencil puzzle NURIKABE is intractable from the computational complexity point of view, that is, it is NP-complete, even when the involved numbers are 1 and 2 only. To this end, we show how to simulate Boolean gates by the puzzle under consideration. Moreover, we also study some NURIKABE variants, which remain NP-complete, too.},
 author = {Markus Holzer and Andreas Klein and Martin Kutrib and Oliver Ruepp},
 doi = {10.3233/FI-2011-534},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/FI-2011-534},
 journal = {Fundamenta Informaticae},
 number = {1–4},
 pages = {159–174},
 title = {Computational Complexity of NURIKABE},
 url = {https://doi-org.crai.referencistas.com/10.3233/FI-2011-534},
 volume = {110},
 year = {2011g}
}

@article{doi:10.3233/FI-2012-643,
 abstract = {This paper investigates the avalanche problem AP for the Kadanoff sandpile model (KSPM). We prove that (a slight restriction of) AP is in NC1 in dimension one, leaving the general case open. Moreover, we prove that AP is P-complete in dimension two. The proof of this latter result is based on a reduction from the monotone circuit value problem by building logic gates and wires which work with an initial sand distribution in KSPM. These results are also related to the known prediction problem for sandpiles which is in NC1 for one-dimensional sandpiles and P-complete for dimension 3 or higher. The computational complexity of the prediction problem remains open for the Bak’s model of two-dimensional sandpiles.},
 author = {Enrico Formenti and Eric Goles and Bruno Martin},
 doi = {10.3233/FI-2012-643},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/FI-2012-643},
 journal = {Fundamenta Informaticae},
 number = {1},
 pages = {107–124},
 title = {Computational Complexity of Avalanches in the Kadanoff Sandpile Model},
 url = {https://doi-org.crai.referencistas.com/10.3233/FI-2012-643},
 volume = {115},
 year = {2012g}
}

@article{doi:10.3233/FI-2013-813,
 abstract = {We study the complexity of Bongartz’s algorithm for determining a maximal common direct summand of a pair of modules M, N over k-algebra Λ; in particular, we estimate its pessimistic computational complexity 𝒪(rm6n2(n + m log n)), where m = dimkM ≤ n = dimkN and r is a number of common indecomposable direct summands of M and N. We improve the algorithm to another one of complexity 𝒪(rm4n2(n+m log m)) and we show that it applies to the isomorphism problem (having at least an exponential complexity in a direct approach). Moreover, we discuss a performance of both algorithms in practice and show that the “average” complexity is much lower, especially for the improved one (which becomes a part of QPA package for GAP computer algebra system).},
 author = {Andrzej Mróz},
 doi = {10.3233/FI-2013-813},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/FI-2013-813},
 journal = {Fundamenta Informaticae},
 number = {3},
 pages = {317–329},
 title = {On the Computational Complexity of Bongartz’s Algorithm},
 url = {https://doi-org.crai.referencistas.com/10.3233/FI-2013-813},
 volume = {123},
 year = {2013j}
}

@article{doi:10.3233/FI-2017-1535,
 abstract = {Polarizationless P systems with active membranes are non-cooperative systems, that is, the left-hand side of their rules have a single object. Usually, these systems make use of division rules as a mechanism to produce an exponential workspace in linear time. Division rules are inspired by cell division, a process of nuclear division that occurs when a parent cell divides to produce two identical daughter cells. On the other hand, separation rules are inspired by the membrane fission process, a mechanism by which a biological membrane is split into two new ones in such a manner that the contents of the initial membrane is distributed between the new membranes. In this paper, separation rules are used instead of division rules. The computational efficiency of these models is studied and the role of the (minimal) cooperation in object evolution rules is explored from a computational complexity point of view.},
 author = {Luis Valencia-Cabrera and David Orellana-Martín and Miguel A. Martínez-del-Amor and Agustín Riscos-Núñez and Mario J. Pérez-Jiménez and Bogdan Aman and Jetty Kleijn and Maciej Koutny and Dorel Lucanu},
 doi = {10.3233/FI-2017-1535},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/FI-2017-1535},
 journal = {Fundamenta Informaticae},
 number = {1–2},
 pages = {147–172},
 title = {Computational Efficiency of Minimal Cooperation and Distribution in Polarizationless P Systems with Active Membranes},
 url = {https://doi-org.crai.referencistas.com/10.3233/FI-2017-1535},
 volume = {153},
 year = {2017s}
}

@article{doi:10.3233/FI-2018-1749,
 abstract = {Cluster validity indices are proposed in the literature to measure the goodness of a clustering result. The validity measure provides a value which shows how good or bad the obtained clustering result is, as compared to the actual clustering result. However, the validity measures are not arbitrarily generated. A validity measure should satisfy some of the important properties. However, there are cases when in-spite of satisfying these properties, a validity measure is not able to differentiate the two clustering results correctly. In this regard, sensitivity as a property of validity measure is introduced to capture the differences between the two clustering results. However, sensitivity computation is a computationally expensive task as it requires to explore all the possible combinations of clustering results which are very large in number and these are growing exponentially. So, it is required to compute the sensitivity efficiently. As the possible combinations of clustering results grow exponentially, so it is required to first obtain an upper bound on this possible number of combinations which will be sufficient to compute the value of the sensitivity. In this paper, we obtain an upper bound on the number of possible combinations of clustering results. For this purpose, a generic approach which is suitable for various validity measures and a specific approach which is applicable for two validity measures are proposed. It is also shown that this upper bound is sufficient to compute the sensitivity of various validity measures. This upper bound is very less as compared to the total number of possible combinations of clustering results.},
 author = {Sumit Mishra and Samrat Mondal and Sriparna Saha},
 doi = {10.3233/FI-2018-1749},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/FI-2018-1749},
 journal = {Fundamenta Informaticae},
 number = {4},
 pages = {351–374},
 title = {Towards Obtaining Upper Bound on Sensitivity Computation Process for Cluster Validity Measures},
 url = {https://doi-org.crai.referencistas.com/10.3233/FI-2018-1749},
 volume = {163},
 year = {2018l}
}

@article{doi:10.3233/FI-2019-1754,
 abstract = {In this short note we present an elementary matrix-constructive algorithmic proof of the Quillen-Suslin theorem for Ore extensions A := K[x; σ, δ], where K is a division ring, σ : K → K is a division ring automorphism and σ : K → K is a σ-derivation of K. It asserts that every finitely generated projective A-module is free. We construct a symbolic algorithm that computes the basis of a given finitely generated projective A-module. The algorithm is implemented in a computational package. Its efficiency is illustrated by four representative examples.},
 author = {William Fajardo and Oswaldo Lezama},
 doi = {10.3233/FI-2019-1754},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/FI-2019-1754},
 journal = {Fundamenta Informaticae},
 number = {1},
 pages = {41–59},
 title = {Elementary Matrix-computational Proof of Quillen-Suslin Theorem for Ore Extensions},
 url = {https://doi-org.crai.referencistas.com/10.3233/FI-2019-1754},
 volume = {164},
 year = {2019d}
}

@article{doi:10.3233/FI-2020-1872,
 abstract = {Computational Intelligence (CI) is a computer science discipline encompassing the theory, design, development and application of biologically and linguistically derived computational paradigms. Traditionally, the main elements of CI are Evolutionary Computation, Swarm Intelligence, Fuzzy Logic, and Neural Networks. CI aims at proposing new algorithms able to solve complex computational problems by taking inspiration from natural phenomena. In an intriguing turn of events, these nature-inspired methods have been widely adopted to investigate a plethora of problems related to nature itself. In this paper we present a variety of CI methods applied to three problems in life sciences, highlighting their effectiveness: we describe how protein folding can be faced by exploiting Genetic Programming, the inference of haplotypes can be tackled using Genetic Algorithms, and the estimation of biochemical kinetic parameters can be performed by means of Swarm Intelligence. We show that CI methods can generate very high quality solutions, providing a sound methodology to solve complex optimization problems in life sciences.},
 author = {Daniela Besozzi and Luca Manzoni and Marco S. Nobile and Simone Spolaor and Mauro Castelli and Leonardo Vanneschi and Paolo Cazzaniga and Stefano Ruberto and Leonardo Rundo and Andrea Tangherloni et al.},
 doi = {10.3233/FI-2020-1872},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/FI-2020-1872},
 journal = {Fundamenta Informaticae},
 number = {1–4},
 pages = {57–80},
 title = {Computational Intelligence for Life Sciences},
 url = {https://doi-org.crai.referencistas.com/10.3233/FI-2020-1872},
 volume = {171},
 year = {2019d}
}

@article{doi:10.3233/FI-2020-1886,
 abstract = {How robust is a healthcare system? How does a patient navigate the system and what is the cost (e.g., number of medical services required or number of times the medical provider had to be changed to get access to the required medical services) incurred from the first symptoms to getting cured? How will it fare in the wake to a sudden epidemic or a disaster? How are all of these affected by administrative decisions such as allocating/diminishing resources in various areas or centralising services? These are the questions motivating our study on a formal prototype model for a healthcare system. We propose that a healthcare system can be understood as a distributed system with independent nodes (healthcare providers) computing according to their own resources and constraints, with tasks (patient needs) being allocated between the nodes. The questions about the healthcare system become in this context questions about resource availability and distribution between the nodes. We construct in this paper an Event-B model capturing the basic functionality of a simplified healthcare system: patients with different types of medical needs being allocated to suitable medical providers, and navigating between different providers for their turn for multi-step treatments.},
 author = {Luigia Petre and Usman Sanwal and Gohar Shah and Charmi Panchal and Dwitiya Tyagi and Ion Petre and Alberto Dennunzio and Gheorghe Păun and Grzegorz Rozenberg and Claudio Zandron},
 doi = {10.3233/FI-2020-1886},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/FI-2020-1886},
 journal = {Fundamenta Informaticae},
 number = {1–4},
 pages = {331–343},
 title = {A Computational Model for The Access to Medical Service in a Basic Prototype of a Healthcare System},
 url = {https://doi-org.crai.referencistas.com/10.3233/FI-2020-1886},
 volume = {171},
 year = {2019r}
}

@article{doi:10.3233/FI-2020-1888,
 abstract = {Computational Biology is a fast-growing field that is enriched by different data-driven methodological approaches and by findings and applications in a broad range of biological areas. Fundamental to these approaches are the mathematical and computational models used to describe the different states at microscopic (for example a biochemical reaction), mesoscopic (the signalling effects at tissue level), and macroscopic levels (physiological and pathological effects) of biological processes. In this paper we address the problem of combining two powerful classes of methodologies: Flux Balance Analysis (FBA) methods which are now producing a revolution in biotechnology and medicine, and Petri Nets (PNs) which allow system generalisation and are central to various mathematical treatments, for example Ordinary Differential Equation (ODE) specification of the biosystem under study. While the former is limited to modelling metabolic networks, i.e. does not account for intermittent dynamical signalling events, the latter is hampered by the need for a large amount of metabolic data. A first result presented in this paper is the identification of three types of cross-talks between PNs and FBA methods and their dependencies on available data. We exemplify our insights with the analysis of a pancreatic cancer model. We discuss how our reasoning framework provides a biologically and mathematically grounded decision making setting for the integration of regulatory, signalling, and metabolic networks and greatly increases model interpretability and reusability. We discuss how the parameters of PN and FBA models can be tuned and combined together so to highlight the computational effort needed to perform this task. We conclude with speculations and suggestions on this new promising research direction.},
 author = {Pernice Simone and Follia Laura and Balbo Gianfranco and Milanesi Luciano and Sartini Giulia and Totis Niccoló and Lió Pietro and Merelli Ivan and Cordero Francesca and Beccuti Marco et al.},
 doi = {10.3233/FI-2020-1888},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/FI-2020-1888},
 journal = {Fundamenta Informaticae},
 number = {1–4},
 pages = {367–392},
 title = {Integrating Petri Nets and Flux Balance Methods in Computational Biology Models: a Methodological and Computational Practice},
 url = {https://doi-org.crai.referencistas.com/10.3233/FI-2020-1888},
 volume = {171},
 year = {2019p}
}

@article{doi:10.3233/FI-2020-1979,
 abstract = {The effective method (based on Theorem 5.3) of classifying tubular algebras by the Cartan matrices of tilting sheaves over weighted projective lines with all indecomposable direct summands in some finite “fundamental domain” , by the reduction to the two elementary problems of discrete mathematics having algorithmic solutions is presented in details (see Problem A and B). The software package CART_TUB being an implementation of this method yields the precise classification of all up to isomorphism tubular algebras of a fixed tubular type p, by creating the complete lists of their Cartan matrices, and furnish their tilting realizations. In particular, the number of isomorphism classes of tubular algebras of the type p is determined (Theorem 2.3).},
 author = {Piotr Dowbor and Yan Kim},
 doi = {10.3233/FI-2020-1979},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/FI-2020-1979},
 journal = {Fundamenta Informaticae},
 number = {1},
 pages = {39–67},
 title = {Computational Classification of Tubular Algebras},
 url = {https://doi-org.crai.referencistas.com/10.3233/FI-2020-1979},
 volume = {177},
 year = {2020d}
}

@article{doi:10.3233/FI-2021-2054,
 abstract = {We present the concept of a theory machine, which is an atemporal computational formalism that is deployable within an arbitrary logical system. Theory machines are intended to capture computation on an arbitrary system, both physical and unphysical, including quantum computers, Blum-Shub-Smale machines, and infinite time Turing machines. We demonstrate that for finite problems, the computational power of any device characterisable by a finite first-order theory machine is equivalent to that of a Turing machine. Whereas for infinite problems, their computational power is equivalent to that of a type-2 machine. We then develop a concept of complexity for theory machines, and prove that the class of problems decidable by a finite first order theory machine with polynomial resources is equal to 𝒩𝒫 ∩ co-𝒩𝒫.},
 author = {Richard Whyman and Jérôme Durand-Lose and Jarkko Kari and Sergey Verlan},
 doi = {10.3233/FI-2021-2054},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/FI-2021-2054},
 journal = {Fundamenta Informaticae},
 number = {2–3},
 pages = {129–161},
 title = {Physical Computational Complexity and First-order Logic},
 url = {https://doi-org.crai.referencistas.com/10.3233/FI-2021-2054},
 volume = {181},
 year = {2021t}
}

@article{doi:10.3233/FI-2021-2056,
 abstract = {A simple semi-conditional (SSC) grammar is a form of regulated rewriting system where the derivations are controlled either by a permitting string alone or by a forbidden string alone and this condition is specified in the rule. The maximum length i (j, resp.) of the permitting (forbidden, resp.) strings serves as a measure of descriptional complexity known as the degree of such grammars. In addition to the degree, the numbers of nonterminals and of conditional rules are also counted into the descriptional complexity measures of these grammars. We improve on some previously obtained results on the computational completeness of SSC grammars by minimizing the number of nonterminals and / or the number of conditional rules for a given degree (i, j). More specifically we prove, using a refined analysis of a normal form for type-0 grammars due to Geffert, that every recursively enumerable language is generated by an SSC grammar of (i) degree (2, 1) with eight conditional rules and nine nonterminals, (ii) degree (3, 1) with seven conditional rules and seven nonterminals (iii) degree (4, 1) with six conditional rules and seven nonterminals and (iv) degree (4, 1) with eight conditional rules and six nonterminals.},
 author = {Henning Fernau and Lakshmanan Kuppusamy and Rufus O. Oladele and Indhumathi Raman and Jérôme Durand-Lose and Jarkko Kari and Sergey Verlan},
 doi = {10.3233/FI-2021-2056},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/FI-2021-2056},
 journal = {Fundamenta Informaticae},
 number = {2–3},
 pages = {189–211},
 title = {Improved Descriptional Complexity Results for Simple Semi-Conditional Grammars},
 url = {https://doi-org.crai.referencistas.com/10.3233/FI-2021-2056},
 volume = {181},
 year = {2021c}
}

@article{doi:10.3233/FI-2021-2099,
 abstract = {Reversible computations constitute an unconventional form of computing where any sequence of performed operations can be undone by executing in reverse order at any point during a computation. It has been attracting increasing attention as it provides opportunities for low-power computation, being at the same time essential or eligible in various applications. In recent work, we have proposed a structural way of translating Reversing Petri Nets (RPNs) – a type of Petri nets that embeds reversible computation, to bounded Coloured Petri Nets (CPNs) – an extension of traditional Petri Nets, where tokens carry data values. Three reversing semantics are possible in RPNs: backtracking (reversing of the lately executed action), causal reversing (action can be reversed only when all its effects have been undone) and out of causal reversing (any previously performed action can be reversed). In this paper, we extend the RPN to CPN translation with formal proofs of correctness. Moreover, the possibility of introduction of cycles to RPNs is discussed. We analyze which type of cycles could be allowed in RPNs to ensure consistency with the current semantics. It emerged that the most interesting case related to cycles in RPNs occurs in causal semantics, where various interpretations of dependency result in different net’s behaviour during reversing. Three definitions of dependence are presented and discussed.},
 author = {Kamila Barylska and Anna Gogolińska},
 doi = {10.3233/FI-2021-2099},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/FI-2021-2099},
 journal = {Fundamenta Informaticae},
 number = {4},
 pages = {273–296},
 title = {Acyclic and Cyclic Reversing Computations in Petri Nets},
 url = {https://doi-org.crai.referencistas.com/10.3233/FI-2021-2099},
 volume = {184},
 year = {2022b}
}

@article{doi:10.3233/HSM-120777,
 abstract = {This article is an appeal to incorporate systems thinking into topics and courses in business education and the social sciences. After reviewing the lack of systems thinking and systems theory in American education, its negative impact on decision making and risk analysis, the authors demonstrate how to weave systems thinking and systems theory into the existing curricula, and how to assess the effectiveness of such pedagogy on the decision making and risk analysis process.},
 author = {Kurt M. Gehlert and Thomas Ressler and Donoxti Baylon},
 doi = {10.3233/HSM-120777},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/HSM-120777},
 journal = {Human Systems Management},
 number = {2},
 pages = {79–94},
 title = {Global challenges demand global education of systems thinking},
 url = {https://doi-org.crai.referencistas.com/10.3233/HSM-120777},
 volume = {32},
 year = {2013j}
}

@article{doi:10.3233/HSM-201118,
 abstract = {BACKGROUND: Today’s uncertain economic and social dynamics are leading companies to seek the sort of human talent that will help them to survive and thrive. Training demands are thus arising for specific skills and competencies that would make current students and employment seekers more employable. OBJECTIVE: The aim of this study was to identify the twenty-first century’s major employability skills and competencies as well as the main demand trends for skills and competencies. METHODS: An international panel of experts (from Spain, Thailand and Poland), from both, academic and professional business backgrounds, were asked to quantitatively project the importance of different generic and specific skills and competencies over the next five years. They were asked to do so twice, once before and once after a four-year interval (in 2016 and in 2019). Each time, they were interviewed to discuss the results. RESULTS: The most valued employability skills were of a generic nature, in all three countries. Regarding specific skills, those of a social and managerial nature were the most highly valued. Work experience and formal education became less relevant for employability. CONCLUSIONS: The study’s results can lead to recommendations on how to design a more employability-oriented curriculum in educational institutions.},
 author = {Anna Rakowska and Susana de Juana-Espinosa and Aleš Trunk and David Dawson},
 doi = {10.3233/HSM-201118},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/HSM-201118},
 journal = {Human Systems Management},
 number = {5},
 pages = {669–684},
 title = {Ready for the future? Employability skills and competencies in the twenty-first century: The view of international experts},
 url = {https://doi-org.crai.referencistas.com/10.3233/HSM-201118},
 volume = {40},
 year = {2021n}
}

@article{doi:10.3233/IA-2011-0001,
 abstract = {We introduce the special issue on Evolutionary Computation (EC) reporting a non-exhaustive list of topics which have recently attracted much interest from the EC community, with particular regard to the ones dealt with by the papers included in this issue: EC research, hybrid neuro-evolutionary systems and synergies between EC and complex systems. In addition, we introduce a more technological emerging topic: the parallel implementation of evolutionary and Swarm Intelligence algorithms on graphics processor units (GPUs), by which new applications of evolutionary algorithms have been made possible, even in real-time environments.},
 author = {Leonardo Vanneschi and Luca Mussi and Stefano Cagnoni},
 doi = {10.3233/IA-2011-0001},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/IA-2011-0001},
 journal = {Intelligenza Artificiale},
 number = {1},
 pages = {5–17},
 title = {Hot topics in Evolutionary Computation},
 url = {https://doi-org.crai.referencistas.com/10.3233/IA-2011-0001},
 volume = {5},
 year = {2011s}
}

@article{doi:10.3233/IA-2011-0005,
 author = {Matteo Baldoni and Cristina Baroglio},
 doi = {10.3233/IA-2011-0005},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/IA-2011-0005},
 journal = {Intelligenza Artificiale},
 number = {1},
 pages = {67–69},
 title = {A journey in Computational Logic in Italy},
 url = {https://doi-org.crai.referencistas.com/10.3233/IA-2011-0005},
 volume = {5},
 year = {2011c}
}

@article{doi:10.3233/IA-2011-0012,
 abstract = {In this paper, we briefly describe recent research directions of the Artificial Intelligence group of the University of L’Aquila, Italy. Research activities concern Computational Logic and Artificial Intelligence. About Intelligent Logical Agents, in the last years the group has developed the logical agent-oriented language DALI. Work is under way also in other areas, namely, Non-Monotonic Reasoning and Natural Language Processing. We particularly emphasize recent and future work directions.},
 author = {Stefania Costantini and Alessio Paolucci and Arianna Tocchio and Panagiota Tsintza},
 doi = {10.3233/IA-2011-0012},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/IA-2011-0012},
 journal = {Intelligenza Artificiale},
 number = {1},
 pages = {107–111},
 title = {DALI, RASP, mnemosine: Computational Logic at work},
 url = {https://doi-org.crai.referencistas.com/10.3233/IA-2011-0012},
 volume = {5},
 year = {2011g}
}

@article{doi:10.3233/IA-2011-0017,
 abstract = {We describe recent work on the deployment of computational logic to support the formalisation and implementation of agents in multi-agent systems. Several forms of computational logic systems are needed in this setting, including abductive, argumentative and preference-based systems. We briefly sketch the agent model called KGP, and an ongoing extension of it which is needed to model agents in distributed settings such as the Grid and, more generally, Service-Oriented Architectures.},
 author = {Paolo Mancarella and Francesca Toni},
 doi = {10.3233/IA-2011-0017},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/IA-2011-0017},
 journal = {Intelligenza Artificiale},
 number = {1},
 pages = {139–143},
 title = {Computational logic in agent based systems},
 url = {https://doi-org.crai.referencistas.com/10.3233/IA-2011-0017},
 volume = {5},
 year = {2011l}
}

@article{doi:10.3233/IA-2011-0018,
 abstract = {Computational Logic plays a very relevant role in engineering complex systems. It can be used to specify systems at different levels of abstraction. The specifications are executable, thus providing a working prototype for free. Thanks to its well-founded semantics it can be used to reason about the correctness of the specifications, a fundamental aspect when safety critical applications are developed. Researchers working in the Logic Programming Group at DISI, a Genova University Department, have applied methods and tools of Computational Logic for modelling, prototyping, and verifying complex systems. These three research lines are largely overlapping: the complex systems we take into account are often multiagent systems, for which we propose modelling languages as well as prototyping environments and verification techniques. In this paper we describe activities that, in the last decade, we carried out along these research lines.},
 author = {Viviana Mascardi and Giorgio Delzanno and Maurizio Martelli},
 doi = {10.3233/IA-2011-0018},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/IA-2011-0018},
 journal = {Intelligenza Artificiale},
 number = {1},
 pages = {145–149},
 title = {Some applications of Computational Logic to the development of intelligent systems and verification methods},
 url = {https://doi-org.crai.referencistas.com/10.3233/IA-2011-0018},
 volume = {5},
 year = {2011q}
}

@article{doi:10.3233/ISB-00180,
 abstract = {Prokaryotic genomes annotation has focused on genes location and function. The lack of regulatory information has limited the knowledge on cellular transcriptional regulatory networks. However, as more phylogenetically close genomes are sequenced and annotated, the implementation of phylogenetic footprinting strategies for the recognition of regulators and their regulons becomes more important. In this paper we describe a comparative genomics approach to the prediction of new gamma-proteobacterial regulon members. We take advantage of the phylogenetic proximity of Escherichia coli and other 16 organisms of this subdivision and the intensive search of the space sequence provided by a pattern-matching strategy. Using this approach we complement predictions of regulatory sites made using statistical models currently stored in Tractor_DB, and increase the number of transcriptional regulators with predicted binding sites up to 86. All these computational predictions may be reached at Tractor_DB (www.bioinfo.cu/Tractor_DB, www.tractor.lncc.br, www.ccg.unam.mx/Computational_Genomics/tractorDB/). We also take a first step in this paper towards the assessment of the conservation of the architecture of the regulatory network in the gamma-proteobacteria through evaluating the conservation of the overall connectivity of the network.},
 author = {Marylens     Hernández Guía and Abel González Pérez and Vladimir Espinosa Angarica and Ana T. Vasconcelos and Julio Collado-Vides},
 doi = {10.3233/ISB-00180},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/ISB-00180},
 journal = {In Silico Biology},
 number = {2},
 pages = {209–219},
 title = {Complementing Computationally Predicted Regulatory Sites in     Tractor_DB Using a Pattern Matching Approach},
 url = {https://doi-org.crai.referencistas.com/10.3233/ISB-00180},
 volume = {5},
 year = {2005h}
}

@article{doi:10.3233/ISB-00347,
 abstract = {Three-way junctions in folded RNAs have been investigated both experimentally and computationally. The interest in their analysis stems from the fact that they have significantly been found to possess a functional role. In recent work, three-way junctions have been categorized into families depending on the relative lengths of the segments linking the three helices. Here, based on ideas originating from computational geometry, an algorithm is proposed for detecting three-way junctions in data sets of genes that are related to a metabolic pathway of interest. In its current implementation, the algorithm relies on a moving window that performs energy minimization folding predictions, and is demonstrated on a set of genes that are involved in purine metabolism in plants. The pattern matching algorithm can be extended to other organisms and other metabolic cycles of interest in which three-way junctions have been or will be discovered to play an important role. In the test case presented here with, the computational prediction of a three-way junction in Arabidopsis that was speculated to have an interesting functional role is verified experimentally.},
 author = {Adaya Cohen and Samuel Bocobza and Isana Veksler and Idan Gabdank and Danny Barash and Asaph Aharoni and Michal Shapira and Klara Kedem},
 doi = {10.3233/ISB-00347},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/ISB-00347},
 journal = {In Silico Biology},
 number = {2},
 pages = {105–120},
 title = {Computational Identification of Three-Way Junctions in Folded     RNAs: A Case Study in Arabidopsis},
 url = {https://doi-org.crai.referencistas.com/10.3233/ISB-00347},
 volume = {8},
 year = {2008d}
}

@article{doi:10.3233/ISB-140461,
 abstract = {Abstract Are we close to a complete inventory of living processes so that we might expect in the near future to reproduce every essential aspect necessary for life? Or are there mechanisms and processes in cells and organisms that are presently inaccessible to us? Here I argue that a close examination of a particularly well-understood system— that of Escherichia coli chemotaxis— shows we are still a long way from a complete description. There is a level of molecular uncertainty, particularly that responsible for fine-tuning and adaptation to myriad external conditions, which we presently cannot resolve or reproduce on a computer. Moreover, the same uncertainty exists for any process in any organism and is especially pronounced and important in higher animals such as humans. Embryonic development, tissue homeostasis, immune recognition, memory formation, and survival in the real world, all depend on vast numbers of subtle variations in cell chemistry most of which are presently unknown or only poorly characterized. Overcoming these limitations will require us to not only accumulate large quantities of highly detailed data but also develop new computational methods able to recapitulate the massively parallel processing of living cells.},
 author = {Dennis Bray},
 doi = {10.3233/ISB-140461},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/ISB-140461},
 journal = {In Silico Biology},
 number = {1–2},
 pages = {1–7},
 title = {Limits of computational biology},
 url = {https://doi-org.crai.referencistas.com/10.3233/ISB-140461},
 volume = {12},
 year = {2015c}
}

@article{doi:10.3233/ISB-140464,
 abstract = {Abstract Analysis of metabolic networks typically begins with construction of the stoichiometry matrix, which characterizes the network topology. This matrix provides, via the balance equation, a description of the potential steady-state flow distribution. This paper begins with the observation that the balance equation depends only on the structure of linear redundancies in the network, and so can be stated in a succinct manner, leading to computational efficiencies in steady-state analysis. This alternative description of steady-state behaviour is then used to provide a novel method for network reduction, which complements existing algorithms for describing intracellular networks in terms of input-output macro-reactions (to facilitate bioprocess optimization and control). Finally, it is demonstrated that this novel reduction method can be used to address elementary mode analysis of large networks: the modes supported by a reduced network can capture the input-output modes of a metabolic module with significantly reduced computational effort.},
 author = {Brian P. Ingalls and Eric Bembenek},
 doi = {10.3233/ISB-140464},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/ISB-140464},
 journal = {In Silico Biology},
 number = {1–2},
 pages = {55–67},
 title = {Exploiting stoichiometric redundancies for computational efficiency and           network reduction},
 url = {https://doi-org.crai.referencistas.com/10.3233/ISB-140464},
 volume = {12},
 year = {2015h}
}

@article{doi:10.3233/ISB-2010-0415,
 abstract = {Cell Illustrator is a software platform for Systems Biology that uses the concept of Petri net for modeling and simulating biopathways. It is intended for biological scientists working at bench. The latest version of Cell Illustrator 4.0 uses Java Web Start technology and is enhanced with new capabilities, including: automatic graph grid layout algorithms using ontology information; tools using Cell System Markup Language (CSML) 3.0 and Cell System Ontology 3.0; parameter search module; high-performance simulation module; CSML database management system; conversion from CSML model to programming languages (FORTRAN, C, C++, Java, Python and Perl); import from SBML, CellML, and BioPAX; and, export to SVG and HTML. Cell Illustrator employs an extension of hybrid Petri net in an object-oriented style so that biopathway models can include objects such as DNA sequence, molecular density, 3D localization information, transcription with frame-shift, translation with codon table, as well as biochemical reactions.},
 author = {Masao Nagasaki and Ayumu Saito and Euna Jeong and Chen Li and Kaname Kojima and Emi Ikeda and Satoru Miyano},
 doi = {10.3233/ISB-2010-0415},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/ISB-2010-0415},
 journal = {In Silico Biology},
 number = {1–2},
 pages = {5–26},
 title = {Cell Illustrator 4.0: A Computational Platform for Systems Biology},
 url = {https://doi-org.crai.referencistas.com/10.3233/ISB-2010-0415},
 volume = {10},
 year = {2010m}
}

@article{doi:10.3233/ISP-1979-2629703,
 abstract = {The work described is part of a programme exploring the utility of digital signal-processing techniques for studying certain aspects of the behaviour of water-waves. When such studies are undertaken in a closed wave-tank reflections from the boundaries can be troublesome. To this end a cepstral analysis has been attempted on wave-height measurements to locate and assess these reflections. The auto-regressive power-spectral estimate has been used to obtain the cepstrum with some success and the results are compared with those obtained by more conventional data processing methods.},
 author = {J.O. Flower and S.C. Forge},
 doi = {10.3233/ISP-1979-2629703},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/ISP-1979-2629703},
 journal = {International Shipbuilding Progress},
 number = {297},
 pages = {103–107},
 title = {Cepstral estimation of water-wave reflection-times using the auto-regressive power-spectral computational technique},
 url = {https://doi-org.crai.referencistas.com/10.3233/ISP-1979-2629703},
 volume = {26},
 year = {1979g}
}

@article{doi:10.3233/ISP-1981-2832102,
 abstract = {A theoretical computational method to evaluate the performance characteristics of cycloidal propellers is presented. Starting from Taniguchi’s method, improvements have been derived to make it applicable for various eccentricities in the whole advance coefficient region. First the effect of the curved orbit of the blades and the blade rotation is taken into account, that influences camber and zero lift angle. Then the effect of low Reynolds number on drag coefficient and the effect of stalling on lift coefficient are analysed and discussed in detail. Some linearizations are given up to improve the computation accuracy. The improved method is described. Using this improved method to compute the performance of a DTNSRDC model and comparing the results with systematic experiment results (see DTNSRDC Report 2983), it is shown that the agreement between them is quite good for a larger range of advance ratios and eccentricities.},
 author = {D.M. Zhu},
 doi = {10.3233/ISP-1981-2832102},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/ISP-1981-2832102},
 journal = {International Shipbuilding Progress},
 number = {321},
 pages = {102–111},
 title = {A computational method for cycloidal propellers},
 url = {https://doi-org.crai.referencistas.com/10.3233/ISP-1981-2832102},
 volume = {28},
 year = {1981t}
}

@article{doi:10.3233/ISP-1984-3135701,
 abstract = {Using two-dimensional linearized potential theory, strip theory and a finite element method, mathematical and numerical models are developed for the computation of the hydrodynamic coefficients of a slender ship form, travelling with forward speed in shallow water. The theory is applied to the computation of hydrodynamic mass and damping of a Todd Series ship model. Both experimental and numerical results are presented as functions of oscillatory frequency, water depth and forward speed.},
 author = {J.J.M. Baar},
 doi = {10.3233/ISP-1984-3135701},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/ISP-1984-3135701},
 journal = {International Shipbuilding Progress},
 number = {357},
 pages = {120–130},
 title = {Computation by finite element method of hydrodynamic coefficients of ships in shallow water},
 url = {https://doi-org.crai.referencistas.com/10.3233/ISP-1984-3135701},
 volume = {31},
 year = {1984c}
}

@article{doi:10.3233/ISP-220001,
 abstract = {Hydrodynamic loads acting on a ship can nowadays be reliably obtained from Computational Fluid Dynamics (CFD) techniques. In particular for the determination of the hydrodynamic coefficients of a mathematical manoeuvring model, the forces and moments on a ship sailing at a drift angle or with a yaw rate can be computed efficiently with CFD. While computations with a drift angle are relatively straightforward, computations involving a yaw rate present a challenge. This challenge consists in how to deal with the grid, the setup and the ship encountering its own wake when rotating. A solution based on a single grid setup with consistent boundary conditions and utilising a body force wake damping zone to remedy this challenge is proposed in this paper, leading to an effective, fast, and accurate method to compute hydrodynamic loads of a ship in steady yaw manoeuvres.},
 author = {Guido Oud and Serge Toxopeus},
 doi = {10.3233/ISP-220001},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/ISP-220001},
 journal = {International Shipbuilding Progress},
 number = {1},
 pages = {3–24},
 title = {A technique for efficient computation of steady yaw manoeuvres using CFD},
 url = {https://doi-org.crai.referencistas.com/10.3233/ISP-220001},
 volume = {69},
 year = {2022n}
}

@article{doi:10.3233/JAD-200276,
 abstract = {Background: Alzheimer’s disease (AD) etiopathogenesis remains partially unexplained. The main conceptual framework used to study AD is the Amyloid Cascade Hypothesis, although the failure of recent clinical experimentation seems to reduce its potential in AD research. Objective: A possible explanation for the failure of clinical trials is that they are set too late in AD progression. Recent studies suggest that the ventral tegmental area (VTA) degeneration could be one of the first events occurring in AD progression (pre-plaque stage). Methods: Here we investigate this hypothesis through a computational model and computer simulations validated with behavioral and neural data from patients. Results: We show that VTA degeneration might lead to system-level adjustments of catecholamine release, triggering a sequence of events leading to relevant clinical and pathological signs of AD. These changes consist first in a midfrontal-driven compensatory hyperactivation of both VTA and locus coeruleus (norepinephrine) followed, with the progression of the VTA impairment, by a downregulation of catecholamine release. These processes could then trigger the neural degeneration at the cortical and hippocampal levels, due to the chronic loss of the neuroprotective role of norepinephrine. Conclusion: Our novel hypothesis might contribute to the formulation of a wider system-level view of AD which might help to devise early diagnostic and therapeutic interventions.},
 author = {Daniele Caligiore and Massimo Silvetti and Marcello D’Amelio and Stefano Puglisi-Allegra and Gianluca Baldassarre and Gholamreza Anbarjafari},
 doi = {10.3233/JAD-200276},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/JAD-200276},
 journal = {Journal of Alzheimer’s Disease},
 number = {1},
 pages = {275–290},
 title = {Computational Modeling of Catecholamines Dysfunction in Alzheimer’s Disease at Pre-Plaque Stage},
 url = {https://doi-org.crai.referencistas.com/10.3233/JAD-200276},
 volume = {77},
 year = {2020c}
}

@article{doi:10.3233/JAD-2005-7301,
 abstract = {Prediction or early-stage diagnosis of Alzheimer’s disease (AD) requires a comprehensive understanding of the underlying mechanisms of the disease and its progression. Researchers in this area have approached the problem from multiple directions by attempting to develop (a) neurological (neurobiological and neurochemical) models, (b) analytical models for anatomical and functional brain images, (c) analytical feature extraction models for electroencephalograms (EEGs), (d) classification models for positive identification of AD, and (e) neural models of memory and memory impairment in AD. This article presents a state-of-the-art review of research performed on computational modeling of AD and its markers. The review covers the following approaches: computer imaging, classification models, connectionist neural models, and biophysical neural models. It is concluded that a mixture of markers and a combination of novel computational techniques such as neural computing, chaos theory, and wavelets can increase the accuracy of algorithms for automated detection and diagnosis of AD.},
 author = {Hojjat Adeli and Samanwoy Ghosh-Dastidar and Nahid Dadmehr},
 doi = {10.3233/JAD-2005-7301},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/JAD-2005-7301},
 journal = {Journal of Alzheimer’s Disease},
 note = {PMID:16006662},
 number = {3},
 pages = {187–199},
 title = {Alzheimer’s disease and models of computation: Imaging, classification, and neural models},
 url = {https://doi-org.crai.referencistas.com/10.3233/JAD-2005-7301},
 volume = {7},
 year = {2005a}
}

@article{doi:10.3233/JAD-200941,
 abstract = {Background: The most important hallmark in the neuropathology of Alzheimer’s disease (AD) is the formation of amyloid-β (Aβ) fibrils due to the misfolding/aggregation of the Aβ peptide. Preventing or reverting the aggregation process has been an active area of research. Naturally occurring products are a potential source of molecules that may be able to inhibit Aβ42 peptide aggregation. Recently, we and others reported the anti-aggregating properties of curcumin and some of its derivatives in vitro, presenting an important therapeutic avenue by enhancing these properties. Objective: To computationally assess the interaction between Aβ peptide and a set of curcumin derivatives previously explored in experimental assays. Methods: The interactions of ten ligands with Aβ monomers were studied by combining molecular dynamics and molecular docking simulations. We present the in silico evaluation of the interaction between these derivatives and the Aβ42 peptide, both in the monomeric and fibril forms. Results: The results show that a single substitution in curcumin could significantly enhance the interaction between the derivatives and the Aβ42 monomers when compared to a double substitution. In addition, the molecular docking simulations showed that the interaction between the curcumin derivatives and the Aβ42 monomers occur in a region critical for peptide aggregation. Conclusion: Results showed that a single substitution in curcumin improved the interaction of the ligands with the Aβ monomer more so than a double substitution. Our molecular docking studies thus provide important insights for further developing/validating novel curcumin-derived molecules with high therapeutic potential for AD.},
 author = {Adrian Orjuela and Johant Lakey-Beitia and Randy Mojica-Flores and Muralidhar L. Hegde and Isaias Lans and Jorge Alí-Torres and K.S. Rao and K.S. Jagannatha Rao and Gabrielle B. Britton and Luisa Lilia Rocha Arrieta et al.},
 doi = {10.3233/JAD-200941},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/JAD-200941},
 journal = {Journal of Alzheimer’s Disease},
 number = {s1},
 pages = {S321–S333},
 title = {Computational Evaluation of Interaction Between Curcumin Derivatives and Amyloid-β Monomers and Fibrils: Relevance to Alzheimer’s Disease},
 url = {https://doi-org.crai.referencistas.com/10.3233/JAD-200941},
 volume = {82},
 year = {2021o}
}

@article{doi:10.3233/JAD-201011,
 abstract = {Background: Although the abnormal depositions of amyloid plaques and neurofibrillary tangles are the hallmark of Alzheimer’s disease (AD), converging evidence shows that the individual’s neurodegeneration trajectory is regulated by the brain’s capability to maintain normal cognition. Objective: The concept of cognitive reserve has been introduced into the field of neuroscience, acting as a moderating factor for explaining the paradoxical relationship between the burden of AD pathology and the clinical outcome. It is of high demand to quantify the degree of conceptual cognitive reserve on an individual basis. Methods: We propose a novel statistical model to quantify an individual’s cognitive reserve against neuropathological burdens, where the predictors include demographic data (such as age and gender), socioeconomic factors (such as education and occupation), cerebrospinal fluid biomarkers, and AD-related polygenetic risk score. We conceptualize cognitive reserve as a joint product of AD pathology and socioeconomic factors where their interaction manifests a significant role in counteracting the progression of AD in our statistical model. Results: We apply our statistical models to re-investigate the moderated neurodegeneration trajectory by considering cognitive reserve, where we have discovered that 1) high education individuals have significantly higher reserve against the neuropathology than the low education group; however, 2) the cognitive decline in the high education group is significantly faster than low education individuals after the level of pathological burden increases beyond the tipping point. Conclusion: We propose a computational proxy of cognitive reserve that can be used in clinical routine to assess the progression of AD.},
 author = {Ying Zhang and Yajing Hao and Lang Li and Kai Xia and Guorong Wu},
 doi = {10.3233/JAD-201011},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/JAD-201011},
 journal = {Journal of Alzheimer’s Disease},
 number = {3},
 pages = {1217–1228},
 title = {A Novel Computational Proxy for Characterizing Cognitive Reserve in Alzheimer’s Disease},
 url = {https://doi-org.crai.referencistas.com/10.3233/JAD-201011},
 volume = {78},
 year = {2020s}
}

@article{doi:10.3233/JAD-221020,
 abstract = {Background: The development of therapeutic agents against Alzheimer’s disease (AD) has stalled recently. Drug candidates targeting amyloid-β (Aβ) deposition have often failed clinical trials at different stages, prompting the search for novel targets for AD therapy. The NLRP3 inflammasome is an integral part of innate immunity, contributing to neuroinflammation and AD pathophysiology. Thus, it has become a promising new target for AD therapy. Objective: The study sought to investigate the potential of bioactive compounds derived from Azadirachta-indica to inhibit the NLRP3 protein implicated in the pathophysiology of AD. Methods: Structural bioinformatics via molecular docking and density functional theory (DFT) analysis was utilized for the identification of novel NLRP3 inhibitors from A. indica bioactive compounds. The compounds were further subjected to pharmacokinetic and drug-likeness analysis. Results obtained from the compounds were compared against that of oridonin, a known NLRP3 inhibitor. Results: The studied compounds optimally saturated the binding site of the NLRP3 NACHT domain, forming principal interactions with the different amino acids at its binding site. The studied compounds also demonstrated better bioactivity and chemical reactivity as ascertained by DFT analysis and all the compounds except 7-desacetyl-7-benzoylazadiradione, which had two violations, conformed to Lipinski’s rule of five. Conclusion: In silico studies show that A. indica derived compounds have better inhibitory potential against NLRP3 and better pharmacokinetic profiles when compared with the reference ligand (oridonin). These compounds are thus proposed as novel NLRP3 inhibitors for the treatment of AD. Further wet-lab studies are needed to confirm the potency of the studied compounds.},
 author = {Felix Oluwasegun Ishabiyi and James Okwudirichukwu Ogidi and Baliqis Adejoke Olukade and Chizoba Christabel Amorha and Lina Y. El-Sharkawy and Chukwuemeka Calistus Okolo and Titilope Mary Adeniyi and Nkechi Hope Atasie and Abdulwasiu Ibrahim and Toheeb Adewale Balogun et al.},
 doi = {10.3233/JAD-221020},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/JAD-221020},
 journal = {Journal of Alzheimer’s Disease},
 number = {s1},
 pages = {S67–S85},
 title = {Computational Evaluation of Azadirachta indica-Derived Bioactive Compounds as Potential Inhibitors of NLRP3 in the Treatment of Alzheimer’s Disease},
 url = {https://doi-org.crai.referencistas.com/10.3233/JAD-221020},
 volume = {94},
 year = {2023h}
}

@article{doi:10.3233/JAD-230825,
 abstract = {Background: There is increasing evidence from animal and clinical studies that network hyperexcitability (NH) may be an important pathophysiological process and potential target for treatment in early Alzheimer’s disease (AD). Measures of functional connectivity (FC) have been proposed as promising biomarkers for NH, but it is unknown which measure has the highest sensitivity for early-stage changes in the excitation/inhibition balance. Objective: We aim to test the performance of different FC measures in detecting NH at the earliest stage using a computational approach. Methods: We use a whole brain computational model of activity dependent degeneration to simulate progressive AD pathology and NH. We investigate if and at what stage four measures of FC (amplitude envelope correlation corrected [AECc], phase lag index [PLI], joint permutation entropy [JPE] and a new measure: phase lag time [PLT]) can detect early-stage AD pathophysiology. Results: The activity dependent degeneration model replicates spectral changes in line with clinical data and demonstrates increasing NH. Compared to relative theta power as a gold standard the AECc and PLI are shown to be less sensitive in detecting early-stage NH and AD-related neurophysiological abnormalities, while the JPE and the PLT show more sensitivity with excellent test characteristics. Conclusions: Novel FC measures, which are better in detecting rapid fluctuations in neural activity and connectivity, may be superior to well-known measures such as the AECc and PLI in detecting early phase neurophysiological abnormalities and in particular NH in AD. These markers could improve early diagnosis and treatment target identification.},
 author = {Cornelis Jan Stam and Willem de Haan and Miguel Castelo-Branco},
 doi = {10.3233/JAD-230825},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/JAD-230825},
 journal = {Journal of Alzheimer’s Disease},
 number = {4},
 pages = {1333–1348},
 title = {Network Hyperexcitability in Early-Stage Alzheimer’s Disease: Evaluation of Functional Connectivity Biomarkers in a Computational Disease Model},
 url = {https://doi-org.crai.referencistas.com/10.3233/JAD-230825},
 volume = {99},
 year = {2024q}
}

@article{doi:10.3233/JHS-130457,
 abstract = {Over the past decade, the ever-increasing energy demands of IT infrastructures have posed significant challenges for the research community in terms of reducing their total power consumption and minimizing their environmental impact. Optical communication networks are envisioned to be promising candidates to help preventing this problem affecting the Internet backbone, as well as for distributed applications such as computational grids. In this paper, we propose an adaptive and distributed scheme for the establishment of energy-efficient lightpaths in computational grids. The grid is deployed over an optical circuit-switched backbone network, defining an optical grid network. Each node of the backbone network maintains two different dynamic thresholds values and estimates the changes in network performance by evaluating the moving average of the total wavelength channel occupancy on all its input/output links. The nodes have the ability of reducing the energy consumption by entering into an Energy Saving Mode (ESM) on the basis of a comparison between their channel occupancy and the thresholds. Furthermore, we extend our framework by allowing the thresholds to be dynamically adapted depending on the network performance in terms of blocking probability. We show that the proposed method achieves considerable energy savings when compared to a normal energy-unaware operational mode and still allows to maintain an acceptable level of network performance in terms of blocking probability and end-to-end delay. Numerical results are obtained with a Java event-driven simulator of two different optical network topologies.},
 author = {Daniele Tafani and Burak Kantarci and Hussein T. Mouftah and Conor McArdle and Liam P. Barry},
 doi = {10.3233/JHS-130457},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/JHS-130457},
 journal = {Journal of High Speed Networks},
 number = {1},
 pages = {1–18},
 title = {A distributed framework for energy-efficient lightpaths in computational grids},
 url = {https://doi-org.crai.referencistas.com/10.3233/JHS-130457},
 volume = {19},
 year = {2013q}
}

@article{doi:10.3233/JHS-150519,
 abstract = {Abstract Rational computation means secure multi-party computation in the presence of rational parties, who care about maximizing their utility. Here the notion of utility denotes payoffs when parties take certain actions in the computation. Therefore rational parties may carefully choose their actions before they interact with others. Traditionally, the main tasks of rational computation is to encourage parties to exchange information with their opponents, even if they do not trust each other. Exchanging information is similar to the action of cooperation towards the view of game theory. Therefore, measures should be taken to encourage parties to cooperate with others in rational computation. In this paper, we assume that rational parties who participate in the computation protocol form a social cloud. Parties in the social cloud may interact within several rounds and in each round, some desirable properties such as reputation may be generated. Parties who have good reputation means they are likely to cooperate with others. The structure of social cloud is not static. Instead, it evolutes when parties complete one round of computation. We mainly discuss the impact of social cloud structure on rational computation. Simulation results show that when the community structure reach to a proper value, parties are more likely to cooperate in the computation protocol. In addition, rational parties can gain optimal utilities.},
 author = {Yilei Wang and Tao Li and Qianqian Liu and Jing Sun and Zhe Liu},
 doi = {10.3233/JHS-150519},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/JHS-150519},
 journal = {Journal of High Speed Networks},
 number = {3},
 pages = {181–194},
 title = {The impact of social cloud reputation and structure on rational computation},
 url = {https://doi-org.crai.referencistas.com/10.3233/JHS-150519},
 volume = {21},
 year = {2015s}
}

@article{doi:10.3233/JID-2007-11302,
 abstract = {In early phases of design a wide exploration of the design space is crucial to the development of creative solutions. In this regard, Evolutionary Computation (EC), and in particular Genetic Algorithms, contain several qualities that can enhance exploration by opening the search process beyond the focus of finding a single “best” solution. Over the years many researchers in the area of creative thinking including Gordon, de Bono, Parnes, Osborn and others, have suggested design strategies that have interesting parallels in EC processes. For instance, a well known inhibitor of creative thinking is design fixation, where the suggestion of a particular solution makes it difficult to imagine other good solutions. Unlike many other computational search algorithms, EC methods work with populations of “fairly good” solutions. Therefore, there is less danger that creativity will be harmed by design fixation on one “best” solution. This paper shows through a specific example of a truss bridge how an EC based design exploration program can aid the designer by providing a selection of “pretty good” solutions rather than a single optimal solution. Other aspects of the EC program are also discussed including drawbacks to the method such as computational intensity as well as directions of future development.},
 author = {Peter von Buelow},
 doi = {10.3233/JID-2007-11302},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/JID-2007-11302},
 journal = {Journal of Integrated Design and Process Science},
 number = {3},
 pages = {5–18},
 title = {ADVANTAGES OF EVOLUTIONARY COMPUTATION USED FOR EXPLORATION IN THE     CREATIVE DESIGN PROCESS},
 url = {https://doi-org.crai.referencistas.com/10.3233/JID-2007-11302},
 volume = {11},
 year = {2007c}
}

@article{doi:10.3233/JID-2007-11303,
 abstract = {This paper focuses on three connected to each other fundamental beliefs that appear to be unconsciously taken for granted at the basis of modern cognitive neuroscience: (i) the attractor hypothesis, (ii) the neural code hypothesis, and (iii) the supervenience hypothesis about human subjective experience. Their precise understanding and experimental verification is an important challenge that calls for a scientific paradigm change and needs to be addressed for future progress in cognitive neuroscience. The work presents a theoretical analysis and preliminary experimental and modeling results that suggest a roadmap to solving the challenge.},
 author = {Alexei V. Samsonovich},
 doi = {10.3233/JID-2007-11303},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/JID-2007-11303},
 journal = {Journal of Integrated Design and Process Science},
 number = {3},
 pages = {19–30},
 title = {BRINGING CONSCIOUSNESS TO COGNITIVE NEUROSCIENCE: A COMPUTATIONAL     PERSPECTIVE},
 url = {https://doi-org.crai.referencistas.com/10.3233/JID-2007-11303},
 volume = {11},
 year = {2007o}
}

@article{doi:10.3233/jid-2012-0009,
 abstract = {Pervasive computing (PerC) is leading the way in a fast-growing trend of integrating transparently physical heterogeneous computational devices into our private and professional lives. The ubiquity of these devices and advances in developing software solutions in PerC across domains, have raised hopes for the creation of true wide-spread pervasive computing environments (PCE). In this paper we explore the possibility of applying semantics of PCEs in the healthcare domain, and in Self Care Homes (SeCH) in particular, in order to define and comment on its computationally significant semantics. Our aim is to illustrate that we can manipulate the computationally significant semantics of SeCH through OWL/SWRL enabled ontologies, as candidate technologies for achieving effective and automated decision making in SeCH. The possibility of reasoning upon OWL/SWRL enabled concepts and creating computations from them, and enables the delivery of healthcare services to SeCH residents. They are automatically supported by software applications generated from the Assistive Self Care Systems (ASeCS) software architecture, which hosts our OWL/SWRL enabled ontology and its reasoning.},
 author = {Reza Shojanoori and Radmila Juric and Mahi Lohi},
 doi = {10.3233/jid-2012-0009},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/jid-2012-0009},
 journal = {Journal of Integrated Design and Process Science},
 number = {1},
 pages = {43–62},
 title = {Computationally Significant Semantics in Pervasive     Healthcare},
 url = {https://doi-org.crai.referencistas.com/10.3233/jid-2012-0009},
 volume = {16},
 year = {2012p}
}

@article{doi:10.3233/jid-2015-0018,
 author = {Raymond Yeh},
 doi = {10.3233/jid-2015-0018},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/jid-2015-0018},
 journal = {Journal of Integrated Design and Process Science},
 number = {4},
 pages = {3–8},
 title = {Convergence of Transdisciplinary Education},
 url = {https://doi-org.crai.referencistas.com/10.3233/jid-2015-0018},
 volume = {19},
 year = {2015s}
}

@article{doi:10.3233/jid-2017-0004,
 abstract = {Constructionism is at once a learning epistemology, a theory of learning, and a pedagogical approach that literally places education in the hands of learners. Constructionism situates students and mentors together in student-directed, project-oriented environments, often enabled with state-of-the-art computational technologies, to foster playful exploration and experimentation. Over time, shared learning experiences in constructionist environments may lead to the formation of learning cultures. To orient the design of constructionist environments for designers of engineering design education, this paper provides a historical context for Seymour Papert’s development of constructionism and distinguishes it from Jean Piaget’s philosophy of constructivism. Constructionism is introduced as an effort targeted at the reform of traditional education. Examples of constructionist environments for learning are then provided. The applicability of constructionism for the design of learning environments for engineering design is also discussed and observations are given for designing the tools, strategies, and activities that comprise constructionist learning environments for engineering design education.},
 author = {Carolyn E. Psenka and Kyoung-Yun Kim and Gül E. Okudan Kremer and Karl R. Haapala and Kathy L. Jackson},
 doi = {10.3233/jid-2017-0004},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/jid-2017-0004},
 journal = {Journal of Integrated Design and Process Science},
 number = {2},
 pages = {3–20},
 title = {Translating Constructionist Learning to Engineering Design Education},
 url = {https://doi-org.crai.referencistas.com/10.3233/jid-2017-0004},
 volume = {21},
 year = {2017o}
}

@article{doi:10.3233/JID-220009,
 abstract = {The design of real-world industrial systems is subject to a natural tendency towards modularization in order to manage complexity. In addition, this article considers that patterns of self-similarity in many problem domains have made many such solutions naturally representable as holarchies. Likewise, the increasing need for autonomous local decision making as well as the demand to produce solutions at scale has increased the relevance of the multi-agent paradigm to the creation of modern software systems. A variety of software development patterns are explored for their compatibility with holonic multi-agency. The current skill sets required by software development workers and concomitant training activities focus on instilling computational thinking abilities, a set of related cognitive competencies useful in the development of such systems. Intelligent systems play an increasingly important role in modern development and often benefit from computational intelligence techniques for the purpose of parameter tuning. This position paper explores the intersections between holonic multi-agency, modern information systems development, the computational intelligence which train them and the computational thinking skills those developers should be trained in.},
 author = {Duncan Anthony Coulter},
 doi = {10.3233/JID-220009},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/JID-220009},
 journal = {Journal of Integrated Design and Process Science},
 number = {3–4},
 pages = {369–391},
 title = {The convergence of computational thinking, computational intelligence, and multi-agency},
 url = {https://doi-org.crai.referencistas.com/10.3233/JID-220009},
 volume = {26},
 year = {2022b}
}

@article{doi:10.3233/JID-220024,
 abstract = {System knowledge and reasoning mechanisms are essential means for intellectualization of cyber-physical systems (CPSs). As enablers of system intelligence, they make such systems able to solve application problems and to maintain their efficient operation. Normally, system intelligence has a human-created initial part and a system-produced (extending) part, called synthetic system intelligence (SSI). This position paper claims that SSI can be converted to a new industrial asset and utilized as such. Unfortunately, no overall theory of SSI exists and its conceptual framework, management strategy, and computational methodologies are still in a premature stage. This is the main reason why no significant progress has been achieved in this field, contrary to the latent potentials. This paper intends to contribute to: (i) understanding the nature and fundamentals of SSI, (ii) systematizing the elicitation and transfer of SSI, (iii) exploration of analogical approaches to utilization of SSI, and (iv) road-mapping and scenario development for the exploitation of SSI as an industrial asset. First, the state of the art is surveyed and the major findings are presented. Then, four families of analogical approaches to SSI transfer are analyzed. These are: (i) knowledge transfer based on repositories, (ii) transfer among agents, (iii) transfer of learning resources, and (iv) transfer by emerging approaches. A procedural framework is proposed that identifies the generic functionalities needed for a quasi-autonomous handling of SSI as an industrial asset. The last section casts light on some important open issues and necessary follow-up research and development activities.},
 author = {Imre Horváth},
 doi = {10.3233/JID-220024},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/JID-220024},
 journal = {Journal of Integrated Design and Process Science},
 number = {2},
 pages = {111–133},
 title = {Utilization of synthetic system intelligence as a new industrial asset},
 url = {https://doi-org.crai.referencistas.com/10.3233/JID-220024},
 volume = {27},
 year = {2024e}
}

@article{doi:10.3233/JID-229010,
 abstract = {Digital engineering transformation is a crucial process for the engineering paradigm shifts in the fourth industrial revolution (4IR), and artificial intelligence (AI) is a critical enabling technology in digital engineering transformation. This article discusses the following research questions: What are the fundamental changes in the 4IR? More specifically, what are the fundamental changes in engineering? What is digital engineering? What are the main uncertainties there? What is trustworthy AI? Why is it important today? What are emerging engineering paradigm shifts in the 4IR? What is the relationship between the data-intensive paradigm and digital engineering transformation? What should we do for digitalization? From investigating the pattern of industrial revolutions, this article argues that ubiquitous machine intelligence (uMI) is the defining power brought by the 4IR. Digitalization is a condition to leverage ubiquitous machine intelligence. Digital engineering transformation towards Industry 4.0 has three essential building blocks: digitalization of engineering, leveraging ubiquitous machine intelligence, and building digital trust and security. The engineering design community at large is facing an excellent opportunity to bring the new capabilities of ubiquitous machine intelligence, and trustworthy AI principles, as well as digital trust, together in various engineering systems design to ensure the trustworthiness of systems in Industry 4.0.},
 author = {Jingwei Huang},
 doi = {10.3233/JID-229010},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/JID-229010},
 journal = {Journal of Integrated Design and Process Science},
 number = {3–4},
 pages = {267–290},
 title = {Digital engineering transformation with trustworthy AI towards industry 4.0: emerging paradigm shifts},
 url = {https://doi-org.crai.referencistas.com/10.3233/JID-229010},
 volume = {26},
 year = {2022e}
}

@article{doi:10.3233/JID-230046,
 abstract = {This Extended Editorial has been compiled by the members of the Editorial Board to celebrate the 25th anniversary of the establishment of the Journal of Integrated Design and Process Science, which operates as the Transactions of the Society for Process and Design Science. The paper divides in three parts. The first part provides a detailed overview of the preliminaries, the objectives, and the periods of operation. It also includes a summary of the current application-orientated professional fields of interests, which are: (i) convergence mechanisms of creative scientific disciplines, (ii) convergence of artificial intelligence, team and health science, (iii) convergence concerning next-generation cyber-physical systems, and (iv) convergence in design and engineering education. The second part includes invited papers, which exemplify domains within the four fields of interest, and also represent good examples of science communication. Short synopses of the contents of these representative papers are included. The third part takes the major changes in scientific research and the academic publication arena into consideration, circumscribes the mission and vision as formulated by the current Editorial Board, and elaborates on the planned strategic exploration and utilization domains of interest.},
 author = {Imre Horváth and Thomas T.H. Wan and Jingwei Huang and Eric Coatanéa and Julia M. Rayz and Yong Zeng and Kyoung-Yun Kim},
 doi = {10.3233/JID-230046},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/JID-230046},
 journal = {Journal of Integrated Design and Process Science},
 number = {3–4},
 pages = {197–221},
 title = {Seeing the Past, Planning the Future: Proudly Celebrating 25 Years of Assisting the Convergence of Process Sciences and Design Science},
 url = {https://doi-org.crai.referencistas.com/10.3233/JID-230046},
 volume = {26},
 year = {2022e}
}

@article{doi:10.3233/JNR-190139,
 abstract = {This contribution describes the computational methodology behind an optimization procedure for a scattered beam collimator. The workflow includes producing a file that can be manufactured via additive methods. A conical collimator, optimized for neutron diffraction experiments in a high pressure clamp cell, is presented as an example. In such a case the scattering from the sample is much smaller than that of the pressure cell. Monte Carlo Ray tracing in MCViNE was used to model scattering from a Si powder sample and the cell. A collimator was inserted into the simulation and the number and size of channels were optimized to maximize the rejection of the parasitic signal coming from the complex sample environment. Constraints, provided by the additive manufacturing process as well as a specific neutron diffractometer, were also included in the optimization. The source code and the tutorials are available in c3dp (Islam (2019)).},
 author = {Fahima Islam and Jiao Lin and Thomas Huegle and Ian Lumsden and David Anderson and Amy Elliott and Bianca Haberl and Garrett Granroth and Kenneth W. Herwig and Erik B. Iverson},
 doi = {10.3233/JNR-190139},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/JNR-190139},
 journal = {Journal of Neutron Research},
 number = {2–3},
 pages = {155–168},
 title = {Computational optimization of a 3D printed collimator},
 url = {https://doi-org.crai.referencistas.com/10.3233/JNR-190139},
 volume = {22},
 year = {2020k}
}

@article{doi:10.3233/NRE-230193,
 abstract = {BACKGROUND: Premature newborns have a higher risk of abnormal visual development and visual impairment. OBJECTIVE: To develop a computational methodology to help assess functional vision in premature infants by tracking iris distances. METHODS: This experimental study was carried out with children up to two years old. A pattern of image capture with the visual stimulus was proposed to evaluate visual functions of vertical and horizontal visual tracking, visual field, vestibulo-ocular reflex, and fixation. The participants’ visual responses were filmed to compose a dataset and develop a detection algorithm using the OpenCV library allied with FaceMesh for the detection and selection of the face, detection of specific facial points and tracking of the iris positions is done. A feasibility study was also conducted from the videos processed by the software. RESULTS: Forty-one children of different ages and diagnoses participated in the experimental study, forming a robust dataset. The software resulted in the tracking of iris positions during visual function evaluation stimuli. Furthermore, in the feasibility study, 8 children participated, divided into Pre-term and Term groups. There was no statistical difference in any visual variable analyzed in the comparison between groups. CONCLUSION: The computational methodology developed was able to track the distances traveled by the iris, and thus can be used to help assess visual function in children.},
 author = {Ricardo Pires Maciel and Bruna Samantha Marchi and Henrique da Silva da Silveira and Giovana Pascoali Rodovanski and Aicha Al-Rob and Rodrigo Souza and Marcelo Fernandes Costa and Cristiane Aparecida Moran and Antonio Carlos Sobieranski},
 doi = {10.3233/NRE-230193},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/NRE-230193},
 journal = {NeuroRehabilitation},
 number = {2},
 pages = {227–235},
 title = {Computational methodology to support functional vision assessment in premature infants: A viability study},
 url = {https://doi-org.crai.referencistas.com/10.3233/NRE-230193},
 volume = {54},
 year = {2024l}
}

@article{doi:10.3233/WOR-210496,
 abstract = {BACKGROUND: The emotional management of workers can not only increase the efficiency of work, but also contribute to the improvement of the productivity of a company. OBJECTIVE: This scoping review surveyed the literature to identify the relationship between postural expression and emotion during sedentary tasks. METHODS: We searched relevant literature published up to December 1, 2019 using seven electronic databases (PubMed, CINAHL, Embase, Web of Science, PsycINFO, IEEE Xplore, and MEDLINE Complete). RESULTS: A total of 14 publications were included in this scoping review. It was found that the application of pressure sensor and camera-based measurement equipment was effective. Additionally, it was proposed to predict the emotional state of the worker by using forward and backward movements as the main variable as opposed to left and right movements. The information-based analysis technique was able to further increase the accuracy of workers’ emotion prediction. CONCLUSIONS: The emotion prediction of workers based on sitting posture could be confirmed for certain movements, and the information-based technical method could further increase the accuracy of prediction. Expansion of information-based technical research will further increase the possibility of predicting the emotions of workers based on posture, and this will in turn promote safer and more efficient work performance.},
 author = {Jihye Do and Ingyu Yoo},
 doi = {10.3233/WOR-210496},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/WOR-210496},
 journal = {WORK},
 number = {3},
 pages = {831–841},
 title = {Measuring posture change to detect emotional conditions for workers: A scoping review},
 url = {https://doi-org.crai.referencistas.com/10.3233/WOR-210496},
 volume = {73},
 year = {2022e}
}

@article{doi:10.3233/XST-140420,
 abstract = {This study presents a computational fluid dynamics (CFD) model to simulate the three-dimensional airflow in the trachea before and after the vascular ring surgery (VRS). The simulation was based on CT-scan images of the patients with the vascular ring diseases. The surface geometry of the tracheal airway was reconstructed using triangular mesh by the Amira software package. The unstructured tetrahedral volume meshes were generated by the ANSYS ICEM CFD software package. The airflow in the tracheal airway was solved by the ESI CFD-ACE+ software package. Numerical simulation shows that the pressure drops across the tracheal stenosis before and after the surgery were 0.1789 and 0.0967 Pa, respectively, with the inspiratory inlet velocity 0.1 m/s. Meanwhile, the improvement percentage by the surgery was 45.95%. In the expiratory phase, by contrast, the improvement percentage was 40.65%. When the inspiratory velocity reached 1 m/s, the pressure drop became 4.988~Pa and the improvement percentage was 43.32%. Simulation results further show that after treatment the pressure drop in the tracheal airway was significantly decreased, especially for low inspiratory and expiratory velocities. The CFD method can be applied to quantify the airway pressure alteration and to evaluate the treatment outcome of the vascular ring surgery under different respiratory velocities.},
 author = {Fong-Lin Chen and Tzyy-Leng Horng and Tzu-Ching Shih},
 doi = {10.3233/XST-140420},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/XST-140420},
 journal = {Journal of X-Ray Science and Technology},
 number = {2},
 pages = {213–225},
 title = {Simulation analysis of airflow alteration in the trachea following     the vascular ring surgery based on CT images using the computational fluid     dynamics method},
 url = {https://doi-org.crai.referencistas.com/10.3233/XST-140420},
 volume = {22},
 year = {2014e}
}

@article{doi:10.3233/XST-2012-00353,
 abstract = {Background and purpose: Computational fluid dynamics method (CFDM) and optical flow method (OFM) effectively provide the hemodynamic information based on the digital subtraction angiogram (DSA). However, the quantitative analysis in comparison of CFDM and OFM is still absent. The goal of this study is to apply CFDM and OFM in quantitative analysis of stenting treatment. Material and method: A left carotid stenosis patient underwent stenting of percutaneous transluminal angioplasty was analyzed as an example. CFDM and OFM for hemodynamic analysis on digital subtraction angiography before and after stenting treatment were presented. Results: Improvement gains of blood flow velocities on left internal carotid artery after stenting treament for different initial conditions on the common carotid artery were 1.91 ∼ 2.13, 1.62 ∼ 2.09, and 0.69 by CFDM with Newtonian and non-Newtonian fluids and OFM, respectively. With the CFDM analysis, the flow mapping by OFM using time resolved DSA data on the fly to estimate hemodynamic significance of a cervical carotid stenosis was explained. Conclusion: Quantificative blood flow estimations by CFDM and OFM to evaluate the treatment outcomes to patient with carotid stenosis are practical. Both methods are able to provide quantitative information of blood flow for stenting treatment. It is advantagious to use both methods in treatment evaluation.},
 author = {Tzu-Ching Shih and Yang-Hsien Lin and Yung-Jen Ho and Hung-Da Hsiao and Yung-Hui Huang and Tzung-Chi Huang},
 doi = {10.3233/XST-2012-00353},
 eprint = {https://doi-org.crai.referencistas.com/10.3233/XST-2012-00353},
 journal = {Journal of X-Ray Science and Technology},
 number = {4},
 pages = {469–481},
 title = {Hemodynamic analysis of vascular stenting treatment outcome:     Computational fluid dynamics method vs optical flow method},
 url = {https://doi-org.crai.referencistas.com/10.3233/XST-2012-00353},
 volume = {20},
 year = {2012r}
}

@article{doi:10.4137/BBI.S13402,
 abstract = {Saint Louis encephalitis virus, a member of the flaviviridae subgroup, is a culex mosquito-borne pathogen. Despite severe epidemic outbreaks on several occasions, not much progress has been made with regard to an epitope-based vaccine designed for Saint Louis encephalitis virus. The envelope proteins were collected from a protein database and analyzed with an in silico tool to identify the most immunogenic protein. The protein was then verified through several parameters to predict the T-cell and B-cell epitopes. Both T-cell and B-cell immunity were assessed to determine that the protein can induce humoral as well as cell-mediated immunity. The peptide sequence from 330–336 amino acids and the sequence REYCYEATL from the position 57 were found as the most potential B-cell and T-cell epitopes, respectively. Furthermore, as an RNA virus, one important thing was to establish the epitope as a conserved one; this was also done by in silico tools, showing 63.51% conservancy. The epitope was further tested for binding against the HLA molecule by computational docking techniques to verify the binding cleft epitope interaction. However, this is a preliminary study of designing an epitope-based peptide vaccine against Saint Louis encephalitis virus; the results awaits validation by in vitro and in vivo experiments.},
 author = {Anayet Hasan and Mehjabeen Hossain and Jibran Alam},
 doi = {10.4137/BBI.S13402},
 eprint = {https://doi-org.crai.referencistas.com/10.4137/BBI.S13402},
 journal = {Bioinformatics and Biology Insights},
 note = {PMID:24324329},
 number = { },
 pages = {BBI.S13402},
 title = {A Computational Assay to Design an Epitope-Based Peptide Vaccine against Saint Louis Encephalitis Virus},
 url = {https://doi-org.crai.referencistas.com/10.4137/BBI.S13402},
 volume = {7},
 year = {2013m}
}

@article{doi:10.4137/BBI.S13403,
 abstract = {Sea anemone neurotoxins are peptides that interact with Na+ and K+ channels, resulting in specific alterations on their functions. Some of these neurotoxins (1ROO, 1BGK, 2K9E, 1BEI) are important for the treatment of about 80 autoimmune disorders because of their specificity for Kv1.3 channel. The aim of this study was to identify the common residues among these neurotoxins by computational methods, and establish whether there is a pattern useful for the future generation of a treatment for autoimmune diseases. Our results showed eight new key common residues between the studied neurotoxins interacting with a histidine ring and the selectivity filter of the receptor, thus showing a possible pattern of interaction. This knowledge may serve as an input for the design of more promising drugs for autoimmune treatments.},
 author = {Angélica Sabogal-Arango and George E. Barreto and David Ramírez-Sánchez and Juan González-Mendoza and Viviana Barreto and Ludis Morales and Janneth González},
 doi = {10.4137/BBI.S13403},
 eprint = {https://doi-org.crai.referencistas.com/10.4137/BBI.S13403},
 journal = {Bioinformatics and Biology Insights},
 note = {PMID:24812496},
 number = { },
 pages = {BBI.S13403},
 title = {Computational Insights of the Interaction among Sea Anemones Neurotoxins and Kv1.3 Channel},
 url = {https://doi-org.crai.referencistas.com/10.4137/BBI.S13403},
 volume = {8},
 year = {2014j}
}

@article{doi:10.4137/BBI.S13649,
 abstract = {MicroRNAs (miRNAs) are small, noncoding RNA molecules that regulate transcriptional and posttranscriptional gene regulation of the cell. Experimental evidence shows that miRNAs have a direct role in different cellular processes, such as immune function, apoptosis, and tumorigenesis. In a viral infection context, miRNAs have been connected with the interplay between host and pathogen, occupying a major role in pathogenesis. While numerous viral miRNAs from DNA viruses have been identified, characterization of functional RNA virus-encoded miRNAs and their potential targets is still ongoing. Here, we used an in silico approach to analyze dengue Virus genome sequences. Pre-miRNAs were extracted through VMir software, and the identification of putative pre-miRNAs and mature miRNAs was accessed using Support Vector Machine web tools. The targets were scanned using miRanda software and functionally annotated using ClueGo. Via computational tools, eight putative miRNAs were found to hybridize with numerous targets of morphogenesis, differentiation, migration, and growth pathways that may play a major role in the interaction of the virus and its host. Future approaches will focus on experimental validation of their presence and target messenger RNA genes to further elucidate their biological functions in human and mosquito cells.},
 author = {Maicol Ospina-Bedoya and Natalia Campillo-Pedroza and Juan P. Franco-Salazar and Juan C. Gallego-Gómez},
 doi = {10.4137/BBI.S13649},
 eprint = {https://doi-org.crai.referencistas.com/10.4137/BBI.S13649},
 journal = {Bioinformatics and Biology Insights},
 note = {PMID:25210446},
 number = { },
 pages = {BBI.S13649},
 title = {Computational Identification of Dengue Virus MicroRNA-Like Structures and Their Cellular Targets},
 url = {https://doi-org.crai.referencistas.com/10.4137/BBI.S13649},
 volume = {8},
 year = {2014j}
}

@article{doi:10.4137/BBI.S30884,
 abstract = {RNA sequencing (RNA-seq) has revolutionized transcriptome analysis through profiling the expression of thousands of genes at the same time. Systematic analysis of orthologous transcripts across species is critical for understanding the evolution of gene expression and uncovering important information in animal models of human diseases. Several computational methods have been published for analyzing gene expression between species, but they often lack crucial details and therefore cannot serve as a practical guide. Here, we present the first step-by-step protocol for cross-species RNA-seq analysis with a concise workflow that is largely based on the free open-source R language and Bioconductor packages. This protocol covers the entire process from short-read mapping, gene expression quantification, differential expression analysis to pathway enrichment. Many useful utilities for data visualization are included. This complete and easy-to-follow protocol provides hands-on guidance for users who are new to cross-species gene expression analysis.},
 author = {Peter R. LoVerso and Feng Cui},
 doi = {10.4137/BBI.S30884},
 eprint = {https://doi-org.crai.referencistas.com/10.4137/BBI.S30884},
 journal = {Bioinformatics and Biology Insights},
 note = {PMID:26692761},
 number = { },
 pages = {BBI.S30884},
 title = {A Computational Pipeline for Cross-Species Analysis of RNA-seq Data Using R and Bioconductor},
 url = {https://doi-org.crai.referencistas.com/10.4137/BBI.S30884},
 volume = {9},
 year = {2015l}
}

@article{doi:10.4137/BBI.S430,
 abstract = {Apoptosis is the phenotypic result of an active, regulated process of self-destruction. Following various cellular insults, apoptosis has been demonstrated in numerous unicellular eukaryotes, but very little is known about the genes and proteins that initiate and execute this process in this group of organisms. A bioinformatic approach presents an array of powerful methods to direct investigators in the identification of the apoptosis machinery in protozoans. In this review, we discuss some of the available computational methods and illustrate how they may be applied using the identification of a Plasmodium falciparum metacaspase gene as an example.},
 author = {Pierre Marcel Durand and Theresa Louise Coetzer},
 doi = {10.4137/BBI.S430},
 eprint = {https://doi-org.crai.referencistas.com/10.4137/BBI.S430},
 journal = {Bioinformatics and Biology Insights},
 note = {PMID:19812769},
 number = { },
 pages = {BBI.S430},
 title = {Utility of Computational Methods to Identify the Apoptosis Machinery in Unicellular Eukaryotes},
 url = {https://doi-org.crai.referencistas.com/10.4137/BBI.S430},
 volume = {2},
 year = {2008f}
}

@article{doi:10.4137/BBI.S5983,
 abstract = {Time series of gene expression often exhibit periodic behavior under the influence of multiple signal pathways, and are represented by a model that incorporates multiple harmonics and noise. Most of these data, which are observed using DNA microarrays, consist of few sampling points in time, but most periodicity detection methods require a relatively large number of sampling points. We have previously developed a detection algorithm based on the discrete Fourier transform and Akaike’s information criterion. Here we demonstrate the performance of the algorithm for small-sample time series data through a comparison with conventional and newly proposed periodicity detection methods based on a statistical analysis of the power of harmonics. We show that this method has higher sensitivity for data consisting of multiple harmonics, and is more robust against noise than other methods. Although “combinatorial explosion” occurs for large datasets, the computational time is not a problem for small-sample datasets. The MATLAB/GNU Octave script of the algorithm is available on the author’s web site: http://www.cbrc.jp/%7Etominaga/piccolo/.},
 author = {Daisuke Tominaga},
 doi = {10.4137/BBI.S5983},
 eprint = {https://doi-org.crai.referencistas.com/10.4137/BBI.S5983},
 journal = {Bioinformatics and Biology Insights},
 note = {PMID:21151841},
 number = { },
 pages = {BBI.S5983},
 title = {Periodicity Detection Method for Small-Sample Time Series Datasets},
 url = {https://doi-org.crai.referencistas.com/10.4137/BBI.S5983},
 volume = {4},
 year = {2010q}
}

@article{doi:10.4137/BBI.S8971,
 abstract = {Computational design of small molecule putative inhibitors of Polo-like kinase 1 (Plk1) is presented. Plk1, which regulates the cell cycle, is often over expressed in cancers. Down regulation of Plk1 has been shown to inhibit tumor progression. Most kinase inhibitors interact with the ATP binding site on Plk1, which is highly conserved. This makes the development of Plk1-specific inhibitors challenging, since different kinases have similar ATP sites. However, Plk1 also contains a unique region called the polo-box domain (PBD), which is absent from other kinases. In this study, the PBD site was used as a target for designed Plk1 putative inhibitors. Common structural features of several experimentally known Plk1 ligands were first identified. The findings were used to design small molecules that specifically bonded Plk1. Drug likeness and possible toxicities of the molecules were investigated. Molecules with no implied toxicities and optimal drug likeness values were used for docking studies. Several molecules were identified that made stable complexes only with Plk1 and LYN kinases, but not with other kinases. One molecule was found to bind exclusively the PBD site of Plk1. Possible utilization of the designed molecules in drugs against cancers with over expressed Plk1 is discussed.},
 author = {Krupa S. Jani and D.S. Dalafave},
 doi = {10.4137/BBI.S8971},
 eprint = {https://doi-org.crai.referencistas.com/10.4137/BBI.S8971},
 journal = {Bioinformatics and Biology Insights},
 note = {PMID:22399850},
 number = { },
 pages = {BBI.S8971},
 title = {Computational Design of Targeted Inhibitors of Polo-Like Kinase 1 (Plk1)},
 url = {https://doi-org.crai.referencistas.com/10.4137/BBI.S8971},
 volume = {6},
 year = {2012m}
}

@article{doi:10.4137/BECB.S5594,
 abstract = {Molecular biology focuses on genes and their interactions at the transcription, regulation and protein level. Finding genes that cause certain behaviors can make therapeutic interventions more effective. Although biological tools can extract the genes and perform some analyses, without the help of computational methods, deep insight of the genetic function and its effects will not occur. On the other hand, complex systems can be modeled by networks, introducing the main data as nodes and the links in-between as the transactions occurring within the network. Gene regulatory networks are examples that are modeled and analyzed in order to gain insight of their exact functions. Since a cell’s specific functionality is greatly determined by the genes it expresses, translation or the act of converting mRNA to proteins is highly regulated by the control network that directs cellular activities. This paper briefly reviews the most important computational methods for analyzing, modeling and controlling the gene regulatory networks.},
 author = {Zahra Zamani and Amirhossein Hajihosseini and Ali Masoudi-Nejad},
 doi = {10.4137/BECB.S5594},
 eprint = {https://doi-org.crai.referencistas.com/10.4137/BECB.S5594},
 journal = {Biomedical Engineering and Computational Biology},
 number = { },
 pages = {BECB.S5594},
 title = {Computational Methodologies for Analyzing, Modeling and Controlling Gene Regulatory Networks},
 url = {https://doi-org.crai.referencistas.com/10.4137/BECB.S5594},
 volume = {2},
 year = {2010t}
}

@article{doi:10.4137/CIN.S13890,
 abstract = {Quality control and read preprocessing are critical steps in the analysis of data sets generated from high-throughput genomic screens. In the most extreme cases, improper preprocessing can negatively affect downstream analyses and may lead to incorrect biological conclusions. Here, we present PathoQC, a streamlined toolkit that seamlessly combines the benefits of several popular quality control software approaches for preprocessing next-generation sequencing data. PathoQC provides a variety of quality control options appropriate for most high-throughput sequencing applications. PathoQC is primarily developed as a module in the PathoScope software suite for metagenomic analysis. However, PathoQC is also available as an open-source Python module that can run as a stand-alone application or can be easily integrated into any bioinformatics workflow. PathoQC achieves high performance by supporting parallel computation and is an effective tool that removes technical sequencing artifacts and facilitates robust downstream analysis. The PathoQC software package is available at http://sourceforge.net/projects/PathoScope/.},
 author = {Changjin Hong and Solaiappan Manimaran and William Evan Johnson},
 doi = {10.4137/CIN.S13890},
 eprint = {https://doi-org.crai.referencistas.com/10.4137/CIN.S13890},
 journal = {Cancer Informatics},
 note = {PMID:25983538},
 number = { },
 pages = {CIN.S13890},
 title = {PathoQC: Computationally Efficient Read Preprocessing and Quality Control for High-Throughput Sequencing Data Sets},
 url = {https://doi-org.crai.referencistas.com/10.4137/CIN.S13890},
 volume = {13s1},
 year = {2014k}
}

@article{doi:10.4137/CIN.S30747,
 abstract = {The problem of publication bias has long been discussed in research fields such as medicine. There is a consensus that publication bias is a reality and that solutions should be found to reduce it. In methodological computational research, including cancer informatics, publication bias may also be at work. The publication of negative research findings is certainly also a relevant issue, but has attracted very little attention to date. The present paper aims at providing a new formal framework to describe the notion of publication bias in the context of methodological computational research, facilitate and stimulate discussions on this topic, and increase awareness in the scientific community. We report an exemplary pilot study that aims at gaining experiences with the collection and analysis of information on unpublished research efforts with respect to publication bias, and we outline the encountered problems. Based on these experiences, we try to formalize the notion of publication bias.},
 author = {Anne-Laure Boulesteix and Veronika Stierle and Alexander Hapfelmeier},
 doi = {10.4137/CIN.S30747},
 eprint = {https://doi-org.crai.referencistas.com/10.4137/CIN.S30747},
 journal = {Cancer Informatics},
 note = {PMID:26508827},
 number = { },
 pages = {CIN.S30747},
 title = {Publication Bias in Methodological Computational Research},
 url = {https://doi-org.crai.referencistas.com/10.4137/CIN.S30747},
 volume = {14s5},
 year = {2015b}
}

@article{doi:10.4137/CMC.S15710,
 abstract = {Shape change of the left atrium (LA) and LA appendage in atrial fibrillation (AF) patients is hypothesized to be linked to AF pathology and to play a role in thrombogenesis; however, many aspects of shape variation in the heart are poorly understood. To date, studies of the LA shape in AF have been limited to empirical observation and summary metrics, such as volume and its likeness to a sphere. This paper describes a more comprehensive approach to the study of the LA shape through the use of computationally derived statistical shape models. We describe practical approaches that we have developed to extract shape parameters automatically from the three-dimensional MR images of the patient. From these images and our techniques, we can produce a more comprehensive description of LA geometric variability than that has been previously possible. We present the methodology and results from two examples of specific analyses using shape models: (1) we describe statistically significant group differences between the normal control and AF patient populations (n = 137) and (2) we describe characteristic shapes of the LA appendage that are associated with the risk of thrombogenesis determined by transesophageal echocardiography (n = 203).},
 author = {Joshua Cates and Erik Bieging and Alan Morris and Gregory Gardner and Nazem Akoum and Eugene Kholmovski and Nassir Marrouche and Christopher McGann and Rob S. MacLeod},
 doi = {10.4137/CMC.S15710},
 eprint = {https://doi-org.crai.referencistas.com/10.4137/CMC.S15710},
 journal = {Clinical Medicine Insights: Cardiology},
 note = {PMID:26380559},
 number = { },
 pages = {CMC.S15710},
 title = {Computational Shape Models Characterize Shape Change of the Left Atrium in Atrial Fibrillation},
 url = {https://doi-org.crai.referencistas.com/10.4137/CMC.S15710},
 volume = {8s1},
 year = {2014f}
}

@article{doi:10.4137/CMC.S39708,
 abstract = {Image-based computational modeling is becoming an increasingly used clinical tool to provide insight into the mechanisms of reentrant arrhythmias. In the context of ischemic heart disease, faithful representation of the electrophysiological properties of the infarct region within models is essential, due to the scars known for arrhythmic properties. Here, we review the different computational representations of the infarcted region, summarizing the experimental measurements upon which they are based. We then focus on the two most common representations of the scar core (complete insulator or electrically passive tissue) and perform simulations of electrical propagation around idealized infarct geometries. Our simulations highlight significant differences in action potential duration and focal effective refractory period (ERP) around the scar, driven by differences in electrotonic loading, depending on the choice of scar representation. Finally, a novel mechanism for arrhythmia induction, following a focal ectopic beat, is demonstrated, which relies on localized gradients in ERP directly caused by the electrotonic sink effects of the neighboring passive scar.},
 author = {Adam J. Connolly and Martin J. Bishop},
 doi = {10.4137/CMC.S39708},
 eprint = {https://doi-org.crai.referencistas.com/10.4137/CMC.S39708},
 journal = {Clinical Medicine Insights: Cardiology},
 note = {PMID:27486348},
 number = { },
 pages = {CMC.S39708},
 title = {Computational Representations of Myocardial Infarct Scars and Implications for Arrhythmogenesis},
 url = {https://doi-org.crai.referencistas.com/10.4137/CMC.S39708},
 volume = {10s1},
 year = {2016f}
}

@article{doi:10.4137/EBO.S2765,
 abstract = {Coalescent-based Bayesian Markov chain Monte Carlo (MCMC) inference generates estimates of evolutionary parameters and their posterior probability distributions. As the number of sequences increases, the length of time taken to complete an MCMC analysis increases as well. Here, we investigate an approach to distribute the MCMC analysis across a cluster of computers. To do this, we use bootstrapped topologies as fixed genealogies, perform a single MCMC analysis on each genealogy without topological rearrangements, and pool the results across all MCMC analyses. We show, through simulations, that although the standard MCMC performs better than the bootstrap-MCMC at estimating the effective population size (scaled by mutation rate), the bootstrap-MCMC returns better estimates of growth rates. Additionally, we find that our bootstrap-MCMC analyses are, on average, 37 times faster for equivalent effective sample sizes.},
 author = {Allen G. Rodrigo and Peter Tsai and Helen Shearman},
 doi = {10.4137/EBO.S2765},
 eprint = {https://doi-org.crai.referencistas.com/10.4137/EBO.S2765},
 journal = {Evolutionary Bioinformatics},
 note = {PMID:19812730},
 number = { },
 pages = {EBO.S2765},
 title = {On the Use of Bootstrapped Topologies in Coalescent-Based Bayesian MCMC Inference: A Comparison of Estimation and Computational Efficiencies},
 url = {https://doi-org.crai.referencistas.com/10.4137/EBO.S2765},
 volume = {5},
 year = {2009o}
}

@article{doi:10.4137/EBO.S419,
 abstract = {The subtree prune and regraft distance (dSPR) between phylogenetic trees is important both as a general means of comparing phylogenetic tree topologies as well as a measure of lateral gene transfer (LGT). Although there has been extensive study on the computation of dSPR and similar metrics between rooted trees, much less is known about SPR distances for unrooted trees, which often arise in practice when the root is unresolved. We show that unrooted SPR distance computation is NP-Hard and verify which techniques from related work can and cannot be applied. We then present an efficient heuristic algorithm for this problem and benchmark it on a variety of synthetic datasets. Our algorithm computes the exact SPR distance between unrooted tree, and the heuristic element is only with respect to the algorithm’s computation time. Our method is a heuristic version of a fixed parameter tractability (FPT) approach and our experiments indicate that the running time behaves similar to FPT algorithms. For real data sets, our algorithm was able to quickly compute dSPR for the majority of trees that were part of a study of LGT in 144 prokaryotic genomes. Our analysis of its performance, especially with respect to searching and reduction rules, is applicable to computing many related distance measures.},
 author = {Glenn Hickey and Frank Dehne and Andrew Rau-Chaplin and Christian Blouin},
 doi = {10.4137/EBO.S419},
 eprint = {https://doi-org.crai.referencistas.com/10.4137/EBO.S419},
 journal = {Evolutionary Bioinformatics},
 note = {PMID:19204804},
 number = { },
 pages = {EBO.S419},
 title = {SPR Distance Computation for Unrooted Trees},
 url = {https://doi-org.crai.referencistas.com/10.4137/EBO.S419},
 volume = {4},
 year = {2008g}
}

@article{doi:10.4256/mio.2012.003,
 abstract = {During the last few years a growing amount of content produced by Internet users has become publicly available online. These data come from a variety of places, including popular social web services like Facebook and Twitter, consumer services like Amazon or weblogs. The research opportunities opened up by this socio-technological innovation are, as shown by the growing literature on the topic, huge. At the same time new challenges for social scientists arise. In this paper we will focus on two of the main challenges posed to the growth of the so-called computational social science: interdisciplinarity and ethics. While the searchability and persistence of this information make it ideal for sociological research, a quantitative approach is still challenging because of the size and complexity of the data. Collecting, storing and analyzing these data often require technical skills beyond the traditional curricula of social scientists. These projects require, in fact, collaboration with computer scientists. Nevertheless developing a common interdisciplinary project is often challenging because of the different backgrounds of the researchers. At the same time the availability of this content poses a challenge concerning privacy and research ethics. Due to the amount of data and the fact that the real identity of the author is often hidden behind a nickname, it is often impossible to ask the subject involved to consent to the use of their data. On the other hand, especially in the first wave of web 2.0, this information has been – intentionally or not – publicly shared by the users. While a technique of dis-embedding the identity of the user from the content analyzed is often the solution used to bypass this issue, an even more important privacy-related challenge for computational social science is emerging. Due to the wide adoption of social network sites such Facebook or Google+, where a user may decide to share his content with his/her group of friends only, the amount of public data will change and decrease in the future. We will discuss this issue by enumerating a number of possible future scenarios.},
 author = {Fabio Giglietto and Luca Rossi},
 doi = {10.4256/mio.2012.003},
 eprint = {https://doi-org.crai.referencistas.com/10.4256/mio.2012.003},
 journal = {Methodological Innovations Online},
 number = {1},
 pages = {25–36},
 title = {Ethics and Interdisciplinarity in Computational Social Science},
 url = {https://doi-org.crai.referencistas.com/10.4256/mio.2012.003},
 volume = {7},
 year = {2012h}
}

@article{doi:10.5301/ijao.5000482,
 abstract = {Aim In current rotary blood pumps, complications related to blood trauma due to shear stresses are still frequently observed clinically. Reducing the rotor tip speed might decrease blood trauma. Therefore, the aim of this project was to design a two-stage rotary blood pump leading to lower shear stresses. Methods Using the principles of centrifugal pumps, two diagonal rotor stages were designed with an outer diameter of 22 mm. The first stage begins with a flow straightener and terminates with a diffusor, while a volute casing behind the second stage is utilized to guide fluid to the outlet. Both stages are combined into one rotating part which is pivoted by cup-socket ruby bearings. Details of the flow field were analyzed employing computational fluid dynamics (CFD). A functional model of the pump was fabricated and the pressure-flow dependency was experimentally assessed. Results Measured pressure-flow performance of the developed pump indicated its ability to generate adequate pressure heads and flows with characteristic curves similar to centrifugal pumps. According to the CFD results, a pressure of 70 mmHg was produced at a flow rate of 5 L/min and a rotational speed of 3200 rpm. Circumferential velocities could be reduced to 3.7 m/s as compared to 6.2 m/s in a clinically used axial rotary blood pump. Flow fields were smooth with well-distributed pressure fields and comparatively few recirculation or vortices. Substantially smaller volumes were exposed to high shear stresses >150 Pa. Conclusions Hence, blood trauma might be reduced with this design. Based on these encouraging results, future in vitro investigations to investigate actual blood damage are intended.},
 author = {Bente Thamsen and Ricardo Mevert and Michael Lommel and Philip Preikschat and Julia Gaebler and Thomas Krabatsch and Ulrich Kertzscher and Ewald Hennig and Klaus Affeld},
 doi = {10.5301/ijao.5000482},
 eprint = {https://doi-org.crai.referencistas.com/10.5301/ijao.5000482},
 journal = {The International Journal of Artificial Organs},
 note = {PMID:27034319},
 number = {4},
 pages = {178–183},
 title = {A Two-stage Rotary Blood Pump Design with Potentially Lower Blood Trauma: A Computational Study},
 url = {https://doi-org.crai.referencistas.com/10.5301/ijao.5000482},
 volume = {39},
 year = {2016s}
}

@article{doi:10.5301/JVA.2011.8440,
 abstract = {Purpose A-v anastomosis entails dramatic changes in hemodynamic conditions, which may lead to major alterations to the vessels involved; primarily dilatations and devastating stenoses. Wall shear stress is thought to play a key role in the remodeling of the vessels exposed to abnormal levels and oscillating wall shear stress. In this study we sought to develop a framework suitable for thorough in vivo analyses of wall shear stress and vessel morphology of a-v fistulas in patients. Methods Using ultrasound and Magnetic Resonance Imaging (MRI) transverse image stacks from six patient a-v fistulas were obtained. From the image stacks three-dimensional geometries of the patient fistulas were created using dedicated segmentation software. Geometries of three a-v fistulas were imported into Finite Element software in order to perform fluid flow simulations of blood flows and frictional forces on the vessel walls in the a-v fistulas. Boundary conditions for the simulations were obtained using both a MRI Phase Contrast and an Ultrasound Doppler technique. Results The segmentation of the six fistulas of very different age and morphology (two end-to-side and four side-to-side) showed the ability of the approach to create geometries of various fistula morphologies. Simulations of the three fistulas showed an instant picture of the present status of the exposure to different levels of wall shear stress and the morphological status in the vessel remodeling process. Conclusion The study demonstrated the capability of the CFD framework to analyze patient a-v fistulas on a regular basis using both MRI and ultrasound-based approaches.},
 author = {Anders Koustrup Niemann and Samuel Thrysoe and Jens Vinge Nygaard and John Michael Hasenkam and Steffen Ellebaek Petersen},
 doi = {10.5301/JVA.2011.8440},
 eprint = {https://doi-org.crai.referencistas.com/10.5301/JVA.2011.8440},
 journal = {The Journal of Vascular Access},
 note = {PMID:21725950},
 number = {1},
 pages = {36–44},
 title = {Computational Fluid Dynamics Simulation of a-v Fistulas: From MRI and Ultrasound Scans to Numeric Evaluation of Hemodynamics},
 url = {https://doi-org.crai.referencistas.com/10.5301/JVA.2011.8440},
 volume = {13},
 year = {2012n}
}

@article{doi:10.5301/jva.5000226,
 abstract = {Purpose Stenosis in a vascular access circuit is the predominant cause of access dysfunction. Hemodynamic significance of a stenosis identified by angiography in an access circuit is uncertain. This study utilizes computational fluid dynamics (CFD) to model flow through arteriovenous fistula to predict the functional significance of stenosis in vascular access circuits. Methods Three-dimensional models of fistulas were created with a range of clinically relevant stenoses using SolidWorks. Stenoses diameters ranged from 1.0 to 3.0 mm and lengths from 5 to 60 mm within a fistula diameter of 7 mm. CFD analyses were performed using a blood model over a range of blood pressures. Eight patient-specific stenoses were also modeled and analyzed with CFD and the resulting blood flow calculations were validated by comparison with brachial artery flow measured by duplex ultrasound. Results Predicted flow rates were derived from CFD analysis of a range of stenoses. These stenoses were modeled by CFD and correlated with the ultrasound measured flow rate through the fistula of eight patients. The calculated flow rate using CFD correlated within 20% of ultrasound measured flow for five of eight patients. The mean difference was 17.2% (ranged from 1.3% to 30.1%). Conclusions CFD analysis-generated flow rate tables provide valuable information to assess the functional significance of stenosis detected during imaging studies. The CFD study can help in determining the clinical relevance of a stenosis in access dysfunction and guide the need for intervention.},
 author = {David M. Hoganson and Cameron J. Hinkel and Xiaomin Chen and Ramesh K. Agarwal and Surendra Shenoy},
 doi = {10.5301/jva.5000226},
 eprint = {https://doi-org.crai.referencistas.com/10.5301/jva.5000226},
 journal = {The Journal of Vascular Access},
 note = {PMID:24811588},
 number = {5},
 pages = {409–414},
 title = {Validation of Computational Fluid Dynamics-Based Analysis to Evaluate                     Hemodynamic Significance of access Stenosis},
 url = {https://doi-org.crai.referencistas.com/10.5301/jva.5000226},
 volume = {15},
 year = {2014j}
}

@article{doi:10.5301/jva.5000241,
 abstract = {Background The creation and management of an autologous arteriovenous fistula (AVF) as vascular access (VA) for hemodialysis patients is still a critical procedure. The placement of a functional and long-lasting VA derives from adequate planning of the surgical procedure based on physical examination, vascular mapping and selection of the best modality for arteriovenous anastomosis. The risk of AVF non-maturation and early failure is high, even when all precautions are taken to minimize these events. In addition, AVF surgery may develop very high blood flow exposing the patient to the risk of heart failure or hand ischemia. Methods The choices of the surgeons on the modalities to perform a surgical intervention for AVF should take into consideration several factors including patient clinical condition, arterial and venous vessel sizes and elasticity. However, these evaluations cannot give direct indication on VA outcome in terms of blood flow after AVF maturation. We then took advantage of theoretical models of vascular network hemodynamics and of computational fluid dynamics to develop a numerical tool for the prediction of potential blood flow of a planned VA surgery on the basis of preoperative ultrasound evaluation of arterial and venous sizes and blood flow. Results Here we present the numerical model, previously developed and tested, and we describe the web-based application that has been developed to help during surgical planning. Conclusions The use of this tool in the clinical setting should allow to reduce the incidence of AVF non-maturation as well as incidence of VA complications.},
 author = {Andrea Remuzzi and Simone Manini},
 doi = {10.5301/jva.5000241},
 eprint = {https://doi-org.crai.referencistas.com/10.5301/jva.5000241},
 journal = {The Journal of Vascular Access},
 number = {7_suppl},
 pages = {64–69},
 title = {Computational Model for Prediction of Fistula Outcome},
 url = {https://doi-org.crai.referencistas.com/10.5301/jva.5000241},
 volume = {15},
 year = {2014p}
}

@article{doi:10.5301/jva.5000607,
 abstract = {Introduction A radiocephalic arteriovenous fistula (AVF) is the best choice for achieving vascular access (VA) for hemodialysis, but this AVF has high rates of early failure due to juxta-anastomotic stenosis, making it impossible to use for dialysis. Low hemodynamic shear stress contributes to the pathophysiology of VA failure due to secondary thrombosis, stenosis, and re-occlusion after percutaneous intervention. Methods We used a computational fluid dynamics (CFDs) approach to evaluate the shear stress distribution and minimize its effects under various conditions including changes in the anastomosis angle. A three-dimensional computational domain was designed for arteriovenous end-to-side anastomosis based on anastomosis angles of 45°, 90° and including 135° angle of an obtuse anastomosis using three-dimensional design software. COMSOL Multiphysics® simulation software was used to identify the hemodynamic factors influencing wall shear stress at the anastomosis site using a low Reynolds number k-ε turbulence model that included non-Newtonian blood flow characteristics, the complete cardiac pulse cycle, and distention of blood vessels. In preliminary clinical study, all 201 patients who received a radiocephalic wrist AVF from January 2009 to February 2014 were divided into classic and obtuse angle groups. Results The CFD results showed that the largest anastomosis angle (135°) resulted in lower shear stress, which would help reduce AVF failures. This obtuse angle was preferred, as it minimized the development of anastomotic stenosis and tended to favor primary and primary-assisted patency in clinical study. Conclusions An obtuse radiocephalic wrist AVF shows more favorable patency compared to a classic radiocephalic AVF. Surgeons establishing a radiocephalic wrist AVF would be better to consider an AVF with an obtuse anastomosis.},
 author = {Jinkee Lee and Sunho Kim and Sung-Min Kim and Ryungeun Song and Hyun Kyu Kim and Jang Sang Park and Sun Cheol Park},
 doi = {10.5301/jva.5000607},
 eprint = {https://doi-org.crai.referencistas.com/10.5301/jva.5000607},
 journal = {The Journal of Vascular Access},
 note = {PMID:27791257},
 number = {6},
 pages = {512–520},
 title = {Assessing Radiocephalic Wrist Arteriovenous Fistulas of Obtuse Anastomosis using Computational Fluid Dynamics and Clinical Application},
 url = {https://doi-org.crai.referencistas.com/10.5301/jva.5000607},
 volume = {17},
 year = {2016o}
}

@article{doi:10.5547/ISSN0195-6574-EJ-Vol32-No1-3,
 abstract = {This paper evaluates with numerical computations the respective merits of two competing notions of coalition stability in the standard global public goods model of climate change. To this effect it uses the CWS integrated assessment model. After a reminder of the two game theoretical stability notions involved—core-stability and internal-external stability—and of the CWS model, the former property is shown to hold for the grand coalition if resource transfers of a specific form between countries are introduced. The latter property appears to hold neither for the grand coalition nor for most large coalitions whereas it is verified for most small coalitions in a weak sense that involves transfers. Finally, coalitions, stable in either sense, that perform best in terms of carbon concentration and global welfare are always heterogeneous ones. Therefore, if coalitional stability is taken as an objective, promoting small or homogeneous coalitions is not to be recommended.},
 author = {Thierry Bréchet and François Gerard and Henry Tulkens},
 doi = {10.5547/ISSN0195-6574-EJ-Vol32-No1-3},
 eprint = {https://doi-org.crai.referencistas.com/10.5547/ISSN0195-6574-EJ-Vol32-No1-3},
 journal = {The Energy Journal},
 number = {1},
 pages = {49–76},
 title = {Efficiency vs. Stability in Climate Coalitions: A Conceptual and Computational Appraisal},
 url = {https://doi-org.crai.referencistas.com/10.5547/ISSN0195-6574-EJ-Vol32-No1-3},
 volume = {32},
 year = {2011b}
}

@article{doi:10.5547/ISSN0195-6574-EJ-Vol32-No2-2,
 abstract = {The idea of rearranging generation assets amongst firms to improve competition has once again surfaced in a recent report on improvements to the New Zealand Electricity Market. We present counterexamples to show that rearranging assets, either with asset divestiture to a new firm, or asset swaps between existing firms, may actually reduce competition in electricity markets. Our examples emphasize features that are particular to electricity, such as seasonality and transmission constraints. These results warn that applying economic rules of thumb to electricity markets may lead to erroneous conclusions.},
 author = {Anthony Downward and David Young and Golbon Zakeri},
 doi = {10.5547/ISSN0195-6574-EJ-Vol32-No2-2},
 eprint = {https://doi-org.crai.referencistas.com/10.5547/ISSN0195-6574-EJ-Vol32-No2-2},
 journal = {The Energy Journal},
 number = {2},
 pages = {31–58},
 title = {Swapping Generators’ Assets: Market Salvation or Wishful Thinking?},
 url = {https://doi-org.crai.referencistas.com/10.5547/ISSN0195-6574-EJ-Vol32-No2-2},
 volume = {32},
 year = {2011d}
}

@article{doi:10.5772/50932,
 abstract = {Conflict management is one of the most important issues in leveraging organizational competitiveness. However, traditional social scientists built theories or models in this area which were mostly expressed in words and diagrams are insufficient. Social science research based on computational modeling and simulation is beginning to augment traditional theory building. Simulation provides a method for people to try their actions out in a way that is cost effective, faster, appropriate, flexible, and ethical. In this paper, a computational simulation model for conflict management in team building is presented. The model is designed and used to explore the individual performances related to the combination of individuals who have a range of conflict handling styles, under various types of resources and policies. The model is developed based on agent-based modeling method. Each of the agents has one of the five conflict handling styles: accommodation, compromise, competition, contingency, and learning. There are three types of scenarios: normal, convex, and concave. There are two types of policies: no policy, and a reward and punishment policy. Results from running the model are also presented. The simulation has led us to derive two implications concerning conflict management. First, a concave type of resource promotes competition, while convex type of resource promotes compromise and collaboration. Second, the performance ranking of different styles can be influenced by introducing different policies. On the other hand, it is possible for us to promote certain style by introducing different policies.},
 author = {W. M. Wang and S. L. Ting},
 doi = {10.5772/50932},
 eprint = {https://doi-org.crai.referencistas.com/10.5772/50932},
 journal = {International Journal of Engineering Business Management},
 number = { },
 pages = {14},
 title = {Development of a Computational Simulation Model for Conflict Management in Team Building},
 url = {https://doi-org.crai.referencistas.com/10.5772/50932},
 volume = {3},
 year = {2011r}
}

@article{doi:10.5772/56760,
 abstract = {A method for computing the distance between two moving robots or between a mobile robot and a dynamic obstacle with linear or arc-like motions and with constant accelerations is presented in this paper. This distance is obtained without stepping or discretizing the motions of the robots or obstacles. The robots and obstacles are modelled by convex hulls. This technique obtains the future instant in time when two moving objects will be at their minimum translational distance - i.e., at their minimum separation or maximum penetration (if they will collide). This distance and the future instant in time are computed in parallel. This method is intended to be run each time new information from the world is received and, consequently, it can be used for generating collision-free trajectories for non-holonomic mobile robots.},
 author = {Enrique J. Bernabeu and Angel Valera and Javier Gomez-Moreno},
 doi = {10.5772/56760},
 eprint = {https://doi-org.crai.referencistas.com/10.5772/56760},
 journal = {International Journal of Advanced Robotic Systems},
 number = {9},
 pages = {329},
 title = {Distance Computation Between Non-Holonomic Motions with Constant Accelerations},
 url = {https://doi-org.crai.referencistas.com/10.5772/56760},
 volume = {10},
 year = {2013b}
}

@article{doi:10.5772/58249,
 abstract = {Our goal in this article is to reflect on the role LEGO robotics has played in college engineering education over the last 15 years, starting with the introduction of the RCX in 1998 and ending with the introduction of the EV3 in 2013. By combining a modular computer programming language with a modular building platform, LEGO Education has allowed students (of all ages) to become active leaders in their own education as they build everything from animals for a robotic zoo to robots that play children’s games. Most importantly, it allows all students to develop different solutions to the same problem to provide a learning community. We look first at how the recent developments in the learning sciences can help in promoting student learning in robotics. We then share four case studies of successful college-level implementations that build on these developments.},
 author = {Ethan Danahy and Eric Wang and Jay Brockman and Adam Carberry and Ben Shapiro and Chris B. Rogers},
 doi = {10.5772/58249},
 eprint = {https://doi-org.crai.referencistas.com/10.5772/58249},
 journal = {International Journal of Advanced Robotic Systems},
 number = {2},
 pages = {27},
 title = {LEGO-based Robotics in Higher Education: 15 Years of Student Creativity},
 url = {https://doi-org.crai.referencistas.com/10.5772/58249},
 volume = {11},
 year = {2014c}
}

@article{doi:10.5964/ps.6115,
 abstract = {Computational methods have increased the objectivity of measures of human behavior and positioned personality science to benefit from the ongoing digital revolution. In this review, we define and discuss computational personality assessment (CPA), a measurement process that uses computational technologies to obtain estimates of personality. We briefly review some of the most promising sources of data currently used for CPA: mobile sensing, digital footprints from social media, images, language, and experience sampling. We present a concise overview of key findings, discuss the promise and opportunities of CPA (e.g., moving towards objective measures of personality, obtaining new insights from big data), and highlight important limitations and challenges in the development and application of CPA (e.g., establishing reliability and validity, selecting appropriate ground truth criterion, assessing affect and cognition, implications for ethics and privacy). We conclude with our perspective on how CPA could change our understanding of individual differences.},
 author = {Clemens Stachl and Ryan L. Boyd and Kai T. Horstmann and Poruz Khambatta and Sandra C. Matz and Gabriella M. Harari},
 doi = {10.5964/ps.6115},
 eprint = {https://doi-org.crai.referencistas.com/10.5964/ps.6115},
 journal = {Personality Science},
 number = {1},
 pages = {e6115},
 title = {Computational Personality Assessment},
 url = {https://doi-org.crai.referencistas.com/10.5964/ps.6115},
 volume = {2},
 year = {2021s}
}

@article{doi:10.7227/IJMEE.0013,
 abstract = {The use of computational tools such as Mathcad or Microsoft Excel to enhance the students’ understanding of the physics behind psychrometric processes in buildings in an air conditioning technical elective course is explored in this paper. By utilizing Mathcad and/or Excel, students can be taught to create a set of psychrometric-related functions that can greatly diminish the number of calculations required to tackle design-oriented problems. By doing this, two levels of conceptual understanding are enhanced. First, students gain an appreciation of the basic psychrometric processes for buildings when creating specific functions in Mathcad/Excel, and second, students gain the ability to create automated worksheets for specific applications, allowing them to see immediate results to variations in input design conditions, as well as on different parameters. Feedback from students, as well as class instructors, demonstrates that the use of these tools enhance the student experience in an air conditioning class and make the course more dynamic and engaging.},
 author = {Pedro J. Mago and Rogelio Luck},
 doi = {10.7227/IJMEE.0013},
 eprint = {https://doi-org.crai.referencistas.com/10.7227/IJMEE.0013},
 journal = {International Journal of Mechanical Engineering Education},
 number = {3},
 pages = {251–266},
 title = {Use of Computational Tools to Enhance the Study of Psychrometric Processes for Buildings in an Air Conditioning Course},
 url = {https://doi-org.crai.referencistas.com/10.7227/IJMEE.0013},
 volume = {42},
 year = {2014e}
}

@article{doi:10.7227/IJMEE.32.1.4,
 abstract = {The use of spreadsheet programs in teaching the numerical aspects of computational fluid dynamics is explored. Spreadsheets allow the student to investigate important features of numerical algorithms without having to invest the time required to code in a traditional programming language. Thus they represent a short cut to the experiential part of the learning cycle. Four examples are presented with detailed methodology and learning outcomes, including solutions to the basic partial differential equations for fluid flow and iterative techniques for matrix inversion.},
 author = {G. Tabor},
 doi = {10.7227/IJMEE.32.1.4},
 eprint = {https://doi-org.crai.referencistas.com/10.7227/IJMEE.32.1.4},
 journal = {International Journal of Mechanical Engineering Education},
 number = {1},
 pages = {31–53},
 title = {Teaching Computational Fluid Dynamics Using Spreadsheets},
 url = {https://doi-org.crai.referencistas.com/10.7227/IJMEE.32.1.4},
 volume = {32},
 year = {2004m}
}

@article{doi:10.7227/IJMEE.33.3.4,
 abstract = {A computational implementation of modal analysis of continuous systems is presented. Modal analysis of truss, beam and shaft structures is developed in Matlab®. A numerical and an analytical method are developed for the computation of mode shapes, natural frequencies and modal equations of continuous structures using the method of separation of variables. Time domain techniques are programmed in modular functions for structural analysis. The functions constitute a set of tools of the Structural Analysis Toolbox (SAT-Lab), developed recently for teaching modelling, analysis and design of structures and mechanical systems. Application examples of the computational tools are presented.},
 author = {Ariel E. Matusevich and José A. Inaudi},
 doi = {10.7227/IJMEE.33.3.4},
 eprint = {https://doi-org.crai.referencistas.com/10.7227/IJMEE.33.3.4},
 journal = {International Journal of Mechanical Engineering Education},
 number = {3},
 pages = {215–234},
 title = {A Computational Implementation of Modal Analysis of Continuous Dynamic Systems},
 url = {https://doi-org.crai.referencistas.com/10.7227/IJMEE.33.3.4},
 volume = {33},
 year = {2005h}
}

@article{doi:10.7227/IJMEE.41.3.5,
 abstract = {This paper describes a module for the teaching of diffusion to mechanical engineering students that can be part of a course on numerical methods or advanced engineering mathematics and as a precursor to a heat transfer course. The purpose of the module is to pull together many of the mathematical concepts in the early education of mechanical engineering students while at the same time introducing them to other important topics – an application of finite differences to the numerical approximation of partial differential equations, the importance of controlling round-off error in computational algorithms, and the time complexity of some numerical models. Students learn these topics through the execution of computer experiments. The first set of experiments cover the numerical approximation of heat conduction in a ring, and a second set cover the numerical approximation of the reaction and diffusion of chemicals that will produce the markings on an animal’s tail. The learning outcomes of the module are evaluated with data from pre- and post-module surveys given to mechanical engineering students in several sections of a required course in numerical methods.},
 author = {Kathie A. Yerion},
 doi = {10.7227/IJMEE.41.3.5},
 eprint = {https://doi-org.crai.referencistas.com/10.7227/IJMEE.41.3.5},
 journal = {International Journal of Mechanical Engineering Education},
 number = {3},
 pages = {227–245},
 title = {Computer Experiments with Diffusion: Finite Differences, round-off Error and Animal Stripes?},
 url = {https://doi-org.crai.referencistas.com/10.7227/IJMEE.41.3.5},
 volume = {41},
 year = {2013s}
}
