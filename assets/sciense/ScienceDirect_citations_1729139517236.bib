@article{TRICHKOVAKASHAMOVA2024123,
title = {Criteria and Approaches for Optimization of Innovative Methods for STEM Education},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {3},
pages = {123-128},
year = {2024},
note = {22nd IFAC Conference on Technology, Culture and International Stability TECIS 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.07.137},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324002209},
author = {E. Trichkova-Kashamova and E. Paunova-Hubenova and Y. Boneva and S. Dimitrov},
keywords = {STEM education, innovative educational methods, optimisation, data analyses, technology-based learning},
abstract = {The proposed research aims to evaluate the modern learning process in STEM subjects in a technology-rich environment. The study examines contemporary teaching methods and evaluates their application in different educational levels in Bulgaria. The aim is to provide information for developing a concept of a modern technology-based learning process and integrating innovative methods with appropriate technological tools.}
}
@article{GAESSER2020104325,
title = {Episodic mindreading: Mentalizing guided by scene construction of imagined and remembered events},
journal = {Cognition},
volume = {203},
pages = {104325},
year = {2020},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2020.104325},
url = {https://www.sciencedirect.com/science/article/pii/S001002772030144X},
author = {Brendan Gaesser},
keywords = {Episodic simulation, Memory, Scene construction, Mentalizing, Theory of mind, Perspective taking, Social cognition, Morality},
abstract = {Attributing mental states to other people fundamentally shapes how we bond, coordinate, and predict the actions of others. Perceiving a person's facial expressions and body language in the present contribute to our ability to understand what they are thinking and feeling. Yet, people do not exist in a vacuum and individuals often think about people who are not directly in front of them. People inhabit remembered and imagined episodes, where the surrounding location and objects can guide attributions of their mental states. In this article, I propose the episodic mindreading hypothesis, arguing that the episodic representation of past and future events in which a target person is embedded will affect whether and how the target's mind is read. The content and phenomenological quality of imagined and remembered episodes can alter what mental states are attributed to a target and the accessibility of those mental states. This hypothesis encourages researchers to think about mentalizing as neither dependent on nor completely exclusive from the episodic memory system. Instead, the episodic memory system can modulate and inform mindreading, and likely vice versa. The article reviews extant knowledge and highlights open questions for future research to explore with implications for healthy and impaired social cognition.}
}
@article{LIU2018772,
title = {Ecosystem services in life cycle assessment - Part 2: Adaptations to regional and serviceshed information},
journal = {Journal of Cleaner Production},
volume = {197},
pages = {772-780},
year = {2018},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2018.05.283},
url = {https://www.sciencedirect.com/science/article/pii/S0959652618316421},
author = {Xinyu Liu and Guy Ziv and Bhavik R. Bakshi},
keywords = {Computational structure, Life cycle assessment, Ecosystem service, Environmental sustainability},
abstract = {Regionalized life cycle assessment (LCA) is receiving more attention among LCA practitioners due to spatial variation in process efficiency of technological systems and current status of ecological systems. However, the role of ecosystem services (ES) in supporting technological activities is still ignored. Techno-Ecological Synergies in Life Cycle Assessment (TES-LCA) is a methodology that captures the interactions between and within technological and ecological systems, along a product's life cycle. It accounts for local and absolute environmental sustainability by comparing the demand and supply of ES at multiple spatial scales. To facilitate its wider adoption, the basic computational framework has been proposed in Part 1, which includes technologies and ecosystems in an integrated manner. To handle the complications induced by explicitly considering ES, which operate within servicesheds at multiple spatial scales, the TES-LCA computational structure is modified to account for spatial variation in technological and ecological systems in Part 2. The regionalized TES-LCA framework is demonstrated through an expanded case study to show its capability to capture different scenarios of regionalization, including variation in process efficiency, ecological carrying capacity (CC), characterization factors (CF), and the scales at which ES operate. The approach is then proved to be general and able to subsume existing approaches, such as regionalized LCA, GIS-LCA and recent extensions based on normalizing CF by ecological CC to calculate absolute sustainability metrics. It is recommended that the developed computational structure should be implemented in LCA software with the functionality for handling geographical information. Also, the regionalized information about ES demand and supply needs to be made available in future versions of life cycle inventory databases.}
}
@article{FAVELA2019156,
title = {Editor’s introduction: Innovative dynamical approaches to cognitive systems},
journal = {Cognitive Systems Research},
volume = {58},
pages = {156-159},
year = {2019},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2019.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041719303389},
author = {Luis H. Favela},
keywords = {Cognition, Cognitive systems, Dynamical systems theory},
abstract = {This Editor’s Introduction to the Cognitive Systems Research special issue, “Innovative Dynamical Approaches to Cognitive Systems,” has three aims: First, the background and motivation for the topic are stated. Second, overviews of the contributing papers are presented. Third, based on the papers, speculations on future directions in dynamical approaches to the investigation of cognitive systems are presented. Here, the focus is on concepts, data analysis methods, and computational modeling.}
}
@article{RICHARDS2013113,
title = {Bayesian belief modeling of climate change impacts for informing regional adaptation options},
journal = {Environmental Modelling & Software},
volume = {44},
pages = {113-121},
year = {2013},
note = {Thematic Issue on Innovative Approaches to Global Change Modelling},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2012.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S136481521200206X},
author = {R. Richards and M. Sanó and A. Roiko and R.W. Carter and M. Bussey and J. Matthews and T.F. Smith},
keywords = {Bayesian Belief Networks, Climate change, Adaptation, Group-model building, Stakeholder beliefs},
abstract = {A sequential approach to combining two established modeling techniques (systems thinking and Bayesian Belief Networks; BBNs) was developed and applied to climate change adaptation research within the South East Queensland Climate Adaptation Research Initiative (SEQ-CARI). Six participatory workshops involving 66 stakeholders based within SEQ produced six system conceptualizations and 22 alpha-level BBNs. The outcomes of the initial systems modeling exercise successfully allowed the selection of critical determinants of key response variables for in depth analysis within more homogeneous, sector-based groups of participants. Using two cases, this article focuses on the processes and methodological issues relating to the use of the BBN modeling technique when the data are based on expert opinion. The study expected to find both generic and specific determinants of adaptive capacity based on the perceptions of the stakeholders involved. While generic determinants were found (e.g. funding and awareness levels), sensitivity analysis identified the importance of pragmatic, context-based determinants, which also had methodological implications. The article raises questions about the most appropriate scale at which the methodology applied can be used to identify useful generic determinants of adaptive capacity when, at the scale used, the most useful determinants were sector-specific. Comparisons between individual BBN conditional probabilities identified diverging and converging beliefs, and that the sensitivity of response variables to direct descendant nodes was not always perceived consistently. It was often the accompanying narrative that provided important contextual information that explained observed differences, highlighting the benefits of using critical narrative with modeling tools.}
}
@article{ZHANG2022846,
title = {A novel resilience modeling method for community system considering natural gas leakage evolution},
journal = {Process Safety and Environmental Protection},
volume = {168},
pages = {846-857},
year = {2022},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2022.10.030},
url = {https://www.sciencedirect.com/science/article/pii/S0957582022008953},
author = {Xinqi Zhang and Guoming Chen and Dongdong Yang and Rui He and Jingyu Zhu and Shengyu Jiang and Jiawei Huang},
keywords = {Resilience modeling, Natural gas pipeline leakage, Community system, Computational Fluid Dynamics (CFD), Dynamic Bayesian network (DBN)},
abstract = {With rising natural gas demand, the issue of emergency management of gas leaks in communities is becoming more prominent. Resilience engineering is a successful approach to improving the system's ability in dealing with emergencies. In contrast to natural disasters such as floods and tornadoes, the consequences of a gas leak are closely related to the accident's evolutionary path. However, few reported works have taken the evolution of disasters into account in the assessment of resilience. Conventional methods fail to quantify the public safety performance of a disturbance caused by a natural gas leak. This work presents a novel dynamic approach to assessing resilience that incorporates the evolution of an incident and its interaction with emergency measures. A network structuring model of accident evolution is developed by the Functional Resonance Analysis Method (FRAM) to analyze the potential accident propagation. The explosion consequence of gas leakage escalation is simulated based on Computational Fluid Dynamics (CFD), and the personnel injury criterion is adopted to quantify the temporal and spatial variation characteristics of the system degradation. Then, the dynamic Bayesian network (DBN) is then used to track the interactions between accidents and emergency measures. Taking a real accident case (Shiyan underground gas explosion) as an example, the case study indicates that the proposed method can identify and prioritize emergency measures, as well as provide strong support for decision-making and arrangements in emergency management.}
}
@article{ANDERSON2024108366,
title = {Trichotomy revisited: A monolithic theory of attentional control},
journal = {Vision Research},
volume = {217},
pages = {108366},
year = {2024},
issn = {0042-6989},
doi = {https://doi.org/10.1016/j.visres.2024.108366},
url = {https://www.sciencedirect.com/science/article/pii/S0042698924000105},
author = {Brian A. Anderson},
keywords = {Attentional control, Visual attention, Selection history, Memory, Learning},
abstract = {The control of attention was long held to reflect the influence of two competing mechanisms of assigning priority, one goal-directed and the other stimulus-driven. Learning-dependent influences on the control of attention that could not be attributed to either of those two established mechanisms of control gave rise to the concept of selection history and a corresponding third mechanism of attentional control. The trichotomy framework that ensued has come to dominate theories of attentional control over the past decade, replacing the historical dichotomy. In this theoretical review, I readily affirm that distinctions between the influence of goals, salience, and selection history are substantive and meaningful, and that abandoning the dichotomy between goal-directed and stimulus-driven mechanisms of control was appropriate. I do, however, question whether a theoretical trichotomy is the right answer to the problem posed by selection history. If we reframe the influence of goals and selection history as different flavors of memory-dependent modulations of attentional priority and if we characterize the influence of salience as a consequence of insufficient competition from such memory-dependent sources of priority, it is possible to account for a wide range of attention-related phenomena with only one mechanism of control. The monolithic framework for the control of attention that I propose offers several concrete advantages over a trichotomy framework, which I explore here.}
}
@article{KABOSOVA2022109668,
title = {Shape optimization during design for improving outdoor wind comfort and solar radiation in cities},
journal = {Building and Environment},
volume = {226},
pages = {109668},
year = {2022},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2022.109668},
url = {https://www.sciencedirect.com/science/article/pii/S0360132322008988},
author = {Lenka Kabošová and Angelos Chronis and Theodoros Galanos and Stanislav Kmeť and Dušan Katunský},
keywords = {Parametric architecture, Real-time wind analysis, Real-time sun analysis, Performative design, InFraRed, CFD, Computational fluid dynamics},
abstract = {This paper delivers an idea of weather-based optimization as a sustainable design strategy addressing the changing climate. An environment-driven design technique is introduced and tested at the urban and architectural scale. Utilizing the interplay between the architectural intention and weather influences (specifically solar radiation and wind effects), the optimal design solution for the urban configuration and architectural shape emerges. An exploratory case study near the amphitheater in Kosice, Slovakia, demonstrates the proposed approach. Through the real-time iterative analysis of the environmental performance of multiple design variants, an urban concept of offices/apartment blocks, reacting to the local wind and sun situation, is formed. The wind flow situation and comfort are investigated in a design loop, blending the newly developed AI-driven simulation prediction models of InFraRed11InFraRed: AI-based real-time analysis of wind, sun, and thermal comfort in the streets developed by the CIL of the AIT in Vienna, Austria. and the sun hours analysis in Ladybug with the Galapagos optimization within Grasshopper. The final step of this method is the design and subsequent wind and sun analysis of fluid-shaped lamellae in three variants acting as wind catchers/shading systems. Improved pedestrian wind comfort for outdoor sitting (more than 30% increase in areas suitable for short and prolonged sitting) and optimum sunlight hours (25% gain in sunlight during winter solstice) is achieved using the proposed technique.}
}
@article{SACOUTO202297,
title = {Using brain inspired principles to unsupervisedly learn good representations for visual pattern recognition},
journal = {Neurocomputing},
volume = {495},
pages = {97-104},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.04.130},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222005306},
author = {Luis Sa-Couto and Andreas Wichert},
keywords = {Hubel Wiesel’s Hypothesis, Brain inspired architectures, Invariant pattern recognition, Deep learning},
abstract = {Although deep learning has solved difficult problems in visual pattern recognition, it is mostly successful in tasks where there are lots of labeled training data available. Furthermore, the global back-propagation based training rule and the amount of employed layers represents a departure from biological inspiration. The brain is able to perform most of these tasks in a very general way from limited to no labeled data. For these reasons it is still a key research question to look into computational principles in the brain that can help guide models to unsupervisedly learn good representations which can then be used to perform tasks like classification. To that end, we start by recalling four key brain-inspired principles that relate to simple vision: modeling ”whats” and ”wheres” separately; including a time component; context dependency; and layer-wise learning. Then, we take these principles and use them to convey an a priori structure to our model that makes the learning problem easier. With that, our model is able to generate such high quality representations for the MNIST data set. We compare the obtained results with similar recent works and verify extremely competitive results.}
}
@article{TUTHILL2019259,
title = {Decision Support to Enhance Automated Laboratory Testing by Leveraging Analytical Capabilities},
journal = {Clinics in Laboratory Medicine},
volume = {39},
number = {2},
pages = {259-267},
year = {2019},
note = {Clinical Decision Support: Tools, Strategies, and Emerging Technologies},
issn = {0272-2712},
doi = {https://doi.org/10.1016/j.cll.2019.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0272271219300058},
author = {J. Mark Tuthill},
keywords = {Business analytics, Clinical decision support, Laboratory automation, Dashboards, Artificial intelligence, Learning health systems}
}
@article{NASCIMENTO2023105421,
title = {Core–shell clustering approach for detection and analysis of coastal upwelling},
journal = {Computers & Geosciences},
volume = {179},
pages = {105421},
year = {2023},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2023.105421},
url = {https://www.sciencedirect.com/science/article/pii/S0098300423001255},
author = {Susana Nascimento and Alexandre Martins and Paulo Relvas and Joaquim F. Luís and Boris Mirkin},
keywords = {Spatio-temporal clustering, Time series segmentation, SST images, Coastal upwelling, Core–shell cluster},
abstract = {A comprehensive approach is presented to analyze season’s coastal upwelling represented by weekly sea surface temperature (SST) image grids. The proposed model, core–shell clustering, assumes that the season’s upwelling can be divided into shorter periods of stability, time ranges, consisting of constant core and variable shell parts. A one-by-one core–shell clustering algorithm is provided. The algorithm parameters are automatically derived from the least-squares clustering criterion. The approach applies to SST gridded data for sixteen successive years (2004–2019) of coastal upwelling in the western Iberian coast, the northernmost branch of the Canary Current Upwelling System. Our results show that at each season, there are 3 to 5 time intervals, the ranges, at which the upwelling presents stable core patterns of relatively cold water surrounded by somewhat larger shell areas of warmer waters. Based on other experimental computations performed by our team, we conclude that this pattern is not just a purely local phenomenon but has a more global meaning. Inter-annual time series analysis are consistent among themselves and with existing expert domain knowledge.}
}
@article{GARDNER2013167,
title = {Dark matter studies entrain nuclear physics},
journal = {Progress in Particle and Nuclear Physics},
volume = {71},
pages = {167-184},
year = {2013},
note = {Fundamental Symmetries in the Era of the LHC},
issn = {0146-6410},
doi = {https://doi.org/10.1016/j.ppnp.2013.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0146641013000203},
author = {Susan Gardner and George M. Fuller},
keywords = {Dark matter, Nuclear astrophysics, Neutrinos},
abstract = {We review theoretically well-motivated dark-matter candidates, and pathways to their discovery, in the light of recent results from collider physics, astrophysics, and cosmology. Taken in aggregate, these encourage broader thinking in regards to possible dark-matter candidates — dark-matter need not be made of “WIMPs”, i.e., elementary particles with weak-scale masses and interactions. Facilities dedicated to nuclear physics are well-poised to investigate certain non-WIMP models. In parallel to this, developments in observational cosmology permit probes of the relativistic energy density at early epochs and thus provide new ways to constrain dark-matter models, provided nuclear physics inputs are sufficiently well-known. The emerging confluence of accelerator, astrophysical, and cosmological constraints permit searches for dark-matter candidates in a greater range of masses and interaction strengths than heretofore possible.}
}
@article{AMOORE2024102547,
title = {The deep border},
journal = {Political Geography},
volume = {109},
pages = {102547},
year = {2024},
issn = {0962-6298},
doi = {https://doi.org/10.1016/j.polgeo.2021.102547},
url = {https://www.sciencedirect.com/science/article/pii/S0962629821002079},
author = {Louise Amoore},
keywords = {Borders, Machine learning, Immigration, Computation, Algorithms, Biometric},
abstract = {Deep neural network algorithms are becoming intimately involved in the politics of the border, and are themselves bordering devices in that they classify, divide and demarcate boundaries in data. Deep learning involves much more than the deployment of technologies at the border, and is reordering what the border means, how the boundaries of political community can be imagined. Where the biometric border rendered the border mobile through its inscription in the body, the deep border generates the racialized body in novel forms that extend the reach of state violence. The deep border is written through the machine learning models that make the world in their own image – as clusters of attributes and feature spaces from which data examples can be drawn. The ‘depth’ that becomes imaginable in computer science models of the indefinite multiplication of layers in a neural network begins to resonate with state desires for a reach into the attributes of population. The border is spatially reimagined as a set of always possible functions, features, and clusters – as a ‘line of best fit’ where the fraught politics of the border can be condensed and resolved.}
}
@article{ZHAO2025100817,
title = {User entertainment experience analysis of artificial intelligence entertainment robots based on convolutional neural networks in park plant landscape design},
journal = {Entertainment Computing},
volume = {52},
pages = {100817},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100817},
url = {https://www.sciencedirect.com/science/article/pii/S187595212400185X},
author = {Jingjing Zhao and Juan Yin and Yaqi Shi and Liang Qiao and Guihua Ma},
keywords = {Convolutional neural network, AI entertainment robots, Park plants, Landscape design, User experience},
abstract = {Currently, the application of artificial intelligence entertainment robots in park plant landscape design has attracted increasing attention. This study aims to design an artificial intelligence entertainment robot that can provide a high-quality user experience. Through virtual reality and robotics technology, designers can be provided with visual and entertaining design solutions, and more interactive experiences can be provided for design clients. Convolutional neural networks can effectively extract features from images, and utilizing spectral feature extraction technology to further improve the accuracy of image recognition. Subsequently, this study designed a robot control system and calibrated the hand eye system. The robot control system can coordinate the various functions of the robot and ensure its smooth operation in the park plant landscape design. The calibration of the hand eye system is to ensure that the robot can accurately perceive the environment and locate its own position. Through real-time control strategies, robots can respond and adjust in a timely manner based on current environmental changes and user needs. By comparing with the actual position on the ground, the accuracy of robot positioning is obtained, and the system is further optimized and improved.}
}
@article{SAENZROYO2024121922,
title = {Ordering vs. AHP. Does the intensity used in the decision support techniques compensate?},
journal = {Expert Systems with Applications},
volume = {238},
pages = {121922},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121922},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423024247},
author = {Carlos Sáenz-Royo and Francisco Chiclana and Enrique Herrera-Viedma},
keywords = {AHP, IBR, Decision Support Systems, Expertise, Intensity Judgment},
abstract = {The manifestation of the intensity in the judgment of one alternative versus another in the peer comparison processes is a central element in some decision support techniques, such as the Analytical Hierarchy Process (AHP). However, its contribution regarding quality (expected performance) with respect to the priority vector has not been evaluated so far. Using the Intentional Bounded Rationality Methodology (IBRM), this work analyzes the gains obtained from requiring the decision-maker to report an intensity judgment in pairs (AHP) with respect to a technique that only requires expressing a preference (Ordering). The results show that when decision-makers have low levels of expertise, it is possible that a less informative and computational cheap technique (Ordering) performs better than a more informative and computational expensive one (AHP). When decision-makers have medium and high levels of expertise, AHP technique obtains modest gains with respect to the Ordering technique. This study proposes a cost-benefit analysis of decision support techniques contrasting the gains of a technique that requires more resources (AHP) against other that require less resources (Ordering). Our results can change the managing approach of the information obtained from experts’ judgments.}
}
@article{SCHROEDER2015530,
title = {Situated phenomenology and biological systems: Eastern and Western synthesis},
journal = {Progress in Biophysics and Molecular Biology},
volume = {119},
number = {3},
pages = {530-537},
year = {2015},
note = {Integral Biomathics: Life Sciences, Mathematics, and Phenomenological Philosophy},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2015.06.019},
url = {https://www.sciencedirect.com/science/article/pii/S0079610715000942},
author = {Marcin J. Schroeder and Jordi Vallverdú},
keywords = {Phenomenology, Experience, Situated cognition, Eastern, Western, Cybernetics, Robotics, System, Biology},
abstract = {Phenomenology was born with the mission to give foundations for science of experience and to open consciousness to scientific study. The influence of phenomenology initiated in the works of Husserl and continued in a wide range of works of others was immense, but mainly within the confines of philosophy and the humanities. The actual attempts to develop a scientific discipline of the study of consciousness and to carry out research on cognition and consciousness were always based on the methods of traditional science in which elimination of the subjective has been always a primary tenet. Thus, focus was mainly on neurological correlates of conscious phenomena. The present paper is an attempt to initiate an extension and revision of phenomenological methodology with the use of philosophical and scientific experience and knowledge accumulated in a century of inquiry and research in relevant disciplines. The question which disciplines are relevant is crucial and our answer is innovative. The range of disciplines involved here is from information science and studies of computation, up to cultural psychology and the studies of philosophical traditions of the East. Concepts related to information and computation studies provide a general conceptual framework free from the limitations of particular languages and of linguistic analysis. This conceptual framework is extending the original perspective of phenomenology to issues of modern technology and science. Cultural psychology gives us tools to root out what in phenomenology was considered universal for humanity, but was a result of European ethnocentrism. Most important here is the contrast between individualistic and collectivistic cultural determinants of consciousness. Finally, philosophical tradition of the East gives alternatives in seeking solutions for fundamental problems. This general outline of the research methodology is illustrated by an example of its use when phenomenology is studied within the conceptual framework of information.}
}
@article{KADANE1985256,
title = {Parallel and sequential computation: a statistician's view},
journal = {Journal of Complexity},
volume = {1},
number = {2},
pages = {256-263},
year = {1985},
issn = {0885-064X},
doi = {https://doi.org/10.1016/0885-064X(85)90014-7},
url = {https://www.sciencedirect.com/science/article/pii/0885064X85900147},
author = {Joseph B Kadane},
abstract = {I borrow themes from statistics—epsecially the Bayesian ideas underlying average-case analysis and ideas of sequential design of experiments—to discuss when parallel computation is likely to be an attractive technique.}
}
@article{WEST2023300,
title = {Agent-based methods facilitate integrative science in cancer},
journal = {Trends in Cell Biology},
volume = {33},
number = {4},
pages = {300-311},
year = {2023},
issn = {0962-8924},
doi = {https://doi.org/10.1016/j.tcb.2022.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0962892422002409},
author = {Jeffrey West and Mark Robertson-Tessi and Alexander R.A. Anderson},
keywords = {agent-based mathematical models, integrative science, tissue homeostasis, cancer metabolism, immune–tumor interactions},
abstract = {In this opinion, we highlight agent-based modeling as a key tool for exploration of cell–cell and cell–environment interactions that drive cancer progression, therapeutic resistance, and metastasis. These biological phenomena are particularly suited to be captured at the cell-scale resolution possible only within agent-based or individual-based mathematical models. These modeling approaches complement experimental work (in vitro and in vivo systems) through parameterization and data extrapolation but also feed forward to drive new experiments that test model-generated predictions.}
}
@article{SCHWARZ2014283,
title = {On computing time-to-collision for automation scenarios},
journal = {Transportation Research Part F: Traffic Psychology and Behaviour},
volume = {27},
pages = {283-294},
year = {2014},
note = {Vehicle Automation and Driver Behaviour},
issn = {1369-8478},
doi = {https://doi.org/10.1016/j.trf.2014.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S1369847814000898},
author = {Chris Schwarz},
keywords = {TTC, Automation, Computational methods, Algorithms, Computational geometry},
abstract = {Time to collision (TTC) has been a key vehicle safety metric for decades. With the increasing prevalence of advanced driver assistance systems and vehicle automation, TTC and many related metrics are being applied to the analysis of more complicated scenarios, as well as being integrated into automation algorithms. While the TTC metric was originally conceived to be inclusive of generic two-dimensional situations, its applications has been mostly limited to one-dimensional scenarios. This paper derives general equations and algorithms using two-dimensional information. Additionally, methods from computational geometry, a field that didn’t exist when TTC was first used, are employed for the general case of computing TTC between bounding boxes. Parametric equations for lines play a prominent role and offer an elegant way to express the geometry of the scenarios described in this paper. Throughout, the approach is not to derive specific algebraic conditions as in previous efforts. Rather, the focus in on developing general algorithms for computation. The techniques presented are not necessary for traditional car following scenarios; but offer options for more complex situations that trade off analytic solutions for computational flexibility.}
}
@article{CHEN2022100602,
title = {Modern views of machine learning for precision psychiatry},
journal = {Patterns},
volume = {3},
number = {11},
pages = {100602},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100602},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922002276},
author = {Zhe Sage Chen and Prathamesh (Param) Kulkarni and Isaac R. Galatzer-Levy and Benedetta Bigio and Carla Nasca and Yu Zhang},
keywords = {machine learning, ML, artificial intelligence, AI, deep learning, precision psychiatry, digital psychiatry, computational psychiatry, neuroimaging, neurobiomarker, molecular biomarker, digital phenotyping, multi-modal data fusion, neuromodulation, causality, explainable AI, XAI, teletherapy},
abstract = {Summary
In light of the National Institute of Mental Health (NIMH)’s Research Domain Criteria (RDoC), the advent of functional neuroimaging, novel technologies and methods provide new opportunities to develop precise and personalized prognosis and diagnosis of mental disorders. Machine learning (ML) and artificial intelligence (AI) technologies are playing an increasingly critical role in the new era of precision psychiatry. Combining ML/AI with neuromodulation technologies can potentially provide explainable solutions in clinical practice and effective therapeutic treatment. Advanced wearable and mobile technologies also call for the new role of ML/AI for digital phenotyping in mobile mental health. In this review, we provide a comprehensive review of ML methodologies and applications by combining neuroimaging, neuromodulation, and advanced mobile technologies in psychiatry practice. We further review the role of ML in molecular phenotyping and cross-species biomarker identification in precision psychiatry. We also discuss explainable AI (XAI) and neuromodulation in a closed human-in-the-loop manner and highlight the ML potential in multi-media information extraction and multi-modal data fusion. Finally, we discuss conceptual and practical challenges in precision psychiatry and highlight ML opportunities in future research.}
}
@article{ZOU2024134011,
title = {Synthesis and mechanism of quaternary ammonium salts based on porphyrin as high-performance copper levelers},
journal = {Tetrahedron},
volume = {159},
pages = {134011},
year = {2024},
issn = {0040-4020},
doi = {https://doi.org/10.1016/j.tet.2024.134011},
url = {https://www.sciencedirect.com/science/article/pii/S0040402024001911},
author = {Peikun Zou and Xuyang Li and Xin Chen and Wenhao Zhou and Kexin Du and Limin Wang},
keywords = {Porphyrin, Porphyrin quaternary ammonium salts, Through-hole electroplating, Electroplating leveler, Quantum chemical calculations},
abstract = {The molecular structure and energy distribution of organic compounds have a great influence on their adsorption capacity on the metal surface. However, there are still insufficient researches on the influence of energy distribution on adsorption properties of organic molecules. Herein, a family of porphyrin derivatives (TPyP-Et, TPyP-Oct, TPyP-Bn and TPyP-Al) bearing quaternary ammonium groups were synthesized for the first time as promising levelers for through-hole copper electrodeposition. Electrochemical tests revealed that all four TPyP derivatives displayed enhanced electrochemical properties. Theoretical calculations and molecular dynamics simulations were carried out to investigate the physisorption capacity and chemical reaction activity of the TPyP molecules, as well as the adsorption capacity on the surface of the copper layer. Through optical and scanning electron microscopy as well as X-ray diffractometry, it was demonstrated that TPyP molecules are effective electroplating levelers. TPyP-Oct, with its longer carbon chain substituent, exhibited superior hole-filling performance in practical PCB experiments among the four compounds. This study expandes the application range of porphyrin compounds, analyzes the influence of organic molecular adsorption properties in copper electrodeposition, and provides theoretical guidance for the future study of organic compounds adsorbed on metal surfaces.}
}
@article{JALALIAN2023103602,
title = {Learning about me and you: Only deterministic stimulus associations elicit self-prioritization},
journal = {Consciousness and Cognition},
volume = {116},
pages = {103602},
year = {2023},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2023.103602},
url = {https://www.sciencedirect.com/science/article/pii/S1053810023001393},
author = {Parnian Jalalian and Marius Golubickis and Yadvi Sharma and C. {Neil Macrae}},
keywords = {Self, Instrumental learning, Probabilistic selection task, Self-prioritization, Drift diffusion model},
abstract = {Self-relevant material has been shown to be prioritized over stimuli relating to others (e.g., friend, stranger), generating benefits in attention, memory, and decision-making. What is not yet understood, however, is whether the conditions under which self-related knowledge is acquired impacts the emergence of self-bias. To address this matter, here we used an associative-learning paradigm in combination with a stimulus-classification task to explore the effects of different learning experiences (i.e., deterministic vs. probabilistic) on self-prioritization. The results revealed an effect of prior learning on task performance, with self-prioritization only emerging when participants acquired target-related associations (i.e., self vs. friend) under conditions of certainty (vs. uncertainty). A further computational (i.e., drift diffusion model) analysis indicated that differences in the efficiency of stimulus processing (i.e., rate of information uptake) underpinned this self-prioritization effect. The implications of these findings for accounts of self-function are considered.}
}
@article{HUBALOVSKY2019691,
title = {Assessment of the influence of adaptive E-learning on learning effectiveness of primary school pupils},
journal = {Computers in Human Behavior},
volume = {92},
pages = {691-705},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2018.05.033},
url = {https://www.sciencedirect.com/science/article/pii/S0747563218302590},
author = {S. Hubalovsky and M. Hubalovska and M. Musilek},
keywords = {Learning analytics, Cognitive computing, Adaptive e-learning, Primary education, Learning effectiveness, Bloom's taxonomy},
abstract = {The paper deals with assessment of the influence of adaptive e-learning as a part of learning analytics on learning effectiveness of primary school pupils. E-learning exercises containing implemented adaptive elements were created in accordance with the Bloom's Taxonomy. Within the pilot study the authors detected high percentage success rate during e-learning exercise completion. This leads to formulation of the question „Can any e-learning exercise of lower cognitive levels of Bloom's taxonomy be skipped without affecting the cognitive thinking for solution of the e-learning exercises on upper cognitive levels of Bloom's taxonomy?” To answer the question, the algorithm of adaptive e-learning was defined and hypotheses were established. The research was carried out as pedagogical experiment comparing the results of both experimental and control groups of pupils. The research hypotheses were confirmed by statistical analysis of the research data. The results confirm that adaptive features of e-learning can be implemented in the primary education. The research results confirm the fact that educational objectives can be achieved with some pupils more effectively. Consequently, the implementation of adaptive elements into e-learning at the primary school supports an individual approach when completing e-learning exercises according to the principle of cognitive computing.}
}
@article{CLEMENTI198713,
title = {Large-scale computations on a scalar, vector and parallel ‘supercomputer’},
journal = {Parallel Computing},
volume = {5},
number = {1},
pages = {13-44},
year = {1987},
note = {Proceedings of the International Conference on Vector and Parallel Computing-Issues in Applied Research and Development},
issn = {0167-8191},
doi = {https://doi.org/10.1016/0167-8191(87)90004-4},
url = {https://www.sciencedirect.com/science/article/pii/0167819187900044},
author = {E Clementi and J Detrich and S Chin and G Corongiu and D Folsom and D Logan and R Caltabiano and A Carnevali and J Helin and M Russo and A Gnudi and P Palamidese},
keywords = {Parallel computer systems, 1CAP-1, 1CAP-2, 1CAP-3, programming strategy, migration of code from sequential to parallel systems, performance analysis},
abstract = {We discuss two experimental parallel computer systems 1CAP-1 and 1CAP-2 which can be applied to the entire spectrum of scientific and engineering applications. These systems achieve ‘supercomputer’ levels of performance by spreading large scale computations across multiple cooperating processors—several with vector capabilities. We outline system hardware and software, and discuss our programming strategy for migrating codes from a conventional sequential system to a parallel one. The performance of a variety of applications programs is analyzed to demonstrate the merits of this approach. Finally, we discuss 1CAP-3, an extension to this computing system, which has been recently assembled.}
}
@article{KENETT2019271,
title = {A Semantic Network Cartography of the Creative Mind},
journal = {Trends in Cognitive Sciences},
volume = {23},
number = {4},
pages = {271-274},
year = {2019},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2019.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364661319300245},
author = {Yoed N. Kenett and Miriam Faust},
keywords = {creativity, semantic networks, network science},
abstract = {The role of semantic memory in creativity is theoretically assumed, but far from understood. In recent years, computational network science tools have been applied to investigate this role. These studies shed unique quantitative insights on the role of semantic memory structure in creativity, via measures of connectivity, distance, and structure.}
}
@article{BURR2020R907,
title = {Horace Barlow (1921–2020)},
journal = {Current Biology},
volume = {30},
number = {16},
pages = {R907-R910},
year = {2020},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2020.07.060},
url = {https://www.sciencedirect.com/science/article/pii/S096098222031085X},
author = {David Burr and Simon Laughlin}
}
@article{LIU2024100129,
title = {Extracting multi-objective multigraph features for the shortest path cost prediction: Statistics-based or learning-based?},
journal = {Green Energy and Intelligent Transportation},
volume = {3},
number = {1},
pages = {100129},
year = {2024},
issn = {2773-1537},
doi = {https://doi.org/10.1016/j.geits.2023.100129},
url = {https://www.sciencedirect.com/science/article/pii/S2773153723000658},
author = {Songwei Liu and Xinwei Wang and Michal Weiszer and Jun Chen},
keywords = {Multi-objective multigraph, Feature extraction, Shortest path cost prediction, Node patterns, Node embeddings, Regression},
abstract = {Efficient airport airside ground movement (AAGM) is key to successful operations of urban air mobility. Recent studies have introduced the use of multi-objective multigraphs (MOMGs) as the conceptual prototype to formulate AAGM. Swift calculation of the shortest path costs is crucial for the algorithmic heuristic search on MOMGs, however, previous work chiefly focused on single-objective simple graphs (SOSGs), treated cost enquires as search problems, and failed to keep a low level of computational time and storage complexity. This paper concentrates on the conceptual prototype MOMG, and investigates its node feature extraction, which lays the foundation for efficient prediction of shortest path costs. Two extraction methods are implemented and compared: a statistics-based method that summarises 22 node physical patterns from graph theory principles, and a learning-based method that employs node embedding technique to encode graph structures into a discriminative vector space. The former method can effectively evaluate the node physical patterns and reveals their individual importance for distance prediction, while the latter provides novel practices on processing multigraphs for node embedding algorithms that can merely handle SOSGs. Three regression models are applied to predict the shortest path costs to demonstrate the performance of each. Our experiments on randomly generated benchmark MOMGs show that (i) the statistics-based method underperforms on characterising small distance values due to severe overestimation; (ii) A subset of essential physical patterns can achieve comparable or slightly better prediction accuracy than that based on a complete set of patterns; and (iii) the learning-based method consistently outperforms the statistics-based method, while maintaining a competitive level of computational complexity.}
}
@article{NESI2024,
title = {Exploring enactivism: A scoping review of its key concepts and theorical approach},
journal = {Advances in Integrative Medicine},
year = {2024},
issn = {2212-9588},
doi = {https://doi.org/10.1016/j.aimed.2024.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S221295882400082X},
author = {Jacson Nesi and Roberta Lemos {dos Santos} and Michele Benites},
keywords = {Enactivism, Enaction, body, Anatomy, Embodiment, Scoping Review},
abstract = {Enactivism is a theoretical perspective in the fields of philosophy of mind and cognition that emphasizes the active role of the organism in constructing and giving meaning to the world around it. It highlights that the mind is not merely a passive receiver of information from the environment, but rather an active participant in the creation of meaning and experience. The idea for this article arises from the uncertainty surrounding the distinction of practice, principles, and osteopathic concepts, which have been raised by various regarding osteopathic principles: the anachronism of their distinction, whether the role of these principles could still be accepted as a guide for osteopathy in the contemporary world, whether the biopsychosocial model could be the basis for a proposal to redefine them and even whether the use of these principles could do more harm than good. Objectives: Facilitate access to essential definitions and concepts related to enactivism, and make the understanding of these elements more accessible, as they play a crucial role in the reconceptualization of osteopathy. Materials and methods: The work was elaborated as a scoping review, using the PRISMA-P 2020 Checklist.}
}
@article{FOELLMI2019103136,
title = {Loss aversion at the aggregate level across countries and its relation to economic fundamentals},
journal = {Journal of Macroeconomics},
volume = {61},
pages = {103136},
year = {2019},
issn = {0164-0704},
doi = {https://doi.org/10.1016/j.jmacro.2019.103136},
url = {https://www.sciencedirect.com/science/article/pii/S0164070419301028},
author = {Reto Foellmi and Adrian Jaeggi and Rina Rosenblatt-Wisch},
abstract = {Preferences are important when thinking about macroeconomic problems and questions. Differences in preferences might, for example, explain cross-country variations in economic fundamentals. In recent years, differences in preferences across countries and cultures have been studied more frequently, usually concentrating on micro evidence. However, it is an open question as to how differences in average preferences affect the aggregate economy. Coming from a macroeconomic perspective, we test whether preferences stated in Kahneman and Tversky’s prospect theory, namely, reference point dependence and loss aversion, prevail on the aggregate and whether the average degree of loss aversion differs across countries. We find evidence of loss aversion for a broad set of OECD countries, while the average loss aversion clearly differs across these countries. We find little evidence that these differences could be linked to micro evidence. Furthermore, we analyse whether the different degrees of loss aversion correlate with economic fundamentals such as the level of GDP and consumption per capita. We find that indeed loss aversion is negatively correlated with GDP and consumption per capita and positively correlated with consumption smoothing.}
}
@article{DICARLO2007333,
title = {Untangling invariant object recognition},
journal = {Trends in Cognitive Sciences},
volume = {11},
number = {8},
pages = {333-341},
year = {2007},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2007.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S1364661307001593},
author = {James J. DiCarlo and David D. Cox},
abstract = {Despite tremendous variation in the appearance of visual objects, primates can recognize a multitude of objects, each in a fraction of a second, with no apparent effort. However, the brain mechanisms that enable this fundamental ability are not understood. Drawing on ideas from neurophysiology and computation, we present a graphical perspective on the key computational challenges of object recognition, and argue that the format of neuronal population representation and a property that we term ‘object tangling’ are central. We use this perspective to show that the primate ventral visual processing stream achieves a particularly effective solution in which single-neuron invariance is not the goal. Finally, we speculate on the key neuronal mechanisms that could enable this solution, which, if understood, would have far-reaching implications for cognitive neuroscience.}
}
@article{XIONG2023105811,
title = {Neural vortex method: From finite Lagrangian particles to infinite dimensional Eulerian dynamics},
journal = {Computers & Fluids},
volume = {258},
pages = {105811},
year = {2023},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2023.105811},
url = {https://www.sciencedirect.com/science/article/pii/S0045793023000361},
author = {Shiying Xiong and Xingzhe He and Yunjin Tong and Yitong Deng and Bo Zhu},
keywords = {Vortex method, Neural network, Lagrangian dynamics, Eulerian dynamics},
abstract = {In fluid analysis, there has been a long-standing problem: lacking a rigorous mathematical tool to map from a continuous flow field to finite discrete particles, hurdling the Lagrangian particles from inheriting the high resolution of a large-scale Eulerian solver. To tackle this challenge, we propose a novel learning-based framework, the neural vortex method (NVM). NVM builds a neural-network description of the Lagrangian vortex structures and their interaction dynamics to reconstruct the high-resolution Eulerian flow field in a physically-precise manner. The key components of our infrastructure consist of two networks: a vortex detection network to identify the Lagrangian vortices from a grid-based velocity field and a vortex dynamics network to learn the underlying governing interactions of these finite structures. By embedding these two networks with a vorticity-to-velocity Poisson solver and training its parameters using the fluid data obtained from grid-based numerical simulation, we can predict the accurate fluid dynamics on a precision level that was infeasible for all the previous conventional vortex methods. We demonstrate the efficacy of our method in generating highly accurate prediction results with low computational cost by predicting the evolution of the leapfrogging vortex rings system, the turbulence system, and the systems governed by Navier–Stokes (NS) equations with different external forces. We compare the prediction results made by NVM and the Lagrangian vortex method (LVM) for solving the NS equation in the periodic box and find that the relative error of the predicted velocity using NVM is more than 10 times lower than that of the LVM. Moreover, our method only requires data collected from a very short training window, more than 100 times smaller than the prediction period, which potentially facilitates data acquisition in real systems.}
}
@article{GALDO2022101508,
title = {The quest for simplicity in human learning: Identifying the constraints on attention},
journal = {Cognitive Psychology},
volume = {138},
pages = {101508},
year = {2022},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2022.101508},
url = {https://www.sciencedirect.com/science/article/pii/S0010028522000445},
author = {Matthew Galdo and Emily R. Weichart and Vladimir M. Sloutsky and Brandon M. Turner},
keywords = {Attention, Category learning, Eye-tracking, Cognitive bias, Capacity, Model comparison},
abstract = {For better or worse, humans live a resource-constrained existence; only a fraction of physical sensations ever reach conscious awareness, and we store a shockingly small subset of these experiences in memory for later use. Here, we examined the effects of attention constraints on learning. Among models that frame selective attention as an optimization problem, attention orients toward information that will reduce errors. Using this framing as a basis, we developed a suite of models with a range of constraints on the attention available during each learning event. We fit these models to both choice and eye-fixation data from four benchmark category-learning data sets, and choice data from another dynamic categorization data set. We found consistent evidence for computations we refer to as “simplicity”, where attention is deployed to as few dimensions of information as possible during learning, and “competition”, where dimensions compete for selective attention via lateral inhibition.}
}
@article{TONNANG201788,
title = {Advances in crop insect modelling methods—Towards a whole system approach},
journal = {Ecological Modelling},
volume = {354},
pages = {88-103},
year = {2017},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2017.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S030438001630549X},
author = {Henri E.Z. Tonnang and Bisseleua D.B. Hervé and Lisa Biber-Freudenberger and Daisy Salifu and Sevgan Subramanian and Valentine B. Ngowi and Ritter Y.A. Guimapi and Bruce Anani and Francois M.M. Kakmeni and Hippolyte Affognon and Saliou Niassy and Tobias Landmann and Frank T. Ndjomatchoua and Sansao A. Pedro and Tino Johansson and Chrysantus M. Tanga and Paulin Nana and Komi M. Fiaboe and Samira F. Mohamed and Nguya K. Maniania and Lev V. Nedorezov and Sunday Ekesi and Christian Borgemeister},
keywords = {Insect modelling approaches, Integrated pest management, Crop production, Climate change, Impact assessment, Yield losses, System thinking},
abstract = {A wide range of insects affect crop production and cause considerable yield losses. Difficulties reside on the development and adaptation of adequate strategies to predict insect pests for their timely management to ensure enhanced agricultural production. Several conceptual modelling frameworks have been proposed, and the choice of an approach depends largely on the objective of the model and the availability of data. This paper presents a summary of decades of advances in insect population dynamics, phenology models, distribution and risk mapping. Existing challenges on the modelling of insects are listed; followed by innovations in the field. New approaches include artificial neural networks, cellular automata (CA) coupled with fuzzy logic (FL), fractal, multi-fractal, percolation, synchronization and individual/agent-based approaches. A concept for assessing climate change impacts and providing adaptation options for agricultural pest management independently of the United Nations Intergovernmental Panel on Climate Change (IPCC) emission scenarios is suggested. A framework for estimating losses and optimizing yields within crop production system is proposed and a summary on modelling the economic impact of pests control is presented. The assessment shows that the majority of known insect modelling approaches are not holistic; they only concentrate on a single component of the system, i.e. the pest, rather than the whole crop production system. We suggest system thinking as a possible approach for linking crop, pest, and environmental conditions to provide a more comprehensive assessment of agricultural crop production.}
}
@article{NAVEIRO2019133,
title = {Adversarial classification: An adversarial risk analysis approach},
journal = {International Journal of Approximate Reasoning},
volume = {113},
pages = {133-148},
year = {2019},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2019.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X18304705},
author = {Roi Naveiro and Alberto Redondo and David {Ríos Insua} and Fabrizio Ruggeri},
keywords = {Classification, Bayesian methods, Adversarial machine learning, Influence diagrams, Robustness},
abstract = {Classification techniques are widely used in security settings in which data can be deliberately manipulated by an adversary trying to evade detection and achieve some benefit. However, traditional classification systems are not robust to such data modifications. Most attempts to enhance classification algorithms in adversarial environments have focused on game theoretical ideas under strong underlying common knowledge assumptions, which are not actually realistic in security domains. We provide an alternative framework to such problems based on adversarial risk analysis which we illustrate with examples. Computational, implementation and robustness issues are discussed.}
}
@article{CARDENASROBLEDO2019299,
title = {A holistic self-regulated learning model: A proposal and application in ubiquitous-learning},
journal = {Expert Systems with Applications},
volume = {123},
pages = {299-314},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419300089},
author = {Leonor Adriana Cárdenas-Robledo and Alejandro Peña-Ayala},
keywords = {Technology enhanced learning, Ubiquitous–Learning, Self–regulated learning, metacognition, cognitive load},
abstract = {Technology enhanced learning (TEL) represents an expert and intelligent paradigm in which technological affordances are used to facilitate learners' acquisition of domain knowledge (DK), where Ubiquitous–Learning (u–learning) is a TEL instance that recreates situated and immersive settings. However in such settings, learners are stressed by diverse, heterogeneous, and simultaneous stimuli that challenge their cognitive skills, increase the cognitive load, trigger emotional reactions, and bias conduct. Thus this research proposes a smart Sequencing approach that enables TEL systems to lead students to regulate their learning process. The essence of the proposal is a holistic self–regulated learning (SRL) model that encourage students to develop higher–order thinking through the practice of metacognitive skills, motivational factors, and behavioral affairs to become aware of their own learning endeavors. The approach was applied as part of the sequencing module of a u–Learning system, where students follow its suggested cognitive strategies to assist them during the programming of control equipment. Results show that, although experimental subjects had to deal with higher cognitive load, they are expected to reach higher learning achievements than their control peers. The experience reveals how TEL systems are enabled to foster learners to handle their own apprenticeship processes. As a consequence, the smart sequencing functionality of TEL is cognitively enhanced to facilitate students the simultaneal acquisition DK and development of higher–order thinking.}
}
@article{MULET200632,
title = {Functional requirements for computer-based design support systems, derived from experimental studies},
journal = {Knowledge-Based Systems},
volume = {19},
number = {1},
pages = {32-42},
year = {2006},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2005.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0950705105000869},
author = {Elena Mulet and Rosario Vidal},
keywords = {Computer design support, Knowledge-based design systems, Design process, Computational synthesis methods, Experimental research in engineering design},
abstract = {In this paper, we examine the functions that a computational system for knowledge-based design support may undertake. We present a set of functions that bring together previous approaches and allow us to locate the work that has been developed to enhance these systems concerning those functions. We describe some new proposals, based on experimental research work, for improving some of these functions so that they can be taken into account in the development of design support systems to help the designer or group of designers reach a suitable solution in a more effective way.}
}
@article{VASQUEZ2019306,
title = {Curriculum change for graduate-level control engineering education at the Universidad Pontificia Bolivariana},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {9},
pages = {306-311},
year = {2019},
note = {12th IFAC Symposium on Advances in Control Education ACE 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.08.225},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319305622},
author = {Rafael E. Vásquez and Fabio Castrillón and Santiago Rúa and Norha L. Posada and Carlos A. Zuluaga},
keywords = {Control engineering, control education, active learning, curriculum change},
abstract = {This paper addresses the graduate-level control engineering curriculum change performed at the Universidad Pontificia Bolivariana (UPB), Medellin, Colombia. New proposed methodologies include active learning activities using a new multipurpose experimental test bed that was developed with industrial components. The renovated graduate-level control engineering related courses include: Continuous Processes, Discrete Processes, Fuzzy Logic, Neural Networks and Genetic Algorithms, Linear Control, Nonlinear Control, and Optimal Estimation. The new experimental station was developed for teaching, research, and industrial training activities for the School of Engineering at the UPB. In this work, we report the use of the station in an Optimal Estimation course to replace a traditional homework/exams evaluation approach with an applied work that required independent study, the implementation of different observers in a real lab-scale industrial plant, and a paper-style written report. Increasing independent study activities resulted in academic discussions that are valuable for the learning process of the student. The use of the experimental station and the real comparison of estimation algorithms, implemented by using industrial controllers and high-level programming environments, provided the student skills that cannot be acquired by using only simulations in which real implementation restrictions/challenges do not appear. This work represents one of the first approaches for the implementation of the new curriculum model at the UPB for graduate education. The methodology used in the Optimal Estimation class promoted independent learning, critical thinking and writing skills through significant learning activities.}
}
@article{KIRCHER2018515,
title = {Formal thought disorders: from phenomenology to neurobiology},
journal = {The Lancet Psychiatry},
volume = {5},
number = {6},
pages = {515-526},
year = {2018},
issn = {2215-0366},
doi = {https://doi.org/10.1016/S2215-0366(18)30059-2},
url = {https://www.sciencedirect.com/science/article/pii/S2215036618300592},
author = {Tilo Kircher and Henrike Bröhl and Felicitas Meier and Jennifer Engelen},
abstract = {Summary
Formal thought disorder (FTD) is present in most psychiatric disorders and in some healthy individuals. In this Review, we present a comprehensive, integrative, and multilevel account of what is known about FTD, covering genetic, cellular, and neurotransmitter effects, environmental influences, experimental psychology and neuropsychology, brain imaging, phenomenology, linguistics, and treatment. FTD is a dimensional, phenomenologically defined construct, which can be clinically subdivided into positive versus negative and objective versus subjective symptom clusters. Because FTDs have been traditionally linked to schizophrenia, studies in other diagnoses are scarce. Aetiologically, FTD is the only symptom under genetic influence in schizophrenia as shown in linkage studies, but familial communication patterns (allusive thinking) have also been associated with the condition. Positive FTDs are related to synaptic rarefication in the glutamate system of the superior and middle lateral temporal cortices. Cortical volume of the left superior temporal gyrus is decreased in patients with schizophrenia who have positive FTD in structural MRI studies and shows reversed hemispheric (right more than left) activation in functional MRI experiments during speech production. Semantic network dysfunction in positive FTD has been demonstrated in experiments of indirect semantic hyperpriming (reaction time). In acute positive FTD, antipsychotics are effective, but a subgroup of patients have treatment-resistant, chronic, positive or negative FTD. Specific psychotherapy as treatment for FTD has not yet been developed. With this solid data on the pathogenesis of FTD, we can now implement clinical studies to treat this condition.}
}
@article{HUDA2024122380,
title = {Experts and intelligent systems for smart homes’ Transformation to Sustainable Smart Cities: A comprehensive review},
journal = {Expert Systems with Applications},
volume = {238},
pages = {122380},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122380},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423028828},
author = {Noor Ul Huda and Ijaz Ahmed and Muhammad Adnan and Mansoor Ali and Faisal Naeem},
keywords = {Artificial intelligence, Automation, Block chain, Energy management, Expert intelligent systems, Management systems, Smart cities},
abstract = {In this constantly evolving landscape of urbanization, the relationship between technology and automation, in regards to sustainability, holds immense significance. The intricate strands of human intelligence are seamlessly interwoven with the fabric of technological progress, giving rise to exquisite patterns of synergy and collaborative innovation. Automation is just another step in this process which started with the industrial revolution and now has paved way towards urbanization. Smart homes or home automation is a subset of Internet of Things (IoT) based automation that has added into the comfort, ease, and quality of our living standards and is now being integrated to form the concept of Smart Cities. In the past decade, various techniques and processes of smart home automation have been proposed and implemented. To extend and translate the existing methods into new one, the understanding of the former is imperative to the research procedure. This review stands as a comprehensive exploration, diving into the pivotal role of intelligent systems and expert knowledge in driving the transformation of smart homes into sustainable smart cities. By meticulously analyzing and aggregating an array of contemporary techniques used in smart homes, this paper offers profound contributions to the intersection of urban evolution and technological innovation. The review’s holistic approach not only facilitates a deep understanding of smart homes’ contributions but also charts a course for innovative strategies in city planning, infrastructure, and technological integration. In bridging the gap between technology and sustainable urban development, this exploration underscores the transformative power of leveraging smart home techniques to lay the foundation for harmonious and forward-thinking smart cities. The technologies cover a wide range of methodologies and intelligent systems used for communication, security and management in an urban infrastructure. The paper focuses on analysis of the technology to provide an outlook into achieving the goal of sustainable smart cities and deal with challenges like scalability and big data computation. Our comprehensive analysis yields a holistic set of technology comparisons and illuminates the promising future prospects within this domain. The information is highly insightful in creating a bigger picture for adopting state of the art technologies like Federated Learning (FL), Digital Twin and Embedded Edge computing in better planning and infrastructure management in smart cities. These findings offer reliable and potent methods to chart not only the course of research but also to enhance these technologies for the betterment of mankind’s convenience and advancement.}
}
@article{SOLARI2008106,
title = {Confabulation Theory},
journal = {Physics of Life Reviews},
volume = {5},
number = {2},
pages = {106-120},
year = {2008},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2008.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S1571064508000122},
author = {Soren Solari and Andrew Smith and Rupert Minnett and Robert Hecht-Nielsen},
keywords = {Confabulation Theory},
abstract = {Confabulation Theory [Hecht-Nielsen R. Confabulation theory. Springer-Verlag; 2007] is the first comprehensive theory of human and animal cognition. Here, we briefly describe Confabulation Theory and discuss experimental results that suggest the theory is correct. Simply put, Confabulation Theory proposes that thinking is like moving. In humans, the theory postulates that there are roughly 4000 thalamocortical modules, the “muscles of thought”. Each module performs an internal competition (confabulation) between its symbols, influenced by inputs delivered via learned axonal associations with symbols in other modules. In each module, this competition is controlled, as in an individual muscle, by a single graded (i.e., analog) thought control signal. The final result of this confabulation process is a single active symbol, the expression of which also results in launching of action commands that trigger and control subsequent movements and/or thought processes. Modules are manipulated in groups under coordinated, event-contingent control, in a similar manner to our 700 muscles. Confabulation Theory hypothesizes that the control of thinking is a direct evolutionary outgrowth of the control of movement. Establishing a complete understanding of Confabulation Theory will require launching and sustaining a massive new phalanx of confabulation neuroscience research.}
}
@article{TROCHIM2017176,
title = {Hindsight is 20/20: Reflections on the evolution of concept mapping},
journal = {Evaluation and Program Planning},
volume = {60},
pages = {176-185},
year = {2017},
issn = {0149-7189},
doi = {https://doi.org/10.1016/j.evalprogplan.2016.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0149718916301690},
author = {William M. Trochim},
keywords = {Concept mapping, Construct validity, Pattern matching, Theory of conceptualization, Multidimensional scaling, Cluster analysis, Bridging analysis, Go-zone plot, Bibliometric analysis},
abstract = {This paper considers the origins and development of the concept mapping methodology, a summary of its growth, and its influence in a variety of fields. From initial discussions with graduate students, through the rise of the theory-driven approach to program evaluation and the development of a theoretical framework for conceptualization methodology, the paper highlights some of the key early efforts and pilot projects that culminated in a 1989 special issue on the method in Evaluation and Program Planning that brought the method to the attention of the field of evaluation. The paper details the thinking that led to the standard version of the method (the analytic sequence, “bridging” index, and pattern matching) and the development of the software for accomplishing it. A bibliometric analysis shows that the rate of citation continues to increase, where it has grown geographically and institutionally, that the method has been used in a wide variety of disciplines and specialties, and that the literature had an influence on the field. The article concludes with a critical appraisal of some of the key aspects of the approach that warrant further development.}
}
@article{UMAIR2024106224,
title = {Emotion Fusion-Sense (Emo Fu-Sense) – A novel multimodal emotion classification technique},
journal = {Biomedical Signal Processing and Control},
volume = {94},
pages = {106224},
year = {2024},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2024.106224},
url = {https://www.sciencedirect.com/science/article/pii/S1746809424002829},
author = {Muhammad Umair and Nasir Rashid and Umar {Shahbaz Khan} and Amir Hamza and Javaid Iqbal},
keywords = {EEG, ECG, GSR, Respiration amplitude, Body temperature, LSTM, Feature fusion, Modality biasing, Multimodal emotion classification},
abstract = {Human emotions play a vital role in overall well-being. With the advent of advance technologies growing interest has been observed in developing a multimodal emotion classification system that can accurately interpret human emotions. The article presents a comprehensive overview of a multimodal emotion classification system (Emo Fu-Sense) designed to capture the rich and nuanced nature of human emotions. Objective of Emo Fu-Sense is to integrates information from Electrocardiogram (ECG), Galvanic Skin Response (GSR), Electroencephalograph (EEG), respiration amplitude and body temperature to achieve holistic understanding of emotional states. To effectively extract information from multimodal data, designed system employs conventional methods with sophisticated machine learning algorithms including Long Short-Term Memory (LSTM), a variety of Recurrent Neural Network (RNN). Recommended solution extracts column wise features independently from different modalities based on the windowing operation. Finally, feature fusion and modality biasing were used to combine the information from different modalities. The proposed method has not only highlighted the limitations of unimodal system but has achieved a classification accuracy of 92.62 %, with an average F1-Score of 93 % and 9.2 % of Mean Absolute Error (MAE). Obtained results are better than existing state-of-the-art approaches. Evaluation of the multimodal emotion classification system was conducted on MAHNOB-HCI dataset, which encompasses a wide range of emotional expressions across various contexts and individuals. The integration of multiple modalities and advanced machine learning techniques enables a more comprehensive understanding of emotional states and highlights the significance of research and development in the field of affective computing.}
}
@article{GROOTHUIJSEN2024100290,
title = {AI chatbots in programming education: Students’ use in a scientific computing course and consequences for learning},
journal = {Computers and Education: Artificial Intelligence},
volume = {7},
pages = {100290},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100290},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000936},
author = {Suzanne Groothuijsen and Antoine {van den Beemt} and Joris C. Remmers and Ludo W. {van Meeuwen}},
keywords = {AI chatbots, ChatGPT, Programming education, Pair programming, Student learning, Engineering education},
abstract = {Teaching and learning in higher education require adaptation following students' inevitable use of AI chatbots. This study contributes to the empirical literature on students' use of AI chatbots and how they influence learning. The aim of this study is to identify how to adapt programming education in higher engineering education. A mixed-methods case study was conducted of a scientific computing course in a Mechanical Engineering Master's program at a Eindhoven University of Technology in the Netherlands. Data consisted of 29 student questionnaires, a semi-structured group interview with three students, a semi-structured interview with the teacher, and 29 students' grades. Results show that students used ChatGPT for error checking and debugging of code, increasing conceptual understanding, generating, and optimizing solution code, explaining code, and solving mathematical problems. While students reported advantages of using ChatGPT, the teacher expressed concerns over declining code quality and student learning. Furthermore, both students and teacher perceived a negative influence from ChatGPT usage on pair programming, and consequently on student collaboration. The findings suggest that learning objectives should be formulated in more detail, to highlight essential programming skills, and be expanded to include the use of AI tools. Complex programming assignments remain appropriate in programming education, but pair programming as a didactic approach should be reconsidered in light of the growing use of AI Chatbots.}
}
@article{BAUMANN20101561,
title = {Numerical solution of level dependent quasi-birth-and-death processes},
journal = {Procedia Computer Science},
volume = {1},
number = {1},
pages = {1561-1569},
year = {2010},
note = {ICCS 2010},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2010.04.175},
url = {https://www.sciencedirect.com/science/article/pii/S1877050910001766},
author = {Hendrik Baumann and Werner Sandmann},
keywords = {Continuous-time Markov chains, Block-tridiagonal generator matrices, Level dependent quasi-birth-and-death processes, Numerical solution, Matrix continued fractions},
abstract = {We consider the numerical computation of stationary distributions for level dependent quasi-birth-and-death processes. An algorithm based on matrix continued fractions is presented and compared to standard solution techniques. Its computational efficiency and numerical stability is demonstrated by numerical examples.}
}
@article{BOLANOS201926,
title = {Energy, uncertainty, and entrepreneurship: John D Rockefeller’s sequential approach to transaction costs management in the early oil industry},
journal = {Energy Research & Social Science},
volume = {55},
pages = {26-34},
year = {2019},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2019.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S2214629618304444},
author = {Jose A. Bolanos},
keywords = {Uncertainty, Rockefeller, Standard Oil Company, Entrepreneurship, Transaction costs},
abstract = {This article delves into the challenge of successful entrepreneurship in the energy industry under conditions of uncertainty by examining the case of John D Rockefeller’s Standard Oil Company, which rapidly seized control of an initially-uncertain industry. It finds that Rockefeller cemented control through a willingness to internalise contextual uncertainty (related to the nature of the energy business) as a stepping stone to managing contractual uncertainty (related to transactions with other parties). This finding suggests that thinking sequentially about the management of contextual and contractual uncertainty aids entrepreneurial success in the field of energy. This suggestion accords with standing calls in the transaction costs literature, which means that findings may generalise to some extent. However, the exploratory nature of the analysis implies the need for further research about the argument’s compatibility with modern energy practices and its generalisability.}
}
@incollection{BLOCKLEY2013229,
title = {9 - Earthquake risk management of civil infrastructure: integrating soft and hard risks},
editor = {S. Tesfamariam and K. Goda},
booktitle = {Handbook of Seismic Risk Analysis and Management of Civil Infrastructure Systems},
publisher = {Woodhead Publishing},
pages = {229-254},
year = {2013},
series = {Woodhead Publishing Series in Civil and Structural Engineering},
isbn = {978-0-85709-268-7},
doi = {https://doi.org/10.1533/9780857098986.2.229},
url = {https://www.sciencedirect.com/science/article/pii/B9780857092687500098},
author = {D. Blockley},
keywords = {seismic risk, uncertainty, safety, systems thinking, integration, hard and soft systems},
abstract = {Abstract:
Risk is an inevitable part of all human activity. Similarly sized earthquakes can have very different impacts in different countries depending on the degree of engineering input into the design and construction of the facilities. In this chapter we will propose an approach based on systems thinking and new systems boundaries. We will identify and characterise three different sources of uncertainty: hard physical system parameter uncertainty, hard system model uncertainty and soft system human uncertainty. We will explore ways in which evidence from previously disparate sources can be managed in an integrated way.}
}
@article{SUWA1997385,
title = {What do architects and students perceive in their design sketches? A protocol analysis},
journal = {Design Studies},
volume = {18},
number = {4},
pages = {385-403},
year = {1997},
note = {Descriptive models of design},
issn = {0142-694X},
doi = {https://doi.org/10.1016/S0142-694X(97)00008-2},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X97000082},
author = {Masaki Suwa and Barbara Tversky},
keywords = {architectural design, design cognition, drawings, perception, protocol analysis},
abstract = {The present research aims at examining what information architects think of and read off from their own freehand sketches, and at revealing how they perceptually interact with and benefit from sketches. We explored this in a protocol analysis of retrospective reports; each participant worked on an architectural design task while drawing freehand sketches and later reported what she/he had been thinking of during the design task. This research lies within the scope of examinations of why freehand sketches as external representation are essential for crystallizing design ideas in early design processes.}
}
@article{REN2024122745,
title = {Pooling-based Visual Transformer with low complexity attention hashing for image retrieval},
journal = {Expert Systems with Applications},
volume = {241},
pages = {122745},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122745},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423032475},
author = {Huan Ren and Jiangtao Guo and Shuli Cheng and Yongming Li},
keywords = {Pooling-based Visual Transformers, Attention, Image retrieval, Deep hash},
abstract = {Retrieving similar images is becoming an urgent need for us with the continuous growth of large-scale data. However, whether the dominant image retrieval methods are Convolutional Neural Networks (CNNs) or the recently emerging Visual Transformer (ViT), their complex computation, insufficient feature extraction, and mismatched weights greatly influence the efficiency and retrieval accuracy. In this paper, we propose a Pooling-based Visual Transformer with low complexity attention hashing (PTLCH) for image retrieval. First, a backbone network for Pooling-based Vision Transformer (PiT) feature learning is designed to combine the pooling in CNN and the ViT to achieve the purpose of spatial dimensionality reduction while learning rich semantic information. Second, a low complexity attention (LCA) module is incorporated into PiT, which works by combining the positional deviation with the key matrix and the value matrix and then matrix multiplying with the query matrix. LCA explores rich contextual information to enable network learning of more granular feature information. Finally, a new loss framework is proposed where we focus on the effect of difficult and erroneous samples on accuracy. By using different improved cross-entropy losses, better weights are assigned to the learning samples of our network, which effectively improves learning hash coding. We have conducted extensive experiments on three public datasets, CIFAR-10, ImageNet100, and MS-COCO, which have the highest mean average precision of 93.76%, 92.62%, and 90.60%, respectively.}
}
@article{YUTING2023e15851,
title = {Current status of digital humanities research in Taiwan},
journal = {Heliyon},
volume = {9},
number = {5},
pages = {e15851},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e15851},
url = {https://www.sciencedirect.com/science/article/pii/S240584402303058X},
author = {Pan Yuting and Jiang Yinfeng and Zhang Jingli},
keywords = {Digital humanities, Text mining, Social network analysis, GIS},
abstract = {Purpose
Review the current research status of the theory, techniques, and practice of digital humanities in Taiwan.
Methods
Select the 8 issues of the Journal of Digital Archives and Digital Humanities from its inception in 2018–2021, and the papers of the 5-year International Conference of Digital Archives and Digital Humanities from 2017 to 2021 as the research data, and conduct text analysis of the collected 252 articles.
Results
From the statistical analysis results, the number of practical articles is the largest, followed by tools and techniques, and the least number of theoretical articles. Text tools and literature research are the most concentrated aspects of digital humanities research in Taiwan.
Limitations
It still needs to be further compared with the current research status of digital humanities in Mainland China.
Conclusions
Digital humanities in Taiwan focuses on the development of tools and techniques, and practical applications of literature and history, and focuses on Taiwan's native culture to form its own digital humanities research characteristics.}
}
@article{SRIDHAR2022113207,
title = {Extraction techniques in food industry: Insights into process parameters and their optimization},
journal = {Food and Chemical Toxicology},
volume = {166},
pages = {113207},
year = {2022},
issn = {0278-6915},
doi = {https://doi.org/10.1016/j.fct.2022.113207},
url = {https://www.sciencedirect.com/science/article/pii/S0278691522004057},
author = {Adithya Sridhar and Vijay Vaishampayan and P. {Senthil Kumar} and Muthamilselvi Ponnuchamy and Ashish Kapoor},
keywords = {Extraction, Food, Modelling, Optimization, Sustainability},
abstract = {This review presents critical evaluation of the key parameters that affect the extraction of targeted components, giving due consideration to safety and environmental aspects. The crucial aspects of the extraction technologies along with protocols and process parameters for designing unit operations have been emphasized. The parameters like solvent usage, substrate type, concentration, particle size, temperature, quality and storage of extract as well as stability of extraction have been elaborately discussed. The process optimization using mathematical and computational modeling highlighting information and communication technologies have been given importance aiming for a green and sustainable industry level scaleup. The findings indicate that the extraction processes vary significantly depending on the category of food and its structure. There is no single extraction method or universal set of process conditions identified for extracting all value-added products from respective sources. A comprehensive understanding of process parameters and their optimization as well as synergistic combination of multiple extraction processes can aid in enhancement of the overall extraction efficiency. Future efforts must be directed toward the design of integrated unit operations that cause minimal harm to the environment along with investigations on economic feasibility to ensure sustainable extraction systems.}
}
@article{DIESTER20242265,
title = {Internal world models in humans, animals, and AI},
journal = {Neuron},
volume = {112},
number = {14},
pages = {2265-2268},
year = {2024},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2024.06.019},
url = {https://www.sciencedirect.com/science/article/pii/S0896627324004549},
author = {Ilka Diester and Marlene Bartos and Joschka Bödecker and Adam Kortylewski and Christian Leibold and Johannes Letzkus and Matthew M. Nour and Monika Schönauer and Andrew Straw and Abhinav Valada and Andreas Vlachos and Thomas Brox},
abstract = {Summary
How do brains—biological or artificial—respond and adapt to an ever-changing environment? In a recent meeting, experts from various fields of neuroscience and artificial intelligence met to discuss internal world models in brains and machines, arguing for an interdisciplinary approach to gain deeper insights into the underlying mechanisms.}
}
@incollection{AKAN20253,
title = {Chapter 0 - From the ground up!},
editor = {Aydin Akan and Luis F. Chaparro},
booktitle = {Signals and Systems Using MATLAB ® (Fourth Edition)},
publisher = {Academic Press},
edition = {Fourth Edition},
pages = {3-62},
year = {2025},
isbn = {978-0-443-15709-7},
doi = {https://doi.org/10.1016/B978-0-44-315709-7.00009-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443157097000094},
author = {Aydin Akan and Luis F. Chaparro},
keywords = {Signals and systems, mathematical models, digital signal processing applications, concrete mathematics, complex variables, system dynamics, MATLAB},
abstract = {This chapter provides an overview of the material in the book, briefly illustrates the applications and highlights the mathematical background needed to understand the analysis of signals and systems. A signal is a function of time like a voice signal, or of space like an image, or of time and space like a video. A system then is a mathematical model of a device, just like the ordinary differential equations representing circuits. We illustrate the importance of the theory of signals and systems by means of practical applications, hint to how to implement them, and connect concepts in Calculus with more concrete mathematics from a computational point of view—using computers. A review of complex variables and their connection with the dynamics of systems is given. We end the chapter with a soft introduction to MATLAB®, a widely used high-level computational tool for analysis and design.}
}
@article{BETTINGER2023105459,
title = {Conceptual foundations of physiological regulation incorporating the free energy principle and self-organized criticality},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {155},
pages = {105459},
year = {2023},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2023.105459},
url = {https://www.sciencedirect.com/science/article/pii/S0149763423004281},
author = {Jesse S. Bettinger and Karl J. Friston},
keywords = {Physiological regulation, Homeostasis, Allostasis, Variational systems, Free energy principle, Criticality, Griffiths region, Complex adaptive systems, Dynamic stability, Metastability, Control theory, Neuro-immunology, Computational psychiatry, Resilience},
abstract = {Bettinger, J. S., K. J. Friston. Conceptual Foundations of Physiological Regulation incorporating the Free Energy Principle & Self-Organized Criticality. NEUROSCI BIOBEHAV REV 23(x) 144-XXX, 2022. Since the late nineteen-nineties, the concept of homeostasis has been contextualized within a broader class of "allostatic" dynamics characterized by a wider-berth of causal factors including social, psychological and environmental entailments; the fundamental nature of integrated brain-body dynamics; plus the role of anticipatory, top-down constraints supplied by intrinsic regulatory models. Many of these evidentiary factors are integral in original descriptions of homeostasis; subsequently integrated; and/or cite more-general operating principles of self-organization. As a result, the concept of allostasis may be generalized to a larger category of variational systems in biology, engineering and physics in terms of advances in complex systems, statistical mechanics and dynamics involving heterogenous (hierarchical/heterarchical, modular) systems like brain-networks and the internal milieu. This paper offers a three-part treatment. 1) interpret "allostasis" to emphasize a variational and relational foundation of physiological stability; 2) adapt the role of allostasis as "stability through change" to include a "return to stability" and 3) reframe the model of homeostasis with a conceptual model of criticality that licenses the upgrade to variational dynamics.}
}
@article{MAHAJAN2022103942,
title = {Participatory resilience: Surviving, recovering and improving together},
journal = {Sustainable Cities and Society},
volume = {83},
pages = {103942},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2022.103942},
url = {https://www.sciencedirect.com/science/article/pii/S2210670722002633},
author = {Sachit Mahajan and Carina I. Hausladen and Javier {Argota Sánchez-Vaquerizo} and Marcin Korecki and Dirk Helbing},
keywords = {Risk, Resilience, Participation, Collective intelligence, Connective action, Sustainability},
abstract = {In the context of urbanization and a growing population, cities and citizens are becoming more exposed and vulnerable to social and environmental changes, ranging from natural disasters like earthquakes and floods to uncertainties caused by issues related to climate change and complex social dynamics or even pandemics. There have been many debates about implementing resilience thinking that allow cities and communities to prepare for possible stresses and shocks. Although there are sets of frameworks aimed at building inclusive resilience strategies fostering participation and engagement, there is limited resilience-related literature on how to conceptualize participation. Through an extensive review of various kinds of publications on resilience, policy documents, and case studies, which emphasize the concepts of participation, coordination, and co-creation, this review explores and investigates how citizen participation is discussed and applied in the context of participatory resilience. We conclude that participatory approaches possess a great potential to enhance multi-stakeholder cooperation, social innovation, and capacity building for resilience. Realization of the potential of participatory resilience will remain limited, however, unless participation strategies and frameworks are made more transparent, inclusive, and context-sensitive.}
}
@article{MOHAMED2023108104,
title = {A discrete-based multi-scale modeling approach for the propagation of seismic waves in soils},
journal = {Soil Dynamics and Earthquake Engineering},
volume = {173},
pages = {108104},
year = {2023},
issn = {0267-7261},
doi = {https://doi.org/10.1016/j.soildyn.2023.108104},
url = {https://www.sciencedirect.com/science/article/pii/S0267726123003494},
author = {Tarek Mohamed and Jérôme Duriez and Guillaume Veylon and Laurent Peyras and Patrick Soulat},
keywords = {Multi-scale, DEM, Toyoura sand, Seismic waves propagation, Bounding surface plasticity, Inertial effect},
abstract = {A three-dimensional multi-scale discrete–continuum model (Finite Volume Method × Discrete Element Method, FVM × DEM) is developed for a discrete-based description of the mechanical behavior of granular soils in boundary value problems (BVPs). In such a scheme, the constitutive response of the material is derived through direct DEM computations on a representative volume element attached to each mesh element. The developed multi-scale approach includes the inertial effect in the stress homogenization formulation and serves to study the mechanism of propagation of seismic waves, in comparison with a more classical BVP simulation that adopts an advanced bounding surface plasticity model “P2PSand”. We start with a detailed and fair calibration and validation of these two models against laboratory tests for Toyoura sand under monotonic and cyclic loading. Then, the performance of the two approaches is compared for the case of a seismic wave loading passing through a saturated soil column with different relative densities, revealing several differences between the results of the two models.}
}
@article{ANAZKHAN2023,
title = {Metal additive manufacturing of alloy structures in architecture: A review on achievements and challenges},
journal = {Materials Today: Proceedings},
year = {2023},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2023.05.192},
url = {https://www.sciencedirect.com/science/article/pii/S2214785323028183},
author = {Muhammed {Anaz Khan} and Aysha Latheef},
keywords = {Architecture, Additive manufacturing, Facades, Construction industry, Structural engineering},
abstract = {There is a growing trend in modern architecture towards asymmetrical building layouts. This development can be attributed to the proliferation of cutting-edge production methods and structurally novel approaches. With the advent of computational design and digital manufacturing techniques, formerly static designs may now be modified to create one-of-a-kind, high-performance prototypes. Metal additive manufacturing (MAM) is a cutting-edge production method that paves the way for new architectural designs, construction processes, and materials. Interesting possibilities for optimising structural elements are made possible by MAM because of its ability to deposit material just where it is structurally necessary. The construction sector places a premium on directed energy deposition additive manufacturing (DED AM) and wire arc additive manufacturing (WAAM). The adaptability of these two technologies in the construction industry and their possible future applications are explored in this literature review.}
}
@article{AVEN2002195,
title = {Implementing the Bayesian paradigm in risk analysis},
journal = {Reliability Engineering & System Safety},
volume = {78},
number = {2},
pages = {195-201},
year = {2002},
issn = {0951-8320},
doi = {https://doi.org/10.1016/S0951-8320(02)00161-8},
url = {https://www.sciencedirect.com/science/article/pii/S0951832002001618},
author = {T. Aven and J.T. Kvaløy},
keywords = {Bayesian, Risk analysis, True probabilities},
abstract = {The Bayesian paradigm comprises a unified and consistent framework for analyzing and expressing risk. Yet, we see rather few examples of applications where the full Bayesian setting has been adopted with specifications of priors of unknown parameters. In this paper, we discuss some of the practical challenges of implementing Bayesian thinking and methods in risk analysis, emphasizing the introduction of probability models and parameters and associated uncertainty assessments. We conclude that there is a need for a pragmatic view in order to ‘successfully’ apply the Bayesian approach, such that we can do the assignments of some of the probabilities without adopting the somewhat sophisticated procedure of specifying prior distributions of parameters. A simple risk analysis example is presented to illustrate ideas.}
}
@article{LI20151,
title = {The association between alexithymia as assessed by the 20-item Toronto Alexithymia Scale and depression: A meta-analysis},
journal = {Psychiatry Research},
volume = {227},
number = {1},
pages = {1-9},
year = {2015},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2015.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0165178115000761},
author = {Shuwen Li and Bin Zhang and Yufang Guo and Jingping Zhang},
keywords = {Alexithymia, Depression, Meta-analysis, Questionnaire},
abstract = {Patients with depression exhibit high rates of alexithymia, representing a major public health concern. We sought to examine relationships between depression severity and alexithymia as assessed by the Toronto Alexithymia Scale (TAS-20) and the TAS-20 subscales of difficulty identifying feelings (DIF), difficulty describing feelings (DDF), and externally oriented thinking (EOT). Potentially relevant studies were obtained independently by two reviewers. Chi-square statistics based on the Q-test and I2 index assessed statistical heterogeneity between studies. Subgroup analyses were mainly used to explore sources of heterogeneity. Begg׳s test and Duval and Tweedie’ trim and fill were used to assess potential publication bias. Altogether, 3572 subjects from 20 study groups across 19 studies were included. Medium relationships were observed between depression and TAS-total score (TAS-TS), DIF, and DDF. There was also a weak relationship between EOT and depression. Subgroup analyses showed a stronger correlation between TAS-TS and depression assessed by self-reported tools than that assessed by the Hamilton Rating Scale for Depression. The heterogeneity significantly decreased only in the subgroup analysis by depression tool. We conclude that alexithymia, as assessed by the TAS-20 and its subscales DIF and DDF, is closely related to depression. These relationships were affected by depression measurement tools.}
}
@article{KNIGHT2020100624,
title = {Researching the future of purchasing and supply management: The purpose and potential of scenarios},
journal = {Journal of Purchasing and Supply Management},
volume = {26},
number = {3},
pages = {100624},
year = {2020},
issn = {1478-4092},
doi = {https://doi.org/10.1016/j.pursup.2020.100624},
url = {https://www.sciencedirect.com/science/article/pii/S1478409220300777},
author = {Louise Knight and Joanne Meehan and Efstathios Tapinos and Laura Menzies and Alexandra Pfeiffer},
keywords = {Scenario planning, Procurement, Future studies, Prescience, Critical management, Covid-19 coronavirus},
abstract = {Drawing on prior research, the value of scenario planning as a methodology for researching the future of purchasing and supply management (PSM) is explored. Using three criteria of research quality – rigour, originality and significance – it is shown how developing scenarios and analysing their implications present new, important research opportunities for PSM academics, practitioners, and leaders of the profession. Researching the future of PSM supports the identification of uncertainties and anticipates change across many units and levels of analysis of interest to PSM scholars and practitioners, such as the profession/discipline, markets/sectors, or organisations. Scenarios are particularly effective for: considering how the complex interaction of macro-environmental factors affects the PSM context; avoiding incremental thinking; surfacing assumptions and revealing significant blind spots. PSM research using scenarios aligns with Corley and Gioia's (2011) call for prescience-oriented research in which academics aim for more impactful research, enhancing sense-giving potential and theoretical relevance to practice, to better perform their adaptive role in society.}
}
@incollection{GOYAMARTINEZ2016171,
title = {Chapter 8 - The Emulation of Emotions in Artificial Intelligence: Another Step into Anthropomorphism},
editor = {Sharon Y. Tettegah and Safiya Umoja Noble},
booktitle = {Emotions, Technology, and Design},
publisher = {Academic Press},
address = {San Diego},
pages = {171-186},
year = {2016},
series = {Emotions and Technology},
isbn = {978-0-12-801872-9},
doi = {https://doi.org/10.1016/B978-0-12-801872-9.00008-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128018729000089},
author = {Mariana Goya-Martinez},
keywords = {Emotions, Artificial intelligence, Anthropomorphism, Human emulation},
abstract = {The conceptualization of emotions as unique to biological organisms and a flaw in human intelligence has been challenged in recent times. Artificial intelligence researchers defy the idea of emotions as solely biological and try to incorporate them in their agents' design. Emotions and feelings are now considered cognitive percepts that play an important role in decision making processes. Based on artificial intelligence discourses and inventions, this chapter explores how emotions are defined in the realm of artificial intelligence, what is their role in the agents' performance, and how and why they are being emulated. From improving human-machine interaction and achieving empathy, to providing machines with cognitive shortcuts for rational thinking, emotions could be a key element in building a coherent system of thought capable of organizing several kinds of knowledge. This could provide a way to finally pass the Turing test or to provide a smooth transformation of the human nature when we finally merge with the machines. The emulation of emotions is just another step in the objective of replicating the human, promoting a higher level of anthropomorphism in the field of artificial intelligence.}
}
@article{MANDEVILLE1997397,
title = {The effect of teacher certification and task level on mathematics achievement},
journal = {Teaching and Teacher Education},
volume = {13},
number = {4},
pages = {397-407},
year = {1997},
issn = {0742-051X},
doi = {https://doi.org/10.1016/S0742-051X(96)00031-5},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X96000315},
author = {Garrett K Mandeville and Qiduan Liu},
abstract = {In this study it was hypothesized that the degree of content area preparation of seventh grade mathematics teachers would differentially effect student performance as a function of the level of the mathematics tasks used to assess that performance. More specifically, students of high MATHPREP teachers were hypothesized to outperform those of low MATHPREP teachers on the higher level tasks. The sample consisted of over 9000 seventh grade students from 33 matched pairs of schools whose teachers differed on level of mathematics preparation. Aggregate measures of achievement for the students at each school on test items representing three ordered levels of thinking were obtained. The primary hypothesis was addressed using a single df interaction contrast and produced a statistically and practically significant result in the hypothesized direction. Implications for teacher preparation and the hiring practices of school administrators are considered.}
}
@article{GIGERENZER2004587,
title = {Mindless statistics},
journal = {The Journal of Socio-Economics},
volume = {33},
number = {5},
pages = {587-606},
year = {2004},
note = {Statistical Significance},
issn = {1053-5357},
doi = {https://doi.org/10.1016/j.socec.2004.09.033},
url = {https://www.sciencedirect.com/science/article/pii/S1053535704000927},
author = {Gerd Gigerenzer},
keywords = {Rituals, Collective illusions, Statistical significance, Editors, Textbooks},
abstract = {Statistical rituals largely eliminate statistical thinking in the social sciences. Rituals are indispensable for identification with social groups, but they should be the subject rather than the procedure of science. What I call the “null ritual” consists of three steps: (1) set up a statistical null hypothesis, but do not specify your own hypothesis nor any alternative hypothesis, (2) use the 5% significance level for rejecting the null and accepting your hypothesis, and (3) always perform this procedure. I report evidence of the resulting collective confusion and fears about sanctions on the part of students and teachers, researchers and editors, as well as textbook writers.}
}
@article{VEZOLI2021117479,
title = {Cortical hierarchy, dual counterstream architecture and the importance of top-down generative networks},
journal = {NeuroImage},
volume = {225},
pages = {117479},
year = {2021},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2020.117479},
url = {https://www.sciencedirect.com/science/article/pii/S1053811920309642},
author = {Julien Vezoli and Loïc Magrou and Rainer Goebel and Xiao-Jing Wang and Kenneth Knoblauch and Martin Vinck and Henry Kennedy},
keywords = {Non-human primate, Human, Brain, Electrophysiology, Anatomy, Modeling, Connectivity, Predictive coding, Perception, Consciousness},
abstract = {Hierarchy is a major organizational principle of the cortex and underscores modern computational theories of cortical function. The local microcircuit amplifies long-distance inter-areal input, which show distance-dependent changes in their laminar profiles. Statistical modeling of these changes in laminar profiles demonstrates that inputs from multiple hierarchical levels to their target areas show remarkable consistency, allowing the construction of a cortical hierarchy based on a principle of hierarchical distance. The statistical modeling that is applied to structure can also be applied to laminar differences in the oscillatory coherence between areas thereby determining a functional hierarchy of the cortex. Close examination of the anatomy of inter-areal connectivity reveals a dual counterstream architecture with well-defined distance-dependent feedback and feedforward pathways in both the supra- and infragranular layers, suggesting a multiplicity of feedback pathways with well-defined functional properties. These findings are consistent with feedback connections providing a generative network involved in a wide range of cognitive functions. A dynamical model constrained by connectivity data sheds insight into the experimentally observed signatures of frequency-dependent Granger causality for feedforward versus feedback signaling. Concerted experiments capitalizing on recent technical advances and combining tract-tracing, high-resolution fMRI, optogenetics and mathematical modeling hold the promise of a much improved understanding of lamina-constrained mechanisms of neural computation and cognition. However, because inter-areal interactions involve cortical layers that have been the target of important evolutionary changes in the primate lineage, these investigations will need to include human and non-human primate comparisons.}
}
@article{SECCHI2024105891,
title = {Modeling and theorizing with agent-based sustainable development},
journal = {Environmental Modelling & Software},
volume = {171},
pages = {105891},
year = {2024},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2023.105891},
url = {https://www.sciencedirect.com/science/article/pii/S1364815223002773},
author = {D. Secchi and V. Grimm and D.B. Herath and F. Homberg},
keywords = {Sustainable development, Theory development, Human–environment interaction, Agent-based modeling, Common language, ODD protocol},
abstract = {Sustainable development is an expression that permeates large areas of knowledge. For it to be meaningful, environmental aspects must be considered as intertwined with economic and social aspects. This is a multidisciplinary effort that is made challenging by the task of synthesizing the many emerging contributions. This has limited theory development where the definition of mechanisms, assumptions, dynamics and the determination of the entities involved are largely left to the reader’s imagination. We suggest to engage with the rationale of agent-based modeling to better define the assumptions, mechanisms, and boundaries of sustainable development. For this, the O-part of the widely used ODD protocol for describing agent-based models (ABM) provides a standardized structure, which we here augment to OsDD to specifically take into account sustainability issues. Even without formulating and implementing the full ABM, using OsDD requires to be explicit about the mechanisms, assumptions, dynamics and the entities involved and thereby provides a common language for theory development.}
}
@article{SU2022100072,
title = {Artificial Intelligence (AI) in early childhood education: Curriculum design and future directions},
journal = {Computers and Education: Artificial Intelligence},
volume = {3},
pages = {100072},
year = {2022},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2022.100072},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X22000273},
author = {Jiahong Su and Yuchun Zhong},
keywords = {Artificial intelligence (AI), AI curriculum, Kindergarten, Early childhood education, AI literacy},
abstract = {With the rapid technological development of society brought on by Artificial Intelligence (AI), the demand for AI-literate workers will increase in the future. It is critical to develop the next generation's AI competencies and educate them about how to work with and use AI. Previous studies on AI were predominantly focused on secondary and university education; however, research on the Artificial Intelligence curriculum in early childhood education is scarce. Due to the lack of conformity on the standardisation of AI curriculum for early childhood education, this study examines the AI curriculum for kindergarten children using the framework which consists of four key components, including (1) aims, goals, objectives, or declarations of outcome, (2) subject matter, domains, or content, (3) methods or procedure, (4) evaluation and assessment. We recommend that AI literacy be achieved by three competencies: AI Knowledge, AI Skill, and AI Attitude. The employment of a social robot as a learning companion and programmable artifact was proven to be helpful in assisting young children in grasping AI principles. We also discovered which teaching methods had the most greatest influence on students' learning. We recommend problem-based learning for future AI education based on the findings.}
}
@article{SOEMER201912,
title = {Text difficulty, topic interest, and mind wandering during reading},
journal = {Learning and Instruction},
volume = {61},
pages = {12-22},
year = {2019},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2018.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S095947521830389X},
author = {Alexander Soemer and Ulrich Schiefele},
keywords = {Mind wandering, Reading comprehension, Interest, Text difficulty},
abstract = {The present article deals with the question of how the difficulty of a text affects a reader's tendency to engage in task-unrelated thinking (mind wandering) during reading, and the potential role of topic interest as a mediator of the relation between text difficulty and mind wandering. Two-hundred and sixteen participants read three texts with each text either being easy, moderate, or difficult in terms of readability and cohesion. From time to time during reading, participants were interrupted and required to indicate whether they were voluntarily or involuntarily engaging in mind wandering. After reading each text, they rated their interest in and familiarity with the topic, and subsequently answered a number of comprehension questions. The results revealed that reading difficult texts increased both voluntary and involuntary mind wandering and this increase partially explained the negative relation between text difficulty and comprehension. Furthermore, topic interest fully mediated the effect of text difficulty on both forms of mind wandering.}
}
@article{TAYLOR2022109098,
title = {Path integral radiative transfer via polyline representation allowing GPU implementation},
journal = {Annals of Nuclear Energy},
volume = {173},
pages = {109098},
year = {2022},
issn = {0306-4549},
doi = {https://doi.org/10.1016/j.anucene.2022.109098},
url = {https://www.sciencedirect.com/science/article/pii/S0306454922001335},
author = {Brennen Taylor and John Keyser and Jerry Tessendorf},
keywords = {Radiative transport, Feynman path integral, Monte Carlo, Particle transport, CUDA},
abstract = {Many research areas require simulating particle transport through scattering media. For instance, radiative transfer is useful for computer graphics and for neutron transport in nuclear physics. These transport simulations tend to be computationally expensive for problems involving large amounts of multiple scattering in generic geometries, requiring significant time to compute. Finding a fast solution for these types of problems remains an open area for research. Previous work shows that radiative transfer can be represented as a Feynman Path Integral over all paths between two points in a space. The path integral assigns a weight to each path based on the local curvature of the path, accumulating a transport kernel by summing the weights of all of the paths. Previous work demonstrated a Monte Carlo method for computing the radiative transfer Feynman path integral via repeatedly perturbing paths to generate new paths, using a discrete Frenet-Serret framework and an expensive root-solve calculation. The approach is highly parallelizable on the CPU, however computations require a supercomputer to complete in reasonable time. While a GPU implementation was considered for this previous approach, the root-solve and path representation mapped poorly to the GPU, hindering a reasonable implementation. In the present work, we propose a representation of paths as polylines, and a new curve perturbation method that guarantees production of a new unique path each execution while maintaining the boundary conditions of each path, but without the time-consuming root-solve. The new perturbation method’s structure maps well to the GPU, allowing implementation in CUDA which outperforms the CPU, and allows the utilization of GPU hardware.}
}
@article{SMOLARCZYK2024100669,
title = {Let’s get them on board: Focus group discussions with adolescents on empowering leisure engagement in Fab Labs and makerspaces},
journal = {International Journal of Child-Computer Interaction},
volume = {41},
pages = {100669},
year = {2024},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100669},
url = {https://www.sciencedirect.com/science/article/pii/S2212868924000370},
author = {Kathrin Smolarczyk and Marios Mouratidis and Sophie Uhing and Rolf Becker and Stephan Kröner},
keywords = {Maker activities, Leisure, Youth, Focus groups, Sustainability},
abstract = {Makerspaces and Fab Labs are growing in number all over the world, holding the potential to empower children and adolescents. They form an important pathway to provide young people with access to digital manufacturing technologies while fostering self-determination, collaboration, and creativity. We explore how the engagement in Fab Lab based leisure maker activities may be promoted, taking into account both the perspectives of adolescents and the potential of surrounding systems. For this, we conducted focus group discussions with N = 61 non-maker, adolescent girls and boys from 6th to 9th grade, to scrutinize hindering and promoting factors of their engagement in leisure maker activities, and to explore their preferences regarding the involvement of parents, teachers and peers while considering the ecological sustainability of the activities. A reflexive thematic analysis identified the hindering and promoting factors across different aspects of maker activities such as the purpose, location and setting, content, and learning processes. Implications for the promotion and design of maker activities, as well as implications for further research, are discussed.}
}
@article{SAMSONOVICH2012100,
title = {On a roadmap for the BICA Challenge},
journal = {Biologically Inspired Cognitive Architectures},
volume = {1},
pages = {100-107},
year = {2012},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2012.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X12000126},
author = {Alexei V. Samsonovich},
keywords = {Human-level AI, Cognitive architectures, Turing test, Newell list, Critical mass},
abstract = {The BICA Challenge is the challenge to create a general-purpose, real-life computational equivalent of the human mind using an approach based on biologically inspired cognitive architectures (BICA). To solve it, we need to understand at a computational level how natural intelligent systems develop their cognitive, metacognitive and learning functions. The solution is expected to lead us to a breakthrough to intelligent agents integrated into the human society as its members. This outcome has the potential to solve many problems of the modern world. The article starts from the roadmap proposed by Dr. James Albus for a national program unifying artificial intelligence, neuroscience and cognitive science. The BICA Challenge is introduced in this context as a waypoint on the expanded roadmap. The gap between the state of the art and challenge demands is analyzed. Specific problems and barriers are identified, an approach to overcoming them is proposed, and an ultimate practical criterion for success is formulated. It is estimated that the BICA Challenge can be solved within a decade.}
}
@incollection{GOLDSCHMIDT202050,
title = {Architecture☆},
editor = {Steven Pritzker and Mark Runco},
booktitle = {Encyclopedia of Creativity (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Oxford},
pages = {50-56},
year = {2020},
isbn = {978-0-12-815615-5},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.23520-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093245235207},
author = {Gabriela Goldschmidt},
keywords = {Architectural design, Architectural education, Culture, Digital design, Form, Function, Ideas, Performance, Starchitect, Style},
abstract = {Architecture is a cultural arena based on ideas, which jointly produce styles and individually, at their best, generate outstanding buildings. In our era architecture is expected to innovate in its forms, while ensuring perfect performance and adaptation to the environment. Form and function handling are rough correlates of originality and practicality, by which we measure design creativity. Architecture is also a product of the technological state of its time. At present we experience computational advances that fundamentally change buildings and the way they are designed. Architectural education is groping to adjust to the changes.}
}
@incollection{VOIT20211,
title = {Networks and Dynamic Models in Systems Medicine: Overview},
editor = {Olaf Wolkenhauer},
booktitle = {Systems Medicine},
publisher = {Academic Press},
address = {Oxford},
pages = {1-7},
year = {2021},
isbn = {978-0-12-816078-7},
doi = {https://doi.org/10.1016/B978-0-12-801238-3.11661-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128012383116617},
author = {Eberhard O. Voit},
keywords = {Complexity, Dynamic system, Emergent properties, Exposome, Graph, Network, Personalized medicine, Precision medicine, Predictive health, Reductionism, Systems biology},
abstract = {Systems medicine approaches health and disease as a holistic response of the human body. Its main challenge is the complexity of the web of uncounted processes that govern normal physiology, as well as deviations toward disease. Expanding the vast knowledge amassed by traditional medicine, systems medicine addresses this challenge by adopting and adapting concepts and methods from systems biology and by making use of powerful new datasets stemming, for instance, from wearable devices and electronic health records. Systems medicine shares concepts and goals with exposome research and can be considered the foundation for personalized or precision medicine and predictive health. Systems medicine is in its early infancy, and its character, concepts, and methodologies will probably change over time. Nevertheless, many of its computational approaches will continue to benefit from the representation of health and disease processes as networks and dynamic systems. This overview describes the basic concepts of computational systems medicine and briefly summarizes the subsequent chapters, which discuss methods, challenges, and applications of networks and dynamic models in systems medicine.}
}
@article{DAHMANI2021128847,
title = {Smart circular product design strategies towards eco-effective production systems: A lean eco-design industry 4.0 framework},
journal = {Journal of Cleaner Production},
volume = {320},
pages = {128847},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.128847},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621030432},
author = {Noureddine Dahmani and Khalid Benhida and Amine Belhadi and Sachin Kamble and Said Elfezazi and Sunil Kumar Jauhar},
keywords = {Lean design, Eco-design, Industry 4.0, Circular economy, Sustainable products, Circular business models, Smart circular product design},
abstract = {For industrial enterprises, the transformation to circular business models can be curbed both by operational tools and the lack of relevant data. Lean thinking has offered great flexibility in production processes and systems by challenging mass production practices, resulting in more “Lean” products with less waste. Lean design and Eco-design, associated with Industry 4.0 technologies, can be an efficient structured and methodological approach in developing products based on the circular economy strategies. Indeed, decisions made during the product design stage can significantly impact the sustainability of products throughout their life cycle. Hence, lean design combined with eco-design and Industry 4.0 represents an innovative model to include sustainability throughout the product life cycle. This paper explores the relationship between lean eco-design and I4.0 strategies for designing eco-efficient products based on a literature review. The proposed framework is based on the synergic use of Lean design, Eco-design, and Industry 4.0. It offers the right formula to deliver better and cleaner products using appropriate processes to support manufacturers in designing products, fulfilling customers' needs and expectations. It provides scholars, designers, and managers with valuable insights into deploying strategies to design sustainable products.}
}
@article{BAIDYA2024100680,
title = {Comprehensive survey on resource allocation for edge-computing-enabled metaverse},
journal = {Computer Science Review},
volume = {54},
pages = {100680},
year = {2024},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2024.100680},
url = {https://www.sciencedirect.com/science/article/pii/S1574013724000649},
author = {Tanmay Baidya and Sangman Moh},
keywords = {Augmented reality, Edge computing, Metaverse, Offloading, Resource allocation, Virtual reality},
abstract = {With the rapid evaluation of virtual and augmented reality, massive Internet of Things networks and upcoming 6 G communication give rise to an emerging concept termed the “metaverse,” which promises to revolutionize how we interact with the digital world by offering immersive experiences between reality and virtuality. Edge computing, another novel paradigm, propels the metaverse functionality by enhancing real-time interaction and reducing latency, providing a responsive and seamless virtual environment. However, realizing the full potential of the metaverse requires dynamic and efficient resource-allocation strategies to handle the immense demand for communicational, computational, and storage resources required by its diverse applications. This survey comprehensively explores resource-allocation strategies in the context of an edge-computing-enabled metaverse, investigating various challenges, existing techniques, and emerging trends in this rapidly expanding field. We first explore the underlying metaverse characteristics and pivotal role of edge computing, after which we investigate various types of resources and their key issues and challenges. We also provide a brief discussion on offloading and caching strategies, which are the most prominent research issues in this context. In this study, we compare and analyze 35 different resource-allocation strategies, benchmark 19 algorithms, and investigate their suitability across diverse metaverse scenarios, offering a broader scope than existing surveys. The survey aims to serve as a comprehensive guide for researchers and practitioners, helping them navigate the complexities of resource allocation in the metaverse and supporting the development of more efficient, scalable, and user-centric virtual environments.}
}
@article{ACAR2010405,
title = {Designing insightful inquiring systems for sustainable organizational foresight},
journal = {Futures},
volume = {42},
number = {4},
pages = {405-416},
year = {2010},
note = {Learning the Future Faster},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2009.11.025},
url = {https://www.sciencedirect.com/science/article/pii/S0016328709001992},
author = {William Acar and Douglas A. Druckenmiller},
abstract = {This paper contributes to the theory of collaborative problem solving and strategy design by reviewing the state of the art in the application of problem solving and dialectical methods, and then linking up with analytical and computer-aided approaches. Churchman's concept of dialectical inquiry (DI) is presented, and some major derivatives of DI are reviewed, as well as an integrative method for sustainable insightful foresight developed by the authors called comprehensive situation mapping (CSM). In addition to its dialectical process side, CSM offers computational capabilities for devising and figuring out change scenarios. The theory for the manual application of CSM is summarized; in addition, its recent computerized version is presented and likely future improvements are sketched out in light of a current spate of case studies investigating the user-friendliness of its computerization.}
}
@article{RODRIGUES2021406,
title = {Convolutional Neural Network for Respiratory Mechanics Estimation during Pressure Support Ventilation},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {15},
pages = {406-411},
year = {2021},
note = {11th IFAC Symposium on Biological and Medical Systems BMS 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.10.290},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321016955},
author = {Adriano S. Rodrigues and Marcos R.O.A. Maximo and Marcus H. Victor},
keywords = {Mechanical Ventilation, Respiratory Mechanics, Respiratory Effort, Deep Learning, Convolutional Neural Networks},
abstract = {In mechanically ventilated patients, some lung injuries can be reduced or avoided with therapy individualization, while the lung function is evaluated continuously, breath by breath. However, obtaining information on respiratory mechanics (respiratory system resistance and compliance) in the presence of respiratory effort is challenging, even if using invasive and complex procedures. The contribution of this work is to predict both respiratory system resistance and compliance over time using a convolutional neural network (CNN) and estimate the respiratory effort profile using the respiratory dynamics. Therefore, the approach used in this work was to generate a large amount of simulated data to feed a CNN so it could learn how to predict the correct values of the respiratory system resistance and compliance. Then, the respiratory effort was estimated by solving a first-order linear model. The main results showed a normalized mean squared error of 5.7% for the respiratory system resistance and 11.56% for compliance from Bland-Altman plots derived from the computational simulator. Finally, the method was validated using real data from an active lung simulator within which respiratory mechanics varied, and some ventilator settings were adjusted to mimic actual patient situations. The active lung simulator effort profile was obtained with a normalized mean squared error of 8.31% considering the use of an active lung simulator. The results have shown that the simulated data were valuable for the CNN training, while the performance over the real data suggested that the network was generalized accordingly for estimating respiratory parameters and effort profile.}
}
@article{LOPEZPERSEM2023273,
title = {Conceptual promises and mechanistic challenges of the creative metacognition framework: Comment on “A systematic framework of creative metacognition” by Izabela Lebuda and Mathias Benedek},
journal = {Physics of Life Reviews},
volume = {47},
pages = {273-275},
year = {2023},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2023.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S1571064523001860},
author = {Alizée Lopez-Persem and Marion Rouault and Emmanuelle Volle}
}
@article{BEESON1988297,
title = {Towards a computation system based on set theory},
journal = {Theoretical Computer Science},
volume = {60},
number = {3},
pages = {297-340},
year = {1988},
issn = {0304-3975},
doi = {https://doi.org/10.1016/0304-3975(88)90115-6},
url = {https://www.sciencedirect.com/science/article/pii/0304397588901156},
author = {Michael J. Beeson},
abstract = {An axiomatic theory of sets and rules is formulated, which permits the use of sets as data structures and allows rules to operate on rules, numbers, or sets. We might call it a “polymorphic set theory”. Our theory combines the λ-calculus with traditional set theories. A natural set-theoretic model of the theory is constructed, establishing the consistency of the theory and bounding its proof-theoretic strength, and giving in a sense its denotational semantics. Another model, a natural recursion-theoretic model, is constructed, in which only recursive operations from integers to integers are represented, even though the logic can be classical. Some related philosophical considerations on the notions of set, type, and data structure are given in an appendix.}
}
@article{YUE2023940,
title = {A guidebook of spatial transcriptomic technologies, data resources and analysis approaches},
journal = {Computational and Structural Biotechnology Journal},
volume = {21},
pages = {940-955},
year = {2023},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2023.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S2001037023000156},
author = {Liangchen Yue and Feng Liu and Jiongsong Hu and Pin Yang and Yuxiang Wang and Junguo Dong and Wenjie Shu and Xingxu Huang and Shengqi Wang},
keywords = {Spatial transcriptomic technologies},
abstract = {Advances in transcriptomic technologies have deepened our understanding of the cellular gene expression programs of multicellular organisms and provided a theoretical basis for disease diagnosis and therapy. However, both bulk and single-cell RNA sequencing approaches lose the spatial context of cells within the tissue microenvironment, and the development of spatial transcriptomics has made overall bias-free access to both transcriptional information and spatial information possible. Here, we elaborate development of spatial transcriptomic technologies to help researchers select the best-suited technology for their goals and integrate the vast amounts of data to facilitate data accessibility and availability. Then, we marshal various computational approaches to analyze spatial transcriptomic data for various purposes and describe the spatial multimodal omics and its potential for application in tumor tissue. Finally, we provide a detailed discussion and outlook of the spatial transcriptomic technologies, data resources and analysis approaches to guide current and future research on spatial transcriptomics.}
}
@article{WANG2022477,
title = {Methodology of network pharmacology for research on Chinese herbal medicine against COVID-19: A review},
journal = {Journal of Integrative Medicine},
volume = {20},
number = {6},
pages = {477-487},
year = {2022},
issn = {2095-4964},
doi = {https://doi.org/10.1016/j.joim.2022.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S2095496422000966},
author = {Yi-xuan Wang and Zhen Yang and Wen-xiao Wang and Yu-xi Huang and Qiao Zhang and Jia-jia Li and Yu-ping Tang and Shi-jun Yue},
keywords = {Chinese traditional medicine, Herbal medicine, Network pharmacology, Compound identification, COVID-19},
abstract = {Traditional Chinese medicine, as a complementary and alternative medicine, has been practiced for thousands of years in China and possesses remarkable clinical efficacy. Thus, systematic analysis and examination of the mechanistic links between Chinese herbal medicine (CHM) and the complex human body can benefit contemporary understandings by carrying out qualitative and quantitative analysis. With increasing attention, the approach of network pharmacology has begun to unveil the mystery of CHM by constructing the heterogeneous network relationship of “herb-compound-target-pathway,” which corresponds to the holistic mechanisms of CHM. By integrating computational techniques into network pharmacology, the efficiency and accuracy of active compound screening and target fishing have been improved at an unprecedented pace. This review dissects the core innovations to the network pharmacology approach that were developed in the years since 2015 and highlights how this tool has been applied to understanding the coronavirus disease 2019 and refining the clinical use of CHM to combat it.}
}
@article{KAJIC201935,
title = {The semantic pointer theory of emotion: Integrating physiology, appraisal, and construction},
journal = {Cognitive Systems Research},
volume = {58},
pages = {35-53},
year = {2019},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2019.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S1389041718303838},
author = {Ivana Kajić and Tobias Schröder and Terrence C. Stewart and Paul Thagard},
keywords = {Emotion, Language, Appraisal, Construction, Multi-level mechanisms, Affective computing, Neural engineering framework, Semantic pointers},
abstract = {Emotion theory needs to explain the relationship of language and emotions, and the embodiment of emotions, by specifying the computational mechanisms underlying emotion generation in the brain. We used Chris Eliasmith’s Semantic Pointer Architecture to develop POEM, a computational model that explains numerous important phenomena concerning emotions, including how some stimuli generate immediate emotional reactions, how some emotional reactions depend on cognitive evaluations, how bodily states influence the generation of emotions, how some emotions depend on interactions between physiological inputs and cognitive appraisals, and how some emotional reactions concern syntactically complex representations. We contrast our theory with current alternatives, and discuss some possible applications to individual and social emotions.}
}
@article{RAYBOURN2014471,
title = {A new paradigm for serious games: Transmedia learning for more effective training and education},
journal = {Journal of Computational Science},
volume = {5},
number = {3},
pages = {471-481},
year = {2014},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2013.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1877750313001014},
author = {Elaine M. Raybourn},
keywords = {Transmedia learning, Serious games, Transmedia campaigns, Storytelling, Social media, Data mining, xAPI, MOOC},
abstract = {Serious games present a relatively new approach to training and education for international organizations such as NATO (North Atlantic Treaty Organization), non-governmental organizations (NGOs), the U.S. Department of Defense (DoD) and the U.S. Department of Homeland Security (DHS). Although serious games are often deployed as stand-alone solutions, they can also serve as entry points into a comprehensive training pipeline in which content is delivered via different media to rapidly scale immersive training and education for mass audiences. The present paper introduces a new paradigm for more effective and scalable training and education called transmedia learning. Transmedia learning leverages several new media trends including the peer communications of social media, the scalability of massively openonline course (MOOCs), and the design of transmedia storytelling used by entertainment, advertising, and commercial game industries to sustain audience engagement. Transmedia learning is defined as the scalable system of messages representing a narrative or core experience that unfolds from the use of multiple media, emotionally engaging learners by involving them personally in the story. In the present paper, we introduce the transmedia learning paradigm as offering more effective use of serious games for training and education. This approach is consistent with the goals of international organizations implementing approaches similar to those described by the Army Learning Model (ALM) to deliver training and education to Soldiers across multiple media. We discuss why the human brain is wired for transmedia learning and demonstrate how the Simulation Experience Design Method can be used to create transmedia learning story worlds for serious games. We describe how social media interactions and MOOCs may be used in transmedia learning, and how data mining social media and experience tracking can inform the development of computational learner models for transmedia learning campaigns. Examples of how the U.S. Army has utilized transmedia campaigns for strategic communication and game-based training are provided. Finally, we provide strategies the reader can use today to incorporate transmedia storytelling elements such as Internet, serious games, video, social media, graphic novels, machinima, blogs, and alternate reality gaming into a new paradigm for training and education: transmedia learning.}
}
@article{LI2024,
title = {Paradigm shifts from data-intensive science to robot scientists},
journal = {Science Bulletin},
year = {2024},
issn = {2095-9273},
doi = {https://doi.org/10.1016/j.scib.2024.09.029},
url = {https://www.sciencedirect.com/science/article/pii/S2095927324006807},
author = {Xin Li and Yanlong Guo}
}
@article{MATLI2024100286,
title = {Extending the theory of information poverty to deepfake technology},
journal = {International Journal of Information Management Data Insights},
volume = {4},
number = {2},
pages = {100286},
year = {2024},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2024.100286},
url = {https://www.sciencedirect.com/science/article/pii/S2667096824000752},
author = {Walter Matli},
keywords = {Deepfake technology, Information poverty theory, Artificial intelligence (AI), Synthetic media, Societal implications, Technological advancements},
abstract = {The advent of deepfake technology has introduced complex challenges to the information technology landscape, simultaneously presenting benefits and novel risks and ethical considerations. This paper delves into the evolution of deepfakes through the prism of information poverty theory, scrutinising how deepfakes may contribute to a growing information access/use inequality. The research focuses on the risks of misinformation and the ensuing expansion of digital divides, particularly when manipulative media could delude individuals lacking access to legitimate information sources. The study outlines the potential exacerbation of information asymmetries and examines the societal implications across various demographics. By integrating an analytical discussion on the risks associated with deepfakes, the study aligns the observed trends with the theoretical underpinnings of information poverty. As part of its contribution, the paper offers actionable policy-making recommendations and educational strategies to combat the proliferation of harmful deepfake content. The article aims to ensure a more equitable distribution of authentic information and foster media literacy. Through a multifaceted approach, this study endeavours to provide a foundational understanding for stakeholders to navigate the ethical minefield posed by deepfakes and to instil a framework for information equity in the digital era. The article provides critical insights into the discourse on deepfake technology and its relation to information poverty, underscoring the urgent need for equitable access to informed digital spaces. As deepfake technology evolves and more data emerges, a societal demand exists for comprehensive knowledge about deepfakes to promote discernment, decision-making and awareness. Policymakers are tasked with recognising the significance of widening access to sophisticated information technologies whilst addressing their negative repercussions. Their efforts will be particularly crucial for disseminating knowledge about deepfakes to those with limited or non-existent information and communication awareness and infrastructures. Learning from past successes and failures becomes pivotal in shaping effective strategies to address the challenges posed by deepfakes and fostering accessible, informed digital communities.}
}
@incollection{GILLESPIE2024124,
title = {Differentiation},
editor = {Samuel M. Scheiner},
booktitle = {Encyclopedia of Biodiversity (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Oxford},
pages = {124-133},
year = {2024},
isbn = {978-0-323-98434-8},
doi = {https://doi.org/10.1016/B978-0-12-822562-2.00167-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128225622001675},
author = {Rosemary G. Gillespie},
keywords = {Adaptive landscape, Cline, Coalescent process, Gene flow, Hybrid zone, Local adaptation, Natural selection, Neutral theory, Population structure and Speciation},
abstract = {Differentiation generally considers the accumulation of genetic differences between populations or species but can be applied more broadly to the diversification of genes, organisms, and populations. This article examines how elementary evolutionary and ecological processes lead to differentiation in both the narrow and broad senses and introduces successively more complex processes involved in differentiation. Most mutations are selectively neutral and neutral evolution can be considered the default process of genomic change, with natural selection being measurable from changes in allele frequencies that lead to deviation from neutrality. While the basic tenets of genetic differentiation are well established, the advent of new genomic technologies and associated computational tools have provided unprecedented insights into the mechanism of genetic differentiation and how these might lead to the formation of species. Recent work has highlighted in particular the importance of genetic admixture events and genomic interactions in shaping the process of differentiation.}
}
@article{BLEMKER2023111745,
title = {In vivo imaging of skeletal muscle form and function: 50 years of insight},
journal = {Journal of Biomechanics},
volume = {158},
pages = {111745},
year = {2023},
issn = {0021-9290},
doi = {https://doi.org/10.1016/j.jbiomech.2023.111745},
url = {https://www.sciencedirect.com/science/article/pii/S0021929023003159},
author = {Silvia S. Blemker},
keywords = {Skeletal muscle, Imaging, In vivo},
abstract = {Skeletal muscle form and function has fascinated scientists for centuries. Our understanding of muscle function has long been driven by advancements in imaging techniques. For example, the sliding filament theory of muscle, which is now widely leveraged in biomechanics research, stemmed from observations made possible by scanning electron microscopy. Over the last 50 years, advancing in medical imaging, combined with ingenuity and creativity of biomechanists, have provide a wealth of new and important insights into in vivo human muscle function. Incorporation of in vivo imaging has also advanced computational modeling and allowed our research to have an impact in many clinical populations. While this review does not provide a comprehensive or meta-analysis of the all the in vivo muscle imaging work over the last five decades, it provides a narrative about the past, present, and future of in vivo muscle imaging.}
}
@article{HUANG2019592,
title = {Challenges, opportunities and paradigm of applying big data to production safety management: From a theoretical perspective},
journal = {Journal of Cleaner Production},
volume = {231},
pages = {592-599},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.05.245},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619317810},
author = {Lang Huang and Chao Wu and Bing Wang},
keywords = {Big data, Production safety management, Big-data-driven, Challenges, Opportunities},
abstract = {Big data has caused the scientific community to re-examine the scientific research methodologies and has triggered a revolution in scientific thinking. As a branch of scientific research, production safety management is also exploring methods to take advantage of big data. This research aims to provide a theoretical basis for promoting the application of big data in production safety management. First, four different types of production safety management paradigms were identified, namely small-data-based, static-oriented, interpretation-based and causal-oriented paradigm, and the challenges to these paradigms in the presence of big data were introduced. Second, the opportunities of employing big data in production safety management were identified from four aspects, including better predict the future production safety phenomena, promote production safety management highlight relevance, achieve the balance between deductive and inductive approaches and promote the interdisciplinary development of production safety management. Third, the paradigm shifting trend of production safety management was concluded, and the discipline foundation of the new paradigm was considered as the integration of data science, production management and safety science. Fourth, a new big-data-driven production safety management paradigm was developed, which consists of the logical line of production safety management, the macro-meso-micro data spectrum, the key big data analytics, and the four-dimensional morphology. At last, the strengths (e.g., supporting better-informed safety description, safety inquisition, safety prediction) and future research direction (e.g., theory research focuses on safety-related data mining/capturing/cleansing) of the new paradigm were discussed. The research results not only can provide theoretical and practical basis for big-data-driven production safety management, but also can offer advice to managerial consideration and scholarly investigation.}
}
@article{ZHANG2024127373,
title = {Adaptive emotion neural network based on ITCSO and grey correlation contribution},
journal = {Neurocomputing},
volume = {577},
pages = {127373},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.127373},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224001449},
author = {Wei Zhang and Wanfeng Wei},
keywords = {Emotion neural network, Hormone regulation, Competitive swarm optimization, Grey correlation contribution, Convergence analysis},
abstract = {In order to further improve the performance of emotion neural network (ENN), a novel adaptive hormone regulation emotion neural network (HRENN) is proposed, which is based on the improved triple competitive swarm optimization (ITCSO) algorithm and grey correlation contribution. Firstly, the structure of HRENN is designed that is inspired by the biological mechanism of hormone regulation. The fast response characteristic of emotion processing and the feedback effect of hormone regulation can effectively improve the learning ability of HRENN. Secondly, the ITCSO algorithm is proposed for optimizing the parameters of HRENN. In order to strike a well balance between exploration and exploitation, triple competition mechanism is adopted. Two-thirds of particles participate in the optimization and different learning strategies for different particles are provided. These operations can greatly improve the optimization efficiency and the convergence accuracy. Moreover, grey correlation contribution is used to add or delete the dimension of particles. It means that the structure and parameters of HRENN can be adjusted simultaneously and the compact structure can be obtained. Finally, the stability and the convergence of ITCSO are proved using the Banach space and the principle of compression mapping. Experiment results show that the proposed ITCSO-HRENN has good self-organization ability, compact network structure, high convergence accuracy and superior computation efficiency compared with other methods.}
}
@article{BLACKMAN2022101661,
title = {Persistent mysteries of jet engines, formation, propagation, and particle acceleration: Have they been addressed experimentally?},
journal = {New Astronomy Reviews},
volume = {95},
pages = {101661},
year = {2022},
issn = {1387-6473},
doi = {https://doi.org/10.1016/j.newar.2022.101661},
url = {https://www.sciencedirect.com/science/article/pii/S1387647322000197},
author = {Eric G. Blackman and Sergey V. Lebedev},
keywords = {Jets, Laboratory astrophysics, Accretion, Magnetic fields, Young stellar objects, Active galactic nuclei, Microquasars, Particle acceleration, High energy density physics},
abstract = {The physics of astrophysical jets can be divided into three regimes: (i) engine and launch (ii) propagation and collimation, (iii) dissipation and particle acceleration. Since astrophysical jets comprise a huge range of scales and phenomena, practicality dictates that most studies of jets intentionally or inadvertently focus on one of these regimes, and even therein, one body of work may be simply boundary condition for another. We first discuss long standing persistent mysteries that pertain the physics of each of these regimes, independent of the method used to study them. This discussion makes contact with frontiers of plasma astrophysics more generally. While observations theory, and simulations, and have long been the main tools of the trade, what about laboratory experiments? Jet related experiments have offered controlled studies of specific principles, physical processes, and benchmarks for numerical and theoretical calculations. We discuss what has been accomplished on these fronts. Although experiments have indeed helped us to understand certain processes, proof of principle concepts, and benchmarked codes, they have yet to solved an astrophysical jet mystery on their own. A challenge is that experimental tools used for jet-related experiments so far, are typically not machines originally designed for that purpose, or designed with specific astrophysical mysteries in mind. This presents an opportunity for a different way of thinking about the development of future platforms: start with the astrophysical mystery and build an experiment to address it.}
}
@article{MANFRE201612,
title = {Exploiting interactive genetic algorithms for creative humanoid dancing},
journal = {Biologically Inspired Cognitive Architectures},
volume = {17},
pages = {12-21},
year = {2016},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2016.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X16300378},
author = {Adriano Manfré and Agnese Augello and Giovanni Pilato and Filippo Vella and Ignazio Infantino},
keywords = {Robot, Dance, Computational creativity, Music perception, Co-creative tool},
abstract = {The paper discusses an approach aimed at endowing a cognitive architecture with artificial creativity capabilities in order to make a humanoid able to dance in a pleasant manner. The robot associates movements to music perception creating an aesthetically valuable dance by using a Hidden Markov Model with a nonclassical approach. Two matrices mainly influence the model: a Transition matrix TM, and an Emission Matrix EM. The TM matrix rules the transition between two subsequent movements. The EM matrix constitutes the link between a set of movements and the perceived music features. In order to compute the EM matrix, we exploit a genetic algorithm approach. The approach makes use of two kinds of fitness functions. The first one is an internal evaluation fitness that allows the robot to autonomously learn the association between music and movements. The second one depends on the interaction with a human teacher, leading to the determination of different dance styles, which constitute the robot repertoire. The experimental part discusses the effects on the creativity of different distances to compute fitness.}
}
@article{CUI2022110595,
title = {Neural mechanisms of aberrant self-referential processing in patients with generalized anxiety disorder},
journal = {Progress in Neuro-Psychopharmacology and Biological Psychiatry},
volume = {119},
pages = {110595},
year = {2022},
issn = {0278-5846},
doi = {https://doi.org/10.1016/j.pnpbp.2022.110595},
url = {https://www.sciencedirect.com/science/article/pii/S0278584622000872},
author = {Qian Cui and Yuyan Chen and Qin Tang and Wei Sheng and Di Li and Yuhong Zeng and Kexing Jiang and Zongling He and Huafu Chen},
keywords = {Generalized anxiety disorder, Traits, Self-related processing, Task-related fMRI, Functional connectivity},
abstract = {Massive theoretical studies in clinical psychology have implicated the self in understanding internalizing disorders (i.e., anxiety and mood disorders), in which self-related tasks were frequently used to investigate internalizing psychopathology. As one of the most frequently seen internalizing disorder in primary care, patients with generalized anxiety disorder (GAD) are characterized by inappropriate self-related processing such as negative self-referential thinking. However, relevant neural mechanisms remain unknown. In this study, participants underwent a self-related task which they were presented with several positive and negative trait words and were required to judge the extent to which these traits matched themselves when compared to their average peers. Aberrant brain activation and functional connectivity of GAD were detected during processing positive and negative traits. Compared to healthy controls (HCs), patients with GAD exhibited abnormal self-processing which manifested as lower biased self-rating scores particularly for negative traits and weaker brain activity in the left dorsomedial prefrontal cortex, inferior frontal gyrus, superior temporal sulcus (STS), and bilateral lingual gyrus when processing trait words. Abnormal functional connections between these hypoactive regions and regions associated with reward, emotion, and theory of mind were observed in subsequent psychophysiological interaction analysis. An attenuation of connectivity between the left insula and left STS was associated with greater severity of anxiety symptom in GAD patients. These findings provide insight into the abnormal neurocognitive mechanisms of biased self-related processing in GAD patients, which involves distorted self-schema accompanied by abnormal activation and functional connections of regions implicated in self-related and social cognition processing.}
}
@article{KIM2019141,
title = {AI for design: Virtual design assistant},
journal = {CIRP Annals},
volume = {68},
number = {1},
pages = {141-144},
year = {2019},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2019.03.024},
url = {https://www.sciencedirect.com/science/article/pii/S0007850619300289},
author = {Sang-Gook Kim and Sang Min Yoon and Maria Yang and Jungwoo Choi and Haluk Akay and Edward Burnell},
keywords = {Design method, Machine learning, Hybrid intelligence},
abstract = {Engineering faces many wicked problems: irreducibly interdisciplinary with multiple competing objectives, and of such large scale and complexity that will require processes to deeply rely on human insights and power of computation. The resurgence of machine learning offers the possibility for new forms of human/computer collaboration where each fuels hybrid intelligence in complementary ways. A concept of virtual design assistant (VDA) is developed as a platform to bring the hybrid intelligence in solving complex design challenges. A deep learning-based abstraction process is developed to provide VDA a function to extract structured functional requirements from fragmental design specifications and customer needs.}
}
@article{CYSEWSKI201623,
title = {Efficacy of bi-component cocrystals and simple binary eutectics screening using heat of mixing estimated under super cooled conditions},
journal = {Journal of Molecular Graphics and Modelling},
volume = {68},
pages = {23-28},
year = {2016},
issn = {1093-3263},
doi = {https://doi.org/10.1016/j.jmgm.2016.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S1093326316300870},
author = {Piotr Cysewski},
keywords = {Cocrystals, Simple binary eutectics, Theoretical screening, Heat of mixing, Confusion matrix, COSMO-RS, COSMOtherm},
abstract = {The values of excess heat characterizing sets of 493 simple binary eutectic mixtures and 965 cocrystals were estimated under super cooled liquid condition. The application of a confusion matrix as a predictive analytical tool was applied for distinguishing between the two subsets. Among seven considered levels of computations the BP-TZVPD-FINE approach was found to be the most precise in terms of the lowest percentage of misclassified positive cases. Also much less computationally demanding AM1 and PM7 semiempirical quantum chemistry methods are likewise worth considering for estimation of the heat of mixing values. Despite intrinsic limitations of the approach of modeling miscibility in the solid state, based on components affinities in liquids under super cooled conditions, it is possible to define adequate criterions for classification of coformers pairs as simple binary eutectics or cocrystals. The predicted precision has been found as 12.8% what is quite accepted, bearing in mind simplicity of the approach. However, tuning theoretical screening to such precision implies the exclusion of many positive cases and this wastage exceeds 31% of cocrystals classified as false negatives.}
}
@article{SURYANARAYANA2024100495,
title = {Artificial Intelligence Enhanced Digital Learning for the Sustainability of Education Management System},
journal = {The Journal of High Technology Management Research},
volume = {35},
number = {2},
pages = {100495},
year = {2024},
issn = {1047-8310},
doi = {https://doi.org/10.1016/j.hitech.2024.100495},
url = {https://www.sciencedirect.com/science/article/pii/S104783102400004X},
author = {K.S. Suryanarayana and V.S. Prasad Kandi and G. Pavani and Akuthota Sankar Rao and Sandeep Rout and T. {Siva Rama Krishna}},
keywords = {Artificial intelligence, Digital education, Sustainability, Role of AI in education, Educational management},
abstract = {Maintenance schedules are scheduled ahead of time and automatically based on the continuous monitoring of the equipment by statistical methods, thanks to artificial intelligence-enabled digital transformation and the best fit model based on Machine Management Index in a pedagogical system. One of the most important aspects of universities is the widespread use of machine learning methods to evaluate students' progress. Machine learning approaches are designed to speed up the learning process without sacrificing accuracy. The dynamics of teaching and learning have shifted since the introduction of modern technological tools. The educational system as a whole has changed and developed over time. These days, people can get an education outside of the classroom as well, thanks to the proliferation of online courses and resources. Everyone's professional life begins with their education. By analyzing past data, artificial intelligence methods can resolve existing problems. When applied properly, artificial intelligence can be a highly efficient method for solving problems with a predictable and repeatable solution space. The learner's personality can be predicted based on a number of factors using machine learning approaches. This article examines how AI may improve digital learning in education management systems to sustain the education ecosystem. AI in education improves student results, learning experiences, and administrative processes. This study discusses AI applications in education management systems and associated problems and opportunities. We also explore ethical issues and the roadmap for using AI to improve education. Educational institutions can provide individualized curriculum for students based on their unique personalities and areas of interest. Institutions of higher learning can benefit greatly from this instrument for personality prediction by recommending a course of study that will better prepare students to enter the field of their choice and achieve professional success.}
}
@article{SHAH201494,
title = {Towards a Managed Aquifer Recharge strategy for Gujarat, India: An economist’s dialogue with hydro-geologists},
journal = {Journal of Hydrology},
volume = {518},
pages = {94-107},
year = {2014},
note = {Creating Partnerships Between Hydrology and Social Science: A Priority for Progress},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2013.12.022},
url = {https://www.sciencedirect.com/science/article/pii/S0022169413009190},
author = {Tushaar Shah},
keywords = {Managed Aquifer Recharge, Energy-irrigation nexus, Groundwater depletion, Groundwater economics},
abstract = {Summary
Gujarat state in Western India exemplifies all challenges of an agrarian economy founded on groundwater overexploitation sustained over decades by perverse energy subsidies. Major consequences are: secular decline in groundwater levels, deterioration of groundwater quality, rising energy cost of pumping, soaring carbon footprint of agriculture and growing financial burden of energy subsidies. In 2009, Government of Gujarat asked the present author, an economist, to chair a Taskforce of senior hydro-geologists and civil engineers to develop and recommend a Managed Aquifer Recharge (MAR) strategy for the state. This paper summarizes the recommended strategy and its underlying logic. It also describes the imperfect fusion of socio-economic and hydro-geologic perspectives that occurred in course of the working of the Taskforce and highlights the need for trans-disciplinary perspectives on groundwater governance.}
}
@incollection{WALTZ1997327,
title = {AI applications of massive parallelism: An experience report},
editor = {James Geller and Hiroaki Kitano and Christian B. Suttner},
series = {Machine Intelligence and Pattern Recognition},
publisher = {North-Holland},
volume = {20},
pages = {327-339},
year = {1997},
booktitle = {Parallel Processing for Artificial Intelligence 3},
issn = {0923-0459},
doi = {https://doi.org/10.1016/S0923-0459(97)80016-X},
url = {https://www.sciencedirect.com/science/article/pii/S092304599780016X},
author = {David L. Waltz},
abstract = {For nearly ten years my group and I at Thinking Machines Corporation worked at selling massively parallel computers for a variety of applications that fall broadly in the area now called “database mining.” We had an amazing team of scientists and engineers, saw trends far ahead of the rest of the world, and developed several great systems. However, we began as novices in the business arena. Sometimes we made sales, sometimes we did not; but we learned a great deal in either case. This chapter recounts the sales process and a brief history, mostly in the form of “war stories” mixed with technical details, and attempts to summarize some messages to take away, based on what we learned.}
}
@article{MAHY201468,
title = {How and where: Theory-of-mind in the brain},
journal = {Developmental Cognitive Neuroscience},
volume = {9},
pages = {68-81},
year = {2014},
issn = {1878-9293},
doi = {https://doi.org/10.1016/j.dcn.2014.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S1878929314000048},
author = {Caitlin E.V. Mahy and Louis J. Moses and Jennifer H. Pfeifer},
keywords = {Theory of mind, Neuroimaging, Modularity, Theory theory, Simulation, Executive functioning},
abstract = {Theory of mind (ToM) is a core topic in both social neuroscience and developmental psychology, yet theory and data from each field have only minimally constrained thinking in the other. The two fields might be fruitfully integrated, however, if social neuroscientists sought evidence directly relevant to current accounts of ToM development: modularity, simulation, executive, and theory theory accounts. Here we extend the distinct predictions made by each theory to the neural level, describe neuroimaging evidence that in principle would be relevant to testing each account, and discuss such evidence where it exists. We propose that it would be mutually beneficial for both fields if ToM neuroimaging studies focused more on integrating developmental accounts of ToM acquisition with neuroimaging approaches, and suggest ways this might be achieved.}
}
@article{MOZUNI2017303,
title = {An Introduction to the Morphological Delphi Method for Design: A Tool for Future-Oriented Design Research},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {3},
number = {4},
pages = {303-318},
year = {2017},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2018.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S2405872617300710},
author = {Mehdi Mozuni and Wolfgang Jonas},
keywords = {Morphological analysis, Systemic design, Future-oriented design, Delphi method, Scenario development, Strategic foresight},
abstract = {Projecting analytical concepts is a difficult, though established process in innovation management. Designers face methodological obstacles, however, when engaging with a future system with rapidly changing factors. First, the system’s users do not yet exist. Second, continuing changes in key factors and their interactions make conceiving of relationships and delivering synthesizable data impossible. The rational core for making projections suffers from a lack of substantiation. Both morphological analysis and the Delphi method are established tools in strategic foresight. We suggest that a morphology-based Delphi method supports the process of projecting future outcomes in innovative, complex projects. In addition, each tool compensates for the other’s theoretical and functional deficits by illustrating transparent, value-based arguments in a modifiable, iterative manner.}
}
@article{XU2024100549,
title = {Prediction of environmental pollution hazard index of water conservancy system based on fuzzy logic},
journal = {International Journal of Thermofluids},
volume = {21},
pages = {100549},
year = {2024},
issn = {2666-2027},
doi = {https://doi.org/10.1016/j.ijft.2023.100549},
url = {https://www.sciencedirect.com/science/article/pii/S2666202723002641},
author = {Bingshu Xu},
keywords = {Water Conservancy System, Fuzzy Logic, Combined Forecasting, Environmental Pollution, Hazard Index Prediction},
abstract = {The water conservancy framework is a significant foundation for China's financial and social turn of events. Simultaneously monetary and social turn of events, likewise tremendously affect the biological climate. In the prediction method of the environmental pollution hazard index of water conservancy projects, a risk assessment must be carried out to enhance its applicability. This article mainly focuses on traditional mathematical prediction models and combines the characteristics of fuzzy logic mathematics (good nonlinear quality) to establish a new combination prediction method. By comparing the convergence errors of traditional methods with this method, the results show that the algorithm proposed in this paper can effectively reduce prediction errors with fewer iterations, making it stable at around 500 times, with good convergence and high computational accuracy. The fuzzy logic-based prediction method for environmental pollution hazards in water conservancy systems proposed in this article can overcome the difficulty of nonlinear combination prediction. This achieves real-time prediction of environmental pollution hazards in water conservancy systems and shortens response time, greatly improving the accuracy of prediction.}
}
@article{JOHN2022133624,
title = {How key-enabling technologies’ regimes influence sociotechnical transitions: The impact of artificial intelligence on decarbonization in the steel industry},
journal = {Journal of Cleaner Production},
volume = {370},
pages = {133624},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.133624},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622032024},
author = {Nikhil John and Joeri Hendrik Wesseling and Ernst Worrell and Marko Hekkert},
keywords = {Digitalization, Energy-intensive processing industries, Barriers to innovation, Multi-regime interactions, Sociotechnical systems, Multi-level perspective},
abstract = {Key Enabling Technologies (KETs) are pervasive groups of technologies expected to enable innovation. They have been promoted as technologies with tremendous potential for boosting economic growth and sustainability in all sectors of society – a claim whose validity remains underexplored. Building on systems thinking and the Multi-Level Perspective, we develop a novel approach to assess the socio-technical impact of a KET regime on a transitioning sectoral regime. This approach is applied to the case of the Artificial Intelligence (AI) KET impacting the decarbonization of the energy-intensive steel industry. To assess AI's technical impact, we compiled an inventory of AI tools based on reviewing technical scientific articles. Our analysis shows that AI adds technological value to the full range of areas in the steel industry, like predicting process parameters; optimizing operations, scheduling, and electrical energy; and forecasting product demand, quality, and site emissions. Semi-structured interviews were the primary data source to assess AI's broader socio-institutional impact. The results indicate that AI may currently be reinforcing path dependencies of the steel industry, as AI tools are more focused on incremental improvement for existing technologies rather than novel low-carbon technologies. However, AI also offers capabilities to reduce barriers to sustainability innovation, like system integration challenges, flexibility challenges, demand-side barriers, and risk-related barriers. Finally, we reflect on the generalizability of our approach for studying other transitions, and we induce characteristics of the AI-Digital KET regimes. We find that through these characteristics, the AI-Digital KET regime alters existing and creates new system structures (actors, networks, and institutions) within the impacted sector.}
}
@article{CASTILLOFELISOLA2023108748,
title = {Cadabra and Python algorithms in general relativity and cosmology II: Gravitational waves},
journal = {Computer Physics Communications},
volume = {289},
pages = {108748},
year = {2023},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2023.108748},
url = {https://www.sciencedirect.com/science/article/pii/S0010465523000930},
author = {Oscar Castillo-Felisola and Dominic T. Price and Mattia Scomparin},
keywords = {Computer algebra system, , Gravitation, Gravitational waves, Perturbative field theory, Python, Cosmology, General relativity},
abstract = {Computer Algebra Systems (CASs) like Cadabra Software play a prominent role in a wide range of research activities in physics and related fields. We show how Cadabra language is easily implemented in the well established Python programming framework, gaining excellent flexibility and customization to address the issue of tensor perturbations in General Relativity. We obtain a performing algorithm to decompose tensorial quantities up to any perturbative order of the metric. The features of our code are tested by discussing some concrete computational issues in research activities related to first/higher-order gravitational waves.}
}