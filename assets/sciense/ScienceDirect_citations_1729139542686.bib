@article{MONROE2020293,
title = {Moral elevation: Indications of functional integration with welfare trade-off calibration and estimation mechanisms},
journal = {Evolution and Human Behavior},
volume = {41},
number = {4},
pages = {293-302},
year = {2020},
issn = {1090-5138},
doi = {https://doi.org/10.1016/j.evolhumbehav.2020.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1090513820300581},
author = {Amy Monroe},
keywords = {Moral elevation, Welfare trade-off ratios, Competitive altruism, Emotion},
abstract = {Moral elevation is a positive social emotion, which is triggered by observing third parties behaving benevolently, and which in turn triggers a motivation to behave benevolently towards others in general. It has been suggested that this relatively obscure emotion may be the output of a naturally selected cognitive adaptation which functions to help us retain our position in the competition for access to beneficial social relationships. This suggestion is here interpreted within the framework of ‘recalibrational emotions’. This framework offers the computational vocabulary necessary to understand how mental adaptations governing affect and motivation perform their functions at the cognitive level. Parallels are drawn between the suggested function and known phenomenological attributes of moral elevation, and the recently explicated functional operation of other social emotions (such as anger, guilt, and gratitude). Specifically, these other social emotions are thought to share a common computational pathway; recalibration of our welfare trade-off ratios (WTRs). WTRs are the computational element which dictate our willingness to benefit others at some cost to ourselves. A series of studies was conducted to explore whether a reliable relationship exists between moral elevation and WTRs. The results suggest that elevation does have a positive recalibrational effect on our WTRs, and that it may also be functionally integrated with a mental mechanism designed by natural selection to estimate the WTRs of other social actors.}
}
@article{SPORRER2024115192,
title = {Induced worry increases risk aversion in patients with generalized anxiety},
journal = {Behavioural Brain Research},
volume = {474},
pages = {115192},
year = {2024},
issn = {0166-4328},
doi = {https://doi.org/10.1016/j.bbr.2024.115192},
url = {https://www.sciencedirect.com/science/article/pii/S0166432824003486},
author = {Juliana K. Sporrer and Alexandra Johann and Justin Chumbley and Oliver J. Robinson and Dominik R. Bach},
keywords = {Depression, Anxiety, Worry, Decision making, Loss aversion, Risk aversion},
abstract = {Generalized anxiety disorder is characterized by disruptions in decision-making, including an enhanced aversion to uncertain outcomes (i.e., risk aversion), which is not specific to negative outcomes (i.e., no loss aversion). It is unknown if this uncertainty bias is a trait-like causal factor contributing to anxiety symptoms, or a state-like feature triggered by anxiety symptoms such as worry chains. Here, in-patients with Major Depression Disorder (MDD), with (N = 16) or without (N = 24) Generalized anxiety (GA) symptoms, and healthy controls (N = 23), completed an economic decision-making task before and after worry induction. They were asked to choose between a certain monetary payoff, and an uncertain gamble, allowing for estimation of risk and loss aversion through a computational prospect-theoretic model. There were no significant differences in risk and loss aversion between any of the three groups at baseline. After worry induction, patients with GA symptoms, compared to those without, showed increased risk aversion. This increase was modulated by the severity of anxiety symptoms. These findings suggest that decision-making disruptions in anxiety disorder may be driven by anxiety symptoms such as worry, rather than causing them. This could shape etiological models, motivate standardization of emotional state in research on decision-making in anxiety disorders, support treatment strategies primarily aimed at worry management, and could guide novel interventions focusing on uncertainty exposure across aversive and appetitive domains.}
}
@article{THEODOROU20075697,
title = {Hierarchical modelling of polymeric materials},
journal = {Chemical Engineering Science},
volume = {62},
number = {21},
pages = {5697-5714},
year = {2007},
issn = {0009-2509},
doi = {https://doi.org/10.1016/j.ces.2007.04.048},
url = {https://www.sciencedirect.com/science/article/pii/S000925090700382X},
author = {Doros N. Theodorou},
keywords = {Polymers, Mathematical modelling, Simulation, Rheology, Nanostructure, Diffusion},
abstract = {Within the last 20 years, computer simulations of materials have evolved from an academic curiosity to a predictive tool for addressing structure–property–processing–performance relations that are critical to the design of new products and processes. Chemical engineers, with their problem-oriented thinking and their systems approach, have played a significant role in this development. The computational prediction of physical properties is particularly challenging for polymeric materials, because of the extremely broad spectra of length and time scales governing structure and molecular motion in these materials. This challenge can only be met through the development of hierarchical analysis and simulation strategies encompassing many interconnected levels, each level addressing phenomena over a specific window of time and length scales. In this paper we will briefly discuss the fundamental underpinnings and example applications of new methods and algorithms for the hierarchical modelling of polymers. Questions to be addressed include: How can one equilibrate atomistic models of long-chain polymer melts at all length scales and thereby predict thermodynamic and conformational properties reliably? How can one quantify the structure of entanglement networks present in these melts through topological analysis and relate it to rheological properties? Are there ways to predict the microphase-separated morphology and stress–strain behaviour of multicomponent block copolymer-based materials, such as pressure sensitive adhesives? Is it possible to anticipate changes in the barrier properties of glassy amorphous polymers used in packaging applications as a consequence of modifications in the chemical constitution of chains?}
}
@article{NOBRE2019132,
title = {Premembering Experience: A Hierarchy of Time-Scales for Proactive Attention},
journal = {Neuron},
volume = {104},
number = {1},
pages = {132-146},
year = {2019},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2019.08.030},
url = {https://www.sciencedirect.com/science/article/pii/S0896627319307366},
author = {Anna C. Nobre and Mark G. Stokes},
keywords = {memory, attention, decision-making, hippocampus, prefrontal cortex, priming, working memory, episodic memory, implicit memory},
abstract = {Memories are about the past, but they serve the future. Memory research often emphasizes the former aspect: focusing on the functions that re-constitute (re-member) experience and elucidating the various types of memories and their interrelations, timescales, and neural bases. Here we highlight the prospective nature of memory in guiding selective attention, focusing on functions that use previous experience to anticipate the relevant events about to unfold—to “premember” experience. Memories of various types and timescales play a fundamental role in guiding perception and performance adaptively, proactively, and dynamically. Consonant with this perspective, memories are often recorded according to expected future demands. Using working memory as an example, we consider how mnemonic content is selected and represented for future use. This perspective moves away from the traditional representational account of memory toward a functional account in which forward-looking memory traces are informationally and computationally tuned for interacting with incoming sensory signals to guide adaptive behavior.}
}
@article{ALLGOWER2019147,
title = {Position paper on the challenges posed by modern applications to cyber-physical systems theory},
journal = {Nonlinear Analysis: Hybrid Systems},
volume = {34},
pages = {147-165},
year = {2019},
issn = {1751-570X},
doi = {https://doi.org/10.1016/j.nahs.2019.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S1751570X19300603},
author = {Frank Allgöwer and João {Borges de Sousa} and James Kapinski and Pieter Mosterman and Jens Oehlerking and Patrick Panciatici and Maria Prandini and Akshay Rajhans and Paulo Tabuada and Philipp Wenzelburger},
keywords = {cyber–physical systems theory},
abstract = {Cyber-physical systems theory offers a powerful framework for modeling, analyzing, and designing real engineering systems integrating communication, control, and computation functionalities (the cyber part) within a natural and/or man-made system governed by the laws of physics (the physical part). New methodological developments in cyber-physical systems theory are required by traditional application domains such as manufacturing, transportation, and energy systems, which are currently experiencing significant and – to some extent – revolutionary changes to address the needs of our modern society. The goal of this position paper is to provide the cyber-physical systems community, and especially young researchers, a clear view on what are research directions worth pursuing motivated by the challenges posed by modern applications.}
}
@article{TERZIEVA2024106,
title = {Trends, Challenges, Opportunities, and Innovations in STEM Education},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {3},
pages = {106-111},
year = {2024},
note = {22nd IFAC Conference on Technology, Culture and International Stability TECIS 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.07.134},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324002179},
author = {Valentina Terzieva and Elena Paunova-Hubenova and Savina Slavcheva},
keywords = {STEM education, STEM teaching approaches, STEM challenges, innovative teaching methods},
abstract = {STEM education aims to prepare students for their future jobs, providing authentic tasks and problems to solve. Usually, approaches to teaching STEM subjects are based on a constructivist learning theory that accentuates active, practical, and interactive learning approaches. Nowadays, the implementation of STEM education faces several logistical and pedagogical challenges, which can impact the effectiveness of STEM education programs. The conditions for applying information technologies in STEM education in Bulgarian schools and universities are presented. The paper proposes a conceptual model of the innovative STEM educational system, which includes personalization and optimization of the applied teaching methods.}
}
@article{CRAIG20233427,
title = {FEFOS: a method to derive oxide formation energies from oxidation states††Electronic supplementary information (ESI) available. See DOI: https://doi.org/10.1039/d3cy00107e},
journal = {Catalysis Science & Technology},
volume = {13},
number = {11},
pages = {3427-3435},
year = {2023},
issn = {2044-4753},
doi = {https://doi.org/10.1039/d3cy00107e},
url = {https://www.sciencedirect.com/science/article/pii/S2044475323006822},
author = {Michael John Craig and Felix Kleuker and Michal Bajdich and Max García-Melchor},
abstract = {ABSTRACT
Herein we report a method to extract formation energies from oxidation states, which we call FEFOS. This new scheme predicts the formation energies of binary oxides through analyzing unary oxide formation energies as a function of their oxidation states. Taking averages of fitted quadratic equations that represent how elements respond to oxidation and reduction, the weights of these averages are determined by constraining the compound to be neutral. The application of FEFOS results in mean absolute errors of ca. 0.10 eV per atom when tested against Materials Project data for oxides with general formulas A1−zBzO, A1−zBzO1.5, and A1−zBzO2 with specific coordinations. Our FEFOS method not only allows for the prediction of binary oxide formation energies with low variance and high interpretability, but also compares well with state-of-the-art deep learning methods without being biased by training data and the need for large resources to compute it. Finally, we discuss the potential applications of the FEFOS method in tackling the problem of inverse catalyst design.}
}
@article{NG2023116585,
title = {Development of a system model to predict flows and performance of regional waste management planning: A case study of England},
journal = {Journal of Environmental Management},
volume = {325},
pages = {116585},
year = {2023},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2022.116585},
url = {https://www.sciencedirect.com/science/article/pii/S0301479722021582},
author = {Kok Siew Ng and Aidong Yang},
keywords = {Circular economy, Recycling, Stock-and-flow, Sustainable waste management, Resource recovery, Systems thinking},
abstract = {Significant loss of valuable resources and increasing burdens on landfills are often associated with a lack of proper planning in waste management and resource recovery strategy. A sustainable waste management model is thus urgently needed to improve resource efficiency and divert more waste from landfills. This paper proposes a comprehensive system model using stock-and-flow diagram to examine the current waste management performance and project the future waste generation, treatment and disposal scenarios, using England as a case study. The model comprises three integrated modules to represent household waste generation and collection; waste treatment and disposal; and energy recovery. A detailed mass and energy balance has been established and waste management performance has been evaluated using six upstream and downstream indicators. The base case scenario that assumes constant waste composition shows that waste to landfills can be reduced to less than 10% of the total amount, by 2035. However, it entails greater diversion of waste to energy-from-waste facilities, which is not sustainable and would incur higher capital investment and gate fees. Alternative case scenarios that promote recycling instead of energy recovery result in lower capital investment and gate fees. Complete elimination of the food and organic fraction from the residual waste stream will help meet the 65% recycling target by 2035. In light of the need for achieving a more circular economy in England, enhancing material recovery through reuse and recycling, reducing reliance on energy-from-waste and deploying more advanced waste valorisation technologies should be considered in future policy and planning for waste management.}
}
@article{BRIAN2023129074,
title = {Introducing mindset streams to investigate stances towards STEM in high school students and experts},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {626},
pages = {129074},
year = {2023},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2023.129074},
url = {https://www.sciencedirect.com/science/article/pii/S0378437123006295},
author = {Kieran Brian and Massimo Stella},
keywords = {Cognitive modelling, Network science, Cognitive network science, Mindset reconstruction, STEM learning},
abstract = {We introduce mindset streams for assessing ways of bridging two target concepts in concept maps. We focus on behavioural forma mentis networks (BFMN), which map the associative and affective dimensions of memory recalls. Inspired by trains of thoughts taking several paths to link ideas, mindset streams are defined as BFMN subgraphs induced by all shortest paths between two target concepts, e.g. all recalls in shortest paths bridging “math” and “learning”. These streams quantify the following features of the mindset encoded in a BFMN: (i) semantic content (i.e. which ideas mediate connections between targets?), (ii) valence coherence/conflict (i.e. are connections mediated by entwining ideas perceived negatively, positively or neutrally?), and (iii) semantic relevance (i.e. are the bridges between targets peripheral or central for the connectivity/betweenness of the BFMN?). We investigate mindset streams between ‘maths”/“physics” and key motivational aspects of learning (“fun”, “work”, “failure”) in two BFMNs, encoding how 159 students and 59 experts perceived and associated concepts about Science Technology Engineering and Maths (STEM), respectively. Statistical comparisons against configuration models show that high schoolers bridge “maths” and “fun” only through overabundant levels of valence-conflicting associations, contrasting negatively perceived domain knowledge with peer-related positive experiences. This conflict is absent in the researchers’ mindset stream, which rather bridges “math” and “fun” through positive, science-related associations. The mindset streams of both groups bridge “maths” and “physics” to “work” through mostly positive career-related jargon. Students’ mindset streams of “failure” and “math”/“physics” are dominated by negative associations with test anxiety, whereas researchers integrate “failure” and “math”/“physics” in semantically richer and more positive contexts, denoting failure itself as a cornerstone of STEM learning. We discuss our findings and future research directions in view of relevant psychology/education literature.}
}
@article{WANG2007254,
title = {An efficient algorithm for generalized discriminant analysis using incomplete Cholesky decomposition},
journal = {Pattern Recognition Letters},
volume = {28},
number = {2},
pages = {254-259},
year = {2007},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2006.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167865506001966},
author = {Haixian Wang and Zilan Hu and Yu’e Zhao},
keywords = {Generalized discriminant analysis, Nonlinear feature extraction, Eigenvalue decomposition, Gram–Schmidt orthonormalization, Incomplete Cholesky decomposition},
abstract = {Generalized discriminant analysis (GDA) has provided an extremely powerful approach to extracting nonlinear features via kernel trick. And it has been suggested for a number of applications, such as classification problem. Whereas the GDA could be solved by the utilization of Mercer kernels, a drawback of the standard GDA is that it may suffer from computational problem for large scale data set. Besides, there is still attendant problem of numerical accuracy when computing the eigenvalue problem of large matrices. Also, the GDA would occupy large memory (to store the kernel matrix). To overcome these deficiencies, we use Gram–Schmidt orthonormalization and incomplete Cholesky decomposition to find a basis for the entire training samples, and then formulate GDA as another eigenvalue problem of matrix whose size is much smaller than that of the kernel matrix by using the basis, while still working out the optimal discriminant vectors from all training samples. The theoretical analysis and experimental results on both artificial and real data set have shown the superiority of the proposed method for performing GDA in terms of computational efficiency and even the recognition accuracy, especially when the training samples size is large.}
}
@article{KRAUSS2024e36066,
title = {Science of science: A multidisciplinary field studying science},
journal = {Heliyon},
volume = {10},
number = {17},
pages = {e36066},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e36066},
url = {https://www.sciencedirect.com/science/article/pii/S240584402412097X},
author = {Alexander Krauss},
keywords = {Science of science, Metascience, Foundations of science, Foundations of knowledge, Limits of science, Limits of knowledge, Origins of science, Origins of knowledge},
abstract = {Science and knowledge are studied by researchers across many disciplines, examining how they are developed, what their current boundaries are and how we can advance them. By integrating evidence across disparate disciplines, the holistic field of science of science can address these foundational questions. This field illustrates how science is shaped by many interconnected factors: the cognitive processes of scientists, the historical evolution of science, economic incentives, institutional influences, computational approaches, statistical, mathematical and instrumental foundations of scientific inference, scientometric measures, philosophical and ethical dimensions of scientific concepts, among other influences. Achieving a comprehensive overview of a multifaceted field like the science of science requires pulling together evidence from the many sub-fields studying science across the natural and social sciences and humanities. This enables developing an interdisciplinary perspective of scientific practice, a more holistic understanding of scientific processes and outcomes, and more nuanced perspectives to how scientific research is conducted, influenced and evolves. It enables leveraging the strengths of various disciplines to create a holistic view of the foundations of science. Different researchers study science from their own disciplinary perspective and use their own methods, and there is a large divide between quantitative and qualitative researchers as they commonly do not read or cite research using other methodological approaches. A broader, synthesizing paper employing a qualitative approach can however help provide a bridge between disciplines by pulling together aspects of science (economic, scientometric, psychological, philosophical etc.). Such an approach enables identifying, across the range of fields, the powerful role of our scientific methods and instruments in shaping most aspects of our knowledge and science, whereas economic, social and historical influences help shape what knowledge we pursue. A unifying theory is then outlined for science of science – the new-methods-drive-science theory.}
}
@article{JOHANN2016420,
title = {Soil moisture modeling based on stochastic behavior of forces on a no-till chisel opener},
journal = {Computers and Electronics in Agriculture},
volume = {121},
pages = {420-428},
year = {2016},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2015.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S0168169915004020},
author = {André L. Johann and Augusto G. {de Araújo} and Hevandro C. Delalibera and André R. Hirakawa},
keywords = {Soil physics, Computational models, Precision agriculture, Soft computing, Force sensors},
abstract = {Crop-yield variability is frequently associated with soil moisture and its real-time measurement can be an alternative for the automatic control of no-till seeding to improve soil–crop conditions. Soil moisture has a significant influence on soil behavior, markedly on its temporal and spatial variability; however, the measurement of soil moisture is generally time consuming and expensive. Many studies employ electric, electromagnetic, optical, or radiometric sensors for the direct measurement of soil moisture. It is also possible to develop an estimation method employing existing machinery components using mechanical sensors such as load cells. Auto-regressive error function (AREF) combined with computational models is applied in this study for estimating soil moisture using a data set of forces acting on a chisel and speed as inputs to assess the feasibility of achieving more accurate results than previously obtained by Sakai et al. (2005). AREF is a stochastic method that can be applied to the analysis of soil-force patterns acting on a tool. Three computational models are developed, including two artificial neural networks (a Multi-Layer Perceptron (MLP) and a Radial Basis Function (RBF)) and one Neuro-Fuzzy model (ANFIS). These are compared with two multiple linear regression (MLR) models with two and six independent variables. The models’ performances are evaluated using root mean square error (RMSE), determination coefficient (R2), and average percentage error (APE). The computational models demonstrated superior performance compared to MLR, confirming the hypothesis. The neural network models had similar performances with RMSE between 1.27% and 1.30%, R2 around 0.80, and APE between 3.77% and 3.75% for testing data. These results indicate that using AREF parameters combined with computational models may be a suitable technique to estimate soil moisture and has potential to be used in control systems applied to no-till machinery.}
}
@article{ALRAKHAMI2021107573,
title = {A deep learning-based edge-fog-cloud framework for driving behavior management},
journal = {Computers & Electrical Engineering},
volume = {96},
pages = {107573},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107573},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621005127},
author = {Mabrook S. Al-Rakhami and Abdu Gumaei and Mohammad Mehedi Hassan and Atif Alamri and Musaed Alhussein and Md. Abdur Razzaque and Giancarlo Fortino},
keywords = {Deep learning, Car mobile edge (CME), Fog and cloud computing, Aggressive driving behaviors},
abstract = {Among the various reasons behind vehicle accidents, drivers' aggressiveness and distractions play a significant role. Deep learning (DL) algorithms inside a car mobile edge (CME) have been used for driver monitoring and to perform automated decision-making processes. Training and retraining the DL models in resource-constrained CME devices come with several challenges, especially regarding computational and memory space costs. Moreover, training the DL models periodically on representative data nearest to CME without imposing communication overheads on the cloud improves the quality of service (QoS) parameters, such as memory demand, processing time, power consumption, and bandwidth. This paper investigates the deployment of a deep neural network (DNN) model on a cloud-fog-edge computing framework for aggressive driver behavior detection and monitoring. To reach this goal, our framework proposes utilizing effective systems and databases of sensor-based metrics and data, cost-effective wireless networks, cloud-and fog-edge computing technologies, and the Internet. Experimental results of the DNN model showed that the accuracy of detection is improved by 1.84% compared with the current related work without any pre-processing window on data points that come from bio-signal sensors. Moreover, the experimental results of the networking part prove the efficiency and effectiveness of the proposed framework.}
}
@article{BAMOROVAT202321,
title = {Poor adherence is a major barrier to the proper treatment of cutaneous leishmaniasis: A case-control field assessment in Iran},
journal = {International Journal for Parasitology: Drugs and Drug Resistance},
volume = {21},
pages = {21-27},
year = {2023},
issn = {2211-3207},
doi = {https://doi.org/10.1016/j.ijpddr.2022.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S2211320722000331},
author = {Mehdi Bamorovat and Iraj Sharifi and Setareh {Agha Kuchak Afshari} and Ali Karamoozian and Amirhossein Tahmouresi and Amireh Heshmatkhah and Ehsan Salarkia and Ahmad Khosravi and Maryam {Hakimi Parizi} and Maryam Barghi},
keywords = {Poor adherence, Cutaneous leishmaniasis, Major barrier, Treatment, Iran},
abstract = {Leishmaniasis is an overlooked, poverty-stricken, and complex disease with growing social and public health problems. In general, leishmaniasis is a curable disease; however, there is an expansion of unresponsive cases to treatment in cutaneous leishmaniasis (CL). One of the effective and ignored determinants in the treatment outcome of CL is poor treatment adherence (PTA). PTA is an overlooked and widespread phenomenon to proper Leishmania treatment. This study aimed to explore the effect of poor adherence in unresponsiveness to treatment in patients with anthroponotic CL (ACL) by comparing conventional statistical modalities and machine learning analyses in Iran. Overall, 190 cases consisting of 50 unresponsive patients (case group), and 140 responsive patients (control group) with ACL were randomly selected. The data collecting form that included 25 queries (Q) was recorded for each case and analyzed by R software and genetic algorithm (GA) approaches. Complex treatment regimens (Q11), cultural and lay views about the disease and therapy (Q8), life stress, hopelessness and negative feelings (Q22), adverse effects of treatment (Q13), and long duration of the lesion (Q12) were the most prevalent significant variables that inhibited effective treatment adherence by the two methods, in decreasing order of significance. In the inherent algorithm approach, similar to the statistical approach, the most significant feature was complex treatment regimens (Q11). Providing essential knowledge about ACL and treatment of patients with chronic diseases and patients with misconceptions about chemical drugs are important issues directly related to the disease's unresponsiveness. Furthermore, early detection of patients to prevent the long duration of the disease and the process of treatment, efforts to minimize side effects of treatment, induction of positive thinking, and giving hope to patients with stress and anxiety by medical staff, and family can help patients adhere to the treatment.}
}
@article{BRIGNONE2021232,
title = {Moment-matching approximations for stochastic sums in non-Gaussian Ornstein–Uhlenbeck models},
journal = {Insurance: Mathematics and Economics},
volume = {96},
pages = {232-247},
year = {2021},
issn = {0167-6687},
doi = {https://doi.org/10.1016/j.insmatheco.2020.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167668720301645},
author = {Riccardo Brignone and Ioannis Kyriakou and Gianluca Fusai},
keywords = {Mean reversion, Non-Gaussian processes, Moment-matching, Asian option valuation, Stochastic annuities},
abstract = {In this paper, we recall actuarial and financial applications of sums of dependent random variables that follow a non-Gaussian mean-reverting process and contemplate distribution approximations. Our work complements previous related studies restricted to lognormal random variables; we revisit previous approximations and suggest new ones. We then derive moment-based distribution approximations for random sums attuned to Asian option pricing and computation of risk measures of random annuities. Various numerical experiments highlight the speed–accuracy benefits of the proposed methods.}
}
@article{YUWONO2024100272,
title = {Co-creation in action: Bridging the knowledge gap in artificial intelligence among innovation champions},
journal = {Computers and Education: Artificial Intelligence},
volume = {7},
pages = {100272},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100272},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000754},
author = {Elizabeth Irenne Yuwono and Dian Tjondronegoro and Carla Riverola and Jennifer Loy},
keywords = {Artificial intelligence, K-12 AI education, AI learning, AI co-creation, AI literacy, AI innovation},
abstract = {The increasing significance of artificial intelligence (AI) in various industries highlights the necessity for industry leaders and professionals to comprehend and gain knowledge about AI. The urgency for AI literacy is more critical than ever due to the potential unethical use of AI resulting from insufficient knowledge. This issue is particularly crucial for educators because they need to understand and adapt to the impact of AI within educational institutions, with some needing to use and become literate in AI, given that students now have increased access to public AI tools. This research presents a multi-phase study to address the issue of bridging the knowledge gap in AI by conducting co-creation with innovation champions, illustrated by a case of Generative AI innovation in a K-12 school. Using action design research, we engaged experienced teachers who are experts in pedagogical innovation to co-create a generative AI-enhanced platform at a leading K-12 education institution known for its pedagogical innovation in Australia. The findings reveal that champions enhance their knowledge through their subject-matter expertise, organizational knowledge, and AI knowledge gained through external exposure and experience. The study also highlights the key elements that facilitate a cross-domain knowledge exchange platform, enabling champions to be exposed to and experience AI technological learning, leading to shifts in their understanding and perception of AI. The initially unaware and sceptical champions become more aware and capable of articulating more technical AI knowledge rooted in a shared value. This research demonstrates how co-creation serves as a pathway for learning AI, particularly among K-12 teachers who are innovation champions. It underscores the impact of experiential and organizational learning on AI collaborative learning and behavioral intentions. Additionally, the study presents that aligning organizational and personal visions and values can influence perceptions about AI technologies, enhancing the discourse on AI and education innovation.}
}
@article{ZHOU2025106971,
title = {Integration of deep learning in the diagnosis, chemical analysis, and therapeutic approaches for neurodegenerative disorders},
journal = {Biomedical Signal Processing and Control},
volume = {100},
pages = {106971},
year = {2025},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2024.106971},
url = {https://www.sciencedirect.com/science/article/pii/S1746809424010292},
author = {Wdeclarencan Zhou and Chao Sun and Lihong Liu},
keywords = {Neurodegenerative Disorders (NDD), Diagnosis, Linear Discriminant Analysis (LDA), Chemical reaction optimization based improved generative adversarial network (CRO-IGAN)},
abstract = {Neurodegenerative disorders (NDD) are a group of progressive conditions that primarily affect neurons in the brain. The diseases gradually impair cognitive function, movement, and other neurological processes, leading to a decline in the individual’s quality of life. Reliable biomarkers that accurately detect and track the growth of neurological disorders are crucial for the development of effective therapeutics. People with NDDs have damage to the brainneurons, which causes strange walking patterns. To overcome this problem we proposed Chemical reaction optimization based improved generative adversarial network (CRO-IGAN) method is to improve the patient’s conditions that are affected in neurodegenerative diseases. The patient’s dataset is gathered, here we utilized min max normalization for data preprocessing is used to clean the data. Principal component analysis (PCA) is employed for feature extraction to extract the pre-processed data and remove the unwanted data. The appropriate data is selected using linear discriminant analysis (LDA) for feature selection. The parameter metrics used in this study are recall, sensitivity and specificity. The suggest techniques CRO-IGAN provides high performance of accuracy for diagnosing NDD to improve the patient health which provides a superior performance than other existing methods.}
}
@article{MARZUNI2021188,
title = {Cross-MapReduce: Data transfer reduction in geo-distributed MapReduce},
journal = {Future Generation Computer Systems},
volume = {115},
pages = {188-200},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20305847},
author = {Saeed Mirpour Marzuni and Abdorreza Savadi and Adel N. Toosi and Mahmoud Naghibzadeh},
keywords = {MapReduce, Geo-distributed, Data center, Big data},
abstract = {The MapReduce model is widely used to store and process big data in a distributed manner. MapReduce was originally developed for a single tightly coupled cluster of computers. Approaches such as Hierarchical and Geo-Hadoop are designed to address geo-distributed MapReduce processing. However, these methods still suffer from high inter-cluster data transfer over the Internet, which is prohibitive for processing today’s globally big data. In line with our thinking that there is no need to transfer the entire intermediate results to a single global reducer, we propose Cross-MapReduce, a framework for geo-distributed MapReduce processing. Before any massive data transfer, our proposed method finds a set of best global reducers to minimize transferred data volumes. We propose a graph called Global Reduction Graph (GRG) to determine the number and the locations of the global reducers. We conducted extensive experimental evaluations using a real testbed to demonstrate the effectiveness of Cross-MapReduce. The experimental results show that Cross-MapReduce significantly outperforms the Hierarchical and Geo-Hadoop approaches and reduces the amount of data transfer over the Internet by 40%.}
}
@article{JOOKEN202336,
title = {Features for the 0-1 knapsack problem based on inclusionwise maximal solutions},
journal = {European Journal of Operational Research},
volume = {311},
number = {1},
pages = {36-55},
year = {2023},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2023.04.023},
url = {https://www.sciencedirect.com/science/article/pii/S0377221723003065},
author = {Jorik Jooken and Pieter Leyman and Patrick {De Causmaecker}},
keywords = {Combinatorial optimization, 0-1 knapsack problem, Packing, Problem instance hardness, Instance space analysis},
abstract = {Decades of research on the 0-1 knapsack problem led to very efficient algorithms that are able to quickly solve large problem instances to optimality. This prompted researchers to also investigate the structure of problem instances that are hard for existing solvers. In the current paper we are interested in investigating which features make 0-1 knapsack problem instances hard to solve to optimality for the state-of-the-art 0-1 knapsack solver. We propose a set of 14 features based on previous work by the authors in which so-called inclusionwise maximal solutions (IMSs) play a central role. Calculating these features is computationally expensive and requires one to solve hard combinatorial problems. Based on new structural results about IMSs, we formulate polynomial and pseudopolynomial time algorithms for calculating these features. These algorithms were executed for two large datasets on a supercomputer in approximately 540 CPU-hours. We show that the proposed features contain important information related to the empirical hardness of a problem instance that was missing in earlier features from the literature by training machine learning models that can accurately predict the empirical hardness of a wide variety of 0-1 knapsack problem instances. Moreover, we show that these features can be cheaply approximated at the cost of less accurate hardness predictions. Using the instance space analysis methodology, we show that hard 0-1 knapsack problem instances are clustered together around a relatively dense region of the instance space and several features behave differently in the easy and hard parts of the instance space.}
}
@article{WARING2015254,
title = {Managerial and non-technical factors in the development of human-created disasters: A review and research agenda},
journal = {Safety Science},
volume = {79},
pages = {254-267},
year = {2015},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2015.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S0925753515001575},
author = {Alan Waring},
keywords = {Major hazards, Disasters, Safety management, Safety culture, Risk decisions},
abstract = {A number of common underlying factors in the development of human-created disasters, as cited in numerous official inquiry reports, encompass in particular, safety management system defects and weaknesses in an organization’s safety culture. Human factors such as faulty risk cognition, bounded rationality, groupthink, failure of foresight and organizational learning, suspect motivations, reactive attitudes, and inappropriate risk decision-making, are commonly associated characteristics of such shortcomings. This article summarizes and discusses underlying managerial and non-technical factors in human-created major hazard accidents in the light of theories of accident causation, findings from disaster inquiries and published research, and the systemic holism-versus-reductionism debate. Ideally, all site operators would know and understand disaster aetiology and preventive requirements and be motivated to enact them. However, there is sufficient empirical evidence from inquiry reports into major hazard incidents and disasters that idealized enactment rarely occurs and in many cases safety policy and strategy as enacted is distant from espoused safety policy and strategy. Research questions relating to board level thinking and actions on major hazard risks are posited and a proposal for a more holistic and potentially more effective major hazard safety research framework is put forward.}
}
@article{LI2021151508,
title = {Reverse vaccinology approach for the identifications of potential vaccine candidates against Salmonella},
journal = {International Journal of Medical Microbiology},
volume = {311},
number = {5},
pages = {151508},
year = {2021},
issn = {1438-4221},
doi = {https://doi.org/10.1016/j.ijmm.2021.151508},
url = {https://www.sciencedirect.com/science/article/pii/S1438422121000370},
author = {Jie Li and Jingxuan Qiu and Zhiqiang Huang and Tao Liu and Jing Pan and Qi Zhang and Qing Liu},
keywords = {, Reverse vaccinology, Computational model, Vaccine target, Immunoprotective},
abstract = {Salmonella is a leading cause of foodborne pathogen which causes intestinal and systemic diseases across the world. Vaccination is the most effective protection against Salmonella, but the identification and design of an effective broad-spectrum vaccine is still a great challenge, because of the multi-serotypes of Salmonella. Reverse vaccinology is a new tool to discovery and design vaccine antigens combining human immunology, structural biology and computational biology with microbial genomics. In this study, reverse vaccinology, an in-silico approach was established to screen appropriate immunogen targets by calculating the immunogenicity score of 583 non-redundant outer membrane and secreted proteins of Salmonella. Herein among 100 proteins identified with top-ranked scores, 15 representative antigens were selected randomly. Applying the sequence conservation test, four proteins (FliK, BcsZ, FhuA and FepA) remained as potential vaccine candidates for in vivo evaluation of immunogenicity and immunoprotection. All four candidates were capable to trigger the immune response and stimulate the production of antiserum in mice. Furthermore, top-ranked proteins including FliK and BcsZ provided wide antigenic coverage among the multi-serotype of Salmonella. The S. Typhimurium LT2 challenge model used in mice immunized with FliK and BcsZ showed a high relative percentage survival (RPS) of 52.74 % and 64.71 % respectively. In conclusion, this study constructed an in-silico pipeline able to successfully pre-screen the vaccine targets characterized by high immunogenicity and protective immunity. We show that reverse vaccinology allowed screening of appropriate broad-spectrum vaccines for Salmonella.}
}
@article{ZAREI2024175691,
title = {Integrated nexus approach to assessing climate change impacts on grassland ecosystem dynamics: A case study of the grasslands in Tanzania},
journal = {Science of The Total Environment},
volume = {952},
pages = {175691},
year = {2024},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2024.175691},
url = {https://www.sciencedirect.com/science/article/pii/S0048969724058479},
author = {Azin Zarei and Kaveh Madani and Edeltraud Guenther and Hamid Mohammadi Nasrabadi and Holger Hoff},
keywords = {Climate change impacts, Emission scenarios, Vegetation dynamics, Nexus approach, Vulnerability},
abstract = {This study addresses the intricate interplay between climate, vegetation, and livestock dynamics in Tanzania within the Climate-Vegetation-Livestock (CVL) nexus through a quantitative assessment. By examining the temporal and spatial relationships between vegetation indices (NDVI, EVI, NPP) and key climatic variables (Precipitation, Temperature, Evapotranspiration) from 2009 to 2019, and projecting to 2050, this research aims to elucidate vegetation responses to climate change and its subsequent impacts on livestock. To this end, the relationship between the vegetation dynamics indicators (NDVI, NPP) and climate parameters is evaluated to quantify the vegetation response to climate change using statistical models. Next, an examination of multicollinearity is conducted to investigate potential interactions (nexus) between variables, incorporating the correlation among independent variables. Notably, the evaluation of performance and accuracy for the mentioned models is conducted through the cross-validation method and validation indices. Ultimately, the variation between projected NPP and NDVI (average for 2040–2060) and the present NPP and NDVI (average for 2009–2020) identifies the regions that are most likely susceptible, showcasing the vegetation cover's reaction to climate change in different emission scenarios. The results unveil significant spatio-temporal variations in vegetation dynamics influenced by climatic factors, where higher precipitation and temperatures correlate with increased vegetation health and productivity. The projected fluctuations in NDVI and NPP values indicate varying trends across different regions, with a general decrease in vegetation density and productivity from the northeast to the west under both RCP2.6 and RCP8.5 scenarios by 2050. This decline is attributed to anticipated changes in precipitation and temperature patterns driven by climate change. Furthermore, significant declines in vegetation density and productivity under emission scenarios, particularly in the southern regions compared to the present, suggest greater vulnerability to climate change impacts. This highlights the need for targeted mitigation strategies in these vulnerable areas. Meanwhile, northeast areas under both NDVI and NPP will remain unchanged across both climate scenarios. Moreover, analysis of livestock distribution maps indicates areas of vulnerability under climate change scenarios, with implications for future livestock management and agricultural practices. These findings underscore the importance of proactive planning and targeted interventions to enhance resilience and sustainable development in vulnerable regions, emphasizing the need for integrated approaches that consider the complex interactions between climate, vegetation, and livestock dynamics.}
}
@article{SCHOLL201856,
title = {Understanding psychiatric disorder by capturing ecologically relevant features of learning and decision-making},
journal = {Behavioural Brain Research},
volume = {355},
pages = {56-75},
year = {2018},
note = {SI: MCC 2016},
issn = {0166-4328},
doi = {https://doi.org/10.1016/j.bbr.2017.09.050},
url = {https://www.sciencedirect.com/science/article/pii/S0166432817305673},
author = {Jacqueline Scholl and Miriam Klein-Flügge},
keywords = {Reinforcement learning, Decision-making, Computational psychiatry},
abstract = {Recent research in cognitive neuroscience has begun to uncover the processes underlying increasingly complex voluntary behaviours, including learning and decision-making. Partly this success has been possible by progressing from simple experimental tasks to paradigms that incorporate more ecological features. More specifically, the premise is that to understand cognitions and brain functions relevant for real life, we need to introduce some of the ecological challenges that we have evolved to solve. This often entails an increase in task complexity, which can be managed by using computational models to help parse complex behaviours into specific component mechanisms. Here we propose that using computational models with tasks that capture ecologically relevant learning and decision-making processes may provide a critical advantage for capturing the mechanisms underlying symptoms of disorders in psychiatry. As a result, it may help develop mechanistic approaches towards diagnosis and treatment. We begin this review by mapping out the basic concepts and models of learning and decision-making. We then move on to consider specific challenges that emerge in realistic environments and describe how they can be captured by tasks. These include changes of context, uncertainty, reflexive/emotional biases, cost-benefit decision-making, and balancing exploration and exploitation. Where appropriate we highlight future or current links to psychiatry. We particularly draw examples from research on clinical depression, a disorder that greatly compromises motivated behaviours in real-life, but where simpler paradigms have yielded mixed results. Finally, we highlight several paradigms that could be used to help provide new insights into the mechanisms of psychiatric disorders.}
}
@article{YUAN2023107911,
title = {Surface profile evolution model for titanium alloy machined using abrasive waterjet},
journal = {International Journal of Mechanical Sciences},
volume = {240},
pages = {107911},
year = {2023},
issn = {0020-7403},
doi = {https://doi.org/10.1016/j.ijmecsci.2022.107911},
url = {https://www.sciencedirect.com/science/article/pii/S0020740322007895},
author = {Yemin Yuan and Jianfeng Chen and Hang Gao},
keywords = {Abrasive waterjet, Ti-6Al-4V alloy, Computational fluid dynamics (CFD), Stagnation zone, Surface profile evolution},
abstract = {The surface profile evolution model, which was initially developed for glass and polymers, can accurately predict a channel profile cross-section produced by abrasive jet (AJ) machining. In this study, the model is modified and applied for estimating the profiles of a Ti-6Al-4V alloy eroded by an abrasive waterjet (AWJ). First, the velocity and mass fraction distributions of the gas–liquid–solid phases in the AWJ at the nozzle exit were derived and compared, and several improvements were proposed, such as considering the divergence angle of the jet and particles, as well as the length of the jet core area, to precisely construct a theoretical connection of the erosion efficiency distribution before impacting the workpiece. Computational fluid dynamics (CFD) simulations were then performed to investigate the behaviour of the erosion jets during surface evolution. The results revealed that the jet diffusion provoked by the stagnation zone effect became more pronounced as the surface profile depth deepened, which led to jet directional deflection and suppressed the erosion capacities of the AWJ. Therefore, a central erosion depth function was introduced to correct this detrimental effect with the intention of obtaining an accurate channel profile. In addition, a second-order single-step fitting function was suggested to eliminate the fluctuations caused by uneven abrasive particles and the problem of reduced erosion efficiency due to channel depth variation. Finally, based on the determination of the parameters affecting the channel profile, a normalised centre erosion rate function, which only depends on the channel depth and is isolated from the material properties and the standoff distance, was recommended to simplify the calculation. The erosion function conforming to a Gaussian surface was fitted using MATLAB (R2019b, MathWorks, USA). The results demonstrated that the channel profiles predicted by the surface evolution model were consistent with the measured profiles, with an average error of 11.4%.}
}
@article{IVANITSKY2009101,
title = {Brain science: On the way to solving the problem of consciousness},
journal = {International Journal of Psychophysiology},
volume = {73},
number = {2},
pages = {101-108},
year = {2009},
note = {Neural Processes in Clinical Psychophysiology},
issn = {0167-8760},
doi = {https://doi.org/10.1016/j.ijpsycho.2009.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167876009001044},
author = {Alexey M. Ivanitsky and George A. Ivanitsky and Olga V. Sysoeva},
keywords = {Consciousness and brain problem, Event-related potentials, EEG rhythms, Semantic brain systems, Artificial intelligence},
abstract = {Four issues are discussed: the possible mechanism of subjective events, conscious versus unconscious brain functions, the rhythmic coding of mental operations and the possible brain basis of understanding.i.Several approaches have been developed to explain how subjective experience emerges from brain activity. One of them is the return of the nervous impulses to the sites of their primary projections, providing a synthesis of sensory information with memory and motivation [Ivanitsky, A.M., 1976. Brain Mechanisms of the Signal Evaluation. Medicina, Moscow 264 pp. (in Russian)]. Support for the existence of such a mechanism stems from studies upon the brain activity that subserves perception (visual and somato-sensory) and thought (verbal and imaginative). The cortical centres for information synthesis have been found. For perception, these are located in projection areas; for thinking — in frontal and temporal-parietal associative cortex. Closely related ideas were also developed by G. Edelman [Edelman, G.M., 1978. Group selection and phasic reentrant signaling: A theory of higher brain function. In: Eds. Edelman, G.M., Mountcastle, V.B. The Mindful Brain. Cortical Organization and the Group-selective Theory of Higher Brain Function. Cambridge, MA, MIT Press, pp 51–100.] in his re-entry theory of consciousness. Both theories emphasize the key role of memory and motivation in the origin of conscious function.ii.Conscious experience elucidates not all, but only salient brain functions. As a rule, voluntary control is switched on when additional cognitive resources are needed. Even a rather complicated mental operation, such as the discrimination between concrete and abstract words, could be executed very rapidly and implicitly; explicit analysis being engaged only in more difficult tasks. Furthermore, these two different kinds of mental operations, i.e., automatic and conscious, are predominantly associated with two different kinds of memory: a recognition memory for implicit analysis, and an episodic memory for explicit functions.iii.Rearrangements of EEG rhythms underlie mental functions. Certain rhythmical patterns are related with definite types of mental activity. The dependence of one upon the other is rather pronounced and expressive, so it becomes possible to recognize the type of mental operation being performed in mind with few seconds of the ongoing EEG, provided that the analysis of rhythms is accomplished using an artificial neural network.iv.It is commonly recognized that the computer, in contrast to the living brain, can calculate, yet cannot understand [Penrose, R., 1996. Shadows of the Mind: A Search for the Missing Science of Consciousness New York, Oxford, Oxford University Press 480 pp.]. Comprehension implies the comparison of new and old information that requires the ability to search for associations, grouping similar objects together, and distinguishing different objects from one another. However, these functions may also be implemented on a computer. Still, it is believed that computers perform these complicated operations without genuine understanding. Evidently, comprehension additionally has to be based upon some biologically significant ground. It is hypothesized that the subjective feeling of understanding appears when current information is attributed to a definite need, which is scaled in sign (+/−) coordinates. This coordinate system ceases the brain calculations, when “comprehension” is reached, i.e., the acceptable level of need satisfaction is attained.}
}
@article{RADAIDEH2021106836,
title = {Rule-based reinforcement learning methodology to inform evolutionary algorithms for constrained optimization of engineering applications},
journal = {Knowledge-Based Systems},
volume = {217},
pages = {106836},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.106836},
url = {https://www.sciencedirect.com/science/article/pii/S095070512100099X},
author = {Majdi I. Radaideh and Koroush Shirvan},
keywords = {Reinforcement learning, RL-guided evolutionary computation, proximal policy optimization, constrained combinatorial optimization, nuclear fuel assembly},
abstract = {For practical engineering optimization problems, the design space is typically narrow, given all the real-world constraints. Reinforcement Learning (RL) has commonly been guided by stochastic algorithms to tune hyperparameters and leverage exploration. Conversely in this work, we propose a rule-based RL methodology to guide evolutionary algorithms (EA) in constrained optimization. First, RL proximal policy optimization agents are trained to master matching some of the problem rules/constraints, then RL is used to inject experiences to guide various evolutionary/stochastic algorithms such as genetic algorithms, simulated annealing, particle swarm optimization, differential evolution, and natural evolution strategies. Accordingly, we develop RL-guided EAs, which are benchmarked against their standalone counterparts. RL-guided EA in continuous optimization demonstrates significant improvement over standalone EA for two engineering benchmarks. The main problem analyzed is nuclear fuel assembly combinatorial optimization with high-dimensional and computationally expensive physics. The results demonstrate the ability of RL to efficiently learn the rules that nuclear fuel engineers follow to realize candidate solutions. Without these rules, the design space is large for RL/EA to find many candidates. With imposing the rule-based RL methodology, we found that RL-guided EA outperforms standalone algorithms by a wide margin, with >10 times improvement in exploration capabilities and computational efficiency. These insights imply that when facing a constrained problem with numerous local optima, RL can be useful in focusing the search space in the areas where expert knowledge has demonstrated merit, while evolutionary/stochastic algorithms utilize their exploratory features to improve the number of feasible solutions.}
}
@article{PANTALEON201579,
title = {Taylor series expansion using matrices: An implementation in MATLAB®},
journal = {Computers & Fluids},
volume = {112},
pages = {79-82},
year = {2015},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2015.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0045793015000183},
author = {Carlos Pantaleón and Amitabha Ghosh},
keywords = {Taylor series, Finite differences, Truncation error, Modified equation, Symbolic computation},
abstract = {Taylor series expansions are widely used in engineering approximations, for instance, to develop finite differences schemes or numerical integration methods. This technical note presents a novel technique to generate, display and manipulate Taylor series expansion by using matrices. The resulting approach allows algebraic manipulation as well as differentiation in a very intuitive manner in order to experiment with different numerical schemes, their truncation errors and their structures, while avoiding manual calculation errors. A detailed explanation of the mathematical procedure to generate a matrix form of the Taylor series expansion for a function of two variables is presented along with the algorithm of an implementation in MATLAB®. Example cases of different orders are tabulated to illustrate the generation and manipulation capabilities of this technique. Additionally, an extended application is developed to determine the modified equations of finite difference schemes for partial differential equations, with one-dimensional examples of the wave equation and the heat equation using explicit and implicit schemes.}
}
@article{RECANATINI20208653,
title = {Drug Research Meets Network Science: Where Are We?},
journal = {Journal of Medicinal Chemistry},
volume = {63},
number = {16},
pages = {8653-8666},
year = {2020},
issn = {1520-4804},
doi = {https://doi.org/10.1021/acs.jmedchem.9b01989},
url = {https://www.sciencedirect.com/science/article/pii/S1520480420001702},
author = {Maurizio Recanatini and Chiara Cabrelle},
abstract = {Network theory provides one of the most potent analysis tools for the study of complex systems. In this paper, we illustrate the network-based perspective in drug research and how it is coherent with the new paradigm of drug discovery. We first present data sources from which networks are built, then show some examples of how the networks can be used to investigate drug-related systems. A section is devoted to network-based inference applications, i.e., prediction methods based on interactomes, that can be used to identify putative drug–target interactions without resorting to 3D modeling. Finally, we present some aspects of Boolean networks dynamics, anticipating that it might become a very potent modeling framework to develop in silico screening protocols able to simulate phenotypic screening experiments. We conclude that network applications integrated with machine learning and 3D modeling methods will become an indispensable tool for computational drug discovery in the next years.
}
}
@article{RZHETSKY20089,
title = {Seeking a New Biology through Text Mining},
journal = {Cell},
volume = {134},
number = {1},
pages = {9-13},
year = {2008},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2008.06.029},
url = {https://www.sciencedirect.com/science/article/pii/S0092867408008167},
author = {Andrey Rzhetsky and Michael Seringhaus and Mark Gerstein},
abstract = {Tens of thousands of biomedical journals exist, and the deluge of new articles in the biomedical sciences is leading to information overload. Hence, there is much interest in text mining, the use of computational tools to enhance the human ability to parse and understand complex text.}
}
@article{NELSON2019100758,
title = {Designing and transforming yield-stress fluids},
journal = {Current Opinion in Solid State and Materials Science},
volume = {23},
number = {5},
pages = {100758},
year = {2019},
issn = {1359-0286},
doi = {https://doi.org/10.1016/j.cossms.2019.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1359028619300762},
author = {Arif Z. Nelson and Kenneth S. Schweizer and Brittany M. Rauzan and Ralph G. Nuzzo and Jan Vermant and Randy H. Ewoldt},
keywords = {Soft matter, Yield-stress fluid, Design, Engineering, Extension, Thixotropy, Elasticity, Colloids, Emulsions, Polymers, 3D printing, Chemistry, Physics, Rheology, Complex fluids},
abstract = {We review progress in designing and transforming multi-functional yield-stress fluids and give a perspective on the current state of knowledge that supports each step in the design process. We focus mainly on the rheological properties that make yield-stress fluids so useful and the trade-offs which need to be considered when working with these materials. Thinking in terms of “design with” and “design of” yield-stress fluids motivates how we can organize our scientific understanding of this field. “Design with” involves identification of rheological property requirements independent of the chemical formulation, e.g. for 3D direct-write printing which needs to accommodate a wide range of chemistry and material structures. “Design of” includes microstructural considerations: conceptual models relating formulation to properties, quantitative models of formulation-structure-property relations, and chemical transformation strategies for converting effective yield-stress fluids to be more useful solid engineering materials. Future research directions are suggested at the intersection of chemistry, soft-matter physics, and material science in the context of our desire to design useful rheologically-complex functional materials.}
}
@article{LASSITER201927,
title = {Language and simplexity: A powers view},
journal = {Language Sciences},
volume = {71},
pages = {27-37},
year = {2019},
note = {Simplexity, agency and language},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2018.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0388000118300366},
author = {Charles Lassiter},
keywords = {Distributed language, Simplexity, Causal powers, Speech acts},
abstract = {The notion of simplexity is that complex problems are often solved by novel combinations of simple mechanisms. These solutions aren't simple; they're simplex. Language use, as a complex behavior, is ripe for simplex analysis. In this paper, I argue that the notion of powers—an organism's capacity to instigate or undergo change—is doubly useful. First, powers, as opposed to mental representations, are a suitable object for simplex analysis. So conceptualizing languaging in terms of powers gets us one step closer to a simplex analysis of language. But thinking of languaging in terms of powers has an additional payoff. Berthoz asserts that the concept of simplexity is related to the concept of meaning. How they're related is unclear. Conceptualizing languaging in terms of powers injects meaningfulness into lived world of the organism. Consequently, the concept of powers can act as a bridge between the concepts of meaningfulness and simplexity.}
}
@incollection{HEFNER20163,
title = {Chapter 1 - A Brief History of Biological Distance Analysis},
editor = {Marin A. Pilloud and Joseph T. Hefner},
booktitle = {Biological Distance Analysis},
publisher = {Academic Press},
address = {San Diego},
pages = {3-22},
year = {2016},
isbn = {978-0-12-801966-5},
doi = {https://doi.org/10.1016/B978-0-12-801966-5.00001-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128019665000019},
author = {J.T. Hefner and M.A. Pilloud and J.E. Buikstra and C.C.M. Vogelsberg},
keywords = {aDNA, Analytical scales, Biodistance, Cranial nonmetric traits, Craniometrics, Kinship, Odontometrics, Typology},
abstract = {Biological distance, or biodistance, analysis employs data derived from skeletal remains to reflect population relatedness (similarity/dissimilarity) through the application of multivariate statistical methods. The approaches used in biodistance studies have changed markedly over recent centuries, exploring phenotypic expressions assumed to be informative. Biodistance analysis began as the study of anomalous variants in the human skull, but the field has transformed over the centuries now seeking to incorporate skeletal morphology in the interpretation of genetic affinity, providing insight into the genetics governing trait expression, and providing understanding into the role of developmental biology on the expression of morphological variants. As methodological approaches improve, so too has the application of these analyses. We present here a brief historical overview of biodistance analysis research, focusing on meta-themes in the field, shifts in thinking among researchers in biological anthropology, and several of the outside influences that impact biodistance analysis.}
}
@article{ROBERTS2020116758,
title = {Creative, internally-directed cognition is associated with reduced BOLD variability},
journal = {NeuroImage},
volume = {219},
pages = {116758},
year = {2020},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2020.116758},
url = {https://www.sciencedirect.com/science/article/pii/S1053811920302457},
author = {Reece P. Roberts and Cheryl L. Grady and Donna Rose Addis},
keywords = {Episodic simulation, Imagination, Creativity, BOLD variability},
abstract = {In a range of externally-directed tasks, intra-individual variability of fMRI BOLD signal has been shown to be a stronger predictor of cognitive performance than mean BOLD signal. BOLD variability’s strong association with cognitive performance is hypothesised to be due to it capturing the dynamic range of neural systems. Although increased BOLD variability is also speculated to play a role in internally-directed thought, particularly when creative and flexible cognition is required, there is a relative lack of research exploring whether BOLD variability is related to internally-directed cognition. Thus, we investigated the relationship between BOLD variability and a key component of creativity – divergent thinking – in various tasks that required participants to think flexibly. We also determined whether any associations between BOLD variability and creativity overlapped with, or differed, from associations between mean BOLD signal and creativity. First, we performed task Partial Least Squares (PLS) analyses that compared BOLD signal (either mean or variability) during two future imagination conditions that differed in the amount of cognitive flexibility required: a Congruent condition in which autobiographical details (people, places, objects) comprising an imagined event belonged to the same social sphere (e.g., university) and an Incongruent condition in which details belonged to different social spheres and required greater cognitive flexibility to integrate. Results indicated that the Incongruent condition was associated with a widespread reduction in both BOLD variability and mean signal (relative to the Congruent condition), but in largely non-overlapping regions. Next, we used behavioral PLS to determine whether individual differences in performance on future simulation tasks as well as the Alternate Uses Task relates to BOLD variability and mean BOLD signal. Better performance on these tasks was predominantly associated with increases in mean BOLD signal and decreases in BOLD variability, in a range of disparate brain regions. Together, the results suggest that, unlike tasks requiring externally-directed cognition, superior performance on tasks requiring creative internal mentation is associated with less (not more) variability.}
}
@article{WANG2023101451,
title = {Unpacking the essential tension of knowledge recombination: Analyzing the impact of knowledge spanning on citation impact and disruptive innovation},
journal = {Journal of Informetrics},
volume = {17},
number = {4},
pages = {101451},
year = {2023},
issn = {1751-1577},
doi = {https://doi.org/10.1016/j.joi.2023.101451},
url = {https://www.sciencedirect.com/science/article/pii/S1751157723000767},
author = {Cheng-Jun Wang and Lihan Yan and Haochuan Cui},
keywords = {Knowledge Recombination, Category spanning, Disruption, Citation, Team size},
abstract = {Drawing on the theories of knowledge recombination, we aim to unpack the essential tension between tradition and innovation in scientific research. Using the American Physical Society data and computational methods, we analyze the relationship between knowledge spanning, citation impact, and disruptive innovation. The findings show that there exists a U-shaped relationship between knowledge spanning and disruptive innovation. In contrast, there is an inverted U-shaped relationship between knowledge spanning and citation impact, and the inverted U-shaped effect is moderated by team size. This study contributes to the theories of knowledge recombination by suggesting that either intellectual conformism or knowledge recombination can lead to disruptive innovation. That is, when evaluating the quality of scientific research with disruptive innovation, the essential tension seems to disappear.}
}
@article{AKTAR2025107587,
title = {Architecture decisions in quantum software systems: An empirical study on Stack Exchange and GitHub},
journal = {Information and Software Technology},
volume = {177},
pages = {107587},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107587},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924001927},
author = {Mst Shamima Aktar and Peng Liang and Muhammad Waseem and Amjed Tahir and Aakash Ahmad and Beiqi Zhang and Zengyang Li},
keywords = {Architecture decision, Quantum software system, Stack Exchange, GitHub, Empirical study},
abstract = {Context:
Quantum computing provides a new dimension in computation, utilizing the principles of quantum mechanics to potentially solve complex problems that are currently intractable for classical computers. However, little research has been conducted about the architecture decisions made in quantum software development, which have a significant influence on the functionality, performance, scalability, and reliability of these systems.
Objective:
The study aims to empirically investigate and analyze architecture decisions made during the development of quantum software systems, identifying prevalent challenges and limitations by using the posts and issues from Stack Exchange and GitHub.
Methods:
We used a qualitative approach to analyze the obtained data from Stack Exchange Sites and GitHub projects — two prominent platforms in the software development community. Specifically, we collected data from 385 issues (from 87 GitHub projects) and 70 posts (from 3 Stack Exchange sites) related to architecture decisions in quantum software development.
Results:
The results show that in quantum software development (1) architecture decisions are articulated in six linguistic patterns, the most common of which are Solution Proposal and Information Giving, (2) the two major categories of architectural decisions are Implementation Decision and Technology Decision, (3) Software Development Tools are the most common application domain among the twenty application domains identified, (4) Maintainability is the most frequently considered quality attribute, and (5) Design Issues and High Error Rates are the major limitations and challenges that practitioners face when making architecture decisions in quantum software development.
Conclusions:
Our results show that the limitations and challenges encountered in architecture decision-making during the development of quantum software systems are strongly linked to the particular features (e.g., quantum entanglement, superposition, and decoherence) of those systems. These issues mostly pertain to technical aspects and need appropriate measures to address them effectively.}
}
@incollection{WALSH2017,
title = {Sensory Systems},
booktitle = {Reference Module in Neuroscience and Biobehavioral Psychology},
publisher = {Elsevier},
year = {2017},
isbn = {978-0-12-809324-5},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.06867-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012809324506867X},
author = {V. Walsh},
keywords = {Auditory system, Multisensory integration, Nerves, Somatosensation, Visual system},
abstract = {Sensory systems have an old school ring to them, a very old school ring to them. In the 15th century Benedetti was able to write, “By means of nerves, the pathways of the senses are distributed like the roots and fibers of a tree” (Alessandro Benedetti, 1497). This is still a good place to start because it gives one a feel for the 3D structure of our sensory apparatus, but the challenge of understanding the senses has, of course, gone well beyond structure (which is not to imply that all structural descriptions are complete or that we have joined all the dots of structure–function relationships), and any serious scholar needs to have a working knowledge of the development, physiology, psychophysics (physiology without the blood), genetics, pathology, and computational models of the senses.}
}
@article{KAIN2024104739,
title = {Mapping the landscape: A scoping review of 21st century skills literature in secondary education},
journal = {Teaching and Teacher Education},
volume = {151},
pages = {104739},
year = {2024},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2024.104739},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X24002713},
author = {Christina Kain and Corinna Koschmieder and Marlies Matischek-Jauk and Sabine Bergner},
keywords = {21 century skills, Secondary school, Scoping review, PRISMA flow chart},
abstract = {21st century skills prepare students to adapt to a rapidly changing world, ensuring their capability of continuous learning and problem-solving. This review provides a systematic overview of how 21st century skills are addressed in research. It focuses on the context of secondary education and uses a PRISMA flow diagram to analyze 82 research articles. Results reveal that research on 21st century skills focuses on educational stakeholders’ opinions and attitudes, their potential effects and how they are implemented or assessed. The findings highlight a need for research to enhance the implementation of 21st century skills in secondary education.}
}
@article{RUAN2022103133,
title = {Closed-form Minkowski sums of convex bodies with smooth positively curved boundaries},
journal = {Computer-Aided Design},
volume = {143},
pages = {103133},
year = {2022},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2021.103133},
url = {https://www.sciencedirect.com/science/article/pii/S0010448521001445},
author = {Sipu Ruan and Gregory S. Chirikjian},
keywords = {Minkowski sums, Computer-aided design, Computational geometry},
abstract = {This article derives closed-form parametric formulas for the Minkowski sums of convex bodies in d-dimensional Euclidean space with boundaries that are smooth and have all positive sectional curvatures at every point. Under these conditions, there is a unique relationship between the position of each boundary point and the surface normal. The main results are presented as two theorems. The first theorem directly parameterizes Minkowski sum boundaries using the unit normal vector at each surface point. Although simple to express mathematically, such a parameterization is not always practical to obtain computationally. Therefore, the second theorem derives a more useful parametric closed-form expression using the gradient that is not normalized. In the special case of two ellipsoids, the proposed expressions are identical to those derived previously using geometric interpretations. In order to examine the results, numerical validations and comparisons of the Minkowski sums between two superquadric bodies are conducted. Applications to generate configuration space obstacles in motion planning problems and to improve optimization-based collision detection algorithms are introduced and demonstrated.}
}
@incollection{SIEGEL20163,
title = {Chapter 1 - Introduction: Defining the Role of Statistics in Business},
editor = {Andrew F. Siegel},
booktitle = {Practical Business Statistics (Seventh Edition)},
publisher = {Academic Press},
edition = {Seventh Edition},
pages = {3-17},
year = {2016},
isbn = {978-0-12-804250-2},
doi = {https://doi.org/10.1016/B978-0-12-804250-2.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128042502000018},
author = {Andrew F. Siegel},
abstract = {We begin this chapter with an overview of the competitive advantage provided by a knowledge of statistical methods, followed by some basic facts about statistics and probability and their role in business. Statistical activities can be grouped into five main activities (designing, exploring, modeling, estimating, and hypothesis testing), and one way to clarify statistical thinking is to be able to match the business task at hand with the correct collection of statistical methods. This chapter sets the stage for the rest of the book, which follows up with many important detailed procedures for accomplishing business goals that involve these activities. Next follows an overview of data mining of Big Data (which involves these main activities) and its importance in business. Then we distinguish the field of probability (where, based on assumptions, we reach conclusions about what is likely to happen—a useful exercise in business where nobody knows for sure what will happen) from the field of statistics (where we know from the data what happened, from which we infer conclusions about the system that produced these data) while recognizing that probability and statistics will work well together in future chapters. The chapter concludes with some words of advice on how to integrate statistical thinking with other business viewpoints and activities.}
}
@article{BECK2024545,
title = {Understanding the cell: Future views of structural biology},
journal = {Cell},
volume = {187},
number = {3},
pages = {545-562},
year = {2024},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2023.12.017},
url = {https://www.sciencedirect.com/science/article/pii/S0092867423013491},
author = {Martin Beck and Roberto Covino and Inga Hänelt and Michaela Müller-McNicoll},
keywords = {structural biology, digital twin, computational modeling, cellular self-organization},
abstract = {Summary
Determining the structure and mechanisms of all individual functional modules of cells at high molecular detail has often been seen as equal to understanding how cells work. Recent technical advances have led to a flush of high-resolution structures of various macromolecular machines, but despite this wealth of detailed information, our understanding of cellular function remains incomplete. Here, we discuss present-day limitations of structural biology and highlight novel technologies that may enable us to analyze molecular functions directly inside cells. We predict that the progression toward structural cell biology will involve a shift toward conceptualizing a 4D virtual reality of cells using digital twins. These will capture cellular segments in a highly enriched molecular detail, include dynamic changes, and facilitate simulations of molecular processes, leading to novel and experimentally testable predictions. Transferring biological questions into algorithms that learn from the existing wealth of data and explore novel solutions may ultimately unveil how cells work.}
}
@article{CARLI2012119,
title = {Efficient algorithms for large scale linear system identification using stable spline estimators},
journal = {IFAC Proceedings Volumes},
volume = {45},
number = {16},
pages = {119-124},
year = {2012},
note = {16th IFAC Symposium on System Identification},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20120711-3-BE-2027.00394},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015379386},
author = {Francesca P. Carli and Alessandro Chiuso and Gianluigi Pillonetto},
keywords = {Parametric prediction error methods, output error models, model complexity, marginal likelihood, kernel eigenfunctions},
abstract = {A new nonparametric approach for system identification has been recently proposed where, in place of postulating parametric classes of impulse responses, the estimation process starts from an infinite-dimensional space. In particular, the impulse response is seen as the realization of a zero-mean Gaussian process. Its covariance, the so called stable spline kernel, encodes information on system stability and depends on few hyperparameters estimated from data via marginal likelihood optimization. This approach has been proved to compare much favorably with classical parametric methods but, in data rich situations, a possible drawback may be represented by its computational complexity which scales with the cube of the number of available samples. In this work we design a new computational strategy which may reduce significantly the computational load required by the stable spline estimator, thus extending its practical applicability also to large-scale scenarios.}
}
@article{HE2023126590,
title = {Global priors guided modulation network for joint super-resolution and SDRTV-to-HDRTV},
journal = {Neurocomputing},
volume = {554},
pages = {126590},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126590},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223007130},
author = {Gang He and Shaoyi Long and Li Xu and Chang Wu and Wenxin Yu and Jinjia Zhou},
keywords = {Convolutional neural network, Super-resolution, SDRTV-to-HDRTV, High dynamic range},
abstract = {Watching low resolution standard dynamic range (LR SDR) video on a 4K high dynamic range (HDR) TV is not the best viewing experience. Joint super-resolution (SR) and SDRTV-to-HDRTV aims to enhance the visual quality of LR SDR videos that have quality deficiencies in resolution and dynamic range. Previous methods that rely on learning local information typically cannot do well in preserving color conformity and long-range structural similarity, resulting in unnatural color transition and texture artifacts. In order to tackle these challenges, we propose a global priors guided modulation network (GPGMNet). In particular, we design a global priors extraction module (GPEM) to extract color conformity prior and structural similarity prior that are beneficial for SDRTV-to-HDRTV and SR tasks, respectively. To further exploit the global priors and preserve spatial information, we devise multiple global priors-guided spatial-wise modulation blocks (GSMBs) with a few parameters for intermediate feature modulation. In these GSMBs, the modulation parameters are generated by the shared global priors and the spatial features map from the spatial pyramid convolution block (SPCB). With these elaborate designs, the GPGMNet can achieve higher visual quality with lower computational complexity. Extensive experiments demonstrate that our proposed GPGMNet is superior to the state-of-the-art methods. Specifically, our proposed model exceeds the state-of-the-art by 0.64 dB in PSNR, with 69% fewer parameters and 3.1× speedup.}
}
@article{XIAO2023103864,
title = {APRS: Automatic pruning ratio search using Siamese network with layer-level rewardsImage 1},
journal = {Digital Signal Processing},
volume = {133},
pages = {103864},
year = {2023},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2022.103864},
url = {https://www.sciencedirect.com/science/article/pii/S105120042200481X},
author = {Huachao Xiao and Yangxin Wang and Jianyi Liu and Jiaxin Huo and Yang Hu and Yu Wang},
keywords = {Structured pruning, Deep reinforcement learning, Pruning ratio search, Siamese network},
abstract = {Structured pruning is still a mainstream model compression technique, for its merit of easy to implement and no reliance on specific hardware supporting library. In most previous works, the layer-wise channel pruning ratios were determined empirically. In this paper, we propose an Automatic Pruning Ratio Search (APRS) algorithm that can find the layer-wise optimal pruning ratio within the deep reinforcement learning framework. To solve the coarse-granularity reward problem existing in some previous works like AMC and CACP, a novel layer-level reward function is designed based on the Siamese network architecture for the fine-granularity agent-environment interaction purpose. We use a computationally efficient way to evaluate the effect of pruning action on each single layer. The incurred “backwardness disadvantage” problem has also been analyzed and addressed. The experiments are performed using the VGG-16, and MobileNet-v1 on the CIFAR10/100 and UC Merced Land-use datasets. The results verified that our method can better reveal the underlying sparse sensitivities of different layers in both high redundancy networks and compact networks, so that resulting a higher network accuracy after pruning compared to the traditional methods.}
}
@article{PALANCACASTAN2021e06268,
title = {Towards an interdisciplinary framework about intelligence},
journal = {Heliyon},
volume = {7},
number = {2},
pages = {e06268},
year = {2021},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2021.e06268},
url = {https://www.sciencedirect.com/science/article/pii/S240584402100373X},
author = {Nicolas Palanca-Castan and Beatriz {Sánchez Tajadura} and Rodrigo Cofré},
keywords = {Theoretical framework, Artificial intelligence, Philosophy, Non-human intelligence},
abstract = {In recent years, advances in science, technology, and the way in which we view our world have led to an increasingly broad use of the term “intelligence”. As we learn more about biological systems, we find more and more examples of complex and precise adaptive behavior in animals and plants. Similarly, as we build more complex computational systems, we recognize the emergence of highly sophisticated structures capable of solving increasingly complex problems. These behaviors show characteristics in common with the sort of complex behaviors and learning capabilities we find in humans, and therefore it is common to see them referred to as “intelligent”. These analogies are problematic as the term intelligence is inextricably associated with human-like capabilities. While these issues have been discussed by leading researchers of AI and renowned psychologists and biologists highlighting the commonalities and differences between AI and biological intelligence, there have been few rigorous attempts to create an interdisciplinary approach to the modern problem of intelligence. This article proposes a comparative framework to discuss what we call “purposeful behavior”, a characteristic shared by systems capable of gathering and processing information from their surroundings and modifying their actions in order to fulfill a series of implicit or explicit goals. Our aim is twofold: on the one hand, the term purposeful behavior allows us to describe the behavior of these systems without using the term “intelligence”, avoiding the comparison with human capabilities. On the other hand, we hope that our framework encourages interdisciplinary discussion to help advance our understanding of the relationships among different systems and their capabilities.}
}
@incollection{ALEKSANDER200599,
title = {Machine consciousness},
editor = {Steven Laureys},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {150},
pages = {99-108},
year = {2005},
booktitle = {The Boundaries of Consciousness: Neurobiology and Neuropathology},
issn = {0079-6123},
doi = {https://doi.org/10.1016/S0079-6123(05)50008-6},
url = {https://www.sciencedirect.com/science/article/pii/S0079612305500086},
author = {Igor Aleksander},
abstract = {The work from several laboratories on the modeling of consciousness is reviewed. This ranges, on one hand, from purely functional models where behavior is important and leads to an attribution of consciousness to, on the other hand, material work closely derived from the information about the anatomy of the brain. At the functional end of the spectrum, applications are described specifically directed at a job-finding problem, where the person being served should not discern between being served by a conscious human or a machine. This employs an implementation of global workspace theories. At the material end, attempts at modeling attentional brain mechanisms, and basic biochemical processes in children are discussed. There are also general prescriptions for functional schemas that facilitate discussions for the presence of consciousness in computational systems and axiomatic structures that define necessary architectural features without which it would be difficult to represent sensations. Another distinction between these two approaches is whether one attempts to model phenomenology (material end) or not (functional end). The former is sometimes called “synthetic phenomenology.” The upshot of this chapter is that studying consciousness through the design of machines is likely to have two major outcomes. The first is to provide a wide-ranging computational language to express the concept of consciousness. The second is to suggest a wide-ranging set of computational methods for building competent machinery that benefits from the flexibility of conscious representations.}
}
@article{CAMELODAZA2024101760,
title = {Parameter estimation in single-phase transformers via the generalized normal distribution optimizer while considering voltage and current measurements},
journal = {Results in Engineering},
volume = {21},
pages = {101760},
year = {2024},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2024.101760},
url = {https://www.sciencedirect.com/science/article/pii/S2590123024000136},
author = {Juan David Camelo-Daza and Diego Noel Betancourt-Alonso and Oscar Danilo Montoya and Ernesto Gómez-Vargas},
keywords = {Nonlinear optimization, Metaheuristic optimization algorithms, Generalized normal distribution optimizer, Parameter estimation, Single-phase transformers, Mean square error minimization, Voltage and current measurements},
abstract = {This research addresses, from a perspective of metaheuristic optimization, the problem regarding parametric estimation in single-phase transformers while considering voltage and current measures at the terminals of the transformer and weighing linear loads. Transformer parametric estimation is modeled as a nonlinear problem in order to minimize the mean square error between the calculated voltage and current variables and the measurements taken. The nonlinearities are associated with Kirchhoff's first and second laws applied to the equivalent electrical circuit of the single-phase transformer. The nonlinear optimization problem is solved by applying a metaheuristic optimization algorithm known as the generalized normal distribution optimizer (GNDO), which uses evolution rules that allow exploring and exploiting the solution space via the classical probability function based on normal distributions. Numerical results in three test transformers of 20, 45, and 112.5 kVA demonstrate the effectiveness and robustness of the proposed GNDO approach when compared to other optimizers reported in the literature, such as the crow search algorithm, the coyote optimization algorithm, and the exact solution of the nonlinear optimization model using the fmincon solver of the MATLAB software. All numerical simulations confirm the potential of the GNDO approach to deal with complex optimization problems in engineering and science with promising results and low computational effort.}
}
@article{DELLACORTE2016209,
title = {Referential description of the evolution of a 2D swarm of robots interacting with the closer neighbors: Perspectives of continuum modeling via higher gradient continua},
journal = {International Journal of Non-Linear Mechanics},
volume = {80},
pages = {209-220},
year = {2016},
note = {Dynamics, Stability, and Control of Flexible Structures},
issn = {0020-7462},
doi = {https://doi.org/10.1016/j.ijnonlinmec.2015.06.016},
url = {https://www.sciencedirect.com/science/article/pii/S0020746215001468},
author = {Alessandro {Della Corte} and Antonio Battista and Francesco dell׳Isola},
keywords = {Swarm robot, Second gradient continua, Generalized continua, Deformable bodies},
abstract = {In the present paper a discrete robotic system model whose elements interact via a simple geometric law is presented and some numerical simulations are provided and discussed. The main idea of the work is to show the resemblance between the cases of first and second neighbors interaction with (respectively) first and second gradient continuous deformable bodies. Our numerical results showed indeed that the interaction and the evolution process described is suitable to closely reproduce some basic characteristics of the behavior of bodies whose deformation energy depends on first or on higher gradients of the displacement. Moreover, some specific qualitative characteristics of the continuous deformation are also reproduced. The model introduced here will need further investigation and generalization in both theoretical and numerical directions.}
}
@article{VIEIRA2020106268,
title = {Symmetry exploitation to reduce impedance evaluations in grounding grids},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {123},
pages = {106268},
year = {2020},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2020.106268},
url = {https://www.sciencedirect.com/science/article/pii/S0142061519342188},
author = {Pedro H.N. Vieira and Rodolfo A.R. Moura and Marco Aurélio O. Schroeder and Antonio C.S. Lima},
keywords = {Electromagnetic analysis, Frequency response, Grounding, Method of moments, Numerical methods, Symmetry},
abstract = {One main concern on wideband evaluation of grounding systems is the high computational burden related to the determination of the impedance matrices. Traditionally, one has to divide any given conductor in a large number of segments which leads to a rather time consuming procedure. However, there are a number of geometrical symmetries that if exploited can significantly reduce the overall computational time. This work aims at investigating the adequacy of using some existing symmetries to reduce computer burden in the assessment of a wideband grounding system in models based on the Method of Moments. An algorithmic approach is proposed to extend the symmetry exploitation to arbitrarily oriented uniform rectangular grounding systems. Several topologies are used to assess the performance of the proposed approach. According to results, the proposed methodology can be more than 12 times faster than the traditional approach without loss of accuracy because it is not a numerical approximation.}
}
@article{ARTHURS2013443,
title = {Efficient simulation of cardiac electrical propagation using high-order finite elements II: Adaptive p-version},
journal = {Journal of Computational Physics},
volume = {253},
pages = {443-470},
year = {2013},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2013.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0021999113004841},
author = {Christopher J. Arthurs and Martin J. Bishop and David Kay},
keywords = {Adaptive finite element method, -version, Monodomain simulation, Computational cardiology, Numerical efficiency},
abstract = {We present a computationally efficient method of simulating cardiac electrical propagation using an adaptive high-order finite element method to automatically concentrate computational effort where it is most needed in space on each time-step. We drive the adaptivity using a residual-based error indicator, and demonstrate using norms of the error that the indicator allows us to control it successfully. Our results using two-dimensional domains of varying complexity demonstrate that significant improvements in efficiency are possible over the standard linear FEM in our single-thread studies, and our preliminary three-dimensional results suggest that improvements are also possible in 3D. We do not work in parallel or investigate the challenges for adaptivity such as dynamic load-balancing which are associated with parallelisation. However, based upon recent work demonstrating that in some circumstances and with moderate processor counts parallel h-adaptive methods are efficient, and upon the claim that p-adaptivity will outperform h-adaptivity, we argue that p-adaptivity should be investigated for efficiency in parallel for simulation on moderate numbers of processors.}
}
@incollection{GINSBURGH2006947,
title = {Chapter 27 The Computation of Prices Indices},
editor = {Victor A. Ginsburg and David Throsby},
series = {Handbook of the Economics of Art and Culture},
publisher = {Elsevier},
volume = {1},
pages = {947-979},
year = {2006},
issn = {1574-0676},
doi = {https://doi.org/10.1016/S1574-0676(06)01027-1},
url = {https://www.sciencedirect.com/science/article/pii/S1574067606010271},
author = {Victor Ginsburgh and Jianping Mei and Michael Moses},
keywords = {prices indices, repeat sales, hedonic pricing, auctions},
abstract = {While there are no significant investment characteristics that inhibit art from being considered as an asset, a major hurdle has long been the lack of a systematic measure of its financial performance. Due to its heterogeneity (each piece is different) and its infrequency of trading (the exact same piece does not come to the market very often), the determination of changes in market value is difficult to ascertain. Two estimation methods are commonly used to construct indices. Repeat-sales regression (RSR) uses prices of individual objects traded at two distinct moments in time. If the characteristics of an object do not change (which is usually so for collectibles), the heterogeneity issue is bypassed. The basic idea of the hedonic regression (HR) method is to regress prices on various attributes of objects (dimensions, artist, subject matter, etc.) and to use the residuals of the regression which can be considered as “characteristic-free prices” to compute the price index. The chapter deals with the basics of hedonic and repeat-sales estimators, and tries to interpret in economic terms what both are trying to achieve. It also goes into some more technical details which may be useful for researchers who want to construct such indices, and gives some guidelines on how to go about collecting data, and the choice between RSR and HR that this induces. Both methods are compared using simulated returns, pointing to which method should be used given the data at hand.}
}
@article{HALL1996115,
title = {The role of creativity within best practice manufacturing},
journal = {Technovation},
volume = {16},
number = {3},
pages = {115-121},
year = {1996},
issn = {0166-4972},
doi = {https://doi.org/10.1016/0166-4972(95)00050-X},
url = {https://www.sciencedirect.com/science/article/pii/016649729500050X},
author = {David J. Hall},
abstract = {‘Best practice’ manufacturing is linked directly to aspects of creativity, through an appreciation of the operation of the practitioner's brain. It is suggested that the introduction of techniques such as benchmarking or business process re-engineering cannot succeed in the long term, unless the correct understanding is developed within the management team. Concepts of mind set and lateral thinking are related to the top-down introduction of step change, whilst ‘total quality’ programmes develop the culture necessary for bottom-up continuous improvement. Successful companies will run the two approaches in parallel.}
}
@article{OSPINAAGUDELO2021121224,
title = {Application domain extension of incremental capacity-based battery SoH indicators},
journal = {Energy},
volume = {234},
pages = {121224},
year = {2021},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.121224},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221014729},
author = {Brian {Ospina Agudelo} and Walter Zamboni and Eric Monmasson},
keywords = {Battery, State of health, Battery ageing, Capacity degradation, Incremental capacity, Randomised usage pattern},
abstract = {The Incremental Capacity (IC) analysis is used to characterise the capacity and the battery state of health, aged by cycling patterns with randomly selected pulsed current levels and duration. The batteries are periodically characterised at 1C current, which is a high value with respect to the typical IC tests in pseudo-equilibrium condition. The high-current IC curves generation from raw voltage/current data includes two filtering stages, one for the input voltage and one for the incremental capacity curve smoothing, which are optimised for the application on the basis of the data characteristics. The correlations between the IC main peak features and the battery full capacity for 28 Lithium–Cobalt oxide batteries with 18650 packaging were evaluated, finding that the main peak area is a general feature to evaluate the state of health under high current tests and random usage pattern, and, therefore, it can be used as a battery health indicator in practical applications. The effects of the computational parameters on the relationship between the peak area and the battery capacity are also investigated. The results are confirmed by a further analysis performed over an additional set of cells with different technology, aged with a fixed cycling pattern. Additionally, the performance of the peak area as a health indicator was compared with an ohmic resistance-based estimation approach.}
}
@incollection{MORA2019215,
title = {Chapter 7 - The social shaping of smart cities},
editor = {Luca Mora and Mark Deakin},
booktitle = {Untangling Smart Cities},
publisher = {Elsevier},
pages = {215-234},
year = {2019},
isbn = {978-0-12-815477-9},
doi = {https://doi.org/10.1016/B978-0-12-815477-9.00007-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128154779000074},
author = {Luca Mora and Mark Deakin},
keywords = {Interdisciplinarity, Expectations, Social shaping, Open innovation, Technological advancement, Urban innovations, Technological change, Hype, Smart city, Smart city research, Lessons, Recommendations, Open community, Co-design, Collaboration, Collaborative environment, Quadruple helix, Triple helix, Organization dynamics, Urban innovation, Urban sustainability, Sustainable urban development, Hyped behaviors, Urban utopia, Expectations, Smart city development, Smart urbanism},
abstract = {This last chapter concludes the investigation by summing up the key lessons and recommendations that this book can offer to the community of stakeholders involved in smart city research, policy, and practice. The series of complementary analyses that the previous chapters report on demonstrate that, when untangled from the technocentric urban utopia pictured by the corporate sector, smart cities have the potential to develop into innovation systems that set the stage for a technology-enabled approach to urban sustainability. But realizing this opportunity requires to move beyond traditional boundaries, separate the hype from reality, and strengthen the focus on the social shaping of smart cities. The investigation demonstrates that, in order for such a social shaping to develop, the design of smart cities needs to be understood as a collective action in which two complementary forces are combined. On the one hand, the faith in the technological advancement exposed in the utopian thinking. On the other, the knowledge, skills, and interests of a quadruple-helix collaborative environment where the need for technological innovation in response to urban sustainability goals is not shaped by the corporate sector and its technocentric and market-oriented logic, but an open community whose actions serve the public interest and are based on a holistic interpretation of smart city development.}
}
@article{MULLER2021103546,
title = {Kandinsky Patterns},
journal = {Artificial Intelligence},
volume = {300},
pages = {103546},
year = {2021},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103546},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221000977},
author = {Heimo Müller and Andreas Holzinger},
keywords = {Explainable AI, Explainability, Synthetic test data, Ground truth},
abstract = {Kandinsky Figures and Kandinsky Patterns are mathematically describable, simple, self-contained hence controllable synthetic test data sets for the development, validation and training of visual tasks and explainability in artificial intelligence (AI). Whilst Kandinsky Patterns have these computationally manageable properties, they are at the same time easily distinguishable by human observers. Consequently, controlled patterns can be described by both humans and computers. We define a Kandinsky Pattern as a set of Kandinsky Figures, where for each figure an “infallible authority” defines that the figure belongs to the Kandinsky Pattern. With this simple principle we build training and validation data sets for testing explainability, interpretability and context learning. In this paper we describe the basic idea and some underlying principles of Kandinsky Patterns. We provide a Github repository and invite the international AI research community to a challenge to experiment with our Kandinsky Patterns. The goal is to help expand and advance the field of AI, and in particular to contribute to the increasingly important field of explainable AI.}
}
@article{STEANE2003469,
title = {A quantum computer only needs one universe},
journal = {Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics},
volume = {34},
number = {3},
pages = {469-478},
year = {2003},
note = {Quantum Information and Computation},
issn = {1355-2198},
doi = {https://doi.org/10.1016/S1355-2198(03)00038-8},
url = {https://www.sciencedirect.com/science/article/pii/S1355219803000388},
author = {A.M Steane},
keywords = {Quantum computation, Classical computation, Parallel universes, Entanglement},
abstract = {The nature of quantum computation is discussed. It is argued that, in terms of the amount of information manipulated in a given time, quantum and classical computation are equally efficient. Quantum superposition does not permit quantum computers to “perform many computations simultaneously” except in a highly qualified and to some extent misleading sense. Quantum computation is therefore not well described by interpretations of quantum mechanics which invoke the concept of vast numbers of parallel universes. Rather, entanglement makes available types of computation processes which, while not exponentially larger than classical ones, are unavailable to classical systems. The essence of quantum computation is that it uses entanglement to generate and manipulate a physical representation of the correlations between logical entities, without the need to completely represent the logical entities themselves.}
}
@article{VARGA202491,
title = {Foundations of Programmable Process Structures for the unified modeling and simulation of agricultural and aquacultural systems},
journal = {Information Processing in Agriculture},
volume = {11},
number = {1},
pages = {91-108},
year = {2024},
issn = {2214-3173},
doi = {https://doi.org/10.1016/j.inpa.2022.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S2214317322000737},
author = {Monika Varga and Bela Csukas},
keywords = {Unified process model, Meta-prototype-based architecture, Transition-based structure representation, Locally programmable functionality prototypes, Agricultural systems, Aquacultural systems},
abstract = {This research paper defines the theoretical foundations and computational implementation of a non-conventional modeling and simulation methodology, inspired by the needs of problem solving for biological, agricultural, aquacultural and environmental systems. The challenging practical problem is to develop a framework for automatic generation of causally right and balance-based, unified models that can also be applied for the effective coupling amongst the various (sophisticated field-specific, sensor data processing-based, upper level optimization-driven, etc.) models. The scientific problem addressed in this innovation is to develop Programmable Process Structures (PPS) by combining functional basis of systems theory, structural approach of net theory and computational principles of agent based modeling. PPS offers a novel framework for the automatic generation of easily extensible and connectible, unified models for the underlying complex systems. PPS models can be generated from one state and one transition meta-prototypes and from the transition oriented description of process structure. The models consist of unified state and transition elements. The local program containing prototype elements, derived also from the meta-prototypes, are responsible for the case-specific calculations. The integrity and consistency of PPS architecture are based on the meta-prototypes, prepared to distinguish between the conservation-laws-based measures and the signals. The simulation is based on data flows amongst the state and transition elements, as well as on the unification based data transfer between these elements and their calculating prototypes. This architecture and its AI language-based (Prolog) implementation support the integration of various field- and task-specific models, conveniently. The better understanding is helped by a simple example. The capabilities of the recently consolidated general methodology are discussed on the basis of some preliminary applications, focusing on the recently studied agricultural and aquacultural cases.}
}
@article{PURI2023104439,
title = {Automatic detection of Alzheimer’s disease from EEG signals using low-complexity orthogonal wavelet filter banks},
journal = {Biomedical Signal Processing and Control},
volume = {81},
pages = {104439},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.104439},
url = {https://www.sciencedirect.com/science/article/pii/S174680942200893X},
author = {Digambar V. Puri and Sanjay L. Nalbalwar and Anil B. Nandgaonkar and Jayanand P. Gawande and Abhay Wagh},
keywords = {Alzheimer’s disease, Electroencephalogram, Fractal dimension, Orthogonal filter banks, Support vector machine, Wavelets},
abstract = {Background:
Alzheimer’s disease (AD) is one of the most common neurodegenerative disorder. As the incidence of AD is rapidly increasing worldwide, detecting it at an early stage can prevent memory loss and cognitive dysfunctions in patients. Recently, Electroencephalogram (EEG) signals in AD cases show less synchronization and a slowing effect. The abrupt and transient behavior of EEG signals can be detected from specific frequency bands that are cortical rhythms of interest such as delta (0−4Hz), theta (4−8Hz), alpha (8−12Hz), beta1 (12−16Hz), beta2 (16−32Hz), and gamma (32−48Hz).
Method:
This paper proposes novel low-complexity orthogonal wavelet filter banks with vanishing moments (LCOWFBs-v) to decompose the AD and normal controlled (NC) EEG signals into subbands (SBs). A generalized design technique is suggested to reduce the computational complexity of original irrational wavelet filter banks (FBs). The two features, Higuchi’s fractal dimension (HFD) and Katz’s fractal dimension (KFD), were extracted from EEG SBs. The significance of these extracted features has been inspected using Kruskal–Wallis test.
Results:
The present study analyzed the EEG recordings of 23 subjects (AD-12 and NC-11) with the combination of LCOWFBs, HFD, and KFD. The proposed technique achieved a classification accuracy of 98.5% and 98.6% using the LCOWFBs-4 and LCOWFBs-6, respectively with a cubic-support vector machine classifier and 10-fold cross-validation technique.
Conclusion:
The proposed method with newly designed LCOWFBs is efficient compared with the well-known FBs and existing techniques for detecting AD.}
}
@article{BANDARAGODA2019104424,
title = {Enabling Collaborative Numerical Modeling in Earth Sciences using Knowledge Infrastructure},
journal = {Environmental Modelling & Software},
volume = {120},
pages = {104424},
year = {2019},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2019.03.020},
url = {https://www.sciencedirect.com/science/article/pii/S1364815219301562},
author = {C. Bandaragoda and A. Castronova and E. Istanbulluoglu and R. Strauch and S.S. Nudurupati and J. Phuong and J.M. Adams and N.M. Gasparini and K. Barnhart and E.W.H. Hutton and D.E.J. Hobley and N.J. Lyons and G.E. Tucker and D.G. Tarboton and R. Idaszak and S. Wang},
keywords = {Cyberinfrastructure, Knowledge infrastructure, Reproducible modeling, Landlab, HydroShare, Earth science education},
abstract = {Knowledge infrastructure is an intellectual framework for creating, sharing, and distributing knowledge. In this paper, we use knowledge infrastructure to address common barriers to entry into numerical modeling in Earth sciences as demonstrated in three computational narratives: physical process modeling education, replicating published model results, and reusing published models to extend research. We outline six critical functional requirements: 1) workflows designed for new users; 2) community-supported collaborative web platform; 3) distributed data storage; 4) software environment; 5) personalized cloud-based high-performance computing platform; and 6) a standardized open source modeling framework. Our methods meet these functional requirements by providing three interactive computational narratives for hands-on, problem-based research using Landlab on HydroShare. Landlab is an open-source toolkit for building, coupling, and exploring two-dimensional numerical models. HydroShare is an online collaborative environment for the sharing of data and models. We describe the methods we are using to accelerate knowledge development by providing a suite of modular and interoperable process components that allows students, domain experts, collaborators, researchers, and sponsors to learn by exploring shared data and modeling resources. The system is designed to support uses on the continuum from fully-developed modeling applications to prototyping research software tools. Landlab notebooks are available for interactive computing on HydroShare at https://doi.org/10.4211/hs.fdc3a06e6ad842abacfa5b896df73a76 and for further development on Github at https://zenodo.org/badge/latestdoi/187289993.}
}
@article{TOKAC2023101220,
title = {A programming grammar for robotic fabrication: Incorporating material agency into clay textures},
journal = {Design Studies},
volume = {88},
pages = {101220},
year = {2023},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2023.101220},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X23000613},
author = {Iremnur Tokac and Herman Bruyninckx and Andrew Vande Moere},
keywords = {human–computer interaction, computational model(s), design technology, reflective practice, making grammars},
abstract = {Material agency describes how material affordances and constraints have the inherent capacity to suggest formal transformations. Digital fabrication typically excludes material agency because it requires the final form is digitally modelled before it can be fabricated. To enrich the fabrication design space with material agency, we introduce 1) a programming grammar that relates the sensing of material states with the transformation of fabrication actions via explicit rule notations; 2) a grammatical compiler that translates these rule notations into a responsive robot executable program; 3) a set of critical reflections on how this grammar enhances the fabrication design space with material agency. Consequently, our contributions broaden digital fabrication to produce intricate material forms that cannot be simulated by geometrical definitions.}
}
@article{CRAGG201463,
title = {Skills underlying mathematics: The role of executive function in the development of mathematics proficiency},
journal = {Trends in Neuroscience and Education},
volume = {3},
number = {2},
pages = {63-68},
year = {2014},
issn = {2211-9493},
doi = {https://doi.org/10.1016/j.tine.2013.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S2211949313000422},
author = {Lucy Cragg and Camilla Gilmore},
keywords = {Mathematics, Executive function, Working memory, Development},
abstract = {The successful learning and performance of mathematics relies on a range of individual, social and educational factors. Recent research suggests that executive function skills, which include monitoring and manipulating information in mind (working memory), suppressing distracting information and unwanted responses (inhibition) and flexible thinking (shifting), play a critical role in the development of mathematics proficiency. This paper reviews the literature to assess concurrent relationships between mathematics and executive function skills, the role of executive function skills in the performance of mathematical calculations, and how executive function skills support the acquisition of new mathematics knowledge. In doing so, we highlight key theoretical issues within the field and identify future avenues for research.}
}
@article{LI2023103984,
title = {Improving short-term bike sharing demand forecast through an irregular convolutional neural network},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {147},
pages = {103984},
year = {2023},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2022.103984},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X22003977},
author = {Xinyu Li and Yang Xu and Xiaohu Zhang and Wenzhong Shi and Yang Yue and Qingquan Li},
keywords = {Bike sharing, Deep learning, Travel demand forecast, Spatial–temporal analysis, Irregular convolution},
abstract = {As an important task for the management of bike sharing systems, accurate forecast of travel demand could facilitate dispatch and relocation of bicycles to improve user satisfaction. In recent years, many deep learning algorithms have been introduced to improve bicycle usage forecast. A typical practice is to integrate convolutional (CNN) and recurrent neural network (RNN) to capture spatial–temporal dependency in historical travel demand. For typical CNN, the convolution operation is conducted through a kernel that moves across a “matrix-format” city to extract features over spatially adjacent urban areas. This practice assumes that areas close to each other could provide useful information that improves prediction accuracy. However, bicycle usage in neighboring areas might not always be similar, given spatial variations in built environment characteristics and travel behavior that affect cycling activities. Yet, areas that are far apart can be relatively more similar in temporal usage patterns. To utilize the hidden linkage among these distant urban areas, the study proposes an irregular convolutional Long-Short Term Memory model (IrConv+LSTM) to improve short-term bike sharing demand forecast. The model modifies traditional CNN with irregular convolutional architecture to leverage the hidden linkage among “semantic neighbors”. The proposed model is evaluated with a set of benchmark models in five study sites, which include one dockless bike sharing system in Singapore, and four station-based systems in Chicago, Washington, D.C., New York, and London. We find that IrConv+LSTM outperforms other benchmark models in the five cities. The model also achieves superior performance in areas with varying levels of bicycle usage and during peak periods. The findings suggest that “thinking beyond spatial neighbors” can further improve short-term travel demand prediction of urban bike sharing systems.}
}
@article{ONKAL2013772,
title = {Scenarios as channels of forecast advice},
journal = {Technological Forecasting and Social Change},
volume = {80},
number = {4},
pages = {772-788},
year = {2013},
note = {Scenario Method: Current developments in theory and practice},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2012.08.015},
url = {https://www.sciencedirect.com/science/article/pii/S0040162512002090},
author = {Dilek Önkal and Kadire Zeynep Sayım and Mustafa Sinan Gönül},
keywords = {Forecast, Scenario, Group, Judgment, Advice taking},
abstract = {Today's business environment provides tougher competition than ever before, stressing the important role played by information and forecasts in decision-making. The scenario method has been popular for focused organizational learning, decision making and strategic thinking in business contexts, and yet, its use in communicating forecast information and advice has received little research attention. This is surprising since scenarios may provide valuable tools for communication between forecast providers and users in organizations, offering efficient platforms for information exchange via structured storylines of plausible futures. In this paper, we aim to explore the effectiveness of using scenarios as channels of forecast advice. An experimental study is designed to investigate the effects of providing scenarios as forecast advice on individual and group-based judgmental predictions. Participants are given time series information and model forecasts, along with (i) best-case, (ii) worst-case, (iii) both, or (iv) no scenarios. Different forecasting formats are used (i.e., point forecast, best-case forecast, worst-case forecast, and surprise probability), and both individual predictions and consensus forecasts are requested. Forecasts made with and without scenarios are compared for each of these formats to explore the potential effects of providing scenarios as forecast advice. In addition, group effects are investigated via comparisons of composite versus consensus predictions. The paper concludes with a discussion of results and implications for future research on scenario use in forecasting.}
}
@article{BOTTI2017481,
title = {Integrating ergonomics and lean manufacturing principles in a hybrid assembly line},
journal = {Computers & Industrial Engineering},
volume = {111},
pages = {481-491},
year = {2017},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2017.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0360835217302152},
author = {Lucia Botti and Cristina Mora and Alberto Regattieri},
keywords = {Lean manufacturing, Occupational safety, Ergonomics, Automation, Human factors, Hybrid assembly line},
abstract = {Lean manufacturing is a production method that was established in the wake of the Japanese Toyota Production System and rapidly established in the worldwide manufacturing industry. Lean characteristics combine just-in-time practices, work-in-progress and waste reduction, improvement strategies, defect-free production, and standardization. The primary goal of lean thinking is to improve profits and create value by minimizing waste. This study introduces a novel mathematical model to design lean processes in hybrid assembly lines. The aim was to provide an effective, efficient assembly line design tool that meets the lean principles and ergonomic requirements of safe assembly work. Given the production requirements, product characteristics and assembly tasks, the model defines the assembly process for hybrid assembly lines with both manual workers and automated assembly machines. Each assembly line solution ensures an acceptable risk level of repetitive movements, as required by current law. This model helps managers and practitioners to design hybrid assembly lines with both manual workers and automated assembly machines. The model was tested in a case study of an assembly line for hard shell tool cases. Results show that worker ergonomics is a key parameter of the assembly process design, as other lean manufacturing parameters, e.g. takt time, cycle time and work in progress.}
}
@article{YUCEL2019352,
title = {Battling gender stereotypes: A user study of a code-learning game, “Code Combat,” with middle school children},
journal = {Computers in Human Behavior},
volume = {99},
pages = {352-365},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2019.05.029},
url = {https://www.sciencedirect.com/science/article/pii/S0747563219302109},
author = {Yeliz Yücel and Kerem Rızvanoğlu},
keywords = {User experience (UX), Digital gender divide, Gender stereotypes, Stereotype threat, Games, Serious games},
abstract = {Abstract.
Gender has been consistently controlled as a variable in usability and playability tests. However, there is no consensus on whether and how gender differences should influence the design of digital environments. According to some research, digital environments may be unintentionally designed especially for males as a result of the existing gender biases which risks reproducing gender-polarized culture in a computational field. This study attempts to highlight that females are still being negatively affected by existing gender stereotypes and prescribed gender identities despite relatively equal access and use of computer technology. This qualitative study aims to provide insights about the first-time user experience in a home environment of 16 middle school children in Turkey (8 males - 8 females), aged between 11 and 14 years, with a code learning game named “Code Combat”. The analysis is supported with complementary quantitative findings. The present study investigates the participants' conceptualizations and opinions toward coding concept and this specific coding game. Further, it explores how existing gender stereotypes and gender biased expectations impact their behaviors and attitudes in the context of game experience. Our results indicated that perceived computer competence and perceived coding difficulty had important effects on the participants’ performance relatedly with their gender identity. According to our findings, there are important gender differences to be found in our 9 constructs, namely; perceived computer competence, perceived coding difficulty, identification, perceived game difficulty, perceived success, level of enjoyment, level of anxiety, the likelihood of playing it another time and the likelihood of trying new features.}
}
@article{HUEY2022101211,
title = {Assessing the impact of standards-based grading policy changes on student performance and practice work completion in secondary mathematics},
journal = {Studies in Educational Evaluation},
volume = {75},
pages = {101211},
year = {2022},
issn = {0191-491X},
doi = {https://doi.org/10.1016/j.stueduc.2022.101211},
url = {https://www.sciencedirect.com/science/article/pii/S0191491X22000888},
author = {Maryann E. Huey and Patrick R. Silvey and Amy G. Vaughan and Asa L. Fisher},
keywords = {Grading, Standards-based grading, Mathematics, Secondary, Motivation, High-achieving students},
abstract = {We report upon an intervention study conducted over two academic calendar years involving high-achieving, grade 8 and 9 students (n = 122 and 123 respectively) enrolled in a year-long geometry course. The study assesses the impact of a change in grading policy, namely removing practice work from grade computations, on student performance levels and behaviors. After the change in grading policy was implemented, findings reveal that performance decreased on some, but not all, standards assessed. Completion rates of practice work also decreased overall. Potential causes are discussed as well as implications for implementing aspects of standards-based grading systems in secondary mathematics classrooms.}
}
@incollection{BERNINGER2002273,
title = {Chapter 10 - Building a Computing Brain Pedagogically},
editor = {Virginia W. Berninger and Todd L. Richards},
booktitle = {Brain Literacy for Educators and Psychologists},
publisher = {Academic Press},
address = {San Diego},
pages = {273-294},
year = {2002},
series = {Practical Resources for the Mental Health Professional},
issn = {18730450},
doi = {https://doi.org/10.1016/B978-012092871-2/50011-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780120928712500115},
author = {Virginia W. Berninger and Todd L. Richards},
abstract = {Publisher Summary
This chapter focuses on building a computing brain pedagogically. The brain, as it interacts with the world, constructs more concepts that are represented as mental models in distributed neural networks. Both the quantitative dimension and logical structures contribute to how these mental models are constructed and represented in the brain. The brain draws on both inductive thinking. During the construction process, the brain also uses multiple codes to represent and understand this emerging conceptual domain. The hand that is instrumental in development of the written language system also plays a major role in development of conceptual representations of the world. Working memory also plays an important role in conceptual development in the math domain. Like the writing brain, the computing brain also develops from both play and conscious work. The chapters conclude that development of the computing brain requires guided assistance in translating implicit knowledge based on experience into explicit knowledge that can be used for the hard work of math problem solving, which is conducted in resource-limited, temporally constrained working memory.}
}
@incollection{YANG2001291,
title = {Curved Beam Theories and Related Computational Aspects},
editor = {S. Valliappan and N. Khalili},
booktitle = {Computational Mechanics–New Frontiers for the New Millennium},
publisher = {Elsevier},
address = {Oxford},
pages = {291-298},
year = {2001},
isbn = {978-0-08-043981-5},
doi = {https://doi.org/10.1016/B978-0-08-043981-5.50046-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780080439815500468},
author = {Yeong-Bin Yang},
keywords = {Buckling, curved beam, curved beam element, joint equilibrium, stability, straight beam element},
abstract = {ABSTRACT
The theories of buckling for horizontal curved beams presented by Timoshenko, Vlasov, Yoo, and Yang and Kuo are first reviewed, with their key features identified. The previous argument concerning the incapability of straight beam elements to predict the buckling loads of curved beams in incorrect, due to overlook of the conditions of equilibrium for structural joints connecting non-aligned members in the deformed position, as implied by conventional finite element approaches. If such conditions are duly taken into account, then the straight beam elements derived, which are referred to as the semitangential elements, can be used as a reliable tool for predicting the buckling loads of curved beams. Moreover, by simulating a curved beam in the limit as an infinite number of infinitesimal elements, the theory for straight beams can be manipulated through use of the concept of transfer matrix to yield the ones for curved beams for the cases of uniform bending and uniform compression. It is in this sense that the theories of straight beams and curved beams are unified.}
}
@article{JURKOVA2023105046,
title = {Turing and von Neumann machines: Completing the new mechanism},
journal = {Biosystems},
volume = {234},
pages = {105046},
year = {2023},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2023.105046},
url = {https://www.sciencedirect.com/science/article/pii/S0303264723002216},
author = {Barbora Jurková and Lukáš Zámečník},
keywords = {Turing machine, von Neumann probe, New mechanism, Code biology, Extended mechanism},
abstract = {Turing (1937) introduces a model of code that is followed by other pioneers of computing machines (such as Flowers 1983, Eckert, Mauchly, Brainerd 1945 and others). One of them is John von Neumann, who defines the concept of optimal code in the context of the conception of EDVAC. He later uses it to build on in his theoretical considerations of the universal constructor (von Neumann 1966). Von Neumann (1963) further presents one of the first neural network models, in relation to the work of McCulloch and Pitts (1943), for both theoretical purposes (von Neumann probe) and practical applications (computer architecture of EDVAC). The aim of this paper is (1) to describe the differences between Turingʼs and von Neumannʼs conceptualizations of code and the mechanical computing model. Between von Neumann's abstract technical conception (von Neumann 1963 and 1966) and Turingʼs more concrete biochemical conception (Turing 1952). Furthermore, (2) we want to answer the question why these influential models of mechanisms (predominantly in computer science) have so far been ignored by philosophers of the new mechanism (Machamer, Darden, Craver 2000, Glennan 2017). We will show that these classical models of machines are not only compatible with the new mechanism, but moreover complement it, since they represent a completely separate type of model of mechanism, alongside producing, maintaining and underlying (Zámečník 2021). The final (3) and main goal of our paper will be an attempt to relate von Neumannʼs and Turingʼs notion of mechanism to Barbieriʼs notion of extended mechanism (Barbieri 2015).}
}
@article{MUTH1992278,
title = {Extraneous information and extra steps in arithmetic word problems},
journal = {Contemporary Educational Psychology},
volume = {17},
number = {3},
pages = {278-285},
year = {1992},
issn = {0361-476X},
doi = {https://doi.org/10.1016/0361-476X(92)90066-8},
url = {https://www.sciencedirect.com/science/article/pii/0361476X92900668},
author = {K.Denise Muth},
abstract = {To determine how middle school students cope with some of the demands imposed on them by arithmetic word problems, 140 eighth graders were asked to solve word problems modeled after those used by the National Assessment of Educational Progress. Processing demands were imposed on the students by adding extraneous information and extra steps to the problems. Results indicated that the presence of extraneous information and extra steps reduced the accuracy of students' solutions. Thinking-out-loud protocols also revealed several misconceptions that students have about solving word problems.}
}
@article{KHARE2022105028,
title = {A hybrid decision support system for automatic detection of Schizophrenia using EEG signals},
journal = {Computers in Biology and Medicine},
volume = {141},
pages = {105028},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.105028},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521008222},
author = {Smith K. Khare and Varun Bajaj},
keywords = {Schizophrenia, Electroencephalography, Optimization, Robust variational mode decomposition, Optimized extreme learning machine classifier},
abstract = {Background
Schizophrenia (SCZ) is a serious neurological condition in which people suffer with distorted perception of reality. SCZ may result in a combination of delusions, hallucinations, disordered thinking, and behavior. This causes permanent disability and hampers routine functioning. Trained neurologists use interviewing and visual inspection techniques for the detection and diagnosis of SCZ. These techniques are manual, time-consuming, subjective, and error-prone. Therefore, there is a need to develop an automatic model for SCZ classification. The aim of this study is to develop an automated SCZ classification model using electroencephalogram (EEG) signals. The EEG signals can capture the changes in neural dynamics of human cognition during SCZ.
Method
Based on the nature of the SCZ condition, the EEG signals must be examined. For accurate interpretation of EEG signals during SCZ, an automated model integrating a robust variational mode decomposition (RVMD) and an optimized extreme learning machine (OELM) classifier is developed. Traditional VMD suffers from noisy mode generation, mode duplication, under segmentation, and mode discarding. These problems are suppressed in RVMD by automating the selection of quadratic penalty factor (α) and a number of modes (L). The hyperparameters (HPM) of the OELM classifier are automatically selected to ensure maximum accuracy for each mode without overfitting or underfitting. For the selection of α and L in RVMD and HPM in the OELM classifier, a whale optimization algorithm is used. The root mean square error is minimized for RVMD and classification accuracy of each mode is maximized for the OELM classifier. The EEG signals of three conditions performing basic sensory tasks have been analyzed to detect SCZ.
Results
The Kruskal Wallis test is used to select different features extracted from the modes produced by RVMD. An OELM classifier is tested using a ten-fold cross-validation technique. An accuracy, precision, specificity, F-1 measure, sensitivity, and Cohen's Kappa of 92.93%, 93.94%, 91.06% 94.07%, 97.15%, and 85.32% are obtained.
Conclusion
The third mode's chaotic features helped to capture the significant changes that occurred during the SCZ state. The findings of the RVMD-OELM-based hybrid decision support system can help neuro-experts for the accurate identification of SCZ in real-time scenarios.}
}
@incollection{PREISIG20121242,
title = {HAZOP - an automaton-inspired approach},
editor = {Ian David Lockhart Bogle and Michael Fairweather},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {30},
pages = {1242-1246},
year = {2012},
booktitle = {22nd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-59520-1.50107-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044459520150107X},
author = {Heinz A Preisig and Flavio Manenti},
keywords = {dynamic system, safety, HAZOP},
abstract = {If there exists a gradient pointing outwards a save-operation region, then there exists a possible path for the plant to cross out of the save-operation region. A method is introduced that allows the identification of such surface pieces in the phase space of the state variable for the analysed equipment. The idea is based on splitting the phase space into subspaces separated by the zero-dynamic component surface. The computation requires only root solving of the right-hand side of the dynamic equations, one at the time.}
}
@article{HUANG2024109100,
title = {Knowledge graph based reasoning in medical image analysis: A scoping review},
journal = {Computers in Biology and Medicine},
volume = {182},
pages = {109100},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.109100},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524011855},
author = {Qinghua Huang and Guanghui Li},
keywords = {Medical diagnosis, Medical expert systems, Knowledge graph, Medical image analysis},
abstract = {Automated computer-aided diagnosis (CAD) is becoming more significant in the field of medicine due to advancements in computer hardware performance and the progress of artificial intelligence. The knowledge graph is a structure for visually representing knowledge facts. In the last decade, a large body of work based on knowledge graphs has effectively improved the organization and interpretability of large-scale complex knowledge. Introducing knowledge graph inference into CAD is a research direction with significant potential. In this review, we briefly review the basic principles and application methods of knowledge graphs firstly. Then, we systematically organize and analyze the research and application of knowledge graphs in medical imaging-assisted diagnosis. We also summarize the shortcomings of the current research, such as medical data barriers and deficiencies, low utilization of multimodal information, and weak interpretability. Finally, we propose future research directions with possibilities and potentials to address the shortcomings of current approaches.}
}
@article{ZHU2021118730,
title = {From gratitude to injustice: Neurocomputational mechanisms of gratitude-induced injustice},
journal = {NeuroImage},
volume = {245},
pages = {118730},
year = {2021},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2021.118730},
url = {https://www.sciencedirect.com/science/article/pii/S1053811921010028},
author = {Ruida Zhu and Zhenhua Xu and Song Su and Chunliang Feng and Yi Luo and Honghong Tang and Shen Zhang and Xiaoyan Wu and Xiaoqin Mai and Chao Liu},
keywords = {Gratitude, Protection tendency, Injustice, Mentalizing, Reward processing},
abstract = {Gratitude shapes individuals’ behaviours and impacts the harmony of society. Many previous studies focused on its association with prosocial behaviours. A possibility that gratitude can lead to moral violation has been overlooked until recently. Nevertheless, the neurocognitive mechanisms of gratitude-induced moral violation are still unclear. On the other hand, though neural correlates of the gratitude's formation have been examined, the neural underpinnings of gratitude-induced behaviour remain unknown. For addressing these two overlapped research gaps, we developed novel tasks to investigate how participants who had received voluntary (Gratitude group) or involuntary help (Control group) punished their benefactors’ unfairness with functional magnetic resonance imaging (fMRI). The Gratitude group punished their benefactors less than the Control group. The self-report and computational modelling results demonstrated a crucial role of the boosted protection tendency on behalf of benefactors in the gratitude-induced injustice. The fMRI results showed that activities in the regions associated with mentalizing (temporoparietal junction) and reward processing (ventral medial prefrontal cortex) differed between the groups and were related to the gratitude-induced injustice. They suggest that grateful individuals concern for benefactors’ benefits, value chances to interact with benefactors, and refrain from action that perturbs relationship-building (i.e., exert less punishment on benefactors’ unfairness), which reveal a dark side of gratitude and enrich the gratitude theory (i.e., the find-bind-remind theory). Our findings provide psychological, computational, and neural accounts of the gratitude-induced behaviour and further the understanding of the nature of gratitude.}
}
@article{ROSEGGER1992iii,
title = {Editorial},
journal = {Technovation},
volume = {12},
number = {1},
pages = {iii-iv},
year = {1992},
issn = {0166-4972},
doi = {https://doi.org/10.1016/0166-4972(92)90028-G},
url = {https://www.sciencedirect.com/science/article/pii/016649729290028G},
author = {Gerhard Rosegger},
abstract = {Alfred North Whitehead observed that “It is a profoundly erroneous truism, repeated by all copy-books and by eminent people when they are making speeches, that we should cultivate the habit of thinking what we are doing. The precise opposite is the case. Civilization advances by extending the number of important operations which we can perform without thinking about them.”}
}
@article{ZHENG2018214,
title = {An improved genetic approach for composing optimal collaborative learning groups},
journal = {Knowledge-Based Systems},
volume = {139},
pages = {214-225},
year = {2018},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2017.10.022},
url = {https://www.sciencedirect.com/science/article/pii/S0950705117304914},
author = {Yaqian Zheng and Chunrong Li and Shiyu Liu and Weigang Lu},
keywords = {Collaborative learning, Learner group formation, Genetic algorithm, Optimal solution},
abstract = {Collaborative learning is an effective strategy for promoting learning in both traditional face-to-face and online environments. When applying it, students should be assigned to best collaborative groups at the first step, which is called the learner group formation task. In previous studies, various approaches have been proposed to solve this problem. However, they failed to meet all the problem requirements. To address this problem, a generic group formation method that covers all aspects of the problem is proposed in this study. In this method, all requirements of the learner group formation problem are formulated into an integrated mathematical model and an improved genetic algorithm is proposed to solve the model and obtain optimal learning groups to meet various grouping requirements for different educational contexts. To analyse the performance of the proposed approach from a computational perspective, a series of computational experiments are conducted based on eight simulation datasets with different levels of complexity. The simulation results indicate that the proposed method is effective and stable for solving the learner group formation problem. An empirical study is also carried out to validate the proposed approach from a pedagogical view by comparing it with two traditional group formation strategies. The results show that groups formed through the proposed method produce better outcomes than others in terms of group grades, individual grades and student satisfaction.}
}
@article{UHLMANN2021100389,
title = {Algorithmic adaptation and generalization of physically-constrained games},
journal = {Entertainment Computing},
volume = {36},
pages = {100389},
year = {2021},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2020.100389},
url = {https://www.sciencedirect.com/science/article/pii/S1875952120300975},
author = {Jeffrey Uhlmann},
keywords = {Aesthetic image transformations, Circular-shift operator, Computer games, Engineering education, Game development, Sliding-tile puzzles, Solitaire, TriPeaks},
abstract = {In this paper we introduce a novel strategy for generalizing existing puzzles and games by mathematically expressing the operations of the game; then deriving mathematical generalizations of those expressions; and finally implementing a new variant of the game using those generalized operations. The strategy is illustrated in a case study involving the adapting of a traditional game/puzzle to exploit the computational power of smart devices. The focus here is not so much on the end product as it is on the process and considerations underpinning its development by use of the proposed approach. Ancillary results of the venture include generalizations of the circular-shift operator and examination of its computational complexity.}
}
@article{KRIVY2023100057,
title = {Digital ecosystem: The journey of a metaphor},
journal = {Digital Geography and Society},
volume = {5},
pages = {100057},
year = {2023},
issn = {2666-3783},
doi = {https://doi.org/10.1016/j.diggeo.2023.100057},
url = {https://www.sciencedirect.com/science/article/pii/S2666378323000090},
author = {Maroš Krivý},
keywords = {Nature, Ecosystem, Digital capitalism, Platform capitalism, Metaphor, Imaginaries},
abstract = {The term “digital ecosystem” has become ubiquitous through a seemingly endless stream of scholarship, punditry and hyperbole around digitalization, to the point that the metaphor is becoming dead. Considering “ecosystem” as a traveling concept straddling natural, social and technical systems, this article traces the extension of “digital ecosystem,” along with the adjacent “business ecosystem” and “entrepreneurial ecosystem,” in the fields of computer science, economy, governance and environmental policy. The origins of the concept as a form of circuitry applied to nature are outlined as a background against which to trace its role as a socio-technical metaphor for digital capitalism. Since the 1990s, various formulations of “ecosystem” have offered a naturalistic interpretation to phenomena ranging from economic interactions to digital infrastructure and the urban everyday. I conclude that by representing the internet and the market as complex, self-organizing processes, the metaphor prioritizes the imperative of adapting to—and downplays the possibility of challenging—our erratic digital capitalism. The article contributes by illuminating the ideological work of naturalistic models in the digital political economy. Evidence on using digital ecosystems in environmental policy is still emerging but points to a form of legitimacy exchange that reduces environmental problems to technical issues.}
}
@article{GREENWOOD2021106597,
title = {Exploring a causal model in observational cohort data: The role of parents and peers in shaping substance use trajectories},
journal = {Addictive Behaviors},
volume = {112},
pages = {106597},
year = {2021},
issn = {0306-4603},
doi = {https://doi.org/10.1016/j.addbeh.2020.106597},
url = {https://www.sciencedirect.com/science/article/pii/S0306460320307279},
author = {C.J. Greenwood and G.J. Youssef and P. Letcher and E.A. Spry and K.C. Thomson and L.J. Hagg and D.M. Hutchinson and J.A. Macdonald and J. McIntosh and A. Sanson and J.W. Toumbourou and C.A. Olsson},
keywords = {Causal modeling, substance use, Adolescence, Young adulthood, Trajectory, Parents, Peers},
abstract = {Aims
To explore the process of applying counterfactual thinking in examining causal determinants of substance use trajectories in observational cohort data. Specifically, we examine the extent to which quality of the parent-adolescent relationship and affiliations with deviant peers are causally related to trajectories of alcohol, tobacco, and cannabis use across adolescence and into young adulthood.
Methods
Data were drawn from the Australian Temperament Project, a population-based cohort study that has followed a sample of young Australians from infancy to adulthood since 1983. Parent-adolescent relationship quality and deviant peer affiliations were assessed at age 13–14 years. Latent curve models were fitted for past month alcohol, tobacco, and cannabis use (n = 1590) from age 15–16 to 27–28 years (5 waves). Confounding factors were selected in line with the counterfactual framework.
Results
Following confounder adjustment, higher quality parent-adolescent relationships were associated with lower baseline cannabis use, but not alcohol or tobacco use trajectories. In contrast, affiliations with deviant peers were associated with higher baseline binge drinking, tobacco, and cannabis use, and an earlier peak in the cannabis use trajectory.
Conclusions
Despite careful application of the counterfactual framework, interpretation of associations as causal is not without limitations. Nevertheless, findings suggested causal effects of both parent-adolescent relationships and deviant peer affiliations on the trajectory of substance use. Causal effects were more pervasive (i.e., more substance types) and protracted for deviant peer affiliations. The exploration of causal relationships in observational cohort data is encouraged, when relevant limitations are transparently acknowledged.}
}
@article{LEVINSON2012167,
title = {Tools from evolutionary biology shed new light on the diversification of languages},
journal = {Trends in Cognitive Sciences},
volume = {16},
number = {3},
pages = {167-173},
year = {2012},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2012.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364661312000290},
author = {Stephen C. Levinson and Russell D. Gray},
abstract = {Computational methods have revolutionized evolutionary biology. In this paper we explore the impact these methods are now having on our understanding of the forces that both affect the diversification of human languages and shape human cognition. We show how these methods can illuminate problems ranging from the nature of constraints on linguistic variation to the role that social processes play in determining the rate of linguistic change. Throughout the paper we argue that the cognitive sciences should move away from an idealized model of human cognition, to a more biologically realistic model where variation is central.}
}
@incollection{SEN201629,
title = {3 - History of zero including its representation and role},
editor = {Syamal K. Sen and Ravi P. Agarwal},
booktitle = {Zero},
publisher = {Academic Press},
pages = {29-75},
year = {2016},
isbn = {978-0-08-100774-7},
doi = {https://doi.org/10.1016/B978-0-08-100774-7.00003-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008100774700003X},
author = {Syamal K. Sen and Ravi P. Agarwal},
keywords = {Algorithms for arithmetic operations, alphabetical positional number system, assumption versus axioms, avoidance of subtraction, Brahmagupta’s rule to compute with zero, building block of matter, direction separator, driver of calculus, dwarf and machine epsilon, exponential growth of computing power, Godel’s incompleteness theorem, Gregorian calendar, history of zero, image of the earth, infinite versus finite precisions, infinitive universe, Maya numbers and long count, mean value theorem, Mohanjodaro and Harappa civilization, most pervasive global symbol, object of zero dimension, Quipu, representation of nothingness, Rolle’s theorem, sexagesimal (base 60) positional number system, stone/copper plate inscription, Vedas and Puranas, violation of a law of nature, Zeno’s paradoxes, zero as a number, zero as a vacant position, zero-free system, zero with its eternal spiritual significance},
abstract = {The chronological development of the history of zero over the centuries is a tough job due to both poor man to man communication and also poor publication machinery. However, the time period 7000 BC–2015 AD is broadly divided into four parts based on the landmark innovations in each part. During 7000–2000 BC, the most important contribution, that is, the modern decimal based place value system with 0 as a number due to Aryabhatta was developed and used. The Maya numbers and Long Count days that were tallied in a modified radix-20 number system are notable. Zero with representation and arithmetic operations was fully developed during 2000 BC–1000 AD. Brahmagupta’s rules for arithmetic operations were developed. The Romans and the Greeks had no zero then and their system was order-valued. Egyptian numerals were base-10 while Babylonian mathematics had a base-60 positional number system. With better understanding of zero, calculus was born. Arab and Persian mathematicians were active and became an important interface between the east and the west in promoting number systems with arithmetic. The period 1000–1900 AD saw the introduction of the Hindu–Arabic numeral system in Europe. The link between the system and European mathematics is the Italian mathematician Fibonacci. During the late eleventh century AD, Shen Gua introduced infinitesimal and exhaustion. He described piling up very small things. During 1900–2015 AD, increasingly high-speed modern digital computing made its presence felt very intensely globally by one and all. Specifically due to its finite precision, unlike the infinite precision which the regular and natural mathematics have, the advent of numerical zero, as opposed to the exact zero, changed the face of all real-world computation. Understanding natural, regular, and computational mathematics with calculus, and specifically the role of zero, became extraordinarily important in all engineering computations. The computational error (implying quality of solution) and computational complexity (cost of computation) due to the presence of numerical zero became integral parts of any algorithm to justify the acceptability of a solution. During the early twentieth century, Ramanujan, to whom each number is a living being and his personal friend carrying an important distinct message, felt intensely the eternal spiritual significance of zero and its inverse infinity. He built a theory of reality around zero and infinity.}
}
@article{BOCK2020100960,
title = {On the semantics for spreadsheets with sheet-defined functions},
journal = {Journal of Computer Languages},
volume = {57},
pages = {100960},
year = {2020},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2020.100960},
url = {https://www.sciencedirect.com/science/article/pii/S2590118420300204},
author = {Alexander Asp Bock and Thomas Bøgholm and Peter Sestoft and Bent Thomsen and Lone Leth Thomsen},
keywords = {Spreadsheet, Semantics, Funcalc, Sheet-defined function, Recalculation},
abstract = {We give an operational semantics for the evaluation of spreadsheets, including sheet-defined and built-in numeric functions in the Funcalc spreadsheet platform. The semantics allows for different implementations and we discuss sheet-defined functions implemented using both interpretation and run-time code generation. The semantics specifies the expected result of a computation, also considering non-deterministic functions, independently of an evaluation mechanism. It can be extended to include the cost of formula evaluation for a cost analysis e.g. for use in parallelization of computations. An interesting future direction is to investigate experimentally how close our semantics is to that of major spreadsheet implementations.}
}
@incollection{GOLDSCHMIDT201146,
title = {Architecture},
editor = {Mark A. Runco and Steven R. Pritzker},
booktitle = {Encyclopedia of Creativity (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {46-51},
year = {2011},
isbn = {978-0-12-375038-9},
doi = {https://doi.org/10.1016/B978-0-12-375038-9.00010-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780123750389000108},
author = {G. Goldschmidt},
keywords = {Architectural design, Architectural education, Culture, Digital design, Form, Function, Ideas, Leading idea, Starchitect, Style},
abstract = {Architecture is a cultural arena based on ideas, which communally produce styles and individually, at their best, generate outstanding buildings. Every building tackles form and function. In our era architecture is expected to innovate in its forms, while ensuring perfect functionality. Form and function handling are rough correlates of originality and practicality, by which we measure design creativity. Architecture is also a product of the technological state of its time. At present we experience computational advances that promise to fundamentally change buildings and the manner in which they are designed. Architectural education is groping to adjust to the changes.}
}
@article{LAVALLE2024100889,
title = {Study of gender perspective in STEM degrees and its relationship with video games},
journal = {Entertainment Computing},
pages = {100889},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100889},
url = {https://www.sciencedirect.com/science/article/pii/S187595212400257X},
author = {Ana Lavalle and Miguel A. Teruel and Alejandro Maté and Juan Trujillo},
keywords = {Gender studies, Games, Cultural and social implications},
abstract = {At present, even though gender equality is an important matter of public interest, there are still areas in higher education where male presence is overwhelming. We refer to STEM (Science, Technology, Engineering, and Mathematics) studies in general and Computer Engineering in particular where there is only 16% of female presence in Spain. This fact made us think about the reason for this inequality. We attempted to answer such questions by means of a survey filled out by 138 students of STEM university degrees. Thanks to this survey, we have been able to understand the motivations and opinions of the students of STEM degrees regarding gender perspective. Our study highlights the possible influence of computer and video game use on enrollment in STEM degrees. Furthermore, it points out the differences between men and women in computer science skills before they start their studies. The answers provided by the surveyed women showed a correlation between women who play video games and those who get better grades. In addition, women who play video games feel more integrated into STEM degrees. Finally, differences were found in gender perspective between the male and female participants.}
}
@article{GURDURBROO2021100192,
title = {Cyber-physical systems research and education in 2030: Scenarios and strategies},
journal = {Journal of Industrial Information Integration},
volume = {21},
pages = {100192},
year = {2021},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2020.100192},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X20300674},
author = {Didem {Gürdür Broo} and Ulf Boman and Martin Törngren},
keywords = {Cyber-physical systems, Research and education, 2030, Future studies, Scenario planning TAIDA framework, Future of engineering research, Future of research},
abstract = {Future cyber-physical systems (CPS), such as smart cities, collaborative robots, autonomous vehicles or intelligent transport systems, are expected to be highly intelligent, electrified, and connected. This study explores a focal question about how these new characteristics may affect the education and research related to CPS in 2030, the date identified by the United Nations to achieve the Agenda for Sustainable Development. To this end, first, we have conducted a trend spotting activity, seeking to identify possible influencing factors that may have a great impact on the future of CPS education and research. These factors were clustered in a total of 12 trends – four certainties; namely connectivity, electrification, data and automation – and eight uncertainties; namely intelligence, data ethics, labour market, lifelong learning, higher education, trust in technology, technological development speed, and sustainable development goals. After that, two of the eight uncertainties are identified and used to construct a scenario matrix, which includes four scenarios. These two uncertainties – the so-called strategic uncertainties – are: fulfilment of sustainable development goals and the nature of the technological development, respectively. These two important uncertainties are considered to build the scenarios due to their potential impact on the research and education of CPS. For instance, sustainable development goals are significant targets for many initiatives, organisations and countries. While 2030 is the deadline to achieve these goals, the relationship between the sustainable development goals related to CPS research and education is not studied well. Similarly, the speed of technological development is seen as a driving force behind future CPS. However, the effect of this speed to CPS research and education environment is not known. Different outcomes of the chosen two uncertainties are, then, combined with the remaining trends and uncertainties. Consequently, four scenarios are derived. The Terminator scenario illustrates a dystopian future where profit is the driving force behind technological progress and sustainable development goals are not accomplished. In contrast, The Iron Giant scenario represents the successful implementation of the sustainable development goals where technological development is the force behind the accomplishment of these goals. The scenario called Slow Progress represents a future where gradual technological improvements are present, but sustainability is still not seen as concerning the issue. The Humanist scenario illustrates a future where slow technological development is happening yet sustainable development goals are successfully implemented. Finally, the scenarios are used to initiate discussions by illustrating what the future of research and education could look like and a list of strategies for future CPS research and education environments is proposed. To this end, we invite educators, researchers, institutions and governments to develop the necessary strategies to enable data-orientated, continuous, interdisciplinary, collaborative, ethical, and sustainable research and education by improving digital fluency, advancing digital equality, contributing to new ways of teaching complex thinking, expanding access to learning platforms and preparing next generations to adapt for a rapidly changing future of work conditions.}
}
@article{CHAKRABORTY2013180,
title = {Secret image sharing using grayscale payload decomposition and irreversible image steganography},
journal = {Journal of Information Security and Applications},
volume = {18},
number = {4},
pages = {180-192},
year = {2013},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.istr.2013.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S1363412713000162},
author = {Soumendu Chakraborty and Anand Singh Jalal and Charul Bhatnagar},
keywords = {DSF matrix, Error matrix, Sign matrix, Bit plane},
abstract = {To provide an added security level most of the existing reversible as well as irreversible image steganography schemes emphasize on encrypting the secret image (payload) before embedding it to the cover image. The complexity of encryption for a large payload where the embedding algorithm itself is complex may adversely affect the steganographic system. Schemes that can induce same level of distortion, as any standard encryption technique with lower computational complexity, can improve the performance of stego systems. In this paper, we propose a secure secret image sharing scheme, which bears minimal computational complexity. The proposed scheme, as a replacement for encryption, diversifies the payload into different matrices which are embedded into carrier image (cover image) using bit X-OR operation. A payload is a grayscale image which is divided into frequency matrix, error matrix, and sign matrix. The frequency matrix is scaled down using a mapping algorithm to produce Down Scaled Frequency (DSF) matrix. The DSF matrix, error matrix, and sign matrix are then embedded in different cover images using bit X-OR operation between the bit planes of the matrices and respective cover images. Analysis of the proposed scheme shows that it effectively camouflages the payload with minimum computation time.}
}
@article{LIETO20161,
title = {From human to artificial cognition and back: New perspectives on cognitively inspired AI systems},
journal = {Cognitive Systems Research},
volume = {39},
pages = {1-3},
year = {2016},
note = {From human to artificial cognition (and back): new perspectives of cognitively inspired AI systems},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2016.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S1389041716300183},
author = {Antonio Lieto and Daniele P. Radicioni},
keywords = {Cognitive systems, Artificial intelligence, Computational models of cognition, Epistemology of the artificial},
abstract = {We overview the main historical and technological elements characterising the rise, the fall and the recent renaissance of the cognitive approaches to Artificial Intelligence and provide some insights and suggestions about the future directions and challenges that, in our opinion, this discipline needs to face in the next years.}
}
@article{LEE201618,
title = {Affective Computing as Complex Systems Science},
journal = {Procedia Computer Science},
volume = {95},
pages = {18-23},
year = {2016},
note = {Complex Adaptive Systems Los Angeles, CA November 2-4, 2016},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.09.288},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916324607},
author = {William Lee and Michael D. Norman},
keywords = {Affective Computing, Computational Models, Complexity, Emotion, Apprasial},
abstract = {Pioneered in the early ‘90s by Rosalind Picard, a professor and IEEE Fellow of the MIT Media Lab, Affective Computing – rooted originally in artificial intelligence – now branches into wearable computing, big data, psychology, neuroscience, and modeling in order to advance the knowledge, understanding, and development of systems for sensing, recognizing, categorizing, and reacting to human emotion. Yet, the challenges of sensing multiple modalities simultaneously, disambiguating complex emotional states non-linearly, and modeling multiple individuals’ emotional states dynamically have continued to ring true, despite dramatic advances in affective computing. This paper seeks to serve two objectives. The first objective is to discuss how these three challenges are related to the three characteristics of complex systems – namely multiple components, non-linearity, and emergent behaviors. The second objective is to identify opportunities from the complex systems domain to address these challenges in novel and comprehensive ways. Recent advances in the utilization of Dynamical Systems Theory (an applied complexity science methodology) have shown that complex human interaction can be rigorously studied and modeled. Coupling the technological advances that cloud-based affective computing have brought with the emerging complex systems science-perspective may well catalyze a new era of human-machine and human-human collaboration.}
}
@article{BASOV2020101433,
title = {Socio-semantic and other dualities},
journal = {Poetics},
volume = {78},
pages = {101433},
year = {2020},
note = {Discourse, Meaning, and Networks: Advances in Socio-Semantic Analysis},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2020.101433},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X19304073},
author = {Nikita Basov and Ronald Breiger and Iina Hellsten},
keywords = {Social network, Semantic network, Socio-semantic network, Duality, Culture, Special Issue},
abstract = {The social and the cultural orders are dual – that is, they constitute each other. To understand either we need to account for both. Socio-semantic network analysis brings together the study of relations among actors (social networks), relations among elements of actors’ cultural structures (their semantic networks), and relations among these two orders of networks. In this introductory essay, we describe how the duality of the social and semantic networks that constitute each other, as well as other related dualities (including material / symbolic, micro / macro, computational / qualitative, in-presence contexts / online contexts, ‘Big’ data / ‘thick’ data), have evolved in recent decades to mold socio-semantic network analysis into its present form. In doing so, we delineate the current state of the art and the main features of socio-semantic network analysis as highlighted by the papers included in this Special Issue. These articles range from in-depth analysis of ‘thick’ data on small group interactions to automated analysis of ‘Big’ online data in contexts extending from Renaissance parliamentary discussions to cutting-edge global scientific fields of the 21st century. We conclude by delineating current problems of and future prospects for socio-semantic network analysis.}
}
@incollection{KALNOOR2021575,
title = {Chapter 24 - The brain-machine interface, nanosensor technology, and artificial intelligence: Their convergence with a novel frontier},
editor = {Chaudhery Mustansar Hussain and Suresh Kumar Kailasa},
booktitle = {Handbook of Nanomaterials for Sensing Applications},
publisher = {Elsevier},
pages = {575-587},
year = {2021},
series = {Micro and Nano Technologies},
isbn = {978-0-12-820783-3},
doi = {https://doi.org/10.1016/B978-0-12-820783-3.00013-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128207833000130},
author = {Gauri Kalnoor},
keywords = {Neuroscience, Machine learning, Nanotechnology, Artificial intelligence (AI), Brain-computer interface, Brain-machine interface (BMI), Computational neuroscience},
abstract = {A confluence of technological capabilities is creating an opportunity for machine learning and artificial intelligence (AI) to enable “smart” nanoengineered brain-machine interfaces (BMI). This new generation of technologies will be able to communicate with the brain in ways that support contextual learning and adaptation to change functional requirements. This applies to both invasive technologies aimed at restoring neurological function, as in the case of neural prosthesis, as well as noninvasive technologies enabled by signals such as electroencephalograph (EEG). Advances in computation, hardware, and algorithms that learn and adapt in a contextually dependent way will be able to leverage the capabilities that nanoengineering offers the design and functionality of BMI. We explore the enabling capabilities that these devices may exhibit, why they matter, and the state of the technologies necessary to build them. We also discuss a number of open technical challenges and problems that will need to be solved to achieve this.}
}
@incollection{CLAUSER20231,
title = {Past, present, and future of educational measurement},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {1-14},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.10001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305100016},
author = {Brian E. Clauser and Melissa J. Margolis},
keywords = {Karl Pearson, Francis Galton, Classical test theory, Item response theory, Generalizability theory, Frederic Lord, Lee Cronbach, Coefficient alpha, Validity theory, Charles Spearman, Eugenics movement, Spearman-Brown formula, Alfred Binet, Army Alpha test, Georg Rasch, Intelligence testing},
abstract = {This article provides an overview of the past, present, and future of educational measurement. We begin by examining the historical events in the 1800s that led to the development of a coherent mathematical theory of test scores in the first half of the 20th century. In this section we describe the contributions of Francis Galton, Karl Pearson, Charles Spearman, Truman Kelley, and Lee Cronbach. In addition to outlining the theoretical contributions of these researchers, we describe the rise of large-scale testing beginning with the Army Alpha test in 1917 and the administration of IQ tests to millions of school children in the decade that followed. We continue by discussing the current state of educational measurement theory and practice including the development and widespread use of item response theory, generalizability theory, validity theory, and large-scale national and international achievement testing to evaluate educational systems. Finally, we consider directions and developments that are likely to define the future of the field. These directions include increased use of computational power in assessment, the use of new sources of data (referred to as process data), automated systems to create test materials, and an increased emphasis on fairness.}
}
@incollection{CORICELLI2025432,
title = {Omission of rewards and regret representations in the brain},
editor = {Jordan Henry Grafman},
booktitle = {Encyclopedia of the Human Brain (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {432-439},
year = {2025},
isbn = {978-0-12-820481-8},
doi = {https://doi.org/10.1016/B978-0-12-820480-1.00104-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204801001042},
author = {Giorgio Coricelli},
keywords = {Regret, Counterfactual thinking, Decision-making, Learning},
abstract = {Regret is a cognitive based emotion that results from an upward counterfactual thought: “if I had done something different, I would have been better off.” How does the brain generate the emotion of regret? And how does the brain anticipate and avoid such a negative emotion? Neuropsychological and neuroimaging studies showed the fundamental role of the orbitofrontal cortex in the feeling and anticipation of regret. The interplay of cognitive and emotional signals in the orbitofrontal cortex underlies an adaptive mechanism based on regret avoidance.}
}
@incollection{ZHANG2022363,
title = {Chapter 18 - KPF: A retrospective view on urban planning AI for 2020},
editor = {Imdat As and Prithwish Basu and Pratap Talwar},
booktitle = {Artificial Intelligence in Urban Planning and Design},
publisher = {Elsevier},
pages = {363-380},
year = {2022},
isbn = {978-0-12-823941-4},
doi = {https://doi.org/10.1016/B978-0-12-823941-4.00004-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128239414000044},
author = {Snoweria Zhang and Kate Ringo and Richard Chou and Brandon Pachuca and Eric Pietraszkiewicz and Luc Wilson},
keywords = {Computational design, Digital twin, Urban design, Future history, City planning},
abstract = {Architectural historians have been fascinated by the year 1000, as the expectation of an impending apocalypse drove the sharp contrast between a dearth of construction before and a booming market after. One thousand years later, residents of 2020 found themselves at the crossroads again with the effects of climate change looming as a global threat. We constructed this chapter as a piece of a future, speculative, and historical document that examines the use of AI in urban planning and design in 2020. As historians from 2120, we study the evolution of tools at this critical junction with the backdrop of a confluence of crises. From explorative visual interfaces, open data initiatives, and computational design to AI that augments and collaborates with humans in the design and development of the city, we present case studies of both the technology and the projects that demonstrate some of the first applications of AI in negotiating the threat of climate change. Through these first examples, we trace the development of tools and corresponding trends in urban AI to the present year of 2120. The speculative narrative frame allows for an explication of the current urban design workflow using AI alongside an opportunity to conjecture where we believe AI development in design and planning ought to be. City makers in 2020 were not involved in the development of AI technologies. This work can act to inspire technologists who are envisioning the future of the city.}
}
@article{PIANTADOSI2012199,
title = {Bootstrapping in a language of thought: A formal model of numerical concept learning},
journal = {Cognition},
volume = {123},
number = {2},
pages = {199-217},
year = {2012},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2011.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0010027711002769},
author = {Steven T. Piantadosi and Joshua B. Tenenbaum and Noah D. Goodman},
keywords = {Number word learning, Bootstrapping, Cognitive development, Bayesian model, Language of thought, CP transition},
abstract = {In acquiring number words, children exhibit a qualitative leap in which they transition from understanding a few number words, to possessing a rich system of interrelated numerical concepts. We present a computational framework for understanding this inductive leap as the consequence of statistical inference over a sufficiently powerful representational system. We provide an implemented model that is powerful enough to learn number word meanings and other related conceptual systems from naturalistic data. The model shows that bootstrapping can be made computationally and philosophically well-founded as a theory of number learning. Our approach demonstrates how learners may combine core cognitive operations to build sophisticated representations during the course of development, and how this process explains observed developmental patterns in number word learning.}
}
@article{WAHYUNINGSIH2024349,
title = {Comparison of Effectiveness of Logistic Regression, Naive Bayes, and Random Forest Algorithms in Predicting Student Arguments},
journal = {Procedia Computer Science},
volume = {234},
pages = {349-356},
year = {2024},
note = {Seventh Information Systems International Conference (ISICO 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924003715},
author = {Tri Wahyuningsih and Danny Manongga and Irwan Sembiring and Sutarto Wijono},
keywords = {Comparison Algorithm, Logistic Regression, Naive Bayes, Random Forest, Student Arguments},
abstract = {Currently, in the process of assessing and giving feedback on students' argumentative writing, educators have to spend a considerable amount of time reading and analyzing each essay individually. This can be a complicated and time-consuming process, especially if the number of students to be assessed is quite large. The problem of this research is to find the most effective algorithm in providing accurate and reliable predictions in the context of evaluation and feedback of students' argumentation. This study compares three algorithms (logistic regression, Naive Bayes, and Random Forest) to predict student argumentation using essays from grades 6-12. Logistic regression performed best with 94.34% accuracy, followed by random forest with 91.98% accuracy, and Naive Bayes with 88.93% accuracy. The study optimized preprocessing and selected algorithms for an automated guidance model. It is the first stage of a three-part study for developing automated guidance models. Data came from Kaggle, and the study aims to improve the accuracy of automated guidance models for student argumentation.}
}
@article{DELATORRE2014653,
title = {Monte Carlo advances and concentrated solar applications},
journal = {Solar Energy},
volume = {103},
pages = {653-681},
year = {2014},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2013.02.035},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X13001448},
author = {J. Delatorre and G. Baud and J.J. Bézian and S. Blanco and C. Caliot and J.F. Cornet and C. Coustet and J. Dauchet and M. {El Hafi} and V. Eymet and R. Fournier and J. Gautrais and O. Gourmel and D. Joseph and N. Meilhac and A. Pajot and M. Paulin and P. Perez and B. Piaud and M. Roger and J. Rolland and F. Veynandt and S. Weitz},
keywords = {Monte Carlo algorithm, Concentrated solar energy, Solar energy flux density distribution, Solar concentrators design optimization, Sensitivity computation},
abstract = {The Monte Carlo method is partially reviewed with the objective of illustrating how some of the most recent methodological advances can benefit to concentrated solar research. This review puts forward the practical consequences of writing down and handling the integral formulation associated to each Monte Carlo algorithm. Starting with simple examples and up to the most complex multiple reflection, multiple scattering configurations, we try to argue that these formulations are very much accessible to the non specialist and that they allow a straightforward entry to sensitivity computations (for assistance in design optimization processes) and to convergence enhancement techniques involving subtle concepts such as control variate and zero variance. All illustration examples makePROMES - UPR CNRS 8521 - 7, rue du Four Solaire, 66120 Font Romeu Odeillo, France use of the public domain development environment EDStar (including advanced parallelized computer graphics libraries) and are meant to serve as start basis either for the upgrading of existing Monte Carlo codes, or for fast implementation of ad hoc codes when specific needs cannot be answered with standard concentrated solar codes (in particular as far as the new generation of solar receivers is concerned).}
}
@article{SHAWKY2023100547,
title = {Adaptive chaotic map-based key extraction for efficient cross-layer authentication in VANETs},
journal = {Vehicular Communications},
volume = {39},
pages = {100547},
year = {2023},
issn = {2214-2096},
doi = {https://doi.org/10.1016/j.vehcom.2022.100547},
url = {https://www.sciencedirect.com/science/article/pii/S2214209622000948},
author = {Mahmoud A. Shawky and Muhammad Usman and Muhammad Ali Imran and Qammer H. Abbasi and Shuja Ansari and Ahmad Taha},
keywords = {Chebyshev chaotic mapping, Cross-layer authentication, Doppler emulation, Physical-layer signatures, Secret key extraction, Vehicular ad-hoc networks},
abstract = {Vehicle-to-everything (V2X) communication is expected to offer users available and ultra-reliable transmission, particularly for critical applications related to safety and autonomy. In this context, establishing a secure and resilient authentication process with low latency and high functionality may not be achieved using conventional cryptographic methodologies due to their significant computation costs. Recent research has focused on employing the physical (PHY) characteristics of wireless channels to develop efficient discrimination techniques to overcome the shortcomings of crypto-based authentication. This paper presents a cross-layer authentication scheme for multicarrier communication, leveraging the spatially/temporally correlated wireless channel features to facilitate key verification without exposing its secrecy. By mapping the time-stamped hashed key and masking it with channel phase responses, we create a PHY-layer signature, allowing for verifying the sender's identity while employing the correlated channel responses between subcarriers to verify messages' integrity. Furthermore, we developed a Diffie-Hellman secret key extraction algorithm that employs the computationally intractable problems of the Chebyshev chaotic mapping for channel probing. Thus, terminals can extract high entropy shared keys that can be used to create dynamic PHY-layer signatures, supporting forward and backward secrecy. We evaluated the scheme's security strength against active/passive attacks. Besides theoretical analysis, we designed a 3-Dimensional (3D) scattering Doppler emulator to investigate the scheme's performance at different speeds of a moving vehicle and signal-to-noise ratios (SNRs) for a realistic vehicular channel. Theoretical and hardware implementation analyses proved the capability of the proposed scheme to support high detection probability at SNR ≥ 0 dB and speed ≤ 45 m/s.}
}
@article{JIANG2024100795,
title = {Generative urban design: A systematic review on problem formulation, design generation, and decision-making},
journal = {Progress in Planning},
volume = {180},
pages = {100795},
year = {2024},
note = {Generative urban design: A systematic review on problem formulation, design generation, and decision-making},
issn = {0305-9006},
doi = {https://doi.org/10.1016/j.progress.2023.100795},
url = {https://www.sciencedirect.com/science/article/pii/S0305900623000569},
author = {Feifeng Jiang and Jun Ma and Christopher John Webster and Alain J.F. Chiaradia and Yulun Zhou and Zhan Zhao and Xiaohu Zhang},
keywords = {Generative urban design, Urban form generation, Generative method, AI-generated content (AIGC), Generative AI, Human-machine collaboration},
abstract = {Urban design is the process of designing and shaping the physical forms of cities, towns, and suburbs. It involves the arrangement and design of street systems, groups of buildings, public spaces, and landscapes, to make the urban environment performative and sustainable. The typical design process, reliant on manual work and expert experience has unavoidable low efficiency in generating high-performing design solutions due to the involvement of complex social, institutional, and economic contexts and the trade-off between conflicting preferences of different stakeholder groups. Taking advantage of artificial intelligence (AI) and computational capacity, generative urban design (GUD) has been developed as a trending technical direction to narrow the gaps and produce design solutions with high efficiency at early design stages. It uses computer-aided generative methods, such as evolutionary optimization and deep generative models, to efficiently explore complex solution spaces and automatically generate design options that satisfy conflicting objectives and various constraints. GUD experiments have attracted much attention from academia, practitioners, and public authorities in recent years. However, a systematic review of the current stage of GUD research is lacking. This study, therefore, reports on a systematic investigation of the existing literature according to the three key stages in the GUD process: (1) design problem formulation, (2) design option generation, and (3) decision-making. For each stage, current trends, findings, and limitations from GUD studies are examined. Future directions and potential challenges are discussed and presented. The review is highly interdisciplinary and involves articles from urban study, computer science, social science, management, and other fields. It reports what scholars have found in GUD experiments and organizes a diverse and complicated technical agenda into something accessible to all stakeholders. The results and discoveries will serve as a holistic reference for GUD developers and users in both academia and industry and form a baseline for the field of GUD development in the coming years.}
}
@article{REMINGTON2018938,
title = {A Dynamical Systems Perspective on Flexible Motor Timing},
journal = {Trends in Cognitive Sciences},
volume = {22},
number = {10},
pages = {938-952},
year = {2018},
note = {Special Issue: Time in the Brain},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2018.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S1364661318301724},
author = {Evan D. Remington and Seth W. Egger and Devika Narain and Jing Wang and Mehrdad Jazayeri},
keywords = {dynamical systems, flexible timing, sensorimotor control, learning, movement sequences, movement planning},
abstract = {A hallmark of higher brain function is the ability to rapidly and flexibly adjust behavioral responses based on internal and external cues. Here, we examine the computational principles that allow decisions and actions to unfold flexibly in time. We adopt a dynamical systems perspective and outline how temporal flexibility in such a system can be achieved through manipulations of inputs and initial conditions. We then review evidence from experiments in nonhuman primates that support this interpretation. Finally, we explore the broader utility and limitations of the dynamical systems perspective as a general framework for addressing open questions related to the temporal control of movements, as well as in the domains of learning and sequence generation.}
}
@article{AUBIN2014204,
title = {“Principles of Mechanics that are Susceptible of Application to Society”: An unpublished notebook of Adolphe Quetelet at the root of his social physics},
journal = {Historia Mathematica},
volume = {41},
number = {2},
pages = {204-223},
year = {2014},
issn = {0315-0860},
doi = {https://doi.org/10.1016/j.hm.2014.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0315086014000020},
author = {David Aubin},
keywords = {Mechanics, Sociology, Adolphe Quetelet, Astronomy, Social physics, Average man, Applications of mathematics, Analogical thinking},
abstract = {Founder of the Brussels Observatory, Adolphe Quetelet (1796–1874) is especially well known for his theory of the average man. Like the average position of a star obtained through a large quantity of observed data, the average man was, according to Quetelet, subject to fixed causal laws. Published in 1835, his book On Man: Essay of Social Physics is one of the founding works of sociology and mathematical statistics. The sources of the analogy between astronomy and social physics have been debated by historians. To shed light on this question and the conditions of application of mathematics in the 19th century, we publish for the first time a manuscript that is kept in Quetelet's papers at the Royal Academy of Belgium, and give an English translation of it.}
}
@article{SHUKLA2024128716,
title = {Deep Belief Network with Fuzzy Parameters and Its Membership Function Sensitivity Analysis},
journal = {Neurocomputing},
pages = {128716},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128716},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224014875},
author = {Amit K. Shukla and Pranab K. Muhuri},
keywords = {Deep learning, Deep belief networks, restricted Boltzmann machine, Fuzzy sets, Type-1 fuzzy sets, Contrastive divergence},
abstract = {Over the last few years, deep belief networks (DBNs) have been extensively utilized for efficient and reliable performance in several complex systems. One critical factor contributing to the enhanced learning of the DBN layers is the handling of network parameters, such as weights and biases. The efficient training of these parameters significantly influences the overall enhanced performance of the DBN. However, the initialization of these parameters is often random, and the data samples are normally corrupted by unwanted noise. This causes the uncertainty to arise among weights and biases of the DBNs, which ultimately hinders the performance of the network. To address this challenge, we propose a novel DBN model with weights and biases represented using fuzzy sets. The approach systematically handles inherent uncertainties in parameters resulting in a more robust and reliable training process. We show the working of the proposed algorithm considering four widely used benchmark datasets such as: MNSIT, n-MNIST (MNIST with additive white Gaussian noise (AWGN) and MNIST with motion blur) and CIFAR-10. The experimental results show superiority of the proposed approach as compared to classical DBN in terms of robustness and enhanced performance. Moreover, it has the capability to produce equivalent results with a smaller number of nodes in the hidden layer; thus, reducing the computational complexity of the network architecture. Additionally, we also study the sensitivity analysis for stability and consistency by considering different membership functions to model the uncertain weights and biases. Further, we establish the statistical significance of the obtained results by conducting both one-way and Kruskal-Wallis analyses of variance tests.}
}