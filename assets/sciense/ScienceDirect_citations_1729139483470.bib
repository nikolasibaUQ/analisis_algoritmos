@article{LU2024112309,
title = {The physical information LSTM surrogate model for establishing a digital twin model of reciprocating air compressors},
journal = {Applied Soft Computing},
volume = {167},
pages = {112309},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.112309},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624010834},
author = {Yingkang Lu and Yanfei Li and Gaocai Fu and Yu Jiang and Yuzhe Huang and Jiaxing Zhu and Buyun Sheng},
keywords = {Reciprocating air compressors, Digital twin, Surrogate model, Pressure prediction, Long short-term memory neural network},
abstract = {Reciprocating air compressors play a crucial role in industrial production processes. However, due to the complex structure and long operating time of reciprocating air compressors, real-time monitoring to grasp the operating status of reciprocating air compressors has become particularly important. Digital twin is a technology that can reflect the behavior of physical entities in real time, accurately predicting and evaluating the operation status of reciprocating air compressors. However, the establishment of a digital twin model for reciprocating air compressors requires a significant amount of computational resources, which can result in the inability to meet the requirements of real-time performance evaluation. To overcome this limitation, this paper proposes a method for constructing a digital twin model of reciprocating air compressors based on a surrogate model. The surrogate model is constructed based on a long short-term memory neural network with physical information(PILSTM). This model can accurately describe the changes in cylinder pressure by combining physical information. According to the characteristics of cylinder pressure changes, regularization formulas are added to ensure the smoothness of the predicted pressure. The experimental results show that the digital twin model based on the surrogate model has high prediction accuracy and real-time performance. Therefore, this model provides a new method for monitoring the operating status of reciprocating air compressors.}
}
@article{BARBOSA2025100864,
title = {An interchangeable editor to create generic and adaptable decision trees for versatile applications and game development scenarios},
journal = {Entertainment Computing},
volume = {52},
pages = {100864},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100864},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124002325},
author = {Rafael Garcia Barbosa and Maria Andréia Formico Rodrigues},
keywords = {Decision tree editor, Standard interchange format, Interoperability and adaptability, Personalized learning, Game engine integration, Game development scenarios},
abstract = {This paper introduces a novel software tool developed to serve as an editor for the manual construction of decision trees, characterized by their generic nature, flexibility, and adaptability across a wide range of applications and game development scenarios. The editor enables straightforward definition and modification of decision tree elements and data, dynamically updating to meet changing needs and contexts. A key feature of this tool is its capability to export decision trees in a standardized interchange format, enhancing interoperability by allowing seamless integration with other computational platforms, including game engines. We demonstrate the utility and versatility of our editor with three distinct and comprehensive use cases, highlighting its potential as a significant contribution to interactive technology. The tool facilitates the development of decision trees, enhancing informed decision-making and strategic planning, and supports personalized learning to improve engagement and outcomes.}
}
@article{SALINGER1994139,
title = {Massively parallel finite element computations of three-dimensional, time-dependent, incompressible flows in materials processing systems},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {119},
number = {1},
pages = {139-156},
year = {1994},
issn = {0045-7825},
doi = {https://doi.org/10.1016/0045-7825(94)00081-6},
url = {https://www.sciencedirect.com/science/article/pii/0045782594000816},
author = {Andrew G. Salinger and Qiang Xiao and Yuming Zhou and Jeffrey J. Derby},
abstract = {A parallel implementation of the Galerkin finite element method for three-dimensional, incompressible flows is presented. The inherent element-by-element parallelism of the method is exploited to make efficient use of the architecture of the CM-5 computer. Our implementation features a mixed formulation to expand the primitive variables using triquadratic brick elements with linear, discontinuous pressure basis functions, and the GMRES method with diagonal preconditioning is employed to solve the linear system at each Newton iteration. Transitions among flow states in the classical Taylor-Couette system, which are representative of the complexity of flows found in materials processing systems, are computed as benchmark solutions, and preliminary results are presented for flow in a large-scale, solution crystal growth system. Sustained calculation rates of up to 6 GigaFLOPS are achieved on 512 processors of the CM-5.}
}
@article{RUAN2023100872,
title = {Public perception of electric vehicles on Reddit and Twitter: A cross-platform analysis},
journal = {Transportation Research Interdisciplinary Perspectives},
volume = {21},
pages = {100872},
year = {2023},
issn = {2590-1982},
doi = {https://doi.org/10.1016/j.trip.2023.100872},
url = {https://www.sciencedirect.com/science/article/pii/S2590198223001197},
author = {Tao Ruan and Qin Lv},
keywords = {Cross-platform, Twitter, Reddit, Electric vehicles, Public perception, Topic modeling, Computational social science},
abstract = {Electrified mobility such as electric vehicles (EVs) is a promising solution to reduce carbon emissions in transportation and mitigate global warming. Understanding public perception of EVs can help better support their adoption. A previous study shows that online social networks (OSNs) such as Reddit can be a valuable source for studying public perceptions of EVs and provide different perspectives from traditional methods that leverage surveys, questionnaires, or interviews (Ruan and Lv, 2022). Our work aims to investigate this direction further through the following research question: Given the distinct mechanisms of various OSNs, can we obtain a more comprehensive picture of public perception of EVs by integrating the analysis from different platforms? Specifically, our study is based on EV-related discussions on two popular OSN platforms: Twitter and Reddit. We have collected 3,437,917 Reddit posts (including 274,979 submissions and 3,162,938 comments) and 7,383,327 Tweets between January 2011 and December 2020 and analyzed them from several perspectives. Our analysis shows that users have had different topic and sentiment patterns in EV-related discussions on the two platforms over the past decade. We also leverage the verified account information on Twitter to reveal that the most influential users are politicians and news media; however, the general public has very different conversation patterns with the two types of accounts — politicians seem to be increasingly (over) optimistic about EVs while the public may think differently.}
}
@article{SATPUTE2024109583,
title = {Exploring large language models for microstructure evolution in materials},
journal = {Materials Today Communications},
volume = {40},
pages = {109583},
year = {2024},
issn = {2352-4928},
doi = {https://doi.org/10.1016/j.mtcomm.2024.109583},
url = {https://www.sciencedirect.com/science/article/pii/S2352492824015642},
author = {Prathamesh Satpute and Saurabh Tiwari and Maneet Gupta and Supriyo Ghosh},
keywords = {Large language models, Materials science, Phase-field models, Microstructure evolution, Partial differential equations},
abstract = {There is a significant potential for coding skills to transition fully to natural language in the future. In this context, large language models (LLMs) have shown impressive natural language processing abilities to generate sophisticated computer code for research tasks in various domains. We report the first study on the applicability of LLMs to perform computer experiments on microstructure pattern formation in model materials. In particular, we exploit LLM’s ability to generate code for solving various types of phase-field-based partial differential equations (PDEs) that integrate additional physics to model material microstructures. The results indicate that LLMs have a remarkable capacity to generate multi-physics code and can effectively deal with materials microstructure problems up to a certain complexity. However, for complex multi-physics coupled PDEs for which a detailed understanding of the problem is required, LLMs fail to perform the task efficiently, since much more detailed instructions with many iterations of the same query are required to generate the desired output. Nonetheless, at their current stage of development and potential future advancements, LLMs offer a promising outlook for accelerating materials education and research by supporting beginners and experts in their physics-based methodology. We hope this paper will spur further interest to leverage LLMs as a supporting tool in the integrated computational materials engineering (ICME) approach to materials modeling and design.}
}
@incollection{STAPLETON2014127,
title = {8.07 - Administrative Evil and Patient Health: A Critique of the Impact of Manufacturing Systems on Health Care},
editor = {Saleem Hashmi and Gilmar Ferreira Batalha and Chester J. {Van Tyne} and Bekir Yilbas},
booktitle = {Comprehensive Materials Processing},
publisher = {Elsevier},
address = {Oxford},
pages = {127-150},
year = {2014},
isbn = {978-0-08-096533-8},
doi = {https://doi.org/10.1016/B978-0-08-096532-1.00813-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008096532100813X},
author = {L. Stapleton},
keywords = {AMAT, Health, Manufacturing},
abstract = {Manufacturing systems principles underpin enterprise information systems. Nowadays these principles, and the systems that accompany them, are widely applied across various sectors, including health services management systems. The question arises: To what extent are these principles appropriate for health care management applications? This chapter explores the question from a human-centered systems perspective by examining the rationalities and assumptions that underpin manufacturing systems and applying these ideas to health care contexts. Human-centered systems have a long theoretical tradition within the automation and control community stretching back at least into the 1970s. It is a particularly strong theme in manufacturing systems research. As automation and control systems are increasingly important outside the factory, many researchers are revisiting core concepts within this tradition. One particularly important sector is health care, which, in recent years, has implemented a range of AMAT (automation and machine-assisted thinking)-type solutions not the least of which are enterprise resource planning systems (ERPs). These implementations have been accompanied by highly publicized systems failures. Ethical problems have also arisen. The chapter exposes an ‘administrative evil’ that relegates the patient to the status of a subassembly, a component in an ever-more complex health care production line. Humans are dehumanized in the rationality of our health care administrative systems. The chapter concludes that health care systems projects should adopt a human-centered approach that draws on research in manufacturing, automation, and control engineering as well as other disciplines.}
}
@article{ISOLAN2024111786,
title = {Monte Carlo analysis of dosimetric issues in space exploration},
journal = {Radiation Physics and Chemistry},
volume = {221},
pages = {111786},
year = {2024},
issn = {0969-806X},
doi = {https://doi.org/10.1016/j.radphyschem.2024.111786},
url = {https://www.sciencedirect.com/science/article/pii/S0969806X24002780},
author = {Lorenzo Isolan and Valentina Sumini and Marco Sumini},
keywords = {Space habitat, MCNP6, Unstructured mesh, Topology optimization, Radiation protection},
abstract = {The Radiation protection is of paramount importance in the planning of human exploration activities in space. The related risks must be considered with respect to two aspects: devising a proper shielding and providing answers to the requirement of an effective dosimetry evaluation in astronaut's activities. Both aspects have been considered using the Monte Carlo (MC) code MCNP 6.2 as the reference tool. As case study an application devised for the National Aeronautics and Space Administration (NASA) Artemis program has been chosen. The project aims to establish a sustainable human presence on the Moon, envisioning the realization of an outpost that will serve as a steppingstone for space exploration endeavors. A Class III shelter, in situ resource utilization (ISRU) built habitat for the Moon, has been designed through computational methods and topology optimization techniques, and analyzed in terms of radiation shielding performances and the strictly related structural behavior. The outpost must be able to withstand temperature variations, micrometeorite impacts, and the absence of a substantial atmosphere. Any solution studied to respect the constraints must devise robust and innovative materials and techniques to create habitats that have as goal the shielding from the Galactic Cosmic Rays (GCR) and from the solar flares to provide a safe and habitable environment at the time scales scheduled for the missions. Moreover, the outpost design must incorporate strategies for extracting and utilizing local resources. Overcoming such challenges will pave the way for the establishment of a sustainable human presence on the Moon and serve as a crucial leap for future space exploration missions.}
}
@article{WU2020107246,
title = {miRNA-324/-133a essential for recruiting new synapse innervations and associative memory cells in coactivated sensory cortices},
journal = {Neurobiology of Learning and Memory},
volume = {172},
pages = {107246},
year = {2020},
issn = {1074-7427},
doi = {https://doi.org/10.1016/j.nlm.2020.107246},
url = {https://www.sciencedirect.com/science/article/pii/S1074742720300903},
author = {Ruixiang Wu and Shan Cui and Jin-Hui Wang},
keywords = {Associative learning, Memory cell, Neural circuit, Barrel cortex, Piriform cortex},
abstract = {After the integrative storage of associated signals, a signal induces the recollection of its associated signal, or the other way around. This associative memory is essential to associative thinking, logical reasoning, imagination and computation. In terms of cellular mechanisms underlying associative memory, new mutual synapse innervations are formed among those coactivated neurons, so that they are recruited to be associative memory cells or associative memory neurons. These associative memory cells receive new synapse innervations alongside innate synapse inputs and encode signals carried by these inputs. We proposed to examine microRNAs as initiative factors for recruiting new synapse innervations and associative memory cells. In a mouse model of associative memory characterized as the reciprocal retrieval of associated whisker and odor signals, barrel and piriform cortical neurons gain their ability to encode whisker and odorant signals based on the newly formed synapse innervations between these coactivated cortices besides innate synapse inputs. miRNA-324 and miRNA-133a are required for recruiting these new synapse innervations and associative memory cells as well as sufficient for facilitating their recruitments, but not for innate synapse inputs. Therefore, the coactivation of sensory cortices through microRNA as initiative factor to recruit new mutual synapse innervations and associative memory cells for associative memory.}
}
@article{OFOSUAMPONG2024100127,
title = {Artificial intelligence research: A review on dominant themes, methods, frameworks and future research directions},
journal = {Telematics and Informatics Reports},
volume = {14},
pages = {100127},
year = {2024},
issn = {2772-5030},
doi = {https://doi.org/10.1016/j.teler.2024.100127},
url = {https://www.sciencedirect.com/science/article/pii/S2772503024000136},
author = {Kingsley Ofosu-Ampong},
keywords = {Artificial intelligence, Classification, Literature review, Technological issues, Research agenda},
abstract = {This article presents an analysis of artificial intelligence (AI) in information systems and innovation-related journals to determine the current issues and stock of knowledge in AI literature, research methodology, frameworks, level of analysis and conceptual approaches. By doing this, the article aims to identify research gaps that can guide future investigations. A total of 85 peer-reviewed articles from 2020 to 2023 were used in the analysis. The findings show that extant literature is skewed towards the prevalence of technological issues and highlights the relatively lower focus on other themes, such as contextual knowledge co-creation issues, conceptualisation, and application domains. While there have been increasing technological issues with artificial intelligence, the three identified areas of security concern are data security, model security and network security. Furthermore, the review found that contemporary AI, which continually drives the boundaries of computational capabilities to tackle increasingly intricate decision-making challenges, distinguishes itself from earlier iterations in two primary aspects that significantly affect organisational learning in dealing with AI's potential: autonomy and learnability. This study contributes to AI research by providing insights into current issues, research methodology, level of analysis and conceptual approaches, and AI framework to help identify research gaps for future investigations.}
}
@incollection{SARSANI2011231,
title = {Computers and Creativity},
editor = {Mark A. Runco and Steven R. Pritzker},
booktitle = {Encyclopedia of Creativity (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {231-240},
year = {2011},
isbn = {978-0-12-375038-9},
doi = {https://doi.org/10.1016/B978-0-12-375038-9.00041-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780123750389000418},
author = {M.R. Sarsani},
keywords = {Approaches to creativity, Computer applications, Computer functions, Computers, Computers and creativity, Creativity definitions, Metaphor, Problem solving, Productivity tools},
abstract = {Computers have entered all walks of human life across the world. Computers are being used by people of all ages and in every profession, in their work as well as in their leisure. There is growing interest in the application of computer-based productivity tools to support simulation effects, higher level thinking, metacognitive processes, maintaining interest, promoting learning, developing curiosity, and fostering creativity. The Internet has brought abort a revolution in the world of information technology by providing searching facilities for exploring or seeking information from all over the world (e.g., e-learning, e-shopping, e-mail, Telnet and Usenet, audio and video conferences, etc.). Different viewpoints have been put forward to explain the concept, emphasizing different aspects of creativity. Generally, creativity has been discussed in terms of its end product, creative person, creative process, and creative press or environments. There are no substantial researches directly measuring the effect of the computer simulation technology to support either uncreative drill or creative production. Some researchers speculate that computer simulation technology may have a positive effect on creativity. However, due to a lack of empirical research, the true effect of simulation technology on creativity is still unknown and inconclusive.}
}
@article{DING2012264,
title = {Finding MicroRNA Targets in Plants: Current Status and Perspectives},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {10},
number = {5},
pages = {264-275},
year = {2012},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2012.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S1672022912000733},
author = {Jiandong Ding and Shuigeng Zhou and Jihong Guan},
keywords = {MicroRNA, Target prediction, Degradome-seq, Integration},
abstract = {MicroRNAs (miRNAs), a class of ∼20–24nt long non-coding RNAs, have critical roles in diverse biological processes including development, proliferation, stress response, etc. With the development and availability of experimental technologies and computational approaches, the field of miRNA biology has advanced tremendously over the last decade. By sequence complementarity, miRNAs have been estimated to regulate certain mRNA transcripts. Although it was once thought to be simple and straightforward to find plant miRNA targets, this viewpoint is being challenged by genetic and biochemical studies. In this review, we summarize recent progress in plant miRNA target recognition mechanisms, principles of target prediction, and introduce current experimental and computational tools for plant miRNA target prediction. At the end, we also present our thinking on the outlook for future directions in the development of plant miRNA target finding methods.}
}
@article{BACHMANN2020102937,
title = {Account of consciousness by Christof Koch: Review and questions},
journal = {Consciousness and Cognition},
volume = {82},
pages = {102937},
year = {2020},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2020.102937},
url = {https://www.sciencedirect.com/science/article/pii/S1053810020300143},
author = {Talis Bachmann},
keywords = {Consciousness, Integrated information, Cognitive computation, Microgenesis, Phenomenal experience},
abstract = {This review is set to present the gist of the theoretical account of consciousness recently presented by Christof Koch and pose a couple of questions instigated by this account. The expected answers to these questions would hopefully help to advance our understanding of the basic nature of the conscious mind.}
}
@article{DASILVA2024105785,
title = {Optimization of open web steel beams using the finite element method and genetic algorithms},
journal = {Structures},
volume = {60},
pages = {105785},
year = {2024},
issn = {2352-0124},
doi = {https://doi.org/10.1016/j.istruc.2023.105785},
url = {https://www.sciencedirect.com/science/article/pii/S2352012423018738},
author = {Amilton Rodrigues {da Silva} and Gabriela Pereira Lubke},
keywords = {Open-web beams, Optimization, Genetic algorithm, Finite element method},
abstract = {Studies on structural optimization have gained prominence recently, and the search to consume resources in a more conscious and effective way encourages the use of such techniques in all fields. In this respect, this study aims to use computational optimization techniques to determine the maximum load-bearing capacity of hollow-core steel beams for two groups of different shear lines, one generating beams with opening in the shape of hexagons and the other having the shape of ellipses. The second group includes beams with circular openings as a particular case. A three-node triangular finite element for the analysis of structures in plane stress is used for the structural analysis of the beams. The design variables define the shape and number of opening in the beam, and a computational formulation using a genetic algorithm is presented to find the cut line that maximizes the load capacity of the beam considering different ultimate and service limit states. Numerical and experimental models in the literature are used to validate the implementations presented in this article, and the results of optimized hollow core beams are presented, demonstrating the efficiency of the formulations used.}
}
@article{ELIAZ2010304,
title = {Paying for confidence: An experimental study of the demand for non-instrumental information},
journal = {Games and Economic Behavior},
volume = {70},
number = {2},
pages = {304-324},
year = {2010},
issn = {0899-8256},
doi = {https://doi.org/10.1016/j.geb.2010.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0899825610000229},
author = {Kfir Eliaz and Andrew Schotter},
abstract = {This paper presents experimental evidence that when individuals are about to make a given decision under risk, they are willing to pay for information on the likelihood that this decision is ex-post optimal, even if this information will not affect their decision. Our findings suggest that this demand for non-instrumental information is caused by what we refer to as a “confidence effect”: the desire to increase one's posterior belief by ruling out “bad news”, even when such news would have no effect on one's decision. We conduct various treatments to show that our subjects' behavior is not likely to be caused by an intrinsic preference for information, failure of backward induction or an attempt to minimize thinking costs.}
}
@article{COSMIDES198951,
title = {Evolutionary psychology and the generation of culture, part II: Case study: A computational theory of social exchange},
journal = {Ethology and Sociobiology},
volume = {10},
number = {1},
pages = {51-97},
year = {1989},
issn = {0162-3095},
doi = {https://doi.org/10.1016/0162-3095(89)90013-7},
url = {https://www.sciencedirect.com/science/article/pii/0162309589900137},
author = {Leda Cosmides and John Tooby},
keywords = {Reciprocal Altruism, Cooperation, Tit for tat, Cognition, Reasoning, Evolution, Learning, Culture},
abstract = {Models of the various adaptive specializations that have evolved in the human psyche could become the building blocks of a scientific theory of culture. The first step in creating such models is the derivation of a so-called “computational theory” of the adaptive problem each psychological specialization has evolved to solve. In Part II, as a case study, a sketch of a computational theory of social exchange (cooperation for mutual benefit) is developed. The dynamics of natural selection in Pleistocene ecological conditions define adaptive information processing problems that humans must be able to solve in order to participate in social exchange: individual recognition, memory for one's history of interaction, value communication, value modeling, and a shared grammar of social contracts that specifies representational structure and inferential procedures. The nature of these adaptive information processing problems places constraints on the class of cognitive programs capable of solving them; this allows one to make empirical predictions about how the cognitive processes involved in attention, communication, memory, learning, and reasoning are mobilized in situations of social exchange. Once the cognitive programs specialized for regulating social exchange are mapped, the variation and invariances in social exchange within and between cultures can be meaningfully discussed.}
}
@article{VALJAK2023191,
title = {Functional modelling through Function Class Method: A case from DfAM domain},
journal = {Alexandria Engineering Journal},
volume = {66},
pages = {191-209},
year = {2023},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2022.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1110016822007852},
author = {Filip Valjak and Nenad Bojčetić},
keywords = {Functional modelling, Function structure, Function class, Design for Additive Manufacturing},
abstract = {Functional modelling is an essential part of systematic design approaches and is often prescribed in engineering design textbooks. However, function models created with current function modelling techniques often lack formal and repeatable representation, limiting their use in computational reasoning. Therefore, this paper presents a new functional modelling method to support function models' creation with formal and repeatable representation. The key element of the proposed method is a Function Class – a function-modelling element that categorises defined functions on a function block level by specifying operating flow, input and output flows, and integrates primary rules for functional modelling such as conservation law. The formalisation on a function block level reduces the number of morphological errors and provides a theoretical framework for future computational processing of function models. This paper proposes a protocol for developing Function Classes and defines a theoretical function modelling framework through Function Class Method. The development and use of the Function Class Method are demonstrated through the development of Function Classes for the Design for Additive Manufacturing domain as the first step toward a universal function modelling approach.}
}
@article{DING2024120338,
title = {Next generation of computer vision for plant disease monitoring in precision agriculture: A contemporary survey, taxonomy, experiments, and future direction},
journal = {Information Sciences},
volume = {665},
pages = {120338},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.120338},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524002512},
author = {Weiping Ding and Mohamed Abdel-Basset and Ibrahim Alrashdi and Hossam Hawash},
keywords = {Computer vision, Deep learning, Convolutional neural networks (CNNs), Vision transformers, Vision MLPs, Plant recognition, Precision agriculture},
abstract = {Efficient and rational monitoring of plant health is an essential prerequisite for ensuring optimal crop production and resource management in the field of agriculture. Computer vision techniques have revolutionized visual disease monitoring with their exceptional visual recognition performance. However, despite the outstanding results, the widespread acceptance of these methods in agriculture practice is still in its early stages. This study presents a comprehensive survey of the next generation of computer vision models applied to plant disease monitoring in precision agriculture. Our study begins by tracing the evolution of agricultural computer vision research over the past decade, encompassing legacy methods such as convolutional neural networks (CNNs), progressing to newer techniques like vision transformers (ViTs), and culminating in cutting-edge vision multi-layer perceptrons (MLPs). Next, our study embraces both qualitative and quantitative approaches, supporting a profound review of literature and classifying methodologies and experimental approaches. A significant contribution lies in our comprehensive taxonomy, offering a fine-grained categorization of current computer vision models. This taxonomy meticulously highlights the potentials and limitations of these models while explaining their roles in improving plant disease management. Moreover, extensive experimental comparisons are conducted on PlantVillage dataset to evaluate the performance of state-of-the-art computer-vision models for plant recognition data. The obtained results are then utilized to draw insightful conclusions about the behavior of these models and provide guidance for selecting the most suitable one for specific tasks at hand. Additionally, we discuss open research avenues and future directions of computer-vision models in plant disease management including challenges related to the data scarcity, the computational efficiency, need for explainability, and multi-modal analysis.}
}
@article{KUMAR202415,
title = {Hybrid approach of type-2 fuzzy inference system and PSO in asthma disease},
journal = {Clinical eHealth},
volume = {7},
pages = {15-26},
year = {2024},
issn = {2588-9141},
doi = {https://doi.org/10.1016/j.ceh.2024.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S2588914124000017},
author = {Tarun Kumar and Anirudh {Kumar Bhargava} and M.K. Sharma and Nitesh Dhiman and Neha Nain},
keywords = {Asthma, Type-2 fuzzy set, Type-2 fuzzy optimized system, Particle swarm optimization, Medical diagnostic},
abstract = {This research work presents a hybrid approach combining a type-2 fuzzy inference system with particle swarm optimization (PSO) to develop a type-2 fuzzy optimized inference system, specifically tailored for asthma patient data. Addressing the inherent uncertainty in medical diagnostics, this model enhances traditional type-1 fuzzy logic by incorporating ambiguity into linguistic variables and utilizing type-2 fuzzy if-then rules. The system is trained to minimize diagnostic error in asthma disease identification. Applied to a dataset comprising eight medical entities from asthma patients, the model demonstrates substantial accuracy improvements. Numerical computations validate the system, showing a decrease in error rate from 1.445 to 0.03, indicating a significant enhancement in diagnostic precision. These results underscore the potential of our model in medical diagnostic problems, providing a novel and effective tool for tackling the complexities of asthma diagnosis.}
}
@article{SCHEFFLER201575,
title = {NeurOS™ and NeuroBlocks™ a neural/cognitive operating system and building blocks},
journal = {Biologically Inspired Cognitive Architectures},
volume = {11},
pages = {75-105},
year = {2015},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2014.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X14000747},
author = {Lee Scheffler},
keywords = {Cognition, Perception, Pattern recognition, Memory, Learning, Behavior},
abstract = {NeurOS is an open platform for accelerating research, development and hosting execution of intelligent applications. A NeurOS application is a directed “neural graph” of modular components connected by signal paths, similar to biological brain connectivity and functional block diagrams of neural pathways. Built-in reusable modules (NeuroBlocks) provide a wide range of general- and special-purpose capabilities: inputs/senses, outputs/effectors, processing, memory, pattern learning and recognition, visualization/instrumentation, custom module development, integrating external intelligence capabilities, and sub-graph reuse. NeurOS sub-graph assemblies address neural/cognitive functions including perception, pattern learning and recognition, working memory, imagination, prediction, context priming, attention, abstraction, classification, associational thinking and behavior. NeurOS applications are inherently portable, scalable, networkable, extensible and embeddable. NeurOS development tools provide simple intuitive graphical drag and drop application assembly from components without programming, along with testing, debugging, monitoring and visualization. Prototype NeurOS applications have begun to explore a wide range of intelligent functions in diverse areas, including aspects of pattern recognition, vision, music, reading, puzzle solving, reasoning, behavior. Building working intelligent systems using NeurOS and NeuroBlocks lets researchers and developers focus on their core functions and rapidly iterate and instrument working models, fostering both analytical and biological insight as well as usable systems.}
}
@article{FEIZABADI2024103461,
title = {When and under what conditions ambidextrous supply chains prove effective? Insights from simulation and empirical studies},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {183},
pages = {103461},
year = {2024},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2024.103461},
url = {https://www.sciencedirect.com/science/article/pii/S1366554524000516},
author = {Javad {Feiz Abadi} and David M. Gligor and Somayeh {Alibakhshi Motlagh} and Raj Srivastava},
keywords = {Supply Chain Archetype, Ambidexterity, NK modeling, Paradoxes},
abstract = {Our research delves into the impact of ambidextrous supply chain activity configurations on performance, particularly in the dynamic and complex contexts of today's business landscape. Drawing from the rich literature on paradox theory, we aim to unravel the efficacy of ambidextrous supply chain setup in mitigating the tensions inherent in managing dynamism, complexities, munificence, and, as well as understanding the contextual factors that modulate this efficacy. To accomplish this, we construct a computational model that captures the resource allocation and search behavior of the ambidextrous supply chain archetype within the ever-shifting terrain of performance. Our findings reveal that ambidextrous supply chain configurations excel at reconciling paradoxical tensions stemming from high complexity, limited resource abundance, and turbulent market conditions. Empirical data substantiate these findings.}
}
@article{CHERNYSHOV20151345,
title = {Information Support and Skill Evaluation of Human-Operators},
journal = {IFAC-PapersOnLine},
volume = {48},
number = {3},
pages = {1345-1350},
year = {2015},
note = {15th IFAC Symposium onInformation Control Problems inManufacturing},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2015.06.273},
url = {https://www.sciencedirect.com/science/article/pii/S2405896315005121},
author = {K.R. Chernyshov and E.Ph Jharko},
keywords = {Human-operator, Information support, Flexible simulation, Evaluation of skills, Random processes, Measures of dependence},
abstract = {The paper presents an approach to design an intelligent information support system to be used as a human-operator assistant to control large complex industrial plants. Tasks and structure of such an intelligent information support system (IISS), IISS design stages, methodology of IISS design, toolkits for IISS design are considered. A flexible simulation complex (FSC) as such an intelligent toolkit has been presented. The complex is used as a “kernel” of IISS for human-operators of a nuclear power plant. A new approach to abnormal situations with regard for the heuristic regularities of human-operator thinking process is proposed. The regularities are revealed on basis of recording the motions of the human- operator eyes over the information field of the control board and processing the experimental data obtained. For data processing, a probability theoretical approach is used based on involving the notion of consistency of measures of dependence of random variables.}
}
@incollection{ADDIS2025501,
title = {Memory and imagination},
editor = {Jordan Henry Grafman},
booktitle = {Encyclopedia of the Human Brain (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {501-513},
year = {2025},
isbn = {978-0-12-820481-8},
doi = {https://doi.org/10.1016/B978-0-12-820480-1.00135-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204801001352},
author = {Donna Rose Addis},
keywords = {Associative, Autobiographical memory, Default mode network, Episodic memory, Frontoparietal control network, Future thinking, Hippocampus, Imagination, Medial prefrontal cortex, Prospection, Relational processing, Schema, Simulation},
abstract = {The human brain has a remarkable capacity to not only remember events from the past but to construct a variety of imagined experiences, ranging from hypothetical and counterfactual past events, to future events and entirely fictional episodes. Both remembering and imagining is underpinned by the brain's simulation system: default mode network. I describe the theoretical beginnings of this relatively new topic in contemporary neuroscience, as well as neuroimaging investigations of autobiographical memory and prospective imagination that together provide evidence of a single “simulation system” supported primarily by core functions of the default mode network: associating elements, associative schematic processes, and buffering the emergent simulation.}
}
@article{SZYMANSKI2021,
title = {Words Are Essential, but Underexamined, Research Tools for Microbes and Microbiomes},
journal = {mSystems},
volume = {6},
number = {4},
year = {2021},
issn = {2379-5077},
doi = {https://doi.org/10.1128/msystems.00769-21},
url = {https://www.sciencedirect.com/science/article/pii/S2379507721002683},
author = {Erika Szymanski},
keywords = {discourse, engineering, metaphor, microbiome, science and technology studies, synthetic biology, synthetic yeast},
abstract = {Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive.
ABSTRACT
Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive. My research addresses this deficit by examining how metaphors for handling microbes shape possibilities for working with yeast and bacteria in synthetic biology, microbiome research, and other fields that reconfigure what microbes can be. Though poised to reexamine assumptions, these fields routinely rest on metaphors and other language tools that quietly embed ways of thinking that may work against wider aims—for example, imagining bacteria as imperfect machines that should therefore be rendered increasingly passive and controllable. Researchers, therefore, need to examine how language tools structure their observations and expectations so that the tools they choose are appropriate for the work they want to do.}
}
@article{RANDALL1991219,
title = {Review of linear least squares computations: by R.W. Farebrother},
journal = {Linear Algebra and its Applications},
volume = {153},
pages = {219-223},
year = {1991},
issn = {0024-3795},
doi = {https://doi.org/10.1016/0024-3795(91)90221-H},
url = {https://www.sciencedirect.com/science/article/pii/002437959190221H},
author = {John H. Randall}
}
@article{SIMONE2021103070,
title = {Rome was not built in a day. Resilience and the eternal city: Insights for urban management},
journal = {Cities},
volume = {110},
pages = {103070},
year = {2021},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2020.103070},
url = {https://www.sciencedirect.com/science/article/pii/S0264275120314189},
author = {Cristina Simone and Francesca Iandolo and Irene Fulco and Francesca Loia},
keywords = {Rome, Urban resilience, Urban management, Aspect-based sentiment analysis (ABSA), Collective perception},
abstract = {Resilience has been intensely investigated as the viable quality of individuals, groups, organizations, and systems to respond productively to notable change without engaging in an extended period of regressive behaviour. Recently, there has been growing attention to the relationship between resilience and cities. To contribute to this stimulating debate, this paper first provides the theoretical framework and links the concept of resilience to urban studies. Subsequently, it enlightens, through a systems perspective and the aspect-based sentiment analysis (ABSA) methodology, the possibility to enrich the information variety endowment of urban policymakers, generated by new information units, to foster resilience capabilities in the urban context. Specifically, a large-scale text analysis study was conducted on the city of Rome to understand the sentiments expressed within the text generated online by citizens and visitors. The positive or negative sentiments linked to the hidden problems of the urban context were organized within collective perception-based maps for each of the analysed points of interest (POIs). Since cities represent complex decision-making contexts, this study aimed to outline a methodology and a tool that would help foster resilient thinking in urban policies by enriching the diversity of the information variety endowment of urban decision-makers.}
}
@article{YONG2023e13529,
title = {Structure bionic topology design method based on biological unit cell},
journal = {Heliyon},
volume = {9},
number = {2},
pages = {e13529},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e13529},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023007363},
author = {Yang Yong and Jiang Xue-tao and Zhu Qi-xin and Lu En-hui and Dong Xin-feng and Li Jing-bin},
keywords = {Biological unit cell, Substructure, Matter element, TRIZ},
abstract = {The mechanical structure topology design based on substructure always adopts the traditional substructure design method, which often comes from the experience and is limited by the inherent or stereotyped design thinking. A substructure design method based on biological unit cell (UC) is proposed, which draws inspiration from the biological efficient load-bearing topology structure. Especially, the thought of the formalized problem-solving of extension matter-element is introduced. Through the matter-element definition of UC substructure, the process model for the structure bionic topology design method based on biological UC is formed, which avoids the random or wild mental stimulation of the structure topology design method based on traditional substructure. In particular, in this proposed method, aiming at the problem about how to achieve the integration of high-efficiency load-bearing advantage of different organisms, furthermore, a biological UC hybridization method based on the principle of inventive problem solving theory (TRIZ) is proposed. The typical case is used to illustrate the process of this method in detail. The results from simulations and experiments both show that: the load-bearing capacity of structure design based on biology UC is improved than the initial design; on this basis, the load-bearing capacity of structure design is improved further through UC hybridization. All these show the feasibility and correctness of the proposed method.}
}
@article{FOSGERAU2021109911,
title = {Some remarks on CCP-based estimators of dynamic models},
journal = {Economics Letters},
volume = {204},
pages = {109911},
year = {2021},
issn = {0165-1765},
doi = {https://doi.org/10.1016/j.econlet.2021.109911},
url = {https://www.sciencedirect.com/science/article/pii/S0165176521001889},
author = {Mogens Fosgerau and Emerson Melo and Matthew Shum and Jesper R.-V. Sørensen},
keywords = {Dynamic discrete choice, Random utility, Linear programming, Convex analysis, Convex optimization},
abstract = {This note provides several remarks relating to the conditional choice probability (CCP) based estimation approaches for dynamic discrete-choice models. Specifically, the Arcidiacono and Miller (2011) estimation procedure relies on the ”inverse-CCP” mapping ψp from CCPs to choice-specific value functions. Exploiting the convex-analytic structure of discrete choice models, we discuss two approaches for computing this mapping, using either linear or convex programming, for models where the utility shocks can follow arbitrary parametric distributions. Furthermore, the ψ function is generally distinct from the ”selection adjustment” term (i.e. the expectation of the utility shock for the chosen alternative), so that computational approaches for computing the latter may not be appropriate for computing ψ.}
}
@article{VASILE201177,
title = {Entry points, interests and attitudes. An integrative approach of learning},
journal = {Procedia - Social and Behavioral Sciences},
volume = {11},
pages = {77-81},
year = {2011},
note = {Teachers for the Knowledge Society},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2011.01.037},
url = {https://www.sciencedirect.com/science/article/pii/S1877042811000395},
author = {Cristian Vasile},
keywords = {multiple intelligence, entry points, personality, interests},
abstract = {The relationship between personality and intelligence is of a major importance in the learning process. Interests and attitudes are related to the entry points on emotional ground. In some educational systems the focus on cognitive abilities and cognitive functions increased, amplified by the neuroscience and the computational approach. The cognitive approach should be enriched with major aspects from the global human psychological system like interests/motivation, emotional profile, attitudes and so on. The focus on cognition only, or the computational view should be completed with personality approaches and behavior regulation, all of these influencing without doubt the intelligence.}
}
@incollection{GORI2024339,
title = {Chapter 6 - Learning with constraints},
editor = {Marco Gori and Alessandro Betti and Stefano Melacci},
booktitle = {Machine Learning (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
pages = {339-442},
year = {2024},
isbn = {978-0-323-89859-1},
doi = {https://doi.org/10.1016/B978-0-32-389859-1.00013-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323898591000131},
author = {Marco Gori and Alessandro Betti and Stefano Melacci},
keywords = {Support constraint machines, Learning from constraints, Lifelong learning, Constraint satisfaction, Penalty functions, Logic constraints, t-norms, Recurrent neural networks, Long short term memory networks (LSTM), Graphical models},
abstract = {This chapter provides a unified view of learning and inference in structured environments that are formally expressed as constraints that involve both data and tasks. A preliminary discussion has been put forward in Section 1.1.5, where we began proposing an abstract interpretation of the ordinary notion of constraint that characterizes human-based learning, reasoning, and decision processes. Here we make an effort to formalize those processes and explore the corresponding computational aspects. A first fundamental remark for the formulation of a sound theory is that most interesting real-world problems correspond with learning environments that are heavily structured, a feature that has been mostly neglected in the previous chapters on linear and kernel machines, as well as on deep networks. So far we have been mostly concerned with machine learning models where the agent takes a decision on patterns represented by x∈Rd, whereas we have mostly neglected the issue of constructing appropriate representations from the environmental information e∈E. The discussion in Section 1.1.5 has already stimulated the need of processing information organized as lists, trees, and graphs. Interestingly, in this chapter, it is shown that computational models, like recurrent neural networks and graph neural networks can also be regarded as a way for expressing appropriate constraints on environmental data by means of diffusion processes. In these cases the distinguishing feature of the computational model is that the focus is on uniform diffusion processes, whereas one can think of constraints that involve both data and tasks in a more general way. Basically, different vertexes of a graph that model the environment can be involved in different relations, thus giving rise to a different treatment. As a result, this yields richer computational mechanisms that involve the meaning attached to the different relations.}
}
@article{BEYNON2008476,
title = {Experimenting with computing},
journal = {Journal of Applied Logic},
volume = {6},
number = {4},
pages = {476-489},
year = {2008},
note = {The Philosophy of Computer Science},
issn = {1570-8683},
doi = {https://doi.org/10.1016/j.jal.2008.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S157086830800044X},
author = {Meurig Beynon and Steve Russ},
keywords = {Empirical Modelling, Observation, Experiment, Computing, Theory, Radical empiricism, Dependency, Agency},
abstract = {We distinguish two kinds of experimental activity: post-theory and exploratory. Post-theory experiment enjoys computer support that is well-aligned to the classical theory of computation. Exploratory experiment, in contrast, arguably demands a broader conception of computing. Empirical Modelling (EM) is proposed as a more appropriate conceptual framework in which to provide computational support for exploratory experiment. In the process, it promises to provide integrated computational support for both exploratory and post-theory experiment. We first sketch the motivation for EM and illustrate its potential for supporting experimentation, then briefly highlight the semantic challenge it poses and the philosophical implications.}
}
@incollection{MENAMADATHIL202463,
title = {Chapter 4 - Machine learning approach for vaccine development-fundamentals},
editor = {Jayashankar Das and Sushma Dave and Siomar de Castro Soares and Sandeep Tiwari},
booktitle = {Reverse Vaccinology},
publisher = {Academic Press},
pages = {63-85},
year = {2024},
isbn = {978-0-443-13395-4},
doi = {https://doi.org/10.1016/B978-0-443-13395-4.00025-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443133954000253},
author = {Dhanalakshmi Menamadathil and Kajari Das and Sushma Dave and Jayashankar Das},
keywords = {Artificial intelligence, machine learning, support vector machine, logistic regression, extreme gradient boosting, convolutional neural network, recurrent neural networks, reverse vaccinology},
abstract = {Artificial intelligence (AI)–assisted vaccine creation has emerged as a significant development among the cutting-edge technologies that will help society in the 21st century. AI and machine learning technologies have brought answers to issues that have arisen as a result of the advent of recurring and emerging infectious diseases and the rise in antibiotic resistance. The rapid discovery of effective vaccines has been crucial in preparation for outbreaks, including epidemics and pandemics. The urgent requirement for precise vaccine creation in a short duration has led the way for researchers to investigate diverse vaccine-development technologies such as computational biology, structure-based antigen design, protein engineering, gene synthesis, and novel manufacturing platforms, With the advent of whole-genome sequencing and big data analytic platforms aided by AI, omics-based vaccine design has emerged, which is also known as reverse vaccinology (RV). RV accomplishes comprehensive immunogenicity profiling employing proteome and structural data. With the advancement of AI and deep learning algorithms, a range of modeling tools for accurate and precise prediction of immune-recognition patterns have been created, which may be utilized to generate novel vaccine candidates. Given that vaccinations are available for a few infectious illnesses, there is an urgent need for the quick development of vaccines for numerous lethal and developing infections, which can give prominence to RV. Within the course of this chapter, a thorough view of AI -employed algorithms and their role in RV is offered, with a particular emphasis on immunoinformatic and AI methods utilized in it.}
}
@article{PURWANTO2019118170,
title = {Using group model building to develop a causal loop mapping of the water-energy-food security nexus in Karawang Regency, Indonesia},
journal = {Journal of Cleaner Production},
volume = {240},
pages = {118170},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.118170},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619330409},
author = {Aries Purwanto and Janez Sušnik and F.X. Suryadi and Charlotte {de Fraiture}},
keywords = {Group model building, Causal loop diagram, Water-energy-food (WEF) security, Nexus modelling},
abstract = {This paper develops a qualitative causal model of a water, energy, and food (WEF) security nexus system to be used in analysing the interlinkages among those and other sectors that influence and are influenced by each other in a local context. Local stakeholder engagement through a group model building (GMB) approach was applied in Karawang Regency, Indonesia, to develop the model with the goals of improving problem understanding, raising consensus among participants, and building acceptance and commitment regarding the subsequent development of a quantitative nexus model. After recognizing the issues regarding water, energy and food sectors in the study area and eliciting opinions about nexus interactions, the next stage was to build a conceptual framework to describe the nexus system and to develop an integrated causal loop diagram (CLD) that describes critical system (inter-)linkages. The developed Karawang WEF security (K-WEFS) model is composed of six sub-models with water, energy and food sectors as endogenous factors. In addition, population, economic and ecosystem services were considered as exogenous drivers of the system. It is expected that all the major internal and external factors and drivers are covered, including possible feedback mechanisms, and key variables will be analysed further in the system. The future achievement of WEF security targets can be based on robust evaluation and planning processes underpinned by thorough understanding of whole system dynamics and the impacts of changes in the linked sectors, even in a qualitative way. In this way, a first step towards breaking silo thinking in regional planning may be attained.}
}
@article{PANULAONTTO2019292,
title = {The AXIOM approach for probabilistic and causal modeling with expert elicited inputs},
journal = {Technological Forecasting and Social Change},
volume = {138},
pages = {292-308},
year = {2019},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0040162518305870},
author = {Juha Panula-Ontto},
keywords = {Systems modeling, Modeling techniques, Decision support, Cross-impact analysis, Belief networks, Expert elicitation},
abstract = {Expert informants can be used as the principal information source in the modeling of socio-techno-economic systems or problems to support planning, foresight and decision-making. Such modeling is theory-driven, grounded in expert judgment and understanding, and can be contrasted with data-driven modeling approaches. Several families of approaches exist to enable expert elicited systems modeling with varying input information requirements and analytical ambitions. This paper proposes a novel modeling language and computational process, which combines aspects from various other approaches in an attempt to create a flexible and practical systems modeling approach based on expert elicitation. It is intended to have high fitness in modeling of systems that lack statistical data and exhibit low quantifiability of important system characteristics. AXIOM is positioned against Bayesian networks, cross-impact analysis, structural analysis, and morphological analysis. The modeling language and computational process are illustrated with a small example model. A software implementation is also presented.}
}
@incollection{HARNAD2005817,
title = {Chapter 36 - A GROUNDED MIND IN A ROBOTIC BODY},
editor = {Henri Cohen and Claire Lefebvre},
booktitle = {Handbook of Categorization in Cognitive Science},
publisher = {Elsevier Science Ltd},
address = {Oxford},
pages = {817-820},
year = {2005},
isbn = {978-0-08-044612-7},
doi = {https://doi.org/10.1016/B978-008044612-7/50091-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780080446127500913},
author = {STEVAN HARNAD},
abstract = {Publisher Summary
This chapter presents the important themes of embodied cognition. In the chapter, Poirier and others first point out that minds (and brains) have bodies, and that this is not only unlikely to be incidental, but also most of the things that minds can do, they do with their bodies. Pure thinking, that is cognition, seems in and of itself to be a disembodied mental activity, conducted autonomously inside our heads without any signs of sensorimotor interaction with the world of objects, organisms, states, events, and properties that most of our thoughts are about. But surely whatever pure thinking does go on in our heads occurs in the service of our present and future doings in the world, and is grounded in our past doings. Both Proulx and Hélie, and Cangelosi are concerned with how to give a cognitive system the sensorimotor capacity, which is the capacity to detect, recognize and do the kinds of things that one is able to do with the kinds of things there are in the world. In other words, it is the capacity to categorize. The shapes that objects project on one's sensory surfaces can be processed by neural networks that do what is called unsupervised learning.}
}
@incollection{VHORA2024709,
title = {Investigating Fluid Flow Dynamics in Triply Periodic Minimal Surfaces (TPMS) Structures Using CFD Simulation},
editor = {Flavio Manenti and Gintaras V. Reklaitis},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {53},
pages = {709-714},
year = {2024},
booktitle = {34th European Symposium on Computer Aided Process Engineering / 15th International Symposium on Process Systems Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-443-28824-1.50119-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780443288241501198},
author = {Kasimhussen Vhora and Tanya Neeraj and Dominique Thévenin and Gábor Janiga and Kai Sundmacher},
keywords = {Computational Fluid Dynamic, TPMS Structure, Pressure Drop, LBM},
abstract = {Efficient absorption processes require optimized packed bed column structures, which affect gas-liquid contact, flow distribution, and pressure drop. An optimal setup ensures efficient mass transfer with high surface area while keeping down the pressure drop, which leads to energy savings and better absorption. TPMS structures such as the Gyroid, Schwarz-P, and Schwarz-D were investigated in this study, with a focus on balancing porosity and surface area to achieve reduced pressure drops and optimal phase contact. Single-phase flow simulations were conducted using the commercial software STAR- CCM+, compared to the lattice Boltzmann method (LBM) to provide an alternative perspective on fluid dynamics. Validation, analysis of the results and identification of possible improvements were achieved through these comparisons. According to the results, the Schwarz-D structure with 70% porosity and 2 mm unit cell leads to the best performance, exhibiting a pressure drop of 655 Pa m-1 and a specific surface area of 1776 m2 m-3 when analysed with STAR-CCM+. The predicted pressure drop was successfully confirmed using LBM simulations, adding robustness to the findings.}
}
@article{BASHIRPOURBONAB2023122642,
title = {In complexity we trust: A systematic literature review of urban quantum technologies},
journal = {Technological Forecasting and Social Change},
volume = {194},
pages = {122642},
year = {2023},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2023.122642},
url = {https://www.sciencedirect.com/science/article/pii/S004016252300327X},
author = {Aysan {Bashirpour Bonab} and Maria Fedele and Vincenzo Formisano and Ihor Rudko},
keywords = {Quantum City, Uncertainty, Duality, Parallelism, Quantum mechanics, Systematic literature review},
abstract = {Today's cities are facing increasingly complex challenges. The growing uncertainty and complexity—caused by the unremitted differentiation of social, environmental, and technological orders—call for novel ways of conceptualizing urban reality. Although technology-oriented solutions shape the most efficient strategies to manage complexity in contemporary cities, ensuring an effective transition toward a Quantum City paradigm can grant considerable advantages for city administrators and managers facing looming urban challenges. In this article, we introduce the Quantum City metaphor—grounded in fundamental notions of quantum mechanics—as a new conceptual lens for investigating urban complexity. We then build upon the metaphor, theorizing a set of assumptions grounded in three fundamental concepts of quantum theory: relativity, uncertainty, and duality/parallelism. Finally, we propose an empirical conceptualization of Quantum Cities based on the concrete adoption of quantum technologies to deal with urban complexity. This is achieved through a systematic literature review of scholarly records on quantum technologies in the context of social sciences, emphasizing related urban problematics and challenges. Principal component analysis and agglomerative hierarchical clustering reveal two types of quantum technologies most useful for city planners and managers: quantum communication and quantum computing. Accordingly, we perform a qualitative thematic synthesis of related scholarly records, emphasizing the negative and positive aspects of both types of urban quantum technologies.}
}
@article{PERIGNAT201931,
title = {STEAM in practice and research: An integrative literature review},
journal = {Thinking Skills and Creativity},
volume = {31},
pages = {31-43},
year = {2019},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2018.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S1871187118302190},
author = {Elaine Perignat and Jen Katz-Buonincontro},
keywords = {STEAM education, Creativity, Arts-integration, Transdisciplinary, Interdisciplinary},
abstract = {This integrative review examines 44 published articles (empirical, descriptive, and pedagogical frameworks) on the topic of STEAM (Science, Technology, Engineering, Arts, Mathematics) education from 2007 to 2018. Despite the emergence of STEAM as a popular pedagogical approach for enhancing students’ creativity, problem-solving skills, and interest in STEM fields, the definitions and purposes of STEAM education remain ubiquitous. Therefore, the review examined descriptions of the overall purpose of STEAM education, definitions of the STEAM acronym and the ‘A’ in STEAM, creativity as a learning outcome, elements of arts education, and arts education learning outcomes. The review found a myriad of definitions of the STEAM concept in general, a variety of interpretations for the “A” in STEAM, and an overall lack of reported learning outcomes in the areas of creativity, problem-solving, and arts education. The articles also differentiate in methods for merging STEAM disciplines, described in one of five ways: transdisciplinary, interdisciplinary, multi-disciplinary, cross-disciplinary, and arts-integration. Recommendations are provided to advance both research and practice in STEAM education.}
}
@article{CUSHEN2011458,
title = {Aha! Voila! Eureka! Bilingualism and insightful problem solving},
journal = {Learning and Individual Differences},
volume = {21},
number = {4},
pages = {458-462},
year = {2011},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2011.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S1041608011000215},
author = {Patrick J. Cushen and Jennifer Wiley},
keywords = {Bilingualism, Creativity, Insight, Problem solving},
abstract = {What makes a person able to solve problems creatively? One interesting factor that may contribute is experience with multiple languages from an early age. Bilingual individuals who acquire two languages by the age of 6 have been shown to demonstrate superior performance on a number of thinking tasks that require flexibility. However, bilingual advantages have yet to be identified particularly on insight problems that are used as a model of creative problem solving following initial impasse. As such, the goal of the present study was to investigate the influence of language experience on problem solving performance on a matched set of insight and non-insight problems. Results demonstrate an interaction between type of problem (insight versus non-insight) and language status.}
}
@article{GERSTENBERG2024924,
title = {Counterfactual simulation in causal cognition},
journal = {Trends in Cognitive Sciences},
volume = {28},
number = {10},
pages = {924-936},
year = {2024},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2024.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S1364661324001074},
author = {Tobias Gerstenberg},
keywords = {counterfactuals, causality, mental simulation, intuitive physics, theory of mind},
abstract = {How do people make causal judgments and assign responsibility? In this review article, I argue that counterfactual simulations are key. To simulate counterfactuals, we need three ingredients: a generative mental model of the world, the ability to perform interventions on that model, and the capacity to simulate the consequences of these interventions. The counterfactual simulation model (CSM) uses these ingredients to capture people’s intuitive understanding of the physical and social world. In the physical domain, the CSM predicts people’s causal judgments about dynamic collision events, complex situations that involve multiple causes, omissions as causes, and causes that sustain physical stability. In the social domain, the CSM predicts responsibility judgments in helping and hindering scenarios.}
}
@article{NORGAARD2023105308,
title = {Linked auditory and motor patterns in the improvisation vocabulary of an artist-level jazz pianist},
journal = {Cognition},
volume = {230},
pages = {105308},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2022.105308},
url = {https://www.sciencedirect.com/science/article/pii/S0010027722002967},
author = {Martin Norgaard and Kevin Bales and Niels Chr. Hansen},
keywords = {Improvisation, Jazz, Music, Audiomotor coupling, Expectation, Entropy},
abstract = {Improvising musicians possess a stored library of musical patterns forming the basis for their improvisations. According to a prominent theoretical framework by Pressing (1988), this library includes linked auditory and motor information. Though examples of libraries of melodic patterns have been shown in extant recordings by some improvising musicians, the underlying motor component has not been experimentally investigated nor related to its auditory counterparts. Here we analyzed a large corpus of ∼100,000 notes from improvisations by one artist-level jazz pianist recorded during 11 live performances with audience. We compared the library identified from these recordings to a control corpus consisting of improvisations by 24 different advanced jazz pianists. In addition to pitch, our recordings included accurate micro-timing and key velocity (i.e., force) data. Following a previously validated procedure, this information was used to identify the underlying motor patterns through correlations between relative timing and velocity between notes in different iterations of the same pitch pattern. A computational model was, furthermore, used to estimate the information content and generated entropy exhibited by recurring pitch patterns with high and low timing and velocity correlations as perceived by a stylistically enculturated expert listener. Though both corpora contained a large number of recurring patterns, the single-player corpus showed stronger evidence that pitch patterns were linked to motor programs in that within-pattern timing and velocity correlations were significantly higher compared to the control corpus. Even when controlling for potentially greater baseline levels of motor self-consistency in the single-player corpus, this effect remained significant for velocity correlations. Amongst recurring 5-tone pitch patterns, those exhibiting more consistent motor schema also used less idiomatic pitch transitions that were both more unexpected and generated more uncertain expectations in enculturated experts than less consistently repeated patterns. Interestingly, we only found partial evidence for fixed pattern boundaries as predicted by the Pressing model and therefore suggest an expanded view in which the beginning and ends of idiomatic audio-motor patterns are not always clear-cut. Our results indicate that the library of melodic patterns may be idiosyncratic to the individual improviser and relies both on motor programming and predictive processing to promote stylistic distinctiveness.}
}
@article{LOU2023541,
title = {Human Creativity in the AIGC Era},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {9},
number = {4},
pages = {541-552},
year = {2023},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2024.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S2405872624000054},
author = {Yongqi Lou},
keywords = {AIGC, Artificial intelligence, Meaning-making, Paradigmatic innovation, Human values},
abstract = {Recent advances in artificial intelligence raise profound questions for humanity. Is the artificial intelligence-generated content (AIGC) technology merely a tool? Or is AIGC developing a level of creativity comparable to that of human beings? This essay explores the challenges and opportunities that AIGC technology brings to creativity, industry, and the ways of living of people around the world. These questions involve scale, authenticity, choice, and wisdom. Further, this essay addresses the core capabilities of future creative workers in the era of AIGC. The author believes that the ability to create meaning—meaning making—is and will remain a distinctive strength of human creativity in the AIGC era. To build on this strength, human beings must focus on six key areas: human-centered values, paradigmatic innovation, holistic experiences, cultural awareness, situational connections, and narrative reasoning. The best outcome for the AIGC is to make machines more machine-like and humans more human. Achieving this goal requires a cultural renaissance. We must break through the limits of computational rationality with the brilliance of humanity.}
}
@article{XIAO1995169,
title = {Three-dimensional melt flows in Czochralski oxide growth: high-resolution, massively parallel, finite element computations},
journal = {Journal of Crystal Growth},
volume = {152},
number = {3},
pages = {169-181},
year = {1995},
issn = {0022-0248},
doi = {https://doi.org/10.1016/0022-0248(95)00090-9},
url = {https://www.sciencedirect.com/science/article/pii/0022024895000909},
author = {Qiang Xiao and Jeffrey J. Derby},
abstract = {Three-dimensional, time-dependent features of melt flows which occur during the Czochralski growth of oxide crystals are analyzed using a theoretical bulk-flow model. The transition from a steady, axisymmetric flow to a time-dependent, three-dimensional state characterized by an annular wave structure is found to strongly affect the temperature distribution and heat transfer through the melt. The results are obtained using a novel, massively parallel implementation of the Galerkin finite element method which affords high spatial resolution of the computed flows.}
}
@incollection{VANLOAN1992247,
title = {Chapter 6 A survey of matrix computations},
series = {Handbooks in Operations Research and Management Science},
publisher = {Elsevier},
volume = {3},
pages = {247-321},
year = {1992},
booktitle = {Computing},
issn = {0927-0507},
doi = {https://doi.org/10.1016/S0927-0507(05)80203-8},
url = {https://www.sciencedirect.com/science/article/pii/S0927050705802038},
author = {Charles {Van Loan}},
abstract = {Publisher Summary
This chapter presents three-level introduction to the field of matrix computations. The chapter discusses analytic and computational tools that underpin numerical linear algebra. Low dimension examples are the rule with appropriate generalizations to follow. The central themes include (a) the language of matrix factorizations, (b) the art of introducing zeros into a matrix, (c) the exploitation of structure, and (d) the distinction between problem sensitivity and algorithmic stability. Matrix factorizations that play a central role in numerical linear algebra are also presented in the chapter. The chapter also discusses factorization. For each factorization, algorithms are surveyed, associated mathematical properties, and applications are discussed. One factorization (Chotesky) is used to illustrate various aspects of high performance matrix computations. Successful computing requires the design of codes that pay careful attention to the flow of data during execution.}
}
@article{KISAALITA201658,
title = {Perspectives on context, design teams and diffusion of technological innovations in low-resource settings: A practical approach based on sub-Saharan African projects},
journal = {Technology in Society},
volume = {46},
pages = {58-62},
year = {2016},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2016.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X16300495},
author = {William S. Kisaalita},
keywords = {Technological innovations, Sustainable development, Developing countries, Design teams, Poverty alleviation, Food and energy security},
abstract = {A human-centered design approach for creating science/engineering-driven solutions or innovations, referred to as “connect-the-dots,” is presented. Dots symbolize the best questions and the connections reveal the best order in which these questions should be answered. In this approach, the number of customer or user behavioral changes are critically analyzed, revealing the overall context in which the solution or innovation will operate; especially to undergraduate students creating solutions to problems from settings that are less familiar, from cultural, economic, and geopolitical viewpoints. Solutions or innovations that result in minimal user behavior changes are preferred. Additional benefits include better incorporation of systems theory thinking, ease with which team multidisciplinarity and diversity can be identified, and seamlessly integrating design and research.}
}
@article{AVEN201633,
title = {On the use of conservatism in risk assessments},
journal = {Reliability Engineering & System Safety},
volume = {146},
pages = {33-38},
year = {2016},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2015.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S0951832015002938},
author = {Terje Aven},
keywords = {Conservatism, Risk assessments, Knowledge},
abstract = {It is common to use conservatism in risk assessments, replacing uncertain quantities with values that lead to a higher level of risk. It is argued that the approach represents a practical method for dealing with uncertainties and lack of knowledge in risk assessment. If the computed probabilities meet the pre-defined criteria with the conservative quantities, there is strong support for the “real risk” to meet these criteria. In this paper we look more closely into this practice, the main aims being to clarify what it actually means and what the implications are, as well as providing some recommendations. The paper concludes that conservatism should be avoided in risk assessments – “best judgements” should be the ruling thinking, to allow for meaningful comparisons of options. By incorporating sensitivity analyses and strength of knowledge judgements for the background knowledge on which the assigned probabilities are based, the robustness of the conclusions can be more adequately assessed.}
}
@incollection{HE2013241,
title = {5.16 - Flood Inundation Dynamics and Socioeconomic Vulnerability under Environmental Change},
editor = {Roger A. Pielke},
booktitle = {Climate Vulnerability},
publisher = {Academic Press},
address = {Oxford},
pages = {241-255},
year = {2013},
isbn = {978-0-12-384704-1},
doi = {https://doi.org/10.1016/B978-0-12-384703-4.00508-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780123847034005086},
author = {Y. He and F. Pappenberger and D. Manful and H. Cloke and P. Bates and F. Wetterhall and B. Parkes},
keywords = {Flood inundation dynamics, Two-faced flood, Model cascade, Uncertainties, Flood vulnerability, Impact studies, Flood risk, Living with floods, Harnessing floods},
abstract = {Floods are a major threat to human existence and historically have both caused the collapse of civilizations and forced the emergence of new cultures. The physical processes of flooding are complex. Increased population, climate variability, change in catchment and channel management, modified landuse and land cover, and natural change of floodplains and river channels all lead to changes in flood dynamics, and as a direct or indirect consequence, social welfare of humans. Section 5.16.1 explores the risks and benefits brought about by floods and reviews the responses of floods and floodplains to climate and landuse change. Section 5.08.2 reviews the existing modeling tools, and the top–down and bottom–up modeling frameworks that are used to assess impacts on future floods. Section 5.08.3 discusses changing flood risk and socioeconomic vulnerability based on current trends in emerging or developing countries and presents an alternative paradigm as a pathway to resilience. Section 5.08.4 concludes the chapter by stating a portfolio of integrated concepts, measures, and avant-garde thinking that would be required to sustainably manage future flood risk.}
}
@article{SIMS1991383,
title = {Computers and experiments in stress analysis: Eds G. M. Carlomagno and C. A. Brebbia Computational Mechanics Publications, Southampton, UK},
journal = {Engineering Structures},
volume = {13},
number = {4},
pages = {383-384},
year = {1991},
issn = {0141-0296},
doi = {https://doi.org/10.1016/0141-0296(91)90027-A},
url = {https://www.sciencedirect.com/science/article/pii/014102969190027A},
author = {P. Sims}
}
@article{SIEGELMANN2013117,
title = {Turing on Super-Turing and adaptivity},
journal = {Progress in Biophysics and Molecular Biology},
volume = {113},
number = {1},
pages = {117-126},
year = {2013},
note = {Can Biology Create a Profoundly New Mathematics and Computation?},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2013.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S0079610713000278},
author = {Hava T. Siegelmann},
keywords = {Adaptive computation, Biological computation, Super-Turing computation},
abstract = {Biological processes are often compared to computation and modeled on the Universal Turing Machine. While many systems or aspects of systems can be well described in this manner, Turing computation can only compute what it has been programmed for. It has no ability to learn or adapt to new situations. Yet, adaptation, choice and learning are all hallmarks of living organisms. This suggests that there must be a different form of computation capable of this sort of calculation. It also suggests that there are current computational models of biological systems that may be fundamentally incorrect. We argue that the Super-Turing model is both capable of modeling adaptive computation, and furthermore, a possible answer to the computational model searched for by Turing himself.}
}
@article{STROBL2024104585,
title = {Counterfactual formulation of patient-specific root causes of disease},
journal = {Journal of Biomedical Informatics},
volume = {150},
pages = {104585},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104585},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000030},
author = {Eric V. Strobl},
keywords = {Root cause analysis, Causal inference, Precision medicine, Causal discovery, Computational medicine},
abstract = {Objective:
Root causes of disease intuitively correspond to root vertices of a causal model that increase the likelihood of a diagnosis. This description of a root cause nevertheless lacks the rigorous mathematical formulation needed for the development of computer algorithms designed to automatically detect root causes from data. We seek a definition of patient-specific root causes of disease that models the intuitive procedure routinely utilized by physicians to uncover root causes in the clinic.
Methods:
We use structural equation models, interventional counterfactuals and the recently developed mathematical formalization of backtracking counterfactuals to propose a counterfactual formulation of patient-specific root causes of disease matching clinical intuition.
Results:
We introduce a definition of patient-specific root causes of disease that climbs to the third rung of Pearl’s Ladder of Causation and matches clinical intuition given factual patient data and a working causal model. We then show how to assign a root causal contribution score to each variable using Shapley values from explainable artificial intelligence.
Conclusion:
The proposed counterfactual formulation of patient-specific root causes of disease accounts for noisy labels, adapts to disease prevalence and admits fast computation without the need for counterfactual simulation.}
}
@article{BARTOLOZZI2011163,
title = {eMorph: Towards Neuromorphic Robotic Vision},
journal = {Procedia Computer Science},
volume = {7},
pages = {163-165},
year = {2011},
note = {Proceedings of the 2nd European Future Technologies Conference and Exhibition 2011 (FET 11)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2011.09.027},
url = {https://www.sciencedirect.com/science/article/pii/S1877050911005874},
author = {Chiara Bartolozzi and Charles Clercq and Neeraj Mandloi and Francesco Rea and Giacomo Indiveri and Daniel Fasnacht and Giorgio Metta and Michael Hofstätter and Ryad Benosman},
keywords = {neuromorphic, humanoid robot, event-driven computation, vision},
abstract = {The eMorph project aims at introducing a new concept for vision in the field of humanoid robotics. The system that is currently being developed is inspired by the biology of mammalian visual systems, introducing concepts such as stimulus-driven signal acquisition and processing, together with space-variant sensor design coupled with active vision. This approach is leading to the realization of a system that goes beyond current thinking in robotic vision.}
}
@article{GU2023120105,
title = {Detection of Attention Deficit Hyperactivity Disorder in children using CEEMDAN-based cross frequency symbolic convergent cross mapping},
journal = {Expert Systems with Applications},
volume = {226},
pages = {120105},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120105},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423006073},
author = {Danlei Gu and Aijing Lin and Guancen Lin},
keywords = {Cross-frequency coupling, Convergent cross mapping, Complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN), Attention Deficit Hyperactivity Disorder (ADHD), Electroencephalogram (EEG), Dispersion},
abstract = {The cross-frequency coupling relationship of EEG signals is of great significance to identify abnormal EEG signals and diagnose diseases. This paper proposes a new algorithm, CEEMDAN (complete ensemble empirical mode decomposition with adaptive noise)-based cross-frequency symbolic convergent cross mapping (CEEMDAN CF-SCCM). In the numerical simulation test, we have confirmed that CEEMDAN CF-SCCM is an effective method to quantify the cross-frequency information transmission of complex system signals from the three dimensions of robustness to noise, coupling sensitivity, and data length sensitivity. It can successfully distinguish the driving factors and response factors in the phase–amplitude interaction and has good robustness to noise. With this approach, we examined differences in cross-frequency phase–amplitude coupling between ADHD patients and normal individuals over classical brain frequency bands (Alpha, Beta, Delta, Gamma, Theta). According to the position of the electrodes, the brain was divided into four regions: front, back, left, and right, and the phase–amplitude coupling between different frequencies in each region was compared. Compared with the normal group, there was more information transmission in the anterior region of Delta waves and Theta waves. The front and left sides of the brain are responsible for thinking, mental and auditory functions. This information helps us gain insight into the brain dynamics of ADHD patients and contributes to the diagnosis of the disease.}
}
@article{KULAKOVA2024103762,
title = {Comparing third-party responsibility with intention attribution: An fMRI investigation of moral judgment},
journal = {Consciousness and Cognition},
volume = {125},
pages = {103762},
year = {2024},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2024.103762},
url = {https://www.sciencedirect.com/science/article/pii/S1053810024001296},
author = {Eugenia Kulakova and Sofia Bonicalzi and Adrian L. Williams and Patrick Haggard},
keywords = {Moral responsibility, Intention, Social cognition, Causality},
abstract = {Neuroimaging studies demonstrate that moral responsibility judgments activate the social cognition network, presumably reflecting mentalising processes. Conceptually, establishing an agent’s intention is a sub-process of responsibility judgment. However, the relationship between both processes on a neural level is poorly understood. To date, neural correlates of responsibility and intention judgments have not been compared directly. The present fMRI study compares neural activation elicited by third-party judgments of responsibility and intention in response to animated pictorial stimuli showing harm events. Our results show that the social cognition network, in particular Angular Gyrus (AG) and right Temporo-Parietal Junction (RTPJ), showed stronger activation during responsibility vs. intention evaluation. No greater activations for the reverse contrast were observed. Our imaging results are consistent with conceptualisations of intention attribution as a sub-process of responsibility judgment. However, they question whether the activation of the social cognition network, particularly AG/RTPJ, during responsibility judgment is limited to intention evaluation.}
}
@article{FRADKIN20231013,
title = {Theory-Driven Analysis of Natural Language Processing Measures of Thought Disorder Using Generative Language Modeling},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {8},
number = {10},
pages = {1013-1023},
year = {2023},
note = {Natural Language Processing in Psychiatry and Clinical Neuroscience Research},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2023.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S2451902223001258},
author = {Isaac Fradkin and Matthew M. Nour and Raymond J. Dolan},
keywords = {Computational psychiatry, GPT-2, Natural language processing, Psychosis, Schizophrenia, Thought disorder},
abstract = {Background
Natural language processing (NLP) holds promise to transform psychiatric research and practice. A pertinent example is the success of NLP in the automatic detection of speech disorganization in formal thought disorder (FTD). However, we lack an understanding of precisely what common NLP metrics measure and how they relate to theoretical accounts of FTD. We propose tackling these questions by using deep generative language models to simulate FTD-like narratives by perturbing computational parameters instantiating theory-based mechanisms of FTD.
Methods
We simulated FTD-like narratives using Generative-Pretrained-Transformer-2 by either increasing word selection stochasticity or limiting the model’s memory span. We then examined the sensitivity of common NLP measures of derailment (semantic distance between consecutive words or sentences) and tangentiality (how quickly meaning drifts away from the topic) in detecting and dissociating the 2 underlying impairments.
Results
Both parameters led to narratives characterized by greater semantic distance between consecutive sentences. Conversely, semantic distance between words was increased by increasing stochasticity, but decreased by limiting memory span. An NLP measure of tangentiality was uniquely predicted by limited memory span. The effects of limited memory span were nonmonotonic in that forgetting the global context resulted in sentences that were semantically closer to their local, intermediate context. Finally, different methods for encoding the meaning of sentences varied dramatically in performance.
Conclusions
This work validates a simulation-based approach as a valuable tool for hypothesis generation and mechanistic analysis of NLP markers in psychiatry. To facilitate dissemination of this approach, we accompany the paper with a hands-on Python tutorial.}
}
@article{PUZANTIAN2021387,
title = {Redesigning a PhD measurement course for a new era in nursing science},
journal = {Journal of Professional Nursing},
volume = {37},
number = {2},
pages = {387-390},
year = {2021},
issn = {8755-7223},
doi = {https://doi.org/10.1016/j.profnurs.2020.04.019},
url = {https://www.sciencedirect.com/science/article/pii/S8755722320300983},
author = {Houry Puzantian and Hala Darwish},
keywords = {Measurement, Quantitative, Nursing research, PhD},
abstract = {Measurement is at the core of the research process. At the PhD level, students need to develop an in-depth understanding of measures relevant to their area of work and refine their knowledge of measurement issues. Traditionally, measurement coursework in Nursing focused on the psychometric evaluation of instruments measuring cognition and behavior. However, in the age of Big Data, precision medicine, and translational science, PhD students need to develop knowledge and skills relevant to these fields and to collaborate with experts from the different disciplines. Therefore, Nursing faculty need to recognize the state-of-the-science of nursing research and tend to a variety of measurement issues across a spectrum of operationalized concepts. Herein we present an overview of learning outcomes, instructional content and methods of delivery for a contemporary PhD-level course on measurement for Nursing Science. We also present our experience in the design, implementation, and evaluation of a novel PhD measurement course.}
}
@article{PARIKH2024138,
title = {A comprehensive study on epigenetic biomarkers in early detection and prognosis of Alzheimer's disease},
journal = {Biomedical Analysis},
volume = {1},
number = {2},
pages = {138-153},
year = {2024},
issn = {2950-435X},
doi = {https://doi.org/10.1016/j.bioana.2024.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S2950435X24000167},
author = {Dhruv Parikh and Manan Shah},
keywords = {Alzheimer’s Disease, Biomarkers, Detection, Epigenetics},
abstract = {Alzheimer's Disease (AD) is a neurodegenerative disorder characterized by beta-amyloid plaques and tau tangles, disrupting brain cell communication, causing atrophy, and leading to cognitive decline. It poses a substantial global health challenge, necessitating urgent research. Molecular biomarkers, reflecting AD progression, have been identified in diverse bodily tissues. Notably, emerging epigenetic biomarkers introduce a novel dimension to AD pathophysiology. However, their precise role in early AD detection and prognosis remains unclear. This review classifies various epigenetic biomarkers, emphasizing their potential in early detection and prognosis. Various epigenetic biomarkers like DNA methylation, non-coding RNAs, histone modification, OMICS, and many more get significantly altered during AD; these biomarkers being distinctly expressed in normal conditions to AD offer a huge therapeutic benefit to stop the progression or worsening it. We explore the therapeutic implications and propose integration with existing diagnostic methods to intervene in AD progression, mitigating exacerbation. Addressing challenges, we envision the future scope of these biomarkers, emphasizing their synergy with computational approaches for enhanced AD detection. This review contributes to the field by proposing a multifaceted approach that combines epigenetic markers with computational analysis to improve early detection and facilitate timely therapeutic interventions. Furthermore, we discuss the economic implications of these biomarkers, proposing that their early application could significantly reduce the financial burden of AD by delaying the progression and severity of the disease.}
}
@article{FELSCHE2023101530,
title = {Evidence for abstract representations in children but not capuchin monkeys},
journal = {Cognitive Psychology},
volume = {140},
pages = {101530},
year = {2023},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2022.101530},
url = {https://www.sciencedirect.com/science/article/pii/S0010028522000664},
author = {Elisa Felsche and Patience Stevens and Christoph J. Völter and Daphna Buchsbaum and Amanda M. Seed},
keywords = {Overhypotheses, Abstraction, Generalization, Animal cognition, Computational modeling, Cognitive development},
abstract = {The use of abstract higher-level knowledge (also called overhypotheses) allows humans to learn quickly from sparse data and make predictions in new situations. Previous research has suggested that humans may be the only species capable of abstract knowledge formation, but this remains controversial. There is also mixed evidence for when this ability emerges over human development. Kemp et al. (2007) proposed a computational model of how overhypotheses could be learned from sparse examples. We provide the first direct test of this model: an ecologically valid paradigm for testing two species, capuchin monkeys (Sapajus spp.) and 4- to 5-year-old human children. We presented participants with sampled evidence from different containers which suggested that all containers held items of uniform type (type condition) or of uniform size (size condition). Subsequently, we presented two new test containers and an example item from each: a small, high-valued item and a large but low-valued item. Participants could then choose from which test container they would like to receive the next sample – the optimal choice was the container that yielded a large item in the size condition or a high-valued item in the type condition. We compared performance to a priori predictions made by models with and without the capacity to learn overhypotheses. Children's choices were consistent with the model predictions and thus suggest an ability for abstract knowledge formation in the preschool years, whereas monkeys performed at chance level.}
}
@incollection{PATEL2023191,
title = {Chapter 6 - Human-intensive techniques},
editor = {Robert A. Greenes and Guilherme {Del Fiol}},
booktitle = {Clinical Decision Support and Beyond (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Oxford},
pages = {191-215},
year = {2023},
isbn = {978-0-323-91200-6},
doi = {https://doi.org/10.1016/B978-0-323-91200-6.00030-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323912006000309},
author = {Vimla L. Patel and Jane Shellum and Timothy Miksch and Edward H. Shortliffe},
keywords = {Knowledge acquisition, Expert decision making, Learning and expertise, Human problem solving, Institutional standard setting, Interactive transfer of expertise},
abstract = {This chapter focuses on human expertise as a source of knowledge, emphasizing human-intensive techniques for acquiring knowledge from a variety of knowledge sources. Much of such work deals with the acquisition of knowledge for the purpose of encoding it to be used in decision support systems. There are also approaches that deal with the role of experts in creating evidence-based guidelines and institutional protocols. Those who want to capture expert knowledge naturally seek interaction with human beings who are excellent at the same task for which the knowledge product is intended. As a result, we need to understand both the factual knowledge that is required to solve the relevant problems and the judgmental knowledge that characterizes an excellent decision maker. The chapter accordingly focuses on computational methods and cognitive perspectives that can assist with the elicitation of knowledge by interacting with expert human beings.}
}
@article{OMIDI20231,
title = {Molecular dynamic study of perovskite with improved thermal and mechanical stability for solar cells application: Calculation the final strength of the modeled atomic structures and the Young's modulus},
journal = {Engineering Analysis with Boundary Elements},
volume = {156},
pages = {1-7},
year = {2023},
issn = {0955-7997},
doi = {https://doi.org/10.1016/j.enganabound.2023.07.037},
url = {https://www.sciencedirect.com/science/article/pii/S0955799723003922},
author = {Mohammad Omidi and Zahra Karimi and Shirin Rahmani and Ali {Naderi Bakhtiyari} and Mahmood {Karimi Abdolmaleki}},
keywords = {Molecular dynamic, LAMMPS, Mechanical properties, Stress-strain, Solar cell},
abstract = {The Large-scale Atomic/Molecular Massively Parallel Simulator (LAMMPS) software is used to do molecular dynamics simulations, which entail modeling atom behavior over time using interatomic potentials. This approach is used to calculate perovskite structures' mechanical characteristics. For testing purposes, stress-strain curves are completed in the X, Y, and Z directions to represent the material's reaction to applied stress in terms of strain. The simulated structures are deformed inside the computational experiments using the loads and deform approaches command to get the stress-strain curves. The mechanical data of the structures may be retrieved by producing a deformation. These stress-strain curves are then compared in three axes of X, Y, and Z for XSnO3 (X= Cs, Rb, and K) at varied temperature and pressure settings. Finally, we applied this material to solar cell devices to find the performance of perovskite materials and calculated the efficiency.}
}
@article{MEARA2000345,
title = {Vocabulary and neural networks in the computational assessment of texts written by second-language learners},
journal = {System},
volume = {28},
number = {3},
pages = {345-354},
year = {2000},
issn = {0346-251X},
doi = {https://doi.org/10.1016/S0346-251X(00)00016-6},
url = {https://www.sciencedirect.com/science/article/pii/S0346251X00000166},
author = {Paul Meara and Catherine Rodgers and Gabriel Jacobs},
keywords = {Neural network, Computational assessment, Vocabulary, French},
abstract = {This paper explores the potential of a neural network in language assessment. Many examination systems rely on subjective judgments made by examiners as a way of grading the writing of non-native speakers. Some research (e.g. Engber, 1995. The relationship of lexical proficiency to the quality of ESL compositions. Journal of Second Language Writing 4(2), 139–155) has shown that these subjective judgements are influenced to a very large extent by the lexical choices made by candidates. We took Engber's basic model, but automated the evaluation of lexical content. A group of non-native speakers of French were asked to produce a short text in response to a picture stimulus. The texts were graded by French native speaker teachers. We identified a number of words which occurred in about half the texts, and coded each text for the occurrence and non-occurrence of each word. We then trained a neural network to grade the texts on the basis of these codings. The results suggest that it might be possible to teach a neural network to mimic the judgements made by human markers.}
}
@article{KATONA2023115228,
title = {Accuracy of the robust design analysis for the flux barrier modelling of an interior permanent magnet synchronous motor},
journal = {Journal of Computational and Applied Mathematics},
volume = {429},
pages = {115228},
year = {2023},
issn = {0377-0427},
doi = {https://doi.org/10.1016/j.cam.2023.115228},
url = {https://www.sciencedirect.com/science/article/pii/S0377042723001723},
author = {Mihály Katona and Miklós Kuczmann and Tamás Orosz},
keywords = {Electrical machines, Optimisation, Finite element method, Robust design analysis, Design of Experiment methods},
abstract = {Mass-produced electrical machines are subjected to manufacturing uncertainties in terms of geometry. A robust design is inevitable to ensure the consistent performance of the electric motor. Some parts of the rotor geometry are often simplified, like the flux barrier at the end of the magnets. This paper presents a design optimisation regarding the torque ripple and the average torque. The aim is to assess the effects of the flux barrier on the main properties of a permanent magnet synchronous motor. Also, robust design analysis is presented on the flux barrier. The computational burden of the robust design analysis is immense, even if uniform uncertainties are assumed. In this case, different Design of Experiment (DoE) methods reduce the number of simulations. The efficiency of the DoE methods is compared in terms of simulation number and extreme value approximation. We found that the Central Composite method is the most accurate, while the Plackett–Burman is the most efficient in this particular case.}
}
@article{BARFAR2019173,
title = {Cognitive and affective responses to political disinformation in Facebook},
journal = {Computers in Human Behavior},
volume = {101},
pages = {173-179},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2019.07.026},
url = {https://www.sciencedirect.com/science/article/pii/S0747563219302699},
author = {Arash Barfar},
keywords = {Political disinformation, Polarization, Echo chamber, Text analysis, Social media, Facebook},
abstract = {The epidemic of political disinformation in social media has in part triggered the transition to the post-truth era in which emotional and ideological appeals are more influential in shaping public opinion than objective facts. In this study we examined the cognitive and affective responses that political disinformation prompted in Facebook, as the most popular social media platform. Through text analysis of user comments corpora on nearly 2,100 political posts from popular sources in Facebook, we found that compared to true news, political disinformation received significantly less analytic responses from Facebook followers. While the results indicated greater anxiety in responses to true news, responses to political disinformation were filled with greater anger and incivility. We also found similar (low) levels of cognitive thinking in responses to extreme conservative and extreme liberal disinformation. Contrary to prior research findings, our results indicated that responses to extreme liberal disinformation in Facebook were filled with greater anger and incivility. This suggests that the incivility and outrage in online political discourses should not be attributed to a specific political party without considering the concurrent political events.}
}
@article{HAMDI2019772,
title = {Fuzzy Approach for Locating Sensors in Industrial Internet of Things},
journal = {Procedia Computer Science},
volume = {160},
pages = {772-777},
year = {2019},
note = {The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919317120},
author = {Sarah El Hamdi and Mustapha Oudani and Abdellah Abouabdellah and Anass Sebbar},
keywords = {I2oT, Architecture, Fuzzy Theory},
abstract = {Nowadays, in this era of a data driven thinking and reflection, data mining and data analysis are keys to any business survival in a competitive conjectural market. The internet of things is an emerging technology that manages to create a path for the new generation of industrial production system. This advanced technology is requirement to the proliferation of Smart factories, it represents the best tool to help this new concept of plants to organize themselves and optimize the available resources and their consumption. The purpose of this paper is two-pronged; a proposal for an architectural framework of the industrial internet of things, and a mathematical formulation based on fuzzy logic to determine the ideal location of sensors at the shop floor taking into consideration several restrictions.}
}
@article{2024100673,
title = {Erratum regarding missing Declaration of Competing Interest statements in previously published articles},
journal = {International Journal of Child-Computer Interaction},
volume = {41},
pages = {100673},
year = {2024},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100673},
url = {https://www.sciencedirect.com/science/article/pii/S2212868924000412}
}
@article{JIANG2023104680,
title = {Using sequence mining to study students’ calculator use, problem solving, and mathematics achievement in the National Assessment of Educational Progress (NAEP)},
journal = {Computers & Education},
volume = {193},
pages = {104680},
year = {2023},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2022.104680},
url = {https://www.sciencedirect.com/science/article/pii/S0360131522002512},
author = {Yang Jiang and Gabrielle A. Cayton-Hodges and Leslie {Nabors Oláh} and Ilona Minchuk},
keywords = {Calculator, Assessment, Problem solving, Sequence mining, Process data, Mathematics},
abstract = {Using appropriate tools strategically to aid in problem solving is a crucial skill identified in K-12 mathematics curriculum standards. As more assessments transition from paper-and-pencil to digital formats, a variety of interactive tools have been made available to test takers in digital testing platforms. Using onscreen calculators as an example, this study illustrates how process data obtained from student interactions with a digitally-based large-scale assessment can be leveraged to explore how and how well test takers use interactive tools and unveil their mathematical problem-solving processes and strategies. Specifically, sequence mining techniques using the longest common subsequence were applied on process data collected from a nationally representative sample who took the National Assessment of Educational Progress (NAEP) mathematics assessment to examine patterns of eighth-grade students’ calculator-use behaviors and the content of calculator input across a series of items. Sequences of keystrokes executed on the onscreen calculator by test takers were compared to reference sequences identified by content experts as proficient and efficient use to infer how well and how consistently the calculator was used. Results indicated that calculator-use behaviors and content differed by item characteristics. Students were more likely to use calculators on calculation-demanding items that involve intensive and complex computations than on items that involve simple or no computation. Using the calculator on more calculation-demanding items and using it in a manner that is more efficient and more similar to reference sequences on these items were related to higher mathematical proficiency. Findings have implications for assessment design and can be used in educational practices to provide educators with actionable process-related information on tool use and problem solving.}
}
@article{AMALINA2023e19539,
title = {Cognitive and socioeconomic factors that influence the mathematical problem-solving skills of students},
journal = {Heliyon},
volume = {9},
number = {9},
pages = {e19539},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e19539},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023067476},
author = {Ijtihadi Kamilia Amalina and Tibor Vidákovich},
keywords = {External factor, Internal factor, Mathematics thinking skills, Middle-school students, Structural equation modeling},
abstract = {Mathematical problem-solving is necessary to encounter professional, 21st-century, and everyday challenges. The relevant context of mathematical problem-solving is related to science, which is presented using natural language. Mathematical problem-solving requires both mathematical skills and nonmathematical skills, e.g., science knowledge and text comprehension skills. Thus, several internal and external factors affect success in mathematical problem-solving. In this study, we investigated the cognitive (i.e., mathematics domain-specific prior knowledge (DSPK), science background knowledge, and text comprehension skills) and socioeconomic status (SES) (i.e., parents' educational level and family income) factors that affect students' mathematical problem-solving skills. The data considered in this study included tests, documents, and a questionnaire from grade seven to nine students (n = 1067). In addition, a theoretical model was constructed using structural equation modeling. We found that this model was close to satisfying the critical values of fit indices. The model was then modified by deleting the nonsignificant paths, and the modified model exhibited a better fit. We found that most of the exploratory variables directly affected mathematical problem-solving skills, with the exception of the parents' educational levels. The strongest factor was mathematics DSPK. Both the father’s and mother’s educational levels indirectly influenced mathematical problem-solving skills through family income. In addition, text comprehension skills indirectly impacted mathematical problem-solving skills with science background knowledge acting as a mediator.}
}
@article{LOPEZ2023104398,
title = {Facets of social problem-solving as moderators of the real-time relation between social rejection and negative affect in an at-risk sample},
journal = {Behaviour Research and Therapy},
volume = {169},
pages = {104398},
year = {2023},
issn = {0005-7967},
doi = {https://doi.org/10.1016/j.brat.2023.104398},
url = {https://www.sciencedirect.com/science/article/pii/S0005796723001468},
author = {Roberto López and Christianne Esposito-Smythers and Annamarie B. Defayette and Katherine M. Harris and Lauren F. Seibel and Emma D. Whitmyre},
keywords = {Social problem-solving, Social rejection, Negative affect},
abstract = {Social rejection predicts negative affect, and theoretical work suggests that problem-solving deficits strengthen this relation in real-time. Nevertheless, few studies have explicitly tested this relation, particularly in samples at risk for suicide. This may be particularly important as social rejection and negative affect are significant predictors of suicide. The aim of the current study was to examine whether cognitive (i.e., perceiving problems as threats) and behavioral (i.e., avoidance) facets of problem-solving deficits moderated the real-time relation between social rejection and negative affect. The sample consisted of 49 young adults with past-month suicidal ideation. Demographic information, social problem-solving deficits, as well as depressive/anxiety symptoms and stress levels were assessed at baseline. Social rejection and negative affect were assessed using ecological momentary assessment over the following 28 days. Dynamic structural equation modeling was used to assess relations among study variables. After accounting for depressive/anxiety symptoms, stress levels, sex, and age, only avoidance of problems bolstered the real-time positive relation between social rejection severity and negative affect (b = 0.04, 95% credibility interval [0.003, 0.072]). Individuals with suicidal ideation who possess an avoidant problem-solving style may be particularly likely to experience heightened negative affect following social rejection and may benefit from instruction in problem-solving skills.}
}
@article{DELIDDO2021102537,
title = {Let's replay the political debate: Hypervideo technology for visual sensemaking of televised election debates},
journal = {International Journal of Human-Computer Studies},
volume = {145},
pages = {102537},
year = {2021},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2020.102537},
url = {https://www.sciencedirect.com/science/article/pii/S1071581920301397},
author = {Anna {De Liddo} and Nieves Pedreira Souto and Brian Plüss},
keywords = {Sensemaking, Public deliberation, Political election debates, Hypervideo, Advanced visual interfaces, Interactive visualisations, Deliberation within},
abstract = {Despite the widespread proliferation of social media in policy and politics, televised election debates are still a prominent form of large-scale public engagement between politicians and the electorate during election campaigns. Advanced visual interfaces can improve these important spaces of democratic engagement. In this paper, we present a user study in which a new hypervideo technology was compared with a publicly available interface for television replay. The results show that hypervideo navigation, coupled with interactive visualisations, improved sensemaking of televised political debates and promoted people's attitude to challenging personal assumptions. This finding suggests that hypervideo interfaces can play a substantial role in supporting citizens in the complex sensemaking process of informing their political choices during an election campaign, and can be used as instruments to promote critical thinking and political opinion shifting.}
}
@article{AMIROUCHE1991293,
title = {Gain in computational efficiency by vectorization in the dynamic simulation of multi-body systems},
journal = {Computers & Structures},
volume = {41},
number = {2},
pages = {293-302},
year = {1991},
issn = {0045-7949},
doi = {https://doi.org/10.1016/0045-7949(91)90432-L},
url = {https://www.sciencedirect.com/science/article/pii/004579499190432L},
author = {F.M.L. Amirouche and N.H. Shareef},
abstract = {This paper presents a new technique developed for increasing the computational efficiency of the dynamic simulation of multi-body systems, providing the computer code with the speed of execution, which is an order of magnitude ahead of the procedure outlined in S. K. Ider and F. M. L. Amirouche [J. appl. Mech.56, (2) (1989)]. This technique is useful with the finite element based algorithm for the solution of dynamical equations of motion for the constrained and unconstrained systems with flexible/rigid interconnected bodies. The implementation of the technique has totally eliminated the costly multiplications of large Boolean matrices, where intensive cpu utilization was required. The overall expensive computer time has been drastically reduced, particularly for the three-dimensional systems involving large degrees of freedom, as a result of their intricate geometry. The algorithmic procedure has been presented in a matrix form and is based on the recursive formulation using Kane's equation, strain energy, mode synthesis, finite element approach, a stable and efficient method for reducing the number of equations subsequent to the constraints resulting from closed loops and/or prescribed motions. Further enhancement in the speed of execution has been achieved by subjecting the developed code to vectorization on the vector-processing machine. A study of simple robot with flexible links has been presented comparing the execution times on the scalar machine (IBM-3081) and the vector-processor (IBM-3090) with and without vector options. Performance figures has been plotted demonstrating the large gains achieved by the technique developed.}
}
@incollection{TAYLOR1995227,
title = {Chapter 13 - Computational needs for process tomography},
editor = {R.A. Williams and M.S. Beck},
booktitle = {Process Tomography},
publisher = {Butterworth-Heinemann},
address = {Oxford},
pages = {227-249},
year = {1995},
isbn = {978-0-08-093801-1},
doi = {https://doi.org/10.1016/B978-0-08-093801-1.50017-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780080938011500174},
author = {R.W. Taylor}
}
@incollection{BURATTINI20021315,
title = {37 - Hybrid Expert Systems: An Approach to Combining Neural Computation and Rule-Based Reasoning},
editor = {Cornelius T. Leondes},
booktitle = {Expert Systems},
publisher = {Academic Press},
address = {Burlington},
pages = {1315-1354},
year = {2002},
isbn = {978-0-12-443880-4},
doi = {https://doi.org/10.1016/B978-012443880-4/50081-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780124438804500818},
author = {Ernesto Burattini and Massimo {De Gregorio} and Guglielmo Tamburrini},
abstract = {Publisher Summary
This chapter examines an approach that integrates neural computation and rule-based reasoning, or the hybrid systems. This integration is actively applied in artificial intelligence and cognitive sciences, such as linguistic theory, natural language processing, and expert systems. The opportunity of employing neural techniques in expert systems is often suggested on the ground that the learning, generalization, fault, and noise tolerance capacities of neural networks can alleviate well-known shortcomings of symbolic problem solvers, such as brittleness in front of incomplete or noisy data, no increase in performance with experience, and time-consuming knowledge acquisition. This chapter explores neurosymbolic integration for rule-based expert systems in connection with automatic data acquisition, rule processing, and explanation. At the periphery of expert systems, sensory processing by neural nets is coupled to rule-based reasoning in order to perform a data acquisition task involving the deployment of expert knowledge and heuristic problem solving. The reaction times of rule-based systems are dramatically reduced by the use of a neurally inspired, parallel inference engine. Informative user interactions with expert systems are achieved by coupling symbolic and neurally supported, pictorial explanation. The relative significance of these aspects of neurosymbolic integration is enhanced by pointing to limitations of neural techniques for automatic knowledge acquisition and robust problem solving in expert systems. These uses of neural nets may often jeopardize an expert system's reliability and reduce its transparency to the user.}
}
@article{KORMAN201530,
title = {The social life of cognition},
journal = {Cognition},
volume = {135},
pages = {30-35},
year = {2015},
note = {The Changing Face of Cognition},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2014.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S001002771400225X},
author = {Joanna Korman and John Voiklis and Bertram F. Malle},
keywords = {History, Social psychology, Theory of mind, Communication, Robotics, Social cognition, Computation},
abstract = {We begin by illustrating that long before the cognitive revolution, social psychology focused on topics pertaining to what is now known as social cognition: people’s subjective interpretations of social situations and the concepts and cognitive processes underlying these interpretations. We then examine two questions: whether social cognition entails characteristic concepts and cognitive processes, and how social processes might themselves shape and constrain cognition. We suggest that social cognition relies heavily on generic cognition but also on unique concepts (e.g., agent, intentionality) and unique processes (e.g., projection, imitation, joint attention). We further suggest that social processes play a prominent role in the development and unfolding of several generic cognitive processes, including learning, attention, and memory. Finally, we comment on the prospects of a recently developing approach to the study of social cognition (social neuroscience) and two potential future directions (computational social cognition and social–cognitive robotics).}
}
@article{SELVERSTON1988109,
title = {A consideration of invertebrate central pattern generators as computational data bases},
journal = {Neural Networks},
volume = {1},
number = {2},
pages = {109-117},
year = {1988},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(88)90013-5},
url = {https://www.sciencedirect.com/science/article/pii/0893608088900135},
author = {Allen I Selverston},
abstract = {The essential features of real neural networks are discussed with respect to their usefulness for connectionist modeling. These features are broken down into cellular and synaptic properties and related to a form of neural circuit known as central pattern generators. The gastric and pyloric rhythm of the lobster stomatogastric system are presented as possible computational data bases for modeling studies.}
}
@article{RITCHIE2012649,
title = {Styles for philosophers of science},
journal = {Studies in History and Philosophy of Science Part A},
volume = {43},
number = {4},
pages = {649-656},
year = {2012},
note = {Part Special Issue: Styles of Thinking},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2012.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0039368112000490},
author = {Jack Ritchie},
keywords = {Ian Hacking, Styles of Thinking, Realism, Self-authentication},
abstract = {In this paper I discuss the bearing of Hacking’s ideas about Scientific Styles on traditional debates in the philosophy of science concerning rationality and realism. I argue that a kind of deflationary position with regard to realism debates is a natural consequence of Hacking’s claim that styles are self-authenticating. I then go on to argue, using an example of van Fraassen’s, that Hacking should allow a methodological role for realism debates and hence they are not idle, as he has claimed, although their resolution may not be important.}
}
@incollection{KRAWCZYK2018101,
title = {Chapter 5 - Reasoning Origins: Human Development During Childhood},
editor = {Daniel C. Krawczyk},
booktitle = {Reasoning},
publisher = {Academic Press},
pages = {101-129},
year = {2018},
isbn = {978-0-12-809285-9},
doi = {https://doi.org/10.1016/B978-0-12-809285-9.00005-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128092859000053},
author = {Daniel C. Krawczyk},
keywords = {Analogies, Causal reasoning, Decision making, Development, Developmental stages, Moral reasoning, Relational reasoning},
abstract = {The developmental process is remarkably dynamic. The process is both a biological one and an environmental one with both factors frequently contributing to the output of increasingly sophisticated and abstract reasoning behavior. Children begin with a process of cortical thickening as large numbers of synaptic connections are formed. From age three onward, the cortex undergoes a tuning process as some synaptic connections strengthen and others weaken. The net result of this process is a decrease in cortical volume from age 5 through 20. Children's thinking is guided by a variety of factors. The context of a problem becomes a significant factor in determining how children will reason and developmental reasoning studies require sensitivity toward making the experimental stimuli understandable and interesting to the child. Children exhibit some competencies in causal reasoning and learning from a very young age. Children show increasing reasoning abilities as they develop. Skills such as relational and analogical reasoning grow during the elementary school years and are supported by increases in cognitive control and decreases in impulsivity. The child becomes less concrete in how he or she views and interacts with the world. This increasing abstraction ability encompasses semantic knowledge, deduction, and moral thinking.}
}
@article{RASS202385,
title = {Adaptive dynamical systems modelling of transformational organizational change with focus on organizational culture and organizational learning},
journal = {Cognitive Systems Research},
volume = {79},
pages = {85-108},
year = {2023},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723000049},
author = {Lars Rass and Jan Treur and Wioleta Kucharska and Anna Wiewiora},
keywords = {Transformational Change, Organizational Culture, Organizational Learning, Safety Culture},
abstract = {Transformative Organizational Change becomes more and more significant both practically and academically, especially in the context of organizational culture and learning. However computational modeling and a formalization of organizational change and learning processes are still largely unexplored. This paper aims to provide an adaptive network model of transformative organizational change and translate a selection of organizational learning and change processes into computationally modelled processes. Additionally, it sets out to connect the dynamic systems view of organizations to self-modelling network models. The creation of the model and the implemented mechanisms of organizational processes are based on extrapolations of an extensive literature study and grounded in related work in this field, and then applied to a specified hospital-related case scenario in the context of safety culture. The model was evaluated by running several simulations and variations thereof. The results of these were investigated by qualitative analysis and comparison to expected emergent behaviour based on related available academic literature. The simulations performed confirmed the occurrence of an organizational transformational change towards a constant learning culture by offering repeated and effective learning and changes to organizational processes. Observations about various interplays and effects of the mechanism have been made, and they exposed that acceptance of mistakes as a part of learning culture facilitates transformational change and may foster sustainable change in the long run. Further, the model confirmed that the self-modelling network model approach applies to a dynamic systems view of organizations and a systems perspective of organizational change. The created model offers the basis for the further creation of self-modelling network models within the field of transformative organizational change and the translated mechanisms of this model can further be extracted and reused in a forthcoming academic exploration of this field.}
}
@article{SHEKHAR2024820,
title = {Topological data analysis enhanced prediction of hydrogen storage in metal–organic frameworks (MOFs)††Electronic supplementary information (ESI) available: Figure showing the effect of training set size. See DOI: https://doi.org/10.1039/d3ma00591g},
journal = {Materials Advances},
volume = {5},
number = {2},
pages = {820-830},
year = {2024},
issn = {2633-5409},
doi = {https://doi.org/10.1039/d3ma00591g},
url = {https://www.sciencedirect.com/science/article/pii/S2633540924000550},
author = {Shivanshu Shekhar and Chandra Chowdhury},
abstract = {Metal–organic frameworks (MOFs) have the capacity to serve as gas capturing, sensing, and storing systems. It is usual practice to select the MOF from a vast database with the best adsorption property in order to do an adsorption calculation. The costs of computing thermodynamic values are sometimes a limiting factor in high-throughput computational research, inhibiting the development of MOFs for separations and storage applications. In recent years, machine learning has emerged as a promising substitute for traditional methods like experiments and simulations when trying to foretell material properties. The most difficult part of this process is choosing characteristics that produce interpretable representations of materials that may be used for a variety of prediction tasks. We investigate a feature-based representation of materials using tools from topological data analysis. In order to describe the geometry of MOFs with greater accuracy, we use persistent homology. We show our method by forecasting the hydrogen storage capacity of MOFs during a temperature and pressure swing from 100 bar/77 K to 5 bar/160 K, using the synthetically compiled CoRE MOF-2019 database of 4029 MOFs. Our topological descriptor is used in conjunction with more conventional structural features, and their usefulness to prediction tasks is explored. In addition to demonstrating significant progress over the baseline, our findings draw attention to the fact that topological features capture information that is supplementary to the structural features.}
}
@article{CRAGG1974315,
title = {Thinking about the future: A critique of the limits to growth: Edited by H. S. D. Cole, Christopher Freeman, Marie Jahoda & K. L. R. Pavitt. Chatto & Windus for Sussex University Press, London: 218 pp., £3.00, 1973},
journal = {Biological Conservation},
volume = {6},
number = {4},
pages = {315-316},
year = {1974},
issn = {0006-3207},
doi = {https://doi.org/10.1016/0006-3207(74)90014-7},
url = {https://www.sciencedirect.com/science/article/pii/0006320774900147},
author = {J.B. Cragg}
}
@article{CARVALHAES2021102165,
title = {An overview & synthesis of disaster resilience indices from a complexity perspective},
journal = {International Journal of Disaster Risk Reduction},
volume = {57},
pages = {102165},
year = {2021},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2021.102165},
url = {https://www.sciencedirect.com/science/article/pii/S221242092100131X},
author = {Thomaz M. Carvalhaes and Mikhail V. Chester and Agami T. Reddy and Braden R. Allenby},
keywords = {Complex adaptive systems, Resilience, Indicators, Disaster index, Urban systems, Socio-ecological systems},
abstract = {Identifying Disaster resilience indices (DRI) for cities and communities remains a common approach for assessing their structural ability and inherent capacity to cope with, recover from, and adapt to disasters. Particularly popular are composite DRI methodologies that are quantitative, top-down, and geographically mappable. DRI have become more comprehensive as the complexity of urban systems is increasingly acknowledged. However, DRI remain criticized as static, reductive, and inadequate when viewed under a complexity paradigm, which views urban systems as Complex Adaptive Systems (CAS), where observed properties (like resilience) emerge from many interactions among heterogenous agents in a network. Literature reviews have covered the state and trends for DRI development. Our objective is to synthesize literature at the nexus of these reviews, CAS, and Socio-ecological Systems (SES) to determine the extent to which commonly adopted indicators relate to widely accepted tenets of CAS. Findings show that DRI indicators usually relate more closely to temporal snapshots of vulnerability, and alternative framings of current indicators along with interdisciplinary approaches could better capture CAS aspects of urban resilience. Research and development should strive to develop DRI based on underlying principles of CAS and SES, and consider adapting top-down quantitative approaches with thick data, network models, and mixed-method triangulations. Explicitly associating complexity theory with DRI can (i) help researchers in socio-technical and socio-ecological domains develop improved resilience indicators and assessment methods that are clearly differentiated from vulnerability metrics, and (ii) guide policy and decision-makers, amid future uncertainty, to better identify, implement and track capacity-enhancing measures.}
}
@article{ZHANG20211358,
title = {Deep learning-based evaluation of factor of safety with confidence interval for tunnel deformation in spatially variable soil},
journal = {Journal of Rock Mechanics and Geotechnical Engineering},
volume = {13},
number = {6},
pages = {1358-1367},
year = {2021},
issn = {1674-7755},
doi = {https://doi.org/10.1016/j.jrmge.2021.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1674775521001268},
author = {Jinzhang Zhang and Kok Kwang Phoon and Dongming Zhang and Hongwei Huang and Chong Tang},
keywords = {Deep learning, Convolutional neural network (CNN), Tunnel safety, Confidence interval, Random field},
abstract = {The random finite difference method (RFDM) is a popular approach to quantitatively evaluate the influence of inherent spatial variability of soil on the deformation of embedded tunnels. However, the high computational cost is an ongoing challenge for its application in complex scenarios. To address this limitation, a deep learning-based method for efficient prediction of tunnel deformation in spatially variable soil is proposed. The proposed method uses one-dimensional convolutional neural network (CNN) to identify the pattern between random field input and factor of safety of tunnel deformation output. The mean squared error and correlation coefficient of the CNN model applied to the newly untrained dataset was less than 0.02 and larger than 0.96, respectively. It means that the trained CNN model can replace RFDM analysis for Monte Carlo simulations with a small but sufficient number of random field samples (about 40 samples for each case in this study). It is well known that the machine learning or deep learning model has a common limitation that the confidence of predicted result is unknown and only a deterministic outcome is given. This calls for an approach to gauge the model's confidence interval. It is achieved by applying dropout to all layers of the original model to retrain the model and using the dropout technique when performing inference. The excellent agreement between the CNN model prediction and the RFDM calculated results demonstrated that the proposed deep learning-based method has potential for tunnel performance analysis in spatially variable soils.}
}
@article{FISCHLER1987257,
title = {Parallel guessing: A strategy for high-speed computation},
journal = {Pattern Recognition},
volume = {20},
number = {2},
pages = {257-263},
year = {1987},
issn = {0031-3203},
doi = {https://doi.org/10.1016/0031-3203(87)90059-8},
url = {https://www.sciencedirect.com/science/article/pii/0031320387900598},
author = {M.A. Fischler and O. Firschein},
keywords = {Parallel processing, Image analysis algorithms, Image processing, Architectures},
abstract = {Conventional approaches to speeding up image understanding computation involving conventional serial algorithms attempt to decompose these algorithms into portions that can be computed in parallel. Because many classes of algorithms do not readily decompose, one seeks some other basis for parallelism. In this paper we argue that “parallel guessing” for image analysis is a useful approach, and that several recent scene analysis algorithms are based on this concept. Problems suitable for this approach have the characteristic that either “distance” from a true solution, or the correctness of a guess, can be readily checked. We review image analysis algorithms that have a parallel guessing or randomness flavor.}
}
@article{GERSHMAN2023104825,
title = {The molecular memory code and synaptic plasticity: A synthesis},
journal = {Biosystems},
volume = {224},
pages = {104825},
year = {2023},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2022.104825},
url = {https://www.sciencedirect.com/science/article/pii/S0303264722002064},
author = {Samuel J. Gershman},
keywords = {Memory, Free energy, Synaptic plasticity, Learning, Inference},
abstract = {The most widely accepted view of memory in the brain holds that synapses are the storage sites of memory, and that memories are formed through associative modification of synapses. This view has been challenged on conceptual and empirical grounds. As an alternative, it has been proposed that molecules within the cell body are the storage sites of memory, and that memories are formed through biochemical operations on these molecules. This paper proposes a synthesis of these two views, grounded in a computational model of memory. Synapses are conceived as storage sites for the parameters of an approximate posterior probability distribution over latent causes. Intracellular molecules are conceived as storage sites for the parameters of a generative model. The model stipulates how these two components work together as part of an integrated algorithm for learning and inference.}
}
@incollection{LOEWER20012166,
title = {Cognitive Science: Philosophical Aspects},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {2166-2171},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/01026-3},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767010263},
author = {B. Loewer},
abstract = {Three questions have dominated the philosophy of mind in the analytic tradition since Descartes. They are: what are thoughts and thinking? How can the mind represent the world? What is consciousness? Most contemporary analytic philosophers attempt to answer these questions within a broadly materialistic framework since they think that there is overwhelming reason to believe that human beings are biological organisms entirely composed of ordinary matter. Recently the central questions in the philosophy of mind have been given some new twists and partial answers by developments within cognitive science. This article reviews some of the main ideas in cognitive science and its impact on these issues in the philosophy of mind.}
}
@article{WANG2024109848,
title = {An effective DOA estimation method for low SIR in small-size hydrophone array},
journal = {Applied Acoustics},
volume = {217},
pages = {109848},
year = {2024},
issn = {0003-682X},
doi = {https://doi.org/10.1016/j.apacoust.2023.109848},
url = {https://www.sciencedirect.com/science/article/pii/S0003682X23006461},
author = {Wenbo Wang and Ye Li and TongSheng Shen and Feng Liu and DeXin Zhao},
abstract = {The estimation ability of traditional direction of arrival (DOA) estimation methods is relatively fragile in small-size hydrophone arrays with limited space. Especially in low signal to interference ratio (SIR), the strong interference signals may submerge some weak signals of interest (SOI) and make DOA estimation difficult in response to this issue. This paper introduces an improved sparse DOA estimation method for practical multi-objective DOA estimation in complex scenarios. The main work is to introduce a noise weight constraint in the sparse iterative covariance process. It leads the algorithm to output sparse peaks and smooth spatial energy spectra and achieve faster fitting while reducing the probability of false peaks. The algorithm can complete DOA estimation of the multi-target reliably without prior information of sources. Then, we propose a fast region grid refinement method based on allocation reconstruction to increase angle resolution. The method increases the accuracy of multi-objective DOA estimation while reducing computational costs. Finally, simulation and experiment have verified the method's effectiveness.}
}
@article{FERNANDEZ20181,
title = {Natural deep eutectic solvents-mediated extractions: The way forward for sustainable analytical developments},
journal = {Analytica Chimica Acta},
volume = {1038},
pages = {1-10},
year = {2018},
issn = {0003-2670},
doi = {https://doi.org/10.1016/j.aca.2018.07.059},
url = {https://www.sciencedirect.com/science/article/pii/S0003267018309231},
author = {María de los Ángeles Fernández and Joana Boiteux and Magdalena Espino and Federico J.V. Gomez and María Fernanda Silva},
keywords = {Natural deep eutectic solvents, Extraction, Green analytical chemistry, Sample prep, Microextractions},
abstract = {The concept of sustainable development has impacted in analytical chemistry changing the way of thinking processes and methods. It is important for analytical chemists to consider how sample preparation can integrate the basic concepts of Green Chemistry. In this sense, the replacement of traditional organic solvents is of utmost importance. Natural Deep Eutectic Solvents (NADES) have come to light as a green alternative. In the last few years, a growing number of contributions have applied these natural solvents proving their efficiency in terms of extraction ability, analyte stabilization capacity and detection compatibility. However, the arising question that has to be answered is: the use of NADES is enough to green an extraction process? This review presents an overview of knowledge regarding sustainability of NADES-based extraction procedures, focused on reported literature within the timeframe spanning from 2011 up to date. The contributions were analyzed from a green perspective in terms of energy, time, sample and solvent consumption. Moreover, we include a critical analysis to clarify whether the use of NADES as extraction media is enough for greening an analytical methodology; strategies to make them even greener are also presented. Finally, recent trends and future perspectives on how NADES-based extraction approaches in combination with computational methodologies can contribute are discussed.}
}
@article{XIE2015262,
title = {Evolutionary sampling: A novel way of machine learning within a probabilistic framework},
journal = {Information Sciences},
volume = {299},
pages = {262-282},
year = {2015},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2014.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0020025514011384},
author = {Zhenping Xie and Jun Sun and Vasile Palade and Shitong Wang and Yuan Liu},
keywords = {Evolutionary sampling, Support sample model, Monte Carlo Markov chain, Rejection sampling, Online learning, Particle swarm optimization},
abstract = {In many traditional machine learning methods, sampling is only a process of acquiring training data. However, some studies (on sequential Markov chains and particle filters) have demonstrated that sampling can be used for solving some intractable optimization problems in classical learning methods. Along this line of thinking, the relationships between sampling and learning are theoretically exploited in this paper, wherein the key feature of the sampling process is selecting representative samples from original data that can be modeled by a probability distribution. In theory, acquiring reliable samples is not an easy task for an arbitrary probability distribution. Motivated by approaches in evolutionary computation, rejection sampling and function approximation, a novel sampling strategy, called the evolutionary sampling, is proposed in this paper, and a machine learning method, called the evolutionary sampling approach (ESA), is put forward afterwards. Within ESA, a computing model, called the support sample model (SSM), is presented as well and is used to approximate an original density function. Accordingly, a concrete implementation of an evolutionary sampling approach (ESA) is proposed to seek the optimal model parameters of the SSM. Benefiting from the combination of rejection sampling and evolutionary searching, the ESA can theoretically converge to the optimal solution by minimizing the total variation distance, and can do this with high computational efficiency. Moreover, the normalized factor of a density function can be automatically estimated with high precision within the ESA. As a result, the ESA may be suitable for machine learning problems that could be transformed into density function approximation problems within a probabilistic framework. In addition, derived from the rejection sampling strategy, the ESA can also have online learning abilities required by large-scale data stream processing tasks. Theoretical analyses and application studies are carried out in this paper, and the results demonstrate that the ESA, as a novel way of machine learning, has several prominent merits aspired by past researches in machine learning.}
}
@article{BYLYA20172358,
title = {Modelling challenges for incremental bulk processes despite advances in simulation technology: example issues and approaches},
journal = {Procedia Engineering},
volume = {207},
pages = {2358-2363},
year = {2017},
note = {International Conference on the Technology of Plasticity, ICTP 2017, 17-22 September 2017, Cambridge, United Kingdom},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.10.1008},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817358010},
author = {O.I. Bylya and M. Ward and B. Krishnamurty and S. Tamang and R.A. Vasin},
keywords = {Flow forming, rotary forging, process modelling, simplification approaches. Introduction},
abstract = {Incremental bulk deformation processes have traditionally been difficult to simulate. This paper will argue that, despite advances in computation and software, they remain difficult to model. The main reason for this is the shortage of ideas on what is the real objective of FE modelling for such processes. Even a very detailed model and data obtained in simulation does not give answers to the main question - how to optimise the process parameters? High computational time and volume of information only aggravate the situation. All modern mathematical techniques of dimensionality reduction (such as POD/PGD) lose their power when the priorities and acceptable compromises of modelling are not clear. This paper tries to use a large volume of available experimental and modelling experience to illustrate this problem and look for possible break-through directions.}
}
@incollection{CUMMINS20171,
title = {Chapter 1 - The Agile Enterprise},
editor = {Fred A. Cummins},
booktitle = {Building the Agile Enterprise (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {1-34},
year = {2017},
series = {The MK/OMG Press},
isbn = {978-0-12-805160-3},
doi = {https://doi.org/10.1016/B978-0-12-805160-3.00001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128051603000016},
author = {Fred A. Cummins},
keywords = {Agile enterprise, Business impact of technology, Capability-based architecture, Business collaboration management, Value delivery management, Value delivery modeling language},
abstract = {This chapter begins with an introduction to the agile enterprise concept and provides a somewhat historical perspective on the evolution of information technology and its impact on business operations and management. It then introduces three new ways of thinking that are key to today's agile enterprise and are referenced in the subtitle of this book: (1) capability-based architecture, (2) business collaboration management (BCM), and (3) value delivery management (VDM). Finally, the impact of VDM is discussed related to the management of major business changes, along with some critical success factors for the journey to agility.}
}
@article{LAPIDUS202183,
title = {The road less traveled in protein folding: evidence for multiple pathways},
journal = {Current Opinion in Structural Biology},
volume = {66},
pages = {83-88},
year = {2021},
note = {Centrosomal Organization and Assemblies ● Folding and Binding},
issn = {0959-440X},
doi = {https://doi.org/10.1016/j.sbi.2020.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0959440X20301809},
author = {Lisa J Lapidus},
abstract = {Free Energy Landscape theory of Protein Folding, introduced over 20 years ago, implies that a protein has many paths to the folded conformation with the lowest free energy. Despite the knowledge in principle, it has been remarkably hard to detect such pathways. The lack of such observations is primarily due to the fact that no one experimental technique can detect many parts of the protein simultaneously with the time resolution necessary to see such differences in paths. However, recent technical developments and employment of multiple experimental probes and folding prompts have illuminated multiple folding pathways in a number of proteins that had all previously been described with a single path.}
}
@article{HICKS2007233,
title = {Lean information management: Understanding and eliminating waste},
journal = {International Journal of Information Management},
volume = {27},
number = {4},
pages = {233-249},
year = {2007},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2006.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0268401206001435},
author = {B.J. Hicks},
keywords = {Information management, SMEs, Waste, Information systems infrastructure, Strategy, Process improvement},
abstract = {This paper deals with the development of a new approach for supporting the improvement of information management and the overall information systems infrastructure. In particular, the paper discusses the application of lean thinking to information management; where information management can be considered to involve adding value to information by virtue of how it is organised, visualised and represented; and enabling information (value) to flow to the end-user (customer) through the processes of exchange, sharing and collaboration. The potential benefits of lean thinking are discussed and the fundamental barriers for its application to information management are highlighted. These include the need to characterise the nature of waste and establish the five principles of; value, value streams, flow, pull and continuous improvement in the context of information management. It follows that the core contribution of this paper is the development of an understanding of these critical elements and the creation of a conceptual framework for a set of lean principles within the context of information management. This framework offers a unique and arguably generic approach for supporting the retrospective improvement of information management systems and the overall information systems infrastructure.}
}
@article{TALANOV2018473,
title = {Simulation of serotonin mechanisms in NEUCOGAR cognitive architecture},
journal = {Procedia Computer Science},
volume = {123},
pages = {473-478},
year = {2018},
note = {8th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2017 (Eighth Annual Meeting of the BICA Society), held August 1-6, 2017 in Moscow, Russia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.01.072},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918300735},
author = {Max Talanov and Fail Gafarov and Jordi Vallverdú and Sergey Ostapenko and Marat Gazizov and Alexander Toschev and Alexey Leukhin and Salvatore Distefano},
keywords = {Serotonin, dopamine, disgust, artificial intelligence, simulation, affective computing, emotion modelling, neuromodulation},
abstract = {This work aims at demonstrating that the neuromodulatory mechanisms that control the emotional states of mammals (specifically rat’s brains) can be represented and re-implemented in a computational model processed by a machine. In particular we specifically focus on two neuro-transmitters, serotonin and dopamine, starting from their fundamental role in basic cognitive processes. In our specific implementation, we represent the simulation of the ‘disgust-like’ state based on the three dimensional neuromodulatory model of affects or emotions, according to the ‘cube of emotions’. These functional mechanisms can be transferred into an artificial cognitive system: inhibition, for example, can elicit a blocking behaviour that, depending on its intensity and duration, can push the system to a general emotional state. We have simulated 1000 milliseconds of the serotonin and dopamine systems using NEST Neural Simulation Tool with the rat brain as the model to artificially reproduce this mechanism on a computational system.}
}
@article{RIVIERE2024637,
title = {Proceedings from the inaugural Artificial Intelligence in Primary Immune Deficiencies (AIPID) conference},
journal = {Journal of Allergy and Clinical Immunology},
volume = {153},
number = {3},
pages = {637-642},
year = {2024},
issn = {0091-6749},
doi = {https://doi.org/10.1016/j.jaci.2024.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0091674924000332},
author = {Jacques G. Rivière and Pere {Soler Palacín} and Manish J. Butte},
keywords = {Artificial intelligence, machine learning, large language models, natural language processing, electronic health records, inborn errors of immunity, diagnosis, ethics},
abstract = {Here, we summarize the proceedings of the inaugural Artificial Intelligence in Primary Immune Deficiencies conference, during which experts and advocates gathered to advance research into the applications of artificial intelligence (AI), machine learning, and other computational tools in the diagnosis and management of inborn errors of immunity (IEIs). The conference focused on the key themes of expediting IEI diagnoses, challenges in data collection, roles of natural language processing and large language models in interpreting electronic health records, and ethical considerations in implementation. Innovative AI-based tools trained on electronic health records and claims databases have discovered new patterns of warning signs for IEIs, facilitating faster diagnoses and enhancing patient outcomes. Challenges in training AIs persist on account of data limitations, especially in cases of rare diseases, overlapping phenotypes, and biases inherent in current data sets. Furthermore, experts highlighted the significance of ethical considerations, data protection, and the necessity for open science principles. The conference delved into regulatory frameworks, equity in access, and the imperative for collaborative efforts to overcome these obstacles and harness the transformative potential of AI. Concerted efforts to successfully integrate AI into daily clinical immunology practice are still needed.}
}
@incollection{STEIN202139,
title = {Chapter 2 - Brain–minds: What’s the best metaphor?},
editor = {Dan J. Stein},
booktitle = {Problems of Living},
publisher = {Academic Press},
pages = {39-59},
year = {2021},
isbn = {978-0-323-90239-7},
doi = {https://doi.org/10.1016/B978-0-323-90239-7.00005-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323902397000055},
author = {Dan J. Stein},
keywords = {Psychiatry, Philosophy, Realism, Psychiatric classification, Pluralism, Erklären, Verstehen, Epistemic humility, Practical wisdom},
abstract = {This chapter addresses the question of how best to think about the brain–mind from both philosophical and psychiatric perspectives. The section on philosophy of mind notes the positions of physicalism, dualism, and functionalism, and proposes that emergent materialism has particular advantages. The section on psychiatry notes the positions of behaviourism and existentialism. Two key metaphors of the brain–mind are then critiqued: the hydraulic model of psychoanalysis, and the computational model of cognitive science. A third metaphor, that of ‘wetware’, which emphasizes that the brain–mind cannot simply be divided into hardware and software, but rather that it must be approached as a complex psychobiological phenomenon, is proposed. Several advantages of this metaphor are discussed, including that it is consistent with emergent materialism and a view of the brain–mind as embodied and embedded in social activity, as well as with current cognitive-affective and psychiatric science.}
}
@article{HEINZE2021R1381,
title = {Fly navigation: Yet another ring},
journal = {Current Biology},
volume = {31},
number = {20},
pages = {R1381-R1383},
year = {2021},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2021.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0960982221012495},
author = {Stanley Heinze},
abstract = {Summary
Flies keep track of a food site by path integration. A novel behavioral paradigm has been combined with computational models to show that Drosophila can track at least three food patches simultaneously by using the center of gravity of all food sites as the reference point for their path integrator.}
}
@incollection{BLISS19921,
title = {REASONING SUPPORTED BY COMPUTATIONAL TOOLS},
editor = {MICHAEL R. KIBBY and J. ROGER HARTLEY},
booktitle = {Computer Assisted Learning: Selected Contributions from the CAL '91 Symposium},
publisher = {Pergamon},
address = {Amsterdam},
pages = {1-9},
year = {1992},
isbn = {978-0-08-041395-2},
doi = {https://doi.org/10.1016/B978-0-08-041395-2.50007-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780080413952500078},
author = {JOAN BLISS and JON OGBORN and RICHARD BOOHAN and JONATHAN BRIGGS and TIM BROSNAN and DEREK BROUGH and HARVEY MELLAR and ROB MILLER and CAROLINE NASH and CATHY RODGERS and BABIS SAKONIDIS},
abstract = {Abstract
This paper sets out the work of the Tools for Exploratory Learning Programme within the ESRC Initiative Information Technology in Education. The research examines young secondary children's reasoning with computational tools. We distinguish between exploratory and expressive modes of learning, that is, interaction with another's model and creation of one's own model, respectively. The research focuses on reasoning, rather than learning, along three dimensions: quantitative, qualitative, and semi-quantitative. It provides a 3 × 2 classification of tasks according to modes of learning and types of reasoning. Modelling tools were developed for the study and descriptions of these are given. The research examined children's reasoning with tools in all three dimensions looking more exhaustively at the semi-quantitative. Pupils worked either in an exploratory mode or an expressive mode on one of the following topics: Traffic, Health and Diet, and Shops and Profits. They spent 3-4 h individually with a researcher over 2 weeks, carrying out four different activities: reasoning without the computer; learning to manipulate first the computer then later the tool and finally carrying out a task with the modelling tool. Pupils were between 12 and 14 yr. Research questions both about children's reasoning when working with or creating models and about the nature of the tools used are discussed. Finally an analytic scheme is set out which describes the nature of the causal and non-causal reasoning observed together with some tentative results.}
}
@article{ADAMS201731,
title = {Patternlets — A teaching tool for introducing students to parallel design patterns},
journal = {Journal of Parallel and Distributed Computing},
volume = {105},
pages = {31-41},
year = {2017},
note = {Keeping up with Technology: Teaching Parallel, Distributed and High-Performance Computing},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2017.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S074373151730014X},
author = {Joel C. Adams},
keywords = {Design patterns, Education, MPI, Multiprocessing, Multithreading, OpenMP, Parallel, Patternlets, Teaching, Threads},
abstract = {Thanks to the ubiquity of multicore processors, today’s CS students must be introduced to parallel computing or they will be ill prepared as modern software developers. Professional developers of parallel software think in terms of parallel design patterns, which are markedly different from traditional (sequential) design patterns. It follows that the more we can teach students to think in terms of parallel patterns, the more their thinking will resemble that of parallel software professionals. In this paper, we present patternlets—minimalist, scalable, syntactically correct programs, each designed to introduce students to a particular parallel design pattern. The collection currently includes 44 patternlets (16 MPI, 17 OpenMP, 9 Pthreads, and 2 heterogeneous), of which we present a representative sample. We also present data that indicate the use of patternlets to introduce parallelism in CS2 produced a modest improvement in student understanding of parallel concepts.}
}
@article{GOTTS2019100728,
title = {Agent-based modelling of socio-ecological systems: Models, projects and ontologies},
journal = {Ecological Complexity},
volume = {40},
pages = {100728},
year = {2019},
note = {Agent-based modelling to study resilience in socio-ecological systems},
issn = {1476-945X},
doi = {https://doi.org/10.1016/j.ecocom.2018.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S1476945X18301272},
author = {Nicholas M. Gotts and George A.K. {van Voorn} and J. Gareth Polhill and Eline de Jong and Bruce Edmonds and Gert Jan Hofstede and Ruth Meyer},
keywords = {Socio-ecological system, Agent-based model, Complexity, Ontology},
abstract = {Socio-Ecological Systems (SESs) are the systems in which our everyday lives are embedded, so understanding them is important. The complex properties of such systems make modelling an indispensable tool for their description and analysis. Human actors play a pivotal role in SESs, but their interactions with each other and their environment are often underrepresented in SES modelling. We argue that more attention should be given to social aspects in models of SESs, but this entails additional kinds of complexity. Modelling choices need to be as transparent as possible, and to be based on analysis of the purposes and limitations of modelling. We recommend thinking in terms of modelling projects rather than single models. Such a project may involve multiple models adopting different modelling methods. We argue that agent-based models (ABMs) are an essential tool in an SES modelling project, but their expressivity, which is their major advantage, also produces problems with model transparency and validation. We propose the use of formal ontologies to make the structure and meaning of models as explicit as possible, facilitating model design, implementation, assessment, comparison and extension.}
}
@article{EVANS2008100,
title = {When can we say ‘if’?},
journal = {Cognition},
volume = {108},
number = {1},
pages = {100-116},
year = {2008},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2008.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0010027708000310},
author = {Jonathan St.B.T. Evans and Helen Neilens and Simon J. Handley and David E. Over},
keywords = {Conditionals, Reasoning, Decision making, Language comprehension},
abstract = {In this study, we focus on the conditions which permit people to assert a conditional statement of the form ‘if p then q’ with conversational relevance. In a broadly decision-theoretic approach, also drawing on hypothetical thinking theory [Evans, J. St. B. T. (2007). Hypothetical thinking: Dual processes in reasoning and judgement. Hove, UK: Psychology Press.], we predicted that conditional tips and promises would appear more useful and persuasive and be more likely to encourage an action p when (a) the conditional link from p to q was stronger, (b) the cost of the action p was lower and (c) the benefit of the consequence q was higher. Similarly, we predicted that conditional warnings and threats would be seen as more useful and persuasive and more likely to discourage an action p when (a) the conditional link from p to q was stronger, (b) the benefit of the action p was lower and (c) the cost of the consequence q was higher. All predictions were strongly confirmed, suggesting that such conditionals may best be asserted when they are of high relevance to the goals of the listener.}
}
@article{TAKAMA20151263,
title = {NFC-based Tangible User Interface for Information Curation and Its Application to Analogy Game},
journal = {Procedia Computer Science},
volume = {60},
pages = {1263-1270},
year = {2015},
note = {Knowledge-Based and Intelligent Information & Engineering Systems 19th Annual Conference, KES-2015, Singapore, September 2015 Proceedings},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.08.192},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915023194},
author = {Yasufumi Takama and Tomohiro Ito and Hiroshi Ishikawa},
keywords = {Tangible user interface (TUI), Near field communication (NFC), Smartphone, Information curation, Analogy game},
abstract = {This paper applies a Tangible User Interface (TUI) for information curation using Near Field Communication (NFC) to an analogy game. The increase in text data is more remarkable in current IT society. Although those are usually accessed with using Graphical User Interface (GUI), users except experienced computer users have difficulty in reading and organizing data with GUI. In particular, information curation such as grouping related data / information and finding relationship among them is difficult. In order to solve this problem, an interface that can access text data intuitively is expected. We are developing a TUI based on NFC, by which a user can move and group text data in a similar manner when handling paper documents. As one of the promising applications of the proposed TUI, this paper focuses on creative thinking support, for which touching externalized thought by hand is expected to be effective. An experiment is conducted, in which test participants did an analogy game with using the proposed TUI. The experimental result shows experience of using the TUI affects the participants’ self-evaluation about idea creation.}
}
@article{ROGOWSKI2024109246,
title = {Unlocking massively parallel spectral proper orthogonal decompositions in the PySPOD package},
journal = {Computer Physics Communications},
volume = {302},
pages = {109246},
year = {2024},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2024.109246},
url = {https://www.sciencedirect.com/science/article/pii/S0010465524001693},
author = {Marcin Rogowski and Brandon C.Y. Yeung and Oliver T. Schmidt and Romit Maulik and Lisandro Dalcin and Matteo Parsani and Gianmarco Mengaldo},
keywords = {Spectral proper orthogonal decomposition, SPOD, Parallel, Distributed, MPI, Modal decomposition, Dynamical systems},
abstract = {We propose a parallel (distributed) version of the spectral proper orthogonal decomposition (SPOD) technique. The parallel SPOD algorithm distributes the spatial dimension of the dataset preserving time. This approach is adopted to preserve the non-distributed fast Fourier transform of the data in time, thereby avoiding the associated bottlenecks. The parallel SPOD algorithm is implemented in the PySPOD library and makes use of the standard message passing interface (MPI) library, implemented in Python via mpi4py. An extensive performance evaluation of the parallel package is provided, including strong and weak scalability analyses. The open-source library allows the analysis of large datasets of interest across the scientific community. Here, we present applications in fluid dynamics and geophysics, that are extremely difficult (if not impossible) to achieve without a parallel algorithm. This work opens the path toward modal analyses of big quasi-stationary data, helping to uncover new unexplored spatio-temporal patterns.
Program summary
Program Title: PySPOD CPC Library link to program files: https://doi.org/10.17632/jf5bf26jcj.1 Developer's repository link: https://github.com/MathEXLab/PySPOD Licensing provisions: MIT License Programming language: Python Nature of problem: Large spatio-temporal datasets may contain coherent patterns that can be leveraged to better understand, model, and possibly predict the behavior of complex dynamical systems. To this end, modal decomposition methods, such as the proper orthogonal decomposition (POD) and its spectral counterpart (SPOD), constitute powerful tools. The SPOD algorithm allows the systematic identification of space-time coherent patterns. This can be used to understand better the physics of the process of interest, and provide a path for mathematical modeling, including reduced order modeling. The SPOD algorithm has been successfully applied to fluid dynamics, geophysics and other domains. However, the existing open-source implementations are serial, and they prevent running on the increasingly large datasets that are becoming available, especially in computational physics. The inability to analyze via SPOD large dataset in turn prevents unlocking novel mechanisms and dynamical behaviors in complex systems. Solution method: We provide an open-source parallel (MPI distributed) code, namely PySPOD, that is able to run on large datasets (the ones considered in the present paper reach about 200 Terabytes). The code is built on the previous serial open-source code PySPOD that was published in https://joss.theoj.org/papers/10.21105/joss.02862.pdf. The new parallel implementation is able to scale on several nodes (we show both weak and strong scalability) and solve some of the bottlenecks that are commonly found at the I/O stage. The current parallel code allows running on datasets that was not easy or possible to analyze with serial SPOD algorithms, hence providing a path towards unlocking novel findings in computational physics. Additional comments including restrictions and unusual features: The code comes with a set of built-in postprocessing tools, for visualizing the results. It also comes with extensive continuous integration, documentation, and tutorials, as well as a dedicated website in addition to the associated GiHub repository. Within the package we also provide a parallel implementation of the proper orthogonal decomposition (POD), that leverages the I/O parallel capabilities of the SPOD algorithm.}
}
@article{ATANASIU2023e20698,
title = {On the utility of Colour in shape analysis: An introduction to Colour science via palaeographical case studies},
journal = {Heliyon},
volume = {9},
number = {10},
pages = {e20698},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e20698},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023079069},
author = {Vlad Atanasiu and Peter Fornaro},
keywords = {Colour science, Colour processing, Colour perception, Image processing, Image enhancement, Palaeography},
abstract = {In this article, we explore the use of colour for the analysis of shapes in digital images. We argue that colour can provide unique information that is not available from shape alone, and that familiarity with the interdisciplinary field of colour science is essential for unlocking the potential of colour. Within this perspective, we offer an illustrated overview of the colour-related aspects of image management and processing, perceptual psychology, and cultural studies, using for exemplary purposes case studies focused on computational palaeography. We also discuss the changing roles of colour in society and the sciences, and provide technical solutions for using digital colour effectively, highlighting the impact of human factors. The article concludes with an annotated bibliography. This work is a primer, and its intended readership are scholars and computer scientists unfamiliar with colour science.}
}