@article{OZIN2024718,
title = {The curiosity-creativity element in HI-AI materials discovery},
journal = {Matter},
volume = {7},
number = {3},
pages = {718-722},
year = {2024},
issn = {2590-2385},
doi = {https://doi.org/10.1016/j.matt.2024.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S2590238524000018},
author = {Geoffrey Ozin and Todd Siler and Chenxi Qian and Wenjie Zhou},
abstract = {A career materials chemist and a group of art-science colleagues explore the colorful tapestry of curiosity-creativity-imagination-intuition-wonderment elements inherent in the process of materials discovery in the past and today by human intelligence and in the future by artificial intelligence.}
}
@incollection{CHAN2024xv,
title = {Preface},
editor = {C.C. Chan and George You Zhou and Wei Han},
booktitle = {Integration of Energy, Information, Transportation and Humanity},
publisher = {Elsevier},
pages = {xv-xix},
year = {2024},
isbn = {978-0-323-95521-8},
doi = {https://doi.org/10.1016/B978-0-323-95521-8.00022-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955218000221},
author = {C.C. Chan}
}
@article{WANG2016377,
title = {ACP-based social computing and parallel intelligence: Societies 5.0 and beyond},
journal = {CAAI Transactions on Intelligence Technology},
volume = {1},
number = {4},
pages = {377-393},
year = {2016},
issn = {2468-2322},
doi = {https://doi.org/10.1016/j.trit.2016.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S246823221630083X},
author = {Xiao Wang and Lingxi Li and Yong Yuan and Peijun Ye and Fei-Yue Wang},
keywords = {Social computing, Societies 5.0, Parallel intelligence, Knowledge automation, Cyber-physical-social system, Artificial societies, Computational experiments, Parallel execution},
abstract = {Social computing, as the technical foundation of future computational smart societies, has the potential to improve the effectiveness of open-source big data usage, systematically integrate a variety of elements including time, human, resources, scenarios, and organizations in the current cyber-physical-social world, and establish a novel social structure with fair information, equal rights, and a flat configuration. Meanwhile, considering the big modeling gap between the model world and the physical world, the concept of parallel intelligence is introduced. With the help of software-defined everything, parallel intelligence bridges the big modeling gap by means of constructing artificial systems where computational experiments can be implemented to verify social policies, economic strategies, and even military operations. Artificial systems play the role of “social laboratories” in which decisions are computed before they are executed in our physical society. Afterwards, decisions with the expected outputs are executed in parallel in both the artificial and physical systems to interactively sense, compute, evaluate and adjust system behaviors in real-time, leading system behaviors in the physical system converging to those proven to be optimal in the artificial ones. Thus, the smart guidance and management for our society can be achieved.}
}
@article{STOJANOVIC2021107270,
title = {Application of distance learning in mathematics through adaptive neuro-fuzzy learning method},
journal = {Computers & Electrical Engineering},
volume = {93},
pages = {107270},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107270},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621002536},
author = {Jelena Stojanović and Dalibor Petkovic and Ibrahim M Alarifi and Yan Cao and Nebojsa Denic and Jelena Ilic and Hamid Assilzadeh and Sead Resic and Biljana Petkovic and Afrasyab Khan and Milosav Milickovic},
keywords = {Pupils, E-learning, Distance learning, Moodle, Computational intelligent},
abstract = {The main aim of the study is analyzing of pupils’ knowledge in mathematics by adaptive neuro fuzzy inference system (ANFIS) after implementation of distance learning application or e-learning (electronic learning). Since a large number of faculties and other institutions are increasingly using e-learning, it can be stated that for this purpose the Modular object-oriented dynamic learning environment (Moodle) learning management system (LMS) is mostly used. This paper deals with the analysis of distance learning and the application of Moodle LMS in higher education institutions, taking into account the impact of such education on the quality of teaching and the acquisition of knowledge by students, and the methods teachers use in Serbia. The ANFIS is used to determine which factors are the most important for pupils’ performance in mathematics. The results show that the main influence on the pupils’ performance is their prior knowledge. The prior knowledge is more effective when it is combined with education software in the lectures of mathematics in elementary school. In secondary school, the prior knowledge is more effective if it is combined with motivation for learning mathematics.}
}
@article{HAVENS2020104571,
title = {Automated Water Supply Model (AWSM): Streamlining and standardizing application of a physically based snow model for water resources and reproducible science},
journal = {Computers & Geosciences},
volume = {144},
pages = {104571},
year = {2020},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2020.104571},
url = {https://www.sciencedirect.com/science/article/pii/S0098300420305598},
author = {Scott Havens and Danny Marks and Micah Sandusky and Andrew Hedrick and Micah Johnson and Mark Robertson and Ernesto Trujillo},
keywords = {Hydrology, Computational method, Software engineering, Data assimilation},
abstract = {Reproducible science requires a shift in thinking and application for how data, code and analysis are shared. Now, scientists must act more like software engineers to design models and perform analysis that use principles and techniques pioneered by software developers. Creating reproducible models that are easy to use and understand is in the best interest for the snow and hydrology community, enabling studies by other researchers and facilitating technology transfer to operational applications. Here, we present the Automated Water Supply Model (AWSM) that streamlines and standardizes the workflow of a physically based snow model to create fully reproducible model simulations that can be utilized by researchers and operational water resource managers. AWSM orchestrates four core components that historically required significant, ad-hoc modeler interaction to load the input data, spatially interpolate to the modeling domain, run the models and process the outputs. Because AWSM was developed using principles and techniques from software engineering, users can quickly perform reproducible simulations on any operating system, from a laptop to the cloud. The three fully reproducible example case studies showcase the simplicity and flexibility of using AWSM to perform simulations from small research catchments to simulations that aid in real time water management decisions.}
}
@article{HALL20156607,
title = {Understanding Sector Dependencies in the Stabilization and Reconstruction of Nation-states},
journal = {Procedia Manufacturing},
volume = {3},
pages = {6607-6614},
year = {2015},
note = {6th International Conference on Applied Human Factors and Ergonomics (AHFE 2015) and the Affiliated Conferences, AHFE 2015},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2015.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S2351978915009932},
author = {Steven Hall and Curtis Blais},
keywords = {Multi-agent, system dynamics, modeling and simulation, panarchy, resiliency, stabilitiy, reconstruction and humanitarian operations.},
abstract = {The United States Army is undergoing a re-definition of its Civil Affairs officer positions. A recent project to define the educational requirements for an Army Civil Affairs Officer (38G) identified an educational requirement to help officers understand the complex ways in which the operations that advance the achievement of one stabilization objective often hinder the achievement of other objectives. The system level thinking was seen to be frequently insufficiently ingrained amongst Civil Affairs Officers (and the leaders they advised), who were both often perceived to be inclined, in the face of the complexities of the situation on the ground, to become too narrowly focused on achieving their specific assigned responsibilities, limiting their ability to see how the mission effectiveness of what they were recommending would be influenced by the state and trajectory of other Sectors and how, in turn, their recommendations would influence the mission effectiveness of other Sector stewards. While system dynamics modeling has proven itself to be effective in capturing and effectively communicating feedback loops that define such non-linear (and non-intuitive) systems they do not, in themselves, provide sufficient modeling richness to comprehensively capture the critical spatial (geographical) determinants of a successful state reconstruction process. For these purposes, a multi-agent cellular automata model is recommended both as a vehicle for introducing students to the complex nature of the state reconstruction process and, eventually, for use in the field by deployed Civilian Affairs Officers at all levels. This paper describes the problem and the modeling approach to address it.}
}
@article{HENDIANI2021107156,
title = {Diagnosing social failures in sustainable supply chains using a modified Pythagorean fuzzy distance to ideal solution},
journal = {Computers & Industrial Engineering},
volume = {154},
pages = {107156},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107156},
url = {https://www.sciencedirect.com/science/article/pii/S0360835221000607},
author = {Sepehr Hendiani and Benjamin Lev and Afsaneh Gharehbaghi},
keywords = {Supply chain management, Corporate social responsibility, Social sustainability, Interval-valued Pythagorean fuzzy sets, Sustainable development},
abstract = {Social sustainability can be mentioned as one of the pivotal objectives towards sustainable development which has received the least attention comparing to environmental and economic dimensions during these past years. Due to its impact on organization’s competitive power, researchers have proposed models to measure social performance in supply chains. However, most of these researches reveal shortcomings once encountering the cases with a huge number of criteria due to their complex computations. In order to fill this gap, this study proposes a new soft computing multi-criteria interval-valued Pythagorean fuzzy distance to ideal solution approach based on interval-valued Pythagorean closeness which performs outstandingly in cases with a high fluctuation in the number of criteria. A new mechanism is defined to distinguish the weak performing social factors through supply chains by classifying them into four categorize based on their performance and distance to the best performing factors. This approach is unique in the sense that it both covers a remarkable amount of uncertainty and eases the computational processes of the previous multi-criteria decision making approaches by modifying the steps to select the most ideal solution. The feasibility and applicability of this approach have been validated by applications to a numerical case and comparative analysis.}
}
@article{ROLAND2002183,
title = {Dynamic depolarization fields in the cerebral cortex},
journal = {Trends in Neurosciences},
volume = {25},
number = {4},
pages = {183-190},
year = {2002},
issn = {0166-2236},
doi = {https://doi.org/10.1016/S0166-2236(00)02125-1},
url = {https://www.sciencedirect.com/science/article/pii/S0166223600021251},
author = {Per E. Roland},
keywords = {general computational elements, voltage sensitive dyes, cortical dynamics, layer II-III neurons, memory, cognition},
abstract = {Recent physiological evidence shows that in response to stimuli and preceding motor activity, large fields of the upper layers of the cerebral cortex depolarize. It is argued that this finding is a general one and that these dynamic depolarization fields represent the computational elements of the cerebral cortex. Each depolarization field engages many more neurons than do columns and hyper-columns. These fields can be explained by cooperative neuronal computing in layers I–III of the cortex. In these layers, the computing modes might be general for all parts of the cerebral cortex and be sufficiently flexible to handle all sorts of cortical computations, including perception, memory storage, memory retrieval, thought and the production of behavior.}
}
@incollection{ASHBY2016211,
title = {Chapter 14 - The Vision: A Circular Materials Economy},
editor = {Michael F. Ashby},
booktitle = {Materials and Sustainable Development},
publisher = {Butterworth-Heinemann},
address = {Boston},
pages = {211-239},
year = {2016},
isbn = {978-0-08-100176-9},
doi = {https://doi.org/10.1016/B978-0-08-100176-9.00014-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780081001769000141},
author = {Michael F. Ashby},
keywords = {Active stock, Circularity metrics, Material efficiency, Natural and industrial ecology, Product life extension, Product–service systems, Reuse, repair and recycling, Take-back schemes},
abstract = {We live at present with a largely linear materials economy. Our use of natural resources is characterized by the sequence “take – make – use – dispose” as materials progress from mine, through product, to landfill. Increasing population, rising affluence and the limited capacity for the planet to provide resources and absorb waste argue for a transition towards a more circular way of using materials. When products come to the end of their lives the materials they contain are still there. Repair, reuse and recycling (the three “Rs”) can return these to active use. Repair, reuse and recycling are not new ideas; they have been used for centuries to recirculate materials and, in less-developed economies, they still are. But in developed nations they have dwindled as the cost of materials fell and that of labor rose over time, making all three Rs uneconomic. So what is novel about the contemporary idea of a circular materials economy? Haven’t we been there before? The “circularity” concept is a way thinking that looks not just for efficiencies but also for new ways of providing the functions we need. In the last decade momentum has gathered about this transition. The idea of deploying rather than consuming materials, of using them not once but many times, and of redesign to make this a reality has economic as well as environmental appeal. Governments now sign up to programs to foster circular economic ideas and mechanisms begin to appear to advance them. This chapter examines the background, the successes and the difficulties of implementing a circular materials economy.}
}
@article{YANG2022119585,
title = {Dynamic neural reconfiguration for distinct strategies during competitive social interactions},
journal = {NeuroImage},
volume = {263},
pages = {119585},
year = {2022},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2022.119585},
url = {https://www.sciencedirect.com/science/article/pii/S1053811922007005},
author = {Ruihan Yang and Yina Ma and Bao-Bao Pan and Meghana A. Bhatt and Terry Lohrenz and Hua-Guang Gu and Jonathan W. Kanen and Colin F. Camerer and P. Read Montague and Qiang Luo},
keywords = {Dynamic behavior modeling, Dynamic brain network, Information flow, Strategic deception},
abstract = {Information exchange between brain regions is key to understanding information processing for social decision-making, but most analyses ignore its dynamic nature. New insights on this dynamic might help us to uncover the neural correlates of social cognition in the healthy population and also to understand the malfunctioning neural computations underlying dysfunctional social behavior in patients with mental disorders. In this work, we used a multi-round bargaining game to detect switches between distinct bargaining strategies in a cohort of 76 healthy participants. These switches were uncovered by dynamic behavioral modeling using the hidden Markov model. Proposing a novel model of dynamic effective connectivity to estimate the information flow between key brain regions, we found a stronger interaction between the right temporoparietal junction (rTPJ) and the right dorsolateral prefrontal cortex (rDLPFC) for the strategic deception compared with the social heuristic strategies. The level of deception was associated with the information flow from the Brodmann area 10 to the rTPJ, and this association was modulated by the rTPJ-to-rDLPFC information flow. These findings suggest that dynamic bargaining strategy is supported by dynamic reconfiguration of the rDLPFC-and-rTPJ interaction during competitive social interactions.}
}
@article{ARIFOVIC20071971,
title = {Call market book information and efficiency},
journal = {Journal of Economic Dynamics and Control},
volume = {31},
number = {6},
pages = {1971-2000},
year = {2007},
note = {Tenth Workshop on Economic Heterogeneous Interacting Agents},
issn = {0165-1889},
doi = {https://doi.org/10.1016/j.jedc.2007.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0165188907000073},
author = {Jasmina Arifovic and John Ledyard},
keywords = {Computer testbeds, Call markets, Learning, Experiments with human subjects, Closed book, Market design},
abstract = {What are the consequences of making bids and offers in the book available to traders in a call market? This is a problem in market design. We employ a computational mechanism design methodology to attack this problem and find that allocative efficiencies are higher in a closed book design. We validate our computational approach by running a series of tests with human subjects in exactly the same environments.}
}
@incollection{GRILLI2025618,
title = {Amnesic syndromes},
editor = {Jordan Henry Grafman},
booktitle = {Encyclopedia of the Human Brain (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {618-631},
year = {2025},
isbn = {978-0-12-820481-8},
doi = {https://doi.org/10.1016/B978-0-12-820480-1.00136-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204801001364},
author = {Matthew D. Grilli and Daniela J. Palombo},
keywords = {Amnesia, Anterograde, Cognition, Cortex, Episodic memory, Future thinking, Hippocampus, Medial temporal lobes, Memory, Retrograde, Learning, Semantic memory},
abstract = {The amnesic syndrome has long had a central position in understanding the role of the hippocampus and surrounding cortical medial temporal lobe structures in memory and cognition. In this article, we review the neuropsychological profile of the prototypical amnesic syndrome, including what is spared and impaired in memory, as well as other aspects of cognition, and we relate these findings to contemporary brain-behavior models. We also discuss how variants of the prototypical amnesic syndrome can arise from a framework that views the hippocampus as part of an extended neural network. As the clinical and cognitive neuroscience of memory evolves, so too will our understanding of the amnesic syndrome.}
}
@article{SHEN2023107022,
title = {Automatic identification of schizophrenia based on EEG signals using dynamic functional connectivity analysis and 3D convolutional neural network},
journal = {Computers in Biology and Medicine},
volume = {160},
pages = {107022},
year = {2023},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2023.107022},
url = {https://www.sciencedirect.com/science/article/pii/S0010482523004870},
author = {Mingkan Shen and Peng Wen and Bo Song and Yan Li},
keywords = {ScZ, EEG, Cross mutual information, 3D convolutional neural network, Default mode network},
abstract = {Schizophrenia (ScZ) is a devastating mental disorder of the human brain that causes a serious impact of emotional inclinations, quality of personal and social life and healthcare systems. In recent years, deep learning methods with connectivity analysis only very recently focused into fMRI data. To explore this kind of research into electroencephalogram (EEG) signal, this paper investigates the identification of ScZ EEG signals using dynamic functional connectivity analysis and deep learning methods. A time-frequency domain functional connectivity analysis through cross mutual information algorithm is proposed to extract the features in alpha band (8–12 Hz) of each subject. A 3D convolutional neural network technique was applied to classify the ScZ subjects and health control (HC) subjects. The LMSU public ScZ EEG dataset is employed to evaluate the proposed method, and a 97.74 ± 1.15% accuracy, 96.91 ± 2.76% sensitivity and 98.53 ± 1.97% specificity results were achieved in this study. In addition, we also found not only the default mode network region but also the connectivity between temporal lobe and posterior temporal lobe in both right and left side have significant difference between the ScZ and HC subjects.}
}
@article{COMPANY2016108,
title = {A mixed derivative terms removing method in multi-asset option pricing problems},
journal = {Applied Mathematics Letters},
volume = {60},
pages = {108-114},
year = {2016},
issn = {0893-9659},
doi = {https://doi.org/10.1016/j.aml.2016.04.011},
url = {https://www.sciencedirect.com/science/article/pii/S0893965916301252},
author = {R. Company and V.N. Egorova and L. Jódar and F. Soleymani},
keywords = {Multiasset option pricing, Multidimensional partial differential equations, Mixed derivative terms,  factorization, Bunch–Kaufman factorization},
abstract = {The challenge of removing the mixed derivative terms of a second order multidimensional partial differential equation is addressed in this paper. The proposed method, which is based on proper algebraic factorization of the so-called diffusion matrix, depends on the semidefinite or indefinite character of this matrix. Computational cost of the transformed equation is considerably reduced and well-known numerical drawbacks are avoided.}
}
@article{MAKSHAKOVA2022118732,
title = {Three-dimensional structures, dynamics and calcium-mediated interactions of the exopolysaccharide, Infernan, produced by the deep-sea hydrothermal bacterium Alteromonas infernus},
journal = {Carbohydrate Polymers},
volume = {276},
pages = {118732},
year = {2022},
issn = {0144-8617},
doi = {https://doi.org/10.1016/j.carbpol.2021.118732},
url = {https://www.sciencedirect.com/science/article/pii/S014486172101119X},
author = {Olga Makshakova and Agata Zykwinska and Stephane Cuenot and Sylvia Colliec-Jouault and Serge Perez},
keywords = {Exopolysaccharides, , 3 dimensional structures, Gel forming, Molecular dynamics, Quantum chemistry, Calcium binding},
abstract = {The exopolysaccharide Infernan, from the bacterial strain GY785, has a complex repeating unit of nine monosaccharides established on a double-layer of sidechains. A cluster of uronic and sulfated monosaccharides confers to Infernan functional and biological activities. We characterized the 3-dimensional structures and dynamics along Molecular Dynamics trajectories and clustered the conformations in extended two-fold and five-fold helical structures. The electrostatic potential distribution over all the structures revealed negatively charged cavities explored for Ca2+ binding through quantum chemistry computation. The transposition of the model of Ca2+complexation indicates that the five-fold helices are the most favourable for interactions. The ribbon-like shape of two-fold helices brings neighbouring chains in proximity without steric clashes. The cavity chelating the Ca2+ of one chain is completed throughout the interaction of a sulfate group from the neighbouring chain. The resulting is a ‘junction zone’ based on unique chain-chain interactions governed by a heterotypic binding mode.}
}
@incollection{GILLAM199523,
title = {Chapter 2 - The Perception of Spatial Layout from Static Optical Information},
editor = {William Epstein and Sheena Rogers},
booktitle = {Perception of Space and Motion},
publisher = {Academic Press},
address = {San Diego},
pages = {23-67},
year = {1995},
series = {Handbook of Perception and Cognition},
isbn = {978-0-12-240530-3},
doi = {https://doi.org/10.1016/B978-012240530-3/50004-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780122405303500043},
author = {Barbara Gillam},
abstract = {Publisher Summary
This chapter reviews the literature on absolute distance, relative distance, surface slant and curvature, and the perception of size and shape within the context of several broad issues that have influenced thinking and experimentation to varying degrees in recent years. One issue that has driven recent research is the way stimulus input is described that carries implicit assumptions about how it is encoded and represented. Euclidian and other conventional frameworks may be restricting and misleading as a basis for visual theory. Another issue raised by computational approaches is the relationship between the processing of different sources of information or cues underlying the perception of spatial layout. Machine vision has tended to treat these cues as separate modules or processing systems, a view that has also received support from psychophysics. Comparison of some seemingly separate processes, specifically perspective and stereopsis, may indicate common mechanisms.}
}
@article{MORRA2024100291,
title = {In memory of Professor David Alexander Yuen},
journal = {Earthquake Research Advances},
volume = {4},
number = {2},
pages = {100291},
year = {2024},
issn = {2772-4670},
doi = {https://doi.org/10.1016/j.eqrea.2024.100291},
url = {https://www.sciencedirect.com/science/article/pii/S2772467024000174},
author = {Gabriele Morra and Henry M. Tufo}
}
@article{LEE2024,
title = {Neural correlates of thought–action fusion and their associations with rumination in patients with major depressive disorder},
journal = {Journal of Psychiatric Research},
year = {2024},
issn = {0022-3956},
doi = {https://doi.org/10.1016/j.jpsychires.2024.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0022395624005818},
author = {Sang Won Lee and Seungho Kim and Hansol Lee and Yongmin Chang and Seung Jae Lee},
keywords = {fMRI, transdiagnostic, salience network, response inhibition, caudate, insula},
abstract = {ABSTRACT
Thought-action fusion (TAF) is the metacognitive belief that the power of thoughts can have real-life consequences, often inducing unpleasant inner experiences and recruiting coping strategies such as rumination. Therefore, this study aimed to investigate the neural correlates of TAF and their associations with rumination in depression. A total of 37 patients with MDD and 37 healthy controls (HCs) underwent functional magnetic resonance imaging with a TAF induction task and psychological assessments. In the TAF induction task, participants were asked to read the name of a close person (CP) or neutral person (NP) in association with negative (Neg) TAF and neutral (Neu) control statements. In this study, our TAF induction task activated brain regions, including the salient and default-mode networks, in the MDD and HC groups. However, along with higher likelihood TAF and rumination, behavioral data showed less negative feelings and longer reaction times under the NegCP condition in the MDD group compared with the HC group. Furthermore, in the MDD group, significantly higher activation in the bilateral caudate nuclei, left inferior frontal gyrus/anterior insula, putamen, and inferior parietal lobule under the NegCP condition was positively correlated with TAF and reflection. These findings suggest that during the TAF task, patients with MDD may activate different brain areas associated with the maintenance of high stimulus saliency and habit formation, which are important neural correlates linking TAF and rumination in depression}
}
@article{PLEBE20164,
title = {What is ‘wrong’ in a neural model},
journal = {Cognitive Systems Research},
volume = {39},
pages = {4-14},
year = {2016},
note = {From human to artificial cognition (and back): new perspectives of cognitively inspired AI systems},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2015.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S1389041716000085},
author = {Alessio Plebe},
keywords = {Moral cognition, Neural computation, Orbitofrontal cortex, Amygdala, Self-organization},
abstract = {Neural computation has an influential role in the study of human capacities and behaviors. It has been the dominant approach in the vision science of the last half century, and it is currently one of the fundamental methods of investigation for most higher cognitive functions. Yet, neurocomputational approaches to moral behavior are lacking. Computational modeling in general has been scarcely pursued in morality, and existent non-neural attempts have failed to account for the mental processes involved in morality. In this paper we argue that recently the situation has evolved in a way that subverted the insufficient knowledge on the basic organization of moral cognition in brain circuits, making the project of modeling morality in neurocomputational terms feasible. We will present an original architecture that combines reinforcement learning and Hebbian learning, aimed at simulating forms of moral behavior in a simple artificial context. The relationship between language and morality is controversial. In the analytic tradition of philosophy, morality is essentially the language of morals. On the other side, current cognitive ethology has shown how non human species display behaviors that are surprisingly similar to those prescribed by human ethics. Nevertheless, morality in humans is deeply entrenched with language, and the semantics of words like ‘wrong’ resists consensual explanations. The model here proposed includes an auditory processing pathway, with the purpose of showing how the coding of “wrong”, even if highly simplified with respect to its rich content in natural language, can emerge in the course of moral learning.}
}
@article{CHEN199799,
title = {Towards designing sustainable urban wastewater infrastructures: A screening analysis},
journal = {Water Science and Technology},
volume = {35},
number = {9},
pages = {99-112},
year = {1997},
note = {Sustainable Sanitation},
issn = {0273-1223},
doi = {https://doi.org/10.1016/S0273-1223(97)00188-1},
url = {https://www.sciencedirect.com/science/article/pii/S0273122397001881},
author = {J. Chen and M.B. Beck},
keywords = {Sustainability, wastewater treatment technologies, screening analysis, uncertainty, urban drainage system},
abstract = {Whether sustainability can, or should, be defined in a practical operational sense, it is clear that the emergence of such a notion has prompted what seems to be a profound re-thinking of whether our society, economic system, and technology are as we would wish them to be. Sustainable development, clean technology, life-cycle analysis, pollution prevention, and so on, are expressions of a willingness to leave no stone unturned, as it were, in the search for what would be appropriate. With respect to the design and operation of a city's wastewater infrastructure, in particular, this search is characterised by a seeming explosion in the possible combinations of appropriate technologies, gross uncertainty about how novel technologies - only now emerging - might perform in the very long term, and a continuing absence of specific criteria of sustainability for determining the grounds on which any candidate technology might be preferred over another. The paper introduces a simple computational procedure for generating and screening candidate combinations of unit-process technologies for an urban wastewater infrastructure. This is based on the use of Monte Carlo simulation, with the identification of those specific technologies (and combinations thereof) that appear to have the greatest probability of being selected for use under different, possibly evolving, criteria of sustainability. Application of the procedure is illustrated with respect to just a part of this infrastructure, i.e., the wastewater treatment plant.}
}
@incollection{AGGARWAL2022135,
title = {Chapter 5 - Models for improving fresh produce chains},
editor = {Wojciech J. Florkowski and Nigel H. Banks and Robert L. Shewfelt and Stanley E. Prussia},
booktitle = {Postharvest Handling (Fourth Edition)},
publisher = {Academic Press},
edition = {Fourth Edition},
address = {San Diego},
pages = {135-164},
year = {2022},
isbn = {978-0-12-822845-6},
doi = {https://doi.org/10.1016/B978-0-12-822845-6.00005-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128228456000051},
author = {Deepak Aggarwal and Stanley E. Prussia},
keywords = {Soft systems, modeling, simulation, postharvest quality simulator, supply chain game, soft systems methodologies},
abstract = {Fresh produce passes through various links of refrigerated or nonrefrigerated value chains from the farmer’s plot to the consumer’s plate. Extensive research since the 1960s has focused on deciphering the ideal postharvest handling conditions to maximize product shelf life for different produce species. However, ideal conditions are seldom met in real-life value chains as the produce travels from field to sorting area, packaging, loading, transportation, unloading, retail display, and the consumer’s car. Further, retail and food service managers often are not provided information about previous handling that affects the remaining shelf life at their link of a value chain. The primary goal is to provide fresh produce with desirable qualities such as firmness, color, ripeness, rupture strength, and taste to the consumers. Postharvest value chains require assimilation of systems thinking, systems dynamics, and physiology of the fresh produce. These can be woven together using modeling and simulation games. Various types of models such as mental models, conceptual models, soft systems, and others can be applied. The underlying equations for the models are the value chain dynamics and the physiological changes in produce at varying storage conditions. The models can then be provided to the intended user as a simple spreadsheet model or as visually appealing games or videos, with the simulations running in the background. The models predicting postharvest quality changes could help decision makers alter shipment destinations and storage conditions so that fresh produce arrives with the desired consumer characteristics. Playing simulation games can be an entertaining method for everyone interested in fresh fruits and vegetables to experience the challenges and satisfactions of learning the consequences of decisions they make while playing the role of a manager at each link in a selected chain. Developing a model requires understanding about the interactions within a system and its surroundings. When developing models, information gaps often are found that require research to learn how a system works. Learning continues as users of the model gain experience without the costs or risks of changing real-life situations. Using soft systems methodology to study fresh fruit and vegetable value chains would include developing models of their activities, interconnections, flows of information, and political and social environments. This could improve our understanding of how chains could function as if they were systems. Other types of models would result from using the critical systems practice methodology as a guide for learning the technical, managerial, and social changes necessary to increase per capita consumption, reduce losses and waste, and improve profits for family farms and other global issues related to postharvest handling of fresh produce. Multiplayer simulation games would help managers and leaders learn how to improve entire fresh fruit and vegetable chains.}
}
@article{ARUN2024108905,
title = {Modeling combination therapies in patient cohorts and cell cultures using correlated drug action},
journal = {iScience},
volume = {27},
number = {3},
pages = {108905},
year = {2024},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2024.108905},
url = {https://www.sciencedirect.com/science/article/pii/S2589004224001263},
author = {Adith S. Arun and Sung-Cheol Kim and Mehmet Eren Ahsen and Gustavo Stolovitzky},
keywords = {Computational chemistry, Applied computing},
abstract = {Summary
Characterizing the effect of combination therapies is vital for treating diseases like cancer. We introduce correlated drug action (CDA), a baseline model for the study of drug combinations in both cell cultures and patient populations, which assumes that the efficacy of drugs in a combination may be correlated. We apply temporal CDA (tCDA) to clinical trial data, and demonstrate the utility of this approach in identifying possible synergistic combinations and others that can be explained in terms of monotherapies. Using MCF7 cell line data, we assess combinations with dose CDA (dCDA), a model that generalizes other proposed models (e.g., Bliss response-additivity, the dose equivalence principle), and introduce Excess over CDA (EOCDA), a new metric for identifying possible synergistic combinations in cell culture.}
}
@article{VELICHKOVSKY2020547,
title = {New Insights into the Human Brain’s Cognitive Organization: Views from the Top, from the Bottom, from the Left and, particularly, from the Right},
journal = {Procedia Computer Science},
volume = {169},
pages = {547-557},
year = {2020},
note = {Postproceedings of the 10th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2019 (Tenth Annual Meeting of the BICA Society), held August 15-19, 2019 in Seattle, Washington, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.211},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920303343},
author = {Boris Velichkovsky and Artem Nedoluzhko and Elkhonon Goldberg and Olga Efimova and Fedor Sharko and Sergey Rastorguev and Anna Krasivskaya and Maxim Sharaev and Anastasia Korosteleva and Vadim Ushakov},
keywords = {fMRI, effective connectivity, levels of cognitive organization, dynamic causal modeling (DCM), protein-coding genes, microRNA, gene expression, modes of attention, hemispheric lateralization},
abstract = {The view that the left cerebral hemisphere in humans “dominates” over the “subdominant” right hemisphere has been so deeply entrenched in neuropsychology that no amount of evidence seems able to overcome it. In this article, we examine inhibitory cause-and-effect connectivity among human brain structures related to different parts of the triune evolutionary stratification —archicortex, paleocortex and neocortex— in relation to early and late phases of a prolonged resting-state functional magnetic resonance imaging (fMRI) experiment. With respect to the evolutionarily youngest parts of the human cortex, the left and right frontopolar regions, we also provide data on the asymmetries in underlying molecular mechanisms, namely on the differential expression of the protein-coding genes and regulatory microRNA sequences. In both domains of research, our results contradict the established view by demonstrating a pronounced right-to-left vector of causation in the hemispheric interaction at multiple levels of brain organization. There may be several not mutually exclusive explanations for the evolutionary significance of this pattern of lateralization. One of the explanations emphasizes the computational advantage of separating the neural substrates for processing novel information ("exploration") mediated predominantly by the right hemisphere, and processing with reliance on established cognitive routines and representations ("exploitation") mediated predominantly by the left hemisphere.}
}
@article{FANG2024100632,
title = {An entrepreneurial education game for effectively Tracing the knowledge structure of college students - based on adaptive algorithms},
journal = {Entertainment Computing},
volume = {49},
pages = {100632},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2023.100632},
url = {https://www.sciencedirect.com/science/article/pii/S1875952123000873},
author = {Ming Fang and Yan Liu and Chenbang Hu and Jian Huang and Lei Wu},
keywords = {Adaptive, Knowledge tracking, Entrepreneurship education, Games, Bayesian network},
abstract = {The application of games in the field of education has brought new developments and updated educational needs to games. To maintain and improve students' learning motivation and interest, and more effectively carry out personalized teaching capabilities, research focuses on students' personalized knowledge structure. Starting from tracking the differences in knowledge structures among different learners, a game model based on entrepreneurial education theory has been designed. Through modeling learner knowledge using Bayesian network, the research constructs a game framework of entrepreneurial education for adaptive learning. On this basis, a new feature crossover method is proposed to construct a deep knowledge tracking model based on feature embedding and attention mechanism. It was combined with adaptive learning technology to ultimately construct an entrepreneurial education game model that integrates adaptive learning and improved deep knowledge tracking. A total of 379 students participated in the experiment. The experimental results on the ASSISTments09 open data set show that the area value under the receptivity curve of this model is 0.913, which is 8.8% to 26.6% higher than that of advanced models of the same type. The performance of the research model is optimal on different training scale data. Students' adaptability to entrepreneurship education games is higher than classroom teaching, with a difference between 2% and 11%. The experimental data demonstrates the high applicability of this research method and also achieves the goal of game design, which has certain practical teaching application value.}
}
@article{LOVE2017113,
title = {On languaging and languages},
journal = {Language Sciences},
volume = {61},
pages = {113-147},
year = {2017},
note = {Orders of Language: A festschrift for Nigel Love},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2017.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0388000117301080},
author = {Nigel Love},
keywords = {Languaging, Linguistic reflexivity, Metalanguage, Ontology of languages, Verbatim repetition, Writing},
abstract = {I consider the ontology of languages and the linguistic units said to constitute them, in the light of a speculative sketch of how languaging about language might give rise to the idea of a language. The focus is principally on the role of reflexivity and the development of writing in facilitating the decontextualisation, abstraction and reification of linguistic units and languages themselves. The main trend in modern linguistics has been to take the products of these processes as realia, and to retroject them on to languagers as the basis for their languaging activities: I touch on some of the deleterious effects of this on theorising about the acquisition, storage and production of language. Finally, I consider how in thinking about these matters the concept of different ‘orders’ of language has been and might be interpreted and deployed. Whether or not this concept has a useful role to play in formulating them, the ideas assembled here are offered in the hope that they might serve as a platform from which to debate the significance and implications of the stultifying effect our modes of metalanguaging have so far had on inquiry into our engagement with language.}
}
@article{FOSTER2021101214,
title = {Translating the grid: How a translational approach shaped the development of grid computing},
journal = {Journal of Computational Science},
volume = {52},
pages = {101214},
year = {2021},
note = {Case Studies in Translational Computer Science},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2020.101214},
url = {https://www.sciencedirect.com/science/article/pii/S187775032030510X},
author = {Ian Foster and Carl Kesselman},
keywords = {Translational computer science, Grid computing},
abstract = {A growing gap between progress in biological knowledge and improved health outcomes inspired the new discipline of translational medicine, in which the application of new knowledge is an explicit part of a research plan. Abramson and Parashar argue that a similar gap between complex computational technologies and ever-more-challenging applications demands an analogous discipline of translational computer science, in which the deliberate movement of research results into large-scale practice becomes a central research focus rather than an afterthought. We revisit from this perspective the development and application of grid computing from the mid-1990s onwards, and find that a translational framing is useful for understanding the technology’s development and impact. We discuss how the development of grid computing infrastructure, and the Globus Toolkit, in particular, benefited from a translational approach. We identify lessons learned that can be applied to other translational computer science initiatives.}
}
@incollection{MISTRY2021467,
title = {17 - Bio-inspired design},
editor = {Igor Yadroitsev and Ina Yadroitsava and Anton {du Plessis} and Eric MacDonald},
booktitle = {Fundamentals of Laser Powder Bed Fusion of Metals},
publisher = {Elsevier},
pages = {467-489},
year = {2021},
series = {Additive Manufacturing Materials and Technologies},
isbn = {978-0-12-824090-8},
doi = {https://doi.org/10.1016/B978-0-12-824090-8.00010-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012824090800010X},
author = {Yash Mistry and Daniel Anderson and Dhruv Bhate},
keywords = {Additive manufacturing, Bio-inspired design, Biomimicry, Cellular materials, Complexity, Hierarchy, Laser powder bed fusion, Periodicity, Texture, Topology optimization},
abstract = {Advances in additive manufacturing, computational design tools, and digitization techniques are converging in an exciting new era of engineering design, as humanity has never experienced before. Within this convergent domain, Bio-Inspired Design (BID) is a particularly promising area of research since the potential space for establishing structure-function correlation is vast, and the majority of it is untapped. In this chapter, bio-inspired design is first introduced, specifically in the context of the Laser Powder Bed Fusion (L-PBF) process. Practical approaches for implementing BID for L-PBF are discussed, followed by a discussion of key general design concepts that can be abstracted for BID. Examples of how BID and L-PBF have been combined are then presented, followed by a consideration of some of the most important design constraints posed by the L-PBF process that the bio-inspired designer needs to be aware of. The chapter concludes with a forward looking discussion of opportunities in this domain.}
}
@article{WOODWARD2006631,
title = {Does prior mathematics knowledge really lead to variation in elementary statistics performance? Evidence from a developing country},
journal = {International Journal of Educational Development},
volume = {26},
number = {6},
pages = {631-639},
year = {2006},
issn = {0738-0593},
doi = {https://doi.org/10.1016/j.ijedudev.2006.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0738059306000071},
author = {George Woodward and Don Galagedera},
keywords = {Curriculum, Mathematics, Educational policy, Elementary statistics},
abstract = {A model incorporating prerequisite mathematics performance and other variables deemed to be associated with learning elementary statistics (ES) is developed. The relationship between ES performance and the explanatory variables is well represented by the logistics form. Aptitude, effort and motivation are the only significant explanatory variables of ES performance. Since prerequisite mathematics is not significant, statistical thinking at the tertiary level may be mostly intuitive and non-mathematical. Students with low aptitude experience increasing returns to effort over the first half of the feasible effort interval, while high-aptitude students experience diminishing returns at all levels of effort. The levels of effort required to achieve a minimum pass are interpreted.}
}
@article{DANILOV2019108891,
title = {On the geometric origin of spurious waves in finite-volume discretizations of shallow water equations on triangular meshes},
journal = {Journal of Computational Physics},
volume = {398},
pages = {108891},
year = {2019},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2019.108891},
url = {https://www.sciencedirect.com/science/article/pii/S0021999119305893},
author = {S. Danilov and A. Kutsenko},
keywords = {Triangular meshes, Finite volume discretization, Computational dispersion branches},
abstract = {Computational wave branches are common to linearized shallow water equations discretized on triangular meshes. It is demonstrated that for standard finite-volume discretizations these branches can be traced back to the structure of the unit cell of triangular lattice, which includes two triangles with a common edge. Only subsets of similarly oriented triangles or edges possess the translational symmetry of unit cell. As a consequence, discrete degrees of freedom placed on triangles or edges are geometrically different, creating an internal structure inside unit cells. It implies a possibility of oscillations inside unit cells seen as computational branches in the framework of linearized shallow water equations, or as grid-scale noise generally. Adding dissipative operators based on smallest stencils to discretized equations is needed to control these oscillations in solutions. A review of several finite-volume discretization is presented with focus on computational branches and dissipative operators.}
}
@article{RODRIGUEZMENDEZ2024103619,
title = {UK Net Zero policy design and deep uncertainty – The need for an alternative approach},
journal = {Environmental Science & Policy},
volume = {151},
pages = {103619},
year = {2024},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2023.103619},
url = {https://www.sciencedirect.com/science/article/pii/S146290112300268X},
author = {Quirina {Rodriguez Mendez} and Mark Workman and Geoff Darch},
keywords = {United Kingdom Net Zero target, Carbon dioxide removal, Climate modelling, Integrated assessment modelling, Robust decision making, exploratory modelling},
abstract = {The majority of global emissions scenarios compatible with holding global warming to less than 2 °C depend on the large-scale use of Greenhouse Gas Removal (GGR) technologies. Recent critiques have highlighted the concerns of building long-term climate policy on such speculative technological scenarios emerging from orthodox modelling approaches – including integrated assessment modelling prominently assessed by the IPCC. Through a stakeholder consultation process with the UK modelling and policy community, we critically examine the integration of GGR technologies into UK Net Zero scenarios and the decision-making philosophy underlying the use of orthodox modelling to inform UK climate policy. We identify a number of features of orthodox modelling approaches which are unable to manage the pervasive extent of deep uncertainty in possible UK climate and energy futures. We further argue that a more fundamental issue lies in the way that the models are used by UK climate policy makers: that the handling of uncertainties which pervade the integration of GGR into Net Zero policy are resulting in substantial distortions in net-zero policy design and associated decision-making. Drawing on the principles of decision-making under deep uncertainty techniques, exemplified by Robust Decision Making, we recommend an alternative approach that explicitly embraces uncertainty, multiple values and diversity among stakeholders and viewpoints, and in which modelling exists in an iterative exchange with policy development rather than separate from it. We advocate that such an approach would provide more relevant and robust information to near-term policymaking and enable an inclusive societal dialogue about the appropriate role of GGR within UK climate policy.}
}
@article{BRAITHWAITE201640,
title = {Non-formal mechanisms in mathematical cognitive development: The case of arithmetic},
journal = {Cognition},
volume = {149},
pages = {40-55},
year = {2016},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2016.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S001002771630004X},
author = {David W. Braithwaite and Robert L. Goldstone and Han L.J. {van der Maas} and David H. Landy},
keywords = {Mathematical cognitive development, Concrete to abstract shift, Arithmetic, Syntax, Perception, Mathematics education},
abstract = {The idea that cognitive development involves a shift towards abstraction has a long history in psychology. One incarnation of this idea holds that development in the domain of mathematics involves a shift from non-formal mechanisms to formal rules and axioms. Contrary to this view, the present study provides evidence that reliance on non-formal mechanisms may actually increase with age. Participants – Dutch primary school children – evaluated three-term arithmetic expressions in which violation of formally correct order of evaluation led to errors, termed foil errors. Participants solved the problems as part of their regular mathematics practice through an online study platform, and data were collected from over 50,000 children representing approximately 10% of all primary schools in the Netherlands, suggesting that the results have high external validity. Foil errors were more common for problems in which formally lower-priority sub-expressions were spaced close together, and also for problems in which such sub-expressions were relatively easy to calculate. We interpret these effects as resulting from reliance on two non-formal mechanisms, perceptual grouping and opportunistic selection, to determine order of evaluation. Critically, these effects reliably increased with participants’ grade level, suggesting that these mechanisms are not phased out but actually become more important over development, even when they cause systematic violations of formal rules. This conclusion presents a challenge for the shift towards abstraction view as a description of cognitive development in arithmetic. Implications of this result for educational practice are discussed.}
}
@article{GRANT2024104197,
title = {On measuring inconsistency in graph databases with regular path constraints},
journal = {Artificial Intelligence},
volume = {335},
pages = {104197},
year = {2024},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2024.104197},
url = {https://www.sciencedirect.com/science/article/pii/S0004370224001334},
author = {John Grant and Francesco Parisi},
keywords = {Inconsistency measures, Graph databases, Computational complexity},
abstract = {Real-world data are often inconsistent. Although a substantial amount of research has been done on measuring inconsistency, this research concentrated on knowledge bases formalized in propositional logic. Recently, inconsistency measures have been introduced for relational databases. However, nowadays, real-world information is always more frequently represented by graph-based structures which offer a more intuitive conceptualization than relational ones. In this paper, we explore inconsistency measures for graph databases with regular path constraints, a class of integrity constraints based on a well-known navigational language for graph data. In this context, we define several inconsistency measures dealing with specific elements contributing to inconsistency in graph databases. We also define some rationality postulates that are desirable properties for an inconsistency measure for graph databases. We analyze the compliance of each measure with each postulate and find various degrees of satisfaction; in fact, one of the measures satisfies all the postulates. Finally, we investigate the data and combined complexity of the calculation of all the measures as well as the complexity of deciding whether a measure is lower than, equal to, or greater than a given threshold. It turns out that for a majority of the measures these problems are tractable, while for the other different levels of intractability are exhibited.}
}
@article{LIN202052,
title = {A novel deep neural network based approach for sparse code multiple access},
journal = {Neurocomputing},
volume = {382},
pages = {52-63},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.11.066},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219316686},
author = {Jinzhi Lin and Shengzhong Feng and Yun Zhang and Zhile Yang and Yong Zhang},
keywords = {Sparse code multiple access, Non-orthogonal multiple access, Machine learning, Dense code multiple access},
abstract = {Sparse code multiple access (SCMA) has been one of the non-orthogonal multiple access (NOMA) schemes aiming to support high spectral efficiency and ubiquitous access requirements for 5G communication networks. Conventional SCMA approaches are confronting challenges in designing low-complexity high-accuracy decoding algorithm and constructing optimum codebooks. Fortunately, the recent spotlighted deep learning technologies are of significant potentials in solving many communication engineering problems. Inspired by this, we propose and train a deep neural network (DNN) called DL-SCMA to learn to decode SCMA modulated signals corrupted by additive white Gaussian noise (AWGN). An autoencoder called AE-SCMA is established and trained to generate optimal SCMA codewords and reconstruct original bits. Furthermore, by manipulating the mapping vectors, an autoencoder is able to generalize SCMA, thus a dense code multiple access (DCMA) scheme is proposed. Simulations show that the DNN SCMA decoder significantly outperforms the conventional message passing algorithm (MPA) in terms of bit error rate (BER), symbol error rate (SER) and computational complexity, and AE-SCMA also demonstrates better performances via constructing better SCMA codebooks. The performance of deep learning aided DCMA is superior to the SCMA.}
}
@article{DANNENHAUER2014226,
title = {Toward Meta-level Control of Autonomous Agents},
journal = {Procedia Computer Science},
volume = {41},
pages = {226-232},
year = {2014},
note = {5th Annual International Conference on Biologically Inspired Cognitive Architectures, 2014 BICA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.11.107},
url = {https://www.sciencedirect.com/science/article/pii/S187705091401552X},
author = {Dustin Dannenhauer and Michael T. Cox and Shubham Gupta and Matt Paisner and Don Perlis},
keywords = {Computational metacognition, cognitive architecture, metareasoning, long-duration autonomy},
abstract = {Metareasoning is an important capability for autonomous systems, particularly for those being deployed on long duration missions. An agent with increased self-observation and the ability to control itself in response to changing environments will be more capable in achieving its goals. This is essential for long-duration missions where system designers will not be able to, theoretically or practically, predict all possible problems that the agent may encounter. In this paper we describe preliminary work that integrates the metacognitive architecture MIDCA with an autonomous TREX agent, creating a more self-observable and adaptive agent.}
}
@article{COOPER20091351,
title = {Emergence as a computability-theoretic phenomenon},
journal = {Applied Mathematics and Computation},
volume = {215},
number = {4},
pages = {1351-1360},
year = {2009},
note = {Physics and Computation},
issn = {0096-3003},
doi = {https://doi.org/10.1016/j.amc.2009.04.050},
url = {https://www.sciencedirect.com/science/article/pii/S0096300309004159},
author = {S. Barry Cooper},
keywords = {Computability, Emergence, Definability, Turing invariance},
abstract = {In dealing with emergent phenomena, a common task is to identify useful descriptions of them in terms of the underlying atomic processes, and to extract enough computational content from these descriptions to enable predictions to be made. Generally, the underlying atomic processes are quite well understood, and (with important exceptions) captured by mathematics from which it is relatively easy to extract algorithmic content. A widespread view is that the difficulty in describing transitions from algorithmic activity to the emergence associated with chaotic situations is a simple case of complexity outstripping computational resources and human ingenuity. Or, on the other hand, that phenomena transcending the standard Turing model of computation, if they exist, must necessarily lie outside the domain of classical computability theory. In this talk we suggest that much of the current confusion arises from conceptual gaps and the lack of a suitably fundamental model within which to situate emergence. We examine the potential for placing emergent relations in a familiar context based on Turing’s 1939 model for interactive computation over structures described in terms of reals. The explanatory power of this model is explored, formalising informal descriptions in terms of mathematical definability and invariance, and relating a range of basic scientific puzzles to results and intractable problems in computability theory.}
}
@article{KUIPERS2008155,
title = {Drinking from the firehose of experience},
journal = {Artificial Intelligence in Medicine},
volume = {44},
number = {2},
pages = {155-170},
year = {2008},
note = {Artificial Consciousness},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2008.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0933365708000985},
author = {Benjamin Kuipers},
keywords = {Consciousness, Sensory trackers, Information content, Dynamical systems},
abstract = {Summary
Objective
Computational concepts from robotics and computer vision hold great promise to account for major aspects of the phenomenon of consciousness, including philosophically problematical aspects such as the vividness of qualia, the first-person character of conscious experience, and the property of intentionality.
Methods
We present a dynamical systems model describing human or robotic agents and their interaction with the environment. In order to cope with the enormous information content of the sensory stream, this model includes trackers for selected coherent spatio–temporal portions of the sensory input stream, and a self-constructed plausible coherent narrative describing the recent history of the agent’s sensorimotor interaction with the world.
Results
We describe how an agent can autonomously learn its own intentionality by constructing computational models of hypothetical entities in the external world. These models explain regularities in the sensorimotor interaction, and serve as referents for the agent’s symbolic knowledge representation. The high information content of the sensory stream allows the agent to continually evaluate these hypothesized models, refuting those that make poor predictions. The high information content of the sensory input stream also accounts for the vividness and uniqueness of subjective experience. We then evaluate our account against 11 features of consciousness “that any philosophical–scientific theory should hope to explain”, according to the philosopher and prominent AI critic John Searle.
Conclusion
The essential features of consciousness can, in principle, be implemented on a robot with sufficient computational power and a sufficiently rich sensorimotor system, embodied and embedded in its environment.}
}
@incollection{AKIMOV2024235,
title = {Fundamentals of Trajectory-Based Methods for Nonadiabatic Dynamics},
editor = {Manuel Yáñez and Russell J. Boyd},
booktitle = {Comprehensive Computational Chemistry (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {235-272},
year = {2024},
isbn = {978-0-12-823256-9},
doi = {https://doi.org/10.1016/B978-0-12-821978-2.00034-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128219782000349},
author = {Alexey V. Akimov},
keywords = {Coupled trajectories, Decoherence, Ehrenfest, Liouville equation, Mean-field, Nonadiabatic dynamics, Quantum-classical, Trajectory surface hopping},
abstract = {In this chapter, I outline the fundamental concepts and frameworks used to construct trajectory-based methods of nonadiabatic dynamics simulations. The chapter is organized according to a common anatomy of multiple trajectory-based approaches rather than as a collection of accounts on existing methods. It highlights the natural evolution of the ideas behind these methods and provides glimpses into a more than 30-years history of such developments. Many reported computational schemes are composed of common methodological “building blocks” – the groups of algorithms tailored to various types of calculations involved in computational algorithms. The chapter overviews a wide variety of possibilities to conduct one or another part of such calculations and that can be regarded the methodological “building blocks”.}
}
@article{LATHA2019122052,
title = {Analysing exposure diversity in collaborative recommender systems—Entropy fusion approach},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {533},
pages = {122052},
year = {2019},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2019.122052},
url = {https://www.sciencedirect.com/science/article/pii/S0378437119311963},
author = {R. Latha and R. Nadarajan},
keywords = {Clustering, Quadratic entropy, Exposure diversity, Novelty, Concordance},
abstract = {Recommender Systems are considered as essential business tools to leverage the potential growth of on-line services. Neighbourhood based collaborative filtering, a successful recommendation approach has mainly focused on improving accuracy of predictions. From user point of view, it is more valuable to obtain novel and diverse recommendations rather than monotonic preferences. Ratings given by a user for different categories of items are considered as a tool to access user exposure diversity which signifies his creative and divergent thinking. On the other hand, pair of items is concordant if highly correlated users agree in rating the items. Based on the user exposure diversity and item concordance, the neighbourhood selection process of item based collaborative recommender systems is refined. Rating predictions are made based on the newly selected neighbours. The performance of the proposed approach is investigated for accuracy and diversity of predictions on Movielens data sets. The results demonstrate that the proposed approach outperforms the state of the art recommendation approaches which address accuracy–diversity trade off. Statistical analysis is done to prove the efficiency of the proposed approach.}
}
@article{LUO2020151,
title = {Three-way decision with incomplete information based on similarity and satisfiability},
journal = {International Journal of Approximate Reasoning},
volume = {120},
pages = {151-183},
year = {2020},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2020.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X19303421},
author = {Junfang Luo and Mengjun Hu and Keyun Qin},
keywords = {Three-way decision, Rough set, Incomplete information, Similarity, Satisfiability, Fuzzy logic},
abstract = {Three-way decision is widely applied with rough set theory to learn classification or decision rules. The approaches dealing with complete information are well established in the literature, including the two complementary computational and conceptual formulations. The computational formulation uses equivalence relations, and the conceptual formulation uses satisfiability of logic formulas. In this paper, based on a brief review of these two formulations, we generalize both formulations into three-way decision with incomplete information that is more practical in real-world applications. For the computational formulation, we propose a new measure of similarity degree of objects as a generalization of equivalence relations. Based on it, we discuss two approaches to three-way decision using α-similarity classes and approximability of objects, respectively. For the conceptual formulation, we propose a measure of satisfiability degree of formulas as a quantitative generalization of satisfiability with complete information. Based on it, we study two approaches to three-way decision using α-meaning sets of formulas and confidence of formulas, respectively. While using similarity classes is a common method of analyzing incomplete information in the literature, the proposed concept of approximability and the two approaches in conceptual formulation point out new promising directions.}
}
@article{WEI2022232125,
title = {Machine learning for battery research},
journal = {Journal of Power Sources},
volume = {549},
pages = {232125},
year = {2022},
issn = {0378-7753},
doi = {https://doi.org/10.1016/j.jpowsour.2022.232125},
url = {https://www.sciencedirect.com/science/article/pii/S0378775322011028},
author = {Zheng Wei and Qiu He and Yan Zhao},
keywords = {Machine learning, Battery materials, Battery state prediction},
abstract = {Batteries are vital energy storage carriers in industry and in our daily life. There is continued interest in the developments of batteries with excellent service performance and safety. Traditional trial-and-error experimental approaches have the limitations of high-cost and low-efficiency. Atomistic computational simulations are relatively expensive and take long time to screen massive materials. The rapid development of machine learning (ML) has brought innovations in many fields and has also changed the paradigm of the battery research. Numerous ML applications have emerged in the battery community, such as novel materials discovery, property prediction, and characterization. In this review, we introduced the workflow of ML, where the task, data, feature engineering, and evaluation were involved. Several typical ML models used in batteries were highlighted. In addition, we summarized the applications of ML for the discovery of novel materials, and for property and battery state prediction. The challenges for the application of ML in batteries were also discussed.}
}
@article{PICCININI2008311,
title = {Some neural networks compute, others don’t},
journal = {Neural Networks},
volume = {21},
number = {2},
pages = {311-321},
year = {2008},
note = {Advances in Neural Networks Research: IJCNN ’07},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2007.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S089360800700250X},
author = {Gualtiero Piccinini},
keywords = {Connectionism, Neural network, Computation, Mechanism, Cognition, Brain},
abstract = {I address whether neural networks perform computations in the sense of computability theory and computer science. I explicate and defend the following theses. (1) Many neural networks compute—they perform computations. (2) Some neural networks compute in a classical way. Ordinary digital computers, which are very large networks of logic gates, belong in this class of neural networks. (3) Other neural networks compute in a non-classical way. (4) Yet other neural networks do not perform computations. Brains may well fall into this last class.}
}
@article{PFEIFER199547,
title = {Cognition — perspectives from autonomous agents},
journal = {Robotics and Autonomous Systems},
volume = {15},
number = {1},
pages = {47-70},
year = {1995},
note = {The Biology and Technology of Intelligent Autonomous Agents},
issn = {0921-8890},
doi = {https://doi.org/10.1016/0921-8890(95)00014-7},
url = {https://www.sciencedirect.com/science/article/pii/0921889095000147},
author = {Rolf Pfeifer},
keywords = {Cognition, Autonomous agents, Cheap designs, “New AI”},
abstract = {The predominant paradigm in cognitive science has been the cognitivistic one, exemplified by the “Physical Symbol Systems Hypothesis”. The cognitivistic approach generated hopes that one would soon understand human thinking — hopes that up till now have still not been fulfilled. It is well-known that the cognitivistic approach, in spite of some early successes, has turned out to be fraught with problems. Examples are the frame problem, the symbol grounding problem, and the problems of interacting with a real physical world. In order to come to grips with the problems of cognitivism the study of embodied autonomous systems has been proposed, for example by Rodney Brooks. Brooks' robots can get away with no or very little representation. However, this approach has often been criticized because of the limited abilities of the agents. If they are to perform more intelligent tasks they will need to be equipped with representations or cognition — is an often heard argument. We will illustrate that we are well-advised not to introduce representational or cognitive concepts too quickly. As long as we do not understand the basic relationships between simple architectures and behavior, i.e. as long as we do not understand the dynamics of the system-environment interaction, it is premature to spend our time with speculations about potentially useful architectures for so-called high-level processes. This paper has a tutorial and a review aspect. In addition to presenting our own research, we will review some of the pertinent literature in order to make it usable as an introduction to “New AI”.}
}
@article{SILVA2023200252,
title = {Boosting the performance of SOTA convolution-based networks with dimensionality reduction: An application on hyperspectral images of wine grape berries},
journal = {Intelligent Systems with Applications},
volume = {19},
pages = {200252},
year = {2023},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2023.200252},
url = {https://www.sciencedirect.com/science/article/pii/S2667305323000777},
author = {Rui Silva and Osvaldo {Gramaxo Freitas} and Pedro Melo-Pinto},
keywords = {Hyperspectral images, Wine grape berries, Oenological parameters, InceptionTime, OmniScale 1D-CNN, Dimensionality reduction},
abstract = {Precision viticulture is an area that is very dependent on methods that allow for a sustainable assessment of grape maturity and, in this work, we apply two state-of-the-art (SOTA) convolution-based networks, namely InceptionTime and OmniScale 1D-CNN, to hyperspectral images of wine grape berries to estimate sugar content. Since attaining generalization capacity and processing the information in such high-dimensional data are the two biggest challenges to overcome in problems of this nature, we also study the impact of two dimensionality reduction techniques, Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE), on the models' performance. Both models underwent different tests with different vintages and varieties of wine grapes in the training/validation steps, as to form a true test to their generalization capacity. Our results show that both PCA and t-SNE succeed in improving the performance of these deep networks when an adequate number of components is chosen that minimizes the ratio between information loss and removing redundant features: additionally, both techniques significantly reduce computational cost, a very important trait when training deep learning models. Both models showed good generalization ability with very competitive results across different varieties and vintages even despite their significant differences in variability, which is an indicator that a relationship between spectras can be found that is reflected on sugar content values.}
}
@incollection{CELKO2008255,
title = {Chapter 13 - Turning Specifications into Code},
editor = {Joe Celko},
booktitle = {Joe Celko's Thinking in Sets},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {255-271},
year = {2008},
series = {The Morgan Kaufmann Series in Data Management Systems},
isbn = {978-0-12-374137-0},
doi = {https://doi.org/10.1016/B978-012374137-0.50014-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780123741370500146},
author = {Joe Celko},
abstract = {Publisher Summary
This chapter delineates the importance of unlearning the procedural thinking and moves to a pure SQL view. Programmers tend to make the same kinds of errors in their designs and their code over and over. They confuse RDBMS with the file systems and 3GL- or OO-oriented programming environments they first learned. Programmers from the C family of languages tend to put the entire program in lowercase as if they were still using a teletype on a UNIX system. Mainframe programmers tend to put the entire program in uppercase as if they were still using punch cards or a 3270 video monitor for input. Cohesion is how well a module of code does one and only one thing, that it is logically coherent. There are several types of cohesion. The original definitions have been extended from procedural code to include OO and class hierarchies. The symptom in DDL is a table with lots of NULL-able columns. It is probably two or more entities crammed into a single table. The symptom in DML is a query or other statement that tries to do too many things. When the same procedure or query checks inventory and build a personnel report, cohesion problems crop up. The table-valued function shows that the programmer still wants to see procedural coding complete with parameters. An SQL programmer would think in terms of VIEWS and CTES.}
}
@article{GATI2021298,
title = {Differentially private data fusion and deep learning Framework for Cyber–Physical–Social Systems: State-of-the-art and perspectives},
journal = {Information Fusion},
volume = {76},
pages = {298-314},
year = {2021},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2021.04.017},
url = {https://www.sciencedirect.com/science/article/pii/S1566253521000890},
author = {Nicholaus J. Gati and Laurence T. Yang and Jun Feng and Xin Nie and Zhian Ren and Samwel K. Tarus},
keywords = {Differential privacy, Deep computation, Data fusion, CPSS},
abstract = {The modern technological advancement influences the growth of the cyber–physical system and cyber–social system to a more advanced computing system cyber–physical–social system (CPSS). Therefore, CPSS leads the data science revolution by promoting tri-space information resource from a single space. The establishment of CPSSs increases the related privacy concerns. To provide privacy on CPSSs data, various privacy-preserving schemes have been introduced in the recent past. However, technological advancement in CPSSs requires the modifications of previous techniques to suit its dynamics. Meanwhile, differential privacy has emerged as an effective method to safeguard CPSSs data privacy. To completely comprehend the state-of-the-art developments and learn the field’s research directions, this article provides a comprehensive review of differentially private data fusion and deep learning in CPSSs. Additionally, we present a novel differentially private data fusion and deep learning Framework for Cyber–Physical–Social Systems , and various future research directions for CPSSs.}
}
@article{BEHESHTIANARDEKANI1988183,
title = {An empirical study of the use of business expert systems},
journal = {Information & Management},
volume = {15},
number = {4},
pages = {183-190},
year = {1988},
issn = {0378-7206},
doi = {https://doi.org/10.1016/0378-7206(88)90044-4},
url = {https://www.sciencedirect.com/science/article/pii/0378720688900444},
author = {Mehdi Beheshtian-Ardekani and Linda M. Salchenberger},
keywords = {Business expert systems, Artificial intelligence, Knowledge-based systems, Fifth-generation},
abstract = {The evolution of computers from computational tools to “thinking machines” is causing businesses to evaluate their views of the computer's role. The inevitable availability of smart computers leads to questions of how and when fifth generation hardware and software will be integrated into corporate culture. Here, we present the results of a survey given to information systems managers to determine the extent of expert systems development by data processing departments and expert systems usage in organizations. The attitudes of management toward the future of expert systems are also discussed using the survey data. It was discovered that, while computer managers are receptive toward this new tool, most have no definite plans to develop expert systems in the near future. These results seem to be in conflict with other evidence about the growing numbers of expert systems in business applications. One explanation is that this new technology is part of the continuing “grass roots” movement of end-user computing.}
}
@article{STOCK2024129942,
title = {Exploring crossing times and congestion patterns at scramble intersections in pedestrian dynamics models: A statistical analysis},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {649},
pages = {129942},
year = {2024},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2024.129942},
url = {https://www.sciencedirect.com/science/article/pii/S0378437124004515},
author = {Eduardo V. Stock and Roberto {da Silva}},
keywords = {Pedestrian dynamics, Scramble intersections, Stochastic process, Agent-based models, Complex mixed pedestrian flow, Monte Carlo simulations},
abstract = {In this study, we present a model designed to unravel the statistical complexities of pedestrian crossing times and related physical quantities in a unique pedestrian intersection, specifically a double intersecting diagonal crossing. Our model employs an agent-based stochastic process based on a lattice gas dynamics to describe a four-species interaction on an underlying square lattice representing the intersection. This model incorporates four static floor fields guiding each species within the scenario. Our findings emphasize the critical role that the density of highly driven pedestrians plays in the statistics of crossing times by triggering the transition from a normal distribution for low densities to a uniform distribution for a crowded scenario. In the steady state, we mapped the system mobility and showed that an anomalous mobile phase emerges when considering a scenario with high density of poorly driven pedestrians. To monitor these phenomena, we focus on two variables: directed mobility and average distance between agents. We conclude our analysis by overcoming a crucial limitation of the lattice gas model with the introduction of viscosity effects among pedestrians. We observe that mixed state regime emerges as the system presents a mobile-to-immobile phase transition when viscosity effects faints. Our research sheds light on the nuanced dynamics of scramble intersections, contributing to a deeper understanding of urban mobility challenges.}
}
@incollection{DALY20173,
title = {8.02 - Molecular Logic Gates as Fluorescent Sensors},
editor = {Jerry L. Atwood},
booktitle = {Comprehensive Supramolecular Chemistry II},
publisher = {Elsevier},
address = {Oxford},
pages = {3-19},
year = {2017},
isbn = {978-0-12-803199-5},
doi = {https://doi.org/10.1016/B978-0-12-409547-2.12626-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095472126265},
author = {B. Daly and V.A.D. Silverson and C.Y. Yao and Z.Q. Chen and A.P. {de Silva}},
keywords = {AND Logic, Fluorescent Sensors, IMPLICATION Logic, INHIBIT Logic, Intracellular AND Logic, Logic Gates, NAND Logic, XOR and XNOR Logic, YES Logic},
abstract = {Some recent developments in the use of molecular logic gates as fluorescent sensors are described. The discussion is classified in terms of the Boolean logical assignment of the sensor system. Even simple fluorescent sensors can be recognized as single-input logic gates. Several YES gates launch the analysis of examples. A consideration of various sensors driven by double inputs and higher multiple inputs then follows. Attention is particularly drawn to the appearance of double-input logical sensor molecules, which successfully operate within living cells—a milieu where conventional semiconductor-based logic devices would struggle on the grounds of compatibility and size. The value of molecular logical thinking in the understanding of fluorescent sensor behavior is emphasized throughout.}
}
@article{KALYAN2024100048,
title = {A survey of GPT-3 family large language models including ChatGPT and GPT-4},
journal = {Natural Language Processing Journal},
volume = {6},
pages = {100048},
year = {2024},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2023.100048},
url = {https://www.sciencedirect.com/science/article/pii/S2949719123000456},
author = {Katikapalli Subramanyam Kalyan},
keywords = {Large language models, LLMs, GPT-3, ChatGPT, GPT-4, Transformers, LLM survey},
abstract = {Large language models (LLMs) are a special class of pretrained language models (PLMs) obtained by scaling model size, pretraining corpus and computation. LLMs, because of their large size and pretraining on large volumes of text data, exhibit special abilities which allow them to achieve remarkable performances without any task-specific training in many of the natural language processing tasks. The era of LLMs started with OpenAI’s GPT-3 model, and the popularity of LLMs has increased exponentially after the introduction of models like ChatGPT and GPT4. We refer to GPT-3 and its successor OpenAI models, including ChatGPT and GPT4, as GPT-3 family large language models (GLLMs). With the ever-rising popularity of GLLMs, especially in the research community, there is a strong need for a comprehensive survey which summarizes the recent research progress in multiple dimensions and can guide the research community with insightful future research directions. We start the survey paper with foundation concepts like transformers, transfer learning, self-supervised learning, pretrained language models and large language models. We then present a brief overview of GLLMs and discuss the performances of GLLMs in various downstream tasks, specific domains and multiple languages. We also discuss the data labelling and data augmentation abilities of GLLMs, the robustness of GLLMs, the effectiveness of GLLMs as evaluators, and finally, conclude with multiple insightful future research directions. To summarize, this comprehensive survey paper will serve as a good resource for both academic and industry people to stay updated with the latest research related to GLLMs.}
}
@article{VODOVOTZ2017116,
title = {Solving Immunology?},
journal = {Trends in Immunology},
volume = {38},
number = {2},
pages = {116-127},
year = {2017},
issn = {1471-4906},
doi = {https://doi.org/10.1016/j.it.2016.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S1471490616302022},
author = {Yoram Vodovotz and Ashley Xia and Elizabeth L. Read and Josep Bassaganya-Riera and David A. Hafler and Eduardo Sontag and Jin Wang and John S. Tsang and Judy D. Day and Steven H. Kleinstein and Atul J. Butte and Matthew C. Altman and Ross Hammond and Stuart C. Sealfon},
keywords = {mathematical modeling, conference, autoimmune disease, personalized medicine, translation},
abstract = {Emergent responses of the immune system result from the integration of molecular and cellular networks over time and across multiple organs. High-content and high-throughput analysis technologies, concomitantly with data-driven and mechanistic modeling, hold promise for the systematic interrogation of these complex pathways. However, connecting genetic variation and molecular mechanisms to individual phenotypes and health outcomes has proven elusive. Gaps remain in data, and disagreements persist about the value of mechanistic modeling for immunology. Here, we present the perspectives that emerged from the National Institute of Allergy and Infectious Disease (NIAID) workshop ‘Complex Systems Science, Modeling and Immunity’ and subsequent discussions regarding the potential synergy of high-throughput data acquisition, data-driven modeling, and mechanistic modeling to define new mechanisms of immunological disease and to accelerate the translation of these insights into therapies.}
}
@incollection{MACLENNAN201584,
title = {Cognitive Modeling: Connectionist Approaches},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {84-89},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.43021-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868430217},
author = {Bruce MacLennan},
keywords = {Artificial intelligence, Backpropagation, Computability, Computational map, Connectionism, Correlational learning, Dynamic systems approach, Embodiment, Language of thought, Machine learning, Neural network, Perceptron, Representation, Situatedness, Subsymbolic},
abstract = {Connectionist approaches to cognitive modeling make use of large networks of simple computational units, which communicate by means of simple quantitative signals. Higher-level information processing emerges from the massively parallel interaction of these units by means of their connections, and a network may adapt its behavior by means of local changes in the strength of the connections. Connectionist approaches are related to neural networks and provide a distinct alternative to cognitive models inspired by the digital computer.}
}
@article{MICHIE19931,
title = {Turing's test and conscious thought},
journal = {Artificial Intelligence},
volume = {60},
number = {1},
pages = {1-22},
year = {1993},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(93)90032-7},
url = {https://www.sciencedirect.com/science/article/pii/0004370293900327},
author = {Donald Michie},
abstract = {Over forty years ago A.M. Turing proposed a test for intelligence in machines. Based as it is solely on an examinee's verbal responses, the Test misses some important components of human thinking. To bring these manifestations within its scope, the Turing Test would require substantial extension. Advances in the application of AI methods in the design of improved human-computer interfaces are now focussing attention on machine models of thought and knowledge from the altered standpoint of practical utility.}
}
@incollection{BOBIS2023518,
title = {Situating teacher learning in the mathematics classroom and everyday practice},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {518-527},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.04062-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305040628},
author = {Janette Bobis and Kristen Tripet},
keywords = {Anticipating, Hypothetical learning trajectory, Intentional noticing, Mathematics, Professional learning, Situated learning, Teacher learning},
abstract = {In this paper we explore the emerging field of teacher noticing as a vehicle for on-going professional learning that is situated in everyday mathematics classrooms. We build on theory and research surrounding teacher learning to argue for a set of practices to guide their noticing of what matters most and through which they learn from to improve their teaching. A lesson planning protocol is proposed with the aim of supporting teachers as they learn to embed these practices into their everyday work of teaching.}
}
@article{BAMU20051794,
title = {Damage, deterioration and the long-term structural performance of cooling-tower shells: A survey of developments over the past 50 years},
journal = {Engineering Structures},
volume = {27},
number = {12},
pages = {1794-1800},
year = {2005},
note = {SEMC 2004 Structural Health Monitoring, Damage Detection and Long-Term Performance},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2005.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0141029605002257},
author = {P.C. Bamu and A. Zingoni},
keywords = {Cooling towers, Shell structures, Long-term performance, Damage modelling, Deterioration phenomena, Concrete cracking, Shell imperfections, Durability},
abstract = {The last 50 years have seen a gradual shift in trend in research on concrete hyperbolic cooling-tower shells, from the issues of response to short-term loading and immediate causes of collapse in the early part of this period, to the issues of deterioration phenomena, durability and long-term performance in more recent times. This paper traces these developments. After a revisit of some historical collapses of cooling-tower shells, and a brief consideration of condition surveys and repair programmes instituted in the aftermath of these events, focus shifts to the important question of damage and deterioration, and progress made over the past 30 years in the understanding of these phenomena. In particular, much research has gone into the modelling of cracking and geometric imperfections, which have a considerable effect on the load-carrying capacity of the shell, and are also manifestations of long-term deterioration. While structural monitoring of the progression of deterioration in cooling-tower shells, and the accurate prediction of this through appropriate numerical models, will always be important, the thinking now seems to be shifting towards designing for durability right from the outset.}
}
@article{MULDNER2015127,
title = {Utilizing sensor data to model students’ creativity in a digital environment},
journal = {Computers in Human Behavior},
volume = {42},
pages = {127-137},
year = {2015},
note = {Digital Creativity: New Frontier for Research and Practice},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2013.10.060},
url = {https://www.sciencedirect.com/science/article/pii/S074756321300410X},
author = {Kasia Muldner and Winslow Burleson},
keywords = {Creativity, Student modeling, Eye tracking, EEG, Skin conductance, Intelligent Tutoring Systems},
abstract = {While creativity is essential for developing students’ broad expertise in Science, Technology, Engineering, and Math (STEM) fields, many students struggle with various aspects of being creative. Digital technologies have the unique opportunity to support the creative process by (1) recognizing elements of students’ creativity, such as when creativity is lacking (modeling step), and (2) providing tailored scaffolding based on that information (intervention step). However, to date little work exists on either of these aspects. Here, we focus on the modeling step. Specifically, we explore the utility of various sensing devices, including an eye tracker, a skin conductance bracelet, and an EEG sensor, for modeling creativity during an educational activity, namely geometry proof generation. We found reliable differences in sensor features characterizing low vs. high creativity students. We then applied machine learning to build classifiers that achieved good accuracy in distinguishing these two student groups, providing evidence that sensor features are valuable for modeling creativity.}
}
@article{GAUR2023102165,
title = {Artificial intelligence for carbon emissions using system of systems theory},
journal = {Ecological Informatics},
volume = {76},
pages = {102165},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102165},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123001942},
author = {Loveleen Gaur and Anam Afaq and Gursimar Kaur Arora and Nabeel Khan},
keywords = {Artificial intelligence, machine learning, Carbon emission, System of systems theory, Sustainability, Carbon footprint},
abstract = {The impact of artificial intelligence (AI) on the environment is the subject of discourse, with arguments for both positive and negative effects. There is a fine line between AI for good and AI for environmental degradation. Today, companies want to seize the benefits of AI, which distinctively involves reducing the company's carbon footprint. However, AI's carbon emissions differ as per the techniques involved in training it. As the saying goes, a coin always has two sides. Therefore, it cannot be denied that AI can be an effective tool for combating climate change, but its role in contributing to carbon emissions cannot be ignored. Multiple studies indicate that AI could be the game-changer in staving off anthropogenic climatic changes due to the deterioration of the environment and global warming. This double-edged relationship and interdependency of AI and carbon emissions are represented through a system of systems (SoS) approach. SoS states that a plan is created through multiple smaller systems, creating complexity in the design and vice versa. A complex system can be assumed as the world in general, where two individual independent systems AI and carbon emissions, when in interaction, create a complex complementary and contradictory relation, adding to the convolution of the system. This connection is demonstrated by conducting a network analysis and calculating the carbon emissions of six machine learning (ML) algorithms and deep learning (DL) models with different datasets but the same hyperparameters on a carbon emission calculator created through AI algorithms. The primary idea of this study is to encourage the AI society to create efficient AI models that may be used without compromising environmental issues. The focus should be on practicing sustainable AI, that is, sustainability from data collection to model deployment, throughout the lifecycle of AI.}
}
@article{RAMESH2021375,
title = {Activation energy process in hybrid CNTs and induced magnetic slip flow with heat source/sink},
journal = {Chinese Journal of Physics},
volume = {73},
pages = {375-390},
year = {2021},
issn = {0577-9073},
doi = {https://doi.org/10.1016/j.cjph.2021.07.016},
url = {https://www.sciencedirect.com/science/article/pii/S0577907321001696},
author = {G.K. Ramesh and J.K. Madhukesh},
keywords = {Carbon nanotubes, Slip flow, Induce magnetic flux, Activation energy, Chemical reaction},
abstract = {Effect of induced magnetic field is critical as a result of much controlled and focused on liquid flow is wanted in numerous modern and clinical procedures for example electromagnetic casting, drug delivery and cooling of nuclear reactors. Hence this investigation explains the behaviour of hybrid carbon nanotubes (CNTs) flow through slipped surface with induced magnetic field. Accumulation of SWCNTs (single wall) and MWCNTs (multi wall) nanomaterial with water base liquid is considered. Thermal performance is analyzed with regular heat source/sink effect. Chemical reaction and activation energy impacts are incorporated in mass equation. Solution of the similarity equations are obtained by adopting RKF45 method. Influence of flow variables are illustrated through graphs and computational values of drag force, Nusselt number and Sherwood number are presented in tables. It is noted that activation energy enhance the concentration field whereas opposite behaviour for reaction rate. Also induce magnetic field boosted with the larger values of magnetic Prandtl number. Furthermore it is observed that hybrid CNTs nanomaterial having higher rate of heating/cooling compare to singular CNTs nanomaterial.}
}
@article{BALASUNDARAM2022764,
title = {Graph signatures: Identification and optimization},
journal = {European Journal of Operational Research},
volume = {296},
number = {3},
pages = {764-775},
year = {2022},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2021.03.051},
url = {https://www.sciencedirect.com/science/article/pii/S0377221721002770},
author = {Balabhaskar Balasundaram and Juan S. Borrero and Hao Pan},
keywords = {Networks, Relational, Temporal, Cross-graph mining, Frequent subgraph mining},
abstract = {We introduce a new graph-theoretic paradigm called a graph signature that describes persistent patterns in a sequence of graphs. This framework is motivated by the need to detect subgraphs of significance in temporal networks, e.g., social and biological networks that evolve over time. Because the subgraphs of interest may not all “look alike” in the snapshots of the temporal network, the framework deems a subgraph to be persistent if it satisfies one of several preselected properties in each snapshot of a consecutive subsequence. The persistency requirement is parameterized by the length of this subsequence. This discrete mathematical framework can be viewed more broadly as a way to generalize classical graph properties and invariants associated with a single graph to a sequence of graphs. In this introductory article, we formulate the graph signature identification problem as a mixed-integer program and propose an algorithmic framework based on dynamic programming. This methodology is applicable to any collection of mixed-integer representable graph properties. We also demonstrate how this framework can be tailored to exploit property-specific decomposition and scale reduction techniques through three different computational case-studies. Our experiments show that the dynamic programming algorithm solves this problem across most instances in our test bed to optimality. Moreover, for the instances in our test bed, the optimal signature sizes are comparable to those of their static counterparts, suggesting that our new framework can identify subgraphs of significance in complex dynamic networks.}
}
@article{LIAO20221139,
title = {Statistical prediction of waterflooding performance by K-means clustering and empirical modeling},
journal = {Petroleum Science},
volume = {19},
number = {3},
pages = {1139-1152},
year = {2022},
issn = {1995-8226},
doi = {https://doi.org/10.1016/j.petsci.2021.12.032},
url = {https://www.sciencedirect.com/science/article/pii/S1995822622000097},
author = {Qin-Zhuo Liao and Liang Xue and Gang Lei and Xu Liu and Shu-Yu Sun and Shirish Patil},
keywords = {Waterflooding, Statistical prediction, K-means clustering, Empirical modeling, Uncertainty quantification},
abstract = {Statistical prediction is often required in reservoir simulation to quantify production uncertainty or assess potential risks. Most existing uncertainty quantification procedures aim to decompose the input random field to independent random variables, and may suffer from the curse of dimensionality if the correlation scale is small compared to the domain size. In this work, we develop and test a new approach, K-means clustering assisted empirical modeling, for efficiently estimating waterflooding performance for multiple geological realizations. This method performs single-phase flow simulations in a large number of realizations, and uses K-means clustering to select only a few representatives, on which the two-phase flow simulations are implemented. The empirical models are then adopted to describe the relation between the single-phase solutions and the two-phase solutions using these representatives. Finally, the two-phase solutions in all realizations can be predicted using the empirical models readily. The method is applied to both 2D and 3D synthetic models and is shown to perform well in the P10, P50 and P90 of production rates, as well as the probability distributions as illustrated by cumulative density functions. It is able to capture the ensemble statistics of the Monte Carlo simulation results with a large number of realizations, and the computational cost is significantly reduced.}
}
@article{SESSIONS2022102549,
title = {Mapping geometric and electromagnetic feature spaces with machine learning for additively manufactured RF devices},
journal = {Additive Manufacturing},
volume = {50},
pages = {102549},
year = {2022},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2021.102549},
url = {https://www.sciencedirect.com/science/article/pii/S2214860421006965},
author = {Deanna Sessions and Venkatesh Meenakshisundaram and Andrew Gillman and Alexander Cook and Kazuko Fuchi and Philip R. Buskohl and Gregory H. Huff},
keywords = {Additive manufacturing, Direct-ink write, Electromagnetics, Machine learning, Radio frequency},
abstract = {Multi-material additive manufacturing enables transformative capabilities in customized, low-cost, and multi-functional electromagnetic devices. However, process-specific fabrication anomalies can result in non-intuitive effects on performance; we propose a framework for identifying defect mechanisms and their performance impact by mapping geometric variances to electromagnetic performance metrics. This method can accelerate additive fabrication feedback while avoiding the high computational cost of in-line electromagnetic simulation. We first used dimension reduction to explore the population of geometric manufacturing anomalies and electromagnetic performance. Convolutional neural networks are then trained to predict the electromagnetic performance of the printed geometries. In generating the networks, we explored two inputs: one image-derived geometric description and one using the same description with additional simulated electromagnetic information. Network latent space analysis shows the networks learned both geometric and electromagnetic values even without electromagnetic input. This result demonstrates it is possible to create accelerated additive feedback systems predicting electromagnetic performance without in-line simulation.}
}
@article{BILLI2024103668,
title = {Systemic modeling strategies in public policy: an appraisal from literature},
journal = {Environmental Science & Policy},
volume = {153},
pages = {103668},
year = {2024},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2024.103668},
url = {https://www.sciencedirect.com/science/article/pii/S1462901124000029},
author = {Marco Billi and Angel Allendes and Rodrigo Jiliberto and Rodrigo Ramos-Jiliberto and Bárbara Salinas and Anahí Urquiza},
keywords = {System, System modeling, Public policy, Modeling strategies, Modeling public policy},
abstract = {Contemporary society has grown increasingly dependent on the integration of knowledge for decision-making. In this context, systemic modeling is acknowledged as a straightforward tool for representing and analyzing complex problems. To address how systemic modeling is being conducted to guide and support public policy-making, this study offers a brief synthesis of the literature on systemic modeling oriented to help public policy decision-making. The results are compared to three principles for good systemic modeling to support public policy, established by the authors: the model must a) be readable and manageable —to a basic level— by non-experts, b) require as little quantitative data as possible, and c) not generate spurious or ambiguous readings of their content or their outputs. To identify modeling patterns the models were subjected to a content analysis under eleven different categories. To depict the possible co-occurrence of these analyzed categories in order to describe different types of modeling, a multiple correspondence analysis was performed. We found different modeling patterns with a marked trend to use system modeling as a performative device to let emerging cognitively a new entity, the structure of a complex problem. Regarding our proposal for modeling public policy problems, it can be said that the modeling strategies that fit better with the proposed principles are those that were identified as qualitative and oriented to public policy.}
}
@incollection{MADIAJAGAN2019245,
title = {Chapter 15 - Parallel Machine Learning and Deep Learning Approaches for Bioinformatics},
editor = {Arun Kumar Sangaiah},
booktitle = {Deep Learning and Parallel Computing Environment for Bioengineering Systems},
publisher = {Academic Press},
pages = {245-255},
year = {2019},
isbn = {978-0-12-816718-2},
doi = {https://doi.org/10.1016/B978-0-12-816718-2.00022-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128167182000221},
author = {M. Madiajagan and S. Sridhar Raj},
keywords = {Machine learning, Deep Learning, Parallel processing, Bioinformatics, Parallel deep neural networks},
abstract = {Deep learning uses multiple layers of artificial neurons for classification and pattern recognition. The biggest drawbacks of deep learning algorithms have been the high computation cost, inter-processor communication bottlenecks and parameters training time. Hence, incorporating parallel computing into deep learning decreases the computation time of complex deep learning algorithms. This chapter presents how parallelization is applied over many processors which are loosely coupled. Up to 4096 processes are scaled linearly with higher accuracy and zero loss percentage. This capacity of huge scaling helps in training billions of training examples in just a few hours. Various applications of Hessian-free parallelization mechanism on bioinformatics applications are in gene therapy, drug development, antibiotic resistance research, waste cleanup, climate change studies, bioweapon creation, improving nutritional quality and veterinary science.}
}
@article{VAKSER20141785,
title = {Protein-Protein Docking: From Interaction to Interactome},
journal = {Biophysical Journal},
volume = {107},
number = {8},
pages = {1785-1793},
year = {2014},
issn = {0006-3495},
doi = {https://doi.org/10.1016/j.bpj.2014.08.033},
url = {https://www.sciencedirect.com/science/article/pii/S0006349514009382},
author = {Ilya A. Vakser},
abstract = {The protein-protein docking problem is one of the focal points of activity in computational biophysics and structural biology. The three-dimensional structure of a protein-protein complex, generally, is more difficult to determine experimentally than the structure of an individual protein. Adequate computational techniques to model protein interactions are important because of the growing number of known protein structures, particularly in the context of structural genomics. Docking offers tools for fundamental studies of protein interactions and provides a structural basis for drug design. Protein-protein docking is the prediction of the structure of the complex, given the structures of the individual proteins. In the heart of the docking methodology is the notion of steric and physicochemical complementarity at the protein-protein interface. Originally, mostly high-resolution, experimentally determined (primarily by x-ray crystallography) protein structures were considered for docking. However, more recently, the focus has been shifting toward lower-resolution modeled structures. Docking approaches have to deal with the conformational changes between unbound and bound structures, as well as the inaccuracies of the interacting modeled structures, often in a high-throughput mode needed for modeling of large networks of protein interactions. The growing number of docking developers is engaged in the community-wide assessments of predictive methodologies. The development of more powerful and adequate docking approaches is facilitated by rapidly expanding information and data resources, growing computational capabilities, and a deeper understanding of the fundamental principles of protein interactions.}
}
@article{DORR2017322,
title = {Common errors in reasoning about the future: Three informal fallacies},
journal = {Technological Forecasting and Social Change},
volume = {116},
pages = {322-330},
year = {2017},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2016.06.018},
url = {https://www.sciencedirect.com/science/article/pii/S0040162516301275},
author = {Adam Dorr},
keywords = {Technological progress, Accelerating change, Computing, Fallacy, Errors in reasoning},
abstract = {The continued exponential growth of the price-performance of computing is likely to effectuate technologies that radically transform both the global economy and the human condition over the course of this century. Conventional visions of the next 50years fail to realistically account for the full implications of accelerating technological change driven by the exponential growth of computing, and as a result are deeply flawed. These flawed visions are, in part, a consequence of three interrelated errors in reasoning: 1) the linear projection fallacy, 2) the ceteris paribus fallacy, and 3) the arrival fallacy. Each of these informal fallacies is likely a manifestation of shortcomings in our intuitions about complex dynamic systems. Recognizing these errors and identifying when and where they affect our own reasoning is an important first step toward thinking more realistically about the future.}
}
@article{KARUNA2019161,
title = {Capital markets research in accounting: Lessons learnt and future implications},
journal = {Pacific-Basin Finance Journal},
volume = {55},
pages = {161-168},
year = {2019},
issn = {0927-538X},
doi = {https://doi.org/10.1016/j.pacfin.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0927538X19301398},
author = {Christo Karuna},
abstract = {I review the capital markets literature in accounting by describing the journey taken by researchers since the inception of this stream of research in the late 1960s. Based on a discussion of topics related to the relation between earnings and stock returns, I show how thinking has evolved depending on changing paradigms, methodologies, and data availability. What is clear from a review of the literature is that the usefulness of earnings in determining firm value is both contextual and broadening over time with changes in the global environment. Thus, more research needs to be conducted on a broader notion of earnings that appeals to not just the shareholder but a wide range of firm stakeholders.}
}
@article{RAMIREZPEDRAZA2020293,
title = {A bio-inspired model of behavior considering decision-making and planning, spatial attention and basic motor commands processes},
journal = {Cognitive Systems Research},
volume = {59},
pages = {293-303},
year = {2020},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2019.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S138904171930508X},
author = {Raymundo Ramirez-Pedraza and Natividad Vargas and Carlos Sandoval and Juan Luis {del Valle-Padilla} and Félix Ramos},
keywords = {Brain model, Decision-making, Planning, Spatial attention, Motor system, Goal-driven},
abstract = {Cognitive architectures (CA) are an IA approach to implement computer systems with human-like behavior. Fundamental exhibited human capabilities include planning and decision-making. In that regard, numerous AI systems successfully exhibit human-like behavior but are limited to either achieving specific objectives or are restrained to too heavily constrained environments, which makes them unsuitable in the presence of unforeseen situations where autonomy is required. To try to alleviate the problem, we present a bio-inspired computational model to solve the autonomous navigation problem of a computational entity in a controlled context. This proposal is the result of the interaction between planning and decision-making, spatial attention and the motor cognitive functions. The proposed model is based on neuroscientific evidence concerning the involved cognitive functions and is part of a more general cognitive architecture. In the case study developed to validate our idea, we can see that the processes previously identified play an important role to accomplish spatial navigation. In the case study presented, an agent achieves the navigation over an unexplored maze from an initial to a final position successfully. The reunited results motivate us to continue improving our model considering attentional information to influence the agent’s motor behavior.}
}
@incollection{KOKINOV200699,
title = {Chapter 4 A Cognitive Approach to Context Effects on Individual Decision Making under Risk},
editor = {Richard Topol and Bernard Walliser},
series = {Contributions to Economic Analysis},
publisher = {Elsevier},
volume = {280},
pages = {99-116},
year = {2006},
booktitle = {Cognitive Economics},
issn = {0573-8555},
doi = {https://doi.org/10.1016/S0573-8555(06)80005-2},
url = {https://www.sciencedirect.com/science/article/pii/S0573855506800052},
author = {Boicho Kokinov and Daniela Raeva},
keywords = {choice under risk, computational models, context effects},
abstract = {This chapter compares and contrasts various approaches to understanding human decision making under risk, and is trying to formulate requirements for a cognitive economics theory of risky decision making. Then a first attempt is made to put forward such a theory by proposing a cognitive model JUDGEMAP based on the general cognitive architecture DUAL. This allows the model to be integrated with other cognitive processes such as perception, analogical reasoning, spreading activation memory retrieval, etc. The fact that all processes in DUAL are based on local computations and parallel processing allows for modelling the interplay between various cognitive processes during the decision-making process, in particular the model predicts that the unconscious and automatic process of spreading activation will influence the conscious process of argument building and comparison. This prediction is tested and confirmed by a psychological experiment that demonstrates that seemingly remote and irrelevant aspects of the environment can change the decision we make.}
}
@article{DIAF2022102179,
title = {Sharks and minnows in a shoal of words: Measuring latent ideological positions based on text mining techniques},
journal = {European Journal of Political Economy},
volume = {75},
pages = {102179},
year = {2022},
issn = {0176-2680},
doi = {https://doi.org/10.1016/j.ejpoleco.2022.102179},
url = {https://www.sciencedirect.com/science/article/pii/S0176268022000015},
author = {Sami Diaf and Jörg Döpke and Ulrich Fritsche and Ida Rockenbach},
keywords = {Political economy, Ideology, Text scaling model, Wordfish, Wordshoal, Computational content analysis, Hierarchical factor model, Bayesian estimation, Polarization, Public policy, Monetary policy, Fiscal policy},
abstract = {We scale theoretical/ideological positions of economic research institutes over debates. Using only parts of German research institutes’ business cycle reports that deal with economic policy advice as an example, we extract sections from these reports dealing with monetary and fiscal policy issues from 1999 to 2020. To these corpora, we apply methods of unsupervised text scaling (Slapin and Proksch, 2008; Lauderdale and Herzog, 2016), namely Wordfish and Wordshoal. Roughly, results are in line with common sense in the public policy discourse. For monetary policy texts, we observe a strong, but short-lived consensus in debate-specific positions at the height of the financial crisis in 2008 and a larger polarization thereafter compared to the sample period before. For the fiscal policy text corpus, the polarization was similarly high before and after the crisis and decreases somewhat during the COVID-19 pandemic. For both policy areas, the German Institute of Economic Research (DIW), Berlin, and the Institute for World Economics (IfW), Kiel, tend to be the most diverse institutes within the spectrum of latent ideological positions. We argue that text-mining techniques might be useful to scale underlying ideological positions in policy-related publications.}
}
@article{KLEINMUNTZ1994457,
title = {Toward intelligent computerized clinicians},
journal = {Computers in Human Behavior},
volume = {10},
number = {4},
pages = {457-466},
year = {1994},
issn = {0747-5632},
doi = {https://doi.org/10.1016/0747-5632(94)90040-X},
url = {https://www.sciencedirect.com/science/article/pii/074756329490040X},
author = {Benjamin Kleinmuntz},
abstract = {The current scientific interest in computer thinking is explored for constructing a simulated psychodiagnostician. Going beyond simply building an expert system, this paper proposes to use a self-learning and generalizing computer architecture. State Operator And Result, or the SOAR system, to model intelligent clinical behavior. Accordingly, it fosters the collection and analysis of process-tracing verbal protocols for building psychodiagnosticians. It also holds out the possibility of going beyond psychodiagnosis for functioning as a multitask clinical psychologist.}
}
@article{AYDIN2013173,
title = {A swarm intelligence based sample average approximation algorithm for the capacitated reliable facility location problem},
journal = {International Journal of Production Economics},
volume = {145},
number = {1},
pages = {173-183},
year = {2013},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2012.10.019},
url = {https://www.sciencedirect.com/science/article/pii/S0925527312004604},
author = {Nezir Aydin and Alper Murat},
keywords = {Reliable, Facility location, Stochastic programming, Sample average approximation, Swarm intelligence},
abstract = {We present a novel hybrid method, swarm intelligence based sample average approximation (SIBSAA), for solving the capacitated reliable facility location problem (CRFLP). The CRFLP extends the well-known capacitated fixed-cost facility problem by accounting for the unreliability of facilities. The standard SAA procedure, while effectively used in many applications, can lead to poor solution quality if the selected sample sizes are not sufficiently large. With larger sample sizes, however, the SAA method is not practical due to the significant computational effort required. The proposed SIBSAA method addresses this limitation by using smaller samples and repetitively applying the SAA method while injecting social learning in the solution process inspired by the swarm intelligence of particle swarm optimization. We report on experimental study results showing that the SIBSAA improves the computational efficiency significantly while attaining same or better solution quality than the SAA method.}
}
@incollection{VIJAYSANKER2022217,
title = {Chapter 10 - Emotion-recognition-based music therapy system using electroencephalography signals},
editor = {Rajeswari Sridhar and G.R. Gangadharan and Michael Sheng and Rajan Shankaran},
booktitle = {Edge-of-Things in Personalized Healthcare Support Systems},
publisher = {Academic Press},
pages = {217-235},
year = {2022},
series = {Cognitive Data Science in Sustainable Computing},
isbn = {978-0-323-90585-5},
doi = {https://doi.org/10.1016/B978-0-323-90585-5.00009-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323905855000096},
author = {Swatthi {Vijay Sanker} and Nivetha B. {Ramya Sri Bilakanti} and Anju Thomas and Varun P. Gopi and  {Palanisamy P.}},
keywords = {DEAP, EEG, emotion recognition, machine learning, music therapy},
abstract = {Recent times show increased levels of research in the development of music therapy systems which reduce stress levels in patients by playing songs based on current emotional state of the patient. In this paper, we propose a music recommendation system incorporating emotion recognition from electroencephalography signals to help patients suffering from mental health issues. Emotion recognition using EEG signals is a popular developing technique amidst other techniques such as facial recognition, ECG signals, etc. This technique makes use of machine learning algorithms to classify emotions. However, the detection accuracy of the algorithm has always been a major issue. In this paper, we attempt to improve the detection accuracy using a novel proposed method at reduced computational loads. In this method, the EEG signals are first decomposed into various frequency sub-bands and the band with the largest power is chosen. Following this, an optimum set of four features namely kurtosis parameter, Hjorth mobility, spectral entropy, and power spectral density from a specific set of 5 electrodes are extracted. They are then fed to a machine learning classifier algorithm that classifies the emotion by the valence arousal emotion model. After detecting the emotion, a music recommendation system is developed to suggest songs accordingly which help change the mood of the user. It is built as a user interface website. We implement the above method using the Database for Emotion Analysis using Physiological signals which resulted in an accuracy of 93.2% for valence classification and 95.3% for arousal classification using random forest classifier.}
}
@article{SUTOYO2015435,
title = {Dynamic Difficulty Adjustment in Tower Defence},
journal = {Procedia Computer Science},
volume = {59},
pages = {435-444},
year = {2015},
note = {International Conference on Computer Science and Computational Intelligence (ICCSCI 2015)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.07.563},
url = {https://www.sciencedirect.com/science/article/pii/S187705091502092X},
author = {Rhio Sutoyo and Davies Winata and Katherine Oliviani and Dedy Martadinata Supriyadi},
keywords = {dynamic game balancing, tower defence, dynamic difficulty adjustment, computational intelligence},
abstract = {When we play tower defence game, generally we repeat the same stages several times with the same enemies. Moreover, when the players play a stage that is ridiculously hard or way too easy, they would probably quit the game because it ismoderately frustrating or boring. The purpose of this research is to createa game that can adapt to the players’ ability so the difficulty of the game becomes dynamic. In other words, the game will have different difficultiesof levels according to the players’ ability. High difficulty levels will be set if the players use good strategy and low difficulty levels will be set if the players use bad strategy. In this work, we determine the difficulties based on players’ lives, enemies’ health, and passive skills (skill points) that are chosen by the player. With three of these factors, players will have varies experience of playing tower defence because different combination will give different results to the system and difficulties of the games will be different for each gameplay. The result of this research is a dynamic difficulty tower defence game, dynamic difficulty adjustment (DDA) document, and gameplay outputs for best, average, and worst strategy cases.}
}
@article{TANG2022106735,
title = {A mechanistic survey of Alzheimer's disease},
journal = {Biophysical Chemistry},
volume = {281},
pages = {106735},
year = {2022},
issn = {0301-4622},
doi = {https://doi.org/10.1016/j.bpc.2021.106735},
url = {https://www.sciencedirect.com/science/article/pii/S0301462221002180},
author = {Yijing Tang and Dong Zhang and Xiong Gong and Jie Zheng},
keywords = {Alzheimer's disease, Amyloid-beta, Protein misfolding, Protein aggregation},
abstract = {Alzheimer's disease (AD) is the most common, age-dependent neurodegenerative disorder. While AD has been intensively studied from different aspects, there is no effective cure for AD, largely due to a lack of a clear mechanistic understanding of AD. In this mini-review, we mainly focus on the discussion and summary of mechanistic causes of Alzheimer's disease (AD). While different AD mechanisms illustrate different molecular and cellular pathways in AD pathogenesis, they do not necessarily exclude each other. Instead, some of them could work together to initiate, trigger, and promote the onset and development of AD. In a broader viewpoint, some AD mechanisms (e.g., amyloid aggregation mechanism, microbial infection/neuroinflammation mechanism, and amyloid cross-seeding mechanism) could also be applicable to other amyloid diseases including type II diabetes, Parkinson's disease, and prion disease. Such common mechanisms for AD and other amyloid diseases explain not only the pathogenesis of individual amyloid diseases, but also the spreading of pathologies between these diseases, which will inspire new strategies for therapeutic intervention and prevention for AD.}
}
@article{WU2024e27335,
title = {Unveiling the dynamics of self-regulated learning in project-based learning environments},
journal = {Heliyon},
volume = {10},
number = {5},
pages = {e27335},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e27335},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024033668},
author = {Xiu-Yi Wu},
keywords = {Project-based learning, Self-regulated learning, Latent profile analysis, Epistemic network analysis, College English},
abstract = {Project-based learning (PBL) has been found to exert a positive influence on learners' academic achievements, while also fostering their intrinsic motivation and playing a crucial role in nurturing sustainable learning capacities. Understanding individual differences in self-regulated learning (SRL) within project-based foreign language learning can guide language teachers in delivering personalized instruction. This study utilized a blended learning management system to collect and analyze data on SRL from 95 learners enrolled in a project-based College English course. Through microanalysis, latent profile analysis (LPA), and epistemic network analysis (ENA), the study identified distinct learner profiles and traced their developmental trajectories in project-based language learning. The findings revealed that PBL facilitates the occurrence of SRL behaviors and strengthens connections across different regulation phases. Notably, learners with diverse profiles displayed variations in their SRL epistemic network structures and developmental trajectories. These results highlight the dynamic nature of SRL within the PBL context, underscoring the significance of considering individual differences and supporting learners’ evolving self-regulatory behaviors and strategies throughout their engagement in PBL activities. To enhance SRL in PBL, educators are encouraged to provide scaffolding support, promote help-seeking behaviors, and implement interventions targeting metacognitive processes and reflective practices.}
}
@article{MARDIA2024105341,
title = {Fisher’s pioneering work on discriminant analysis and its impact on Artificial Intelligence},
journal = {Journal of Multivariate Analysis},
volume = {203},
pages = {105341},
year = {2024},
issn = {0047-259X},
doi = {https://doi.org/10.1016/j.jmva.2024.105341},
url = {https://www.sciencedirect.com/science/article/pii/S0047259X24000484},
author = {Kanti V. Mardia},
keywords = {Canonical variates, Classification, Genetic discriminant, Iris data, Linear discriminant function, Machine learning, Mardia’s measures of skewness and kurtosis, Visualising multivariate data},
abstract = {Sir Ronald Aylmer Fisher opened many new areas in Multivariate Analysis, and the one which we will consider is discriminant analysis. Several papers by Fisher and others followed from his seminal paper in 1936 where he coined the name discrimination function. Historically, his four papers on discriminant analysis during 1936–1940 connect to the contemporaneous pioneering work of Hotelling and Mahalanobis. We revisit the famous iris data which Fisher used in his 1936 paper and in particular, test the hypothesis of multivariate normality for the data which he assumed. Fisher constructed his genetic discriminant motivated by this application and we provide a deeper insight into this construction; however, this construction has not been well understood as far as we know. We also indicate how the subject has developed along with the computer revolution, noting newer methods to carry out discriminant analysis, such as kernel classifiers, classification trees, support vector machines, neural networks, and deep learning. Overall, with computational power, the whole subject of Multivariate Analysis has changed its emphasis but the impact of this Fisher’s pioneering work continues as an integral part of supervised learning in Artificial Intelligence (AI).}
}
@article{GUO20241425,
title = {An empirical study on the driving factors and pathways of research talents innovation capability},
journal = {Procedia Computer Science},
volume = {242},
pages = {1425-1432},
year = {2024},
note = {11th International Conference on Information Technology and Quantitative Management (ITQM 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.08.105},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924018246},
author = {Zheng Guo and Lijun Liang and Yundong Xie},
keywords = {research talents, research institutions, research environment, innovative capability, driving factors},
abstract = {This study designed a survey questionnaire encompassing three levels: research institutions, research environment, and research talents, with 19 specific measurement indicators. The questionnaire was distributed to research personnel from over 20 research institutes, universities and related enterprises in China. Based on 537 valid responses, a structural equation model (SEM) was constructed to analyze the relational effects of the driving factors. The findings are as follows: (1) The innovative capability of research talents is a composite outcome of the interaction between multiple "external-internal" factors. Effective organizational support from research institutions and a favorable research environment, both soft and hard, are indispensable elements in enhancing this capability. (2) A key factor in enhancing innovation capability is the proper resolution of effective supply and scientific utilization of research resources by research institutions, along with the improvement of research satisfaction. (3) Research institutions should establish an integrated research mechanism that combines "environment-organization-talent" to achieve the generation and effective transformation of high-quality research outputs.}
}
@article{PATTEE20025,
title = {The origins of Michael Conrad's research programs (1964–1979)},
journal = {Biosystems},
volume = {64},
number = {1},
pages = {5-11},
year = {2002},
issn = {0303-2647},
doi = {https://doi.org/10.1016/S0303-2647(01)00169-1},
url = {https://www.sciencedirect.com/science/article/pii/S0303264701001691},
author = {H.H Pattee},
keywords = {Artificial ecosystems, Adaptability, Evolvability, Non-programmable computation, Brain-computer disanalogy, Enzymatic neurons},
abstract = {This paper summarizes Michael Conrad's academic and professional career from the time he began his Ph.D. studies in 1964 to his appointment at Wayne State University in 1979. It describes the origins of several of his major research interests and presents a personal evaluation of how this early work continues to be of fundamental importance.}
}
@incollection{BRUDER2017101,
title = {Chapter 5 - Infrastructural intelligence: Contemporary entanglements between neuroscience and AI},
editor = {Tara Mahfoud and Sam McLean and Nikolas Rose},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {233},
pages = {101-128},
year = {2017},
booktitle = {Vital Models},
issn = {0079-6123},
doi = {https://doi.org/10.1016/bs.pbr.2017.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0079612317300547},
author = {Johannes Bruder},
keywords = {Artificial intelligence, Computational neuroscience, Brain imaging, Google DeepMind technologies, Default mode network},
abstract = {In this chapter, I reflect on contemporary entanglements between artificial intelligence and the neurosciences by tracing the development of Google's recent DeepMind algorithms back to their roots in neuroscientific studies of episodic memory and imagination. Google promotes a new form of “infrastructural intelligence,” which excels by constantly reassessing its cognitive architecture in exchange with a cloud of data that surrounds it, and exhibits putatively human capacities such as intuition. I argue that such (re)alignments of biological and artificial intelligence have been enabled by a paradigmatic infrastructuralization of the brain in contemporary neuroscience. This infrastructuralization is based in methodologies that epistemically liken the brain to complex systems of an entirely different scale (i.e., global logistics) and has given rise to diverse research efforts that target the neuronal infrastructures of higher cognitive functions such as empathy and creativity. What is at stake in this process is no less than the shape of brains to come and a revised understanding of the intelligent and creative social subject.}
}
@article{MAZUMDER2023e13359,
title = {Towards next generation digital twin in robotics: Trends, scopes, challenges, and future},
journal = {Heliyon},
volume = {9},
number = {2},
pages = {e13359},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e13359},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023005662},
author = {A. Mazumder and M.F. Sahed and Z. Tasneem and P. Das and F.R. Badal and M.F. Ali and M.H. Ahamed and S.H. Abhi and S.K. Sarker and S.K. Das and M.M. Hasan and M.M. Islam and M.R. Islam},
keywords = {Digital twin, Cyber-physical system, Robotics, Industry 4.0, Smart manufacturing, Human-robot interaction},
abstract = {With the advent of Industry 4.0, several cutting-edge technologies such as cyber-physical systems, digital twins, IoT, robots, big data, cloud computation have emerged. However, how these technologies are interconnected or fused for collaborative and increased functionality is what elevates 4.0 to a grand scale. Among these fusions, the digital twin (DT) in robotics is relatively new but has unrivaled possibilities. In order to move forward with DT-integrated robotics research, a complete evaluation of the literature and the creation of a framework are now required. Given the importance of this research, the paper seeks to explore the trends of DT incorporated robotics in both high and low research saturated robotic domains in order to discover the gap, rising and dying trends, potential scopes, challenges, and viable solutions. Finally, considering the findings, the study proposes a framework based on a hypothesis for the future paradigm of DT incorporated robotics.}
}
@article{MARTIN2018105,
title = {On an inferential model construction using generalized associations},
journal = {Journal of Statistical Planning and Inference},
volume = {195},
pages = {105-115},
year = {2018},
note = {Confidence distributions},
issn = {0378-3758},
doi = {https://doi.org/10.1016/j.jspi.2016.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0378375816301537},
author = {Ryan Martin},
keywords = {Likelihood, Marginalization, Monte Carlo, Plausibility function, Random set, Validity},
abstract = {The inferential model (IM) approach, like fiducial and its generalizations, depends on a representation of the data-generating process. Here, a particular variation on the IM construction is considered, one based on generalized associations. The resulting generalized IM is more flexible in that it does not require a complete specification of the data-generating process and is provably valid under mild conditions. Computation and marginalization strategies are discussed, and two applications of this generalized IM approach are presented.}
}
@article{KAUFFMAN2017115,
title = {Combining machine-based and econometrics methods for policy analytics insights},
journal = {Electronic Commerce Research and Applications},
volume = {25},
pages = {115-140},
year = {2017},
issn = {1567-4223},
doi = {https://doi.org/10.1016/j.elerap.2017.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S1567422317300145},
author = {Robert J. Kauffman and Kwansoo Kim and Sang-Yong Tom Lee and Ai-Phuong Hoang and Jing Ren},
keywords = {Causality, Computational Social Science, Data analytics, Econometrics, E-commerce, Empirical research, Fintech, Fusion analytics, Music popularity, Stock trading, Policy analytics, TV viewing, Video-on-demand (VoD)},
abstract = {Computational Social Science (CSS) has become a mainstream approach in the empirical study of policy analytics issues in various domains of e-commerce research. This article is intended to represent recent advances that have been made for the discovery of new policy-related insights in business, consumer and social settings. The approach discussed is fusion analytics, which combines machine-based methods from Computer Science (CS) and explanatory empiricism involving advanced Econometrics and Statistics. It explores several efforts to conduct research inquiry in different functional areas of Electronic Commerce and Information Systems (IS), with applications that represent different functional areas of business, as well as individual consumer, social and public issues. Recent developments and shifts in the scientific study of technology-related phenomena and Social Science issues in the presence of historically-large datasets prompt new forms of research inquiry. They include blended approaches to research methodology, and more interest in the production of research results that have direct application to industry contexts. This article showcases the methods shifts and several contemporary applications. They discuss: (1) feedback effects in mobile phone-based stock trading; (2) sustainability of top-rank chart popularity of music tracks; (3) household TV viewing patterns; and (4) household sampling and purchases of video-on-demand (VoD) services. The range of applicability of the ideas goes beyond the scope of these illustrations, to include issues in public services, healthcare, product and service deployment, public opinion and elections, electronic auctions, and travel and tourism services. In fact, the coverage is as broad as for-profit and for-non-profit, private and public, and governmental and non-governmental institutions.}
}
@article{BENZEKRY201553,
title = {Metronomic reloaded: Theoretical models bringing chemotherapy into the era of precision medicine},
journal = {Seminars in Cancer Biology},
volume = {35},
pages = {53-61},
year = {2015},
note = {Complexity in Cancer Biology},
issn = {1044-579X},
doi = {https://doi.org/10.1016/j.semcancer.2015.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S1044579X15000759},
author = {Sébastien Benzekry and Eddy Pasquier and Dominique Barbolosi and Bruno Lacarelle and Fabrice Barlési and Nicolas André and Joseph Ciccolini},
keywords = {Metronomic chemotherapy, Mathematical modeling, PK/PD, Precision medicine},
abstract = {Oncology has benefited from an increasingly growing number of groundbreaking innovations over the last decade. Targeted therapies, biotherapies, and the most recent immunotherapies all contribute to increase the number of therapeutic options for cancer patients. Consequently, substantial improvements in clinical outcomes for some disease with dismal prognosis such as lung carcinoma or melanoma have been achieved. Of note, the latest innovations in targeted therapies or biotherapies do not preclude the use of standard cytotoxic agents, mostly used in combination. Importantly, and despite the rise of bioguided (a.k.a. precision) medicine, the administration of chemotherapeutic agents still relies on the maximum tolerated drug (MTD) paradigm, a concept inherited from theories conceptualized nearly half a century ago. Alternative dosing schedules such as metronomic regimens, based upon the repeated and regular administration of low doses of chemotherapeutic drugs, and adaptive therapy (i.e. modulating the dose and frequency of cytotoxics administration to control disease progression rather than eradicate it at all cost) have emerged as possible strategies to improve response rates while reducing toxicities. The recent changes in paradigm in the way we theorize cancer biology and evolution, metastatic spreading and tumor ecology, alongside the recent advances in the field of immunotherapy, have considerably strengthened the interest for these alternative approaches. This paper aims at reviewing the recent evolutions in the field of theoretical biology of cancer and computational oncology, with a focus on the consequences these changes have on the way we administer chemotherapy. Here, we advocate for the development of model-guided strategies to refine doses and schedules of chemotherapy administration in order to achieve precision medicine in oncology.}
}
@article{YETISEN2015724,
title = {Bioart},
journal = {Trends in Biotechnology},
volume = {33},
number = {12},
pages = {724-734},
year = {2015},
issn = {0167-7799},
doi = {https://doi.org/10.1016/j.tibtech.2015.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S016777991500205X},
author = {Ali K. Yetisen and Joe Davis and Ahmet F. Coskun and George M. Church and Seok Hyun Yun},
keywords = {genetics, transgenic art, tissue engineering, ethics, aesthetics},
abstract = {Bioart is a creative practice that adapts scientific methods and draws inspiration from the philosophical, societal, and environmental implications of recombinant genetics, molecular biology, and biotechnology. Some bioartists foster interdisciplinary relationships that blur distinctions between art and science. Others emphasize critical responses to emerging trends in the life sciences. Since bioart can be combined with realistic views of scientific developments, it may help inform the public about science. Artistic responses to biotechnology also integrate cultural commentary resembling political activism. Art is not only about ‘responses’, however. Bioart can also initiate new science and engineering concepts, foster openness to collaboration and increasing scientific literacy, and help to form the basis of artists’ future relationships with the communities of biology and the life sciences.}
}
@article{STAPLETON2024100967,
title = {Shaping the future of advanced automation and control systems for society strategic directions and multidisciplinary collaborations of IFAC's social systems coordinating committee},
journal = {Annual Reviews in Control},
volume = {58},
pages = {100967},
year = {2024},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2024.100967},
url = {https://www.sciencedirect.com/science/article/pii/S1367578824000361},
author = {Larry Stapleton and Fei-Yue Wang and Mariana Netto and Qing-Shan Jia and Antonio Visioli and Peter Kopacek},
keywords = {Social Systems, Finance Systems, Business, Economics, Control for Societal impact, Cyber-Physical and Human Systems (CPHS), responsible design of CPHS, Control Education, Engineering Ethics and Control, Smart Cities, Cost Oriented Automation, Systemic and Structural Effects of Technologies, Intelligent Systems, Diversity and Inclusion, International Affairs, Developing Regions, Culture},
abstract = {In an era of rapid advancements in highly intelligent digital systems, blockchain, and other transformative technologies, the role of control and automation in shaping human civilization is of paramount, even critical, importance. This paper examines the strategic significance of IFAC's Social Systems Coordinating Committee (CC), a unique multidisciplinary global community of researchers and practitioners comprising leading universities, research centers, industry partners and international agencies at the forefront of integrating technological and societal progress. This paper reports the results of a strategic "milestone" review, including an extensive meta-analysis of the Social Systems CC's five Technical Committees (TCs) and their activities. It uncovers key themes emphasizing this CC's contributions to models, systems, infrastructures, and operations. Using content analysis and word clouds, 272 keywords were refined to elucidate the main themes of the CC, revealing significant current and future collaborations with other IFAC communities and external organizations. The paper identifies high-potential new cooperation opportunities between this CC and the other IFAC CCs and their TCs, suggesting ways to achieve these collaborations. The findings highlight the Social Systems CC's unique position at the heart of the global automation and control community, where it offers practical applications in planning, management, and sustainability as well as fostering cross-sector cooperation crucial for human progress and effective humanitarian and environmental responses. This paper underscores the Social Systems CC's role in advancing control science and automation systems engineering to tackle pressing societal challenges, advocating for a future where technology and human systems synergize for the global well-being of all living systems.}
}
@article{OSEIBRYSON20121156,
title = {A context-aware data mining process model based framework for supporting evaluation of data mining results},
journal = {Expert Systems with Applications},
volume = {39},
number = {1},
pages = {1156-1164},
year = {2012},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2011.07.117},
url = {https://www.sciencedirect.com/science/article/pii/S0957417411010797},
author = {Kweku-Muata Osei-Bryson},
keywords = {Context, Data mining process, KDDM, Evaluation, Decision analysis, Multi-criteria decision analysis, Post-processing},
abstract = {The knowledge discovery via data mining process (KDDM) is a multiple phase that aims to at a minimum semi-automatically extract new knowledge from existing datasets. For many data mining tasks, the evaluation phase is a challenging one for various reasons. Given this challenge several studies have presented techniques that could be used for the semi-automated evaluation of data mining results. When taken together, these studies suggest the possibility of a common multi-criteria evaluation framework. The use of such a multi-criteria evaluation framework, however, requires that relevant objectives, measures and preference function be identified. This implies that the context of the DM problem is particularly important for the evaluation phase of the KDDM process. Our framework utilizes and integrates a pair of established tightly coupled techniques (i.e. Value Focused Thinking (VFT) and the Goal–Question–Metric (GQM) methods) as well as established techniques from multi-criteria decision analysis in order to explicate and utilize context information in order to facilitate semi-automated evaluation.}
}
@article{FATTAHI2020107755,
title = {Stochastic optimization of disruption-driven supply chain network design with a new resilience metric},
journal = {International Journal of Production Economics},
volume = {230},
pages = {107755},
year = {2020},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2020.107755},
url = {https://www.sciencedirect.com/science/article/pii/S0925527320301407},
author = {Mohammad Fattahi and Kannan Govindan and Reza Maihami},
keywords = {Resilience metrics, Supply chain network design, Stochastic programming, Conic mixed-integer program},
abstract = {The supply chain (SC) ability to return quickly and effectively to its initial condition or even a more desirable state after a disruption is critically important, and is defined as SC resilience. Nevertheless, it has not been sufficiently quantified in the related literature. This study provides a new metric to quantify the SC resilience by using the stochastic programming. Our metric measures the expected value of the SC's cost increase due to a possible disruption event during its recovery period. Based on this measure, we propose a two-stage stochastic program for the supply chain network design under disruption events that optimizes location, allocation, inventory and order-size decisions. The stochastic program is formulated using quadratic conic optimization, and the sample average approximation (SAA) method is employed to handle the large number of disruption scenarios. A comprehensive computational study is carried out to highlight the applicability of the presented metric, the computational tractability of the stochastic program, and the performance of the SAA. Several key managerial and practical insights are gained based on the computational results. This new metric captures the time and cost of the SC's recovery after disruption events contrast to most of previous studies and main impacts of these two aspects on design decisions are highlighted. Further, it is shown computationally that the increase of SC's capacity is not a suitable strategy for designing resilient SCs in some business environments.}
}
@article{ZAK2019139,
title = {Multiple Criteria Optimization of the Carpooling Problem},
journal = {Transportation Research Procedia},
volume = {37},
pages = {139-146},
year = {2019},
note = {21st EURO Working Group on Transportation Meeting, EWGT 2018, 17th – 19th September 2018, Braunschweig, Germany},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2018.12.176},
url = {https://www.sciencedirect.com/science/article/pii/S235214651830591X},
author = {Jacek Żak and Maciej Hojda and Grzegorz Filcek},
keywords = {carpooling, multiple criteria optimisation, NSGA II, LBS},
abstract = {The paper presents a multiple criteria (MC) formulation of the carpooling optimization (CO) problem and a solution procedure that allows to solve it. The mathematical model of the MCCO problem includes two major sub-problems, such as planning of the routes and matching carpoolers (drivers and passengers). Different aspects, including: economic, social, technical and market-oriented are considered. The MCCO problem is solved with the application of an original computational procedure based on the multiple criteria genetic algorithm, called NSGA II and the solutions’ analysis and review technique, called Light Beam Search (LBS) method. The former method allows to generate a set of Pareto optimal solutions, while the latter assists the carpoolers in finding the most desired compromise solution (common route and match between carpoolers). The results of computational experiments are presented. We find that solving the formulating carpooling problem in a heuristic manner is possible in reasonable time}
}
@article{JIA202138,
title = {A multicriteria group decision-making method based on AIVIFSs, Z-numbers, and trapezium clouds},
journal = {Information Sciences},
volume = {566},
pages = {38-56},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.02.042},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521001845},
author = {Qianlei Jia and Jiayue Hu and Qizhi He and Weiguo Zhang and Ehab Safwat},
keywords = {Multicriteria group decision-making (MCGDM), Atanassov’s interval-valued intuitionistic fuzzy sets (AIVIFSs), Z-numbers, Z-trapezium-trapezium cloud (ZTTC), Coronavirus Disease 2019 (COVID-19)},
abstract = {Multicriteria group decision-making (MCGDM), with the strong uncertainty and randomness, has always been a hotspot in the world. The chief purpose of the paper is to address the problem with Atanassov’s interval-valued intuitionistic fuzzy sets (AIVIFSs), Z-numbers, and trapezium clouds. First, some related concepts and former operators of AIVIFSs, Z-numbers, and trapezium clouds are reviewed, meanwhile, AIVIFSs and Z-numbers are synthesized to come up with a novel linguistic expression. Then, Z-trapezium-trapezium cloud (ZTTC) is proposed to quantify the linguistic evaluation information to avoid excessive computation caused by traditional methods. Later, a new approach of calculating the objective weight vector is presented based on entropy weight method (EWM). To take the huge advantages of technique for order preference by similarity to ideal solution (TOPSIS) method in ranking, 2-norm in mathematical theory is applied to derive a way of calculating the distance between different ZTTCs. Finally, an example about the grade assessment of coronavirus Disease 2019 (COVID-19) is given. For further confirming the validity and feasibility, sensitivity analysis and comparison with other methods are conducted.}
}
@article{GUERRERO2022101155,
title = {Subnational sustainable development: The role of vertical intergovernmental transfers in reaching multidimensional goals},
journal = {Socio-Economic Planning Sciences},
volume = {83},
pages = {101155},
year = {2022},
issn = {0038-0121},
doi = {https://doi.org/10.1016/j.seps.2021.101155},
url = {https://www.sciencedirect.com/science/article/pii/S0038012121001476},
author = {Omar A. Guerrero and Gonzalo Castañeda and Georgina Trujillo and Lucy Hackett and Florian Chávez-Juárez},
keywords = {Subnational development, SDGs, State finances, Fiscal federalism, Sustainability, Policy priorities, Agent-based model},
abstract = {From a public finance point of view, achieving sustainable development hinges on two critical factors: the subnational implementation of public policies and the efficient allocation of resources across regions through vertical intergovernmental transfers. We introduce a framework that links these two mechanisms for analyzing the impact of reallocating federal transfers in the presence of regional heterogeneity from development indicators, budget sizes, expenditure returns, and long-term structural factors. Our study focuses on the case of Mexico and its 32 states. Using an agent-based computational model, we estimate the development gaps that will remain by the year 2030, and characterize their sensitivity to changes in the states’ budget sizes. Then, we estimate the optimal distribution of federal transfers to minimize these gaps. Crucially, these distributions depend on the specific development objectives set by the national government, and by various interdependencies between the heterogeneous qualities of the states. This work sheds new light on the complex problem of budgeting for the Sustainable Development Goals at the subnational level, and it is especially relevant for the study of fiscal decentralization from the expenditure point of view.}
}
@article{GUILE2023100738,
title = {Machine learning – A new kind of cultural tool? A “recontextualisation” perspective on machine learning + interprofessional learning},
journal = {Learning, Culture and Social Interaction},
volume = {42},
pages = {100738},
year = {2023},
issn = {2210-6561},
doi = {https://doi.org/10.1016/j.lcsi.2023.100738},
url = {https://www.sciencedirect.com/science/article/pii/S2210656123000545},
author = {David Guile},
keywords = {Cultural tool, Machine learning, Interprofessional learning, Recontextualisation, Cultural ecosystem},
abstract = {The paper argues that (a) Machine Learning (ML) constitutes a cultural tool capable of learning through perceiving patterns in data, (b) the kind of learning ML is capable of nevertheless constitutes a more circumscribed kind of learning compared with how that concept has been interpreted in sociocultural (S-c) theory; and, (c) the development of ML is therefore further extending and distributing the complex relationship between human and machine cognition and learning. The paper explores these contentions by firstly, providing a broad-based account of the conception of cultural tools in S-c Theory. Secondly, offering a genealogy of ML, including the model of learning that underpins ML and highlights the challenge that a cultural too capable of some kind of learning presents for the extant S-c conception of a cultural tool. Thirdly, identifying the new human-machine working-learning problem the ML model of learning is generating. Finally, argues the concept of recontextualization offers a way to address that problem by providing a holistic perspective on the relationship between ML and IPL models of learning. In making this argument the paper distinguishes between the ML predictive and the Chat GPT answer to question(s) model of learning.}
}
@incollection{DETALLE2017495,
title = {2.20 - Translational Aspects in Drug Discovery},
editor = {Samuel Chackalamannil and David Rotella and Simon E. Ward},
booktitle = {Comprehensive Medicinal Chemistry III},
publisher = {Elsevier},
address = {Oxford},
pages = {495-529},
year = {2017},
isbn = {978-0-12-803201-5},
doi = {https://doi.org/10.1016/B978-0-12-409547-2.12335-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095472123352},
author = {L. Detalle and K. Vanheusden and M.L. Sargentini-Maier and T. Stöhr},
keywords = {Animal model, Biomarker, Imaging, Modeling, Simulation, Translational medicine},
abstract = {The efficiency of drug development has seen a constant decline. This observation is somewhat paradoxical since during the same time there have been huge advancements in drug discovery and development technologies that made it much cheaper, faster, and easier to identify new drug targets and new drug molecules. Translational Science or Translational Medicine (TM) has arisen as an important discipline in modern drug discovery and development. It was triggered by the fact that many promising drugs failed in clinical trials. The challenge was thus to enhance the predictivity of the preclinical models and to design exploratory clinical trial designs and methodologies to test promising molecules earlier and faster. Despite some advancements, the number of drugs that finally receive regulatory approval is still at a low level. The main reason for this drug failure rate was a lack of efficacy observed in clinical trials of drug candidates that showed great promise in drug discovery. There may be two main factors responsible for this: (1) the industrialization of drug discovery and development led to huge specialized departments that operate in isolation. (2) Tools for successful translational research have only been developed in the last one or two decades. We will describe the tools used in translational research, that is, biomarkers, animal models, imaging, in silico modeling, and simulations. Their use will be illustrated with examples and tips of how to implement those into daily project work. We believe, however, that TM is more than these tools and technologies. It is not yet another discipline or department, it is a way of thinking that should become part of every discipline involved in drug development. Thus, in addition to describing the tools and how best to use them, we will elaborate how to design a translational research strategy and exemplify with some case studies as to how this has been successfully implemented in the past.}
}
@article{DEBRUIJN2022101666,
title = {The perils and pitfalls of explainable AI: Strategies for explaining algorithmic decision-making},
journal = {Government Information Quarterly},
volume = {39},
number = {2},
pages = {101666},
year = {2022},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2021.101666},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X21001027},
author = {Hans {de Bruijn} and Martijn Warnier and Marijn Janssen},
keywords = {Artificial intelligence, XAI, Algorithms, Computational intelligence, Data-driven decision, Socio-tech, Transparency, Accountability, Trust, E-government},
abstract = {Governments look at explainable artificial intelligence's (XAI) potential to tackle the criticisms of the opaqueness of algorithmic decision-making with AI. Although XAI is appealing as a solution for automated decisions, the wicked nature of the challenges governments face complicates the use of XAI. Wickedness means that the facts that define a problem are ambiguous and that there is no consensus on the normative criteria for solving this problem. In such a situation, the use of algorithms can result in distrust. Whereas there is much research advancing XAI technology, the focus of this paper is on strategies for explainability. Three illustrative cases are used to show that explainable, data-driven decisions are often not perceived as objective by the public. The context might raise strong incentives to contest and distrust the explanation of AI, and as a consequence, fierce resistance from society is encountered. To overcome the inherent problems of XAI, decisions-specific strategies are proposed to lead to societal acceptance of AI-based decisions. We suggest strategies to embrace explainable decisions and processes, co-create decisions with societal actors, move away from an instrumental to an institutional approach, use competing and value-sensitive algorithms, and mobilize the tacit knowledge of professionals}
}
@article{SWOFFORD2022100220,
title = {Probabilistic reporting and algorithms in forensic science: Stakeholder perspectives within the American criminal justice system},
journal = {Forensic Science International: Synergy},
volume = {4},
pages = {100220},
year = {2022},
issn = {2589-871X},
doi = {https://doi.org/10.1016/j.fsisyn.2022.100220},
url = {https://www.sciencedirect.com/science/article/pii/S2589871X22000055},
author = {H. Swofford and C. Champod},
keywords = {Forensic science, Pattern evidence, Probabilities, Statistics, Algorithms},
abstract = {In recent years, there have been efforts to promote probabilistic reporting and the use of computational algorithms across several forensic science disciplines. Reactions to these efforts have been mixed—some stakeholders argue they promote greater scientific rigor whereas others argue that the opacity of algorithmic tools makes it challenging to meaningfully scrutinize the evidence presented against a defendant resulting from these systems. Consequently, the forensic community has been left with no clear path to navigate these concerns as each proposed approach has countervailing benefits and risks. To explore these issues further and provide a foundation for a path forward, this study draws on semi-structured interviews with fifteen participants to elicit the perspectives of key criminal justice stakeholders, including laboratory managers, prosecutors, defense attorneys, judges, and other academic scholars, on issues related to interpretation and reporting practices and the use of computational algorithms in forensic science within the American legal system.}
}
@article{ROCHMAN2024,
title = {An introduction to Spent Nuclear Fuel decay heat for Light Water Reactors: a review from the NEA WPNCS},
journal = {EPJ - Nuclear Sciences & Technologies},
volume = {10},
year = {2024},
issn = {2491-9292},
doi = {https://doi.org/10.1051/epjn/2024010},
url = {https://www.sciencedirect.com/science/article/pii/S2491929224000098},
author = {Dimitri Rochman and Alejandro Algora and Francisco Àlvarez-Velarde and Aurélie Bardelay and Øystein Bremnes and Oscar Cabellos and Daniel Cano-Ott and Luigi Capponi and Coralie Carmouze and Stefano Caruso and Andrew Cummings and Ron Dagan and Muriel Fallot and Luca Fiorito and Lydie Giot and Kevin Govers and Silja Häkkinen and Volker Hannstein and Axel Hoefer and Tan Dat Huynh and Raphaëlle Ichou and Germina Ilas and Pauli Juutilainen and Lukasz Koszuk and Marjan Kromar and Sébastien Lahaye and James Lam and Frédéric Laugier and Agnés Launay and Vincent Léger and David Lecarpentier and Jaakko Leppanen and Fadhel Malouch and Julie-Fiona Martin and David McGinnes and Robert William Mills and Futoshi Minato and Yasushi Nauchi and Pedro Ortego and Plamen Petkov and Pablo Romojaro and Shunsuke Sato and Marcus Seidl and Ahmed Shama and Teodosi Simeonov and Anders Sjöland and Virginie Solans and Fabian Sommer and Sven Tittelbach and Aimé Tsilanizara and Efstathios Vlassopoulos and Vanessa Vallet and Alexander Vasiliev and Tomoaki Watanabe and Gašper Žerovnik},
abstract = {This paper summarized the efforts performed to understand decay heat estimation from existing spent nuclear fuel (SNF), under the auspices of the Working Party on Nuclear Criticality Safety (WPNCS) of the OECD Nuclear Energy Agency. Needs for precise estimations are related to safety, cost, and optimization of SNF handling, storage, and repository. The physical origins of decay heat (a more correct denomination would be decay power) are then introduced, to identify its main contributors (fission products and actinides) and time-dependent evolution. Due to limited absolute prediction capabilities, experimental information is crucial; measurement facilities and methods are then presented, highlighting both their relevance and our need for maintaining the unique current full-scale facility and developing new ones. The third part of this report is dedicated to the computational aspect of the decay heat estimation: calculation methods, codes, and validation. Different approaches and implementations currently exist for these three aspects, directly impacting our capabilities to predict decay heat and to inform decision-makers. Finally, recommendations from the expert community are proposed, potentially guiding future experimental and computational developments. One of the most important outcomes of this work is the consensus among participants on the need to reduce biases and uncertainties for the estimated SNF decay heat. If it is agreed that uncertainties (being one standard deviation) are on average small (less than a few percent), they still substantially impact various applications when one needs to consider up to three standard deviations, thus covering more than 95% of cases. The second main finding is the need of new decay heat measurements and validation for cases corresponding to more modern fuel characteristics: higher initial enrichment, higher average burnup, as well as shorter and longer cooling time. Similar needs exist for fuel types without public experimental data, such as MOX, VVER, or CANDU fuels. A third outcome is related to SNF assemblies for which no direct validation can be performed, representing the vast majority of cases (due to the large number of SNF assemblies currently stored, or too short or too long cooling periods of interest). A few solutions are possible, depending on the application. For the final repository, systematic measurements of quantities related to decay heat can be performed, such as neutron or gamma emission. This would provide indications of the SNF decay heat at the time of encapsulation. For other applications (short- or long-term cooling), the community would benefit from applying consistent and accepted recommendations on calculation methods, for both decay heat and uncertainties. This would improve the understanding of the results and make comparisons easier.}
}
@article{CHEN2023e21366,
title = {Application of computer image processing technology in old artistic design restoration},
journal = {Heliyon},
volume = {9},
number = {11},
pages = {e21366},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e21366},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023085742},
author = {Guo Chen and Zhiyong Wen and Fazhong Hou},
keywords = {Art design, Edge detection, Gradient distribution, Recurrent learning, Texture classification},
abstract = {Art designs exhibit different principles, textures, color combinations, and creative skills for vivid thinking visualizations. Art exhibits are far from ages, periods, and creators finding their digital patterns in recent years for resurrection. Degraded periodic artworks are digitally handled for reviving their legacy using digital image processing. This article introduces Textural Restoration Technique (TRT) using Deep Feature Processing (DFP) to augment such innovations. The proposed technique analyses the tampered image for its textures, and available features are extracted. The textures are expected to be sequential based on gradient distribution; the missing gradients are identified from the available features near the region of interest (ROI). The ROI is marked by combining missing and available features from which textural edges are sketched. In this process, recurrent learning is employed for verifying the gradient substitutions for even textures. The texture patterns are classified using high and low accuracy features exhibited between two successive ROIs. First, the learning model is trained using gradient distribution accuracy pursued by the texture completion edge. The second training is pursued by the first distribution, achieving the maximum restoration. The filled features and their gradient positions are marked by moving the ROIs for distinguishing textures. The restoration ratio is computed with high accuracy based on the filled edges.}
}
@article{PAULIUK2022130997,
title = {Co-design of digital transformation and sustainable development strategies - What socio-metabolic and industrial ecology research can contribute},
journal = {Journal of Cleaner Production},
volume = {343},
pages = {130997},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.130997},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622006321},
author = {Stefan Pauliuk and Maximilian Koslowski and Kavya Madhu and Simon Schulte and Sebastian Kilchert},
keywords = {Sustainable development, Digital transformation, Systems thinking, Research agenda, Technology scale-up, Development constraints},
abstract = {Sustainable development and digital transformation profoundly re-shape industrial societies but have been studied largely independently. In light of pressing global environmental and social challenges, both transformations need to be well aligned with each other to achieve multiple objectives such as listed under the UN Sustainable Development goals (SDGs). Quantitative research on interlinkages, energy and material implications, and co-dependencies between the different digital transformation (DT) and sustainable development (SD) strategies is emerging and has so far focused on estimating the overall potential and on life cycle assessment (LCA). To frame the problem systematically, we developed a hierarchy of system levels for studying society's material and energy use, including the four levels: product/process, process cluster, life cycle/material cycle, and economy-wide. We mapped major DT strategies and the SDGs to the hierarchy and found a wide gap in system coverage: While most DT strategies focus on the product, process and process cluster levels, the SDGs predominantly target the economy-wide level. Socio-metabolic and industrial ecology research is needed to inform decision makers on how the two transformations can be aligned to reach overarching societal goals, such as the SDGs, expanding on and moving beyond LCA. Future research needs to assess combinations of multiple DT and SD strategies. It needs to study how DT can help decouple human wellbeing from negative environmental and social impacts. Research needs to focus on the strategies’ deployment potential, infrastructure needs, impacts on material cycles, and potential to transform both service demand and industrial production.}
}
@article{YU2023102123,
title = {Air traffic controllers' mental fatigue recognition: A multi-sensor information fusion-based deep learning approach},
journal = {Advanced Engineering Informatics},
volume = {57},
pages = {102123},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102123},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623002513},
author = {Xiaoqing Yu and Chun-Hsien Chen and Haohan Yang},
keywords = {Air traffic management, Fatigue detection, Multi-modal information fusion, Human-automation collaboration, Intelligent ATC},
abstract = {With the growing density of air passenger traffic, accurately recognizing the level of mental fatigue (MF) experienced by air traffic controllers (ATCOs) is crucial for developing intelligent ATCOs' mental state monitoring systems, which can achieve a more effective and safer human–machine cooperative pattern. However, the existing methods for recognizing ATCOs' MF face significant challenges due to pattern variations between ATCOs and sensor artifacts. This study introduces a framework for ATCOs' MF recognition, utilizing a deep neural network called RecMF, which incorporates multi-sensor information fusion to enhance the performance of MF detection. Specifically, the RecMF employs an attention-enabled CNN-LSTM architecture that simultaneously captures time-series feature representations of electroencephalogram (EEG) signals and eye movements. To validate the effectiveness of RecMF, a fatigue-inducing experiment is conducted involving 28 subjects who are tasked with performing a series of air traffic control (ATC) tasks. The model's performance is evaluated across various time horizons and typical cognitive tasks to gain insights into its capabilities. The evaluation results indicate that the proposed model outperforms other existing methods, thereby confirming its feasibility and effectiveness. Additionally, the effects of MF on ATCOs' cognitive performance are analyzed using analysis of variance (ANOVA). The results reveal that higher levels of MF significantly reduce ATCOs' reaction speed and operational accuracy.}
}
@incollection{SNOWDEN2015572,
title = {Semantic Memory},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {572-578},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.51059-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868510599},
author = {Julie S. Snowden},
keywords = {Amodal, Anterior temporal lobe, Brain networks, Conceptual knowledge, Distributed representations, Multimodal, Object knowledge, Schema, Semantic dementia, Semantic features, Semantic memory, Word knowledge},
abstract = {Semantic memory refers to our conceptual knowledge of the world. Understanding of semantic memory has come from several sources: cognitive studies of healthy individuals, computational modeling, patients with disordered semantic memory due to brain disease, and brain imaging and stimulation. The converging evidence indicates that semantic memory involves distributed brain networks, which, at least in part, are linked to the sensory processes involved in perception, action, and language. Whether there is also representation in amodal format remains an area of contention. Knowledge of the world, beyond word and object meanings, is a challenge for future studies of semantic memory.}
}
@incollection{CLEMENTSCROOME202413,
title = {Planning and Design Scenarios for Liveable Cities},
editor = {Martin A. Abraham},
booktitle = {Encyclopedia of Sustainable Technologies (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {13-30},
year = {2024},
isbn = {978-0-443-22287-0},
doi = {https://doi.org/10.1016/B978-0-323-90386-8.00008-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323903868000085},
author = {Derek Clements-Croome and Matthew Marson and Tong Yang and Miimu Airaksinen},
keywords = {City, Digital, Digital cities, Infrastructure, Innovation, Liveability, Quality of life, Sustainability, Sustainability indicators, Technology, Urbanity},
abstract = {As the urban populations increase, we must think more deeply about how to make cities less stressful and more creative for people to live in. Liveability and quality of life are key factors while designing and managing energy, water, pollution, and waste systems which are sustainable for the long term. The rapidly developing digital technologies can help to enable these aims to be achieved. Innovative approaches are proposed with recommendations for achieving these goals cities grow and change, digital technologies will be at the heart of improvement. This paper considers how observed trends will have implications on the digital cities of the future and how digital technologies will empower citizens and enable their cities to become intelligent, liveable, and sustainable.}
}
@article{TAMSTORF2013362,
title = {Discrete bending forces and their Jacobians},
journal = {Graphical Models},
volume = {75},
number = {6},
pages = {362-370},
year = {2013},
issn = {1524-0703},
doi = {https://doi.org/10.1016/j.gmod.2013.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1524070313000209},
author = {Rasmus Tamstorf and Eitan Grinspun},
keywords = {Discrete shells, Hinge angle Hessian, Bending force Jacobians, Dihedral angle},
abstract = {Computation of bending forces on triangle meshes is required for numerous simulation and geometry processing applications. In particular it is a key component in cloth simulation. A common quantity in many bending models is the hinge angle between two adjacent triangles. This angle is straightforward to compute, and its gradient with respect to vertex positions (required for the forces) is easily found in the literature. However, the Hessian of the bend angle, which is required to compute the associated force Jacobians is not documented in the literature. Force Jacobians are required for efficient numerics (e.g., implicit time stepping, Newton-based energy minimization) and are thus highly desirable. Readily available computations of the force Jacobian, such as those produced by symbolic algebra systems, or by autodifferentiation codes, are expensive to compute and therefore less useful. We present compact, easily reproducible, closed form expressions for the Hessian of the bend angle. Compared to automatic differentiation, we measure up to 7× speedup for the evaluation of the bending forces and their Jacobians.}
}