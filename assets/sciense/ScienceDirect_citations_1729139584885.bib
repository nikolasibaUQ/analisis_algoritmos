@article{2011827,
title = {In This Issue},
journal = {Cell},
volume = {144},
number = {6},
pages = {827-829},
year = {2011},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2011.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S0092867411002480},
abstract = {In thinking about complexity, it's frequently invoked that the whole is greater than the sum of its parts. This notion serves as one of the motivating principles of systems biology, which seeks to understand the emergent properties of complex biological systems. Among many biologists, systems biology is also synonymous with the use of particular approaches, including high-throughput techniques, large-scale integration of datasets, and computational modeling to probe system behaviors. There is indeed little doubt that the recent growth of the field has been fueled by the massive expansion in the amount of data being generated in the biological sciences—first from genome sequencing and more recently from such sources as transcriptomics, proteomics, and high-throughput imaging. Given this rising tide of data, there is an urgent need for new ways of analyzing large datasets and for conceptualizing biological complexity. It is in this context that we present our 2011 Special Review Issue on systems biology. The overarching goal of this collection is to highlight biological insights revealed by the quantitative and computational approaches associated with systems biology. To accomplish this, the issue includes topics that span vastly different size and time scales, from protein-protein interactions to disease models, from transcriptional dynamics to evolutionary processes. For the issue's diversity, depth, and thought-provoking insights, we would like to thank the many distinguished authors and reviewers who generously contributed their time and effort. In reading the issue, we hope that you will find that the collection, like biological systems, is more than the sum of its individual parts, providing a new perspective on this rapidly changing field.}
}
@article{WANG2024108446,
title = {Unraveling the distinction between depression and anxiety: A machine learning exploration of causal relationships},
journal = {Computers in Biology and Medicine},
volume = {174},
pages = {108446},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108446},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524005304},
author = {Tiantian Wang and Chuang Xue and Zijian Zhang and Tingting Cheng and Guang Yang},
keywords = {Anxiety, depression, Causal inference, Symptom checklist 90, Machine learning},
abstract = {Objective
Depression and anxiety, prevalent coexisting mood disorders, pose a clinical challenge in accurate differentiation, hindering effective healthcare interventions. This research addressed this gap by employing a streamlined Symptom Checklist 90 (SCL-90) designed to minimize patient response burden. Utilizing machine learning algorithms, the study sought to construct classification models capable of distinguishing between depression and anxiety.
Methods
The study included 4262 individuals currently experiencing depression alone (n = 2998), anxiety alone (n = 716), or both depression and anxiety (n = 548). Counterfactual diagnosis was used to construct a causal network on the dataset. Employing a causal network, the SCL-90 was simplified. Items that have causality with only depression, only anxiety and both depression and anxiety were selected, and these streamlined items served as input features for four distinct machine learning algorithms, facilitating the creation of classification models for distinguishing depression and anxiety.
Results
Cross-validation demonstrated the performance of the classification models with the following metrics: (1) K-nearest neighbors (AUC = 0.924, Acc = 92.81 %); (2) support vector machine (AUC = 0.937, Acc = 94.38 %); (3) random forest (AUC = 0.918, Acc = 94.38 %); and (4) adaptive boosting (AUC = 0.882, Acc = 94.38 %). Notably, the support vector machine excelled, with the highest AUC and superior accuracy.
Conclusion
Incorporating the simplified SCL-90 and machine learning presents a promising, efficient, and cost-effective tool for the precise identification of depression and anxiety.}
}
@article{DEMSKI2009133,
title = {Quantum information and accounting information: Exploring conceptual applications of topology},
journal = {Journal of Accounting and Public Policy},
volume = {28},
number = {2},
pages = {133-147},
year = {2009},
issn = {0278-4254},
doi = {https://doi.org/10.1016/j.jaccpubpol.2009.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0278425409000040},
author = {Joel S. Demski and Stephen A. FitzGerald and Yuji Ijiri and Yumi Ijiri and Haijin Lin},
keywords = {Topology, Quantum information, Topological quantum computation, Braid topology, Fibonacci anyons, Topology applications to accounting},
abstract = {Our previous attempt resulted in a paper by the same five authors, “Quantum information and accounting information: their salient features and conceptual applications,” published in the July–August 2006 issue of the Journal of Accounting and Public Policy. We now extend the previous paper to examine topological quantum computation, a remarkably innovative approach to decoherence and imprecise quantum computation. In this approach, exotic topological states are created for a natural medium to store and manipulate quantum information globally throughout the entire system. The process is intrinsically protected against imprecision and decoherence. We also explore conceptual, if not technical, applications of topological quantum computation to accounting. This is done by introducing topology’s inherent emphasis of qualitative characteristics to traditional accounting which has been dominated by quantitative characteristics. Here, financial statements’ monetary amounts may be contrasted to internal controls’ error frequencies. Part I of the paper deals with applications of topology to quantum information, after a brief introduction to basic tools. In particular the use of Fibonacci anyon and its powerful results are explained. Part II deals with applications of topology to accounting information. Part III deals with applications of topology to other potential fields.}
}
@article{CAPOTA2019204,
title = {Towards mixed criticality task scheduling in cyber physical systems: Challenges and perspectives},
journal = {Journal of Systems and Software},
volume = {156},
pages = {204-216},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.06.099},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219301426},
author = {Eugenia Ana Capota and Cristina Sorina Stangaciu and Mihai Victor Micea and Daniel-Ioan Curiac},
keywords = {Cyber physical systems, Real-time scheduling, Mixed criticality systems, Multiple processing units},
abstract = {Cyber physical systems (CPSs) are a fast-evolving technology based on a strong synergy between heterogeneous sensing, networking, computation and control modules. When coping with critical applications that require real-time performance and autonomous operation in uncertain conditions, the design of such complex systems is still facing significant difficulties. A particular challenge in this respect derives from the software intensive nature of these systems - the need to develop flexible and specifically tailored task scheduling techniques. In our view, an appropriate line of thinking is to take advantage of mixed criticality concepts following the lessons learned from avionics and automotive domains, where complexity, safety, determinism and real-time constraints are extreme. From this perspective, our work aims at facilitating the integration of mixed criticality systems-based strategy in cyber physical systems by identifying the particularities of the latter and their influence on scheduling mechanisms, by describing the standard mixed-criticality task model in the cyber physical systems context, and by analyzing and proposing the most suitable scheduling algorithms to be implemented in cyber physical systems. Moreover, the perspectives on future developments in this area are discussed, as new horizons in research arise with the integration of mixed criticality concepts in the cyber physical systems context.}
}
@article{YU20221796,
title = {Deep learning in target prediction and drug repositioning: Recent advances and challenges},
journal = {Drug Discovery Today},
volume = {27},
number = {7},
pages = {1796-1814},
year = {2022},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2021.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S1359644621004487},
author = {Jun-Lin Yu and Qing-Qing Dai and Guo-Bo Li},
keywords = {Deep learning, Drug repositioning, Target prediction, Drug–target interaction, Heterogeneous network, Drug discovery},
abstract = {Drug repositioning is an attractive strategy for discovering new therapeutic uses for approved or investigational drugs, with potentially shorter development timelines and lower development costs. Various computational methods have been used in drug repositioning, promoting the efficiency and success rates of this approach. Recently, deep learning (DL) has attracted wide attention for its potential in target prediction and drug repositioning. Here, we provide an overview of the basic principles of commonly used DL architectures and their applications in target prediction and drug repositioning, and discuss possible ways of dealing with current challenges to help achieve its expected potential for drug repositioning.}
}
@article{GARCIAMOLINA2024,
title = {Automatic Speech Recognition in Psychiatric Interviews: A Rocket to Diagnostic Support in Psychosis},
journal = {Revista Colombiana de Psiquiatría},
year = {2024},
issn = {0034-7450},
doi = {https://doi.org/10.1016/j.rcp.2023.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0034745024000027},
author = {José Tomás {García Molina} and Pablo A. Gaspar and Alicia Figueroa-Barra},
keywords = {Speech recognition software, Psychotic disorder, Natural language processing, Software de reconocimiento del habla, Trastornos psicóticos, Procesamiento de lenguaje natural},
abstract = {Speech analysis is a crucial tool in discerning the complex cognitive and emotional subtleties of individuals. It holds a significant role in psychiatric research, particularly in the detection and understanding of psychopathological conditions such as psychosis. The process involves computational analysis of speech using natural language processing (NLP) tools, which necessitates a transcription of the speech. However, the manual transcription process is both time-consuming and costly, posing a substantial challenge to large-scale investigations. To address this, we explore the use of “Whisper”, an automated speech recognition (ASR) tool developed by OpenAI©, for transcribing psychiatric interviews in Spanish in heterogeneous environmental conditions. The specific objectives are to compare the transcription accuracy of Whisper with a manual transcription, determine and compare linguistic elements (noun phrases, determiners, and type–token ratio), and examine environmental elements that could alter the quality of the transcription. Sixteen interviews were transcribed using Whisper, and all of them had a manual reference transcription to be compared. A word error ratio (WER, which measures the insertions, deletions, and substitutions that are required to change one word for another) of 7.80% was obtained, with no significant differences by gender. Furthermore, no differences were found in the count and proportionality of nominal phrases, use of determiners, and the type–token ratio (TTR). The findings indicate that Whisper is a precise instrument for transcribing clinical interviews in Spanish. It has a minimal error rate and negligible loss of linguistic data, even in adverse conditions. This could streamline large-scale research endeavors in speech analysis within the clinical domain.
Resumen
El estudio de la producción lingüística es una vía clave para conocer el complejo mundo cognitivo y emocional de las personas, particularmente dentro de la investigación en psiquiatría, donde guarda un rol crucial en la definición de condiciones psicopatológicas como, por ejemplo, la psicosis. Sin embargo, el estudio del lenguaje mediante técnicas de procesamiento de lenguaje natural (NLP) posee una limitante asociada a la transcripción, hasta hoy en día realizada principalmente a mano, significando un desafío temporal y económico, que limita la investigación en este ámbito. Por aquello en este estudio exploramos el uso de «Whisper», una herramienta de reconocimiento automático del habla (ASR) desarrollada por OpenAI©, para transcribir entrevistas fenomenológicas en español en condiciones ambientales heterogéneas. Los objetivos específicos son comparar la precisión de la transcripción de Whisper con una transcripción manual, determinar y comparar los elementos lingüísticos (frases nominales, determinantes y relación tipo-token), y examinar los elementos ambientales que podrían alterar la calidad de la transcripción. Se transcribieron dieciséis entrevistas utilizando Whisper, todas ellas tenían una transcripción de referencia manual para comparar. Se obtuvo una tasa palabra-error (WER, que comprende la medición de inserciones, deleciones y sustituciones necesarias para cambiar una palabra por otra) de 7,80%, sin diferencias significativas por género. Además, no se encontraron diferencias en el recuento y proporcionalidad de las frases nominales, el uso de determinantes y la relación tipo-token (TTR). Los hallazgos indican que Whisper es un instrumento preciso para transcribir entrevistas clínicas en español. Tiene una tasa de error mínima y una pérdida despreciable de datos lingüísticos, incluso en condiciones adversas. Esto podría facilitar la investigación a gran escala en el análisis del habla dentro del dominio clínico.}
}
@incollection{PIOCHI202245,
title = {Chapter Two - From single-omics to interactomics: How can ligand-induced perturbations modulate single-cell phenotypes?},
editor = {Rossen Donev},
series = {Advances in Protein Chemistry and Structural Biology},
publisher = {Academic Press},
volume = {131},
pages = {45-83},
year = {2022},
booktitle = {Protein Interaction Networks},
issn = {1876-1623},
doi = {https://doi.org/10.1016/bs.apcsb.2022.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S1876162322000487},
author = {L.F. Piochi and A.T. Gaspar and N. Rosário-Ferreira and A.J. Preto and I.S. Moreira},
keywords = {Single-cell, Multi-omics, Integrative analysis, Drug development, Perturbations, Ligands},
abstract = {Cells suffer from perturbations by different stimuli, which, consequently, rise to individual alterations in their profile and function that may end up affecting the tissue as a whole. This is no different if we consider the effect of a therapeutic agent on a biological system. As cells are exposed to external ligands their profile can change at different single-omics levels. Detecting how these changes take place through different sequencing technologies is key to a better understanding of the effects of therapeutic agents. Single-cell RNA-sequencing stands out as one of the most common approaches for cell profiling and perturbation analysis. As a result, single-cell transcriptomics data can be integrated with other omics data sources, such as proteomics and epigenomics data, to clarify the perturbation effects and mechanism at the cell level. Appropriate computational tools are key to process and integrate the available information. This chapter focuses on the recent advances on ligand-induced perturbation and single-cell omics computational tools and algorithms, their current limitations, and how the deluge of data can be used to improve the current process of drug research and development.}
}
@article{FRIEDMAN2004167,
title = {Training the next generation of informaticians: the impact of “BISTI” and bioinformatics—A report from the American College of Medical Informatics},
journal = {Journal of the American Medical Informatics Association},
volume = {11},
number = {3},
pages = {167-172},
year = {2004},
issn = {1067-5027},
doi = {https://doi.org/10.1197/jamia.M1520},
url = {https://www.sciencedirect.com/science/article/pii/S1067502704000118},
author = {Charles P. Friedman and Russ B. Altman and Isaac S. Kohane and Kathleen A. McCormick and Perry L. Miller and Judy G. Ozbolt and Edward H. Shortliffe and Gary D. Stormo and M.Cleat Szczepaniak and David Tuck and Jeffrey Williamson},
abstract = {In 2002–2003, the American College of Medical Informatics (ACMI) undertook a study of the future of informatics training. This project capitalized on the rapidly expanding interest in the role of computation in basic biological research, well characterized in the National Institutes of Health (NIH) Biomedical Information Science and Technology Initiative (BISTI) report. The defining activity of the project was the three-day 2002 Annual Symposium of the College. A committee, comprised of the authors of this report, subsequently carried out activities, including interviews with a broader informatics and biological sciences constituency, collation and categorization of observations, and generation of recommendations. The committee viewed biomedical informatics as an interdisciplinary field, combining basic informational and computational sciences with application domains, including health care, biological research, and education. Consequently, effective training in informatics, viewed from a national perspective, should encompass four key elements: (1) curricula that integrate experiences in the computational sciences and application domains rather than just concatenating them; (2) diversity among trainees, with individualized, interdisciplinary cross-training allowing each trainee to develop key competencies that he or she does not initially possess; (3) direct immersion in research and development activities; and (4) exposure across the wide range of basic informational and computational sciences. Informatics training programs that implement these features, irrespective of their funding sources, will meet and exceed the challenges raised by the BISTI report, and optimally prepare their trainees for careers in a field that continues to evolve.}
}
@article{GILLINGS2017121,
title = {Mapping liminality: Critical frameworks for the GIS-based modelling of visibility},
journal = {Journal of Archaeological Science},
volume = {84},
pages = {121-128},
year = {2017},
note = {Archaeological GIS Today: Persistent Challenges, Pushing Old Boundaries, and Exploring New Horizons},
issn = {0305-4403},
doi = {https://doi.org/10.1016/j.jas.2017.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0305440317300675},
author = {Mark Gillings},
keywords = {Viewshed, Relationality, Assemblage, Liminality, Emergence},
abstract = {Since the widespread adoption of GIS by archaeologists in the early 1990s, analyses of visibility have steadily gained traction, becoming commonplace in landscape and regional analysis. This is in large part due to the routine way in which such products can be generated, bolstered by a raft of landscape-based studies that have placed varying degrees of emphasis upon human perception and direct bodily engagement in seeking to understand and explore the past. Despite this seeming popularity, two worrying trends stand out. The first is the lack of any coherent theoretical framework, applications preferring instead to seek justification in the very first wave of experiential landscape approaches that emerged in the early 1990s. Needless to say, the intervening 20 or so years have seen considerable development in the conceptual tools we draw upon in order to make sense of past landscapes, not to mention considerable finessing of the first-wave developments alluded to above. Second is the tendency to relegate viewshed analysis to certain types of predictable problem or question (i.e. viewshed analysis has become typecast). These trends have been compounded by a host of other issues. For example, whilst there have been refinements, tweaks and variations to the basic viewshed (and the frequency with which they are generated and combined), not to mention establishment of robust calibration criteria for controlling them and statistical approaches for assessing the patterns tendered, these have yet to be brought together in any coherent fashion and their veracity critically assessed. Likewise, a failure to establish an agreed vocabulary has resulted in a number of proverbial wheels being reinvented time and again. The argument presented here is that viewsheds have considerably more to offer archaeology but to realise this entails confronting these issues head on. That this is possible and desirable is illustrated through discussion of a new theoretical framework for visibility-studies that draws upon developments in assemblage theory and the author's own work on affordance and relationality. To demonstrate the value of this approach in encouraging different ways of thinking about what viewsheds are and how we might begin to draw creatively upon them, a case-study is described where viewsheds are folded into a detailed exploration of landscape liminality.}
}
@article{PINHEIRO2024100234,
title = {Eye-tracker and fNIRS: Using neuroscientific tools to assess the learning experience during children's educational robotics activities},
journal = {Trends in Neuroscience and Education},
volume = {36},
pages = {100234},
year = {2024},
issn = {2211-9493},
doi = {https://doi.org/10.1016/j.tine.2024.100234},
url = {https://www.sciencedirect.com/science/article/pii/S2211949324000152},
author = {Eneyse Dayane Pinheiro and João Ricardo Sato and Raimundo da Silva Soares Junior and Candida Barreto and Amanda Yumi Ambriola Oku},
keywords = {STEM, STEAM, Student-centered approaches, Educational robotics, fNIRS, Eye-tracker},
abstract = {In technology education, there has been a paradigmatic shift towards student-centered approaches such as learning by doing, constructionism, and experiential learning. Educational robotics allows students to experiment with building and interacting with their creations while also fostering collaborative work. However, understanding the student's response to these approaches is crucial to adapting them during the teaching-learning process. In this sense, neuroscientific tools such as Functional Near-Infrared Spectroscopy and Eye-tracker could be useful, allowing the investigation of relevant states experienced by students. Although they have already been used in educational research, their practical relevance in the teaching-learning process has not been extensively investigated. In this perspective article expressing our position, we bring four examples of learning experiences in a robotics class with children, in which we illustrate the usefulness of these tools.}
}
@article{WILSON202277,
title = {Kinmaking, progeneration, and ethnography},
journal = {Studies in History and Philosophy of Science},
volume = {91},
pages = {77-85},
year = {2022},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2021.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0039368121001588},
author = {Robert A. Wilson},
keywords = {Kinship and kinmaking, Performativism about kinship, Biology and kinship, Constructivism about kinship, Philosophy of anthropology, Extensionism and kin terminologies},
abstract = {Philosophers of biology and biologists themselves for the most part assume that the concept of kin is progenerative: what makes two individuals kin is a direct or indirect function of reproduction. Derivatively, kinship might likewise be presumed to be progenerative in nature. Yet a prominent view of kinship in contemporary cultural anthropology is a kind of constructivism or performativism that rejects such progenerativist views. This paper critically examines an influential line of thinking used to critique progenerativism and support performativism that cites cross-cultural diversity in what I will call kinmaking. I challenge several key assumptions made in moving from this appeal to ethnography to conclusions about kinship and progeneration, arguing that closer scrutiny of both the ethnographic record and inferences that draw on it in fact support progenerative views of kinmaking.}
}
@article{VARY2014155,
title = {Quantum Hamiltonian Physics with Supercomputers},
journal = {Nuclear Physics B - Proceedings Supplements},
volume = {251-252},
pages = {155-164},
year = {2014},
note = {International Conference on Light-Cone Physics: Hadronic and Particle Physics},
issn = {0920-5632},
doi = {https://doi.org/10.1016/j.nuclphysbps.2014.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0920563214000929},
author = {James P. Vary},
keywords = {Computational Physics, ab initio Nuclear Theory},
abstract = {The vision of solving the nuclear many-body problem in a Hamiltonian framework with fundamental interactions tied to QCD via Chiral Perturbation Theory is gaining support. The goals are to preserve the predictive power of the underlying theory, to test fundamental symmetries with the nucleus as laboratory and to develop new understandings of the full range of complex quantum phenomena. Advances in theoretical frameworks (renormalization and many-body methods) as well as in computational resources (new algorithms and leadership-class parallel computers) signal a new generation of theory and simulations that will yield profound insights into the origins of nuclear shell structure, collective phenomena and complex reaction dynamics. Fundamental discovery opportunities also exist in such areas as physics beyond the Standard Model of Elementary Particles, the transition between hadronic and quark–gluon dominated dynamics in nuclei and signals that characterize dark matter. I will review some recent achievements and present ambitious consensus plans along with their challenges for a coming decade of research that will build new links between theory, simulations and experiment. Opportunities for graduate students to embark upon careers in the fast developing field of supercomputer simulations is also discussed.}
}
@article{HUK2012173,
title = {Multiplexing in the primate motion pathway},
journal = {Vision Research},
volume = {62},
pages = {173-180},
year = {2012},
issn = {0042-6989},
doi = {https://doi.org/10.1016/j.visres.2012.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0042698912001137},
author = {Alexander C. Huk},
keywords = {Motion, Multiplexing, 3D, Encoding, Decoding, Interocular velocity difference},
abstract = {This article begins by reviewing recent work on 3D motion processing in the primate visual system. Some of these results suggest that 3D motion signals may be processed in the same circuitry already known to compute 2D motion signals. Such “multiplexing” has implications for the study of visual cortical circuits and neural signals. A more explicit appreciation of multiplexing—and the computations required for demultiplexing—may enrich the study of the visual system by emphasizing the importance of a structured and balanced “encoding/decoding” framework. In addition to providing a fresh perspective on how successive stages of visual processing might be approached, multiplexing also raises caveats about the value of “neural correlates” for understanding neural computation.}
}
@article{VANKDOTHU2022107960,
title = {A Brain Tumor Identification and Classification Using Deep Learning based on CNN-LSTM Method},
journal = {Computers and Electrical Engineering},
volume = {101},
pages = {107960},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.107960},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622002361},
author = {Ramdas Vankdothu and Mohd Abdul Hameed and Husnah Fatima},
keywords = {A Brain Tumor, Convolutional Neural Network(CNN), Classification, Deep Learning, Magnetic Resonance Imaging, LSTM},
abstract = {Brain tumors are one of the most often diagnosed malignant tumors in persons of all ages. Recognizing its grade is challenging for radiologists in health monitoring and automated determination; however, IoT can help. It is critical to detect and classify contaminated tumor locations using Magnetic Resonance Imaging (MRI) images. Numerous tumors exist, including glioma tumor, meningioma tumor, pituitary tumor, and no tumor (benign). Detecting the type of tumor and preventing it is one of the most challenging aspects of brain tumor categorization. Numerous deep learning-based approaches for categorizing brain tumors have been published in the literature. A CNN (Convolutional Neural Network), the most advanced method in deep learning, was used to detect a tumor using brain MRI images. However, there are still issues with the training procedure, which is lengthy. The main goal of this project is to develop an IoT computational system based on deep learning for detecting brain tumors in MRI images. This paper suggests combining A CNN(Convolutional Neural Network) with an STM(Long Short Term Memory), LSTMs can supplement the ability of CNN to extract features. When used for image classification, the layered LSTM-CNN design outperforms standard CNN classification. Experiments are undertaken to forecast the proposed model's performance using the Kaggle data set, which contains 3264 MRI scans. The dataset is separated into two sections: 2870 photos of training sets and 394 images of testing sets. The experimental findings demonstrate that the proposed model outperforms earlier CNN and RNN models in terms of accuracy.}
}
@article{DAS2021102525,
title = {Schizophrenia detection technique using multivariate iterative filtering and multichannel EEG signals},
journal = {Biomedical Signal Processing and Control},
volume = {67},
pages = {102525},
year = {2021},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2021.102525},
url = {https://www.sciencedirect.com/science/article/pii/S1746809421001221},
author = {Kritiprasanna Das and Ram Bilas Pachori},
keywords = {EEG, EEG rhythm separation, Iterative filtering, Multivariate iterative filtering, Schizophrenia diagnosis},
abstract = {A new approach for extension of univariate iterative filtering (IF) for decomposing a signal into intrinsic mode functions (IMFs) or oscillatory modes is proposed for multivariate multi-component signals. Additionally the paper proposes a method to detect schizophrenia (Sz), based on analysing multi-channel electroencephalogram (EEG) signals. Using proposed multivariate iterative filtering (MIF), multi-channel EEG data are decomposed into multivariate IMFs (MIMFs). Depends on mean frequency, IMFs are grouped in order to separate EEG rhythms (delta, theta, alpha, beta, gamma) from EEG signals. The features, such as Hjorth parameters are extracted from EEG rhythms. Extracted features are ranked using student t-test and most discriminant 30 features are used for classification. Different classifier such as K-nearest neighbours (K-NN), linear discriminant analysis (LDA), support vector machine (SVM) with diffident kernels are considered to classify Sz and healthy EEG patterns. The proposed method is employed to evaluate 19-channel EEG signals recorded from 14 paranoid Sz patients and 14 healthy subjects. We have achieved highest accuracy of 98.9% using the SVM (Cubic) classifier. Sensitivity, specificity, positive predictive value (PPV), and area under ROC curve (AUC) of the same classifier are 99.0%, 98.8%, 98.4% and 0.999 respectively. Proposed approach for MIF is computationally efficient as compared to other multivariate signal decomposition algorithms. This paper presents a framework for decomposing multivariate signals efficiently and builds a model for detecting Sz accurately.}
}
@article{SHEN2022101006,
title = {Analytical aspects of meet-in-metabolite analysis for molecular pathway reconstitution from exposure to adverse outcome},
journal = {Molecular Aspects of Medicine},
volume = {87},
pages = {101006},
year = {2022},
note = {Molecular Aspects of the Exposome and Metabolic Diseases},
issn = {0098-2997},
doi = {https://doi.org/10.1016/j.mam.2021.101006},
url = {https://www.sciencedirect.com/science/article/pii/S0098299721000662},
author = {Heqing Shen and Yike Zhang and Karl-Werner Schramm},
keywords = {Human biomonitoring, Molecular exposome, Metabolome, Non-targeted analysis, Adverse outcome pathway, System epidemiology},
abstract = {To explore the etiology of diseases is one of the major goals in epidemiological study. Meet-in-metabolite analysis reconstitutes biomonitoring-based adverse outcome (AO) pathways from environmental exposure to a disease, in which the chemical exposome-related metabolism responses are transmitted to incur the AO-related metabolism phenotypes. However, the ongoing data-dependent acquisition of non-targeted biomonitoring by high-resolution mass spectrometry (HRMS) is biased against the low abundance molecules, which forms the major of molecular internal exposome, i.e., the totality of trace levels of environmental pollutants and/or their metabolites in human samples. The recent development of data-independent acquisition protocols for HRMS screening has opened new opportunities to enhance unbiased measurement of the extremely low abundance molecules, which can encompass a wide range of analytes and has been applied in metabolomics, DNA, and protein adductomics. In addition, computational MS for small molecules is urgently required for the top-down exposome databases. Although a holistic analysis of the exposome and endogenous metabolites is plausible, multiple and flexible strategies, instead of “putting one thing above all” are proposed.}
}
@article{LUCAS2020100425,
title = {Responsible modelling: Unit testing for infectious disease epidemiology},
journal = {Epidemics},
volume = {33},
pages = {100425},
year = {2020},
issn = {1755-4365},
doi = {https://doi.org/10.1016/j.epidem.2020.100425},
url = {https://www.sciencedirect.com/science/article/pii/S1755436520300451},
author = {Tim C.D. Lucas and Timothy M Pollington and Emma L Davis and T Déirdre Hollingsworth},
keywords = {Unit testing, Software development, Reproducible science, Computational models},
abstract = {Infectious disease epidemiology is increasingly reliant on large-scale computation and inference. Models have guided health policy for epidemics including COVID-19 and Ebola and endemic diseases including malaria and tuberculosis. Yet a coding bug may bias results, yielding incorrect conclusions and actions causing avoidable harm. We are ethically obliged to make our code as free of error as possible. Unit testing is a coding method to avoid such bugs, but it is rarely used in epidemiology. We demonstrate how unit testing can handle the particular quirks of infectious disease models and aim to increase the uptake of this methodology in our field.}
}
@incollection{SEJNOWSKI198994,
title = {6 - The Hebb Rule for Synaptic Plasticity: Algorithms and Implementations},
editor = {John H. Byrne and William O. Berry},
booktitle = {Neural Models of Plasticity},
publisher = {Academic Press},
pages = {94-103},
year = {1989},
isbn = {978-0-12-148955-7},
doi = {https://doi.org/10.1016/B978-0-12-148955-7.50010-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780121489557500102},
author = {Terrence J. Sejnowski and Gerald Tesauro},
abstract = {Publisher Summary
The Hebb rule and variations on it have served as the starting point for the study of information storage in simplified neural network models. This chapter presents a framework within which the Hebb rule and other related learning algorithms that serve as an important link between the implementation level of analysis, which is the level at which experimental work on neural mechanisms takes place, and the computational level, on which the behavioral aspects of learning and perception are studied. The chapter illustrates how the Hebb rule can be built out of realistic neural components in several different ways. The notion of an algorithm is central in thinking about information processing in the nervous system. The chapter describes three methods for implementing the Hebb rule, which can be used to form associations between one stimulus and another. Such associations can be either static or they can be temporal.}
}
@article{KONG2024132846,
title = {Enhancing data center cooling efficiency and ability: A comprehensive review of direct liquid cooling technologies},
journal = {Energy},
volume = {308},
pages = {132846},
year = {2024},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2024.132846},
url = {https://www.sciencedirect.com/science/article/pii/S0360544224026203},
author = {Rui Kong and Hainan Zhang and Mingsheng Tang and Huiming Zou and Changqing Tian and Tao Ding},
keywords = {Data center, Direct liquid cooling, Immersion cooling, Spray cooling, Direct microchannel liquid cooling},
abstract = {As data centers increasingly become the backbone of the digital age, managing their substantial energy consumption and mitigating heat generation are paramount. This paper focuses on direct liquid cooling as a transformative technology for enhancing energy efficiency and operational safety in high-density computing environments. Significant advancements and persisting challenges in this field have been identified by analyzing various direct liquid cooling methodologies including immersion, spray/jet, and microchannel cooling. Comparative analysis of these systems reveals their potential to substantially lower thermal resistances and improve energy utilization. Despite these advancements, the research highlights persistent challenges such as integration complexities and scalability issues. Recommendations for future research directions to improve the efficiency and ability of direct liquid cooling applications in data centers were concluded, emphasizing the need for user-friendly and cost-effective solutions, comprehensive guidelines for cooling method selection, and hybrid cooling systems. It also recommends advanced energy management strategies such as real-time power adjustment that dynamically matches energy supply with computational demand to optimize efficiency. These contributions underscore the importance of advancing data center cooling technologies to meet future demands.}
}
@article{TALAOUI2023102290,
title = {Recovering the divide: A review of the big data analytics—strategy relationship},
journal = {Long Range Planning},
volume = {56},
number = {2},
pages = {102290},
year = {2023},
issn = {0024-6301},
doi = {https://doi.org/10.1016/j.lrp.2022.102290},
url = {https://www.sciencedirect.com/science/article/pii/S0024630122001091},
author = {Yassine Talaoui and Marko Kohtamäki and Mikko Ranta and Sotirios Paroutis},
keywords = {Review, Big data analytics, Strategy, Practice, Materiality, Semiotics},
abstract = {Research on big data analytics has been burgeoning in recent decades, yet its relationship with strategy continues to be overlooked. This paper reviews how big data analytics and strategy are portrayed across 228 articles, identifying two dominant discourses: an input-output discourse that views big data analytics as a computational capability supplementing prospective strategy formulation and an entanglement discourse that theorizes big data analytics as a socially constructed agent that (re)shapes the emergent character of strategy formation. We deconstruct the inherent dichotomies of the input-output/entanglement divide and reveal how both discourses adopt disjointed positions vis-à-vis relational causality and agency. We elaborate a semiotic view of big data analytics and strategy that transcends this standoff and provides a novel theoretical account for conjoined relationality between big data analytics and strategy.}
}
@incollection{FISHER19913,
title = {CHAPTER 1 - Computational Models of Concept Learning},
editor = {Douglas H. Fisher and Michael J. Pazzani and Pat Langley},
booktitle = {Concept Formation},
publisher = {Morgan Kaufmann},
pages = {3-43},
year = {1991},
isbn = {978-1-4832-0773-5},
doi = {https://doi.org/10.1016/B978-1-4832-0773-5.50007-9},
url = {https://www.sciencedirect.com/science/article/pii/B9781483207735500079},
author = {DOUG FISHER and MICHAEL PAZZANI},
abstract = {Publisher Summary
This chapter presents the models of inductive learning. Concept formation and unsupervised learning generally are not viewed as methods of improving performance. Rather, an implicit assumption is that the primary performance task of interest for unsupervised methods is communicability or rediscovery. However, it is suggested that an important evaluation task for these systems is attribute prediction. An explicit consideration of suitable performance tasks can have significant implications on the design of both psychological and computational models of unsupervised learning, but the importance of this observation is sometimes overlooked. In addition, research on concept formation continues to progress in several directions that are shared by other learning paradigms. For example, important areas concern more complete representation languages for objects and concepts, notably structured descriptions that place an added burden on search. The complications caused by noise in the environment are a traditional research topic in supervised scenarios, and it is receiving increased attention in concept formation and unsupervised learning.}
}
@article{ADIGUN2024100341,
title = {Reducing mathematics anxiety among deaf learners through relaxation and rational emotive behaviour therapies: A randomised-control study},
journal = {International Journal of Educational Research Open},
volume = {7},
pages = {100341},
year = {2024},
issn = {2666-3740},
doi = {https://doi.org/10.1016/j.ijedro.2024.100341},
url = {https://www.sciencedirect.com/science/article/pii/S2666374024000232},
author = {Olufemi Timothy Adigun and Oladipupo ‘W. Omobosola and Malephoto Niko Ruth Lephoto and Gideon Kwesi Obosu},
keywords = {Rational emotive behavioural therapy, Relaxation therapy, Mathematics anxiety, Deaf learners, Deafness},
abstract = {The effects of Rational Emotive Behavioural Therapy (REBT) and Relaxation Therapy (RT) on mathematics anxiety (MA) were examined among Deaf learners (DLs) in Oyo State in Nigeria. The randomized controlled study adopted a purposive sampling procedure to select three schools for the Deaf and Deaf learners (DLs) in Oyo State. A random sampling procedure was employed to select 60 DLs who were assigned to two experimental groups REBT (n = 25; male = 10; female = 15), RT (n = 17; male = 8; female = 9), and a control group (n = 20; male = 11; female = 9). The Mathematics Anxiety Scale (MAS) was used to screen the participants. The data gathered were analysed using the analysis of variance and descriptive charts. The findings revealed the efficacy of the two therapeutic interventions in reducing MA among DLs. The estimated mean difference between the treatment and control groups showed the following: REBT (15.66), RT (11.63), and control group (9.99). This study, therefore, concluded that REBT and RT were effective at drastically reducing Deaf learners’ anxiety regarding mathematics. Appropriate recommendations were made and implications were highlighted for practice, policy, and research, based on the findings}
}
@article{MORO2015147,
title = {Detecting syntactic and semantic anomalies in schizophrenia},
journal = {Neuropsychologia},
volume = {79},
pages = {147-157},
year = {2015},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2015.10.030},
url = {https://www.sciencedirect.com/science/article/pii/S0028393215302062},
author = {Andrea Moro and Valentina Bambini and Marta Bosia and Simona Anselmetti and Roberta Riccaboni and Stefano F. Cappa and Enrico Smeraldi and Roberto Cavallaro},
keywords = {Language, Schizophrenia, Syntax, Semantics, Anomaly, Grammaticality judgments},
abstract = {One of the major challenges in the study of language in schizophrenia is to identify specific levels of the linguistic structure that might be selectively impaired. While historically a main semantic deficit has been widely claimed, results are mixed, with also evidence of syntactic impairment. This might be due to heterogeneity in materials and paradigms across studies, which often do not allow to tap into single linguistic components. Moreover, the interaction between linguistic and neurocognitive deficits is still unclear. In this study, we concentrated on syntactic and semantic knowledge. We employed an anomaly detection task including short and long sentences with either syntactic errors violating the principles of Universal Grammar, or a novel form of semantic errors, resulting from a contradiction in the computation of the whole sentence meaning. Fifty-eight patients with diagnosis of schizophrenia were compared to 30 healthy subjects. Results showed that, in patients, only the ability to identify syntactic anomaly, both in short and long sentences, was impaired. This result cannot be explained by working memory abilities or psychopathological features. These findings suggest the presence of an impairment of syntactic knowledge in schizophrenia, at least partially independent of the cognitive and psychopathological profile. On the contrary, we cannot conclude that there is a semantic impairment, at least in terms of compositional semantics abilities.}
}
@article{HURT2023111142,
title = {Narrative review of mathematical and psychological studies of staff scheduling for holidays as applicable to anesthesiologists and nurse anesthetists},
journal = {Journal of Clinical Anesthesia},
volume = {88},
pages = {111142},
year = {2023},
issn = {0952-8180},
doi = {https://doi.org/10.1016/j.jclinane.2023.111142},
url = {https://www.sciencedirect.com/science/article/pii/S0952818023000922},
author = {Grant M. Hurt and Franklin Dexter},
keywords = {Anesthesia department, Fairness, Hospital administration, Industrial engineering, Mathematical programming, Staff scheduling},
abstract = {We performed a narrative review of articles applicable to anesthesiologists' and nurse anesthetists' choices of who works each statutory holiday for operating room and non-operating room anesthesia. We include search protocols and detailed supplementary annotated comments. Studies showed that holiday staff scheduling is emotional. Working on holidays often is more stressful and undesirable than comparable workdays. Intrinsic motivation may overall, among practitioners, be greater by preferentially scheduling practitioners who choose to work on holidays, for compensation, before mandating that practitioners who would prefer to be off must work on holidays. Granting each practitioner (who so desires) at least one major holiday off can depend on identifying and scheduling other clinicians who want to work holidays for monetary compensation or extra compensatory time off. Scheduling holidays by random priority (i.e., a lottery choosing who gets to pick their holiday[s] first, second, etc.) is inefficient, resulting in fewer practitioners having their preferences satisfied, especially for small departments or divisions (e.g., cardiac anesthesia). No article that we reviewed implemented a random priority mechanism for staff scheduling. The selection of practitioners to take turns in choosing their holidays is perceived to have less fairness than a selection process that collects each participants' preferences. Although holidays often are scheduled separately from regular workdays and weekends, doing so will not increase efficiency or fairness. Holidays can, in practice, be scheduled simultaneously with non-holidays. Models can explicitly include fairness as an objective. For example, fairness can be based on the difference between the maximum and minimum number of holidays for which practitioners of the same division are scheduled. Holidays can be given greater weights than other shifts when estimating fairness. Staff scheduling for holidays, when done simultaneously with regular workdays, nights, and weekends, can also use personalized weights, specifying practitioners' preferences to be satisfied if possible.}
}
@article{COSTANZA2023104059,
title = {Are generations a useful concept?},
journal = {Acta Psychologica},
volume = {241},
pages = {104059},
year = {2023},
issn = {0001-6918},
doi = {https://doi.org/10.1016/j.actpsy.2023.104059},
url = {https://www.sciencedirect.com/science/article/pii/S0001691823002354},
author = {David P. Costanza and Cort W. Rudolph and Hannes Zacher},
keywords = {Generations, Age, period, cohort, Social constructionism, lifespan theory},
abstract = {The concepts of generations and generational differences have received much attention in the academic literature, in the popular press, and among practitioners, policymakers, and politicians. Despite the continued interest, research has failed to find convincing evidence for the existence of distinct generations, commonly conceptualized as broad groupings of birth cohorts (e.g., 1980–2000) that have been influenced by a set of significant events (e.g., economic depressions) and labeled with names and qualities that supposedly reflect their defining characteristics (e.g., Millennials). Further, any differences that have been found in empirical studies, and that have been attributed to generational membership, are more likely due to age and/or contemporaneous period effects. Nonetheless, some researchers, employers, institutions, governments, and many laypeople continue to treat generations like they are a powerful and actionable phenomenon. We address these issues in two ways. First, we review the science of generations, focusing on what is known, what is not, and why the evidence points to the conclusion that generations, as popularly conceptualized, do not exist in objectively quantifiable ways. We also address the disconnect between science and practice regarding generations. Second, we explore alternate explanations for effects that are attributed to generations and review approaches that are both more theoretically sound and empirically supported, including lifespan theory and social constructionist frameworks. Finally, we address connections between assumptions made about generations and concerns about diversity, equity, and inclusion at work. Specifically, we address what has been termed generationalism, the belief that members of specific generations possess unique, stereotypic characteristics.}
}
@incollection{MOHAN2025473,
title = {Chapter 45 - Advanced EEG signal processing and feature extraction concepts},
editor = {M.A. Ansari and R.S. Anand and Pragati Tripathi and Rajat Mehrotra and Md Belal Bin Heyat},
booktitle = {Artificial Intelligence in Biomedical and Modern Healthcare Informatics},
publisher = {Academic Press},
pages = {473-483},
year = {2025},
isbn = {978-0-443-21870-5},
doi = {https://doi.org/10.1016/B978-0-443-21870-5.00045-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780443218705000455},
author = {Anand Mohan and R.S. Anand},
keywords = {Cognition, Digital signal processing, EEG, ERPs, Feature extraction, Signal decomposition, Signal processing},
abstract = {The electroencephalogram (EEG) signal processing is a noninvasive technique, which uses scalp electrodes for measuring the electrical activity of the brain. EEG signals are very random in nature, and thus, it is extremely difficult to find any relevant information from these EEG signals just by seeing them in time domain. Thus, various advanced EEG signal processing techniques are required to get the information from these highly complex biomedical signals to analyze the brain states. EEG signals can be used effectively in rehabilitation by using brain–computer interfaces (BCIs). In BCI, we utilize brain signals to give a message or for controlling an assistive instrument. Imagined speech also uses BCI to act as a method to communicate with patients with paralyzed muscles and locked-in syndromes. Imagined speech meaning is that a person is thinking some words without any movement of articulators. By applying advanced signal processing techniques and classification methods, we could help these patients to interact with others using their EEG signals. This chapter will discuss in detail about EEG recording procedure, and then some practical application of EEG signal will be discussed. Event-related potential, feature extraction techniques, and EEG signal decomposition techniques are also discussed with results in great detail.}
}
@article{ZHAO2024113339,
title = {Machine learning guided prediction of dynamic energy release in high-entropy alloys},
journal = {Materials & Design},
volume = {246},
pages = {113339},
year = {2024},
issn = {0264-1275},
doi = {https://doi.org/10.1016/j.matdes.2024.113339},
url = {https://www.sciencedirect.com/science/article/pii/S0264127524007147},
author = {Fengyuan Zhao and Zhouran Zhang and Yicong Ye and Yahao Li and Shun Li and Yu Tang and Li’an Zhu and Shuxin Bai},
keywords = {Machine Learning, Energetic Structural Materials, Feature Dimensionality Reduction, High-Entropy Alloy},
abstract = {High-entropy alloy (HEA) type energetic structural materials (ESMs) offer exceptional strength, adequate ductility and reactivity upon dynamic loading, thus demonstrating great potentials in pyrotechnic applications. However, the main factors governing their energetic performance remain elusive, primarily attributable to the intricate mechanical-thermal-chemical coupling effects and the inherent challenges of HEA design. To address this, we propose a small-data machine learning framework designed to predict the energetic performance of HEA-type ESMs, employing support vector regression, leave-one-out cross-validation, and principal component analysis (PCA) to effectively manage a small, unevenly distributed, and highly dimensional dataset. Notably, the framework achieved a coefficient of determination (R2) of 0.854 while upholding robust performance, interpretability and computational efficiency. Fracture elongation (εt) and compressive yield strength (σcys) were identified as critical features, with σcys positively influencing performance while both εt and unit theoretical heat of combustion (UTHC) demonstrated negative effect. Guided by the framework, a series of novel Ti-V-Ta-Zr alloys with the comparable UTHC, velocity (v) and weight (m) but tailored εt and σcys were designed and tested. Ti30V30Ta30Zr10 alloy exhibited a commendable balance of mechanical properties and the smallest mean particle size, aligning with the model predictions and suggesting more thorough energy release during ballistic experiments.}
}
@article{TIAN2021102727,
title = {Using data monitoring algorithms to physiological indicators in motion based on Internet of Things in smart city},
journal = {Sustainable Cities and Society},
volume = {67},
pages = {102727},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.102727},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721000226},
author = {Jian Tian and Lulu Gao},
keywords = {Internet of Things, Data fusion algorithm, Physiological indicators, Monitoring, Smart city},
abstract = {This article discusses the monitoring of physiological indicators during exercise, combined with the data fusion algorithm of the smart city Internet of Things health. We use the hash value of the tuple key to the corresponding data block of the node, use the data block record to obtain the response of the target node, and output the data tuple. It is used as a measure of the load balance of health data streams to determine whether load migration is needed and to determine the way and amount of migration tasks to make migration decisions. The simulation experiments show that the method has good computational performance and dynamic load balancing. A series of mean arterial pressure and heart rate of patients and non-stationary health data, and a series of blood pressure and heart rate of health individuals in different postures are selected to perform experiments to analyze the transfer function and power spectra in the model, validating that the model can be used to reveal the changes associated with severe systemic response syndrome (SIRS), providing a hypothesis for the decomposition of autoregulation of physiological control under health and disease conditions.}
}
@article{LANZ201979,
title = {Learning environment for robotics education and industry-academia collaboration},
journal = {Procedia Manufacturing},
volume = {31},
pages = {79-84},
year = {2019},
note = {Research. Experience. Education. 9th Conference on Learning Factories 2019 (CLF 2019), Braunschweig, Germany},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2019.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S2351978919303762},
author = {Minna Lanz and Roel Pieters and Reza Ghabcheloo},
keywords = {Robotics, learning environment, education, industry-academia collaboration, problem-solving, active learning},
abstract = {It is expected that by utilizing digital technologies, advanced robotics and artificial intelligence, the manufacturing base of Europe will become stronger and allow production re-shoring from other trade areas to take place. The European competitiveness is tied to better competences of the workforce and fast implementation of new technologies. This requires new approaches for formal and non-formal education. For this, we propose a new robotics learning concept and collaboration scheme to support both MSc level education, but also non-formal education with industry. The non-formal education example could be a combination of an education package followed by rapid experimenting with a robot system. In order to facilitate the learning process, we have established the Tampere RoboLab and joint academia-industry education modules for both formal and non-formal education. The Tampere RoboLab operates with similar principles as e.g. Fab Labs (fabrication laboratories), but the focus is on indoor stationary and mobile robotics. Aside from education, the concept allows system interoperability testing and pre-competitive research to be done in the same premises as well as field robotics by providing the state of art localisation and perception sensors, and computation and communication devices. This paper will introduce the concept, used hardware and software configurations, education modules and the forms of industry-academia collaboration.}
}
@incollection{GABBIANI20171,
title = {Chapter 1 - Introduction},
editor = {Fabrizio Gabbiani and Steven James Cox},
booktitle = {Mathematics for Neuroscientists (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {1-8},
year = {2017},
isbn = {978-0-12-801895-8},
doi = {https://doi.org/10.1016/B978-0-12-801895-8.00001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128018958000014},
author = {Fabrizio Gabbiani and Steven James Cox},
keywords = {Book structure, Chapter dependencies, Purkinje cell, Brain Facts, Mathematical preliminaries, Units, Neuron, Neuroethology},
abstract = {Faced with the seemingly limitless qualities of the brain, neuroscience has eschewed provincialism and instead pursued a broad tack that openly draws on insights from biology, physics, chemistry, engineering, psychology and mathematics in its construction of technologies and theories with which to probe and understand the brain. These technologies and theories, in turn, continue to attract scientists and mathematicians to questions of neuroscience. As a result, we may trace over one hundred years of fruitful interplay between neuroscience and mathematics. This text aims to prepare the advanced undergraduate or beginning graduate student to take an active part in this dialogue via the application of existing, or the creation of new, mathematics in the interest of a deeper understanding of the brain. This text aims to prepare the advanced undergraduate or beginning graduate student to take an active part in this dialogue via the application of existing, or the creation of new, mathematics in the interest of a deeper understanding of the brain. Requiring no more than one year of Calculus, and no prior exposure to Neuroscience, we prepare the student by (I) introducing mathematical and computational tools in precisely the contexts that first established their importance for neuroscience, and, (II) developing these tools in concrete incremental steps within a common computational environment. As such, the text may also serve to introduce Neuroscience to readers with a mathematical and/or computational background.}
}
@incollection{BALLARD200931,
title = {Active Perception},
editor = {Larry R. Squire},
booktitle = {Encyclopedia of Neuroscience},
publisher = {Academic Press},
address = {Oxford},
pages = {31-37},
year = {2009},
isbn = {978-0-08-045046-9},
doi = {https://doi.org/10.1016/B978-008045046-9.01436-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780080450469014364},
author = {D.H. Ballard},
keywords = {Active vision, Bayesian statistics, Binocular robot heads, Graphical models, Virtual reality},
abstract = {Almost all perception is active in the sense that we are aware of the percept and can use it to direct behaviors. Neuroscience research has demonstrated precise neural correlates of such percepts in the brain’s neural firing patterns. Computational models programmed on binocular camera robot ‘heads’ have shown that the embodiment of active perception produces great economies in cost. The spatiotemporal coding of an active percept tends to be punctate, as can be demonstrated in virtual environments. These coding mechanisms can be succinctly described with Bayesian statistics, which can form the basic currency in graphical models that provide concise descriptions of extended tasks.}
}
@article{BERGHOUT2022108680,
title = {EL-NAHL: Exploring labels autoencoding in augmented hidden layers of feedforward neural networks for cybersecurity in smart grids},
journal = {Reliability Engineering & System Safety},
volume = {226},
pages = {108680},
year = {2022},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2022.108680},
url = {https://www.sciencedirect.com/science/article/pii/S0951832022003131},
author = {Tarek Berghout and Mohamed Benbouzid},
keywords = {Cybersecurity, Label propagation, Machine learning, Neural networks, Smart grids},
abstract = {Reliability and security of power distribution and data traffic in smart grid (SG) are very important for industrial control systems (ICS). Indeed, SG cyber-physical connectivity is subject to several vulnerabilities that can damage or disrupt its process immunity via cyberthreats. Today's ICSs are experiencing highly complex data change and dynamism, increasing the complexity of detecting and mitigating cyberattacks. Subsequently, and since Machine Learning (ML) is widely studied in cybersecurity, the objectives of this paper are twofold. First, for algorithmic simplicity, a small-scale ML algorithm that attempts to reduce computational costs is proposed. The algorithm adopts a neural network with an augmented hidden layer (NAHL) to easily and efficiently accomplish the learning procedures. Second, to solve the data complexity problem regarding rapid change and dynamism, a label autoencoding approach is introduced for Embedding Labels in the NAHL (EL-NAHL) architecture to take advantage of labels propagation when separating data scatters. Furthermore, to provide a more realistic analysis by addressing real-world threat scenarios, a dataset of an electric traction substation used in the high-speed rail industry is adopted in this work. Compared to some existing algorithms and other previous works, the achieved results show that the proposed EL-NAHL architecture is effective even under massive dynamically changed and imbalanced data.}
}
@incollection{JOHNSON2023143,
title = {Socioscientific issues and STEM learning},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {143-152},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.13051-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305130519},
author = {Joseph A. Johnson and Ryan Batkie and Augusto Macalalag and Julie Dunphy and Shawn Titus},
keywords = {Educational reform, Pedagogical content knowledge, Socioscientific issues, STEM education, Teacher education},
abstract = {Socioscientific issues (SSI) are debatable and ill-defined problems that have a basis in science but necessarily include moral and ethical choices. SSI can provide meaningful contexts for students to learn concepts and practices in science, technology, engineering, and mathematics (STEM) disciplines. This article explores SSI in the landscape of parallel educational reform movements, the goals of STEM education, and how SSI can provide an avenue for attaining what are often viewed as disparate goals. The article concludes by highlighting challenges to SSI implementation found in the literature and outlining potential avenues to foster and support SSI implementation in STEM through teacher education and professional development.}
}
@article{GUHANSESHADRI2023104553,
title = {EEG based classification of children with learning disabilities using shallow and deep neural network},
journal = {Biomedical Signal Processing and Control},
volume = {82},
pages = {104553},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.104553},
url = {https://www.sciencedirect.com/science/article/pii/S1746809422010072},
author = {N.P. {Guhan Seshadri} and Sneha Agrawal and Bikesh {Kumar Singh} and B. Geethanjali and V. Mahesh and Ram Bilas Pachori},
keywords = {Learning disability, Digital wavelet transform, EEG, Shallow network, Deep neural network},
abstract = {Learning disability (LD), a neurodevelopmental disorder that has severely impacted the lives of many children all over the world. LD refers to significant deficiency in children’s reading, writing, spelling, and ability to solve mathematical task despite having normal intelligence. This paper proposes a framework for early detection and classification of LD with non-LD children from rest electroencephalogram (EEG) signals using shallow and deep neural network. Twenty children with LD and twenty non-LD children (aged 8–16 years) participated in this study. Preprocessing the raw EEG signal, segmentation and extraction of various features from the alpha, beta, delta, and theta bands obtained using digital wavelet transform (DWT). Filter based feature selection method were employed for the selection of most relevant features that reduces the computation burden on models. Afterwards, these ranked accumulated features were evaluated separately by machine learning (ML) classifiers and neural network (shallow and deep) models to investigate the performance. The performance of the ML classifiers and one-hidden layer shallow neural network and 3-hidden layer deep neural network were compared. Experimental results showed that the most relevant features computed by ReliefF algorithm along with the shallow neural network based classifier attained the highest average and maximum classification accuracy of 95.8 % and 97.5 % respectively, which is greatest among the existing literatures. The efficient and automatic LD classification from EEG signal could aid in the development of computer-aided diagnosis systems for early detection.}
}
@article{AFIQAHZAMAIN2024849,
title = {The Impact of Artificial Intelligence in the Accounting Profession},
journal = {Procedia Computer Science},
volume = {238},
pages = {849-856},
year = {2024},
note = {The 15th International Conference on Ambient Systems, Networks and Technologies Networks (ANT) / The 7th International Conference on Emerging Data and Industry 4.0 (EDI40), April 23-25, 2024, Hasselt University, Belgium},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.06.102},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924013383},
author = {Nur Syahmina {Afiqah Zamain} and Ulaganathan Subramanian},
keywords = {Artificial Intelligence, Accounting},
abstract = {The evolution of advanced technology across various industries has provided opportunities for improvement in how professionals carry out their jobs. Artificial Intelligence (AI) is a rapidly evolving technology that brings great convenience to life, but at the same time, some risks need to be assessed. This technical paper will discuss AI processes that are taking place in the current scenario at accounting firms and their impact on them. We discussed AI’s benefits to the Accounting and Auditing profession and its employees. This paper discussed AI’s risks and challenges to Accounting firms and their employees.}
}
@article{LOMI1999151,
title = {Learning without experience: strategic implications of deregulation and competition in the electricity industry},
journal = {European Management Journal},
volume = {17},
number = {2},
pages = {151-163},
year = {1999},
issn = {0263-2373},
doi = {https://doi.org/10.1016/S0263-2373(98)00074-7},
url = {https://www.sciencedirect.com/science/article/pii/S0263237398000747},
author = {Alessandro Lomi and Erik Larsen},
abstract = {As deregulation of the electricity industry continues to gain momentum around the world, electricity companies face unprecedented challenges. Competitive complexity and intensity will increase substantially as deregulated companies find themselves competing in new industries, with new rules, against unfamiliar competitors — and without any history to learn from. We describe the different kinds of strategic issues that newly deregulated utility companies are facing, and the risks that these strategic issues implicate. We identify a number of problems induced by experiential learning under conditions of competence-destroying changes, and we illustrate ways in which companies can activate history-independent learning processes. We suggest that Microworlds — a new generation of computer-based learning environments made possible by conceptual and technological progress in the fields of system dynamics and systems thinking — are particularly appropriate tools to accelerate and enhance organizational and managerial learning under conditions of increased competitive complexity.}
}
@article{BELL2011659,
title = {A cybernetic perspective on food protection in rats: simple rules can generate complex and adaptable behaviour},
journal = {Animal Behaviour},
volume = {82},
number = {4},
pages = {659-666},
year = {2011},
issn = {0003-3472},
doi = {https://doi.org/10.1016/j.anbehav.2011.06.016},
url = {https://www.sciencedirect.com/science/article/pii/S0003347211002582},
author = {Heather C. Bell and Sergio M. Pellis},
keywords = {cybernetics, dodging, Perceptual Control Theory, rat, robbing},
abstract = {Many types of animal behaviour, especially seemingly complex social interactions, have been attributed to the existence of complex cognitive mechanisms. Indeed, as specific behaviours are analysed in greater and greater detail, the increasing number of minor variations observed seem to necessitate the operation of increasingly powerful computational devices. An alternate view, inspired by cybernetic theory, is that what is important is not the specific behaviours used by animals, but the goal of the organism in a particular context. When approached in this way, it is possible to deduce simple rules being used by organisms to attain goal states that account for behavioural variation, and importantly, that do not require impossible levels of cognitive power. In this paper, we apply a cybernetic approach to analysing food protective behaviour in rats. We demonstrate that a simple cybernetic rule, rather than complex computation, produces efficient and effective food protective behaviour in rats.}
}
@article{GUO2015252,
title = {The mediating role of LPFC–vmPFC functional connectivity in the relation between regulatory mode and delay discounting},
journal = {Behavioural Brain Research},
volume = {292},
pages = {252-258},
year = {2015},
issn = {0166-4328},
doi = {https://doi.org/10.1016/j.bbr.2015.06.035},
url = {https://www.sciencedirect.com/science/article/pii/S0166432815300590},
author = {Yiqun Guo and Tingyong Feng},
keywords = {Delay discounting, Regulatory mode, Functional connectivity, Resting state fMRI},
abstract = {Previous studies have shown that regulatory mode orientation can affect many human behaviors, such as risk-taking, counterfactual thinking and economic decision making. However, little is known about how regulatory mode affects delay discounting. To address this question, we used resting-state functional magnetic resonance imaging (rs-fMRI) to investigate whether regulatory mode orientations can be represented by functional connectivity and the influence of two regulatory modes (assessment and locomotion) on delay discounting. The behavioral results showed that delay discounting was negatively correlated with assessment scores but positively correlated with locomotion scores. Neuroimaging results indicated that the functional connectivity between lateral prefrontal cortex (LPFC) and ventromedial prefrontal cortex (vmPFC) was negatively correlated with assessment scores but positively correlated with locomotion scores. Furthermore, mediation analysis showed that the effect of regulatory mode on delay discounting is mediated by LPFC–vmPFC functional connectivity. These results suggested that people’s regulatory mode orientation could predict delay discounting, which is mediated by LPFC–vmPFC functional connectivity. Therefore, the present study extends our perspective on regulatory mode and provides neural mechanism for understanding the link between regulatory mode and delay discounting.}
}
@article{CHATURVEDI2015154,
title = {Adaptive Polar Fuzzy logic based Load Frequency Controller},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {66},
pages = {154-159},
year = {2015},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2014.10.024},
url = {https://www.sciencedirect.com/science/article/pii/S0142061514006279},
author = {D.K. Chaturvedi and Rahul Umrao and O.P. Malik},
keywords = {Load Frequency Control, Adaptive Polar Fuzzy Controller, Power systems, Fuzzy system, Genetic algorithms},
abstract = {Performance of a Fuzzy logic controller is dependent on sufficient and accurate knowledge base. As the number of rules in a knowledge base increases, its complexity increases which in turn affects the computation time and memory requirements. To overcome these problems, a Polar Fuzzy logic controller is proposed. The aim of the Polar Fuzzy Controller (PFC) is to restore the frequency and tie-line power in a smooth way to its nominal value in the shortest possible time if any load disturbance is applied to any area of power system. In this paper, the PFC is made adaptive using a Genetic algorithm-fuzzy system (GAF) approach. Performance of the simple PFC and adaptive PFC using GAF is compared with fuzzy and conventional PI controllers on a three area system.}
}
@article{MOHAMMADI2024143410,
title = {Assessing residential sustainable energy autonomous buildings for hot climate applications},
journal = {Journal of Cleaner Production},
volume = {471},
pages = {143410},
year = {2024},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2024.143410},
url = {https://www.sciencedirect.com/science/article/pii/S0959652624028592},
author = {Saeed Mohammadi and Ammar M. Bahman},
keywords = {Autonomous building, 5Z concept, Hot climate, ESP-r model, Sustainable cities},
abstract = {This paper introduces an innovative solution for clean energy autonomous building (AB) sustainability, offering a 5Z concept—zero-carbon, zero-energy, zero grid connections, zero energy bills, and zero emission mobility. This paper focuses on fundamental research to design sustainable, energy-ABs striving for self-sufficiency in arid climates, using Kuwait as a case study. The study highlights the importance of stringent engineering AB modeling, renewable technology integration, and clean energy storage. The technical approaches include characterizing non-thermal and thermal demands, achieving net-zero energy generation, and custom sizing renewable energy and energy storage systems (ESS) for electric vehicle (EV) charging points. The research methodology involves local construction with yearly weather and energy profile data to simulate the actual system using ESP-r building model. The study's findings reveal the need for a large-scale energy system, energy demand reduction (EDR) inclusive of heating, cooling, appliances, and EV, with the corresponding changes in local energy production system size, battery capacity, and the number of photovoltaics (PVs). The results show varying EDR levels ranging from base case to 10% and to 50% leading to proportional changes in energy system size, size of battery from 3415 kWh to 1710 kWh and the number of PVs from 362 to 181, which means EDR not only optimizes space but also proves cost-effective. The significant implication of this study, not only bridging the knowledge gap and the lack of how-know, but also making a significant advancement and forward thinking in sustainable green electric home modernization and environmentally friendly transportation. This approach transforms energy management practices for more sustainable cities and societies, reducing emissions in both urban infrastructure and transportation.}
}
@article{GRATCH2004269,
title = {A domain-independent framework for modeling emotion},
journal = {Cognitive Systems Research},
volume = {5},
number = {4},
pages = {269-306},
year = {2004},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2004.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S1389041704000142},
author = {Jonathan Gratch and Stacy Marsella},
abstract = {In this article, we show how psychological theories of emotion shed light on the interaction between emotion and cognition, and thus can inform the design of human-like autonomous agents that must convey these core aspects of human behavior. We lay out a general computational framework of appraisal and coping as a central organizing principle for such systems. We then discuss a detailed domain-independent model based on this framework, illustrating how it has been applied to the problem of generating behavior for a significant social training application. The model is useful not only for deriving emotional state, but also for informing a number of the behaviors that must be modeled by virtual humans such as facial expressions, dialogue management, planning, reacting, and social understanding. Thus, the work is of potential interest to models of strategic decision-making, action selection, facial animation, and social intelligence.}
}
@article{LI2024102666,
title = {Virtual human on social media: Text mining and sentiment analysis},
journal = {Technology in Society},
volume = {78},
pages = {102666},
year = {2024},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2024.102666},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X24002148},
author = {Sihong Li and Jinglong Chen},
keywords = {Virtual human, Social media, Text mining, Public opinion, Sentiment analysis},
abstract = {Virtual humans are embodied agents with a human-like appearance. Despite the recent booming development that has sparked widespread academic interest, how people perceive these seemingly human but entirely fictional creations remains unclear. To explore the status, trends, emotional tendencies, and focus of attention of the Chinese public towards virtual humans, this paper utilizes text mining techniques to collect and analyze popular posts related to virtual humans on Chinese social media. The results indicate that public discussions primarily focus on the technological and industrial development of virtual humans, applications in the fields of virtual idols and virtual streamers, and the corporate investment and policy development of virtual humans. Despite positive emotions dominating, there is an increasing trend in negative emotions. Concerns are related to service failures, the uncanny valley effect, ethical crises, and technological unemployment. The research findings contribute to policymakers, industry stakeholders, and the public in understanding the general attitudes toward virtual human technology, enabling informed decision-making.}
}
@article{M2021107664,
title = {Sequential Convolutional Neural Networks for classification of cognitive tasks from EEG signals},
journal = {Applied Soft Computing},
volume = {111},
pages = {107664},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107664},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621005858},
author = {Suchetha M. and Madhumitha R. and Sorna Meena M. and Sruthi R.},
keywords = {Electroencephalography, Cognition, Phase–amplitude coupling, Deep learning, Inception module},
abstract = {Cognitive abilities encompass all aspects of mental functioning ranging from simple to complex tasks. These skills have a tremendous effect on our day-to-day routine. The electroencephalogram (EEG) is a powerful tool to analyze brain activities while performing different cognitive tasks. In this paper, we consider four cognitive tasks (Symbol digit modality test, Stroop test, Benton’s visual retention test, and Hopkins verbal learning test) along with a baseline task, carried out by healthy subjects and record their EEG. We perform phase–amplitude coupling to extract the features of classification, and segregate them into the tasks, through deep learning algorithms. The Sequential Convolutional Network (SCN) is designed to classify these features. Multi-branch Convolutional Network (MBCN) is also proposed, which is inspired by the ResNeXt architecture and the inception module. The performance of the proposed model is evaluated using the metrics such as accuracy, F1-score, precision, and specificity using the EEG signals collected from the PAC dataset and real-time recording. The performance evaluation reveals that MBCN outperforms SCN by achieving higher accuracy, F1-score, precision, and sensitivity of 88.33%, 87.9%, 89.18%, and 88.23% respectively. Also, the computational complexity of the MBCN architecture is found to be less than the SCN model. Evaluation results show that the proposed MBCN model outperforms the traditional methods.}
}
@article{SATHIYA2023101510,
title = {Reshaping healthcare supply chain using chain-of-things technology and key lessons experienced from COVID-19 pandemic},
journal = {Socio-Economic Planning Sciences},
volume = {85},
pages = {101510},
year = {2023},
issn = {0038-0121},
doi = {https://doi.org/10.1016/j.seps.2023.101510},
url = {https://www.sciencedirect.com/science/article/pii/S0038012123000034},
author = {V. Sathiya and K. Nagalakshmi and J. Jeevamalar and R. {Anand Babu} and R. Karthi and Ángel Acevedo-Duque and R. Lavanya and S. Ramabalan},
keywords = {Blockchain technology, Chain of things, Healthcare supply chain, Internet of things, Localization, Resilient supply chain, Reverse logistics},
abstract = {The COVID-19 (Corona virus disease 2019) pandemic continues to slash through the entire humanity on the earth causing an international health crisis and financial uncertainty. The pandemic has formed a colossal disruption in supply chain networks. It has caused piling higher mortality in patients with comorbidities and generated a surging demand for critical care equipment, vaccines, pharmaceuticals, and cutting-edge technologies. Personal protective equipment, masks, ventilators, testing kits, and even commodities required for daily care have been scarce as lockdown and social distancing guidelines have kicked in. Amidst COVID-19, implementing and executing key processes of the healthcare supply chain (HSC) in a secured, trusted, effective, universally manageable, and the traceable way is perplexing owing to the fragile nature of the HSC, which is susceptible to redundant efforts and systemic risks that can lead to adverse impacts on consumer health and safety. Though the crisis shone a harsh light on the cracks and weaknesses of the HSC, it brings some significant insights into how HSC can be made more resilient and how healthcare industries figure out solutions to mitigate disruptions. While there are innumerable experiences learned from the disruption of this crisis, in this paper, five important areas to analyze the most vital and immediate HSC enhancements including building a resilient supply chain, thinking localization, implementing reliable reverse logistics, breaking down extant silos to achieve end-to-end visibility, and redesigning HSC using digitalization are emphasized. This work identifies important features related to CoT and HSC. Also, this study links these lessons to a potential solution through Chain of Things (CoT) technology. CoT technology provides a better way to monitor HSC products by integrating the Internet of Things (IoT) with blockchain networks. However, such an integrated solution should not only focus on the required features and aspects but also on the correlation among different features. The major objective of this study is to reveal the influence path of CoT on smart HSC development. Hence, this study exploits (i) fuzzy set theory to eliminate redundant and unrelated features; (ii) the Decision-Making and Experimental Evaluation Laboratory (DEMATEL) method to handle the intricate correlation among different features. This fuzzy-DEMATEL (F-DEMATEL) model attempts to direct CoT technology towards smart HSC by identifying the most influencing factors and investors are recommended to contribute to the development of application systems. This work also demonstrates how CoT can act a vital role in handling the HSC issues triggered by the pandemic now and in the post-COVID-19 world. Also, this work proposes different CoT design patterns for increasing opportunities in the HSC network and applied them as imperative solutions for major challenges related to traditional HSC networks.}
}
@article{CARO201482,
title = {Design and validation of a metamodel for metacognition support in artificial intelligent systems},
journal = {Biologically Inspired Cognitive Architectures},
volume = {9},
pages = {82-104},
year = {2014},
note = {Neural-Symbolic Networks for Cognitive Capacities},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2014.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X14000528},
author = {Manuel F. Caro and Darsana P. Josyula and Michael T. Cox and Jovani A. Jiménez},
keywords = {Metacognition, Metamodeling, Intelligent systems, Metacognitive metamodel},
abstract = {Computational metacognition is a technical area of artificial intelligence whose aim is to increase the degree of autonomy and awareness an intelligent system has about its own reasoning and learning. In the literature, different models of metacognition are applied to artificial intelligent systems. However many of these models have a narrow focus, because they do not address comprehensively the elements of metacognition. This paper presents an analysis of metacognitive models discussed in the literature in order to discover the common (invariants) and varying (variants) elements. The main contribution of this work is the development of a comprehensive and general purpose metamodel named MISM that covers and describes a broad range of commonly referenced concepts in metacognitive models in the area of artificial intelligence. A validation process was conducted to ensure the reliability of MISM in terms of generality, expressiveness and completeness. The validation was performed using three techniques for improvements and adjustments to the metamodel: (i) comparison with other models; (ii) frequency-based selection; and (iii) model tracing. The adjusted and improved version of the metamodel was named MISM 1.1.}
}
@article{DAROLD20189,
title = {Defining embodied cognition: The problem of situatedness},
journal = {New Ideas in Psychology},
volume = {51},
pages = {9-14},
year = {2018},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2018.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X17301794},
author = {Federico {Da Rold}},
keywords = {Situated cognition, Embodied and grounded cognition, Neuro-robotics, Connectionism, Dynamical systems},
abstract = {The embodied view of cognition rejects the substantial dualism between brain and body, claiming the primary role of sensorimotor experience on the development of conceptual knowledge. From this perspective, knowledge is grounded on physical properties of the body and the surrounding world. Furthermore, cognition is situated in a social and environmental context. However, the terms embodied, grounded, and situated are not univocally defined. This article focuses on the notion of situatedness, developing the discussion from the point of view of a computational modeler and roboticist, showing that minor and negligible differences on the definition of the field causes major operational divergences in synthetic models of cognition. A definition of two notions of situatedness are developed a posteriori, that is, by considering epistemological and ontological differences on artificial models. Finally, strengths and weakness of the two approaches are discussed.}
}
@article{THRALL202467,
title = {Speculative frictions: writing civic futures after AI},
journal = {English Teaching: Practice & Critique},
volume = {23},
number = {1},
pages = {67-82},
year = {2024},
issn = {1175-8708},
doi = {https://doi.org/10.1108/ETPC-08-2023-0095},
url = {https://www.sciencedirect.com/science/article/pii/S1175870824000141},
author = {Alexandra Thrall and T. Philip Nichols and Kevin R. Magill},
keywords = {Digital literacies, Digital citizenship, Writing, Artificial intelligence, Civics, Speculative fiction},
abstract = {Purpose
The purpose of this study is to examine how young people imagine civic futures through speculative fiction writing about artificial intelligence (AI) technologies. The authors argue that young people’s speculative fiction writing about AI not only helps make visible the ways they imagine the impacts of emerging technologies and the modes of collective action available for leveraging, resisting or countering them but also the frictions and fissures between the two.
Design/methodology/approach
This practitioner research study used data from student artifacts (speculative fiction stories, prewriting and relevant unit work) as well as classroom fieldnotes. The authors used inductive coding to identify emergent patterns in the ways young people wrote about AI and civics, as well as deductive coding using digital civic ecologies framework.
Findings
The findings of this study spotlight both the breadth of intractable civic concerns that young people associate with AI, as well as the limitations of the civic frameworks for imagining political interventions to these challenges. Importantly, they also indicate that the process of speculative writing itself can help reconcile this disjuncture by opening space to dwell in, rather than resolve, the tensions between “the speculative” and the “civic.”
Practical implications
Teachers might use speculative fiction writing and the digital civic ecologies framework to support students in critically examining possible AI futures and effective civic actions within them.
Originality/value
Speculative fiction writing offers an avenue for students to analyze the growing civic concerns posed by emerging platform technologies like AI.}
}
@article{2024i,
title = {Advisory Board and Contents},
journal = {Trends in Cognitive Sciences},
volume = {28},
number = {10},
pages = {i-ii},
year = {2024},
issn = {1364-6613},
doi = {https://doi.org/10.1016/S1364-6613(24)00232-8},
url = {https://www.sciencedirect.com/science/article/pii/S1364661324002328}
}
@article{MARKOWITZ2023107684,
title = {Self-presentation in medicine: How language patterns reflect physician impression management goals and affect perceptions},
journal = {Computers in Human Behavior},
volume = {143},
pages = {107684},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107684},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223000353},
author = {David M. Markowitz},
keywords = {Self-presentation, Impression management, Psychology of language, Medicine, Automated text analysis},
abstract = {This paper evaluated how physicians' communication patterns reflect their self-presentation goals and link to patient perceptions. Specifically, in a large field study (N = 54,420 profiles from HealthGrades.com), physician self-descriptions were analyzed linguistically through automated means, with evidence suggesting those who were more self-focused and confident tended to have higher patient ratings online than those who were less self-focused and confident. Physicians who discussed their expertise and compassion for patients were also rated more favorably. A within-subjects experiment in Study 2 (N = 500) also demonstrated that linguistic self-presentation patterns can affect patient perceptions. Participants who read physician profiles with high rates of self-references and verbal confidence rated doctors as warmer and more competent than those who read physician profiles with low rates of self-references and certainty. Together, words are indicators of physicians’ self-presentation strategies and can change patient evaluations. Theoretical contributions for self-presentation and psychology of language research are discussed.}
}
@article{LOH1993108,
title = {Massively parallel computing in materials modeling},
journal = {Physica D: Nonlinear Phenomena},
volume = {66},
number = {1},
pages = {108-118},
year = {1993},
issn = {0167-2789},
doi = {https://doi.org/10.1016/0167-2789(93)90228-S},
url = {https://www.sciencedirect.com/science/article/pii/016727899390228S},
author = {Eugene Loh},
abstract = {Realistic materials modeling requires considerable computational resources — both computer speed and memory. In this paper, we review the current trend to massively parallel computers in high-performance computer architectures, algorithmic strategies for using such systems, and some materials modeling successes on parallel computers to date.}
}
@incollection{EDELMAN20019256,
title = {Marr, David (1945–80)},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {9256-9258},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/00297-7},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767002977},
author = {S. Edelman and L.M. Vaina},
abstract = {David Courtnay Marr was born in 1945 in Essex, England. Marr's dissertation, written at Trinity College, Cambridge and published between 1969 and 1971, presented a theory of mammalian brain function, parts of which remain relevant to the present day, despite vast advances in neurobiology in the past three decades. In 1973, Marr joined the Artificial Intelligence Laboratory at the Massachusetts Institute of Technology, where he was made a tenured full professor in 1980. Marr died in November 1980, of leukemia. His highly influential book, Vision: A Computational Investigation into the Human Representation and Processing of Visual Information, which has redefined and revitalized the study of human and machine vision, was published posthumously, in 1982.}
}
@article{FREEMAN2015156,
title = {Open source tools for large-scale neuroscience},
journal = {Current Opinion in Neurobiology},
volume = {32},
pages = {156-163},
year = {2015},
note = {Large-Scale Recording Technology (32)},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2015.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0959438815000756},
author = {Jeremy Freeman},
abstract = {New technologies for monitoring and manipulating the nervous system promise exciting biology but pose challenges for analysis and computation. Solutions can be found in the form of modern approaches to distributed computing, machine learning, and interactive visualization. But embracing these new technologies will require a cultural shift: away from independent efforts and proprietary methods and toward an open source and collaborative neuroscience.}
}
@article{MISHRA2005446,
title = {Hybrid tabu-simulated annealing based approach to solve multi-constraint product mix decision problem},
journal = {Expert Systems with Applications},
volume = {29},
number = {2},
pages = {446-454},
year = {2005},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2005.04.044},
url = {https://www.sciencedirect.com/science/article/pii/S095741740500093X},
author = {Nishikant Mishra and  Prakash and M.K. Tiwari and R. Shankar and Felix T.S. Chan},
keywords = {Multi-constraint product mix decision, Manufacturing systems, Theory of constraints, Simulated annealing, Tabu search},
abstract = {In the recent years, theory of constraints (TOC) has emerged as an effective management philosophy for solving decision making problems with the aim of profit maximization by considering the bottleneck in traditional as well as modern manufacturing plants. One of the key components of TOC application is to enumerate quantity of the various products to be manufactured keeping in view the system constraints. Problem of this kind is termed as TOC product mix decision problem. It is a well-known computationally complex problem and thus warrants the application of heuristics techniques or AI based optimization tools to achieve optimal or near optimal solution in real time. In this research, a hybrid algorithm named tabu-simulated annealing is proposed. It exploits the beauty of tabu search and simulated annealing (SA) to ensure the convergence at faster rate. It is found that the performance of hybrid tabu-SA algorithm on a well known data set of product mix optimization problem is superior as compared to tabu search, SA, TOC heuristic, Revised-TOC (R-TOC) heuristic, and Integer Linear Programming (ILP) based approaches.}
}
@article{MOODLEY2022e1855,
title = {Ethics and governance challenges related to genomic data sharing in southern Africa: the case of SARS-CoV-2},
journal = {The Lancet Global Health},
volume = {10},
number = {12},
pages = {e1855-e1859},
year = {2022},
issn = {2214-109X},
doi = {https://doi.org/10.1016/S2214-109X(22)00417-X},
url = {https://www.sciencedirect.com/science/article/pii/S2214109X2200417X},
author = {Keymanthri Moodley and Nezerith Cengiz and Aneeka Domingo and Gonasagrie Nair and Adetayo Emmanuel Obasa and Richard John Lessells and Tulio {de Oliveira}},
abstract = {Summary
Data sharing in research is fraught with controversy. Academic success is premised on competitive advantage, with research teams protecting their research findings until publication. Research funders, by contrast, often require data sharing. Beyond traditional research and funding requirements, surveillance data have become contentious. Public health emergencies involving pathogens require intense genomic surveillance efforts and call for the rapid sharing of data on the basis of public interest. Under these circumstances, timely sharing of data becomes a matter of scientific integrity. During the COVID-19 pandemic, the transformative potential of genomic pathogen data sharing became obvious and advanced the debate on data sharing. However, when the genomic sequencing data of the omicron (B.1.1.529) variant was shared and announced by scientists in southern Africa, various challenges arose, including travel bans. The scientific, economic, and moral impact was catastrophic. Yet, travel restrictions failed to mitigate the spread of the variant already present in countries outside Africa. Public perceptions of the negative effect of data sharing are detrimental to the willingness of research participants to consent to sharing data in postpandemic research and future pandemics. Global health governance organisations have an important role in developing guidance on responsible sharing of genomic pathogen data in public health emergencies.}
}
@article{MARTINEZ2022100037,
title = {¿Human-like Computers? Velden, Manfred (2022). Human-like Computers: A Lesson in Absurdity. Berlin: Schwabe Verlag.},
journal = {Journal of Responsible Technology},
volume = {11},
pages = {100037},
year = {2022},
issn = {2666-6596},
doi = {https://doi.org/10.1016/j.jrt.2022.100037},
url = {https://www.sciencedirect.com/science/article/pii/S2666659622000142},
author = {Carlos Andrés Salazar Martínez}
}
@article{DAVIS1993269,
title = {Interpreting language: Introduction},
journal = {Language Sciences},
volume = {15},
number = {4},
pages = {269-291},
year = {1993},
issn = {0388-0001},
doi = {https://doi.org/10.1016/0388-0001(93)90006-E},
url = {https://www.sciencedirect.com/science/article/pii/038800019390006E},
author = {Philip W. Davis},
abstract = {The papers in this collection attempt to identify alternative modes in thinking about language and to develop selected aspects of the proposed alternatives. The suggestions contained in these contributions are ‘alternatives’ to those conceptions of language which take it to be a discrete object and one that can be isolated from other aspects of humanity. Although the contributors are expressing a shared concern, this does not imply agreement among them as to appearance of the alternatives. The introduction considers some problems in Ilokano and Taiwanese in order to illustrate more concretely the nature of the issues which are addressed by the remainder of the papers. ‘strange, but not a stranger’}
}
@article{CHELLAPPA2024100281,
title = {Understanding the perception of design students towards ChatGPT},
journal = {Computers and Education: Artificial Intelligence},
volume = {7},
pages = {100281},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100281},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000845},
author = {Vigneshkumar Chellappa and Yan Luximon},
keywords = {ChatGPT, Design education, Students' perceptions, Artificial intelligence},
abstract = {The benefits of artificial intelligence (AI)-enabled language models, such as ChatGPT, have contributed to their growing popularity in education. However, there is currently a lack of evidence regarding the perception of ChatGPT, specifically among design students. This study aimed to understand the product design (PD) and user experience design (UXD) students' views on ChatGPT and focused on an Indian university. The study employed a survey research design, utilizing questionnaires as the primary data collection method. The collected data (n = 149) was analyzed using descriptive statistics (i.e., frequency, percentage, average, and standard deviation (SD). Inferential statistics (i.e., one-way ANOVA) was used to understand the significant differences between the programs of study, gender, and academic level. The findings indicate that the students expressed admiration for the capabilities of ChatGPT and found it to be an interesting and helpful tool for their studies. In addition, the students' motivation towards using ChatGPT was moderate. Furthermore, the study observed significant differences between PD and UXD students and differences based on gender and academic level on certain variables. Notably, UXD students reported that ChatGPT does not understand their questions well, and formulating effective prompts for the tool was more challenging than for PD students. Based on the findings, the study recommends how educators should consider integrating ChatGPT into design education curricula and pedagogical practices. The insights aim to contribute to refining the use of ChatGPT in educational settings and exploring avenues for improving its effectiveness, ultimately advancing the field of AI in design education.}
}
@article{ULYANOV201753,
title = {Fuzzy Cognitive Control System of Autonomous Vehicle: Brain Neurointerface and Soft Computing Modes},
journal = {Procedia Computer Science},
volume = {120},
pages = {53-66},
year = {2017},
note = {9th International Conference on Theory and Application of Soft Computing, Computing with Words and Perception, ICSCCW 2017, 22-23 August 2017, Budapest, Hungary},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.11.210},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917324201},
author = {Sergey V. Ulyanov and Andrey G. Reshetnikov and Alla A. Mamaeva},
keywords = {Type your keywords here, separated by semicolons},
abstract = {The article show the possibility of neurointerface application based on cognitive helmet with different traditional types of controllers for the vehicle driving. Extraction of knowledge from electroencephalogram based on knowledge base soft computing optimizer demonstrated. The commonly application of computational intelligence and cognitive toolkits improve the reliability of fuzzy control system operations.}
}
@article{KRICHMAR201273,
title = {Design principles for biologically inspired cognitive robotics},
journal = {Biologically Inspired Cognitive Architectures},
volume = {1},
pages = {73-81},
year = {2012},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2012.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X12000084},
author = {Jeffrey L. Krichmar},
keywords = {Cognition, Computational neuroscience, Embodiment, Neurorobots},
abstract = {The goals of cognitive robotics are to better understand cognition through the construction of physical artifacts, and to create practical systems that demonstrate cognitive capabilities. I believe for cognitive robotics to move forward, a balanced approach that emphasizes the interaction of brain, body, and environment is necessary. In general, cognitive robots and cognitive architectures focus too much on brain control, and overlook the contributions of morphology to intelligent behavior. On the other hand, the behavior based robotics approach is unbalanced in the opposite direction. For cognitive robotics to move forward, these disparate research communities need to come into balance. The materials, morphology, sensors, actuators, and the nervous system should be balanced and coordinated in their action. In their book, “How the body shapes the way we think: A new view of intelligence” (MIT Press, 2007), Pfeifer and Bongard have suggested that intelligent agents should follow a set of design principles that highlight the importance of embodiment and physical interaction with the environment. In the present paper, I apply each of these principles to biologically inspired cognitive robotics and suggest how the field can shift toward better cognitive architectures by adherence to these principles.}
}
@article{ZHANG20241079,
title = {Does architectural design require single-objective or multi-objective optimisation? A critical choice with a comparative study between model-based algorithms and genetic algorithms},
journal = {Frontiers of Architectural Research},
volume = {13},
number = {5},
pages = {1079-1094},
year = {2024},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2024.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S2095263524000542},
author = {Ran Zhang and Xiaodong Xu and Ke Liu and Lingyu Kong and Xi Wang and Linzhi Zhao and Abudureheman Abuduwayiti},
keywords = {Architectural design optimisation, Single-objective optimisation, Multi-objective optimisation, Energy efficiency, Early design decision},
abstract = {Efficiency and accuracy have been challenging in the design optimisation process driven by building simulation. The literature review identified the limitations of previous studies, prompting this study to explore the performance of single-objective versus multi-objective efficiency and accuracy on equivalent problems based on control variables and to consider more algorithmic options for a broader range of designs. This study constructed a comparative energy-related experiment whose results are in the same unit, either as a single-objective optimisation or split into two objectives. The project aims to reduce annual energy consumption and increase solar utilisation potential. Our approach focuses on the use of a surrogate modelling algorithm, Radial Basis Function Optimisation Algorithm (RBFOpt), with its multi-objective version RBFMOpt, to optimise the energy performance while quickly identifying new energy requirements for an iterative office building design logic, contrast to traditional genetic-algorithm-driven. In addition, the research also conducted a comparative study between RBFOpt and Covariance Matrix Adaptation Evolutionary Strategies (CMAES) in a single-objective comparison and between RBFMOpt and Nondominated Sorting Genetic Algorithm II (NSGA-II) in a multi-objective optimisation process. The comparison of these sets of Opt algorithms with evolutionary algorithms helps to provide data-driven evidence to support early design decisions.}
}
@article{MARTTUNEN2019604,
title = {Methods to inform the development of concise objectives hierarchies in multi-criteria decision analysis},
journal = {European Journal of Operational Research},
volume = {277},
number = {2},
pages = {604-620},
year = {2019},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2019.02.039},
url = {https://www.sciencedirect.com/science/article/pii/S0377221719301870},
author = {Mika Marttunen and Fridolin Haag and Valerie Belton and Jyri Mustajoki and Judit Lienert},
keywords = {Problem structuring, Multiple criteria analysis, OR in environment, Behavioural OR},
abstract = {Building a well-structured objectives hierarchy is central to multi-criteria decision analysis (MCDA). However, in the absence of a systematic methodology to support the process, this task has been described as “more art than science”. Objectives hierarchies often tend to become large and constraining the size of a hierarchy can be challenging. This paper proposes and illustrates the use of a set of methods to support the simplification of the hierarchies in contexts that are “data rich” and characterised by many objectives. The aim of using the proposed approach is to support decision analysts in developing an appropriately concise decision model for the further interactions with the stakeholders. Using data from two completed environmental cases we show retrospectively how qualitative (means-ends networks), semi-quantitative (relevancy analysis) and quantitative (correlation analysis, principal component analysis, local sensitivity analysis of weights) methods, used alone or in combination, can inform hierarchy development. We evaluate the potential benefits and challenges of each method and discuss the advantages and disadvantages of the simplification of an objectives hierarchy. Questionnaire-based relevancy analysis can be a useful method to identify and communicate important objectives in the early phases of an MCDA process with stakeholders, while correlation analysis can help to identify overlapping objectives, particularly in cases having many objectives and alternatives. It is intended that the methods support a facilitator in developing a clear understanding of the problem and also prompt deeper thinking about and discussion of the appropriate structure and content of an objectives hierarchy with the stakeholders involved.}
}
@article{FOX201783,
title = {Cognitive systems at the point of care: The CREDO program},
journal = {Journal of Biomedical Informatics},
volume = {68},
pages = {83-95},
year = {2017},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2017.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S1532046417300333},
author = {John Fox},
keywords = {Decision making, Knowledge engineering, Cognitive computing, Decision support systems, Artificial intelligence, Clinical expertise},
abstract = {CREDO is a framework for understanding human expertise and for designing and deploying systems that support cognitive tasks like situation and risk assessment, decision-making, therapy planning and workflow management. The framework has evolved through an extensive program of research on human decision-making and clinical practice. It draws on concepts from cognitive science, and has contributed new results to cognitive theory and understanding of human expertise and knowledge-based AI. These results are exploited in a suite of technologies for designing, implementing and deploying clinical services, early versions of which were reported by Das et al. (1997) [9] and Fox and Das (2000) [26]. A practical outcome of the CREDO program is a technology stack, a key element of which is an agent specification language (PROforma: Sutton and Fox (2003) [55]) which has proved to be a versatile tool for designing point of care applications in many clinical specialties and settings. Since software became available for implementing and deploying PROforma applications many kinds of services have been successfully built and trialed, some of which are in large-scale routine use. This retrospective describes the foundations of the CREDO model, summarizes the main theoretical, technical and clinical contributions, and discusses benefits of the cognitive approach.}
}
@article{YANG202171,
title = {Incremental fuzzy probability decision-theoretic approaches to dynamic three-way approximations},
journal = {Information Sciences},
volume = {550},
pages = {71-90},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.10.043},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520310288},
author = {Xin Yang and Dun Liu and Xibei Yang and Keyu Liu and Tianrui Li},
keywords = {Three-way decision, Dynamic three-way approximations, Incremental, Fuzzy probability},
abstract = {As a special model of three-way decision, three-way approximations in the fuzzy probability space can be interpreted, represented, and implemented as dividing the universe into three pair-wise disjoint regions, i.e., the positive, negative and boundary regions, which are transformed from the fuzzy membership grades with respect to the fuzzy concept. To consider the temporality and uncertainty of data simultaneously, this paper focuses on the integration of dynamics and fuzziness in the context of three-way approximations. We analyze and investigate three types of fuzzy conditional probability functions based on the fuzzy T-norm operators. Besides, we introduce the matrix-based fuzzy probability decision-theoretic models to dynamic three-way approximations based on the principle of least cost. Subsequently, to solve the time-consuming computational problem, we design the incremental algorithms by the updating strategies of matrices when the attributes evolve over time. Finally, a series of comparative experiments is reported to demonstrate and verify the performance of proposed models.}
}
@article{WALTZ2022633,
title = {From Childhood Trauma to Delusions: It’s Complicated},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {7},
number = {7},
pages = {633-634},
year = {2022},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2022.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S2451902222000982},
author = {James A. Waltz}
}
@article{MAGESHKUMAR2023301,
title = {Hybrid cloud storage system with enhanced multilayer cryptosystem for secure deduplication in cloud},
journal = {International Journal of Intelligent Networks},
volume = {4},
pages = {301-309},
year = {2023},
issn = {2666-6030},
doi = {https://doi.org/10.1016/j.ijin.2023.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666603023000295},
author = {Nagappan Mageshkumar and J. Swapna and A. Pandiaraj and R. Rajakumar and Moez Krichen and Vinayakumar Ravi},
keywords = {Cloud computing, Deduplication, Symmetrical encryption, Diffie-Hellman assumption, Cyber security},
abstract = {Data deduplication is a crucial technique in the field of data compression that aims to eliminate redundant copies of recurring data. This technique has gained significant popularity in the realm of cloud storage due to its ability to effectively reduce storage requirements and optimize bandwidth utilization. To ensure the safeguarding of sensitive data while simultaneously facilitating deduplication, researchers have put forth the concept of convergent encryption as a potential solution. This technique involves encrypting the data prior to its outsourcing, thereby enhancing the confidentiality of the information. In this work, an earnest endeavor is undertaken to formally tackle the issue of authorized data deduplication, with the aim of enhancing data security. Our approach combines the Diffie-Hellman algorithm and symmetrical external decision to protect and popularize information, ensuring end-to-end encryption to encourage user adoption of cloud storage. The proposed model employs block-level deduplication and guarantees the randomness of ciphertexts by generating encryption keys using the Diffie-Hellman algorithm. This method effectively counters both internal and external brute-force attacks, enhancing data security while reducing computational costs. An extensive experimentation is carried out to demonstrate that our approach is particularly beneficial in scenarios with multiple privilege sets. Overall, the proposed model offers an elaborate framework that maintains data privacy and strengthens security measures, contributing to a more efficient and secure cloud-based document search.}
}
@article{HOSHINO2024111660,
title = {Human-inspired similarity control system: Enhancing line-following robot perception},
journal = {Applied Soft Computing},
volume = {160},
pages = {111660},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111660},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624004344},
author = {Yukinobu Hoshino and Yuka Nishiyama and Toshimi Yamamoto and Yuki Shinomiya and Namal Rathnayake and Tuan Linh Dang},
keywords = {Human-inspired control, Similarity inference, Control systems, Robotics, Line-following control, Noise sensitivity},
abstract = {Human-Inspired Control (HIC) holds promise for endowing machines with human-like cognition, decision-making, and adaptability. In this study, we employ a fusion of cognitive modeling, machine learning, and control theory as the foundational architecture and empirically validate its suitability for robot control within the realm of HIC. Fuzzy logic stands out as a viable approach for HIC control, wherein control rules can be devised drawing from human intuitive inspiration. Specifically, this study explores similarity inference control systems in robotics, with the objective of enhancing line-following control as an alternative to fuzzy systems. The experimental optimization results provide insights into the advantages and limitations of the similarity inference control system. Despite achieving performance comparable to that of traditional two-stage fuzzy control systems, careful consideration of noise sensitivity is paramount. While the similarity inference approach streamlines implementation and obviates the necessity for expert-designed fuzzy rules, its susceptibility to noise can compromise performance, particularly in noisy environments. These considerations are pivotal for the development of control systems aimed at mitigating noise sensitivity, enhancing task-specific performance, and ensuring the adaptability and robustness of line-following robots. To tackle this challenge, we both discuss and experimentally evaluate potential solutions and their applicability in this paper.}
}
@article{FAGHIHI201538,
title = {A cognitive model fleshes out Kahneman’s fast and slow systems},
journal = {Biologically Inspired Cognitive Architectures},
volume = {11},
pages = {38-52},
year = {2015},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2014.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X14000772},
author = {Usef Faghihi and Clayton Estey and Ryan McCall and Stan Franklin},
keywords = {Learning Intelligent Distribution Agent (LIDA), Kahneman’s fast and slow systems, Cognitive architecture, Consciously mediated action selection, Deliberative decision making},
abstract = {Daniel Kahneman (2011) posits two main processes that characterize thinking: “System 1” is a fast decision making system responsible for intuitive decision making based on emotions, vivid imagery, and associative memory. “System 2” is a slow system that observes System 1’s outputs, and intervenes when “intuition” is insufficient. Such an intervention occurs “when an event is detected that violates the model of the world that System 1 maintains” (Kahneman, 2011, p. 24). Here, we propose specific underlying mechanisms for Kahneman’s Systems 1 and 2, in terms of the LIDA model, a broad, systems-level, cognitive architecture (Franklin et al., 2014). LIDA postulates that human cognition consists of a continuing, overlapping iteration of cognitive cycles, each a cognitive “atom,” out of which higher-order processes are built. In LIDA terms, System 1 employs consciously mediated action selection in which a stimulus is acted upon within one or two cognitive cycles. In contrast, System 2, which LIDA posits to operate according to James’ ideomotor theory (James, 1950), requires more cognitive cycles in its deliberative decision making. Thus, we suggest that System 2 employs multiple occurrences of System 1 in its operation. To test the proposed mechanisms, we perform an in silico experiment using a LIDA-based software agent.}
}
@article{DAS20221101,
title = {Damage identification of structures using incomplete mode shape and improved TLBO-PSO with self-controlled multi-stage strategy},
journal = {Structures},
volume = {35},
pages = {1101-1124},
year = {2022},
issn = {2352-0124},
doi = {https://doi.org/10.1016/j.istruc.2021.07.089},
url = {https://www.sciencedirect.com/science/article/pii/S2352012421007074},
author = {Subhajit Das and Nirjhar Dhang},
keywords = {Structural health monitoring, Damage identification, Metaheuristics, Hybrid ITLBO-PSO, Inverse problem},
abstract = {The present work presents an efficient multi-stage optimization method-based damage detection method for truss and frame structures equipped with a limited number of sensors. In this approach, a Finite Element (FE) model is developed to simulate the response of the actual structure. The limited sensor condition for this FE model is achieved by the modal reduction method. A comparison study among three well-established modal reduction methods has been performed, and Iterated Improved Reduction System (IIRS) approach has been selected for the present study. Next, the damage identification problem is defined as an unconstrained optimization problem. The objective function of the optimization problem is formulated using the weighted linear combination of the frequencies and mode shapes obtained from the actual damaged structure and reduced FE model. This objective function is minimized by the improved version of hybrid Teaching Learning Based Optimization - Particle Swarm Optimization (ITLBO-PSO) utilizing a self-controlled multi-stage (SCMS) strategy. In this method, the SCMS strategy automatically reduces the search dimension of the optimization problem in each stage. Four examples with different damage scenarios from the relevant literature are considered in the present study to demonstrate the efficacy of the proposed method. The proposed method results for both with noise and without noise are compared with existing literature and nine other well-established algorithms. The results show that the proposed ITLBO-PSO with SCMS strategy identifies damages with adequate precision and outperforms the other algorithms regarding the accuracy and computational cost.}
}
@article{SANUSI2024100212,
title = {Stakeholders’ insights on artificial intelligence education: Perspectives of teachers, students, and policymakers},
journal = {Computers and Education Open},
volume = {7},
pages = {100212},
year = {2024},
issn = {2666-5573},
doi = {https://doi.org/10.1016/j.caeo.2024.100212},
url = {https://www.sciencedirect.com/science/article/pii/S2666557324000521},
author = {Ismaila Temitayo Sanusi and Friday Joseph Agbo and Oluwaseun Alexander Dada and Abdullahi Abubakar Yunusa and Kehinde D. Aruleba and George Obaido and Olayemi Olawumi and Solomon Sunday Oyelere and  {Centre for Multidisciplinary Research and Innovation (CEMRI)}},
keywords = {Artificial intelligence, AI literacy, School education, Teachers, Policymakers, Nigeria},
abstract = {The integration of artificial intelligence (AI) as a subject into K-12 education worldwide is still in its early stages and undoubtedly needs further investigation. There is limited effort on understanding policymakers, teachers and students’ viewpoints on AI learning within the school system. This study gathered the thoughts of key stakeholders, including policymakers, higher education and K-12 teachers, and students in Nigeria, to understand their conceptions, concerns, and dispositions, with the aim of aiding the implementation of AI in schools. We further explored the needs of the diverse stakeholders, how they can be supported and juxtaposed their views to identify their priorities and how their opinions combined could give a holistic approach to the effective implementation of AI education. This research employed a qualitative methodology using semi-structured interviews as the means of data collection. The thematic analysis of the interview data from the 21 participants indicates their conceptions, what they considered the priorities for including AI in the school system, concerns and support needed to implement AI in schools. The findings of this study contribute to the ongoing conversation on how to effectively integrate AI into school curriculum.}
}
@article{SINGH2020721,
title = {BlockIoTIntelligence: A Blockchain-enabled Intelligent IoT Architecture with Artificial Intelligence},
journal = {Future Generation Computer Systems},
volume = {110},
pages = {721-743},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19316474},
author = {Sushil Kumar Singh and Shailendra Rathore and Jong Hyuk Park},
keywords = {Artificial intelligence, Blockchain, Internet of things, Big data analysis, Security and privacy},
abstract = {In the recent year, Internet of Things (IoT) is industrializing in several real-world applications such as smart transportation, smart city to make human life reliable. With the increasing industrialization in IoT, an excessive amount of sensing data is producing from various sensors devices in the Industrial IoT. To analyzes of big data, Artificial Intelligence (AI) plays a significant role as a strong analytic tool and delivers a scalable and accurate analysis of data in real-time. However, the design and development of a useful big data analysis tool using AI have some challenges, such as centralized architecture, security, and privacy, resource constraints, lack of enough training data. Conversely, as an emerging technology, Blockchain supports a decentralized architecture. It provides a secure sharing of data and resources to the various nodes of the IoT network is encouraged to remove centralized control and can overcome the existing challenges in AI. The main goal of our research is to design and develop an IoT architecture with blockchain and AI to support an effective big data analysis. In this paper, we propose a Blockchain-enabled Intelligent IoT Architecture with Artificial Intelligence that provides an efficient way of converging blockchain and AI for IoT with current state-of-the-art techniques and applications. We evaluate the proposed architecture and categorized into two parts: qualitative analysis and quantitative analysis. In qualitative evaluation, we describe how to use AI and Blockchain in IoT applications with “AI-driven Blockchain” and “Blockchain-driven AI.” In quantitative analysis, we present a performance evaluation of the BlockIoTIntelligence architecture to compare existing researches on device, fog, edge and cloud intelligence according to some parameters such as accuracy, latency, security and privacy, computational complexity and energy cost in IoT applications. The evaluation results show that the proposed architecture performance over the existing IoT architectures and mitigate the current challenges.}
}
@article{XU202468,
title = {Frescoes restoration via virtual-real fusion: Method and practice},
journal = {Journal of Cultural Heritage},
volume = {66},
pages = {68-75},
year = {2024},
issn = {1296-2074},
doi = {https://doi.org/10.1016/j.culher.2023.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S1296207423002121},
author = {Hui Xu and Yonghua Zhang and Jiawan Zhang},
keywords = {Mural virtual restoration, Saliency detection, Fusion of virtual and real scene},
abstract = {In the era of artificial intelligence, image-based virtual restoration of cultural relics is one of the methods used in the restoration of cultural relics. As the most informative representative of cultural heritage in the study of historical materials, murals occupy a significant position in archaeology and ancient culture. Currently, most of the existing virtual restoration of murals is limited to the restoration of image information for local damage. The scenes of murals have large spatial scales and complex semantic contents. In order to enhance the semantic relevance of the virtual restoration of murals and the immersion of a visual perception, this paper proposes a kind of mural virtual-real fusion restoration display method based on visual attention mechanism. Based on the case study on the immersive fusion restoration of Dunhuang grotto murals, the feasibility of regional mural image restoration and real space scene fusion restoration is verified. A new paradigm of mural heritage protection in the context of virtual-real fusion is realized.}
}
@incollection{MARWALA2024115,
title = {Chapter 8 - Economics},
editor = {Tshilidzi Marwala},
booktitle = {Mechanism Design, Behavioral Science and Artificial Intelligence in International Relations},
publisher = {Morgan Kaufmann},
pages = {115-131},
year = {2024},
isbn = {978-0-443-23982-3},
doi = {https://doi.org/10.1016/B978-0-443-23982-3.00008-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780443239823000087},
author = {Tshilidzi Marwala},
keywords = {Artificial intelligence, Behavioral science, Computer in society, Economics, Game theory, Macroeconomics, Mechanism design, Rational choice},
abstract = {Artificial intelligence (AI), behavioral science, and mechanism design are transforming the economic landscape. These multidisciplinary technologies have the potential to transform conventional economic paradigms by providing novel insights and tools that can lead to more effective, humane, and sustainable systems. AI introduces computational power and automation, enhancing market efficiency, personalizing services, and creating new avenues for economic expansion. Behavioral science provides a lens through which to comprehend and predict human actions, decisions, and biases, enhancing the interaction between economics and individual and social behavior. Mechanism design, a field closely related to game theory, aims to create rules or systems to accomplish particular outcomes in various economic interactions. These disciplines create a compelling synthesis addressing the complexities of contemporary economic challenges. This integration represents an exciting frontier of economic theory and practice, from understanding the nuances of human choice to designing efficient and fair markets. Applying AI and behavioral science insights in mechanism design must be cautiously approached to ensure that technological and human-centered innovations do not result in unintended consequences such as inequality, manipulation, or loss of privacy.}
}
@article{AGGARI20209,
title = {Leaders shaping leadership: Knowledge, professional values and competency as prognosticators of career growth and development among nurses},
journal = {Enfermería Clínica},
volume = {30},
pages = {9-14},
year = {2020},
note = {Our Lady of Fatima University’s Research Development and Innovation Conference 2019},
issn = {1130-8621},
doi = {https://doi.org/10.1016/j.enfcli.2019.09.020},
url = {https://www.sciencedirect.com/science/article/pii/S1130862119305753},
author = {Michael I. Aggari and Michael Joseph S. Diño and Christian Jay S. Orte},
keywords = {Career growth and development, Prognosticators, SEM-PLS, Descriptive-correlational, Nursing},
abstract = {The study aimed to determine the prognosticators of career growth and development among nurses in tertiary hospitals in Region III, Philippines. The study included 223 nurse-leaders from nineteen (19) participating hospitals as participants via census sampling. The tool utilized for the study was an adopted survey questionnaire that underwent validation before its distribution. Using SEM-PLS through WARPLS, data were analyzed and presented. SPSS v. 21 was used for computation of demographics. Results showed that there is significant result when it comes to knowledge in relation to career growth and development (path=0.217, p<.05, f2=.167). Second, professional values affect the career growth and development of nurses (path=0.564, p<.05, f2=.450). Lastly, competency reveals that it does not affect the career growth and development among nurses (path=0.037, p<.05, f2=.028). It was found that all the three prognosticators of career growth and development (knowledge, professional values, and competency) among nurses have strong significant relationships to each other.}
}
@incollection{HAWKINS198965,
title = {A Biologically Based Computational Model for Several Simple Forms of Learning},
editor = {Robert D. Hawkins and Gordon H. Bower},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {23},
pages = {65-108},
year = {1989},
booktitle = {Computational Models of Learning in Simple Neural Systems},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(08)60109-7},
url = {https://www.sciencedirect.com/science/article/pii/S0079742108601097},
author = {Robert D. Hawkins},
abstract = {Publisher Summary
This chapter describes the model in which several higher-order features of classical conditioning can be generated from combinations of the cellular mechanisms used in simpler forms of learning. The model presented is similar in many respects to the mathematical models proposed by modern behaviorists to account for the same phenomena. The model described in the chapter simulates a wide range of phenomena in habituation, sensitization, basic classical conditioning, and higher-order features of conditioning, but it fails to explain several other important learning phenomena, including sensory preconditioning, conditioned inhibition, and the nature of the conditioned response. However, the model differs from most mathematical models in that it is based on known physiology and neural circuitry. As a consequence, it utilizes separate mechanisms (facilitation and depression) for positive and negative learning. The tests should provide additional insights into the ways in which several simple forms of learning are related to the level of their basic cellular mechanisms.}
}
@incollection{HIBBERT2024135,
title = {Chapter Four - Antimicrobials: An update on new strategies to diversify treatment for bacterial infections},
editor = {Robert K. Poole and David J. Kelly},
series = {Advances in Microbial Physiology},
publisher = {Academic Press},
volume = {84},
pages = {135-241},
year = {2024},
booktitle = {Advances in Microbial Physiology},
issn = {0065-2911},
doi = {https://doi.org/10.1016/bs.ampbs.2023.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0065291123000309},
author = {Tegan Hibbert and Zeljka Krpetic and Joe Latimer and Hollie Leighton and Rebecca McHugh and Sian Pottenger and Charlotte Wragg and Chloë E. James},
keywords = {Antibiotic efficacy, Antimicrobial resistance, Novel therapeutics, Bacterial infection, Susceptibility testing, Discovery pipeline},
abstract = {Ninety-five years after Fleming’s discovery of penicillin, a bounty of antibiotic compounds have been discovered, modified, or synthesised. Diversification of target sites, improved stability and altered activity spectra have enabled continued antibiotic efficacy, but overwhelming reliance and misuse has fuelled the global spread of antimicrobial resistance (AMR). An estimated 1.27 million deaths were attributable to antibiotic resistant bacteria in 2019, representing a major threat to modern medicine. Although antibiotics remain at the heart of strategies for treatment and control of bacterial diseases, the threat of AMR has reached catastrophic proportions urgently calling for fresh innovation. The last decade has been peppered with ground-breaking developments in genome sequencing, high throughput screening technologies and machine learning. These advances have opened new doors for bioprospecting for novel antimicrobials. They have also enabled more thorough exploration of complex and polymicrobial infections and interactions with the healthy microbiome. Using models of infection that more closely resemble the infection state in vivo, we are now beginning to measure the impacts of antimicrobial therapy on host/microbiota/pathogen interactions. However new approaches are needed for developing and standardising appropriate methods to measure efficacy of novel antimicrobial combinations in these contexts. A battery of promising new antimicrobials is now in various stages of development including co-administered inhibitors, phages, nanoparticles, immunotherapy, anti-biofilm and anti-virulence agents. These novel therapeutics need multidisciplinary collaboration and new ways of thinking to bring them into large scale clinical use.}
}
@article{MIRA2008671,
title = {Symbols versus connections: 50 years of artificial intelligence},
journal = {Neurocomputing},
volume = {71},
number = {4},
pages = {671-680},
year = {2008},
note = {Neural Networks: Algorithms and Applications 50 Years of Artificial Intelligence: a Neuronal Approach},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2007.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S0925231207003451},
author = {José Mira Mira},
keywords = {Artificial intelligence, Connectionism, Symbolic paradigm, Situated cognition},
abstract = {Artificial intelligence (AI) was born connectionist when in 1943 Warren S. McCulloch and Walter Pitts introduced the first sequential logic model of neuron. The 1950s sees the passage from numerical to symbolic computation with the christening of AI in 1956. In 1986, there is a rebirth of connectionism at the same time that an emphasis in knowledge modeling and inference, both symbolic and connectionist. We thus reach the present state in which different paradigms coexist (symbolic, connectionist, situated and hybrid). In this work, we will attempt (1) to approach the concept of AI both as a science of the natural and as knowledge engineering (KE); (2) summarize some of the conceptual, formal and methodological approaches to the development of AI during the last 50 years, (3) mention some of the constitutive differences between human knowing and machine knowing and (4) propose some suggestions that we believe must be adopted to progress in developing AI.}
}
@article{STUHLER2021101568,
title = {What's in a category? A new approach to Discourse Role Analysis},
journal = {Poetics},
volume = {88},
pages = {101568},
year = {2021},
note = {Measure Mohr Culture},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2021.101568},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X21000516},
author = {Oscar Stuhler},
keywords = {Discourse Role Analysis, DRA, Refugee, Identity category, Text analysis, Dependency parsing},
abstract = {Building on the work of John Mohr, I propose a new, broadly applicable approach to Discourse Role Analysis (DRA). Whereas the goal of behavioral role analysis is to identify the different kinds of actors that exist in interaction, the goal of DRA is to identify the different kinds of identities that exist in discourse. To do this, I suggest thinking of discourse roles as latent conceptions of identities composed of treatments, actions, and characteristics that are frequently concurrently associated with identities in stories. I propose a method to infer discourse roles from unstructured text data that draws on novel techniques from Natural Language Processing. This framework is leveraged to shed light on German news coverage of refugees (2010-2020), which employs a set of distinct discourse roles such as refugee as claimant of welfare benefits, refugee in distress at sea, and refugee as a criminal. I then assess how different refugee identity categories are situated within this discourse role structure. I pay particular attention to Geflüchtete, a category that emerged only recently in German discourse. Whereas initial use of Geflüchtete was motivated by a language critique that aimed at replacing the general term for refugees (Flüchtlinge), DRA indicates a process of categorical differentiation in which the category increasingly serves to distinguish different kinds of refugees.}
}
@article{CHEN2013521,
title = {Ordering based decision making – A survey},
journal = {Information Fusion},
volume = {14},
number = {4},
pages = {521-531},
year = {2013},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2012.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S1566253512000930},
author = {Shuwei Chen and Jun Liu and Hui Wang and Juan Carlos Augusto},
keywords = {Decision making, Ordering relation, Preference, Lattice, Logic, Aggregation},
abstract = {Decision making is the crucial step in many real applications such as organization management, financial planning, products evaluation and recommendation. Rational decision making is to select an alternative from a set of different ones which has the best utility (i.e., maximally satisfies given criteria, objectives, or preferences). In many cases, decision making is to order alternatives and select one or a few among the top of the ranking. Orderings provide a natural and effective way for representing indeterminate situations which are pervasive in commonsense reasoning. Ordering based decision making is then to find the suitable method for evaluating candidates or ranking alternatives based on provided ordinal information and criteria, and this in many cases is to rank alternatives based on qualitative ordering information. In this paper, we discuss the importance and research aspects of ordering based decision making, and review the existing ordering based decision making theories and methods providing future research directions.}
}
@incollection{FREDERIKSEN200112219,
title = {Propositional Representations in Psychology},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {12219-12224},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/01490-X},
url = {https://www.sciencedirect.com/science/article/pii/B008043076701490X},
author = {C.H. Frederiksen},
abstract = {In psychology the term ‘proposition’ refers to a truth-valued unit of meaning that is represented symbolically in the human cognitive system. Propositions consist of: semantic information structures that represent aspects of experience (e.g., events, states), real and hypothetical situations in the world, and abstract conceptual knowledge; and truth value and modality information that represents an attitude or belief on the part of a speaker, writer, or any ‘holder’ of the proposition, with respect to the truth or falsity of the semantic relationships that are represented by the proposition. Propositions represent types of semantic information that are distinguished in natural languages (e.g., events, states, processes, abstract relationships, logical, conditional and causal dependencies, etc.), and they serve as truth-valued and quantified predicates to which logical reasoning operations may be applied. Propositional representations are intermediate units of cognitive representation within a stratified semantic and natural language processing system where they play a central role in such cognitive processes as comprehension, inference, reasoning, thinking, learning, and interactive communication. Formal models of propositional representations have been developed using both canonical frame theory and the theory of semantic grammars. These models provide a principled basis for methods of propositional analysis and cognitive discourse analysis.}
}
@article{ALABDULATIF20243917,
title = {A Review on Security and Privacy Issues Pertaining to Cyber-Physical Systems in the Industry 5.0 Era},
journal = {Computers, Materials and Continua},
volume = {80},
number = {3},
pages = {3917-3943},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.054150},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824006507},
author = {Abdullah Alabdulatif and Navod Neranjan Thilakarathne and Zaharaddeen Karami Lawal},
keywords = {Cyber-physical systems, CPS, Industry 5.0, security, data privacy, human-machine collaboration, data protection},
abstract = {The advent of Industry 5.0 marks a transformative era where Cyber-Physical Systems (CPSs) seamlessly integrate physical processes with advanced digital technologies. However, as industries become increasingly interconnected and reliant on smart digital technologies, the intersection of physical and cyber domains introduces novel security considerations, endangering the entire industrial ecosystem. The transition towards a more cooperative setting, including humans and machines in Industry 5.0, together with the growing intricacy and interconnection of CPSs, presents distinct and diverse security and privacy challenges. In this regard, this study provides a comprehensive review of security and privacy concerns pertaining to CPSs in the context of Industry 5.0. The review commences by providing an outline of the role of CPSs in Industry 5.0 and then proceeds to conduct a thorough review of the different security risks associated with CPSs in the context of Industry 5.0. Afterward, the study also presents the privacy implications inherent in these systems, particularly in light of the massive data collection and processing required. In addition, the paper delineates potential avenues for future research and provides countermeasures to surmount these challenges. Overall, the study underscores the imperative of adopting comprehensive security and privacy strategies within the context of Industry 5.0.}
}
@article{BARKANA2022103249,
title = {Analysis of working memory from EEG signals under different emotional states},
journal = {Biomedical Signal Processing and Control},
volume = {71},
pages = {103249},
year = {2022},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2021.103249},
url = {https://www.sciencedirect.com/science/article/pii/S1746809421008466},
author = {Buket D. Barkana and Yusuf Ozkan and Joanna A. Badara},
keywords = {Human-computer interface, Working memory, Emotion recognition, EEG},
abstract = {This study analyzes electroencephalography (EEG) measurements during short-term memory retention under different emotional states. A public-domain library with emotion-annotated images (IAPS) was used to stimulate neutral, negative, and positive emotions. The associated EEG data were acquired from twelve volunteers (between 20 and 26 years old; ten males and two females). Each participant was exposed to three sessions back-to-back on the same day. Each session corresponded to the induced emotional states (positive, negative and neutral) and consisted of relaxation, memorization of a list of ten words and ten numbers, watching a set of images to arouse emotion, and recalling the words and numbers memorized earlier. Statistical and spectral features of EEG data were analyzed for two instances: emotion recognition (neutral, negative, and positive) and recall events under the three emotional states. By designing two baseline machine-learning models, support vector machines (SVMs) and K-nearest neighbor (KNN), the significance of the EEG bands and the brain lobes were studied. Experimental results suggest that the short-term (working) memory recalls after exposure to neutral, negative, and positive images (to arouse neutral, negative, and positive emotions) differ from each other significantly (at alpha level 0.001). We have found that each EEG band carries unique information in both emotion and memory recall classification tasks and recommend that the entire EEG signal frequency range must be analyzed in future similar studies. On the other hand, we also have found that each brain region carries similar information as it relates to each task (i.e., memorization, recall), thus only one of the brain regions can be analyzed in future studies in order to avoid complexity and high computation time.}
}
@incollection{GRENANDER19801,
title = {SOME IDEAS IN COMPUTATIONAL PROBABILITY},
editor = {P.M. Kahn},
booktitle = {Computational Probability},
publisher = {Academic Press},
pages = {1-10},
year = {1980},
isbn = {978-0-12-394680-5},
doi = {https://doi.org/10.1016/B978-0-12-394680-5.50007-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780123946805500079},
author = {Ulf Grenander},
abstract = {Publisher Summary
This chapter presents some ideas in computational probability. The computers play an important role in contemporary mathematics. When computers began to be available to the academic community, their importance became obvious to workers in many fields: physics, biology, medicine, and geology. The examples of a computer's use can also be found in the social sciences, such as in statistical data analysis, and even in humanities. The mathematicians do not show the same sort of enthusiastic acceptance of the computer as a new research tool, except for isolated cases such as Lehmer's work in number theory. It is important to realize the qualitative difference between mathematics and the physical sciences. The chemistry or astronomy deals with certain aspects of the physical world, while mathematics deals with an abstract world constructed by man. The probabilist deals with logical structures and not with lots of data so that he could not be confronted with massive number crunching.}
}
@article{LIAN202236,
title = {Comments on “Fiscal and monetary stabilization policy at the zero lower bound: Consequences of limited foresight” by Woodford and Xie},
journal = {Journal of Monetary Economics},
volume = {125},
pages = {36-39},
year = {2022},
issn = {0304-3932},
doi = {https://doi.org/10.1016/j.jmoneco.2021.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0304393221001227},
author = {Chen Lian},
keywords = {Fiscal and monetary policy, Bounded rationality, Finite-horizon planning, Ricardian equivalence}
}
@article{ZOU2021197,
title = {Decentralised task allocation using GDL negotiations in Multi-agent system},
journal = {Cognitive Robotics},
volume = {1},
pages = {197-204},
year = {2021},
issn = {2667-2413},
doi = {https://doi.org/10.1016/j.cogr.2021.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S2667241321000112},
author = {Hui Zou and Yan Xi},
keywords = {Multi-agent System, Allocation Problem, Negotiation, Game De- scription Language, Decentralized algorithm},
abstract = {In large distributed systems, the optimization algorithm of task scheduling may not meet the special requirements of the domain control mechanism, i.e. robustness, optimality, timeliness of solution and computational ease of processing under limited communication. In or- der to satisfy these requirements, a novel decentralized agent scheduling method for dynamic task allocation problems based on Game Descrip- tion Language (GDL) and Game Theory is proposed. Specifically, we define the task allocation problem as a stochastic game model, in which the agent's utility is derived from the marginal utility, and then prove that the global optimal task allocation scheme resides in the Nash equi- librium set by the non-cooperative game. In order to generate an optimal solution, we define Multi-agent Negotiation Game (MNG), in which ne- gotiations are held between agents to decide which tasks to act on next. Building on this, we make a simple extension to adopt GDL more suit- able for negotiations and propose to use it to model such negotiation scenarios. Finally, we use a negotiation example to show that our ap- proach is more amenable to automatic processing by autonomous agents and of great practicality than a centralized task scheduler.}
}
@incollection{KENNEDY2001187,
title = {chapter five - Humans—Actual, Imagined, and Implied},
editor = {James Kennedy and Russell C. Eberhart and Yuhui Shi},
booktitle = {Swarm Intelligence},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {187-259},
year = {2001},
series = {The Morgan Kaufmann Series in Artificial Intelligence},
isbn = {978-1-55860-595-4},
doi = {https://doi.org/10.1016/B978-155860595-4/50005-X},
url = {https://www.sciencedirect.com/science/article/pii/B978155860595450005X},
author = {James Kennedy and Russell C. Eberhart and Yuhui Shi},
abstract = {Publisher Summary
Psychology traces its descent from philosophy and physical and natural sciences. The 20th century saw tremendous improvements in scientific methods for research and analysis of empirical data on psychological phenomena and a rapid expansion of scientific knowledge about human thought, feeling, and behavior. Anthropologists have long noted that cultural change resembles biological evolution in its ability to adapt to new and diverse circumstances. The ultimate example of interpenetration of mind and culture is language. Language is viewed as an emergent system, where each individual in a linguistic community participates in the creation of the language and in its maintenance. Cognitive scientists have expended much effort discussing distinctions between symbolic and statistical modes of thinking. Computers have long been used to explore social and cognitive hypotheses. This chapter describes some of the issues involved in simulating sociocognitive phenomena and previous work that influence the research.}
}
@article{GARDNER2010251,
title = {Stochastic simulation and graphic visualization of mitotic processes},
journal = {Methods},
volume = {51},
number = {2},
pages = {251-256},
year = {2010},
note = {Methods Related to Mitotic Spindle Research},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2010.01.021},
url = {https://www.sciencedirect.com/science/article/pii/S1046202310000368},
author = {Melissa K. Gardner and David J. Odde},
keywords = {Mitosis, Microtubule, Simulation, Stochastic, Imaging, Yeast},
abstract = {Computational modeling can be extremely useful in interpreting experimental results. Here we describe how a relatively sophisticated stochastic model for microtubule dynamic instability in the mitotic spindle can be developed starting with straightforward rules and simple programming code. Once this model is developed, the method for comparing simulation results to experimental data must be carefully considered. The ultimate utility of any computational model relies on its predictive power and the ability to assist in designing new experiments. We describe how “deconstructing” the model through the use of quantitative animations contributes to a better qualitative understanding of model behavior. By extracting key qualitative elements of the model in this fashion, model predictions and new experiments can be more easily extracted from model results.}
}
@article{CUNTZE2004343,
title = {The predictive capability of failure mode concept-based strength criteria for multidirectional laminates},
journal = {Composites Science and Technology},
volume = {64},
number = {3},
pages = {343-377},
year = {2004},
note = {Failure criteria in fibre reinforced polymer composites Part C: Additional theories conclusions and recommendations},
issn = {0266-3538},
doi = {https://doi.org/10.1016/S0266-3538(03)00218-5},
url = {https://www.sciencedirect.com/science/article/pii/S0266353803002185},
author = {R.G. Cuntze and A. Freund},
keywords = {B. Non-linear behaviour, Multi-axial stressing, Multidirectional laminates, C. Failure criterion},
abstract = {This contribution is a post-runner to the ‘failure exercise’. It focuses on two aspects of the theoretical prediction of failure in composites [1–3]: the first is the derivation of failure conditions for a unidirectional (UD) lamina with the prediction of initial failure of the embedded lamina, and the second the treatment of non-linear, progressive failure of 3-dimensionally stressed laminates until final failure. The failure conditions are based on the so-called Failure Mode Concept (FMC) which takes into account the material-symmetries (by the application of invariants) of the UD-lamina homogenized to a ‘material’, and on a strict failure mode thinking. The results of the investigation are stress–strain curves for the various given GFRP-/CFRP-UD-laminae, biaxial failure stress envelopes for the UD-laminae, and initial as well as final biaxial failure envelopes for the laminates. In addition a brief comparison between Puck's and Cuntze's failure theory is presented by the authors themselves.}
}
@article{LEE2021103301,
title = {Improving preservice teachers’ noticing skills through technology-aided interventions in mathematics pedagogy courses},
journal = {Teaching and Teacher Education},
volume = {101},
pages = {103301},
year = {2021},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2021.103301},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X21000251},
author = {Mi Yeon Lee},
keywords = {Preservice teachers, Technology, Noticing skills},
abstract = {This study investigated effects of employing the Three-point framework (Key Point, Difficult Point, Critical Point) and three technology-aided interventions (online discussions, clinical interviews, and graphic lesson plan construction) on developing preservice teachers’ noticing skills in elementary mathematics pedagogy courses. Pre- and post-intervention assessments revealed significant improvement in the treatment group’s scores, and two case studies illustrated how the interventions can help preservice teachers develop noticing skills. These findings suggest that combining the framework with technology support provides an accessible model for guiding preservice teachers to use evidence from task-based interactions with learners to support instructional decisions.}
}
@article{CARNEY2015288,
title = {Hypothesis generation using network structures on community health center cancer-screening performance},
journal = {Journal of Biomedical Informatics},
volume = {57},
pages = {288-307},
year = {2015},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2015.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1532046415001720},
author = {Timothy Jay Carney and Geoffrey P. Morgan and Josette Jones and Anna M. McDaniel and Michael T. Weaver and Bryan Weiner and David A. Haggstrom},
keywords = {Computational modeling, Simulation, Community health centers, Systems-thinking, Cancer screening, Network theory, Learning health system, Health Disparities},
abstract = {Research objectives: Nationally sponsored cancer-care quality-improvement efforts have been deployed in community health centers to increase breast, cervical, and colorectal cancer-screening rates among vulnerable populations. Despite several immediate and short-term gains, screening rates remain below national benchmark objectives. Overall improvement has been both difficult to sustain over time in some organizational settings and/or challenging to diffuse to other settings as repeatable best practices. Reasons for this include facility-level changes, which typically occur in dynamic organizational environments that are complex, adaptive, and unpredictable. This study seeks to understand the factors that shape community health center facility-level cancer-screening performance over time. This study applies a computational-modeling approach, combining principles of health-services research, health informatics, network theory, and systems science. Methods: To investigate the roles of knowledge acquisition, retention, and sharing within the setting of the community health center and to examine their effects on the relationship between clinical decision support capabilities and improvement in cancer-screening rate improvement, we employed Construct-TM to create simulated community health centers using previously collected point-in-time survey data. Construct-TM is a multi-agent model of network evolution. Because social, knowledge, and belief networks co-evolve, groups and organizations are treated as complex systems to capture the variability of human and organizational factors. In Construct-TM, individuals and groups interact by communicating, learning, and making decisions in a continuous cycle. Data from the survey was used to differentiate high-performing simulated community health centers from low-performing ones based on computer-based decision support usage and self-reported cancer-screening improvement. Results: This virtual experiment revealed that patterns of overall network symmetry, agent cohesion, and connectedness varied by community health center performance level. Visual assessment of both the agent-to-agent knowledge sharing network and agent-to-resource knowledge use network diagrams demonstrated that community health centers labeled as high performers typically showed higher levels of collaboration and cohesiveness among agent classes, faster knowledge-absorption rates, and fewer agents that were unconnected to key knowledge resources. Conclusions and research implications: Using the point-in-time survey data outlining community health center cancer-screening practices, our computational model successfully distinguished between high and low performers. Results indicated that high-performance environments displayed distinctive network characteristics in patterns of interaction among agents, as well as in the access and utilization of key knowledge resources. Our study demonstrated how non-network-specific data obtained from a point-in-time survey can be employed to forecast community health center performance over time, thereby enhancing the sustainability of long-term strategic-improvement efforts. Our results revealed a strategic profile for community health center cancer-screening improvement via simulation over a projected 10-year period. The use of computational modeling allows additional inferential knowledge to be drawn from existing data when examining organizational performance in increasingly complex environments.}
}
@incollection{ALTERMAN20019971,
title = {Modularity versus Interactive Processing, Psychology of},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {9971-9973},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/01543-6},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767015436},
author = {R.E. Alterman},
abstract = {Theories of modularity function to root the structural elements of language and thought in special purpose, neurologically hardwired, faculties of mind. Interaction accounts complement modularity stories about biologically produced structures with stories about external structures used during, and created by, activity. They also invite skepticism about the assumptions made in the modularity arguments. There are three points of contrast between the assumptions underlying modularity and interactionism. Modularity accounts assume as a basic unit of analysis a reduction of mind to what goes on in the head; interactionist accounts assume interaction, especially social interaction, as the basic unit of analysis. A second point of contrast concerns the difference between structures as biologically determined and external structures that emerge as a product of human activity. The final point concerns the historical aspects of cognition. These differences in assumption lead to some critical differences in viewpoint. Whether language and its structure can be reduced to an analysis independent of semantics and the social interaction in which language (and language learning) occurs is a point of debate. With regard to Fodor's version of modularity, the issue pivots around the difference between a ‘language of thought’ and the structure of thinking, which depends on the history of such an activity within a community of actors. Additionally, many interactionists argue against the notion of internal representations of the sort supported by a ‘language of thought’ argument.}
}
@article{MURPHY1995409,
title = {A perspective of HPCN requirements in the European Aerospace Industry},
journal = {Future Generation Computer Systems},
volume = {11},
number = {4},
pages = {409-418},
year = {1995},
note = {High Performance Computing and Networking},
issn = {0167-739X},
doi = {https://doi.org/10.1016/0167-739X(95)00019-O},
url = {https://www.sciencedirect.com/science/article/pii/0167739X9500019O},
author = {J.A. Murphy},
keywords = {Design cycles, Product complexity, Computational modelling, Numerical algorithms, Parallel processing, Grid generation},
abstract = {Product complexity is increasing rapidly to address the demanding requirements driven by customers, regulations and safety. More complex designs and an increasing number of design options must be evaluated whilst controlling the cost and length of the design cycle. The engineering requirements come from different disciplines, such as aerodynamics, structures and electromagnetics. The division of engineering into separate disciplines is not recognised by nature, with the result that addressing requirements within one discipline can compromise requirements in other disciplines. Product complexity is now such that requirements from multiple engineering disciplines need to be simultaneously addressed at an early stage of the design cycle to avoid costly re-design. These problems have placed strong demands on computational design, driving key trends in numerically intensive modelling, as well as the requirement for more computational resource. The requirement for more computational resource arises in many disciplines and is compounded by the need for multi-disciplinary design. This results from the increased product complexity which has to be matched by increasing the mathematical complexity of the model providing the underlying framework for the computational design tools. The availability of increased computational resource via parallel platforms at affordable prices enables these problems to be addressed facilitating •• computational analyses of more complex problems•• simultaneous computational analyses of multiple design and planning options•• multi-disciplinary analyses, i.e. simultaneously address requirements in different disciplines. The continual growth in computational design and computational resource provides the potential for the end-user engineer to carry out far more design more rapidly and accurately. However this potential can only be exploited if the end-user can efficiently define more complex problems, multiple problems, multiisciplinary problems and also analyse the large volume of results. Consequently considerable effort needs to be placed on the user environment, encompassing problem set-up and results analysis. This paper expands on these requirements within the aerospace and related sectors with examples of applications from specific disciplines. Some emphasis is attached to the growing supercomputer disciplines, computational electromagnetics in particular. Some key features of parallel processing are discussed including •• the high degree of parallelism required to get a modest percentage of peak performance•• exploitation of parallel platforms without Teraflop performance•• the potential benefits of hybrid or heterogeneous platforms compared to homogeneous platforms. These are key issues affecting the exploitation of parallel platforms and the success of parallel processing in simulation and design in general. The presentation includes examples on colour viewfoils that are not included in the paper.}
}
@incollection{KYLEMCKAY2019213,
title = {Visualization as a Tool for Ecological Analysis},
editor = {Brian Fath},
booktitle = {Encyclopedia of Ecology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {213-220},
year = {2019},
isbn = {978-0-444-64130-4},
doi = {https://doi.org/10.1016/B978-0-12-409548-9.10566-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095489105664},
author = {S. {Kyle McKay}},
keywords = {Big data, Communication, Data visualization, Infographic, Method selection, Model results, Plotting, Spatial data, Time series, Visual analytics},
abstract = {In the spirit of understanding complex interactions between biotic and abiotic systems, ecologists use countless tools and methods to collect, store, analyze, model, and share data. Although often underemphasized, visualization is a crucial step in translating data into ecological understanding and knowledge both within the discipline and externally to other scientists, stakeholders, decision-makers, and citizens. Furthermore, the need for effective visualization only increases as computational power grows, sensors collect higher resolution data, and novel forms of data collection emerge. While never a substitute for rigorous analysis, visual exploration of data can identify patterns not apparent from purely empirical or numerical approaches, guide quantitative analyses, and effectively communicate findings. This article highlights the value of visualization in ecological analysis and synthesis by presenting case studies relative to four common applications: data exploration, experimental analysis, numerical model output and evaluation, and ecological decision-making. The article concludes with a set of questions to guide ecologists in the selection and application of a visualization approach. The fields of visual analytics, information visualization, computer graphics, and scientific communication provide a rich body of literature, and this article serves only as an entry point for uncovering the seemingly endless body of visualization approaches.}
}
@article{JU201761,
title = {On the propagation limits and speeds of premixed cool flames at elevated pressures},
journal = {Combustion and Flame},
volume = {178},
pages = {61-69},
year = {2017},
issn = {0010-2180},
doi = {https://doi.org/10.1016/j.combustflame.2017.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0010218017300068},
author = {Yiguang Ju},
keywords = {Cool flames, High pressure, Dilution, Heat loss, Flammability limit, Flame speed},
abstract = {The flame speeds and propagation limits of premixed cool flames at elevated pressures are numerically modeled using dimethyl ether mixtures. The primary focus is paid on the effects of pressure, mixture dilution, computation domain, and heat loss on cool flame propagation. The results showed that cool flames exist on both fuel lean and fuel rich sides and dramatically extend the lean and rich flammability limits of conventional hot flames. There exist three different flame regimes: the hot flames, lean and rich cool flames, and double flames. A new flame flammability diagram including both cool flames and hot flames at elevated pressures is obtained. The results show that pressure significantly changes cool flame propagation and burning limits. It is found that the increase of pressure affects the propagation speeds of lean and rich cool flames differently due to the negative temperature coefficient effect. On the lean side, the increase of pressure accelerates the cool flame chemistry and shifts the transition limit of cool flame to hot flame to a lower equivalence ratio. At lower pressure, there is an extinction transition from hot flame to cool flame. However, above a critical pressure, the hot flame directly transfers to a cool flame without hot flame extinction. Moreover, increases in dilution reduce the heat release of the hot flame and promote cool flame formation. Furthermore, the results show that a smaller downstream computation domain and higher heat loss also extend the cool flame transition limit and promote cool flame formation.}
}
@article{RUPERT200876,
title = {Frege’s puzzle and Frege cases: Defending a quasi-syntactic solution},
journal = {Cognitive Systems Research},
volume = {9},
number = {1},
pages = {76-91},
year = {2008},
note = {Perspectives on Social Cognition},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2007.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S1389041707000319},
author = {Robert D. Rupert},
keywords = {Mental content, Concept acquisition, Mental representation, Frege’s puzzle, Frege cases, Symbol grounding, Language-learning},
abstract = {When a subject acquires a concept, one of her cognitive vehicles comes into an appropriate causal or informational relation to whatever that concept is a concept of. Social interaction helps in significant ways to ground this relation. I expound, then apply this perspective to a philosophical problem concerning conceptual content: Frege’s puzzle. The socially interactive processes of language-learning and concept acquisition depend heavily on the mastery of reliable inferences involving the terms learned and concepts acquired. As a side effect, we are inclined to think that patterns of inferential relations constitute content itself. Thus, we are inclined to think that a subject’s differing ways of thinking about the same object—i.e., her possession of two cognitive vehicles that refer to the same object but which participate in different patterns of subjectively drawn inferences—correspond to differences in mental, or conceptual, content. I argue that this is an illusion, an understandable one caused by the difficulty of language-learning and concept acquisition and the concomitant need to rely on inferential patterns to get ourselves into the appropriate causal and informational relations to the things represented by our thoughts and words. The illusion is strengthened, I claim, by the way in which subjects acquire the very concept of content.}
}
@article{WANG2014276,
title = {On-line distributed prediction of traffic flow in a large-scale road network},
journal = {Simulation Modelling Practice and Theory},
volume = {47},
pages = {276-303},
year = {2014},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2014.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X14001051},
author = {Yubin Wang and Jan H. {van Schuppen} and Jos Vrancken},
keywords = {Traffic simulation, Parallel simulation, Distributed simulation},
abstract = {For on-line traffic control at traffic control centers there is a need for fast computations of predictions of traffic flow over a short prediction horizon, say 30min, to evaluate the impact of different scenarios for the purpose of on-line scenario selection. A novel approach is presented to predict the traffic flow in a large-scale traffic network in an asynchronous, parallel, and distributed way at two or more subnetworks combined with a consistency check at the network level within a reasonable-small computation time.}
}
@article{TARANU2019123,
title = {Children’s perception of visual and auditory ambiguity and its link to executive functions and creativity},
journal = {Journal of Experimental Child Psychology},
volume = {184},
pages = {123-138},
year = {2019},
issn = {0022-0965},
doi = {https://doi.org/10.1016/j.jecp.2019.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0022096518305617},
author = {Mihaela Taranu and Marina C. Wimmer and Josephine Ross and Dávid Farkas and Raymond {van Ee} and István Winkler and Susan L. Denham},
keywords = {Perceptual bistability, Visual bistability, Auditory bistability, Perceptual switching, Executive functions, Creativity},
abstract = {The phenomenon of perceptual bistability provides insights into aspects of perceptual processing not normally accessible to everyday experience. However, most experiments have been conducted in adults, and it is not clear to what extent key aspects of perceptual switching change through development. The current research examined the ability of 6-, 8-, and 10-year-old children (N = 66) to switch between competing percepts of ambiguous visual and auditory stimuli and links between switching rate, executive functions, and creativity. The numbers of switches participants reported in two visual tasks (ambiguous figure and ambiguous structure from motion) and two auditory tasks (verbal transformation and auditory streaming) were measured in three 60-s blocks. In addition, inhibitory control was measured with a Stroop task, set shifting was measured with a verbal fluency task, and creativity was measured with a divergent thinking task. The numbers of perceptual switches increased in all four tasks from 6 to 10 years of age but differed across tasks in that they were higher in the verbal transformation and ambigous structure-from-motion tasks than in the ambigous figure and auditory streaming tasks for all age groups. Although perceptual switching rates differed across tasks, there were predictive relationships between switching rates in some tasks. However, little evidence for the influence of central processes on perceptual switching was found. Overall, the results support the notion that perceptual switching is largely modality and task specific and that this property is already evident when perceptual switching emerges.}
}
@incollection{LOFTUS202581,
title = {Chapter 8 - A brief review of R},
editor = {Stephen C. Loftus},
booktitle = {An Introductory Handbook of Bayesian Thinking},
publisher = {Academic Press},
pages = {81-92},
year = {2025},
isbn = {978-0-323-95459-4},
doi = {https://doi.org/10.1016/B978-0-32-395459-4.00016-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323954594000169},
author = {Stephen C. Loftus},
keywords = {R, Statistical Computation, Statistical Programming Language},
abstract = {Computation is an essential part of Bayesian Statistics. There are a number of languages that can accomplish statistical computation, but in this chapter we'll focus on providing a brief introduction to R. This will be the software package and language that we focus on going forward.}
}
@incollection{SRIKANTH202239,
title = {Chapter 3 - Complexity science for urban solutions},
editor = {Imdat As and Prithwish Basu and Pratap Talwar},
booktitle = {Artificial Intelligence in Urban Planning and Design},
publisher = {Elsevier},
pages = {39-58},
year = {2022},
isbn = {978-0-12-823941-4},
doi = {https://doi.org/10.1016/B978-0-12-823941-4.00017-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128239414000172},
author = {Anjanaa Devi Sinthalapadi Srikanth and Benny Chin Wei Chien and Roland Bouffanais and Thomas Schroepfer},
keywords = {Artificial intelligence, Complexity science, Network analysis, Urban design, Urban planning},
abstract = {Cities today exhibit three key characteristics: complexity, diversity, and intelligence. Attempting to increase cities’ resilience in view of our current climate emergency means turning away from simplistic top-down solutions toward more holistic and interdisciplinary practices that thoughtfully integrate informed top-down and bottom-up planning and design processes. In this chapter, we describe a new complexity science-based approach to the understanding of the dynamics, growth, and evolution of cities in a scientifically predictable, quantitative way. We discuss innovative AI-aided urban planning and design methods and tools and how these have been and can be applied in the future. We further describe common types of spatial networks as well as computational social science, its application to urban planning and design problems and how the resulting insights into the dynamics of our cities allow us to uncover and understand their underlying structure.}
}
@article{FETSCH201616,
title = {The importance of task design and behavioral control for understanding the neural basis of cognitive functions},
journal = {Current Opinion in Neurobiology},
volume = {37},
pages = {16-22},
year = {2016},
note = {Neurobiology of cognitive behavior},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2015.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0959438815001804},
author = {Christopher R Fetsch},
abstract = {The success of systems neuroscience depends on the ability to forge quantitative links between neural activity and behavior. Traditionally, this process has benefited from the rigorous development and testing of hypotheses using tools derived from classical psychophysics and computational motor control. As our capacity for measuring neural activity improves, accompanied by powerful new analysis strategies, it seems prudent to remember what these traditional approaches have to offer. Here I present a perspective on the merits of principled task design and tight behavioral control, along with some words of caution about interpretation in unguided, large-scale neural recording studies. I argue that a judicious combination of new and old approaches is the best way to advance our understanding of higher brain function in health and disease.}
}
@article{ELVEVAG2007304,
title = {Quantifying incoherence in speech: An automated methodology and novel application to schizophrenia},
journal = {Schizophrenia Research},
volume = {93},
number = {1},
pages = {304-316},
year = {2007},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2007.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S092099640700117X},
author = {Brita Elvevåg and Peter W. Foltz and Daniel R. Weinberger and Terry E. Goldberg},
keywords = {Psychosis, Language, Semantic, Thought disorder},
abstract = {Incoherent discourse, with a disjointed flow of ideas, is a cardinal symptom in several psychiatric and neurological conditions. However, measuring incoherence has often been complex and subjective. We sought to validate an objective, intrinsically reliable, computational approach to quantifying speech incoherence. Patients with schizophrenia and healthy control volunteers were administered a variety of language tasks. The speech generated was transcribed and the coherence computed using Latent Semantic Analysis (LSA). The discourse was also analyzed with a standard clinical measure of thought disorder. In word association and generation tasks LSA derived coherence scores were sensitive to differences between patients and controls, and correlated with clinical measures of thought disorder. In speech samples LSA could be used to localize where in sentence production incoherence occurs, predict levels of incoherence as well as whether discourse “belonged” to a patient or control. In conclusion, LSA can be used to assay disordered language production so as to both complement human clinical ratings as well as experimentally parse this incoherence in a theory-driven manner.}
}