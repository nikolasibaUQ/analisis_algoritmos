@article{BRUNDAGE201532,
title = {Taking superintelligence seriously: Superintelligence: Paths, dangers, strategies by Nick Bostrom (Oxford University Press, 2014)},
journal = {Futures},
volume = {72},
pages = {32-35},
year = {2015},
note = {Confronting Future Catastrophic Threats To Humanity},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2015.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0016328715000932},
author = {Miles Brundage},
keywords = {Existential risk, Artificial intelligence, Superintelligence, Responsible innovation},
abstract = {A new book by Nick Bostrom, Superintelligence: Paths, Dangers, Strategies, is reviewed. Superintelligence explores the future of artificial intelligence and related technologies and the risks they may pose to human civilization. The book ably demonstrates the potential for serious thinking aimed at the long-term future. Bostrom succeeds in arguing that the development of superintelligent machines will, if not properly managed, create catastrophic risks to humanity. The book falls short in some respects, and some sections are more compelling and novel than others. Overall, however, Bostrom’s book succeeds in demolishing the “null hypothesis” according to which the possibility and risks of superintelligence can continue to be ignored, and is a must-read for those interested in the long-term future of humanity.}
}
@article{LEHRER200039,
title = {Developing Model-Based Reasoning in Mathematics and Science},
journal = {Journal of Applied Developmental Psychology},
volume = {21},
number = {1},
pages = {39-48},
year = {2000},
issn = {0193-3973},
doi = {https://doi.org/10.1016/S0193-3973(99)00049-0},
url = {https://www.sciencedirect.com/science/article/pii/S0193397399000490},
author = {Richard Lehrer and Leona Schauble},
abstract = {It is essential to base instruction on a foundation of understanding of children's thinking, but it is equally important to adopt the longer-term view that is needed to stretch these early competencies into forms of thinking that are complex, multifaceted, and subject to development over years, rather than weeks or months. We pursue this topic through our studies of model-based reasoning. We have identified four forms of models and related modeling practices that show promise for developing model-based reasoning. Models have the fortuitous feature of making forms of student reasoning public and inspectable—not only among the community of modelers, but also to teachers. Modeling provides feedback about student thinking that can guide teaching decisions, an important dividend for improving professional practice.}
}
@incollection{PERKINS2002187,
title = {Standard logic as a model of reasoning: The empirical critique},
editor = {Dov M. Gabbay and Ralph H. Johnson and Hans Jürgen Ohlbach and John Woods},
series = {Studies in Logic and Practical Reasoning},
publisher = {Elsevier},
volume = {1},
pages = {187-223},
year = {2002},
booktitle = {Handbook of the Logic of Argument and Inference},
issn = {1570-2464},
doi = {https://doi.org/10.1016/S1570-2464(02)80007-6},
url = {https://www.sciencedirect.com/science/article/pii/S1570246402800076},
author = {David N. Perkins},
abstract = {Publisher Summary
This chapter describes standard logic as a model of reasoning. The notion of formal logic has figured centrally in conceptions of human reasoning, rationality, and adaptiveness. The chapter reviews the evidence, appraises its weight, and offers a summative judgment of the place of logic in human thinking. "Standard logic," includes the canons of formal deduction, the special case of disconfirming hypotheses by finding counterevidence for their implications, and also the principles of probabilistic and statistical inference developed by mathematicians over the past couple of hundred years. It also examines deliberate or reflexive reasoning. The chapter argues that standard logic or subsets of it can be implemented in quite different ways and that human cognition incorporates more than one implementation. In addition, almost all the research on the role of standard logic in human thinking concerns deliberate rather than reflexive reasoning. Accordingly, the present analysis focuses on deliberate reasoning and the place of standard logic in it.}
}
@article{HONG2024422,
title = {AF-FTTSnet: An end-to-end two-stream convolutional neural network for online quality monitoring of robotic welding},
journal = {Journal of Manufacturing Systems},
volume = {74},
pages = {422-434},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524000724},
author = {Yuxiang Hong and Xingxing He and Jing Xu and Ruiling Yuan and Kai Lin and Baohua Chang and Dong Du},
keywords = {Welding quality monitoring, Visual sensing, Molten pool, Defect prediction, Two-stream network},
abstract = {Online welding quality monitoring (WQM) is crucial for intelligent welding, and deep learning approaches considering spatiotemporal features for WQM tasks show great potential. However, one of the important challenges for existing approaches is to balance the spatiotemporal representation learning capability and computational efficiency, which makes it challenging to adapt welding processes with complex and drastic molten pool dynamic behavior. This paper proposes a novel approach for WQM using molten pool visual sensing and deep learning considering spatiotemporal features, the proposed deep learning network called attention fusion based frame-temporality two-stream network (AF-FTTSnet). Firstly, a passive vision sensor is used to acquire continuous dynamic molten pool images. Meanwhile, temporal difference images are computed to provide novel features and temporal representations. Then, a two-stream feature extraction module is designed to concurrently extract rich spatiotemporal features from molten pool images and temporal difference images. Finally, an attention fusion module with the ability to automatically identify and weight the most relevant features is designed to achieve optimal fusion of the two-stream features. The shop welding experimental results indicate that the proposed AF-FTTSnet model can effectively and robustly recognize five typical welding states during helium arc welding, with an accuracy of 99.26%. This model has been demonstrated to exhibit significant performance improvements compared to mainstream temporal sequence models. Available: https://github.com/Just199806/TSCNN/tree/master.}
}
@article{SOSA201656,
title = {Visual divergence in humans and computers},
journal = {Design Studies},
volume = {42},
pages = {56-85},
year = {2016},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2015.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X15000836},
author = {Ricardo Sosa and Nicolas Rojas and John S. Gero and Qinqi Xu},
keywords = {creativity, sketching, computer models, solution space},
abstract = {Studies of design creativity have underlined the importance of divergent reasoning and visual reasoning in idea generation. Connecting these two key design skills, this paper presents a model of divergent visual reasoning for the study of creativity. A visual divergence task called ShapeStorm is demonstrated for the study of creative ideation that can be applied to humans as well as computational systems. The model is examined in a study with human subjects, a computational stochastic generator, and a geometrical analysis of the solution space. The main significance of this task is that it offers a straightforward means to define a simple design task that can be used across research studies. Several scenarios for the application of ShapeStorm for the study of creativity are advanced.}
}
@article{LAI2023101343,
title = {Optimization of urban and rural ecological spatial planning based on deep learning under the concept of sustainable development},
journal = {Results in Engineering},
volume = {19},
pages = {101343},
year = {2023},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2023.101343},
url = {https://www.sciencedirect.com/science/article/pii/S259012302300470X},
author = {Yilin Lai},
keywords = {Sustainable development, Spatial planning, Remote sensing images, CNN, GPU},
abstract = {At present, the speed of urbanization in China is constantly accelerating. At the same time, due to the severe situation of tight resource constraints, severe environmental pollution, and ecosystem degradation, vigorously promoting the construction of ecological civilization has become a key planning direction. However, traditional urban and rural ecological spatial planning is influenced by factors such as region, terrain, and spatial scale, which cannot adapt to the current spatial planning requirements. To achieve sustainable urban and rural ecological spatial planning, we propose a method that uses the optimized remote sensing images and convolutional neural networks to achieve spatial planning. In the analysis of the application effect of the usage method, the experimental results show that increasing the amount of data such as image size can improve the execution performance of the computer when the computer is not fully utilizing its resources and its computational volume fails to saturate the computational capacity. The parallel configuration designed in this experiment can accelerate the performance of the computer better, and the acceleration effect becomes more obvious as the difficulty of the algorithm increases. The Faster RCNN algorithm proposed in this experiment has the highest retrieval accuracy in the Flickr30K dataset and MS-COCO dataset compared with other algorithms. In Flickr30k data set, compared with other models in the table, the model used in this paper has the highest retrieval accuracy. The retrieval accuracy of R@1, R@5, R@10 increased by 23.1%, 8.1% and 5.3%, respectively. In MS-COCO data set, the retrieval accuracy increased by 19.2%, 13.1% and 8.3% respectively. The above results confirm that the combination of remote sensing images and convolutional neural network technology can perform simple ecological planning of a city's urban and rural areas, which proves that the method proposed in this experiment has practicality.}
}
@article{DEY2016177,
title = {A probabilistic approach to diagnose faults of air handling units in buildings},
journal = {Energy and Buildings},
volume = {130},
pages = {177-187},
year = {2016},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2016.08.017},
url = {https://www.sciencedirect.com/science/article/pii/S0378778816306958},
author = {Debashis Dey and Bing Dong},
keywords = {Air Handling Unit, Bayesian belief network, APAR rules, Fault detection and diagnosis},
abstract = {Air handling unit (AHU) is one of the most extensively used equipment in large commercial buildings. This device is typically customized and lacks quality system integration which can result in hardwire failures and control errors. Air handling unit Performance Assessment Rules (APAR) is a fault detection tool that uses a set of expert rules derived from mass and energy balances to detect faults in air handling units. APAR is computationally simple enough that it can be embedded in commercial building automation and control systems and relies only upon sensor data and control signals that are commonly available in these systems. Although APAR has advantages over other methods, for example no training data required and easy to implement commercially, most of the time it is unable to provide the root diagnosis of the faults. For instance, a fault on temperature sensor could be bias, drifting bias, inappropriate location, or complete failure. In addition a fault in mixing box can be return and/or outdoor damper leak or stuck. In addition, when multiple rules are satisfied, the list of faults increases. There is no proper way to have the correct diagnosis for rule based fault detection system. To overcome this limitation, we proposed Bayesian Belief Network (BBN) as a diagnostic tool. BBN can be used to simulate diagnostic thinking of FDD experts through a probabilistic way. In this study we developed a new way to detect and diagnose faults in AHU through combining APAR rules and Bayesian Belief network. Bayesian Belief Network is used as a decision support tool for rule based expert system. BBN is highly capable to prioritize faults when multiple rules are satisfied simultaneously. Also it can get information from previous AHU operating conditions and maintenance records to provide proper diagnosis. The proposed model is validated with real time measured data of a campus building. The results show that BBN correctly prioritize faults that are verified by manual investigation.}
}
@incollection{2007229,
title = {Chapter 6 - Computational Methods for Optimal Filtering of Stochastic Signals},
editor = {A. Torokhti and P. Howlett},
series = {Mathematics in Science and Engineering},
publisher = {Elsevier},
volume = {212},
pages = {229-290},
year = {2007},
booktitle = {Computational Methods for Modelling of Nonlinear Systems},
issn = {0076-5392},
doi = {https://doi.org/10.1016/S0076-5392(07)80049-X},
url = {https://www.sciencedirect.com/science/article/pii/S007653920780049X}
}
@article{KNYAZEV201817,
title = {Resting state connectivity mediates the relationship between collectivism and social cognition},
journal = {International Journal of Psychophysiology},
volume = {123},
pages = {17-24},
year = {2018},
issn = {0167-8760},
doi = {https://doi.org/10.1016/j.ijpsycho.2017.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167876017305470},
author = {Gennady G. Knyazev and Alexander N. Savostyanov and Andrey V. Bocharov and Ekaterina A. Merkulova},
keywords = {Collectivism, Social cognition, Medial prefrontal cortex, Connectivity, Mediation analysis},
abstract = {Humans are intrinsically social beings and it is natural that self-processing is associated with social cognition. The degree to which the self is perceived as a part of social environment is modulated by cultural stereotypes, such as collectivism and individualism. Here, we tested the hypothesis that individuals who endorse collectivist values would spontaneously think more about their relationships with other people and this association would be mediated by connectivity between the medial prefrontal cortex (MPFC) and the rest of the brain. Connectivity was evaluated based on resting state EEG data using the recently developed methods, which combine beamformer spatial filtering with seed based connectivity estimation. The formal mediation analysis revealed that collectivism is associated with an enhanced connectivity of MPFC with a set of cortical regions that are frequently co-activated in moral reasoning, empathy, and theory of mind tasks and with diminished connectivity with the precuneus\posterior cingulate cortex, which is involved in self-centered cognition. The relationship between collectivism and social cognition was mediated by MPFC connectivity with the left middle temporal gyrus implying that in participants with collectivistic attitude, thinking about relationships with other people may be associated with semantic memory retrieval and reasoning on moral issues and others' intentions.}
}
@article{COOK201895,
title = {An investigation of an undergraduate student’s reasoning with zero-divisors and the zero-product property},
journal = {The Journal of Mathematical Behavior},
volume = {49},
pages = {95-115},
year = {2018},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2017.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732312317301748},
author = {John Paul Cook},
keywords = {Abstract algebra, Zero-product property, Zero-divisors, Equation solving, Student thinking, Realistic Mathematics Education},
abstract = {The zero-product property (ZPP), often stated as ‘if ab = 0, then a = 0 or b = 0,’ is an important concept in secondary algebra (as a tool for solving equations) and abstract algebra (as a property of integral domains). This study analyzes results from a teaching experiment to investigate how an undergraduate mathematics major might intuitively reason with zero-divisors and the ZPP. There are two primary findings. First, a procedurally embodied view of equation solving might preclude students’ attention to the algebraic properties (including the ZPP) that justify the equivalence of two equations. Second, students might not carefully attend to zero-divisors because they are employing the converse of the ZPP instead of the ZPP itself. These findings advance a hypothesis about why students might view abstract algebra as a different subject than school algebra and also affirm the utility of the student-centered theoretical perspective that guided the instructional design and analysis of student activity.}
}
@article{ADAMO2024109162,
title = {Crop planting layout optimization in sustainable agriculture: A constraint programming approach},
journal = {Computers and Electronics in Agriculture},
volume = {224},
pages = {109162},
year = {2024},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2024.109162},
url = {https://www.sciencedirect.com/science/article/pii/S0168169924005532},
author = {Tommaso Adamo and Lucio Colizzi and Giovanni Dimauro and Emanuela Guerriero and Deborah Pareo},
keywords = {Constraint programming, Optimization crop planting layout, AI planning, Smart Agriculture, Intercropping systems},
abstract = {In sustainable agriculture, intercropping systems represent a valuable approach. These systems involve placing mutually beneficial plant types in close proximity to each other, with the goal of exploiting biodiversity to reduce pesticide and water usage, as well as improve soil nutrient utilization. Despite its potential, the optimization of intercropping systems has received limited attention in previous studies. One of the first steps in the design of an intercropping system is the solution of the crop planting layout problem, which involves meeting crop demand while maximizing positive interactions between adjacent plants. We perform a complexity analysis of this problem and solve it through constraint programming, an artificial intelligence technique, which relies on automated reasoning, constraint propagation and search heuristics. To this aim, we present two constraint programming models based on integer variables and interval variables, respectively. Through a computational study on real-life instances, we examine the impact of different modelling approaches on the difficulty of solving the crop planting layout problem with standard constraint programming solvers. This research work has also provided the groundwork for a sowing robotic arm (under development), aiming to automate intercropping systems and assist farm workers.}
}
@article{VONRICHTHOFEN2018573,
title = {The ‘Urban Elements’ method for teaching parametric urban design to professionals},
journal = {Frontiers of Architectural Research},
volume = {7},
number = {4},
pages = {573-587},
year = {2018},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2018.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S209526351830044X},
author = {Aurel {von Richthofen} and Katja Knecht and Yufan Miao and Reinhard König},
keywords = {Urban design education, Parametric urban design, Singapore, Urban Elements},
abstract = {The article proposes a method for teaching advanced urban design to working professionals in Singapore. The article aims to expand the discourse on parametric urban design education by introducing ‘Urban Elements’ as conceptual urban design instruments with an inherent rule-based logic, which can help to bridge gaps in teaching parametric urban design thinking. As case study we present a course developed for and delivered to the Urban Redevelopment Authority (URA) in Singapore in 2017 by the Future Cities Laboratory at the Singapore-ETH Centre. The article reports on the pedagogical method, course results and course feedback. The main difficulties of teaching professionals in parametric urban design are described and possible reasons and improvements are discussed. The results show that participants using the ‘Urban Elements’ method successfully linked theoretical input to urban design problems, applied evidence-based urban design strategies to these problems, and developed parametric definitions to explore the solution spaces of these urban design challenges. The teaching methodology presented opens up a new research field for urban design pedagogy at the intersection of explicating urban design intent, integrating multidisciplinary knowledge and exploring new software driven tools.}
}
@article{BIALEK19901227,
title = {Temporal filtering in retinal bipolar cells. Elements of an optimal computation?},
journal = {Biophysical Journal},
volume = {58},
number = {5},
pages = {1227-1233},
year = {1990},
issn = {0006-3495},
doi = {https://doi.org/10.1016/S0006-3495(90)82463-2},
url = {https://www.sciencedirect.com/science/article/pii/S0006349590824632},
author = {W. Bialek and W.G. Owen},
abstract = {Recent experiments indicate that the dark-adapted vertebrate visual system can count photons with a reliability limited by dark noise in the rod photoreceptors themselves. This suggests that subsequent layers of the retina, responsible for signal processing, add little if any excess noise and extract all the available information. Given the signal and noise characteristics of the photoreceptors, what is the structure of such an optimal processor? We show that optimal estimates of time-varying light intensity can be accomplished by a two-stage filter, and we suggest that the first stage should be identified with the filtering which occurs at the first anatomical stage in retinal signal processing, signal transfer from the rod photoreceptor to the bipolar cell. This leads to parameter-free predictions of the bipolar cell response, which are in excellent agreement with experiments comparing rod and bipolar cell dynamics in the same retina. As far as we know this is the first case in which the computationally significant dynamics of a neuron could be predicted rather than modeled.}
}
@article{CHEN2024e26409,
title = {Physiological records-based situation awareness evaluation under aviation context: A comparative analysis},
journal = {Heliyon},
volume = {10},
number = {5},
pages = {e26409},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e26409},
url = {https://www.sciencedirect.com/science/article/pii/S240584402402440X},
author = {Jun Chen and Anqi Chen and Bingkun Jiang and Xinyu Zhang},
keywords = {Situation awareness, Electroencephalogram, Brain electrical activity mapping, Convolutional neural network, Multi-class classification, Aviation decision-making},
abstract = {Situational Awareness (SA) assessment is of paramount importance in various domains, with particular significance in the military for safe aviation decision-making. It involves encompassing perception, comprehension, and projection levels in human beings. Accurate evaluation of SA statuses across these three levels is crucial for mitigating human false-positive and false-negative rates in monitoring complex scenarios in the aviation context. This study proposes a comprehensive comparative analysis by involving two types of physiological records: electroencephalogram (EEG) signals and brain electrical activity mapping (BEAM) images. These two modalities are leveraged to automate precise SA evaluation using both conventional machine learning and advanced deep learning techniques. Benchmarking experiments reveal that the BEAM-based deep learning models attain state-of-the-art performance scores of 0.955 for both SA perception and comprehension levels, respectively. Conversely, the EEG signals-based manual feature extraction, selection, and classification approach achieved a superior accuracy of 0.929 for the projection level of SA. These findings collectively highlight the potential of deploying diverse physiological records as valuable computational tools for enhancing SA evaluation throughout aviation decision-making safety.}
}
@article{KEMPF2023103747,
title = {Point pattern and spatial analyses using archaeological and environmental data – A case study from the Neolithic Carpathian Basin},
journal = {Journal of Archaeological Science: Reports},
volume = {47},
pages = {103747},
year = {2023},
issn = {2352-409X},
doi = {https://doi.org/10.1016/j.jasrep.2022.103747},
url = {https://www.sciencedirect.com/science/article/pii/S2352409X22004102},
author = {Michael Kempf and Gerrit Günther},
keywords = {Environmental archaeology, Quantitative archaeology, Computational methods, Spatial analysis, R, Point pattern analysis (PPA)},
abstract = {Computational methods recently gained momentum in archaeological science, particularly affecting large site distribution samples and environmental explanatory parameters. However, quantitative and environmental archaeology are still considered to be limited to a small number of experts and thus less ready to use in general research. Here, we present a case study that integrates computational methods and environmental data into archaeological spatial analyses using Point Pattern Analysis (PPA). We introduce a basic approach to model, visualise, and interpret archaeological site distributions as functions of explanatory covariates in a regional setting of the Neolithic period in the Carpathian Basin. The integration of environmental and socio-cultural variables in a multicomponent analysis allows to distinguish site location parameters and preferences across different chronological periods. Using the code to this article and open-access spatial data, the workflow can be adapted to different regional contexts and chronological periods, making it particularly suitable for spatial pattern comparison.}
}
@article{ERDMANN202242,
title = {A generative framework for the study of delusions},
journal = {Schizophrenia Research},
volume = {245},
pages = {42-49},
year = {2022},
note = {Computational Approaches to Understanding Psychosis},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2020.11.048},
url = {https://www.sciencedirect.com/science/article/pii/S0920996420306277},
author = {Tore Erdmann and Christoph Mathys},
keywords = {Delusion, Dirichlet process, Hierarchical predictive coding, Auxiliary hypothesis, Epistemic trust},
abstract = {Despite the ubiquity of delusional information processing in psychopathology and everyday life, formal characterizations of such inferences are lacking. In this article, we propose a generative framework that entails a computational mechanism which, when implemented in a virtual agent and given new information, generates belief updates (i.e., inferences about the hidden causes of the information) that resemble those seen in individuals with delusions. We introduce a particular form of Dirichlet process mixture model with a sampling-based Bayesian inference algorithm. This procedure, depending on the setting of a single parameter, preferentially generates highly precise (i.e. over-fitting) explanations, which are compartmentalized and thus can co-exist despite being inconsistent with each other. Especially in ambiguous situations, this can provide the seed for delusional ideation. Further, we show by simulation how the excessive generation of such over-precise explanations leads to new information being integrated in a way that does not lead to a revision of established beliefs. In all configurations, whether delusional or not, the inference generated by our algorithm corresponds to Bayesian inference. Furthermore, the algorithm is fully compatible with hierarchical predictive coding. By virtue of these properties, the proposed model provides a basis for the empirical study and a step toward the characterization of the aberrant inferential processes underlying delusions.}
}
@article{LEUKHIN2018166,
title = {Bio-plausible simulation of three monoamine systems to replicate emotional phenomena in a machine},
journal = {Biologically Inspired Cognitive Architectures},
volume = {26},
pages = {166-173},
year = {2018},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2018.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X1830152X},
author = {Alexey Leukhin and Max Talanov and Jordi Vallverdú and Fail Gafarov},
keywords = {Affective computing, Affective computation, Spiking neural networks, Bio-inspired cognitive architecture},
abstract = {In this paper we present the validation of the three-dimensional model of emotions by Hugo Lövheim the “cube of emotion” via neurosimulation in the NEST. We also present the extension of original “cube of emotion” with the bridge to computational processes parameters. The neurosimulation is done via re-implementation of DA, 5-HT and NA subsystems of a rat brain to replicate 8 basic psycho-emotional states according to the “cube of emotion”. Results of neurosimulations indicate the incremental influence of DA and NA over computational resources of a psycho-emotional state while 5-HT decreases the computational resources used to calculate a psycho-emotional state. This way we indicate the feasibility of the bio-plausible re-implementation of psycho-emotional states in a computational system. This approach could be useful extension of decision making and load balancing components of modern artificial agents as well as intelligent robotic systems.}
}
@article{TONKS2021102036,
title = {How situational competence beliefs and task value relate to inference strategies and comprehension during reading},
journal = {Learning and Individual Differences},
volume = {90},
pages = {102036},
year = {2021},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2021.102036},
url = {https://www.sciencedirect.com/science/article/pii/S104160802100073X},
author = {Stephen M. Tonks and Joseph P. Magliano and John Schwartz and Ryan D. Kopatich},
keywords = {Reading motivation, Inference generation, Reading comprehension, College students},
abstract = {In two studies, we explored the associations among situational reading-related competence beliefs and task value, inference strategies, comprehension during reading, and foundational skills in college age students. In Study 1, 93 participants from a community college completed assessments of comprehension and two types of inference strategies (elaboration and bridging), each immediately followed by a survey of their competence beliefs and task value regarding the task. Results showed that competence beliefs and task value related positively to reading comprehension. In addition, task value was positively associated with both elaborating and bridging inferences, and competence beliefs correlated positively with bridging inferences. In Study 2, we investigated these associations further in a group of 418 students studying at three different colleges. Participants completed the same assessments for competence beliefs, task value, and inference strategies, as well as assessments of comprehension and foundational reading skills. Study 2 analyses revealed that foundational reading skills were a strong predictor of both types of inferencing and also comprehension. Further, when controlling for foundational reading skills, task value predicted elaboration and bridging inferences, whereas competence beliefs did not predict inferencing, but were trending as a predictor of comprehension. Finally, we created a path model to explore mediational effects, and found that task value positively predicted comprehension performance through increased elaborations while thinking aloud.}
}
@article{XHAXHIU2024,
title = {Seaweed boards as value-added natural waste product for insulation and building materials},
journal = {Energy Storage and Saving},
year = {2024},
issn = {2772-6835},
doi = {https://doi.org/10.1016/j.enss.2024.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S2772683524000359},
author = {Kledi Xhaxhiu and Avni Berisha and Nensi Isak and Besnik Baraj and Adelaida Andoni},
keywords = {Seaweed, Natural waste, Waste recycling, Building material, Insulation, Thermal and mechanical properties calculations},
abstract = {Large amounts of seaweed are deposited on shores worldwide daily. The presence of this natural pollutant on the coast is not only considered an environmental burden but also often hinders the development of tourism in the affected areas. Depending on the beach surface area, local governments worldwide spend considerable portions of their budgets to remove seaweed from beaches. Moreover, the removed seaweed occupies increasing space in landfills where it is disposed. Seaweed is noncombustible and decomposes slowly over long periods. In this study, we consider the use of seaweed (a natural waste) as a value-added product for insulation and building materials. Seaweed (Posidonia Oceanica) boards with dimensions of 250 mm × 60 mm × 10 mm were obtained by pressing a mixture of processed seaweed and an organic binder. The as-prepared boards were analyzed for their physical–mechanical properties according to the British standards. The boards with a mean humidity level of 9.15% and density 0.4045 g·cm−3 demonstrated a maximum bending resistance of 2.720 × 103 N·m−2 and mean expansion upon water adsorption of ∼10% with regards to length and width and ∼30% with regards to height. The tested samples showed significant humidity resistance according to the boiling test and an average thermal conductivity of 0.047 W·mK−1, which is comparable to that of polystyrene. Computational analysis of the “‘seaweed material’” model” revealed significant thermal and mechanical properties. The mechanical strength of the computed material, including its high Young's and shear moduli, renders it a promising candidate in construction.}
}
@article{PAMPLONA2020100189,
title = {An overview of air delay: A case study of the Brazilian scenario},
journal = {Transportation Research Interdisciplinary Perspectives},
volume = {7},
pages = {100189},
year = {2020},
issn = {2590-1982},
doi = {https://doi.org/10.1016/j.trip.2020.100189},
url = {https://www.sciencedirect.com/science/article/pii/S2590198220301007},
author = {Daniel Alberto Pamplona and Claudio Jorge Pinto Alves},
keywords = {Air delay, Air traffic flow, Problem-structuring method, Value-focused thinking},
abstract = {Delay is a key point in air transportation activity. As a performance metric, it affects common policy concerns. Delay impacts passenger satisfaction and imposes costs. The complexity that sets in for the air traffic manager is how to mitigate delay, especially in an environment with several stakeholders. The present article applied a problem-structuring method (PSM), named value-focused thinking (VFT), to structure the problem of the air traffic flow management arrival delay. The inflexibility of incorporating a flight operator's specific needs is considered one of the reasons for the limited success of air traffic flow management (ATFM) programs. PSM allows participants to clarify their dilemmas, converge on a mutually liable problem, or agree to the proposed solutions and compromise on what partially solves the issue. The problem is that most papers focus only on the applied solution for air delay mitigation. Before implementing operational research techniques, we investigated the nature and characteristics of air delay. Results showed that there were several stakeholders with distinctive requirements for their business and many of their objectives are interconnected. The use of VFT provided an objective map that can be used as a guide for future solutions.}
}
@article{DUAN2024101258,
title = {Concept cognition for knowledge graphs: Mining multi-granularity decision rule},
journal = {Cognitive Systems Research},
volume = {87},
pages = {101258},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101258},
url = {https://www.sciencedirect.com/science/article/pii/S1389041724000524},
author = {Jiangli Duan and Guoyin Wang and Xin Hu and Qun Liu and Qin Jiang and Huamin Zhu},
keywords = {Granular computing, Cognitive intelligence, Concept cognition, Knowledge graph, Decision rule},
abstract = {As part of cognitive intelligence, concept cognition for knowledge graphs aims to clearly grasp the typical characteristics of the things referred to by the concept, which can provide prior knowledge for machine understanding and thinking. Different from concept learning and formal concept analysis that learn new concepts from data and the general decision rule that comes from an independent decision table, this paper cognizes an existing concept by decision rules that come from multiple granularities. Specifically, 1) concept cognition for knowledge graphs is realized from the perspective of mining multi-granularity decision rule. 2) Decision tables corresponding to four granularities form a multi-granularity decision table group, and then the result from coarser granularity can guide and help obtaining the result from finer granularity. 3) We propose a framework for mining multi-granularity decision rules, which involves going from a multi-granularity decision table group to the frequent maximal attribute patterns to the decision rules to the credible decision rules. Finally, we verified effectiveness of dividing positive and negative data, monotonicity of attribute patterns in a multi-granularity decision table group, and downward monotonicity of credibility, and observed the impact of the parameter min_cov and min_conf on execution times.}
}
@article{KANCHANATAWAN2018168,
title = {Affective symptoms in schizophrenia are strongly associated with neurocognitive deficits indicating disorders in executive functions, visual memory, attention and social cognition},
journal = {Progress in Neuro-Psychopharmacology and Biological Psychiatry},
volume = {80},
pages = {168-176},
year = {2018},
note = {Peripheral markers of inflammation, oxidative & nitrosative stress pathways and memory functions as a new target of pharmacotherapy in depression},
issn = {0278-5846},
doi = {https://doi.org/10.1016/j.pnpbp.2017.06.031},
url = {https://www.sciencedirect.com/science/article/pii/S027858461730129X},
author = {Buranee Kanchanatawan and Supaksorn Thika and George Anderson and Piotr Galecki and Michael Maes},
keywords = {Major depression, Bipolar, Anxiety, Schizophrenia, CANTAB, Cognition},
abstract = {The aim of this study was to assess the neurocognitive correlates of affective symptoms in schizophrenia. Towards this end, 40 healthy controls and 80 schizophrenia patients were investigated with six tests of the Cambridge Neuropsychological Test Automated Battery (CANTAB), assessing spatial working memory, paired-association learning, one touch stocking, rapid visual information (RVP), emotional recognition test and intra/extradimensional set shifting. The Hamilton Depression (HDRS) and Anxiety (HAMA) Rating Scales and the Calgary Depression Scale for Schizophrenia (CDSS) as well as the Positive and Negative Syndrome Scale (PANSS) were also used. There were highly significant associations between all 6 CANTAB tests and HDRS, HAMA and CDSS (except RVP) scores. The most significant items associating with neurocognitive impairments in schizophrenia were self-depreciation (CDSS), fatigue, psychomotor retardation and agitation, psychic and somatic anxiety (HDRS), fears, cognitive symptoms, somatic-muscular, genito-urinary and autonomic symptoms and anxious behavior (HAMA). The selected HDRS and HAMA symptoms indicate fatigue, fears, anxiety, agitation, retardation, somatization and subjective cognitive complaints (SCC) and are therefore labeled “FAARS”. Up to 28.8% of the variance in the 6 CANTAB measurements was explained by FAARS, which are better predictors of neurocognitive impairments than the PANSS negative subscale score. Neurocognitive deficits in schizophrenia are best predicted by FAARS combined with difficulties in abstract thinking. In conclusion, depression and anxiety symptoms accompanying the negative and positive symptoms of schizophrenia are associated with neurocognitive deficits indicating disorders in executive functions, attention, visual memory, and social cognition. Neurocognitive deficits in schizophrenia reflect difficulties in abstract thinking and FAARS, including subjective cognitive complaints.}
}
@article{HARWOOD201610,
title = {Locking up passwords – for good},
journal = {Network Security},
volume = {2016},
number = {4},
pages = {10-13},
year = {2016},
issn = {1353-4858},
doi = {https://doi.org/10.1016/S1353-4858(16)30037-X},
url = {https://www.sciencedirect.com/science/article/pii/S135348581630037X},
author = {Will Harwood},
abstract = {It's clear that bulk identity thefts – that is, the mass stealing of passwords or other personally identifiable information (PII) – are among the most harmful types of cyber-attack faced by businesses. They're a huge problem, not only in terms of the damage each attack causes, but also the volume of attacks overall. A cursory glance over the business headlines for the past few years announces huge password or PII thefts from organisations ranging from Sony PlayStation to eBay and Facebook to JP Morgan. We were barely a week into 2016 when it was revealed that email passwords for up to 320,000 users had been stolen from Time Warner. Bulk identity thefts are among the most harmful types of cyber-attack faced by businesses today and part of the problem is that businesses, security firms and cyber-criminals all share the same playing field. Thinking beyond standard computing architectures is the only solution to the ongoing arms race between hackers and security vendors. In a battle against cyber-criminality, in which businesses are always playing catch-up, this is a way of getting on the front foot and beginning to operate in a world beyond the attackers' reach, says Dr Will Harwood of Silicon:SAFE.}
}
@article{SHIVERSMCNAIR201836,
title = {User-Centered Design In and Beyond the Classroom: Toward an Accountable Practice},
journal = {Computers and Composition},
volume = {49},
pages = {36-47},
year = {2018},
note = {User-Centered Design and Usability in the Composition Classroom},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2018.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S8755461518300379},
author = {Ann Shivers-McNair and Joy Phillips and Alyse Campbell and Hanh H. Mai and Alice Yan and John Forrest Macy and James Wenlock and Savannah Fry and Yishan Guan},
keywords = {user-centered design, user experience, usability testing, design thinking},
abstract = {The authors, an instructor and students, describe our practice of user-centered design on three levels: in the design and structure of an advanced undergraduate course in which we all participated, in student projects designed during the course, and in our reflections on the course presented here. We argue that principles of user-centered design can and should be more than course concepts and assignments; they can be core practices of the course that hold both students and teachers accountable for the impacts of their rhetorical choices. We offer a model for other teacher-scholars looking to involve students in the design of their courses and in writing together about their work.}
}
@article{HELBING2023102061,
title = {Democracy by Design: Perspectives for Digitally Assisted, Participatory Upgrades of Society},
journal = {Journal of Computational Science},
volume = {71},
pages = {102061},
year = {2023},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2023.102061},
url = {https://www.sciencedirect.com/science/article/pii/S1877750323001217},
author = {Dirk Helbing and Sachit Mahajan and Regula Hänggli Fricker and Andrea Musso and Carina I. Hausladen and Cesare Carissimo and Dino Carpentras and Elisabeth Stockinger and Javier {Argota Sanchez-Vaquerizo} and Joshua C. Yang and Mark C. Ballandies and Marcin Korecki and Rohit K. Dubey and Evangelos Pournaras},
keywords = {Computational diplomacy, Digital democracy, Participation, Collective intelligence, Value-based engineering},
abstract = {The technological revolution, particularly the availability of more data and more powerful computational tools, has led to the emergence of a new scientific field called “Computational Diplomacy”. Our work tries to define its scope and focuses on a popular subarea of it, namely “Digital Democracy”. In recent years, there has been a surge of interest in using digital technologies to promote more participatory forms of democracy. While there are numerous potential benefits to using digital tools to enhance democracy, significant challenges must be addressed. It is essential to ensure that digital technologies are used in an accessible, equitable, and fair manner rather than reinforcing existing power imbalances. This paper investigates how digital tools can be used to help design more democratic societies by investigating three key research areas: (1) the role of digital technologies for facilitating civic engagement in collective decision-making; (2) the use of digital tools to improve transparency and accountability in governance; and (3) the potential for digital technologies to enable the formation of more inclusive and representative democracies. We argue that more research on how digital technologies can be used to support democracy upgrade is needed. Along these lines, we lay out a research agenda for the future.}
}
@article{ZANUY2006330,
title = {Computational Study of the Fibril Organization of Polyglutamine Repeats Reveals a Common Motif Identified in β-Helices},
journal = {Journal of Molecular Biology},
volume = {358},
number = {1},
pages = {330-345},
year = {2006},
issn = {0022-2836},
doi = {https://doi.org/10.1016/j.jmb.2006.01.070},
url = {https://www.sciencedirect.com/science/article/pii/S0022283606001112},
author = {David Zanuy and Kannan Gunasekaran and Arthur M. Lesk and Ruth Nussinov},
keywords = {protofibril conformation, polyglutamine repeats, β-helices, structural analysis, huntingtin protein},
abstract = {The formation of fibril aggregates by long polyglutamine sequences is assumed to play a major role in neurodegenerative diseases such as Huntington. Here, we model peptides rich in glutamine, through a series of molecular dynamics simulations. Starting from a rigid nanotube-like conformation, we have obtained a new conformational template that shares structural features of a tubular helix and of a β-helix conformational organization. Our new model can be described as a super-helical arrangement of flat β-sheet segments linked by planar turns or bends. Interestingly, our comprehensive analysis of the Protein Data Bank reveals that this is a common motif in β-helices (termed β-bend), although it has not been identified so far. The motif is based on the alternation of β-sheet and helical conformation as the protein sequence is followed from the N to the C termini (β-αR-β-polyPro-β). We further identify this motif in the ssNMR structure of the protofibril of the amyloidogenic peptide Aβ1-40. The recurrence of the β-bend suggests a general mode of connecting long parallel β-sheet segments that would allow the growth of partially ordered fibril structures. The design allows the peptide backbone to change direction with a minimal loss of main chain hydrogen bonds. The identification of a coherent organization beyond that of the β-sheet segments in different folds rich in parallel β-sheets suggests a higher degree of ordered structure in protein fibrils, in agreement with their low solubility and dense molecular packing.}
}
@incollection{ZIELINSKI2024116,
title = {Coupled-Cluster Theories for Excited States},
editor = {Manuel Yáñez and Russell J. Boyd},
booktitle = {Comprehensive Computational Chemistry (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {116-140},
year = {2024},
isbn = {978-0-12-823256-9},
doi = {https://doi.org/10.1016/B978-0-12-821978-2.00035-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128219782000350},
author = {Patrik Zielinski and Andreas Köhn},
keywords = {Accurate computations, Analytic gradients, Basis-set convergence, Benchmark computations, Cluster expansion, Coupled-cluster theory, Equation of motion, Excited-state properties, Gradient theory, Linear response, Multireference, Open-shell systems, Single-reference, Size consistency, Transition moments},
abstract = {Coupled-cluster theory offers a hierarchy of increasingly accurate methods and provides thus an important basis for accurate quantum chemistry, also for the computation of electronic excited states. This chapter explains and compares the two main approaches, equation-of-motion and linear-response theory and sketches the computation of transition moments and expectation values, as well as analytic geometric gradients. The basic approaches to arrive at approximations are discussed, and recent benchmark works are used to demonstrate their relative accuracy. Some challenges in coupled-cluster theory, like going to large systems, open-shell and multireference theory and the slow basis-set convergence are also covered.}
}
@incollection{SANTOS2024,
title = {Data analysis on Decision-Making},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-443-13701-3.00018-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443137013000189},
author = {Eulália Santos and Margarida F. Oliveira},
keywords = {Artificial intelligence, Business, Data analysis, Decision making, Logistics, Machine learning, Mathematical modeling operations research, Mathematical programming, Optimization, Statistic, Strategic management, Technology},
abstract = {Today, data analysis plays a vital role in identifying market trends and supporting strategic decision-making in organizations. To make an effective decision in order to obtain positive results, it is necessary not only to carefully analyze various pieces of information but also to use artificial intelligence and critical thinking. Mathematics plays an essential role in making effective decisions and providing tools and methods for analyzing, modeling and solving both simple and more complex problems.}
}
@article{WOZNIAK2023489,
title = {BiLSTM deep neural network model for imbalanced medical data of IoT systems},
journal = {Future Generation Computer Systems},
volume = {141},
pages = {489-499},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22004095},
author = {Marcin Woźniak and Michał Wieczorek and Jakub Siłka},
keywords = {Medical informatics, Deep learning, Multi-optimization learning, BiLSTM, IoT},
abstract = {Health informatics is one of the most developed field in recent time. Computational Intelligence is among the most influential factors that may help to improve patient oriented and secure decision support model. In this article we present a model of IoT system, which combines BiLSTM deep learning with Decision Tree model and data balancing strategy used to help in automated diagnosis support. Presented solution include experimental series of data preprocessing using well established balancing algorithms with custom parameters and modifications in order to best prepare the data for the network training. Such algorithms are ADASYN, SMOTE-Tomek, etc. The system helps to evaluate questionnaires and securely exchange documents between patient and corresponding medical team. From the level of system patient and doctors are able to see automated diagnosis provided by deep learning model. The model gives an important advance to help patients faster. Results show that proposed BiLSTM deep learning with decision tree mode detects diseases from questionnaires with accuracy above 96%, precision above 88% and recall above 96% which proves efficiency of our proposed model.}
}
@article{VEGA2008255,
title = {The catwalk task: Reflections and synthesis: Part 2},
journal = {The Journal of Mathematical Behavior},
volume = {27},
number = {4},
pages = {255-263},
year = {2008},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2009.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732312309000042},
author = {Emiliano Vega and Shawn Hicks},
keywords = {Modeling, Representation, Teacher learning, Task design},
abstract = {In this article we recount our experiences with a series of encounters with the catwalk task and reflect on the professional growth that these opportunities afforded. First, we individually reflect on our own mathematical work on the catwalk task. Second, we reflect on our experiences working with a group of community college students on the catwalk task and our interpretations of their mathematical thinking. In so doing we also detail a number of innovative and novel student-generated representations of the catwalk photos. Finally, we each individually reflect on the entire experience with the catwalk problem, as mathematics learners, as teachers, and as professionals.}
}
@incollection{STEEDMAN2011925,
title = {21 - Temporality},
editor = {Johan {van Benthem} and Alice {ter Meulen}},
booktitle = {Handbook of Logic and Language (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {London},
pages = {925-969},
year = {2011},
isbn = {978-0-444-53726-3},
doi = {https://doi.org/10.1016/B978-0-444-53726-3.00021-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780444537263000219},
author = {Mark Steedman},
keywords = {tense, aspect, natural language semantics, computational semantics, temporal semantics, aktionsarten, causality, evidentiality},
abstract = {Publisher Summary
In thinking about the logical and computational semantics of temporal categories in natural languages, issues of temporal ontology, or metaphysics, must be distinguished from issues of temporal relation. The first thing to observe about the temporal ontology implicit in natural languages is that it is not purely temporal. To take a simple example, the English perfect, when predicated of an event like losing a watch, says that some contextually retrievable consequences of the event in question hold at the time under discussion. Thus, conjoining such a perfect with a further clause denying those consequences is infelicitous. The claim that the semantics depends directly on the conceptual representation of action and contingency suggests that this semantics might be universal, despite considerable differences in its syntactic and morphological encoding across languages. The work described in this chapter suggests that such differences across languages are superficial. Ironically, the English tense/aspect system seems to be based on semantic primitives remarkably like those, which Whorf ascribed to Hopi. Matters of temporal sequence and temporal locality seem to be quite secondary to matters of perspective and contingency. This observation in turn suggests that the semantics of tense and aspect is profoundly shaped by concerns with goals, actions, and consequences, and that temporality in the narrow sense of the term is merely one facet of this system among many.}
}
@article{NSSSN2024106769,
title = {VNSMAS: A constraint-based portfolio profit maximization},
journal = {Computers & Operations Research},
volume = {170},
pages = {106769},
year = {2024},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2024.106769},
url = {https://www.sciencedirect.com/science/article/pii/S0305054824002417},
author = {Usha Devi N.S.S.S.N. and R. Mohan},
keywords = {GAN, Reinforcement learning, Stock, Fuzzy},
abstract = {Stock trading has a more significant influence on the global economy. Stock trading with portfolio optimization became challenging due to the complexity of analyzing the high variance in time series stock data. Efficient portfolio management increases profit and avoids risky situations when investing. The present work aims to model a Variable Neighborhood Search Multi-Agent System for Portfolio Optimization (VNSMASPPO) to optimize the profit on defined trading constraints on buying, selling, and holding trading decisions. This work proposes a novel Variable Neighborhood Search-based Multi-Agent System (VNASMAS) algorithm for profit computation with a constraint-based multi-agent system. The stock price history experimental data sets are collected from 8th August 2016 to 31st March 2023 with 14,567 records. The proposed model achieved an RMSE of 10.11, MAE of 2.75, and MAPE of 0.017, outperforming the literature models. VNSMASPPO maximizes the portfolio profit and is a reliable, adaptable approach.}
}
@article{FORREST19901,
title = {Emergent computation: Self-organizing, collective, and cooperative phenomena in natural and artificial computing networks: Introduction to the proceedings of the ninth annual CNLS conference},
journal = {Physica D: Nonlinear Phenomena},
volume = {42},
number = {1},
pages = {1-11},
year = {1990},
issn = {0167-2789},
doi = {https://doi.org/10.1016/0167-2789(90)90063-U},
url = {https://www.sciencedirect.com/science/article/pii/016727899090063U},
author = {Stephanie Forrest}
}
@article{SCHINCKUS20094415,
title = {Economic uncertainty and econophysics},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {388},
number = {20},
pages = {4415-4423},
year = {2009},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2009.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0378437109005494},
author = {Christophe Schinckus},
keywords = {Econophysics, Uncertainty, Economics, Keynes, Knight, Hayek},
abstract = {The objective of this paper is to provide a methodological link between econophysics and economics. I will study a key notion of both fields: uncertainty and the ways of thinking about it developed by the two disciplines. After having presented the main economic theories of uncertainty (provided by Knight, Keynes and Hayek), I show how this notion is paradoxically excluded from the economic field. In economics, uncertainty is totally reduced by an a priori Gaussian framework—in contrast to econophysics, which does not use a priori models because it works directly on data. Uncertainty is then not shaped by a specific model, and is partially and temporally reduced as models improve. This way of thinking about uncertainty has echoes in the economic literature. By presenting econophysics as a Knightian method, and a complementary approach to a Hayekian framework, this paper shows that econophysics can be methodologically justified from an economic point of view.}
}
@article{SEYMOUR2020117212,
title = {Hierarchical models of pain: Inference, information-seeking, and adaptive control.},
journal = {NeuroImage},
volume = {222},
pages = {117212},
year = {2020},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2020.117212},
url = {https://www.sciencedirect.com/science/article/pii/S1053811920306984},
author = {Ben Seymour and Flavia Mancini},
keywords = {Pain, Nociception, Information theory, Reinforcement learning, Optimal control, Predictive coding, Epistemic value, Free energy principle, Endogenous modulation},
abstract = {Computational models of pain consider how the brain processes nociceptive information and allow mapping neural circuits and networks to cognition and behaviour. To date, they have generally have assumed two largely independent processes: perceptual inference, typically modelled as an approximate Bayesian process, and action control, typically modelled as a reinforcement learning process. However, inference and control are intertwined in complex ways, challenging the clarity of this distinction. Here, we consider how they may comprise a parallel hierarchical architecture that combines inference, information-seeking, and adaptive value-based control. This sheds light on the complex neural architecture of the pain system, and takes us closer to understanding from where pain ’arises’ in the brain.}
}
@article{SAMPSON20052095,
title = {Comments on: “Pore network simulation of fluid inbibition into paper during coating: II. Characterization of paper's morphology and computation of its effective permeability tensor” by Ghassemzadeh and Sahimi [Chemical Engineering Science 59(2004) 2265–2280]},
journal = {Chemical Engineering Science},
volume = {60},
number = {7},
pages = {2095},
year = {2005},
issn = {0009-2509},
doi = {https://doi.org/10.1016/j.ces.2004.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0009250904009327},
author = {W.W. Sampson and C.T.J. Dodson}
}
@article{BORSTLER2023111592,
title = {Investigating acceptance behavior in software engineering—Theoretical perspectives},
journal = {Journal of Systems and Software},
volume = {198},
pages = {111592},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111592},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222002680},
author = {Jürgen Börstler and Nauman bin Ali and Martin Svensson and Kai Petersen},
keywords = {Acceptance behavior, Dual process theory, Technology acceptance, Theory, TAM, UTAUT, TPB},
abstract = {Background:
Software engineering research aims to establish software development practice on a scientific basis. However, the evidence of the efficacy of technology is insufficient to ensure its uptake in industry. In the absence of a theoretical frame of reference, we mainly rely on best practices and expert judgment from industry-academia collaboration and software process improvement research to improve the acceptance of the proposed technology.
Objective:
To identify acceptance models and theories and discuss their applicability in the research of acceptance behavior related to software development.
Method:
We analyzed literature reviews within an interdisciplinary team to identify models and theories relevant to software engineering research. We further discuss acceptance behavior from the human information processing perspective of automatic and affect-driven processes (“fast” system 1 thinking) and rational and rule-governed processes (“slow” system 2 thinking).
Results:
We identified 30 potentially relevant models and theories. Several of them have been used in researching acceptance behavior in contexts related to software development, but few have been validated in such contexts. They use constructs that capture aspects of (automatic) system 1 and (rational) system 2 oriented processes. However, their operationalizations focus on system 2 oriented processes indicating a rational view of behavior, thus overlooking important psychological processes underpinning behavior.
Conclusions:
Software engineering research may use acceptance behavior models and theories more extensively to understand and predict practice adoption in the industry. Such theoretical foundations will help improve the impact of software engineering research. However, more consideration should be given to their validation, overlap, construct operationalization, and employed data collection mechanisms when using these models and theories.}
}
@article{MAGRI2023105535,
title = {Scene context is predictive of unconstrained object similarity judgments},
journal = {Cognition},
volume = {239},
pages = {105535},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2023.105535},
url = {https://www.sciencedirect.com/science/article/pii/S0010027723001695},
author = {Caterina Magri and Eric Elmoznino and Michael F. Bonner},
keywords = {Contextual associations, Objects, Scenes, Similarity, Convolutional neural networks, Natural image statistics},
abstract = {What makes objects alike in the human mind? Computational approaches for characterizing object similarity have largely focused on the visual forms of objects or their linguistic associations. However, intuitive notions of object similarity may depend heavily on contextual reasoning—that is, objects may be grouped together in the mind if they occur in the context of similar scenes or events. Using large-scale analyses of natural scene statistics and human behavior, we found that a computational model of the associations between objects and their scene contexts is strongly predictive of how humans spontaneously group objects by similarity. Specifically, we learned contextual prototypes for a diverse set of object categories by taking the average response of a convolutional neural network (CNN) to the scene contexts in which the objects typically occurred. In behavioral experiments, we found that contextual prototypes were strongly predictive of human similarity judgments for a large set of objects and rivaled the performance of models based on CNN representations of the objects themselves or word embeddings for their names. Together, our findings reveal the remarkable degree to which the natural statistics of context predict commonsense notions of object similarity.}
}
@article{KACZYNSKA20214290,
title = {A new multi-criteria model for ranking chess players},
journal = {Procedia Computer Science},
volume = {192},
pages = {4290-4299},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.205},
url = {https://www.sciencedirect.com/science/article/pii/S187705092101944X},
author = {Aleksandra Kaczyńska and Joanna Kołodziejczyk and Wojciech Sałabun},
keywords = {Chess, MCDA, COMET, players evaluation, decision making},
abstract = {Chess is a very demanding sport as it requires advanced planning and strategic thinking skills. The degree of difficulty of the game also depends on the time allotted for a game, which can range from a few minutes to several tens of minutes. For this reason, the games are divided into several categories: standard, blitz, and bullet. However, as many chess players specialize in only some of the categories, it is difficult to determine the best chess player. It is very important to keep a proper ranking of the players. One way to recognize their achievements is the FIDE (Fédération Internationale des Échecs) titles awarded to the best players. However, there is still the problem of how to determine the best among the Grandmasters. There are many very talented players competing in chess. Creating a single ranking for all types of chess, regardless of the time allotted for the game, is a difficult challenge, as many undeniably outstanding chess players do not specialize in all types. Creating a ranking for only one type would not accurately describe the level of players. Therefore, a ranking was created based on all of them using the COMET method, which belongs to the multi-criteria decision-making methods (MCDA). It is based on fuzzy logic and uses characteristic objects for the assessment of alternatives, which guarantees immunity to the paradox of reversal rankings. Expert opinion was used for correct evaluation. This article presents the ranking of chess players regardless of the type of game they specialize in, to prove that it should be possible to identify the single best chess player.}
}
@article{DARICI2024123327,
title = {How will I break AI? Post-Luddism in the AI age: Fuzzy MCDM synergy},
journal = {Technological Forecasting and Social Change},
volume = {202},
pages = {123327},
year = {2024},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2024.123327},
url = {https://www.sciencedirect.com/science/article/pii/S0040162524001239},
author = {Sefer Darıcı and Muhammad Riaz and Gülay Demir and Zekiye Tamer Gencer and Dragan Pamucar},
keywords = {AI, Communication, Post-Luddism, Fuzzy set, DEMATEL, LMAW},
abstract = {This study proposes a fuzzy multi-criteria model to assess the risk of unemployment among professionals in the communication sector in Turkey, prompted by the rapid development and evolution of artificial intelligence (AI) technologies. The method integrates Fuzzy The Decision Making Trial and Evaluation Laboratory (F-DEMATEL) and Fuzzy Logarithm Methodology of Additive Weights (F-LMAW) procedures. Data were collected from 20 experts representing professions such as public relations, advertising, journalism, and design through a 12-question survey. In the analysis, the F-DEMATEL procedure was initially employed to determine attitudes towards AI technologies, followed by the application of the F-LMAW procedure to assess the magnitude of AI's impact on occupational groups. Findings reveal a nuanced stance: while professionals acknowledge the necessity of AI for their work, they are unwilling to accept unemployment due to more advanced AI. This newly identified structure, termed Post-Luddism, highlights concerns over technological unemployment, particularly pronounced in professions like journalism where job prospects are limited and creative thinking is paramount. In other communication fields, the intensive use of technology mitigates fears of AI harm. However, even in journalism, there exists a propensity to perceive AI as detrimental. These insights shed light on communication professionals' apprehensions and attitudes towards AI's effects. Policymakers and stakeholders can leverage this understanding to formulate strategic measures, considering the divergent perspectives among professional groups regarding AI, towards mitigating potential unemployment risks and fostering AI-adaptive strategies.}
}
@article{COON1995787,
title = {Generalized block-tridiagonal matrix orderings for parallel computation in process flowsheeting},
journal = {Computers & Chemical Engineering},
volume = {19},
number = {6},
pages = {787-805},
year = {1995},
note = {Applications of Parallel Computing},
issn = {0098-1354},
doi = {https://doi.org/10.1016/0098-1354(94)00081-6},
url = {https://www.sciencedirect.com/science/article/pii/0098135494000816},
author = {A.B. Coon and M.A. Stadtherr},
abstract = {A new graph partitioning algorithm for use on structurally unsymmetric systems is presented. Unlike other partitioning algorithms that have been used to provide reorderings for structurally symmetric matrices, this algorithm employs a bipartite graph model, and hence, can be used to consider unsymmetric permutations of structurally unsymmetric matrices. It is shown that the algorithm can be used in identifying coarse-granular, balanced tasks in the direct solution of flowsheeting matrices by parallel techniques based on generalized block-tridiagonal and nested-block-tridiagonal matrix structures. It is also shown that such reorderings can be obtained inexpensively, in worst-case running times that increase linearly with the order of the matrix.}
}
@article{LIU2022189,
title = {Granular cabin: An efficient solution to neighborhood learning in big data},
journal = {Information Sciences},
volume = {583},
pages = {189-201},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.11.034},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521011543},
author = {Keyu Liu and Tianrui Li and Xibei Yang and Xin Yang and Dun Liu and Pengfei Zhang and Jie Wang},
keywords = {Computational efficiency, Granular computing, Neighborhood learning, Neighborhood rough set},
abstract = {Neighborhood Learning (NL) is a paradigm covering theories and techniques of neighborhood, which facilitates data organization, representation and generalization. While delivering impressive performances across various fields such as granular computing, cluster analysis, NL is argued to be computationally demanding, thereby limiting its utility and applicability. In this study, a simple and generic scheme named granular cabin is proposed for drastically speeding up the algorithmic implementation of NL. Specifically, this scheme is deployed to Neighborhood Rough Set (NRS) which is a typical NL methodology. And three major applications of NRS are concerned including approximation computation, neighborhood classification and feature selection. Extensive experiments demonstrate that NRS methodology enhanced by granular cabin consumes much less time. This study offers a promising solution that ensures the great potential of NL in big data.}
}
@article{KRUSKOPF2024104574,
title = {Future teachers’ self-efficacy in teaching practical and algorithmic ICT competencies – Does background matter?},
journal = {Teaching and Teacher Education},
volume = {144},
pages = {104574},
year = {2024},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2024.104574},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X24001069},
author = {Milla Kruskopf and Rekar Abdulhamed and Mette Ranta and Heidi Lammassaari and Kirsti Lonka},
keywords = {Teaching self-efficacy, Self-efficacy, ICT competence, Digital competence, Programming, 21st century competencies, Teacher students},
abstract = {Future teachers need to be confidently equipped to teach 21st century ICT skills. We investigated teaching self-efficacy (TSE) in ICT competencies among teacher students. We confirmed distinct ICT competencies among two cohorts from teacher training programs (n = 347; n = 428): practical (i.e., device and data management), and algorithmic (i.e., programming, and data security). Regression analyses indicated TSE-biases regarding younger age, male gender, and a background in natural sciences, with significant interactions between age, gender, and having learned such ICT-skills already in school. The findings point to a need for tailored strategies in teacher education to mitigate TSE disparities.}
}
@article{ALEXIOU2009623,
title = {Exploring the neurological basis of design cognition using brain imaging: some preliminary results},
journal = {Design Studies},
volume = {30},
number = {6},
pages = {623-647},
year = {2009},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2009.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X09000313},
author = {K. Alexiou and T. Zamenopoulos and J.H. Johnson and S.J. Gilbert},
keywords = {design cognition, problem solving, design problems, research methods, cognitive neuroscience},
abstract = {The paper presents a pilot interdisciplinary research study carried out as a step towards understanding the neurological basis of design thinking. The study involved functional magnetic resonance imaging (fMRI) of volunteers while performing design and problem-solving tasks. The findings suggest that design and problem solving involve distinct cognitive functions associated with distinct brain networks. The paper introduces the methodology, presents the findings, and discusses the potential role of brain imaging in design research.}
}
@article{KAMARI2017330,
title = {Sustainability focused decision-making in building renovation},
journal = {International Journal of Sustainable Built Environment},
volume = {6},
number = {2},
pages = {330-350},
year = {2017},
issn = {2212-6090},
doi = {https://doi.org/10.1016/j.ijsbe.2017.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S221260901730064X},
author = {Aliakbar Kamari and Rossella Corrao and Poul Henning Kirkegaard},
keywords = {Sustainability, Building renovation, Decision support, Knowledge management, Soft Systems Methodology (SSM), Value Focused Thinking (VFT)},
abstract = {An overview of recent research related to building renovation has revealed that efforts to date do not address sustainability issues comprehensively. The question then arises in regard to the holistic sustainability objectives within building renovation context. In order to deal with this question, the research adopts a multi-dimensional approach involving literature review, exploration of existing assessment methods and methodologies, individual and focus group interviews, and application of Soft Systems Methodologies (SSM) with Value Focused Thinking (VFT). In doing so, appropriate data about sustainability objectives have been collected and structured, and subsequently verified using a Delphi study. A sustainability framework was developed in cooperation with University of Palermo and Aarhus University to audit, develop and assess building renovation performance, and support decision-making during the project’s lifecycle. The paper represents the results of research aiming at addressing sustainability of the entire renovation effort including new categories, criteria, and indicators. The developed framework can be applied during different project stages and to assist in the consideration of the sustainability issues through support of decision-making and communication with relevant stakeholders. Early in a project, it can be used to identify key performance criteria, and later to evaluate/compare the pros and cons of alternative retrofitting solutions either during the design stage or upon the project completion. According to the procedure of the consensus-based process for the development of an effective sustainability decision-making framework which was employed in this study, the outcome can also be considered as an outset step intended for the establishment of a Decision Support Systems (DSS) and assessment tool suited to building renovation context.}
}
@article{DONALDSON1995301,
title = {Building object-oriented systems: An introduction from concepts to implementation in c++: R. E. Callan, Computational Mechanics Publications, Southampton, UK, 1994. ISBN 1-85312-340-4. 304 pp. £47.00},
journal = {Artificial Intelligence in Engineering},
volume = {9},
number = {4},
pages = {301},
year = {1995},
note = {Selected Papers from the 1994 Japan/Korea Joint Conference on Expert Systems},
issn = {0954-1810},
doi = {https://doi.org/10.1016/0954-1810(95)90016-0},
url = {https://www.sciencedirect.com/science/article/pii/0954181095900160},
author = {Iain Donaldson}
}
@article{ENE2016973,
title = {A genetic algorithm for minimizing energy consumption in warehouses},
journal = {Energy},
volume = {114},
pages = {973-980},
year = {2016},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2016.08.045},
url = {https://www.sciencedirect.com/science/article/pii/S0360544216311586},
author = {Seval Ene and İlker Küçükoğlu and Aslı Aksoy and Nursel Öztürk},
keywords = {Genetic algorithm, Green supply chain, Minimization of energy consumption, Warehouse management},
abstract = {Green supply chain management is generally defined as integration of green thinking and environmental issues into the whole supply chain operations like product design, manufacturing process, warehousing, distribution etc. Within this context green principles should be adopted in warehouse management to minimize negative impact on the environment. In warehouse operations, picking must be analyzed attentively which is widely studied in literature for minimizing service time levels because of its close relation to the higher costs. The efficiency of picking in warehouses mainly depends on storage assignment policy that directly affects picking performance in warehouses. In this paper, picking operation in warehouses is studied to minimize energy consumption with proper storage policy other than service time. Genetic algorithm (GA) is proposed to solve the problem and numerical examples are presented to demonstrate the performance of the GA. Results show that, the GA gives efficient solutions to the problem.}
}
@article{PUTICA2024105836,
title = {Reconceptualizing complex posttraumatic stress disorder: A predictive processing framework for mechanisms and intervention},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {164},
pages = {105836},
year = {2024},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2024.105836},
url = {https://www.sciencedirect.com/science/article/pii/S0149763424003051},
author = {Andrea Putica and James Agathos},
keywords = {Complex Posttraumatic Stress Disorder (C-PTSD), Predictive processing, Trauma, Interoceptive inference, Active inference},
abstract = {In this article, we introduce a framework for interpreting Complex Posttraumatic Stress Disorder (C-PTSD) through predictive processing, a neuroscience concept explaining the brain’s interpretation and prediction of sensory information. While closely related to PTSD, C-PTSD encompasses additional symptom clusters marked by disturbances in self-organization (DSO), such as negative self-concept, affect dysregulation, and relational difficulties, typically resulting from prolonged traumatic stressors. Our model leverages advances in computational psychiatry and neuroscience, offering a mechanistic explanation for these symptoms by illustrating how prolonged trauma disrupts the brain's predictive processing. Specifically, altered predictive mechanisms contribute to C-PTSD's symptomatology, focusing on DSO: (1) Negative self-concept emerges from maladaptive priors that bias perception towards self-criticism, misaligning expected and actual interoceptive states; (2) Misalignment between predicted and actual interoceptive signals leads to affect dysregulation, with sensitivity to bodily cues; and (3) Relationship challenges arise from skewed social prediction errors, fostering mistrust and withdrawal. This precision-focused approach sheds light on the dynamics underpinning C-PTSD and highlights potential intervention targets aimed at recalibrating the predictive processing system.}
}
@article{JIA2024e28858,
title = {The impact of basin horizontal ecological compensation policies on carbon emissions: A case study of the Yangtze river economic Belt},
journal = {Heliyon},
volume = {10},
number = {8},
pages = {e28858},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e28858},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024048898},
author = {Kuiyuan Jia and Ruhua Yuan},
keywords = {Basin horizontal ecological compensation, Carbon emissions, PSM-DID method, Yangtze river economic belt},
abstract = {The issue of global climate change has garnered increasing attention, with carbon emissions emerging as a significant challenge confronting the world today. As an important means of environmental management, river basin ecological compensation must break through the traditional thinking of "water-centric" and move towards the coordinated development of "pollution reduction". Therefore, the study chooses the watershed scale ecological compensation experiment carried out in the Yangtze River Economic Belt as a natural experiment. Based on the prefecture-level city panel data from 2008 to 2021, a double difference model is constructed to examine the impact of the basin horizontal ecological compensation policy on carbon emissions and its mechanism. The study shows that the inter-regional horizontal ecological compensation measures have an obvious inhibitory effect on carbon emissions in the region, and through a series of tests, the conclusion is stable. Mechanism testing shows that policy implementation achieves carbon emission reductions through two channels: improving financial development and promoting scientific and technological innovation. The results of heterogeneity analysis verify that the effect of policy implementation is affected by the dual factors of social economy and innovation and entrepreneurship and that key cities and cities with a high innovation and entrepreneurship environment produce higher carbon emission reduction benefits. The research conclusions provide policy suggestions for promoting watershed ecological compensation policies to achieve carbon emission reduction from three aspects: encouraging small and medium-sized watersheds to implement city-specific policies, promoting innovative technologies and establishing monitoring and evaluation mechanisms, and strengthening policy support and financial investment.}
}
@article{EARL2019303,
title = {Elusive optima: A process tracing analysis of procedural rationality in mobile phone connection plan choices},
journal = {Journal of Economic Behavior & Organization},
volume = {161},
pages = {303-322},
year = {2019},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2019.03.021},
url = {https://www.sciencedirect.com/science/article/pii/S0167268119300988},
author = {Peter E. Earl and Lana Friesen and Christopher Shadforth},
keywords = {Consumer capabilities, Choice overload, Procedural rationality, Process tracing},
abstract = {This paper reports an experiment in which subjects were rewarded on the basis of how close they came to finding the cheapest mobile phone plan to serve a particular usage remit by searching freely in the Internet. During the task, subjects were required to ‘think aloud’ and recordings were made of what they said and what they did on their computer screens. Analysis of the screen-capture movie recordings revealed major shortfalls in procedural rationality, including poor strategic thinking about how to deal with choice overload, poor conceptual understanding of mobile phone plans and pricing systems, as well as cognitive and calculation errors. Our novel method leads to a very different policy focus from that implied by viewing the problem in terms of excess information per se and irrationality as driven by innate heuristics and biases.}
}
@article{PACE2023105433,
title = {Exploring future research and innovation directions for a sustainable blue economy},
journal = {Marine Policy},
volume = {148},
pages = {105433},
year = {2023},
issn = {0308-597X},
doi = {https://doi.org/10.1016/j.marpol.2022.105433},
url = {https://www.sciencedirect.com/science/article/pii/S0308597X22004808},
author = {Lisa A. Pace and Ozcan Saritas and Alan Deidun},
keywords = {Foresight, Blue economy, Interdisciplinary science, Marine science, Sustainable development, Stakeholder participation},
abstract = {The blue economy integrates commercial, research and innovation activities across diverse industrial sectors. Achieving a sustainable blue economy requires unlocking the potential of science and innovation to develop innovative ocean sustainability solutions. This study explores the role of foresight in co-creating alternative, preferred futures for a sustainable blue economy looking towards 2030 and in establishing an interdisciplinary dialogue about research and innovation opportunities to achieve these futures. To this end, a foresight exercise is conducted with marine scientists and researchers in 6 countries in Europe. The exercise is designed in three stages: scanning, scenario-building and strategic orientation, and uses a combination of foresight methods to encourage creative thinking and exploration. The scenarios developed in the study describe alternative future worlds built on the establishment of self-sustaining communities and engaged societies; the diffusion of digitalisation and growth of blue biotechnologies; booming ecosystem services and open and collaborative research infrastructures that impact different sectors of the blue economy. A portfolio of research and innovation areas is developed that aims to inspire new research directions in four domains: (i) integrated ocean management tools; (ii) closed loop, circular polyculture systems; (iii) co-creation of innovation and transdisciplinary research; and (iv) open access and collaborative databases supporting ecosystem services. The study highlights the role of foresight in bridging across disciplinary perspectives and industry sectors. Foresight can be used to complement Decision-Support Systems and other quantitative approaches for research agenda-setting and for decision-making on policies addressing sustainability in the marine sciences. The process contributes to futures skills-building at institutional level and helps establish a futures mindset for strategic planning.}
}
@article{HUANG201724,
title = {Energy and carbon performance evaluation for buildings and urban precincts: review and a new modelling concept},
journal = {Journal of Cleaner Production},
volume = {163},
pages = {24-35},
year = {2017},
note = {Achieving Low/no Fossil-carbon Economies based upon the Essential Transformations to Support them},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2015.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S0959652615018235},
author = {Bin Huang and Ke Xing and Stephen Pullen},
keywords = {Buildings, Integrated modelling, Life cycle energy, Systems thinking, Urban precincts},
abstract = {With the accelerating pace of urbanisation around the world, the planning, development and operation of buildings and precincts have become increasingly important with respect to energy use and the associated carbon footprint of the modern built environment. Over recent decades, much effort, both in research and in practice, has been devoted to building construction and urban planning for the improvement of energy efficiency and greenhouse gas emissions. However, the accuracy of modelling and evaluation of energy and carbon performance for buildings and urban precincts remains limited, affected by inadequate energy intensity data and highly integrated building systems, as well as the complex interactions between buildings and the urban eco-system. This paper presents a critical review of current measures and models for representing and assessing life cycle energy as well as associated emissions profiles at both the building and the precinct levels. It also identifies influential factors and explores interactions among buildings, surrounding environment and user behaviours at the urban precinct level by taking a systems perspective. Based on such a review, this study maps out some key challenges for integrating energy and carbon metrics, and finally proposes a precinct-level system boundary definition and an integrated model following systems thinking. The proposed model can facilitate a critical thinking approach about the evaluations of global energy and emissions, and support the quantification of energy consumption and associated emissions for building precinct systems.}
}
@article{MAHMUD20233933,
title = {Detection of Different Stages of Alzheimer’s Disease Using CNN Classifier},
journal = {Computers, Materials and Continua},
volume = {76},
number = {3},
pages = {3933-3948},
year = {2023},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.039020},
url = {https://www.sciencedirect.com/science/article/pii/S1546221823000450},
author = {S M Hasan Mahmud and Md Mamun Ali and Mohammad Fahim Shahriar and Fahad Ahmed Al-Zahrani and Kawsar Ahmed and Dip Nandi and Francis M. Bui},
keywords = {Alzheimer’s disease, early detection, convolutional neural network, data augmentation, random oversampling, machine learning},
abstract = {Alzheimer’s disease (AD) is a neurodevelopmental impairment that results in a person’s behavior, thinking, and memory loss. The most common symptoms of AD are losing memory and early aging. In addition to these, there are several serious impacts of AD. However, the impact of AD can be mitigated by early-stage detection though it cannot be cured permanently. Early-stage detection is the most challenging task for controlling and mitigating the impact of AD. The study proposes a predictive model to detect AD in the initial phase based on machine learning and a deep learning approach to address the issue. To build a predictive model, open-source data was collected where five stages of images of AD were available as Cognitive Normal (CN), Early Mild Cognitive Impairment (EMCI), Mild Cognitive Impairment (MCI), Late Mild Cognitive Impairment (LMCI), and AD. Every stage of AD is considered as a class, and then the dataset was divided into three parts binary class, three class, and five class. In this research, we applied different preprocessing steps with augmentation techniques to efficiently identify AD. It integrates a random oversampling technique to handle the imbalance problem from target classes, mitigating the model overfitting and biases. Then three machine learning classifiers, such as random forest (RF), K-Nearest neighbor (KNN), and support vector machine (SVM), and two deep learning methods, such as convolutional neuronal network (CNN) and artificial neural network (ANN) were applied on these datasets. After analyzing the performance of the used models and the datasets, it is found that CNN with binary class outperformed 88.20% accuracy. The result of the study indicates that the model is highly potential to detect AD in the initial phase.}
}
@article{KLIGER20217,
title = {Dynamic Archeology or Distant Reading: Literary Study Between Two Formalisms},
journal = {Russian Literature},
volume = {122-123},
pages = {7-28},
year = {2021},
note = {Digital Humanities and Russian and East European Studies},
issn = {0304-3479},
doi = {https://doi.org/10.1016/j.ruslit.2021.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0304347921000429},
author = {Ilya Kliger},
keywords = {Computational Literary Studies, Distant Reading, Literary Form, Russian Formalism, OPOIAZ},
abstract = {Scholars working within computational literary studies often invoke Russian Formalism as a methodologically like-minded school of thought and a repository of useful insights, which can at last be tested with the help of recently developed digital techniques. Yet the two formalisms diverge starkly when it comes to three of their most fundamental categories of analysis: first, in their respective conceptions of literary form itself; next, in their notions of history and of what it means to tell the history of form; and finally, in the ways in which they construe the relationship between literature and society as a whole, or, in other words, in their corresponding sociologies of literary form. This paper, then, is a contribution to creating the conditions for the possibility of a genuine exchange between the two formalisms here at issue by focusing, first and foremost, on what divides them.}
}
@article{LOWENSTEIN20191237,
title = {Visual perception, cognition, and error in dermatologic diagnosis: Diagnosis and error},
journal = {Journal of the American Academy of Dermatology},
volume = {81},
number = {6},
pages = {1237-1245},
year = {2019},
issn = {0190-9622},
doi = {https://doi.org/10.1016/j.jaad.2018.12.072},
url = {https://www.sciencedirect.com/science/article/pii/S0190962219303251},
author = {Eve J. Lowenstein and Richard Sidlow and Christine J. Ko},
keywords = {cognitive error, diagnostic error, heuristic, metacognition, patient safety, visual intelligence},
abstract = {Diagnostic error in dermatology is a large practice gap that has received little attention. Diagnosis in dermatology relies heavily on a heuristic approach that is responsible for our perception of clinical findings. To improve our diagnostic accuracy, a better understanding of the strengths and limitations of heuristics (cognitive shortcuts) used in dermatology is essential. Numerous methods have been proposed to improve diagnostic accuracy, including brain training, reducing cognitive load, and getting feedback and second opinions. Becoming comfortable with the uncertainty intrinsic to medicine is essential. Ultimately, the practice of metacognition, or thinking about how we think, can offer corrective insights to improve accuracy in diagnosis.}
}
@article{SCHIFERL1997249,
title = {Evolution of plastic anisotropy for high-strain-rate computations},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {143},
number = {3},
pages = {249-270},
year = {1997},
issn = {0045-7825},
doi = {https://doi.org/10.1016/S0045-7825(96)01159-0},
url = {https://www.sciencedirect.com/science/article/pii/S0045782596011590},
author = {Sheila K. Schiferl and Paul J. Maudlin},
abstract = {A model for anisotropic material strength, and for changes in the anisotropy due to plastic strain, is described. This model has been developed for use in high-rate, explicit, Lagrangian multidimensional continuum-mechanics codes. The model handles anisotropies, in single-phase materials, in particular the anisotropies due to crystallographic texture—preferred orientations of the single-crystal grains. Textural anisotropies, and the changes in these anisotropies, depend overwhelmingly on the crystal structure of the material and on the deformation history. The changes, particularly for complex deformations, are not amenable to simple analytical forms. To handle this problem, the material model described here includes a texture code, or micromechanical calculation, coupled to a continuum code. The texture code updates grain orientations as a function of tensor plastic strain, and calculates the yield strength in different directions. A yield function is fitted to these yield ‘points’. For each computational cell in the continuum simulation, the texture code tracks a particular set of grain orientations. The orientations will change due to the tensor strain history, and the yield function will change accordingly. Hence, the continuum code supplies a tensor strain to the texture code, and the texture code supplies an updated yield function to the continuum code. Since significant texture changes require relatively large strains—typically, a few percent or more—the texture code is not called very often, and the increase in computer time is not excessive. The model was implemented, using a finite-element continuum code and a texture code specialized for hexagonal-close-packed crystal structures. The results for several uniaxial stress problems and an explosive-forming problem are shown.}
}
@article{FATAHI2016272,
title = {A fuzzy cognitive map model to calculate a user's desirability based on personality in e-learning environments},
journal = {Computers in Human Behavior},
volume = {63},
pages = {272-281},
year = {2016},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2016.05.041},
url = {https://www.sciencedirect.com/science/article/pii/S0747563216303685},
author = {Somayeh Fatahi and Hadi Moradi},
keywords = {Personality, Emotion, User's status, Desirability, E-learning},
abstract = {The recent research in artificial intelligence shows an increasing interest in the modeling of human behavior factors such as personality, mood, and emotion for developing human-friendly systems. That is why there is an interest in developing models and algorithms to determine a human's emotions while interacting with a system to improve the quality of the interaction. In this paper, we propose a computational model to calculate a user's desirability based on personality in e-learning environments. The desirability is one of the most important variables in determining a user's emotions. The model receives several e-learning environmental events and predicts the desirability of the events based on the user's personality and his/her goals. The proposed model has been evaluated in a simulated and real e-learning environment. The results show that the model formulates the relationship between personality and emotions with high accuracy.}
}
@article{ZUO2010268,
title = {Integrating performance-based design in beginning interior design education: an interactive dialog between the built environment and its context},
journal = {Design Studies},
volume = {31},
number = {3},
pages = {268-287},
year = {2010},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2009.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X09000969},
author = {Qun Zuo and Wesley Leonard and Eileen E. MaloneBeach},
keywords = {performance-based design, interior design, design education, computer aided design, design process},
abstract = {This paper presents a new paradigm in interior design education in which building performance simulation was employed for decision making and design generation. Digital technology was intermixed with conventional paper-based media in the design process to explore formal, spatial and passive solar energy solutions. The intention of the study was to re-discover the value of computers in assisting design thinking and improving effective learning. The results indicated the Performance-Based Design approach resulted in an early awareness of sustainable energy for beginning interior design students. Further, it enhanced understanding of the mutual relationship between interior and exterior and between the built and natural environment. This paper acknowledged the achievements as well as limitations and future directions for the integration of Performance-Based Design into interior design curriculum.}
}
@article{BECKER201979,
title = {Two results on slime mold computations},
journal = {Theoretical Computer Science},
volume = {773},
pages = {79-106},
year = {2019},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2018.08.027},
url = {https://www.sciencedirect.com/science/article/pii/S0304397518305590},
author = {Ruben Becker and Vincenzo Bonifaci and Andreas Karrenbauer and Pavel Kolev and Kurt Mehlhorn},
keywords = {, Dynamical systems, Linear programming, Optimization, Approximation algorithms},
abstract = {We present two results on slime mold computations. In wet-lab experiments by Nakagaki et al. (2000) [1] the slime mold Physarum polycephalum demonstrated its ability to solve shortest path problems. Biologists proposed a mathematical model, a system of differential equations, for the slime's adaption process (Tero et al., 2007) [3]. It was shown that the process convergences to the shortest path (Bonifaci et al., 2012) [5] for all graphs. We show that the dynamics actually converges for a much wider class of problems, namely undirected linear programs with a non-negative cost vector. Combinatorial optimization researchers took the dynamics describing slime behavior as an inspiration for an optimization method and showed that its discretization can ε-approximately solve linear programs with positive cost vector (Straszak and Vishnoi, 2016) [14]. Their analysis requires a feasible starting point, a step size depending linearly on ε, and a number of steps with quartic dependence on opt/(εΦ), where Φ is the difference between the smallest cost of a non-optimal basic feasible solution and the optimal cost (opt). We give a refined analysis showing that the dynamics initialized with any strongly dominating point converges to the set of optimal solutions. Moreover, we strengthen the convergence rate bounds and prove that the step size is independent of ε, and the number of steps depends logarithmically on 1/ε and quadratically on opt/Φ.}
}
@article{GARBEY1990293,
title = {Massively parallel computation of conservation laws},
journal = {Parallel Computing},
volume = {16},
number = {2},
pages = {293-304},
year = {1990},
issn = {0167-8191},
doi = {https://doi.org/10.1016/0167-8191(90)90067-J},
url = {https://www.sciencedirect.com/science/article/pii/016781919090067J},
author = {Marc Garbey and David Levine},
keywords = {Cellular automata, Partial differential equations, Method of characteristics, Parallel algorithms, Conservation laws},
abstract = {We present a new method for computing solutions of conservation laws based on the use of cellular automata with the method of characteristics. The method exploits the high degree of parallelism available with cellular automata and retains important features of the method of characteristics. It yields high numerical accuracy and extends naturally to adaptive meshes and domain decomposition methods for perturbed conservation laws. We describe the method and its implementation for a Dirichlet problem with a single conservation law for the one-dimensional case. Numerical results for the one-dimensional law with the classical Burgers nonlinearity or the Buckley-Leverett equation show good numerical accuracy outside the neighborhood of the shocks. The error in the area of the shocks is of the order of the mesh size. The algorithm is well suited for execution on both massively parallel computers and vector machines. We present timing results for an Alliant FX/8, Connection Machine Model 2, and CRAY X-MP.}
}
@article{TEIXEIRADUARTE2022112513,
title = {Review on layout optimization strategies of offshore parks for wave energy converters},
journal = {Renewable and Sustainable Energy Reviews},
volume = {163},
pages = {112513},
year = {2022},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2022.112513},
url = {https://www.sciencedirect.com/science/article/pii/S1364032122004178},
author = {Felipe Teixeira-Duarte and Daniel Clemente and Gianmaria Giannini and Paulo Rosa-Santos and Francisco Taveira-Pinto},
keywords = {Computational intelligence techniques, Wave energy, Renewable energy, Layout optimization, Offshore parks, Energy parks, WEC arrays},
abstract = {Layout optimization of wave energy offshore parks is a challenging task, as it encompasses various design objectives and constraints attributed to the complex hydrodynamic interactions. The wave energy converter (WEC) park performance is affected by local environment and device characteristics. To solve this challenge, advanced numerical algorithms, including artificial intelligence, have been applied to a wide range of case studies. Nevertheless, this process remains incomplete, which keeps it as a pertinent research topic in the field of WEC development. The present paper provides an overview of the current state and research trends of offshore WEC park layout optimization. To analyze the state-of-the-art, the paper targets the last decades’ research on this topic, summarizing the studies, addressing the optimization objective and the employed methods and separating them according to the corresponding technique. The review showed that the results strongly depend on the methodologies applied. Furthermore, a preferential use of computational intelligence techniques has been observed in recent years.}
}
@article{MANZOLLI2022112211,
title = {A review of electric bus vehicles research topics – Methods and trends},
journal = {Renewable and Sustainable Energy Reviews},
volume = {159},
pages = {112211},
year = {2022},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2022.112211},
url = {https://www.sciencedirect.com/science/article/pii/S1364032122001344},
author = {Jônatas Augusto Manzolli and João Pedro Trovão and Carlos Henggeler Antunes},
keywords = {Electric bus, Electric mobility, Research gaps, Sustainability, Fleet operation, Energy management},
abstract = {The transportation sector accounts for a significant share of greenhouse gas emissions. Hence, the electrification of this sector is a crucial contributor to the mitigation of global warming. Recent studies suggest that electric vehicles will be economically paired with internal combustion engine vehicles in the near future. However, relying on private vehicle decarbonization only cannot deliver comprehensive space management efficiency solutions in urban environments. Therefore, it is essential to invest in the technological development and deployment of electric buses for public transportation, directly enhancing the quality of life in large cities. From this perspective, this review examines a wide range of scientific literature on electric bus research using science mapping methods and content analysis to support critical thinking unveiling the main research streams, methods, and gaps of the field. The analysis indicates that future research on electric buses will be mainly devoted to sustainability (encompassing economic, environmental and quality of service dimensions), energy management strategies, and fleet operation.}
}
@incollection{HUTCHINS20012068,
title = {Cognition, Distributed},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {2068-2072},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/01636-3},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767016363},
author = {E. Hutchins},
abstract = {Distributed cognition is a framework for thinking about cognition which seeks to understand how the cognitive properties of aggregates emerge from the interactions of component parts. It can be applied to cognitive systems at many levels of complexity, from areas of an individual brain to communities of interacting persons. Distributed cognition is sometimes construed as a special kind of cognition that occurs when people are in interaction with one another or with material artifacts. This is only partly correct. Rather than being a kind of cognition, distributed cognition is a manner of thinking about cognition that permits one to examine the relationships between what is in the mind and the world the mind is in. When applied to groups of persons, distributed cognition provides a language for cognitive processes that are distributed across the members of a social group, between people and their material environments, and through time. It attempts to use an understanding of the social, cultural, and material context of cognitive practices to constrain models of cognitive processes within and among individual minds.}
}
@article{LI2024120,
title = {Conformal structure-preserving SVM methods for the nonlinear Schrödinger equation with weakly linear damping term},
journal = {Applied Numerical Mathematics},
volume = {205},
pages = {120-136},
year = {2024},
issn = {0168-9274},
doi = {https://doi.org/10.1016/j.apnum.2024.06.024},
url = {https://www.sciencedirect.com/science/article/pii/S0168927424001727},
author = {Xin Li and Luming Zhang},
keywords = {Damped nonlinear Schrödinger equation, Conformal properties, Supplementary variable method, High-order accuracy, Optimization model},
abstract = {In this paper, by applying the supplementary variable method (SVM), some high-order, conformal structure-preserving, linearized algorithms are developed for the damped nonlinear Schrödinger equation. We derive the well-determined SVM systems with the conformal properties and they are then equivalent to nonlinear equality constrained optimization problems for computation. The deduced optimization models are discretized by using the Gauss type Runge-Kutta method and the prediction-correction technique in time as well as the Fourier pseudo-spectral method in space. Numerical results and some comparisons between this method and other reported methods are given to favor the suggested method in the overall performance. It is worthwhile to emphasize that the numerical strategy in this work could be extended to other conservative or dissipative system for designing high-order structure-preserving algorithms.}
}
@article{KROGER2013189,
title = {An ERP study of passive creative conceptual expansion using a modified alternate uses task},
journal = {Brain Research},
volume = {1527},
pages = {189-198},
year = {2013},
issn = {0006-8993},
doi = {https://doi.org/10.1016/j.brainres.2013.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0006899313009566},
author = {Sören Kröger and Barbara Rutter and Holger Hill and Sabine Windmann and Christiane Hermann and Anna Abraham},
keywords = {Creativity, ERP, N400, Conceptual expansion, Alternate uses task, Divergent thinking, Semantic cognition},
abstract = {A novel ERP paradigm was employed to investigate conceptual expansion, a central component of creative thinking. Participants were presented with word pairs, consisting of everyday objects and uses for these objects, which had to be judged based on the two defining criteria of creative products: unusualness and appropriateness. Three subject-determined trial types resulted from this judgement: high unusual and low appropriate (nonsensical uses), low unusual and high appropriate (common uses), and high unusual and high appropriate (creative uses). Word pairs of the creative uses type are held to passively induce conceptual expansion. The N400 component was not specifically modulated by conceptual expansion but was, instead, generally responsive as a function of unusualness or novelty of the stimuli (nonsense=creative>common). Explorative analyses in a later time window (500–900ms) revealed that ERP activity in this phase indexes appropriateness (nonsense>creative=common). In the discussion of these findings with reference to the literature on semantic cognition, both components are proposed as indexing processes relevant to conceptual expansion as they are selectively involved in the encoding and integration of a newly established semantic connection between two previously unrelated concepts.}
}
@article{DENHAM2022105526,
title = {Visualization and modeling of forest fire propagation in Patagonia},
journal = {Environmental Modelling & Software},
volume = {158},
pages = {105526},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105526},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222002262},
author = {Mónica M. Denham and Sigfrido Waidelich and Karina Laneri},
keywords = {Simulation, Modeling, Forest fire behavior, High-performance computing, GPGPU},
abstract = {Fire propagation is a big concern all over the world. Visualization is a valuable tool to test possible different scenarios for fire spread, specially for designing strategies for fire control, mitigation and management. We present a parallel High-Performance Computing (HPC) forest fire simulator with an interactive and intuitive user interface that offers several functionalities to the user. The visualization interface allows to choose the propagation model of preference, the scenario of interest, as well as numerous simulation features including firebreaks and ignition points. We show some of the outputs for two different mathematical models for fire spreading. The simulator was developed with an open source philosophy in the framework of Faster Than Real Time (FTRT) applications thinking on its possible use in the field during a forest fire propagation. It can be run in Linux (Ubuntu) and Windows Operating Systems and for portability purposes the simulator was also implemented on a NVIDIA Jetson Nano.}
}
@article{HUANG2012250,
title = {The effectiveness of using procedural scaffoldings in a paper-plus-smartphone collaborative learning context},
journal = {Computers & Education},
volume = {59},
number = {2},
pages = {250-259},
year = {2012},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2012.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S0360131512000310},
author = {Hui-Wen Huang and Chih-Wei Wu and Nian-Shing Chen},
keywords = {Cooperative/collaborative learning, Improving classroom teaching, Teaching/learning strategies},
abstract = {The purpose of this study was to evaluate the effectiveness of using procedural scaffoldings in fostering students’ group discourse levels and learning outcomes in a paper-plus-smartphone collaborative learning context. All participants used built-in camera smartphones to learn new knowledge by scanning Quick Response (QR) codes, a type of two-dimensional barcode, embedded in paper-based learning materials in this study. Sixty undergraduate and graduate students enrolled at a four-year university in southern Taiwan participated in this study. Participants were randomly assigned into two different groups, using procedural scaffoldings learning and non-procedural scaffoldings learning. The learning unit about the Long Tail, an important concept used in products sales, was the learning task that participants were expected to complete. During the experiment, pretest–posttest and the completed group worksheets were used to collect data. The researchers applied content analyses, chi-square test, t-test, and ANCOVA to answer research questions. The findings indicated that participants in the experimental group using procedural scaffoldings achieved better learning outcomes than their counterparts in the control group in terms of group discourse levels, group learning, and individual learning.}
}
@article{BRANICKY199567,
title = {Universal computation and other capabilities of hybrid and continuous dynamical systems},
journal = {Theoretical Computer Science},
volume = {138},
number = {1},
pages = {67-100},
year = {1995},
note = {Hybrid Systems},
issn = {0304-3975},
doi = {https://doi.org/10.1016/0304-3975(94)00147-B},
url = {https://www.sciencedirect.com/science/article/pii/030439759400147B},
author = {Michael S. Branicky},
abstract = {We explore the simulation and computational capabilities of hybrid and continuous dynamical systems. The continuous dynamical systems considered are ordinary differential equations (ODEs). For hybrid systems we concentrate on models that combine ODEs and discrete dynamics (e.g., finite automata). We review and compare four such models from the literature. Notions of simulation of a discrete dynamical system by a continuous one are developed. We show that hybrid systems whose equations can describe a precise binary timing pulse (exact clock) can simulate arbitrary reversible discrete dynamical systems defined on closed subsets of Rn. The simulations require continuous ODEs in R2n with the exact clock as input. All four hybrid systems models studied here can implement exact clocks. We also prove that any discrete dynamical system in Zn can be simulated by continuous ODEs in R2n + 1. We use this to show that smooth ODEs in R3 can simulate arbitrary Turing machines, and hence possess the power of universal computation. We use the famous asynchronous arbiter problem to distinguish between hybrid and continuous dynamical systems. We prove that one cannot build an arbiter with devices described by a system of Lipschitz ODEs. On the other hand, all four hybrid systems models considered can implement arbiters even if their ODEs are Lipschitz.}
}
@article{ZHANG2022101060,
title = {The neural encoding of productive phonological alternation in speech production: Evidence from Mandarin Tone 3 sandhi},
journal = {Journal of Neurolinguistics},
volume = {62},
pages = {101060},
year = {2022},
issn = {0911-6044},
doi = {https://doi.org/10.1016/j.jneuroling.2022.101060},
url = {https://www.sciencedirect.com/science/article/pii/S0911604422000045},
author = {Jie Zhang and Caicai Zhang and Stephen Politzer-Ahles and Ziyi Pan and Xunan Huang and Chang Wang and Gang Peng and Yuyu Zeng},
keywords = {Tone sandhi, Mandarin Chinese, Speech production, Event-related potentials, Phonological alternation, Word frequency},
abstract = {The understanding of alternation is a key goal in phonological research. But little is known about how phonological alternations are implemented in speech production. The current study tested the hypothesis that the production of words that undergo a highly productive alternation, Mandarin Tone 3 sandhi, is supported by a computation mechanism, which predicts that this alternation is subserved by neural activity in a time-window associated with post-lexical phonological and phonetic encoding regardless of word frequency. ERPs were recorded while participants sub-vocally produced high- and low-frequency disyllabic words that do or do not require sandhi. Sandhi words elicited more positive ERPs than non-sandhi words over left anterior channels around 336–520 ms after participants saw the cue instructing them to initiate sub-vocal production, but this effect was not significantly modulated by word frequency. These findings are consistent with predictions of the computation mechanism and have implications for current psycholinguistic models of speech production. (150 words)}
}
@article{RASTEIRO2009e9,
title = {LABVIRTUAL—A virtual platform to teach chemical processes},
journal = {Education for Chemical Engineers},
volume = {4},
number = {1},
pages = {e9-e19},
year = {2009},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2009.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1749772809000025},
author = {M.G. Rasteiro and L. Ferreira and J. Teixeira and F.P. Bernardo and M.G. Carvalho and A. Ferreira and R.Q. Ferreira and F. Garcia and C.M.S.G. Baptista and N. Oliveira and M. Quina and L. Santos and P.A. Saraiva and A. Mendes and F. Magalhães and A.S. Almeida and J. Granjo and M. Ascenso and R.M. Bastos and R. Borges},
keywords = {Chemical processes, E-learning, Virtual laboratories, Computational platform},
abstract = {The need to develop the capacity for autonomous and critical thinking in students and introduce practical approaches that complement the scientific background, have been acting as driving-forces that motivate engineering educators to develop new teaching methodologies. The Chemical Engineering Departments of both the Universities of Coimbra and Porto have been experimenting in this area and addressing these concerns. Recently, they have been engaged in a broader project, involving a large group of academics with complementary competencies. This project is aimed at developing a virtual platform directed towards the learning of Chemical Processes with a wide scope. From the functional point of view the platform is organized into four main areas: Chemical Engineering, Chemical Processes, Virtual Experiments and Simulators. The Chemical Processes area is further divided into four different sections: Unit Operations and Separations, Chemical Reaction, Process Systems Engineering and Biological Processes. These sections include simulators, applications and case studies to better understand the chemical/biochemical processes. The Virtual Experiments area considers both the laboratory visualization of the basic phenomena related to the processes in the other four sections, and the remote monitoring of laboratory experiments. This platform, constructed around a dynamic Web Portal, allows discussion forums and is also aimed at sharing experiences with other schools. This paper describes the different subjects included in the web platform, as well as the simulation strategies and the web methodologies used for its construction, and also presents examples of application in the classroom.}
}
@article{HICKMAN1995153,
title = {Advanced computational methods for spatial information extraction},
journal = {Computers & Geosciences},
volume = {21},
number = {1},
pages = {153-173},
year = {1995},
issn = {0098-3004},
doi = {https://doi.org/10.1016/0098-3004(94)00063-Z},
url = {https://www.sciencedirect.com/science/article/pii/009830049400063Z},
author = {Betty L. Hickman and Michael P. Bishop and Michael V. Rescigno},
keywords = {Spatial feature extraction, Texture features, Parallel processing, Spatial task partitioning},
abstract = {A variety of mathematical approaches for spatial information extraction using digitized aerial photography and satellite imagery have been developed and implemented on serial computers. However, because of data volume and scale, the computational demands of spatial analysis procedures frequently exceed the capacity of available serial processing technologies. One way of addressing this problem is through parallel processing in which the power of multiple computing units can be used on a single problem. In this study we investigate the utility of parallel processing for spatial feature extraction. Our testing in the situation of texture feature extraction using a cooccurrence matrix indicates that dramatic reductions in execution time are possible—an image that required about 34 min to process using one processor was solved in under 2 min using nineteen processors. The availability of additional processors could result in smaller execution times. This speedup potential is a critical element in future studies focusing on more complex spatial analysis procedures.}
}
@incollection{MILLER2017103,
title = {6 - Graduate and postgraduate education at a crossroads},
editor = {Susan M. Miller and Walter H. Moos and Barbara H. Munk and Stephen A. Munk},
booktitle = {Managing the Drug Discovery Process},
publisher = {Woodhead Publishing},
address = {Boston},
pages = {103-128},
year = {2017},
isbn = {978-0-08-100625-2},
doi = {https://doi.org/10.1016/B978-0-08-100625-2.00006-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780081006252000064},
author = {Susan M. Miller and Walter H. Moos and Barbara H. Munk and Stephen A. Munk},
keywords = {Academia, Career, Critical thinking, Diversity, Education, Graduate school, Immigration, Industry, Jobs, Learn by doing, Medicinal chemistry, Online education, Organic chemistry, Postdoctoral, Postgraduate, Master's degree, Doctorate.},
abstract = {In this chapter we introduce the proverbial crossroads we have reached in graduate and postgraduate education and jobs. Many factors are at play, including an explosion of information, available now, at your fingertips, a move away from memorization toward critical thinking, the importance of learning by doing, and what has been called “the gathering storm.” Core drug discovery disciplines are discussed, such as medicinal and organic chemistry, especially in the context of academia–industry symbiosis. Challenges in making sure we continue to assemble the best and the brightest to tackle important biomedical problems are considered. Finally, we scratch the surface of how to navigate employers, employment, and careers.}
}
@article{BUEHLER20081101,
title = {Theoretical and computational hierarchical nanomechanics of protein materials: Deformation and fracture},
journal = {Progress in Materials Science},
volume = {53},
number = {8},
pages = {1101-1241},
year = {2008},
issn = {0079-6425},
doi = {https://doi.org/10.1016/j.pmatsci.2008.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0079642508000510},
author = {Markus J. Buehler and Sinan Keten and Theodor Ackbarow},
abstract = {Proteins constitute the building blocks of biological materials such as tendon, bone, skin, spider silk or cells. An important trait of these materials is that they display highly characteristic hierarchical structures, across multiple scales, from nano to macro. Protein materials are intriguing examples of materials that balance multiple tasks, representing some of the most sustainable material solutions that integrate structure and function. Here we review progress in understanding the deformation and fracture mechanisms of hierarchical protein materials by using a materials science approach to develop structure-process-property relations, an effort defined as materiomics. Deformation processes begin with an erratic motion of individual atoms around flaws or defects that quickly evolve into formation of macroscopic fractures as chemical bonds rupture rapidly, eventually compromising the integrity of the structure or the biological system leading to failure. The combination of large-scale atomistic simulation, multi-scale modeling methods, theoretical analyses combined with experimental validation provides a powerful approach in studying deformation and failure phenomena in protein materials. Here we review studies focused on the molecular origin of deformation and fracture processes of three types of protein materials. The review includes studies of collagen – Nature’s super-glue; beta-sheet rich protein structures as found in spider silk – a natural fiber that can reach the strength of a steel cable; as well as intermediate filaments – a class of alpha-helix based structural proteins responsible for the mechanical integrity of eukaryotic cells. The article concludes with a discussion of the significance of universally found structural patterns such as the staggered collagen fibril architecture or the alpha-helical protein motif.}
}
@article{WANG1996579,
title = {The IDS model of intelligent design system},
journal = {Computers & Structures},
volume = {61},
number = {3},
pages = {579-586},
year = {1996},
issn = {0045-7949},
doi = {https://doi.org/10.1016/0045-7949(96)00054-5},
url = {https://www.sciencedirect.com/science/article/pii/0045794996000545},
author = {Xiaotong Wang},
abstract = {Existing models of intelligent design system nowadays are generally logic-based, which solve only simple and small-scale design problems. In the author's opinion, these models which concentrate only on far-fetched use of logical inference and abstract knowledge deviate from designer's thinking and decision process; the crux of the deviation is the lack of imitating thinking with mental imagery ability. Considering the nature of design problems and imitating rational thinking with alternate use of pattern association and symbolic operation, a new intelligent design system (IDS) model and its implementation techniques are presented. Imitation of thinking with mental imagery which is also called pattern association in the IDS model is considered by applying artificial neural network (ANN) techniques. The pattern association in the IDS model imitates the rule of human thinking, “comprehending by analogy”, to some extent. Because of the robustness of the pattern-type knowledge used in pattern association, IDS provides a practical way in producing a design scheme using incomplete and/or undeterminate input data, which is very difficult to achieve in general expert design systems. According to the IDS model, an intelligent structural layout design system of wing (ISDW) is developed. ISDW realizes mapping from key parameters of design requirements and the environment of the wing to the layout design of wing structure in not only graphic form, but also in readable data form. After getting a layout of wing structure, the user will modify it interactively by Auto-CAD, and then return to the ISDW environment to produce FEM meshes by an intelligent meshing interface in order to do the preliminary static and dynamic structural analysis. The design schemes created by the system proved to be proper and usable, and this concludes that IDS model is practicable and practical.}
}
@article{SHI2023926,
title = {Decoding Human Biology and Disease Using Single-cell Omics Technologies},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {21},
number = {5},
pages = {926-949},
year = {2023},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2023.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S1672022923001043},
author = {Qiang Shi and Xueyan Chen and Zemin Zhang},
keywords = {Single-cell omics, Computational method, Cellular heterogeneity, Disease, Cancer research},
abstract = {Over the past decade, advances in single-cell omics (SCO) technologies have enabled the investigation of cellular heterogeneity at an unprecedented resolution and scale, opening a new avenue for understanding human biology and disease. In this review, we summarize the developments of sequencing-based SCO technologies and computational methods, and focus on considerable insights acquired from SCO sequencing studies to understand normal and diseased properties, with a particular emphasis on cancer research. We also discuss the technological improvements of SCO and its possible contribution to fundamental research of the human, as well as its great potential in clinical diagnoses and personalized therapies of human disease.}
}
@incollection{MARKOVA2015443,
title = {Representations, Social Psychology of},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {443-449},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.24084-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868240841},
author = {Ivana Marková},
keywords = {Anchoring, Cognitive polyphasia, Common sense, Communication, Dialogicality, Ego–Alter–Object, Ethics, Figurative scheme, Imagination, Interactional epistemology, Intervention strategies, Language, Objectification, Social representations, Themata},
abstract = {The theory of social representations studies the formation and transformation of meanings and activities of complex social phenomena like health and illness, political problems or environmental issues in and through language and communication, history and culture. There are two mutually interdependent meanings of social representations. The first meaning concerns the theory of social representations as an interactional theory of knowledge. It refers to networks of concepts and figurative schemes that are generated in and through tradition, common sense, daily knowledge, and communication; these are shared by particular groups and communities. The main features of this theory are the Ego–Alter–Object, the field, the interdependence of asymmetries and symmetries, ethics, figurative scheme, and cognitive polyphasia. Second, social representations refer to concrete social phenomena and to forms of apprehending and creating social realities in and through communication, experience, social practices, and interventions. Human thinking is characterized by the capacity to make distinctions and understand phenomena as dyadic antinomies or themata. Thematization of dyadic antinomies is linked with anchoring and objectification, through which social representations are formed and transformed.}
}
@article{KOSIKOV2021492,
title = {Data Enrichment in the Information Graphs Environment Based on a Specialized Architecture of Information Channels},
journal = {Procedia Computer Science},
volume = {190},
pages = {492-499},
year = {2021},
note = {2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921013922},
author = {Sergey Kosikov and Larisa Ismailova and Viacheslav Wolfengagen},
keywords = {data enrichment, information channels, conceptual constructions, informational graph, applicative computations, semantics},
abstract = {The paper considers the possibility of constructing a specialized computing system oriented at the transmission of data through information channels, that are determined taking into account the semantics of the selected data. In the process of computations the data is connected with semantic characteristics that describe the channel of computations, which can be considered as a method of semantic data enrichment. The system of information channels as a whole can be considered as an information graph describing the structuring of the processed data. The information graph supports the data model in the form of a network, the framework of which are objects and the relationships between them. The paper proposes language tools for determining the information graph and interpretation tools that provide practical computations. The set of information channels that make up the information graph can be considered as a low-level tool for data enrichment. The paper studies the possibility of determining tools of higher level. An applicative type language is proposed for defining information graphs, the syntax and semantics of the language are specified. The proposed language can be considered as an intermediate level tool for defining semantics. A procedure is proposed for compiling the language into a low-level construct, preserving the semantics of the language. The supporting system for the proposed computing system includes a low-level language interpreter, as well as an intermediate-level language compiler into a low-level language. The supporting system is implemented in an applicative programming environment. Some elements of the supporting system were tested when developing applied information systems in the field of jurisprudence.}
}
@incollection{HALFORD2020327,
title = {Cognitive Developmental Theories☆},
editor = {Janette B. Benson},
booktitle = {Encyclopedia of Infant and Early Childhood Development (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {327-336},
year = {2020},
isbn = {978-0-12-816511-9},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.05787-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093245057874},
author = {G.S. Halford},
keywords = {Analogy, Cognitive complexity, Conceptual chunking, Dynamic systems, Information processing, Mental models, Neural net, Object permanence, Relational knowledge, Symbolic processes, Theory of mind, Working memory},
abstract = {Theories of cognitive development are reviewed, beginning with pioneering theories by Piaget and Vygotsky. Neo-Piagetian theories which integrated Piagetian theory with other conceptions of cognition were developed by McLaughlin, Pascual-Leone, Case, Fischer, and Chapman. Complexity theories propose that children become capable of dealing with more complex relations as they develop. Information processing theories, neural net theories, dynamic systems theories, and theories of reasoning processes all provide models of the reasoning processes employed by children at different ages. Microgenetic analysis methods are used to study the processes of transition from one level of thinking to the next. Conceptual coherence is achieved by categorizing cognitive processes according to their core properties.}
}
@article{WILKINSON2013394,
title = {The past and the future of business marketing theory},
journal = {Industrial Marketing Management},
volume = {42},
number = {3},
pages = {394-404},
year = {2013},
note = {Theoretical Perspectives in Industrial Marketing Management},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2013.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0019850113000266},
author = {Ian F. Wilkinson and Louise C. Young},
keywords = {Complex adaptive systems, Business relations and networks, Dynamics and evolution, Agent based models, Mechanisms},
abstract = {A complex systems approach to understanding and modelling business marketing systems is described. The focus is on the dynamics and evolution of such systems and the processes and mechanisms driving this, rather than the more usual comparative static, variables based statistical models. Order emerges in a self-organising, bottom up way from the local or micro actions and interactions of those involved. We describe the development of our thinking regarding this approach and its main features, including the development of agent based simulation models and the identification and modelling of underlying mechanisms and processes. We conclude by discussing the implications of this approach for business marketing theory and research.}
}
@article{YANG2020119,
title = {A multilevel neighborhood sequential decision approach of three-way granular computing},
journal = {Information Sciences},
volume = {538},
pages = {119-141},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.05.060},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520304734},
author = {Xin Yang and Tianrui Li and Dun Liu and Hamido Fujita},
keywords = {Three-way granular computing, Sequential three-way decision, Neighborhood, Multilevel},
abstract = {The fusion of three-way decision and granular computing provides powerful ideas and methods to understand and solve the problems of cognitive science by thinking and information processing in threes. As a typical representation of three-way granular computing, sequential three-way decision focuses on making a multiple stages of decisions by a sequence of trisecting-acting-outcome (TAO) models. To construct more general granules, levels, and hierarchies, we investigate an integrative multi-granularity approach to sequential three-way decision in a neighborhood system by the evolution mechanism of data and parameters. We employ the γ-cut similarity neighborhood relation based on Gaussian kernel function to the hierarchical granulation of universe. Subsequently, we propose the multilevel neighborhood granular structures by the combinations of horizontal granularity and vertical granularity, and discuss the monotonicity of level measurements associated with the uncertainty of decision. Based on such a neighborhood structured approach, a multilevel framework of sequential three-way decision is examined from coarser to finer concerning the granularity of neighborhood information. Finally, we report a series of experiments to demonstrate the performance of proposed models and algorithms.}
}
@article{GUPTA19941,
title = {On the principles of fuzzy neural networks},
journal = {Fuzzy Sets and Systems},
volume = {61},
number = {1},
pages = {1-18},
year = {1994},
issn = {0165-0114},
doi = {https://doi.org/10.1016/0165-0114(94)90279-8},
url = {https://www.sciencedirect.com/science/article/pii/0165011494902798},
author = {M.M. Gupta and D.H. Rao},
keywords = {Fuzzy logic, neural networks, fuzzy neural networks, confluence operation, synpatic and somatic operations},
abstract = {Over the last decade or so, significant advances have been made in two distinct technological areas: fuzzy logic and computational neutral networks. The theory of fuzzy logic provides a mathematical framework to capture the uncertainties associated with human cognitive processes, such as thinking and reasoning. Also, it provides a mathematical morphology to emulate certain perceptual and linguistic attributes associated with human cognition. On the other hand, the computational neural network paradigms have evolved in the process of understanding the incredible learning and adaptive features of neuronal mechanisms inherent in certain biological species. Computational neural networks replicate, on a small scale, some of the computational operations observed in biological learning and adaptation. The integration of these two fields, fuzzy logic and neural networks; has given birth to an emerging technological field — the fuzzy neural networks. The fuzzy neural networks have the potential to capture the benefits of the two fascinating fields, fuzzy logic and neural networks, into a single capsule. The intent of this tutorial paper is to describe the basic notions of biological and computational neuronal morphologies, and to describe the principles and architectures of fuzzy neural networks. Towards this goal, we develop a fuzzy neural architecture based upon the notion of T-norm and T-conorm connectives. An error-based learning scheme is described for this neural structure.}
}
@incollection{GOI2024353,
title = {13 - Perspective on photonic neuromorphic computing},
editor = {Min Gu and Elena Goi and Yangyundou Wang and Zhengfen Wan and Yibo Dong and Yuchao Zhang and Haoyi Yu},
booktitle = {Neuromorphic Photonic Devices and Applications},
publisher = {Elsevier},
pages = {353-375},
year = {2024},
series = {Photonic Materials and Applications Series},
isbn = {978-0-323-98829-2},
doi = {https://doi.org/10.1016/B978-0-323-98829-2.00009-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323988292000098},
author = {Elena Goi and Min Gu},
keywords = {Neuromorphic photonics, photonic memories, all-optical AI microscopy, hybrid platforms},
abstract = {Bioinspired neuromorphic algorithms can process information more rapidly and more accurately than conventional algorithms, in the attempt to achieve brain-like capacity and efficiency in tasks that are challenging for traditional computers but easy for humans. With the development of applications more performing than ever, the computational requirements for running neuromorphic models are increasing exponentially, motivating efforts to develop new, specialized hardware for fast and efficient execution. Neuromorphic photonics, the implementation of neuromorphic information processing with optoelectronic hardware, is a new computational paradigm based on photons aiming to achieve brain-like information processing in the optical domain, and an interdisciplinary field that is expanding in a multitude of directions. In this chapter, we first revise what we believe are currently the main theoretical and technical challenges in the field and then give a broad perspective on the new directions and opportunities that, in our opinion, represent the current frontiers of neuromorphic photonics.}
}
@article{VANCOUVER20081,
title = {Integrating self-regulation theories of work motivation into a dynamic process theory},
journal = {Human Resource Management Review},
volume = {18},
number = {1},
pages = {1-18},
year = {2008},
issn = {1053-4822},
doi = {https://doi.org/10.1016/j.hrmr.2008.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1053482208000028},
author = {Jeffrey B. Vancouver},
keywords = {Self-regulation, Control theory, Goals, Computational modeling, Dynamic processes},
abstract = {Instead of merely combining theories of self-regulation, the current paper articulates a dynamic process theory of the underlying cognitive subsystems that explain relationships among long-used constructs like goals, expectancies, and valence. Formal elements of the theory are presented in an attempt to encourage the building of computational models of human actors, thinkers, and learners in organizational contexts. Discussion focuses on the application of these models for understanding the dynamics of individuals interacting in their organizations.}
}
@article{LIN2022104649,
title = {Towards a cross-level understanding of Bayesian inference in the brain},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {137},
pages = {104649},
year = {2022},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2022.104649},
url = {https://www.sciencedirect.com/science/article/pii/S0149763422001385},
author = {Chin-Hsuan Sophie Lin and Marta I. Garrido},
keywords = {Probabilistic inference, Bayesian decision theory, Uncertainty, Sampling, Variational approximation, Neural codes, Marr’s level of analysis},
abstract = {Perception emerges from unconscious probabilistic inference, which guides behaviour in our ubiquitously uncertain environment. Bayesian decision theory is a prominent computational model that describes how people make rational decisions using noisy and ambiguous sensory observations. However, critical questions have been raised about the validity of the Bayesian framework in explaining the mental process of inference. Firstly, some natural behaviours deviate from Bayesian optimum. Secondly, the neural mechanisms that support Bayesian computations in the brain are yet to be understood. Taking Marr’s cross level approach, we review the recent progress made in addressing these challenges. We first review studies that combined behavioural paradigms and modelling approaches to explain both optimal and suboptimal behaviours. Next, we evaluate the theoretical advances and the current evidence for ecologically feasible algorithms and neural implementations in the brain, which may enable probabilistic inference. We argue that this cross-level approach is necessary for the worthwhile pursuit to uncover mechanistic accounts of human behaviour.}
}
@article{BOUDIN2016448,
title = {Opinion dynamics: Kinetic modelling with mass media, application to the Scottish independence referendum},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {444},
pages = {448-457},
year = {2016},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2015.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S0378437115008602},
author = {Laurent Boudin and Francesco Salvarani},
keywords = {Opinion formation, Mass media, Kinetic equations},
abstract = {We consider a kinetic model describing some mechanisms of opinion formation in the framework of referendums, where the individuals, who can interact between themselves and modify their opinion by means of spontaneous self-thinking, are moreover under the influence of mass media. We study, at the numerical level, both the transient and the asymptotic regimes. In particular, we point out that a plurality of media, with different orientations, is a key ingredient to allow pluralism and prevent consensus. The forecasts of the model are compared to some surveys related to the Scottish independence referendum of 2014.}
}
@article{LIGABO2023102155,
title = {Practical way to apply fourth-generation assessment tools integrated into creating meaningful learning experiences in biology at high school},
journal = {Evaluation and Program Planning},
volume = {96},
pages = {102155},
year = {2023},
issn = {0149-7189},
doi = {https://doi.org/10.1016/j.evalprogplan.2022.102155},
url = {https://www.sciencedirect.com/science/article/pii/S0149718922001094},
author = {Mateus Ligabo and Fabiana Carvalho Silva and Ana Carolina da S.A. Carvalho and Durval Rodrigues and Rita C.L.B. Rodrigues},
keywords = {Concept maps, Meaningful learning, Hermeneutic-dialectic circle, Hofstede's cultural dimensions, Fourth-generation assessment},
abstract = {The learning process for a Biology topic regarding organisms and animal kingdom diversity was investigated through an innovative Interactive Didactic Sequence (IDS) which integrated the idea of “concept maps” with the Hermeneutic-Dialectic Circle (HDC). HDC is a tool for data collection and a reference for pluralist-constructivist thinking, considered a form of fourth-generation evaluation. Hofstede's cultural dimensions were also integrated into the investigation in order to facilitate mediation in an evaluative context. Students' performances (N = 25) from a São Paulo-Brazil public school were statistically evaluated. Their cultural profile was determined via the Hofstede Value Survey Model 1994 questionnaire. The elaborative process of arranging concept maps was individual (CM-individual) and integrated with HDC in groups (CM-HDC). Concept map assessment methods were based off existing literature. An improvement in students' performances (p < 0.05) that presented concept maps integrated to HDC in a more complex structure when compared to individually-built maps was observed. Employment of HDC helped form motivational/interactive dialogues between students and teachers, which, in turn, assisted in achieving greater learning through the use of concept maps. The application of the fourth-generation evaluation was improved via knowledge regarding students' cultural profiles.}
}
@article{GIORGI2024119928,
title = {Embedding parametric resonance in a 2:1 wave energy converter to get a broader bandwidth},
journal = {Renewable Energy},
volume = {222},
pages = {119928},
year = {2024},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2023.119928},
url = {https://www.sciencedirect.com/science/article/pii/S0960148123018438},
author = {Giuseppe Giorgi},
keywords = {2:1 parametric resonance, Parametric instability, Wave energy converter, Nonlinear Froude–Krylov force},
abstract = {The effort to increase the converted power is a common challenge to players in the field of wave energy conversion, both academic and industrial. In the case devices are found to be prone to parametric resonance, it typically has a negative impact on power harvesting and may jeopardize the reliability of the device. This paper makes the case that parametric resonance is not a danger that should be avoided, but rather a chance to achieve a broader system response bandwidth and ultimately increase the amount of power available at the power take-off. Since a time-varying wetted surface causes the highly nonlinear phenomenon of parametric resonance, linear models are unable to fully capture this instability. As a result, nonlinear Froude–Krylov forces are herein implemented via a computationally effective method for prismatic floaters that is compatible with both exhaustive simulation methods and real-time computing, as the whole simulations runs up to 50 times faster than real-time. A novel pendulum-based device is intentionally defined to exhibit a 2:1 ratio between heave and pitch natural frequencies, causing parametric instability. Results demonstrate that linear models predict a single zone of meaningful potential power extraction around the pitch natural frequency, as expected; however, by using the designed attitude to develop parametric instability, a second additional region develops near the heave natural period. As a result, the free response bandwidth is in fact increased, making more energy available at the power take-off axis thanks to the nonlinear instability embedded in the wave energy converter.}
}
@article{HE2023112111,
title = {Predicting thermodynamic stability of magnesium alloys in machine learning},
journal = {Computational Materials Science},
volume = {223},
pages = {112111},
year = {2023},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2023.112111},
url = {https://www.sciencedirect.com/science/article/pii/S0927025623001052},
author = {Xi He and Jinde Liu and Chen Yang and Gang Jiang},
keywords = {Machine learning, DFT, Thermodynamic stability, Magnesium alloy},
abstract = {Density functional theory (DFT) have been widely used to screen thermodynamically stable material; however, its high computational cost limits its use. In this paper, we explore the use of DFT data from high-throughput calculations to create faster machine learning (ML) models that can be used to screen thermodynamically stable magnesium alloy materials. Our methods work by utilizing the kernel ridge regression (KRR) algorithm, as well as Deep Potential Molecular Dynamics (DeePMD) to train ML models for predicting the formation energy of magnesium alloys. The accuracy, stability, and generalization ability of the ML models created under both methods are evaluated in detail. Meanwhile, we have conducted in-depth comparative analysis of the two methods, which concluded that the accuracy of DeePMD model performs better and time efficiency of KRR model has more advantages. The results show that the best performing DeePMD model and KRR model achieve the RMSE of 0.43 meV/atom and 6.80 meV/atom, indicating that our methods provide a reliable idea for obtaining the formation energy of magnesium alloys.}
}
@article{MILLNER2020704,
title = {Advancing the Understanding of Suicide: The Need for Formal Theory and Rigorous Descriptive Research},
journal = {Trends in Cognitive Sciences},
volume = {24},
number = {9},
pages = {704-716},
year = {2020},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2020.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364661320301480},
author = {Alexander J. Millner and Donald J. Robinaugh and Matthew K. Nock},
keywords = {suicide, suicide theory, formal models},
abstract = {Suicide is a leading cause of death worldwide and perhaps the most puzzling and devastating of all human behaviors. Suicide research has primarily been guided by verbal theories containing vague constructs and poorly specified relationships. We propose two fundamental changes required to move toward a mechanistic understanding of suicide. First, we must formalize theories of suicide, expressing them as mathematical or computational models. Second, we must conduct rigorous descriptive research, prioritizing direct observation and precise measurement of suicidal thoughts and behaviors and of the factors posited to cause them. Together, theory formalization and rigorous descriptive research will facilitate abductive theory construction and strong theory testing, thereby improving the understanding and prevention of suicide and related behaviors.}
}
@article{GOECKE2020101470,
title = {Testing competing claims about overclaiming},
journal = {Intelligence},
volume = {81},
pages = {101470},
year = {2020},
issn = {0160-2896},
doi = {https://doi.org/10.1016/j.intell.2020.101470},
url = {https://www.sciencedirect.com/science/article/pii/S0160289620300489},
author = {B. Goecke and S. Weiss and D. Steger and U. Schroeders and O. Wilhelm},
keywords = {Overclaiming, Declarative knowledge, Self-reported knowledge, Creativity, Intelligence, Faking},
abstract = {Overclaiming has been described as people's tendency to overestimate their cognitive abilities in general and their knowledge in particular. We discuss four different perspectives on the phenomenon of overclaiming that have been proposed in the research literature: Overclaiming as a result of a) self-enhancement tendencies, b) as a cognitive bias (e.g., hindsight bias, memory bias), c) as proxy for cognitive abilities, and d) as sign of creative engagement. Moreover, we discuss two different scoring methods for an OCQ (signal detection theory vs. familiarity ratings). To distinguish between the different viewpoints of what overclaiming is, we juxtaposed overclaiming, as indicated by claiming familiarity with non-existent terms, with fluid and crystallized intelligence, self-reported knowledge, creativity, faking ability, and personality. Overclaiming was measured with a newly comprised overclaiming questionnaire. Results of several latent variable analyses based upon a multivariate study with 298 participants were: First, overclaiming is neither predicted by honesty-humility nor faking ability and therefore reflects something different than mere self-enhancement tendencies. Second, overclaiming is not predicted by crystallized intelligence, but is highly predictive of self-reported knowledge and, thus, not suitable as an index or a proxy for cognitive abilities. Finally, overclaiming is neither related to divergent thinking and originality, and only moderately predicted by self-reported openness creativity from the HEXACO which means that overclaiming does not reflect creative ability. In sum, our results favor an interpretation of overclaiming as a phenomenon that requires more than self-enhancement motivation, in contrast to the claim that was initially proposed in the literature.}
}
@incollection{BROWN201589,
title = {Space, Linguistic Expression of},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {89-93},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.57017-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868570172},
author = {Penelope Brown},
keywords = {Adpositions, Language and cognition, Language universals, Locative constructions, Motion verbs, Space, Spatial frames of reference, Topological language},
abstract = {Spatial cognition is central to human thinking, and spatial language is thus an important area of study, as it may reveal fundamental properties of human thought. Recent research has shown that spatial language is much more divergent across languages than had previously been thought, suggesting significant cultural patterning of spatial conceptualization. This article reviews spatial language cross-linguistically, sets out a typological framework for the language of space, and considers the relationship of spatial language to spatial cognition, in the context of extensive linguistic diversity in the spatial domain.}
}
@article{LITT1993459,
title = {Single neuron computation: T. McKenna, J. Davis and S.F. Zornetzer (Eds.) (Academic Press, San Diego, CA, 1992, 664 p., Price US $59.95)},
journal = {Electroencephalography and Clinical Neurophysiology},
volume = {87},
number = {6},
pages = {459-460},
year = {1993},
issn = {0013-4694},
doi = {https://doi.org/10.1016/0013-4694(93)90160-W},
url = {https://www.sciencedirect.com/science/article/pii/001346949390160W},
author = {Brian Litt}
}
@article{KNOWLTON2012373,
title = {A neurocomputational system for relational reasoning},
journal = {Trends in Cognitive Sciences},
volume = {16},
number = {7},
pages = {373-381},
year = {2012},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2012.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1364661312001283},
author = {Barbara J. Knowlton and Robert G. Morrison and John E. Hummel and Keith J. Holyoak},
abstract = {The representation and manipulation of structured relations is central to human reasoning. Recent work in computational modeling and neuroscience has set the stage for developing more detailed neurocomputational models of these abilities. Several key neural findings appear to dovetail with computational constraints derived from a model of analogical processing, ‘Learning and Inference with Schemas and Analogies’ (LISA). These include evidence that (i) coherent oscillatory activity in the gamma and theta bands enables long-distance communication between the prefrontal cortex and posterior brain regions where information is stored; (ii) neurons in prefrontal cortex can rapidly learn to represent abstract concepts; (iii) a rostral-caudal abstraction gradient exists in the PFC; and (iv) the inferior frontal gyrus exerts inhibitory control over task-irrelevant information.}
}
@incollection{HEGARTY2010265,
title = {Chapter 7 - Components of Spatial Intelligence},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {52},
pages = {265-297},
year = {2010},
booktitle = {The Psychology of Learning and Motivation},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(10)52007-3},
url = {https://www.sciencedirect.com/science/article/pii/S0079742110520073},
author = {Mary Hegarty},
abstract = {This chapter identifies two basic components of spatial intelligence, based on analyses of performance on tests of spatial ability and on complex spatial thinking tasks in domains such as mechanics, chemistry, medicine, and meteorology. The first component is flexible strategy choice between mental imagery (or mental simulation more generally) and more analytic forms of thinking. Research reviewed here suggests that mental simulation is an important strategy in spatial thinking, but that it is augmented by more analytic strategies such as task decomposition and rule-based reasoning. The second is meta-representational competence [diSessa, A. A. (2004). Metarepresentation: Native competence and targets for instruction. Cognition and Instruction, 22, 293–331], which encompasses ability to choose the optimal external representation for a task and to use novel external representations productively. Research on this aspect of spatial intelligence reveals large individual differences in ability to adaptively choose and use external visual–spatial representations for a task. This research suggests that we should not just think of interactive external visualizations as ways of augmenting spatial intelligence, but also consider the types of intelligence that are required for their use.}
}
@article{TONNANG2022100964,
title = {Advances in data-collection tools and analytics for crop pest and disease management},
journal = {Current Opinion in Insect Science},
volume = {54},
pages = {100964},
year = {2022},
issn = {2214-5745},
doi = {https://doi.org/10.1016/j.cois.2022.100964},
url = {https://www.sciencedirect.com/science/article/pii/S2214574522000992},
author = {Henri EZ Tonnang and Daisy Salifu and Bester T Mudereri and Joel Tanui and Andrew Espira and Thomas Dubois and Elfatih M Abdel-Rahman},
abstract = {Innovative methods in data collection and analytics for pest and disease management are advancing together with computational efficiency. Tools, such as the open-data kit, research electronic data capture, fall armyworm monitoring, and early warning- system application and remote sensing have aided the efficiency of all types of data collection, including text, location, images, audio, video, and others. Concurrently, data analytics have also evolved with the application of artificial intelligence and machine learning (ML) for early warning and decision-support systems. ML has repeatedly been used for the detection, diagnosis, modeling, and prediction of crop pests and diseases. This paper thus highlights the innovations, implications, and future progression of these technologies for sustainability.}
}
@article{SCHWARZ201359,
title = {Business wargaming for teaching strategy making},
journal = {Futures},
volume = {51},
pages = {59-66},
year = {2013},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2013.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0016328713000864},
author = {Jan Oliver Schwarz},
keywords = {Business wargaming, Teaching, Simulation, Management education, Strategy making, Strategic thinking},
abstract = {An increasingly complex and dynamic business environment requires new approaches to teaching strategy to management students. Business wargaming, a dynamic strategic simulation, is discussed as a management simulation which can respond to the contemporary challenges in management education. Reflecting on the practical use of business wargaming in the classroom, it is described how such simulations prepare management students for making strategic decisions in complex and dynamic environments characterised by high uncertainty concerning the future.}
}
@article{KAZEMI2002203,
title = {Exploring test performance in mathematics: the questions children’s answers raise},
journal = {The Journal of Mathematical Behavior},
volume = {21},
number = {2},
pages = {203-224},
year = {2002},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(02)00118-9},
url = {https://www.sciencedirect.com/science/article/pii/S0732312302001189},
author = {Elham Kazemi},
keywords = {Children’s thinking, Mathematical performance, Interpreting problems, Testing},
abstract = {This article investigates children’s mathematical performance on test items, specifically multiple-choice questions. Using interviews with 90 fourth-graders, it reveals why particular kinds of items are more or less difficult for students. By using multiple-choice questions and juxtaposing them with similar open-ended problems, the findings underscore the costs of not attending to children’s thinking in designing and interpreting problems. The data from this study suggest that when answering multiple-choice questions, students’ attention is drawn to the choices themselves. They do not necessarily think through the problem first and thus make their choices based on (often incorrect) generalizations they have made about problem-solving. Whether students answered a multiple-choice question or a similar open-ended problem first impacted both their performance and their reasoning. Moreover, children draw on their life experiences when the context of the problem is salient, thus ignoring important parameters of the stated problem. Implications for investigating children’s thinking, instruction, and test design are discussed.}
}
@incollection{OXMAN2001269,
title = {Chapter 12 - The Mind in Design: A Conceptual Framework for Cognition in Design Education},
editor = {Charles M. Eastman and W. Michael McCracken and Wendy C. Newstetter},
booktitle = {Design Knowing and Learning: Cognition in Design Education},
publisher = {Elsevier Science},
address = {Oxford},
pages = {269-295},
year = {2001},
isbn = {978-0-08-043868-9},
doi = {https://doi.org/10.1016/B978-008043868-9/50012-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780080438689500127},
author = {Rivka Oxman},
abstract = {Publisher Summary
This chapter considers the role of cognitive content of design and design thinking as a basis for developing an educational approach. Various design researchers discussed cognitive approaches in design, and the role of knowledge and representations as a cognitive design-thinking tool. Most of these studies are related directly to design and design thinking rather than to the learning task in design learning and design education. Irrespective of the specific design domain, traditional educational models in design education are based upon the replication of professional-task performance. The measure of learning is generally equated with the evaluation of the product of designing rather than on what might be considered a learning increment. The cognitive properties of design learning have never been the subject of design education. As a consequence, there presently exists a lack of educational theories of learning that function as an underpinning of design education. It is now possible to demonstrate that the derivation of design knowledge through constructive processes, in itself, provides a medium for design learning. This chapter suggests that special design learning environments must be developed to enhance and supplement formal education and foster personal development in design learning.}
}
@article{KIM201716,
title = {A study on metadata structure and recommenders of biological systems to support bio-inspired design},
journal = {Engineering Applications of Artificial Intelligence},
volume = {57},
pages = {16-41},
year = {2017},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2016.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0952197616301786},
author = {Sun-Joong Kim and Ji-Hyun Lee},
keywords = {Bio-inspired design, Biological system metadata modeling, Knowledge-based system, Recommendation system, Ontology},
abstract = {Bio-inspired design was introduced as an alternative method to encourage breakthrough innovations during design projects by stimulating analogical reasoning and thinking of designers. However, the method did not perform as well as researchers expected because most designers, who are novices in the fields of biology and ecology, cannot infer the proper analogue (i.e. biological system) from nature. To resolve this fundamental problem, a causal model based representation framework for ‘analogical reasoning’ – searching and selecting the biological systems to apply – have been developed. In addition, ontology based repository structures and retrieval systems have been proposed to support ‘analogical thinking’ of designers. Nevertheless, these systematic approaches still restrict the candidates and inevitably lose potential biological systems relevant to the design project, due to the ‘physical relation’ biased problem and the ambiguity of the indexing mechanism of both current representation frameworks and retrieval systems. For example, the causality based support system known as a robust representation framework for a single biological system, stores information of a biological system only by its internal ‘physical relations’ and retrieves biological systetabms only by the physical relevance. However, from the perspective of ecological thinking, the further relatedness of ‘physical, biological, and ecological relations’ composes the holistic concept used to identify an organism in the flow of evolution because the ‘biological and ecological relations’ are also involved in the traits that designers may be interested in. Therefore, the supplementary information for ‘biological and ecological relations’ must be added to index the biological and environmental interactions, and to use the connectivity among entire organisms in the retrieval process. In this research, a causality based holistic representation framework for biological systems and an ‘all-connected’ ontology based repository and retrieval system are developed as a knowledge-based recommendation system to support bio-inspired design. The knowledge-based system we developed allows engineering designers to search and select a particular biological system and extract design strategy without much biological knowledge. This effort provides more opportunities in a bio-inspired design process by adding potential biological systems that might previously not have been considered.}
}
@article{DEAN2020482,
title = {Deep into that darkness peering: A computational analysis of the role of depression in Edgar Allan Poe's life and death},
journal = {Journal of Affective Disorders},
volume = {266},
pages = {482-491},
year = {2020},
issn = {0165-0327},
doi = {https://doi.org/10.1016/j.jad.2020.01.098},
url = {https://www.sciencedirect.com/science/article/pii/S0165032719322554},
author = {Hannah J. Dean and Ryan L. Boyd},
keywords = {Edgar Allan Poe, LIWC, Depression, Suicide, Digital humanities},
abstract = {Background
To help shed light on the peculiar circumstances surrounding the death of the famed macabre and mystery writer, poet, editor, and literary critic, we explored the potential role of depression in the life and death of Edgar Allan Poe via his written language.
Method
Using computerized language analysis, we analyzed works from Poe's corpora of personal letters (N = 309), poems (N = 49), and short stories (N = 63), and investigated whether a pattern of linguistic cues consistent with depression and suicidal cognition were discernible throughout the writer's life, particularly in his final years. Building on past work, language scores were collapsed into a composite depression metric for each text. Data from each work type was subsequently compiled and graphed into a single plot by year, with scores exceeding the 95th percentile (p < 0.05) considered statistically significant and treated as potential depressive episodes.
Results
Significant, consistent patterns of depression were not found and do not support suicide as a cause of death. However, linguistic evidence was found suggesting the presence of several potential depressive episodes over the course of Poe's life – these episodes were the most pronounced during years of Poe's greatest success, as well as those following the death of his late wife.
Limitations
Given the sampling method, it is not possible to establish direct causality; results should be considered informed but tentative.
Conclusion
This investigation demonstrates the utility of language analysis for capturing disruptive/maladaptive emotional responses to life events.}
}