@article{CAO2023140,
title = {A data and knowledge-jointly driven multimodal intelligent system for enterprise culture assessment},
journal = {Alexandria Engineering Journal},
volume = {83},
pages = {140-147},
year = {2023},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2023.08.083},
url = {https://www.sciencedirect.com/science/article/pii/S1110016823007792},
author = {Ruihong Cao},
keywords = {Data and knowledge drive together, Multimode, Campus culture, Intelligent evaluation},
abstract = {Based on the rapid development of Internet new generation information technology, campus culture has become an important force to promote educational innovation. However, the uneven quality of campus culture teaching is also increasingly exposed. Therefore, we propose a set of campus culture intelligent quality assessment feedback system driven by data and knowledge. Based on real multi-modal campus culture data, the evaluation system covers different online teaching forms, and can comprehensively consider and measure the core elements of quality evaluation from the perspective of teachers and students, so as to achieve accurate evaluation and feedback of campus culture. This paper proposes to combine emotion computing with big data analysis, construct a theoretical framework of multi-modal emotion analysis from the perspective of multi-spatial fusion, and explore the students' learning emotion recognition and intelligent assessment based on multi-modal data fusion. By studying the key issues, logical path and implementation path of multimodal learning emotion analysis in campus culture, this paper reveals the mechanism of student emotion development from the perspective of campus culture, expands the intervention practice of student group emotion, and provides technical support and action guidance for after-school teaching and research on campus culture.}
}
@article{NOROOZI201279,
title = {Argumentation-Based Computer Supported Collaborative Learning (ABCSCL): A synthesis of 15 years of research},
journal = {Educational Research Review},
volume = {7},
number = {2},
pages = {79-106},
year = {2012},
issn = {1747-938X},
doi = {https://doi.org/10.1016/j.edurev.2011.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S1747938X11000522},
author = {Omid Noroozi and Armin Weinberger and Harm J.A. Biemans and Martin Mulder and Mohammad Chizari},
keywords = {Argumentation, Argumentative knowledge construction, Collaborative argumentation, Computer-Supported Collaborative Learning, Argumentation-Based Computer Supported Collaborative Learning},
abstract = {Learning to argue is an essential objective in education; and online environments have been found to support the sharing, constructing, and representing of arguments in multiple formats for what has been termed Argumentation-Based Computer Supported Collaborative Learning (ABCSCL). The purpose of this review is to give an overview of research in the field of ABCSCL and to synthesize the findings. For this review, 108 publications (89 empirical studies and 19 conceptual papers) on ABCSCL research dating from 1995 through 2011 were studied to highlight the foci of the past 15 years. Building on Biggs’ (2003) model, the ABCSCL publications were systematically categorized with respect to student prerequisites, learning environment, processes, and outcomes. Based on the quantitative and qualitative findings, this paper concludes that ABCSCL environments should be designed in a systematic way that takes the variety of specific conditions for learning into account. It also offers suggestions for educational practice and future research.}
}
@article{PFOTENHAUER201638,
title = {Architecting complex international science, technology and innovation partnerships (CISTIPs): A study of four global MIT collaborations},
journal = {Technological Forecasting and Social Change},
volume = {104},
pages = {38-56},
year = {2016},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2015.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0040162515004102},
author = {Sebastian M. Pfotenhauer and Danielle Wood and Dan Roos and Dava Newman},
keywords = {Innovation policy, Research collaboration, Regional development, University partnerships, System architecture, Policy design, MIT, International partnerships},
abstract = {Complex international partnerships have emerged as a policy instrument of choice for many governments to build domestic capacity in science, technology and innovation with the help of foreign partners. At present, these flagship initiatives tend to be primarily practitioner-driven with limited systematic understanding of available design options and trade-offs. Here, we present an analysis of four such partnerships from the university sector between the Massachusetts Institute of Technology (MIT) and governments in the UK, Portugal, Abu Dhabi, and Singapore. Using a system architecture approach in conjunctions with in-depth case studies and elements of interpretive policy analysis, we map how in each country distinct capacity-building goals, activities, and political and institutional contexts translate into different partnership architectures: a bilateral hub-&-spokes architecture (UK), a consortium architecture (Portugal), an institution-building architecture (Abu Dhabi), and a functional expansion architecture (Singapore). Despite these differences in emergent macro-architectures, we show that each partnership draws on an identical, limited set of ‘forms’ that can by organized around four architectural views (education, research, innovation & entrepreneurship, institution-building) and four levels of interaction between partners (people, programs/projects, objects, organization/process). Based on our analysis, we derive a design matrix that can help guide the development future partnerships through a systematic understanding of available design choices. Our research underscores the utility and flexibility of complex international partnerships as systemic policy instruments. It suggests a greater role for global research universities in capacity-building and international development, and emphasizes the potential of targeted cross-border funding. Our research also demonstrates the analytic power of system architecture for policy analysis and design. We argue that architectural thinking provides a useful stepping stone for STS-type interpretive policy analysis into national innovation initiatives in different political cultures, as well as more custom-tailored approaches to program evaluation.}
}
@article{GOMEZIGLESIAS201828,
title = {Performance evaluation of the three-point angular correlation function},
journal = {Parallel Computing},
volume = {76},
pages = {28-41},
year = {2018},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2018.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167819118301236},
author = {Antonio Gómez-Iglesias and Miguel Cárdenas-Montes},
keywords = {Performance, Benchmarking, Atom, ARM, Xeon, FPGA, GPU},
abstract = {In recent years, we have observed an increase in the diversity of the processors ecosystem. Different designs and architectures are being studied based on their performance and power characteristics. While using benchmarks for this purpose allows for reproducibility and easy understanding of the results, using real scientific applications allows researchers to realize the actual implications of each design on the overall performance of their codes. This paper analyzes the performance of different implementations of a three-point angular correlation function. This function is used in the study of Large-Scale Distribution of galaxies in a variety of computational platforms. The function is based on histogram construction and presents a large computational cost. This cost dramatically increases with the size of the datasets. We have considered two different GPUs, a set of x86 Intel machines (multi- and many-core), ARM chipsets, as well as an FPGA. We first study the best possible implementation for each platform in terms of time to solution. We then compare the power used by those platforms for a predefined number of datasets. Energy is one of the main constraints that computer architects are facing nowadays. The results will be used to evaluate the performance of this function considering those two targets – time and energy – for those platforms and to analyze the suitability of each of those platforms for this specific problem.}
}
@article{DENG2020117709,
title = {Numerical investigation of premixed methane-air flame in two-dimensional half open tube in the early stages},
journal = {Fuel},
volume = {272},
pages = {117709},
year = {2020},
issn = {0016-2361},
doi = {https://doi.org/10.1016/j.fuel.2020.117709},
url = {https://www.sciencedirect.com/science/article/pii/S0016236120307043},
author = {Haoxin Deng and Mingming Huang and Xiaoping Wen and Guoyan Chen and Fahui Wang and Zhifeng Yao},
keywords = {Premixed flame, Tulip flame formation, Computational fluid dynamics, Two-dimensional tube, Flame surface area},
abstract = {Premixed methane-air flame propagation in a two-dimensional half-open tube is studied using a computational fluid dynamic method. The flame is simulated using a laminar combustion model with simplified reaction mechanism. The first purpose of this work is to determine whether a 2D planar flame model can be used to predict flame propagation in a square-section tube. Another purpose is to investigate the evolution of the flame tip velocity after the flame skirt touches the wall, using the relationship between the flame surface area and the burnt gas volume growth. The simulation results show that the 2D planar flame model is not suitable for predicting the flame propagation in a square-section tube, and that the simulated position of the flame tip is reliable only when the flame skirt touches the wall. There is an approximate proportional relationship between the volume growth of burnt gas and the flame tip velocity even after the lateral flame touches the wall. The widely used equation for the relationship between burnt gas volume growth and flame surface area fails after the flame touches the wall, and then holds at several later times. The balance is broken by the continuous extinction of the flame skirt. Finally, we report the dimensionless time interval between when the flame skirt touches the wall and flame front inversion, as well as the dimensionless velocity of the flame skirt. There are some differences between the values of the dimensionless time interval in 2D planar tubes and in 3D cylindrical tubes.}
}
@article{NIALL2023100113,
title = {Projective invariance and the measurement of visual shape},
journal = {Methods in Psychology},
volume = {8},
pages = {100113},
year = {2023},
issn = {2590-2601},
doi = {https://doi.org/10.1016/j.metip.2023.100113},
url = {https://www.sciencedirect.com/science/article/pii/S2590260123000048},
author = {Keith K. Niall},
keywords = {Visual shape, Vision, Psychophysics, Shape constancy, Geometric invariance, Projective invariants, Psychometrics, History of psychology},
abstract = {The problem of measurement in psychology is as old as experimental psychology. One narrow domain of psychology is privileged, in that it admits an absolute scale: a ratio scale of measurement with a point of origin. That domain is the assessment of shape or form through vision. Poincaré, Cassirer, and Gibson approached this topic by studying invariants, specifically projective invariants. In the study of vision, shapes should be measured in projective terms. As a convention or a strategy, measurement in projective terms may be conducted without hindrance. Many objections have been posed to the measurement of shape in projective terms; nearly all misrepresent basic geometric ideas. Projective invariants provide the right tools to evaluate goodness of shape constancy, and it provides these tools in its own right (in contrast to measures like distance and angle). Under conditions that allow shape constancy in human vision, sometimes stable estimates of projective invariants by observers do not match projective invariants at the eye. Some examples are given for projective invariants of coplanar conics, the computation of the measures is discussed, current objections to the approach are addressed, and consequences are drawn for the study of visual shape constancy.}
}
@incollection{SENNEWALD2011139,
title = {15 - Planning and Budgeting},
editor = {Charles A. Sennewald},
booktitle = {Effective Security Management (Fifth Edition)},
publisher = {Butterworth-Heinemann},
edition = {Fifth Edition},
address = {Boston},
pages = {139-151},
year = {2011},
isbn = {978-0-12-382012-9},
doi = {https://doi.org/10.1016/B978-0-12-382012-9.00015-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780123820129000150},
author = {Charles A. Sennewald},
abstract = {Publisher Summary
This chapter focuses on the planning and budgeting process in a Security Department. Planning and budgeting go hand in hand as budget is a plan stated in financial terms. Budgeting requires a realistic estimate of programs and their costs and an allocation of resources to achieve planned objectives. As budgets are prepared well in advance, effective budget management requires thinking ahead and anticipating needs based on relatively predictable conditions; thus it becomes a tool to ensure that plans are carried out. It gives direction to planning by defining specific programs, their timing, and their costs. The top–down and bottom–up approaches to budgeting in which senior management establishes goals and guidelines; the Security Manager provides the detailed planning and cost estimates; and senior management reviews these recommendations, establishes priorities, and allocates funds is recommended. Budget costs are generally broken down into capital, salary, and sundry expenses. The capital expenses include costs of physical improvements, physical additions, or major expenditures for hardware, which are usually fixed and hence easily determined. The use of detailed records from month to month and year to year makes it possible to arrive at realistic salary and sundry projections. All ongoing nonsalary expenses are considered sundry expenses, which are categorized according to factors such as the volume of expenditures in a given category and the distinctive nature of such expenditures. It is generally easier and efficient to manage a number of smaller sundry accounts than to rely on large accounts, which can be cumbersome.}
}
@article{MOSER2013765,
title = {Grid Cells and Neural Coding in High-End Cortices},
journal = {Neuron},
volume = {80},
number = {3},
pages = {765-774},
year = {2013},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2013.09.043},
url = {https://www.sciencedirect.com/science/article/pii/S0896627313009008},
author = {Edvard I. Moser and May-Britt Moser},
abstract = {An ultimate goal of neuroscience is to understand the mechanisms of mammalian intellectual functions, many of which are thought to depend extensively on the cerebral cortex. While this may have been considered a remote objective when Neuron was launched in 1988, neuroscience has now evolved to a stage where it is possible to decipher neural-circuit mechanisms in the deepest parts of the cortex, far away from sensory receptors and motoneurons. In this review, we show how studies of place cells in the hippocampus and grid cells in the entorhinal cortex may provide some of the first glimpses into these mechanisms. We shall review the events that led up to the discovery of grid cells and a functional circuit in the entorhinal cortex and highlight what we currently see as the big questions in this field—questions that, if resolved, will add to our understanding of cortical computation in a general sense.}
}
@incollection{ZEMAN2013373,
title = {Chapter 31 - The nature of consciousness},
editor = {James L. Bernat and H. Richard Beresford},
series = {Handbook of Clinical Neurology},
publisher = {Elsevier},
volume = {118},
pages = {373-407},
year = {2013},
booktitle = {Ethical and Legal Issues in Neurology},
issn = {0072-9752},
doi = {https://doi.org/10.1016/B978-0-444-53501-6.00031-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780444535016000317},
author = {Adam Zeman and Jan Adriaan Coebergh},
keywords = {awareness, consciousness, philosophy, self-consciousness, theories of consciousness, unconsciousness}
}
@article{BOLHUIS2022108446,
title = {Attachment insecurity and the biological embedding of reproductive strategies: Investigating the role of cellular aging},
journal = {Biological Psychology},
volume = {175},
pages = {108446},
year = {2022},
issn = {0301-0511},
doi = {https://doi.org/10.1016/j.biopsycho.2022.108446},
url = {https://www.sciencedirect.com/science/article/pii/S0301051122001892},
author = {Emma Bolhuis and Jay Belsky and Willem E. Frankenhuis and Idan Shalev and Waylon J. Hastings and Marieke S. Tollenaar and Kieran J. O’Donnell and Megan G. McGill and Irina Pokhvisneva and David T.S. Lin and Julia L. MacIsaac and Michael S. Kobor and Carolina {de Weerth} and Roseriet Beijers},
keywords = {Life history theory, Attachment, Antisocial and risky behavior, Pubertal onset, Child development, Cellular aging},
abstract = {Evolutionary-developmental psychologists have posited that individuals who grow up in stressful rearing circumstances follow faster life history strategies, thereby increasing their chances of reproduction. This preregistered study tested this stress-acceleration hypothesis in a low-risk longitudinal sample of 193 Dutch mother-child dyads, by investigating whether infant-mother attachment insecurity at 12 months of age predicted earlier pubertal onset and more callous-unemotional traits, aggression and risk-taking about a decade later. Also evaluated were the possible mediating roles of two biomarkers of accelerated aging (i.e., telomere length, epigenetic aging) at age 6. Structural equation modelling revealed no effects of attachment insecurity on biomarkers, pubertal timing or behavior. These null findings suggest that the explanatory value of evolutionary-developmental thinking might be restricted to high-risk samples, though unexplored variation in susceptibility to environmental influences might also explain the null findings.}
}
@article{NEUMAN2014650,
title = {Personality from a cognitive-biological perspective},
journal = {Physics of Life Reviews},
volume = {11},
number = {4},
pages = {650-686},
year = {2014},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2014.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S1571064514001584},
author = {Yair Neuman},
keywords = {Personality, Threat, Trust, Distrust, Psychology, Interdisciplinarity},
abstract = {The term “personality” is used to describe a distinctive and relatively stable set of mental traits that aim to explain the organism's behavior. The concept of personality that emerged in human psychology has been also applied to the study of non-human organisms from birds to horses. In this paper, I critically review the concept of personality from an interdisciplinary perspective, and point to some ideas that may be used for developing a cognitive-biological theory of personality. Integrating theories and research findings from various fields such as cognitive ethnology, clinical psychology, and neuroscience, I argue that the common denominator of various personality theories are neural systems of threat/trust management and their emotional, cognitive, and behavioral dimensions. In this context, personality may be also conceived as a meta-heuristics both human and non-human organisms apply to model and predict the behavior of others. The paper concludes by suggesting a minimal computational model of personality that may guide future research.}
}
@article{PAI201872,
title = {Assessing mobile health applications with twitter analytics},
journal = {International Journal of Medical Informatics},
volume = {113},
pages = {72-84},
year = {2018},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2018.02.016},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618301199},
author = {Rajesh R. Pai and Sreejith Alathur},
keywords = {Mobile health, Sentiment analysis, Twitter analytics, Causal loop diagram, Technology adoption model},
abstract = {Introduction
Advancement in the field of information technology and rise in the use of Internet has changed the lives of people by enabling various services online. In recent times, healthcare sector which faces its service delivery challenges started promoting and using mobile health applications with the intention of cutting down the cost making it accessible and affordable to the people.
Objectives
The objective of the study is to perform sentiment analysis using the Twitter data which measures the perception and use of various mobile health applications among the citizens.
Methods
The methodology followed in this research is qualitative with the data extracted from a social networking site “Twitter” through a tool RStudio. This tool with the help of Twitter Application Programming Interface requested one thousand tweets each for four different phrases of mobile health applications (apps) such as “fitness app”, “diabetes app”, “meditation app”, and “cancer app”. Depending on the tweets, sentiment analysis was carried out, and its polarity and emotions were measured.
Results
Except for cancer app there exists a positive polarity towards the fitness, diabetes, and meditation apps among the users. Following a system thinking approach for our results, this paper also explains the causal relationships between the accessibility and acceptability of mobile health applications which helps the healthcare facility and the application developers in understanding and analyzing the dynamics involved the adopting a new system or modifying an existing one.}
}
@article{YANG2012381,
title = {Combining means-end chain and fuzzy ANP to explore customers’ decision process in selecting bundles},
journal = {International Journal of Information Management},
volume = {32},
number = {4},
pages = {381-395},
year = {2012},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2011.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0268401211001307},
author = {Hao-Wei Yang and Kuei-Feng Chang},
keywords = {Means-end chain, Fuzzy analytic network process, Customer value, Product attributes},
abstract = {Although some researches had submitted the hierarchical model of customer value, there are still questions remaining in the model. How to achieve a more effective method for obtaining and analyzing data from customers concerning their expectations is still lacking. Therefore, this research first applied the modification of the means-end chain (MEC) to construct a hierarchy framework of customer value to allow product attributes to be linked. Next, this research combined fuzzy analytic network process (FANP) in exploring customer preference to catch the multiple needs of customers. Meanwhile, in the measurement of customer preference, fuzzy logic and linguistic variables are utilized to overcome human subjective and imprecise thinking. Overall, this research proposes the hierarchy framework of customer value including the causal relationships of attributes–consequence–value to fill previous model's gap, and identified the factors which could enhance the value of bundle, in contrast to the many monetary research treatments of product bundles. Finally, this research presented the value implications of cosmetics bundles and implication for management and marketing.}
}
@article{TANKSLEY202436,
title = {“We’re changing the system with this one”: Black students using critical race algorithmic literacies to subvert and survive AI-mediated racism in school},
journal = {English Teaching: Practice & Critique},
volume = {23},
number = {1},
pages = {36-56},
year = {2024},
issn = {1175-8708},
doi = {https://doi.org/10.1108/ETPC-08-2023-0102},
url = {https://www.sciencedirect.com/science/article/pii/S1175870824000177},
author = {Tiera Chante Tanksley},
keywords = {Critical literacy, Urban education, Critical race theory, Technology and literacy, Artificial intelligence education},
abstract = {Purpose
This paper aims to center the experiences of three cohorts (n = 40) of Black high school students who participated in a critical race technology course that exposed anti-blackness as the organizing logic and default setting of digital and artificially intelligent technology. This paper centers the voices, experiences and technological innovations of the students, and in doing so, introduces a new type of digital literacy: critical race algorithmic literacy.
Design/methodology/approach
Data for this study include student interviews (called “talk backs”), journal reflections and final technology presentations.
Findings
Broadly, the data suggests that critical race algorithmic literacies prepare Black students to critically read the algorithmic word (e.g. data, code, machine learning models, etc.) so that they can not only resist and survive, but also rebuild and reimagine the algorithmic world.
Originality/value
While critical race media literacy draws upon critical race theory in education – a theorization of race, and a critique of white supremacy and multiculturalism in schools – critical race algorithmic literacy is rooted in critical race technology theory, which is a theorization of blackness as a technology and a critique of algorithmic anti-blackness as the organizing logic of schools and AI systems.}
}
@article{PYKE2017319,
title = {When math operations have visuospatial meanings versus purely symbolic definitions: Which solving stages and brain regions are affected?},
journal = {NeuroImage},
volume = {153},
pages = {319-335},
year = {2017},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2017.03.046},
url = {https://www.sciencedirect.com/science/article/pii/S1053811917302562},
author = {Aryn A. Pyke and Jon M. Fincham and John R. Anderson},
keywords = {Math problem solving, Mental stages, Symbolic, Visuospatial representation, Graphs},
abstract = {How does processing differ during purely symbolic problem solving versus when mathematical operations can be mentally associated with meaningful (here, visuospatial) referents? Learners were trained on novel math operations (↓, ↑), that were defined strictly symbolically or in terms of a visuospatial interpretation (operands mapped to dimensions of shaded areas, answer = total area). During testing (scanner session), no visuospatial representations were displayed. However, we expected visuospatially-trained learners to form mental visuospatial representations for problems, and exhibit distinct activations. Since some solution intervals were long (~10s) and visuospatial representations might only be instantiated in some stages during solving, group differences were difficult to detect when treating the solving interval as a whole. However, an HSMM-MVPA process (Anderson and Fincham, 2014a) to parse fMRI data identified four distinct problem-solving stages in each group, dubbed: 1) encode; 2) plan; 3) compute; and 4) respond. We assessed stage-specific differences across groups. During encoding, several regions implicated in general semantic processing and/or mental imagery were more active in visuospatially-trained learners, including: bilateral supramarginal, precuneus, cuneus, parahippocampus, and left middle temporal regions. Four of these regions again emerged in the computation stage: precuneus, right supramarginal/angular, left supramarginal/inferior parietal, and left parahippocampal gyrus. Thus, mental visuospatial representations may not just inform initial problem interpretation (followed by symbolic computation), but may scaffold on-going computation. In the second stage, higher activations were found among symbolically-trained solvers in frontal regions (R. medial and inferior and L. superior) and the right angular and middle temporal gyrus. Activations in contrasting regions may shed light on solvers’ degree of use of symbolic versus mental visuospatial strategies, even in absence of behavioral differences.}
}
@article{OZTURK2024100940,
title = {Art of food: Systematic literature review of culinary creativity},
journal = {International Journal of Gastronomy and Food Science},
volume = {36},
pages = {100940},
year = {2024},
issn = {1878-450X},
doi = {https://doi.org/10.1016/j.ijgfs.2024.100940},
url = {https://www.sciencedirect.com/science/article/pii/S1878450X24000738},
author = {Betül Öztürk},
keywords = {Culinary creativity, Creative culinary process, Creative culinary product, Creative chef, Systematic literature review, Gastronomy},
abstract = {The growth of competitive culinary culture and the widespread use of restaurant guides have increased awareness of culinary creativity in literature. This understanding extends beyond traditional media to evaluations and critiques found on social media platforms. This review examines the main characteristics of culinary creativity, with a particular focus on the definition provided by the researchers. The research profile outlines the publishing years, affiliations, and themes of the 4Ps, which were investigated using both qualitative and quantitative methods. The review focuses on the 4P dimensions: creative process, person, product, and press, adapted to culinary creativity. The conclusion section identified areas of gaps and research questions for future studies on the possibility of defining and measuring culinary creativity in different dimensions using the 4Ps.}
}
@article{LIANG2023127067,
title = {Hydrocarbon production dynamics forecasting using machine learning: A state-of-the-art review},
journal = {Fuel},
volume = {337},
pages = {127067},
year = {2023},
issn = {0016-2361},
doi = {https://doi.org/10.1016/j.fuel.2022.127067},
url = {https://www.sciencedirect.com/science/article/pii/S0016236122038911},
author = {Bin Liang and Jiang Liu and Junyu You and Jin Jia and Yi Pan and Hoonyoung Jeong},
abstract = {Accurate prediction of hydrocarbon production is crucial for the oil and gas industry. However, the strong heterogeneity of underground formation, the inconsistency in oil–gas-water distribution, and the complex flow mechanisms make hydrocarbon production forecasting (HPF) difficult, which leads to a high level of uncertainty in the prediction results. The explosion of machine learning (ML) methodologies that are capable of analyzing big data shed new light on HPF using production data. In this article, an in-depth review is provided regarding HPF using ML methodologies. Firstly, the merits and drawbacks of traditional HPF methods are analyzed and summarized. Then, the applications of ML algorithms in HPF are reviewed in detail, especially concentrating on artificial neural network, support vector machine, and ensemble learning. For each algorithm, the basic theory and its variants are first introduced, and its applications in HPF are comprehensively demonstrated subsequently. Finally, this article presents the challenge and prospects of machine-learning-based HPF. Sophisticated ML proxy models can be constructed and employed to deal with an extended type of input data such that improving the efficacy of data utilization. On the other hand, deep learning models designed to handle time-series data can gain more attention. Modeling approaches for multivariate time-series hydrocarbon production data using deep neural networks with similar functionality to LSTM may lead to more accurate and computationally efficient production forecasting.}
}
@article{BUDDING202337,
title = {A second-order adaptive mental network model relating dreaming to creativity},
journal = {Cognitive Systems Research},
volume = {80},
pages = {37-49},
year = {2023},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2022.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041722001036},
author = {Dominique Budding and Shaney Doornkamp and Jan Treur},
keywords = {Creativity, Adaptive Temporal-Causal Network Modelling, Dreaming, Internal simulation},
abstract = {This paper introduces a novel controlled adaptive mental causal network model addressing how dreams overnight can influence creativity in waking life. The network model depicts in a causal, dynamic, and generic manner which adaptive mental processes underlie the connection between dreams and creativity and is shown to be validated with the existing cognitive neuroscience literature.}
}
@article{NIE2021101590,
title = {Disperse and preserve the perverse: computing how hip-hop censorship changed popular music genres in China},
journal = {Poetics},
volume = {88},
pages = {101590},
year = {2021},
note = {Measure Mohr Culture},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2021.101590},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X21000802},
author = {Ke Nie},
keywords = {Censorship, Popular music, Genre, China, Computational methods},
abstract = {How do political interventions reshape genre boundaries? Previous studies on genres only tangentially touch on this question as they mostly focus on the artistic, economic, or critical consequences of genre spanning. This paper fills in this gap by exploring the impact of music censorship on the censored genre and other related genres. Using an original dataset of 53,364 songs released on a Chinese online music platform, I study how Hip-Hop censorship in China in 2018 impacted Hip-Hop as well as Pop, Rock, and Folk songs in terms of how they sound and what topics are engaged in the lyrics. I propose a novel, computational approach to measure sound similarities between songs by using Music Information Retrieval (MIR) algorithms which synthesize audio signal processing and neural networks. I also measure the change of topic prevalence in song lyrics by using topic models supplemented with a dictionary approach. I found that post-censorship Hip-Hop songs sound significantly different from pre-censorship ones, with a bigger impact on the high-profile songs. Moreover, Rock, as a close genre to Hip-Hop, became more “Hip-Hoppy”, while Pop, a mixed category that reflects trending genres, became less “Hip-Hoppy”; the impact on these two genres is more salient among their low-profile songs. Folk, a genre distanced from Hip-Hop, remained generally untouched. The censorship also made Hip-Hop musicians engage less with topics related to violence and deviant behaviors but more with sexual terms, albeit in a covered form and not necessarily related to sexual conduct per se. The findings suggest a dispersion model in explaining the outcome of political interventions in genres, where stylistic conventions of the censored genre are dispersed from salient works of that genre to less influential ones as well as to adjacent genres.}
}
@incollection{YIP2023270,
title = {Pre-service STEM teacher education},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {270-275},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.13046-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305130465},
author = {Valerie W.Y. Yip and Promail K.Y. Leung},
keywords = {Integrative STEM education, Professional development models for pre-service STEM teachers, Technological pedagogical content knowledge, Place-based learning, Video-based learning},
abstract = {Pre-service STEM teacher education has gained much importance in the last few decades. This article primarily discusses the challenges of preparing proficient STEM teachers that can be accounted for by the encompassing nature of STEM education, the availability of teacher training models in the field and a need for developing STEM-related technological pedagogical knowledge. Suggestions to address the challenges are provided.}
}
@article{KRING2014725,
title = {The motivation and pleasure dimension of negative symptoms: Neural substrates and behavioral outputs},
journal = {European Neuropsychopharmacology},
volume = {24},
number = {5},
pages = {725-736},
year = {2014},
note = {Negative symptoms of schizophrenia: clinical characteristics and their measurement, experimental modelling, and opportunities for improved treatment},
issn = {0924-977X},
doi = {https://doi.org/10.1016/j.euroneuro.2013.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0924977X13001831},
author = {Ann M. Kring and Deanna M. Barch},
keywords = {Schizophrenia, Motivation, Pleasure, Neural substrates, Effort, Anticipation},
abstract = {A range of emotional and motivation impairments have long been clinically documented in people with schizophrenia, and there has been a resurgence of interest in understanding the psychological and neural mechanisms of the so-called “negative symptoms” in schizophrenia, given their lack of treatment responsiveness and their role in constraining function and life satisfaction in this illness. Negative symptoms comprise two domains, with the first covering diminished motivation and pleasure across a range of life domains and the second covering diminished verbal and non-verbal expression and communicative output. In this review, we focus on four aspects of the motivation/pleasure domain, providing a brief review of the behavioral and neural underpinnings of this domain. First, we cover liking or in-the-moment pleasure: immediate responses to pleasurable stimuli. Second, we cover anticipatory pleasure or wanting, which involves prediction of a forthcoming enjoyable outcome (reward) and feeling pleasure in anticipation of that outcome. Third, we address motivation, which comprises effort computation, which involves figuring out how much effort is needed to achieve a desired outcome, planning, and behavioral response. Finally, we cover the maintenance emotional states and behavioral responses. Throughout, we consider the behavioral manifestations and brain representations of these four aspects of motivation/pleasure deficits in schizophrenia. We conclude with directions for future research as well as implications for treatment.}
}
@article{WHITE2024101199,
title = {Preventive mental health care: A complex systems framework for ambient smart environments},
journal = {Cognitive Systems Research},
volume = {84},
pages = {101199},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.101199},
url = {https://www.sciencedirect.com/science/article/pii/S138904172300133X},
author = {Ben White and Inês Hipólito},
keywords = {Complex systems theory, Smart technology, Mental health, Preventive care, The free energy principle},
abstract = {We offer a framework for the design and use of Ambient Smart Environments (ASEs) for preventive mental health care support. Drawing from Complex Systems Theory (CST) and ‘E’ Cognitive Science (ECS), we claim that ASEs have the potential to act in a preventive capacity in support of good mental health, i.e. supporting dynamics that avoid so-called “struck states” (which are, according to CST, thought generally to underpin forms of psychopathology). Here, we frame our discussion with what has recently been termed the “mind-technology problem”. We define and characterise ASE systems, present some examples, and briefly survey some existing theoretical work. After introducing the essential CST terminology, the paper goes on to apply CST to explain developmental adaptation to continuously changing (smart) environments. Understanding the ASE’s navigation in terms of a dynamic geometry between attracting and repelling points (or local minima/local maxima), allows us to develop neurotechnology that can augment clinical interventions by predicting upcoming shifts for good symptomatic outcomes, i.e. when a preventive intervention (i.e. destabilisation) should take place. We further offer clear directions for the development and design of such neurotechnology.}
}
@article{TOSUN2013121,
title = {Does obligatory linguistic marking of source of evidence affect source memory? A Turkish/English investigation},
journal = {Journal of Memory and Language},
volume = {69},
number = {2},
pages = {121-134},
year = {2013},
issn = {0749-596X},
doi = {https://doi.org/10.1016/j.jml.2013.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0749596X13000235},
author = {Sümeyra Tosun and Jyotsna Vaid and Lisa Geraci},
keywords = {Evidentiality, Turkish, Source monitoring, Recognition memory, Linguistic relativity, Thinking for speaking},
abstract = {This study examined the influence of obligatory linguistic marking of the source of information on source memory. Turkish grammar requires speakers to indicate if an assertion is based on first hand knowledge or non-firsthand knowledge (hearsay or inference); English grammar does not require this distinction. We hypothesized that obligatory coding of source of evidence leads to a greater weighting of first hand relative to non-firsthand accounts of events (an “evidentiality effect”), resulting in better memory for first hand sources. In support of this hypothesis, across two experiments native Turkish speaking adults showed significantly better recognition and source memory for assertions coded with first hand than non-firsthand evidential markers. Further, among Turkish speakers who also knew English, those who learned English later had less accurate recognition and source memory for non-firsthand sources presented in English than those who learned English earlier, suggesting a carryover from the first language (Turkish). English monolingual speakers showed no difference in recognition or source memory as a function of source type, but showed better memory than Turkish speakers for non-firsthand sources. These findings provide the first empirical support for an evidentiality effect, suggesting that when marking the source of evidence is required by the grammar first hand sources are privileged in memory and non-firsthand sources are discounted.}
}
@article{XU2021121622,
title = {Optimization route arrangement: A new concept to achieve high efficiency and quality in heat exchanger network synthesis},
journal = {International Journal of Heat and Mass Transfer},
volume = {178},
pages = {121622},
year = {2021},
issn = {0017-9310},
doi = {https://doi.org/10.1016/j.ijheatmasstransfer.2021.121622},
url = {https://www.sciencedirect.com/science/article/pii/S0017931021007250},
author = {Yue Xu and Guomin Cui and Xinyu Han and Yuan Xiao and Guanhua Zhang},
keywords = {Heat exchanger network synthesis (HENS), Optimization process, Optimization route arrangement, Module, Global searching ability, Local searching ability},
abstract = {Heuristic algorithms are widely used in the global optimization of heat exchanger network synthesis (HENS). However, since heuristic algorithms are often characterized by a divergent search and a combination of multi-parameters, it is difficult to weigh and balance the global and local search in the process. In existing algorithms, the global and local search weight is commonly fixed during optimization. As a result, the algorithms hardly distribute the structural and continuous variables optimization suitably, sometimes even becoming invalid. Even more so, such algorithms fail to realize functions enhancing the searching ability. Therefore, this paper proposes a new, phased, modular concept that supports functional and sub-functional combinations to timely escape the stagnant status and obtain satisfying solutions with high efficiency and high quality. The concept's main goal is to achieve flexible combining and sequencing of different functions during the optimization process. Hence, the optimization route arrangement emphasizes the functions’ distribution, which differs from the sequential methods decomposing the whole problem. Each route's components are not unique, and the structure's current needs can change the modules’ sequences and categories. Such coordination of modules improves the algorithm's exploration and exploitation abilities, enabling the proposed method to attain better results within less computational time and rendering. Therefore, the proposed method is more suitable in large-scale HENS owing to much local optima involved in cases. Three cases are used to demonstrate the proposed routes’ validity, which shows that the optimization route arrangement successfully decreases the total annual cost and improves computational efficiency.}
}
@article{LUQUEAYALA2024100081,
title = {Digital natures: New ontologies, new politics?},
journal = {Digital Geography and Society},
volume = {6},
pages = {100081},
year = {2024},
issn = {2666-3783},
doi = {https://doi.org/10.1016/j.diggeo.2024.100081},
url = {https://www.sciencedirect.com/science/article/pii/S2666378324000035},
author = {Andrés Luque-Ayala and Ruth Machen and Eric Nost},
abstract = {Digital tools and practices are transforming societal relationships with non-human worlds—whether through smartphone apps that city dwellers use to navigate urban forests, robotic bees that pollinate crops, or webcams that livestream rare birds' nests. Recent academic and popular interest in the coming together of digital and natural worlds has generated both creative and critical reflections on what the digital means for the very concept of nature, troubling the latter's ontological stability. In this Introduction to the special issue Digital Natures: Reworking Epistemologies, Ontologies and Politics we claim that the digital, when considered beyond an epistemological register, is a productive and political force that is unsettling, rather than reinforcing, the boundaries between society and nature. We review the extensive body of work from across geography and the social sciences that is actively engaging with digital–nature intersections, and historicise current debates through reference to the figures of the cyborg, technonatures, biomimicry and digital organisms. Asking whether digitalized practices of sensing, abstraction and algorithmic recombination simply mirror a pre-existing and external Nature, or whether they advance a reconceptualization of nature, we set out to trace the progressive political potential of a digitally-entangled ontological redefinition of nature. We discuss how, within emerging digital natures, agencies are entangled in a reimagining of what both nature and society are about. Here, we argue, lies the transformative potential of digital natures—precisely in challenging and subverting the ontological place of an external Nature. The introduction finishes by simultaneously outlining a research agenda for digital natures and presenting the six papers that comprise the special issue.}
}
@article{LIU2023110679,
title = {Feature selection in threes: Neighborhood relevancy, redundancy, and granularity interactivity},
journal = {Applied Soft Computing},
volume = {146},
pages = {110679},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110679},
url = {https://www.sciencedirect.com/science/article/pii/S156849462300697X},
author = {Keyu Liu and Tianrui Li and Xibei Yang and Hengrong Ju and Xin Yang and Dun Liu},
keywords = {Feature selection, Granular computing, Granularity interactivity, Neighborhood granulation, Three-way decision},
abstract = {As a fundamental granular computing strategy, neighborhood granulation has been acknowledged as an intuitive and effective approach to feature evaluation and selection. However, such an approach always has a bias towards a fixed neighborhood granularity, while ignoring the observations across different levels of granularity. To this end, a novel algorithm based on Neighborhood relevancY, redundancY, and granularity interactivitY (N3Y) is proposed. Technically, N3Y adheres well to the rudiment of three-way decision, evaluating and selecting features in threes: 1) feature-to-class relevancy; 2) feature-to-feature redundancy; 3) granularity-to-granularity interactivity. Specifically, firstly, the neighborhood symmetrical uncertainty induced by neighborhood measures is adopted to evaluate the relevancy and redundancy of candidate feature subset; secondly, the proposed neighborhood granularity interactivity allows an uncertainty quantification for finer-to-coarser granularity, and is leveraged as a supplemental factor to guide the relevancy and redundancy, making our procedure more comprehensive; thirdly, a forward-greedy selector is devised, which is required to maximize the evaluation criterion integrating neighborhood relevancy, redundancy, and granularity interactivity. Extensive experiments demonstrate that N3Y outperforms several other advanced feature selectors.}
}
@article{BRANDON2024100902,
title = {The case of Bitcoins: Examining the financial accounting and reporting issues surrounding cryptocurrencies},
journal = {Journal of Accounting Education},
volume = {67},
pages = {100902},
year = {2024},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2024.100902},
url = {https://www.sciencedirect.com/science/article/pii/S0748575124000186},
author = {Duane Brandon and Travis Holt and Jefferson Jones and James H. Long and Jonathan Stanley},
keywords = {Bitcoin, Cryptocurrencies, Financial accounting, Financial accounting research, Revenue recognition},
abstract = {After years of debate, the FASB recently formalized the authoritative financial reporting guidance for cryptocurrencies such as Bitcoin. This case requires you to determine the appropriate financial accounting treatment and assess the reporting implications for Bitcoin using FASB’s authoritative guidance. The modular nature of the case allows students to address these requirements for three different clients: a Bitcoin miner, a Bitcoin trader, and a retailer that accepts Bitcoin as payment. As students complete the case, they are required to think critically about how the new guidance affects each client. Several features of the case allow instructors to adjust the level of difficulty so that they can create case assignments that are appropriate for both undergraduate and graduate accounting classes. Students who completed the case report that they found it to be realistic, interesting, and challenging.}
}
@article{RABB2019891,
title = {Individual Representation in a Community of Knowledge},
journal = {Trends in Cognitive Sciences},
volume = {23},
number = {10},
pages = {891-902},
year = {2019},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2019.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S1364661319301998},
author = {Nathaniel Rabb and Philip M. Fernbach and Steven A. Sloman},
keywords = {collective cognition, knowledge representation},
abstract = {An individual’s knowledge is collective in at least two senses: it often comes from other people’s testimony, and its deployment in reasoning and action requires accuracy underwritten by other people’s knowledge. What must one know to participate in a collective knowledge system? Here, we marshal evidence that individuals retain detailed causal information for a few domains and coarse causal models embedding markers indicating that these details are available elsewhere (others’ heads or the physical world) for most domains. This framework yields further questions about metacognition, source credibility, and individual computation that are theoretically and practically important. Belief polarization depends on the web of epistemic dependence and is greatest for those who know the least, plausibly due to extreme conflation of others’ knowledge with one’s own.}
}
@article{PALAFOXALCANTAR2020598,
title = {The complementary use of game theory for the circular economy: A review of waste management decision-making methods in civil engineering},
journal = {Waste Management},
volume = {102},
pages = {598-612},
year = {2020},
issn = {0956-053X},
doi = {https://doi.org/10.1016/j.wasman.2019.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S0956053X19307111},
author = {P.G. Palafox-Alcantar and D.V.L. Hunt and C.D.F. Rogers},
keywords = {Circular economy, Civil engineering, Cooperation, Decision-making, Game theory, Solid waste},
abstract = {Circular economy principles aim to contribute towards sustainability and resilience through several simultaneous agendas including economic growth, social development and environmental responsibility. Stakeholders from each perspective have their own interests and priorities, which often result in conflict. There are several and varied methodologies which address the decision-making process, however in engineering spheres these techniques are usually limited to optimising resources, time or costs. Decisions that are comprehensive in scope and integrated across all affected systems are required to transition towards a circular economy, effective cross-disciplinary thinking is imperative and cooperation amongst diverse areas is essential. Game theory is a useful technique when analysing the interactions of stakeholders with multiple objectives and perspectives. This paper aims to critically review methodological approaches used in waste management practice and provide a guidance on how game theory differs from, and is complementary to, the primary decision-making tools available where cooperation is a feature too often missing. This review seeks to justify the development of game theory to complement waste management decision-making methods in civil engineering, where resource consumption and waste management is often voluminous. An application of game theory to a waste management example illustrates that this methodological approach is of complementary value. The contribution of this study to circular economy and solid waste agendas is to emphasise the capability of game theory to help facilitate conflict resolution, competition, and stakeholder consensus when capturing multiple (sometimes conflicting) values in line with circular economy principles.}
}
@article{JAUMANN2022235,
title = {Condition Monitoring using Convolutional Neural Network in Agricultural Machinery - Use Case: Disc Mower},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {32},
pages = {235-240},
year = {2022},
note = {7th IFAC Conference on Sensing, Control and Automation Technologies for Agriculture AGRICONTROL 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.11.145},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322027781},
author = {Michael Jaumann and Ertug Olcay and Timo Oksanen},
keywords = {Condition monitoring, deep learning, agricultural machinery},
abstract = {Recent advances in sensing technologies and increasing computation power have accelerated the development of condition monitoring systems based on different approaches. There has been intensive research to automate the detection of anomalies in machines and processes by monitoring the changes in collected sensor data. Especially, a disc mower is prone to damage if it is frequently deployed in places where it might hit solid objects such as boulders and old fence posts. These anomalies cannot be easily recognized by the operator and may cause suboptimal results. In this paper, two deep learning models for intelligent condition monitoring in a disc mower are investigated to notify the machine operator when a failure occurs. For this, a basic convolutional neural network (CNN) and a residual neural network (ResNet) were trained, evaluated and the preliminary results are presented.}
}
@article{ARAUJODERESENDE2022169109,
title = {Non-Abelian fusion rules from Abelian systems with SPT phases and graph topological order},
journal = {Annals of Physics},
volume = {446},
pages = {169109},
year = {2022},
issn = {0003-4916},
doi = {https://doi.org/10.1016/j.aop.2022.169109},
url = {https://www.sciencedirect.com/science/article/pii/S0003491622002111},
author = {M.F. {Araujo de Resende} and J.P. {Ibieta Jimenez} and J. {Lorca Espiro}},
keywords = {Two-dimensional lattice model, Non-abelian fusion rule, Condensation mechanism, Global symmetry breaking, Quantum computation},
abstract = {Since Padmanabhan and Teotonio-Sobrinho (2015) show the emergence of non-Abelian fusion rules in some examples of a class of Abelian models, but does not prove whether these rules also exist in other cases, the purpose of this paper is to present such proof emphasizing the importance of the existence of these rules. By the way, as the ground state of these models can be degenerated as a function of their algebra and, hence, they can support some symmetry-protected topological (SPT) phases, we prove that these non-Abelian fusion rules are always necessary for these SPT phase transitions to occur via a condensation mechanism or/and some global symmetry breaking.}
}
@article{FILVA2019673,
title = {Clickstream for learning analytics to assess students’ behavior with Scratch},
journal = {Future Generation Computer Systems},
volume = {93},
pages = {673-686},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.10.057},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18313499},
author = {Daniel Amo Filvà and Marc Alier Forment and Francisco José García-Peñalvo and David Fonseca Escudero and María José Casañ},
keywords = {Learning analytics, Clickstream, Scratch, Programming, Big data},
abstract = {The construction of knowledge through computational practice requires to teachers a substantial amount of time and effort to evaluate programming skills, to understand and to glimpse the evolution of the students and finally to state a quantitative judgment in learning assessment. The field of learning analytics has been a common practice in research since last years due to their great possibilities in terms of learning improvement. Both, Big and Small data techniques support the analysis cycle of learning analytics and risk of students’ failure prediction. Such possibilities can be a strong positive contribution to the field of computational practice such as programming. Our main objective was to help teachers in their assessments through to make those possibilities effective. Thus, we have developed a functional solution to categorize and understand students’ behavior in programming activities based in Scratch. Through collection and analysis of data generated by students’ clicks in Scratch, we proceed to execute both exploratory and predictive analytics to detect patterns in students’ behavior when developing solutions for assignments. We concluded that resultant taxonomy could help teachers to better support their students by giving real-time quality feedback and act before students deliver incorrectly or at least incomplete tasks.}
}
@article{DAFONSECA2023122601,
title = {‘Take my advice’: Entrepreneurial consumers and the ecosystemic logics of digital platforms},
journal = {Technological Forecasting and Social Change},
volume = {193},
pages = {122601},
year = {2023},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2023.122601},
url = {https://www.sciencedirect.com/science/article/pii/S004016252300286X},
author = {André Luís A. {da Fonseca} and Paula Chimenti and Roberta D. Campos},
keywords = {Entrepreneurial ecosystem, Consumer behavior, Institutional logics, Digital platform, Computational social science},
abstract = {Technology affordances democratize access to entrepreneurship, making it relatively common for consumers to expand their social media activities to become digital entrepreneurs. The decreasing distance between consumption and entrepreneurial practices on digital platforms such as Twitch, Spotify, and YouTube amplifies the influence of cultural elements germane to entrepreneurial ecosystems. This article investigates the effect of this societal change on the discourse and behavior of consumers through the lens of institutional logics theory. It uses computational social science methods to analyze 1,909,370 comments posted on 249 videos published on a YouTube channel that unboxes technological products. The findings show that many end-users offer business advice to digital influencers as if they are collaborating with a buddy entrepreneur. A quantitative procedure demonstrates that these individuals have more subscribers than the average end-user. Further, the percentage of comments on this interaction correlates positively with end-user (consumer) engagement. This relationship indicates that a digital influencer's ability to inspire entrepreneurial behavior among end-users may benefit his/her business. It also suggests the existence of an entrepreneurship-related continuum from consumers to digital influencers.}
}
@article{LIN2006709,
title = {On integration of interface design methods: Can debates be resolved?},
journal = {Interacting with Computers},
volume = {18},
number = {4},
pages = {709-722},
year = {2006},
note = {Special Theme Papers from Special Editorial Board Members (contains Regular Papers)},
issn = {0953-5438},
doi = {https://doi.org/10.1016/j.intcom.2005.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0953543805001104},
author = {Y. Lin and W.J. Zhang and R.J. Koubek and Ronald R. Mourant},
keywords = {Human–computer interface, Total interface design, Interface design principle},
abstract = {There have been many debates on how to design the human–computer interface (HCI). Often, one can find that different views in a debate are simply because these views are attached to different aspects which embody the same thing. In other words, prior to giving an effective judgment of a debate, one needs to establish an understanding of the ‘total’ aspects of a thing the debate is about. Following this line of thinking, in this paper, we propose an understanding of the ‘total’ aspects of designing HCI, which is called the total interface design framework. We then judge several debates under this framework with the purpose of exemplifying the judgment process for any other debate related to designing HCI. At the end, the debates used for exemplifying our judgment process can be resolved. The effectiveness of the total interface design framework for integrating the different HCI approaches is also demonstrated.}
}
@article{CHANG2024101034,
title = {Bayesian data-driven models for pharmaceutical process development},
journal = {Current Opinion in Chemical Engineering},
volume = {45},
pages = {101034},
year = {2024},
issn = {2211-3398},
doi = {https://doi.org/10.1016/j.coche.2024.101034},
url = {https://www.sciencedirect.com/science/article/pii/S2211339824000352},
author = {Hochan Chang and Nathan Domagalski and Jose E Tabora and Jean W Tom},
abstract = {The primary objectives of pharmaceutical development encompass identifying the routes, processes, and conditions for producing medicines while establishing a control strategy to ensure acceptable quality attributes throughout the commercial manufacturing lifecycle. However, achieving these goals is challenged by uncertainties surrounding design decisions for the manufacturing process and variations in manufacturing methods resulting in distributions of outcomes during production. In this discussion, we focus on Bayesian approaches to quantify uncertainty and guide decision-making in process development.Bayesian modeling with Markov chain Monte Carlo proves effective in estimating process reliability. Recent advancements in surrogate models (e.g. Gaussian process, decision trees, and neural networks) offer novel means to quantify uncertainty and have shown success in designing experimental plans that reduce the number of required experiments to determine the optimal process design. By leveraging Bayesian approaches, chemical engineers can enhance their ability to navigate complex decision landscapes and optimize processes for improved efficiency and reliability.}
}
@incollection{TANDON2022409,
title = {Chapter 24 - Pathway modeling and simulation analysis},
editor = {Dev Bukhsh Singh and Rajesh Kumar Pathak},
booktitle = {Bioinformatics},
publisher = {Academic Press},
pages = {409-423},
year = {2022},
isbn = {978-0-323-89775-4},
doi = {https://doi.org/10.1016/B978-0-323-89775-4.00007-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323897754000079},
author = {Gitanjali Tandon and Sunita Yadav and Sukhdeep Kaur},
keywords = {System biology, pathway modeling, simulations analysis, biological network, SBGN, SBML, ordinary differential equations, cell designer, Cytoscape, COPASI},
abstract = {Pathway modeling and simulation analysis is an important aspect of systems biology, and it has always acted as a key player in providing useful meaning to the high-throughput biological data. For the sake of a better understanding of the complex biological systems, pathways need to be modeled. Whenever computation-based modeling is done, it provides an insight into the study of behaviors of various biochemical pathways. Biological network, thus generated, helps in better understanding of the relationships between genes, proteins (enzymes), and other key components. Several databases and tools have been designed for the detailed study of the biological networks. By adding the simulating environments to the modeled pathways can further enhance the information about their behaviors in different conditions with respect to time. Thus these approaches are helpful in dissecting the complexity of biological systems and this information will be further utilized to solve various biological problems.}
}
@article{OSBORNE2024105701,
title = {Food for thought — Paving the way for a UK roadmap towards optimum consumer safety: Development, Endorsement and Regulatory acceptance of New Approach Methodologies (NAMs) in Chemical Risk Assessment and Beyond},
journal = {Regulatory Toxicology and Pharmacology},
volume = {153},
pages = {105701},
year = {2024},
issn = {0273-2300},
doi = {https://doi.org/10.1016/j.yrtph.2024.105701},
url = {https://www.sciencedirect.com/science/article/pii/S0273230024001429},
author = {Olivia J. Osborne and Phil Botham and Cath Mulholland and Claire Potter and David Gott and Amie Adkin and Alan Boobis},
keywords = {Chemical risk assessment, New approach methodologies (NAMs), Roadmap, Regulatory, Toxicology, Human health},
abstract = {Advances in biosciences, chemistry, technology, and computer sciences have resulted in the unparalleled development of candidate New Approach Methodologies over the last few years. Many of these are potentially invaluable in the safety assessment of chemicals, but very few have been adopted for regulatory decision making. There is an immediate opportunity to use NAMs in safety assessment where the vision is to be able to predict risk more rapidly, accurately, and efficiently to further assure consumer safety. In order to achieve this, the UK Food Standards Agency (FSA) and the Committee on Toxicity of Chemicals in Food, Consumer Products and the Environment (COT) have developed a roadmap towards acceptance and integration of these new approach methodologies into safety and risk assessments for regulatory decision making. The roadmap provides a UK blueprint for the transition of NAMs from the research laboratory to their use in regulatory decision making. This will require close collaboration across disciplines (chemists, toxicologists, informaticians, risk assessors and others), and across chemical sectors, to develop, verify and utilise appropriate models. Linking up internationally, and harmonization will be fundamental.}
}
@incollection{FOX2005103,
title = {7 Knowledge, arguments, and intentions in clinical decision-making},
editor = {Ray Paton and Laura A. McNamara},
series = {Studies in Multidisciplinarity},
publisher = {Elsevier},
volume = {3},
pages = {103-129},
year = {2005},
booktitle = {Multidisciplinary Approaches to Theory in Medicine},
issn = {1571-0831},
doi = {https://doi.org/10.1016/S1571-0831(06)80011-0},
url = {https://www.sciencedirect.com/science/article/pii/S1571083106800110},
author = {John Fox and David Glasspool},
abstract = {Publisher Summary
The most influential theories of reasoning and decision making were developed by mathematicians and logicians, often informed by problems in some practical domain such as medicine or economics. Their work led to theoretical concepts with great intellectual depth and formal rigor, such as statistical decision theory (SDT). There are difficulties with expected utility and other mathematical techniques for practical decision making. Any quantitative decision procedure depends upon the ability to estimate the required parameters. This can be problematic in real-world applications. Classical decision theory focuses on only a small part of the decision process—making the choice. There are deep issues about the adequacy of quantitative formalisms to represent the kinds of knowledge and forms of reasoning that are routinely employed in medical thinking. This chapter presents an alternative framework that is formally sound but avoids the shortcomings of standard quantitative decision procedures.}
}
@article{CANDLER201537,
title = {Rate-dependent energetic processes in hypersonic flows},
journal = {Progress in Aerospace Sciences},
volume = {72},
pages = {37-48},
year = {2015},
note = {Celebrating 60 Years of the Air Force Office of Scientific Research (AFOSR): A Review of Hypersonic Aerothermodynamics},
issn = {0376-0421},
doi = {https://doi.org/10.1016/j.paerosci.2014.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0376042114000852},
author = {Graham V. Candler},
keywords = {Nonequilibrium, Hypersonics, Computational fluid dynamics},
abstract = {In celebration of the first 60 years of the Air Force Office of Scientific Research, several studies of hypersonic flows dominated by rate-dependent energetic processes are revisited. The work presented shows the evolution and advancement of computational capabilities in this area, and illustrates some key lessons learned over the previous decade or so. Early work with Leyva and Hornung in the California Institute of Technology T5 Free-Piston Shock Tunnel had the goal of validating thermochemical models for high-enthalpy flows. Several of these flows are re-analyzed with more advanced numerical methods, resulting in improved comparisons with the experimental measurements. This work was followed by a series of experiments in the Calspan-University at Buffalo Research Center (now CUBRC Inc.) facilities at lower enthalpy conditions. Initial comparisons were poor, but with a better understanding of the facility behavior and the inclusion of key finite-rate processes, excellent agreement was obtained for nitrogen flows. An interesting study related to plasmadynamics and finite-rate processes in a different type of flow is discussed. Finally, it is shown that recent advances in numerical methods that are beginning to enable the direct numerical simulation of key rate-dependent energetic processes in hypersonic flows.}
}
@article{LENG2021402,
title = {Selective region enlargement network for fast object detection in high resolution images},
journal = {Neurocomputing},
volume = {462},
pages = {402-411},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.08.015},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221011991},
author = {Jiaxu Leng and Ying Liu and Xinbo Gao},
keywords = {Object detection, Deep learning, Reinforcement learning, Region selection, High resolution image},
abstract = {Detecting objects of varied sizes in high resolution images is difficult due to the challenges of high memory requirement and huge computation burden. Existing state-of-the-art detectors perform well on low resolution images. However, its performance is greatly limited on high resolution images. In this paper, we propose a selective region enlargement network, called SRENet, which significantly reduces processing time and memory requirement while remaining high detection accuracy. The proposed SRENet does not need to conduct detection on original high resolution images but only needs to conduct detection on down-sampled images and some zoom-in regions selected from high resolution images. SRENet first conducts coarse detection on a low resolution image, and then sequentially selects promising regions that are expected to be analyzed at a higher resolution. Specifically, SRENet is built upon Deep Q-learning Network (DQN) and it outputs an action-reward map. The value of the reward map indicates the possibility that the action can improve detection accuracy. The region selected by the action with the maximum reward value will be analyzed further at a higher resolution. Extensive experiments are conducted to demonstrate SRENet on two challenging datasets obtaining high resolution images. Experimental results show that SRENet achieves state-of-the-art detection performance with high efficiency.}
}
@article{SCHINCKUS20133654,
title = {Between complexity of modelling and modelling of complexity: An essay on econophysics},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {392},
number = {17},
pages = {3654-3665},
year = {2013},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2013.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0378437113003014},
author = {C. Schinckus},
keywords = {Econophysics, Agent-based method, Emergence, Complexity},
abstract = {Econophysics is an emerging field dealing with complex systems and emergent properties. A deeper analysis of themes studied by econophysicists shows that research conducted in this field can be decomposed into two different computational approaches: “statistical econophysics” and “agent-based econophysics”. This methodological scission complicates the definition of the complexity used in econophysics. Therefore, this article aims to clarify what kind of emergences and complexities we can find in econophysics in order to better understand, on one hand, the current scientific modes of reasoning this new field provides; and on the other hand, the future methodological evolution of the field.}
}
@article{BAO2024100008,
title = {Enhancing validity through cognitive interviewing: A methodological example using the Registered Nursing Forecasting Nurse survey},
journal = {Measurement and Evaluations in Cancer Care},
pages = {100008},
year = {2024},
issn = {2949-8775},
doi = {https://doi.org/10.1016/j.ymecc.2024.100008},
url = {https://www.sciencedirect.com/science/article/pii/S2949877524000030},
author = {Zhuming Bao and Jenny Harris and Verna Lavender and Anne Marie Rafferty and Jo Armes},
keywords = {Cognitive interview, Nurse workload, Nursing workforce, Questionnaire design, Questionnaire validation, Survey methods},
abstract = {Objectives
Systemic anti-cancer therapy (SACT) nurses are facing increasing.workload challenges against the backdrop of increased treatment complexity and.patient caseload, coupled with a lack of policy guidance or research about the required.composition of SACT workforce to ensure optimal care and outcomes. The Registered.Nursing Forecasting (RN4CAST) study is an international research initiative designed.to model what happens to the quality of patient care and care outcomes when.components of the workforce change in acute and geriatric inpatient units. The insights.from the RN4CAST have not been applied to the oncology setting. Therefore, the.purpose of the study is to amend and test the RN4CAST nurse survey to ensure draft.Registered Nursing Forecasting SACT day unit (RN4CAST-SACT-D) survey items are.relevant, unambiguous/straightforward and the design/format is usable.Methods: This study adopted cognitive interviewing (CI), it combed two analytical.approaches (reparative approach and descriptive approach) while Question Appraisal.System (QAS) and cognitive theory were adopted.Results: Totally, 12 interviews were conducted within two rounds, 48 items remained.unchanged, 20 items underwent rephrasing for enhanced clarity, 20 new items were.incorporated to address test content gaps, and adjustments were made to the provided.answers to 5 questions.Conclusion: This study refined the RN4CAST survey for SACT day units through.cognitive interviewing, addressing comprehension, retrieval, judgment, and response.issues. Adjustments in wording improved clarity and relevance, aligning the survey with.nurses' experiences.}
}
@article{YU20221098,
title = {An Integrated Approach to Line Balancing for a Robotic Production System with the Unlimited Availability of Human Resources},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {1098-1103},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.536},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322018389},
author = {Haiyan Yu and Niu Can and Yongxing Wang and Shengze Wang and Akinola Ogbeyemi and Wenjun Zhang},
keywords = {Production planning, Line balancing, Automation, Textile industry, Function analysis},
abstract = {More and more robots have been introduced into production systems. Such a system may be called robotic production system. One important feature with such systems is the improved versatility in terms of its capability of fulfilling different tasks. The study presented in this paper developed a general approach for balancing of a production line with consideration of robots along with their functions and locations, and reliability of the line by assuming the unlimited availability of human resources. The approach has two parts: (1) the concept design of the production line and (2) computational line balancing. Specifically, for (2), the line balancing problem is formulated into a multi-objective optimization model with three objectives (i.e., balance rate, smoothness index, and economic cost) and the decision variables including robots and their locations. A specific textile production system was taken as an example to illustrate how this approach works and to show its effectiveness. As a result, a considerable improvement of such a robotic production line has been achieved after optimization in terms of the Takt time and the balance rate, particularly the Takt time being reduced by 18.52% and the balance rate being increased by 51.84%. The proposed approach is general, thus applicable to other robotic production systems, and expandable to inclusion of human factors.}
}
@article{JOHNSON20192497,
title = {A Point of Inflection and Reflection on Systems Chemical Biology},
journal = {ACS Chemical Biology},
volume = {14},
number = {12},
pages = {2497-2511},
year = {2019},
issn = {1554-8937},
doi = {https://doi.org/10.1021/acschembio.9b00714},
url = {https://www.sciencedirect.com/science/article/pii/S1554893719001651},
author = {Eachan O. Johnson and Deborah T. Hung},
abstract = {For the past several decades, chemical biologists have been leveraging chemical principles for understanding biology, tackling disease, and biomanufacturing, while systems biologists have holistically applied computation and genome-scale experimental tools to the same problems. About a decade ago, the benefit of combining the philosophies of chemical biology with systems biology into systems chemical biology was advocated, with the potential to systematically understand the way small molecules affect biological systems. Recently, there has been an explosion in new technologies that permit massive expansion in the scale of biological experimentation, increase access to more diverse chemical space, and enable powerful computational interpretation of large datasets. Fueled by these rapidly increasing capabilities, systems chemical biology is now at an inflection point, poised to enter a new era of more holistic and integrated scientific discovery. Systems chemical biology is primed to reveal an integrated understanding of fundamental biology and to discover new chemical probes to comprehensively dissect and systematically understand that biology, thereby providing a path to novel strategies for discovering therapeutics, designing drug combinations, avoiding toxicity, and harnessing beneficial polypharmacology. In this Review, we examine the emergence of new capabilities driving us to this inflection point in systems chemical biology, and highlight holistic approaches and opportunities that are arising from integrating chemical biology with a systems-level understanding of the intersection of biology and chemistry.
}
}
@article{NICKREID2023104397,
title = {True and false recognition in MINERVA 2: Extension to sentences and metaphors},
journal = {Journal of Memory and Language},
volume = {129},
pages = {104397},
year = {2023},
issn = {0749-596X},
doi = {https://doi.org/10.1016/j.jml.2022.104397},
url = {https://www.sciencedirect.com/science/article/pii/S0749596X22000845},
author = {J. {Nick Reid} and Randall K. Jamieson},
keywords = {False recognition, Computational modelling, Metaphor, Distributional semantics},
abstract = {Arndt and Hirshman (1998) used MINERVA 2 to simulate true and false recognition in DRM-style lists and found that the model was able to capture many features of the empirical data. Here, we first replicate their simulations, but using empirically structured vectors derived from Latent Semantic Analysis rather than the randomly generated vectors characteristic of MINERVA 2. We report that the model still captures the DRM effect with fewer free parameters. We then extend our analyses to true and false recognition for full sentences and metaphorical expressions. Using a simple bag-of-words representation for sentences, we find that the MINERVA 2 model captures classic sentence false recognition findings from Bransford and Frank (1971) and a more recent finding from Reid and Katz (2018a) that demonstrates false recognition of unstudied sentences that share a metaphorical but not literal theme to studied sentences. These simulations provide evidence that an instance-based memory model, when amalgamated with structured semantic representations from a distributional semantic model, can account for true and false recognition across different types of language experiences.}
}
@article{LEZAMA2021107045,
title = {Bidding in local electricity markets with cascading wholesale market integration},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {131},
pages = {107045},
year = {2021},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2021.107045},
url = {https://www.sciencedirect.com/science/article/pii/S0142061521002842},
author = {Fernando Lezama and Joao Soares and Ricardo Faia and Zita Vale and Olli Kilkki and Sirpa Repo and Jan Segerstam},
keywords = {Bi-level optimization, Evolutionary computation, Local electricity markets, Renewable energy, Wholesale market},
abstract = {Local electricity markets are a promising idea to foster the efficiency and use of renewable energy at the distribution level. However, as such a new concept, how these local markets will be designed and integrated into existing market structures, and make the most profit from them, is still unclear. In this work, we propose a local market mechanism in which end-users (consumers, small producers, and prosumers) trade energy between peers. Due to possible low liquidity in the local market, the mechanism assumes that end-users fulfill their energy demands through bilateral contracts with an aggregator/retailer with access to the wholesale market. The allowed bids and offers in the local market are bounded by a feed-in tariff and an aggregator tariff guaranteeing that end-users get, at most, the expected cost without considering this market. The problem is modeled as a multi-leader single-follower bi-level optimization problem, in which the upper levels define the maximization of agent profits. In contrast, the lower level maximizes the energy traded in the local market. Due to the complexity of the matter, and lack of perfect information of end-users, we advocate the use of evolutionary computation, a branch of artificial intelligence that has been successfully applied to a wide variety of optimization problems. Throughout three different case studies considering end-users with distinct characteristics, we evaluated the performance of four different algorithms and assessed the benefits that local markets can bring to market participants. Results show that the proposed market mechanism provides overall costs improvements to market players of around 30–40% regarding a baseline where no local market is considered. However, the shift to local markets in energy procurement can affect the conventional retailer/aggregator role. Therefore, innovative business models should be devised for the successful implementation of local markets in the future.}
}
@article{GEFEN2016828,
title = {Cytoskeleton and plasma-membrane damage resulting from exposure to sustained deformations: A review of the mechanobiology of chronic wounds},
journal = {Medical Engineering & Physics},
volume = {38},
number = {9},
pages = {828-833},
year = {2016},
issn = {1350-4533},
doi = {https://doi.org/10.1016/j.medengphy.2016.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S1350453316301114},
author = {Amit Gefen and Daphne Weihs},
keywords = {Chronic wounds, Mechanical loading, Sustained deformation, Cell damage},
abstract = {The purpose of this review paper is to summarize the current knowledge on cell-scale mechanically-inflicted deformation-damage, which is at the frontier of cell mechanobiology and biomechanics science, specifically in the context of chronic wounds. The dynamics of the mechanostructure of cells and particularly, the damage occurring to the cytoskeleton and plasma-membrane when cells are chronically deformed (as in a weight-bearing static posture) is correlated to formation of the most common chronic wounds and injuries, such as pressure ulcers (injuries). The first occurrence is microscopic injury which onsets as damage in individual cells and then progresses macroscopically to the tissue-scale. Here, we specifically focus on sub-catastrophic and catastrophic damage to cells that can result from mechanical loads that are delivered statically or at physiological rates; this results in apoptosis at prolonged times or necrosis, rapidly. We start by providing a basic background of cell mechanics and dynamics, focusing on the plasma-membrane and the cytoskeleton, and discuss approaches to apply and estimate deformations in cells. We then consider the effects of different levels of mechanical loads, i.e. low, high and intermediate, and describe the expected damage in terms of time-scales of application and in terms of cell response, providing experimental examples where available. Finally, we review different theoretical and computational modeling approaches that have been used to describe cell responses to sustained deformation. We highlight the insights that those models provide to explain, for example, experimentally observed variabilities in cell damage and death under loading.}
}
@article{CHEN2022106931,
title = {Neuromorphic display system for intelligent display},
journal = {Nano Energy},
volume = {94},
pages = {106931},
year = {2022},
issn = {2211-2855},
doi = {https://doi.org/10.1016/j.nanoen.2022.106931},
url = {https://www.sciencedirect.com/science/article/pii/S2211285522000167},
author = {Qizhen Chen and Xianghong Zhang and Yaqian Liu and Yujie Yan and Rengjian Yu and Xiumei Wang and Zenan Lin and Huaan Zeng and Lujian Liu and Huipeng Chen and Tailiang Guo},
keywords = {Neuromorphic display, Triboelectric nanogenerators, Synaptic transistors, QLED},
abstract = {Ultra-high integration, ultra-high resolution, intelligence and human-machine interaction is the inevitable trend of flat panel displays. However, most intelligent display systems are based on traditional Von Neumann architecture, which need to call data frequently because the storage and computation are separated, resulting in serious loss of power consumption and speed. In this paper, the concept of "neuromorphic display" device is proposed for the first time. The display technology is organically combined with the artificial neural network system to build a neuromorphic display network including the sensor module, processing module and display module, which endows the tiny artificial neural network system with the ability of perception, dispose and execution. The neuromorphic display enable to avoid serious loss of power consumption and speed without calling data frequently, and the color of the emitted light directly represents the result of neuromorphic display’s perception and processing of various external environment haptic stimuli. Furthermore, the neuromorphic element is applied in display that one-transistor one-synaptic transistor (1T1S) pixel circuit is demonstrated to replace two-transistor one-capacitance (2T1C) pixel circuit in traditional display to simplify pixel circuit. Therefore, the neuromorphic display showed great potentials in interactive input/control devices, intelligent wallpaper, robots and medical/health monitoring devices, and the 1T1S pixel circuit can simplify circuit design for active matrix display.}
}
@article{DOERR2000431,
title = {How Can I Find a Pattern in this Random Data?: The Convergence of Multiplicative and Probabilistic Reasoning},
journal = {The Journal of Mathematical Behavior},
volume = {18},
number = {4},
pages = {431-454},
year = {2000},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(00)00023-7},
url = {https://www.sciencedirect.com/science/article/pii/S0732312300000237},
author = {Helen M Doerr},
abstract = {This classroom-based research study examines the thinking of pre-calculus students about multiplicative growth and decay within a probabilistic context, thus bringing together two research strands in mathematics education: students' understanding of exponential functions and students' reasoning about random events. Using a multi-stage approach to model development, a curriculum unit was designed to elicit students' creation of a model or system that could be used to describe and explain the behavior of an experienced, probabilistic system. The evidence suggests that while the students made sense of the underlying multiplicative structure of the problem situation, many students experienced a conflict between the concept of a pattern and the concept of randomness. Students encountered difficulty in reconciling the deterministic nature of a closed-form analytic solution with the non-deterministic nature of a sequence of random events. These results suggest that there is a need for students to gain experience with non-deterministic models using contexts that provide meaningful empirical data.}
}
@article{GAN2022106637,
title = {Human-computer interaction based interface design of intelligent health detection using PCANet and multi-sensor information fusion},
journal = {Computer Methods and Programs in Biomedicine},
volume = {216},
pages = {106637},
year = {2022},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2022.106637},
url = {https://www.sciencedirect.com/science/article/pii/S0169260722000220},
author = {Senzhong Gan and Qingbin Zhuang and Bingyan Gong},
keywords = {PCANet, Deep learning, Health detector, Human-computer interaction, Interface design},
abstract = {Background and Objective: At present, because health monitoring using human-computer interaction (HCI) has become a demand in society, an intelligent health detector with HCI characteristics is urgently needed. Our device and software framework can provide natural human-machine interaction and facilitate the use of effective, efficient and safe electronic equipment for health detection. This paper integrates the research results of human subjects testing with analysis of our computational algorithms to build the proposed interaction platform. Methods: We collected the pulse signals of normal and sub-health people and used them as pre-processed signals. Then, we input them into the Principal Componcent Analysis Network (PCANet) layer by layer, and extracted the corresponding mapping features in each layer. The extracted features are hash coded, and histogram blocks are used as the feature matrix. Next, the accuracy obtained by the classical classifier is compared with the classification results of other feature extraction methods. The HCI integrated intelligent health detector based on PCANet neural network and multi-sensor information fusion has significantly improved the accuracy of human health detection. Results: The experimental results show that the proposed method achieves high accuracy for sub-health state recognition. Compared with the traditional feature extraction method, our PCANet method improves the recognition rate by more than 10%, which proves the effectiveness of PCANet model in the field of sub-health pulse signal detection. Because the PCANet is a multi-layer architecture model, in order to verify the influence of the number of extended network layers on the experimental results, experiments are carried out on the three-layer architectures represented by PCANet-1, PCANet-2 and PCANet-3 respectively. The Experimental results show that PCANet-3 model is 2.4% higher than PCANet-1, but only 0.6% higher than PCANet-2. The running time is about 2 times and 1.6 times higher than that of PCANet-1 and PCANet-2; Compared with traditional feature extraction algorithms such as MP and Gabor transform, the accuracy of pcanet model in pulse signal sub-health detection is significantly improved. Therefore, this product can effectively distinguish between the health and sub-health states. Conclusion: Our research shows that the intelligent health detector is efficient and convenient to use, and has higher accuracy for health detection. The HCI integrated platform provides a new reference basis for the detection of sub-health state.}
}
@article{ZHANG20231,
title = {New Media, New Literary Theory, and New Literature from an Interological Horizon},
journal = {Signs and Media},
volume = {2},
number = {1},
pages = {1-22},
year = {2023},
issn = {2590-0315},
doi = {https://doi.org/10.1163/25900323-12340020},
url = {https://www.sciencedirect.com/science/article/pii/S2590031523000137},
author = {Peter Zhang},
keywords = {interality, Deleuze, Burroughs, McLuhan, Kafka, Pointillism},
abstract = {This article discusses new developments in the field of literary theory and literary praxis in the era of new media from the perspectives of media theory and interology. It takes new media as a McLuhanesque formal cause and holds that a conspicuous characteristic of literary works in the era of new media lies in the salience and normalization of interality. This development means art forms like mosaic and Pointillism have acquired a paradigmatic significance as a result. In revealing this new paradigm, the article also points to some current social maladies that have come with new media. It holds that literary writing should go beyond the mere embodiment of symptoms and make an intervention in media-induced maladies. The article affirms the irreplaceability of experimental literature and serious literature in an era of attention deficit, and points out that form should occupy a paramount position in literary theory, literary criticism, and literary praxis.}
}
@article{SOUSA2014569,
title = {Sociomaterial Enactment Drive of Business/IT Alignment: From Small Data to Big Impact},
journal = {Procedia Technology},
volume = {16},
pages = {569-582},
year = {2014},
note = {CENTERIS 2014 - Conference on ENTERprise Information Systems / ProjMAN 2014 - International Conference on Project MANagement / HCIST 2014 - International Conference on Health and Social Care Information Systems and Technologies},
issn = {2212-0173},
doi = {https://doi.org/10.1016/j.protcy.2014.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S2212017314002321},
author = {José L.R. Sousa and Ricardo J. Machado},
keywords = {Business / IT alignment, complex-networks, profiling framework, information systems;},
abstract = {Business/IT alignment is an information systems research field with a long existence and a high number of researchers and represents a central direction on the thinking about the relation between business and the information systems. It aims to achieve a paradigm, one in which there is a high degree of visibility and availability of information, about the information systems sociomateriality. Complex-networks constitute an approach to the study of emergent properties of complex systems that strongly focuses and relies on models and measures, through which build the system interdependence. Several contributions of complex-networks are: topology always affects the function; separated from the domain; quantification of element's relationships; visibility and capture of emergent properties. This work expects to contribute for the appropriate use of complex-networks models and measure in the drive of the information systems alignment. This work considers an exploratory case research strategy. It uses an exploratory case developed in the field of information systems that directed its deployment to sustain the business development and evolution. This paper illustrates a profiling framework that introduces a global and elementary use of the sociomaterial enactment. From the analysis of the exploratory case, the paper infers drivers of the information systems Business/IT alignment.}
}
@article{SUN202461,
title = {Understanding news & views articles: Rhetorical structures across different disciplines},
journal = {English for Specific Purposes},
volume = {73},
pages = {61-74},
year = {2024},
issn = {0889-4906},
doi = {https://doi.org/10.1016/j.esp.2023.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0889490623000698},
author = {Haiyang Sun and Xinyuan Mei and Honghui Zhang},
keywords = {News and views articles, Article reviews, Review genre, Rhetorical structure, Disciplinary variation, Textual organization},
abstract = {Article reviews play a pivotal role in fostering students’ critical evaluation and critical thinking skills. News & Views (N&V) articles published in Nature journals are one prominent example of article reviews and serve as excellent models for students to learn and refine their skills in writing article reviews. However, there is limited research on the rhetorical features of N&V articles, especially in terms of textual organization across different disciplines. To address this gap, three corpora were compiled for the present study, representing the disciplinary groups of natural sciences (NS), social sciences (SS), and technology and engineering (TE). Each corpus was comprised of 30 N&V texts from its respective disciplinary group. Using a self-developed coding framework building on previous research, we coded the move and step of each text in the corpora. The coding data were compared and analyzed to uncover the distinguishing textual features of the three disciplinary groups. The analysis revealed that the variations were predominantly at the step level. N&V articles of NS and SS shared most of the moves and steps, but N&Vs in TE exhibited unique stylistic characteristics. These findings hold significant implications for training and teaching article review writing, particularly in relation to disciplinary contexts.}
}
@article{FURFARO2024771,
title = {Artificial Intelligence in Medical Education: A Long Way to Go},
journal = {CHEST},
volume = {165},
number = {4},
pages = {771-774},
year = {2024},
issn = {0012-3692},
doi = {https://doi.org/10.1016/j.chest.2023.11.028},
url = {https://www.sciencedirect.com/science/article/pii/S0012369223058208},
author = {David Furfaro and Leo Anthony Celi and Richard M. Schwartzstein}
}
@article{HAO2021104269,
title = {Real-time semantic segmentation with weighted factorized-depthwise convolution},
journal = {Image and Vision Computing},
volume = {114},
pages = {104269},
year = {2021},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2021.104269},
url = {https://www.sciencedirect.com/science/article/pii/S0262885621001748},
author = {Xiaochen Hao and Xingjun Hao and Yaru Zhang and Yuanyuan Li and Chao Wu},
keywords = {Semantic segmentation, Real-time, Pyramid fusion, Continuous separation},
abstract = {Semantic segmentation has achieved great success with the popularity of convolutional neural networks (CNNs). However, the huge computational burden restricts the application of most existing networks on edge devices with strict inference time constraints. To solve this problem, a weighted factorized-depthwise convolution network (WFDCNet) is presented in this paper, which contains full- dimensional continuous separation convolution (FCS) modules and a lateral asymmetric pyramid fusion (LAPF) module, aiming to obtain high accuracy without damaging inference speed. Specifically, the FCS module enables the calculation of each dimension to be completed independently in a continuous separation process and uses simplified SE (SSE) attention layer to adjust the channels, achieving the extensive extraction of feature information. The LAPF module is able to eliminate semantic divergence and fuse feature maps of three different scales to realize the combination of multiple information from the front-end and the back-end network. WFDCNet shows superior performance on Cityscapes, Camvid, Mapillary Vistas and COCO-Stuff datasets. Especially, the experimental results demonstrate that our network achieves 73.7% mIoU on Cityscapes dataset, with the inference speed of 102.6FPS on a single RTX 2080 Ti GPU, and 17.2FPS on Jetson TX2.}
}
@article{SCHOELLER20191,
title = {Introduction to the special issue on physics of mind},
journal = {Physics of Life Reviews},
volume = {31},
pages = {1-10},
year = {2019},
note = {Physics of Mind},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2019.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S157106451930168X},
author = {Felix Schoeller},
keywords = {Physics of mind, Mind, Brain, Cognition, Cultural invariance, Sacred values, Cognitive technologies, Cultural evolution, Computational psychiatry, Neurotechnology, Emotion, Interoception, Consciousness, Symmetry, Hierarchy, Meaning},
abstract = {In recent years, both fields of physics and psychology have made important scientific advances. The emergence of new instruments gave rise to a data-driven neuroscience allowing us to learn about the state of the brain supporting known mental functions and conversely. In parallel, the appearance of new mathematics allowed the development of computational models describing fundamental brain functions and implementing them in technological applications. While emphasizing the methodology of physics, the special issue aims to bring together these trends in both the experimental and theoretical sciences in order to explain some of the most basic mental processes such as perception, cognition, emotion, consciousness, and learning. In this editorial, we define unsolved problems for brain and psychological sciences, discuss possible means toward their respective solutions, and outline some collaborative initiatives aiming toward these goals. The following problems are defined in gradual order of difficulty: what are the universal properties of human behavior across conditions and cultures? What have each culture learned over historical times and why should specific elements of knowledge be accumulated over cultural evolution? Can computational psychiatry help predict, understand, and cure mental disorders? What is the function of art and cultural artifacts such as music, fiction, or poetry for the cognitive system? How to explain the relation between first-person subjective experience and third-person objective physiological data? What neural mechanisms operate on which mental content at the highest levels of organization of the hierarchical brain? How do abstract ideas emerge from sensory-motor contingencies and what are the conditions for the birth of a new concept? Could symmetry play a role in psychogenesis and support the emergence of new hierarchical layers in cognition? How can we start addressing the question of meaning scientifically, and what does it entail for the physical sciences?}
}
@incollection{GIGERENZER20013684,
title = {Digital Computer: Impact on the Social Sciences},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {3684-3688},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/00101-7},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767001017},
author = {G. Gigerenzer},
abstract = {A new social organization of the workplace, the large-scale division of labor in manufacturing, served as the original model for Charles Babbage's computers. Today's computer-metaphor of mind still reflects this social origin: it pictures cognitive processes—from reasoning to creativity—as a hierarchical organization of subroutines. The two different uses of computers—for calculation (number processing) and computation (symbol processing as in simulation)—vary markedly among scientific disciplines. The computer has revolutionized our conception of mind and society in the sense of a re-volution to the Enlightenment's combinatorial view of thought and Adam Smith's ideal of the division of labor.}
}
@incollection{GARDNER2024175,
title = {Chapter 8 - Smart futures: Responsive and responsible design},
editor = {Nicole Gardner},
booktitle = {Scaling the Smart City},
publisher = {Elsevier},
pages = {175-185},
year = {2024},
series = {Smart Cities},
isbn = {978-0-443-18452-9},
doi = {https://doi.org/10.1016/B978-0-443-18452-9.00003-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443184529000033},
author = {Nicole Gardner},
keywords = {Artificial intelligence (AI), Responsive environments, Responsible design, Responsible innovation, Smart city, Urban technology},
abstract = {The responsive and responsible design of urban technology it is a creative, social, and moral challenge that requires more than technical adroitness. Building from the insights established in the preceding chapters, and with an eye to emerging trends and challenges of rapidly evolving AI technologies, this chapter connects across the themes of ethics, technology, and design to outline the key take-aways of the book. It argues that smart city initiatives and urban technology projects do not always have to be large-scale, costly, and invisible, and that deprioritizing the smart city's central logic of scalability can help bring the materiality, specificity, and lived experience of local contexts into view. To rethink the responsible design of urban technology, the chapter concludes by advocating for a smart future that normalizes and scaffolds ethical reasoning skills in design education towards cultivating practical wisdom to carry into professional contexts and to enable a doing of ethics by design.}
}
@article{KECHID2020106368,
title = {Cultural coalitions detection approach using GPU based on hybrid Bat and Cultural Algorithms},
journal = {Applied Soft Computing},
volume = {93},
pages = {106368},
year = {2020},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106368},
url = {https://www.sciencedirect.com/science/article/pii/S1568494620303082},
author = {Amine Kechid and Habiba Drias},
keywords = {GPU, Parallel approaches, Hybrid approaches, Cultural algorithm, Bat optimization, Culture, Coalitions, Social agents},
abstract = {Currently, robot technology plays a crucial role in our modern life. Its great favor over humanity was not limited to the industrial domain, but also on the social one. Social robots become equipped with an artificial culture that allows them to interact with humans. Although the great importance of this technology, some misdeeds can create a real catastrophe. Coalition creation is one of its most dangerous troubles, where a subset of robots cooperates to impose pernicious decisions. There are efficient coalitions detection methods, but their exponential computation time makes their use limited only to small data. This paper is an extended version of Kechid and Drias (2019) presented at the IEA/AIE 2019 conference. In this paper, we propose a Cultural coalitions detection approach using GPU based on hybrid Bat and Cultural Algorithms. Unlike the existing literature, We view the problem of finding coalitions as an optimization problem to get relevant solutions in a significantly reduced amount of time. The proposed approach can increase the population diversity and improve the searching ability for an optimal exploration–exploitation balance. Also, it can launch several cultural bats in GPU to make real parallelism. Experimental results on several datasets show that the proposed method will considerably reduce the runtime. These datasets represent the result of artificial cultural agents playing the colored trails (CT) game. Concerning the creation of profiles, we use real datasets generated based on the World Values survey.}
}
@article{WANG2024101440,
title = {ChatGPT's capabilities in providing feedback on undergraduate students’ argumentation: A case study},
journal = {Thinking Skills and Creativity},
volume = {51},
pages = {101440},
year = {2024},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101440},
url = {https://www.sciencedirect.com/science/article/pii/S1871187123002080},
author = {Li Wang and Xinya Chen and Chung Wang and Lingna Xu and Rustam Shadiev and Yan Li},
keywords = {ChatGPT (ChatGPT 3.5), Feedback capabilities, Teacher's feedback, Argumentation teaching, Undergraduate students},
abstract = {In argumentation teaching, providing timely and high-quality feedback is always a challenging task for teachers because of the high complexity and large volume of students’ argumentation contents. ChatGPT, a large language model introduced in November 2022, offers a potential solution for this problem. To examine the potential reliability and credibility of leveraging ChatGPT for argumentation feedback, the study conducted a retrospective analysis by applying ChatGPT to generate feedback on 50 sets of argumentation contents that human teachers had previously assessed. The study first assessed the feedback accuracy of ChatGPT and the factors that influenced the evaluation of arguments. The findings showed that ChatGPT demonstrated impressive precision rate (91.8 %) and recall rate (63.2 %) when providing feedback on arguments, indicating that ChatGPT possesses a fundamental capability to provide feedback on arguments. However, this capability of ChatGPT was significantly affected by the length of arguments and the discourse markers used in the arguments. The study then qualitatively compared the ChatGPT's feedback and teacher's feedback. The results revealed that these two types of feedback each had their own advantages and disadvantages. While ChatGPT could potentially generate comprehensive feedback and textual-based feedback, and limited to the linguistic level when provide affective feedback, teacher's feedback was more focused on student's overall learning progress, based on personal teaching experience to correctly identify immediate critical problem of the student, and consideration on the humanistic empathy interaction. Although the overall findings suggested that ChatGPT exhibited potential reliability and credibility for argumentation feedback, the study did identify several limitations.}
}
@article{MAC201613,
title = {Heuristic approaches in robot path planning: A survey},
journal = {Robotics and Autonomous Systems},
volume = {86},
pages = {13-28},
year = {2016},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2016.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0921889015300671},
author = {Thi Thoa Mac and Cosmin Copot and Duc Trung Tran and Robin {De Keyser}},
keywords = {Autonomous navigation, Robot path planning, Heuristic methods, Neural network, Fuzzy logic, Nature inspired algorithms, Potential field method},
abstract = {Autonomous navigation of a robot is a promising research domain due to its extensive applications. The navigation consists of four essential requirements known as perception, localization, cognition and path planning, and motion control in which path planning is the most important and interesting part. The proposed path planning techniques are classified into two main categories: classical methods and heuristic methods. The classical methods consist of cell decomposition, potential field method, subgoal network and road map. The approaches are simple; however, they commonly consume expensive computation and may possibly fail when the robot confronts with uncertainty. This survey concentrates on heuristic-based algorithms in robot path planning which are comprised of neural network, fuzzy logic, nature-inspired algorithms and hybrid algorithms. In addition, potential field method is also considered due to the good results. The strengths and drawbacks of each algorithm are discussed and future outline is provided.}
}
@incollection{YALAMANCHI202247,
title = {Chapter 3 - Artificial intelligence–enabled fuel design},
editor = {Jihad Badra and Pinaki Pal and Yuanjiang Pei and Sibendu Som},
booktitle = {Artificial Intelligence and Data Driven Optimization of Internal Combustion Engines},
publisher = {Elsevier},
pages = {47-67},
year = {2022},
isbn = {978-0-323-88457-0},
doi = {https://doi.org/10.1016/B978-0-323-88457-0.00011-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323884570000114},
author = {Kiran K. Yalamanchi and Andre Nicolle and S. Mani Sarathy},
keywords = {Combustion, Fuel design, Kinetics, Machine learning, Neural networks},
abstract = {Fuel composition plays an important role both in efficiency and effectiveness of engines. Combined with the engine variables, fuel can span a wide range of composition space, which makes it demanding to find an optimal composition. Artificial intelligence (AI) algorithms are attracting significant interest for predicting complex phenomenon. In this chapter, a discussion is presented on exploiting the advantages presented by machine learning algorithms for fuel formulation. The present fuel modeling scenario and a holistic approach necessary for fuel optimization is first presented. A wealth of AI algorithms are available to make use of in fuel formulation. These algorithms are discussed in line with their application to fuel formulation and the literature of the explored space in this area is presented. Additionally, a discussion is presented on how AI also helps in assisting the traditional computational fluid dynamic and chemical kinetic analysis for an elaborate study of fuels. Fuel development is just a step in the entire engine innovation cycle, and a perspective of how the AI fits in to this scenario is presented.}
}
@article{ISMAEEL2019599,
title = {Drawing the operating mechanisms of green building rating systems},
journal = {Journal of Cleaner Production},
volume = {213},
pages = {599-609},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2018.12.115},
url = {https://www.sciencedirect.com/science/article/pii/S095965261833823X},
author = {Walaa S.E. Ismaeel},
keywords = {Green certification, Greenmarket, LEED, Performance measurement and verification, Sustainable building guidelines, Green building rating systems},
abstract = {Abstract:
Green Building Rating and Certification systems (GBRSs) were developed to provide guidelines and benchmarking criteria for conducting sustainable building processes. Yet, they lack a ‘know how’ defining their role and value-contribution, which may eventually limit their role and affect their credibility and application in the decision-making process. Subsequently, this study presents its research hypothesis assuming two mechanisms and four scopes of operation; the rating mechanism operates using the guidelines and measurement metrics while the certification mechanism operates using the verification and certification metrics. The research has adopted an integrated qualitative and quantitative approach where the development of the four interrelated scopes of operation has been traced through literature, and an assigned score weighting has been used to compare them according to different GBRSs- and more specifically for the Leadership in Energy and Environmental Design (LEED) credits. The results present an integrated application framework (IAF) with a particular focus to energy and materials' credits and based on the system's targets as well as credits' intent and interrelations. The proposed framework applies system thinking to the level of individual practices as well as the entire building process. This is followed by two case-study validations and amendments to reflect dominance, temporal precedence and iterative action of some scopes along different project phases. It indicates how the difference in building type and context may alter opportunities for scoring potentials in addition to means of supporting important decisions such as setting building reuse and CWM plans as well as specifying and procuring sustainable materials. The result provides a consistent mean to manage and document building activities and finally report buildings' performance. This shall prove very useful for researchers, practitioners and system developers. Finally, the study provides insights for developing the LEED system as well as different GBRSs using the IAF; this may take the form of a more interactive decision support tool, software management application or a better user friendly system interface.}
}
@article{KISHIDA201293,
title = {Imaging Models of Valuation During Social Interaction in Humans},
journal = {Biological Psychiatry},
volume = {72},
number = {2},
pages = {93-100},
year = {2012},
note = {Recent Advances Using Neuroeconomics to Investigate Psychopathology},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2012.02.037},
url = {https://www.sciencedirect.com/science/article/pii/S0006322312002181},
author = {Kenneth T. Kishida and P. Read Montague},
keywords = {Decision making, dopamine, fMRI, neuroeconomics, reinforcement learning, social cognition},
abstract = {The role of dopamine neurons in value-guided behavior has been described in computationally explicit terms. These developments have motivated new model-based probes of reward processing in healthy humans, and in recent years these same models have also been used to design and understand neural responses during simple social exchange. These latter applications have opened up the possibility of identifying new endophenotypes characteristic of biological substrates underlying psychiatric disease. In this report, we review model-based approaches to functional magnetic resonance imaging in healthy individuals and the application of these paradigms to psychiatric disorders. We show early results from the application of model-based human interaction at three disparate levels: 1) interaction with a single human, 2) interaction within small groups, and 3) interaction with signals generated by large groups. In each case, we show how reward-prediction circuitry is engaged by abstract elements of each paradigm with blood oxygen level–dependent imaging as a read-out; and, in the last case (i.e., signals generated by large groups) we report on direct electrochemical dopamine measurements during decision making in humans. Lastly, we discuss how computational approaches can be used to objectively assess and quantify elements of complex and hidden social decision-making processes.}
}
@article{ROMEO2020112869,
title = {Machine learning-based design support system for the prediction of heterogeneous machine parameters in industry 4.0},
journal = {Expert Systems with Applications},
volume = {140},
pages = {112869},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.112869},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419305792},
author = {Luca Romeo and Jelena Loncarski and Marina Paolanti and Gianluca Bocchini and Adriano Mancini and Emanuele Frontoni},
keywords = {Design support system, Machine learning, Decision tree, Nearest-Neighbor, Neighborhood component features selection},
abstract = {In the engineering practice, it frequently occurs that designers, final or intermediate users have to roughly estimate some basic performance or specification data on the basis of input data available at the moment, which can be time-consuming. There is the need for a tool that will fill the missing gap in the optimization problems in engineering design processes, by making use of the advances in the artificial intelligence field. This paper aims to fill this gap by introducing an innovative Design Support System (DesSS), originated from the Decision Support System, for the prediction and estimation of machine specification data such as machine geometry and machine design on the basis of heterogeneous input parameters. As the main core of the developed DesSS, we introduced different machine learning (ML) approaches based on Decision/Regression Tree, k-Nearest Neighbors, and Neighborhood Component Features Selection. Experimental results obtained on a real use case and using two different real datasets demonstrated the reliability and the effectiveness of the proposed approach. The innovative machine learning-based DesSS meant for supporting the designing choice, can bring various benefits such as the easier decision-making, conservation of the company’s knowledge, savings in man-hours, higher computational speed and accuracy.}
}
@article{LEAVY2011235,
title = {Elementary and middle grade students’ constructions of typicality},
journal = {The Journal of Mathematical Behavior},
volume = {30},
number = {3},
pages = {235-254},
year = {2011},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2011.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732312311000174},
author = {Aisling M. Leavy and James A. Middleton},
keywords = {Statistical reasoning, Mathematical thinking, Elementary students, Middle grade students, Typicality, Data and statistics},
abstract = {This study addresses the measures chosen by students when selecting or constructing indices to properties of distributions of data. A series of individual teaching experiments were conducted to provide insight into the development of five 4th to 8th grade students’ conceptualizations of distribution over the course of 8 weeks of instruction. During the course of the teaching experiment (emergent) statistical tasks and analogous teacher activities were created and refined in an effort to support the development of understanding. In the process of development, attempts were made by students to coordinate center and variability when constructing measures to index properties of distributions. The results indicate that consideration of representativeness was a major factor that motivated modification of approaches to constructing indices of distributions, and subsequent coordination of indices of variation and center. In particular, the defining features of student's self-constructed “typical” values and notions of spread were examined, resulting in a model of development constituting eight “categories” ranging from the construction of values that did not reflect properties of the data (Category 1) to measures employing conceptual use of the mean in combination with other indices of center and spread (Category 8).}
}
@article{CURRAN2024112221,
title = {Estimating probability terms for the background presence of glass when considering activity in forensic casework},
journal = {Forensic Science International},
volume = {364},
pages = {112221},
year = {2024},
issn = {0379-0738},
doi = {https://doi.org/10.1016/j.forsciint.2024.112221},
url = {https://www.sciencedirect.com/science/article/pii/S0379073824003025},
author = {James M. Curran and Patrick Buzzini and Tatiana Trejos},
keywords = {Glass evidence, Activity, Statistics, Estimation},
abstract = {Coulson et al. [1] proposed methodology for the estimation of the P and S terms used in glass interpretation when assessing the value of the findings given activity level propositions. These terms arise in a model proposed by Evett [2], Evett and Buckleton [3], and are based on survey data. Specifically they proposed a model for estimating Pk, k = 0, 1, 2, … and Sn, n = 1, 2, where Pk is the probability of finding k distinct sources (or groups) of glass on a person of interest (POI), and Sn is the probability that the kth source consists of n fragments. In this article we make a number of extensions to the work of Coulson et al. [1]. Firstly we derive an estimate of the uncertainty in the parameter of the Coulson et al. model, and show how this may be used—for example, to compute an estimate of how the probabilities may vary or how to compare estimates resulting from different surveys. Secondly, we extend the model by allowing a more sensible modelling of the “excess” zeros (in the case of the P terms) and excess ones (in the case of the S terms). The methodology used to make these extensions relies on purely frequentist theory of estimation in keeping with the original work. A Bayesian approach to estimation will be the subject of future work. Additionally, we demonstrate the use of an R (R Core Team, [4]) package, called fitPS (Curran, [5]) which makes the methodology described in this article easy to implement in practice.}
}
@article{WILCOX19961,
title = {Invited paper The role of expert systems in integrated curriculum design},
journal = {Expert Systems with Applications},
volume = {11},
number = {1},
pages = {1-11},
year = {1996},
issn = {0957-4174},
doi = {https://doi.org/10.1016/0957-4174(96)00001-2},
url = {https://www.sciencedirect.com/science/article/pii/0957417496000012},
author = {Lyle C. WiLcox},
abstract = {Knowledge-based systems and, particularly, expert systems, are bringing new thinking to how we view knowledge. Increasingly, we see knowledge as a commodity or an object, something generated, discovered, restructured, repackaged and deliverrd Powerful tools have been developed for expediting our work in manipulating knowledge. Knowedge-based systems provide the structure, the process and multidisciplinary integration tools to better understand and further develop projects in areas such as reengineering the organization, concurrent engineering, quality management and other critical management efforts in business and government. Curriculum development in education also benefits greatly from using the integrative powers of the KBS disciplines Examples taken from education focus on knowledge-based systems applied to the areas of K-12 course design, bachelor's degree curriculum design in integrated science, and graduate studies serving “professional transitioning.” Concepts of “idea fields” in organizational behavior are also discussed.}
}
@article{PTAK2017589,
title = {The Dorsal Frontoparietal Network: A Core System for Emulated Action},
journal = {Trends in Cognitive Sciences},
volume = {21},
number = {8},
pages = {589-599},
year = {2017},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2017.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1364661317301006},
author = {Radek Ptak and Armin Schnider and Julia Fellrath},
keywords = {action emulation, brain connectivity, frontoparietal network, spatial attention, working memory, motor control},
abstract = {The dorsal frontoparietal network (dFPN) of the human brain assumes a puzzling variety of functions, including motor planning and imagery, mental rotation, spatial attention, and working memory. How can a single network engage in such a diversity of roles? We propose that cognitive computations relying on the dFPN can be pinned down to a core function underlying offline motor planning: action emulation. Emulation creates a dynamic representation of abstract movement kinematics, sustains the internal manipulation of this representation, and ensures its maintenance over short time periods. Based on these fundamental characteristics, the dFPN has evolved from a pure motor control network into a domain-general system supporting various cognitive and motor functions.}
}
@article{ZHOU2019107534,
title = {Evaluation of mixing performance for the industrial-scale radial multiple jets-in-crossflow mixing structure},
journal = {Chemical Engineering and Processing - Process Intensification},
volume = {141},
pages = {107534},
year = {2019},
issn = {0255-2701},
doi = {https://doi.org/10.1016/j.cep.2019.107534},
url = {https://www.sciencedirect.com/science/article/pii/S025527011930087X},
author = {Meifang Zhou and Hao Jiang and Yanjie Hu and Zhimin Lu and Haibo Jiang and Chunzhong Li},
keywords = {Multiple jets mixing structure, Mixing performance, Computational fluid dynamics, Injection trajectory, Design strategy},
abstract = {The radial multiple jets-in-crossflow mixing structure (RMJCMS) is extensively used in industrial manufacture. In this research, flow behavior induced by an industrial-scale RMJCMS is investigated via computational fluid dynamics considering the example of the mixing of TiCl4 and O2 in the chloride process. The flow features of RMJCMS are accurately captured through simulation. The momentum ratio and size distribution of jet holes are notable elements for controlling the mixing performance. For an injection ring with single-diameter jet holes, the optimal momentum ratio was 3.2. To improve the mixing performance of RMJCMS, a design strategy that maximized the effect of convective mixing by modifying the distribution of the jet hole diameters was proposed based on the simulation results to guide the design of the injection ring. With the designed injection ring, the dimensionless mixing distance decreased by 50% compared with the equal-hole case, validating the effectiveness of the design strategy.}
}
@article{JIANG2024102049,
title = {Artificial intelligence and automation to power the future of chemistry},
journal = {Cell Reports Physical Science},
volume = {5},
number = {7},
pages = {102049},
year = {2024},
issn = {2666-3864},
doi = {https://doi.org/10.1016/j.xcrp.2024.102049},
url = {https://www.sciencedirect.com/science/article/pii/S2666386424003187},
author = {Xuefeng Jiang and Sanzhong Luo and Kuangbiao Liao and Shan Jiang and Jing Ma and Jun Jiang and Zhigang Shuai},
abstract = {In our traditional impression of chemical laboratories, researchers wear white coats and safety goggles to conduct experiments. However, many recent developments in the field make use of autonomous synthesis robots with integrated artificial intelligence (AI)-driven machine-learning units. These benchtop devices might outperform human chemists in terms of speed and accuracy, which could accelerate the discovery of molecules and materials for various applications. In this Voices piece, we ask a panel of experts from institutes in China: How are AI and automation shaping the future of chemistry?}
}
@article{VANDERPLAS2019101504,
title = {Advice-taking as a bridge between decision neuroscience and mental capacity},
journal = {International Journal of Law and Psychiatry},
volume = {67},
pages = {101504},
year = {2019},
issn = {0160-2527},
doi = {https://doi.org/10.1016/j.ijlp.2019.101504},
url = {https://www.sciencedirect.com/science/article/pii/S0160252719301190},
author = {Elisa {van der Plas} and Anthony S. David and Stephen M. Fleming},
keywords = {Capacity, Decision-making capacity, Advice-taking, Decision neuroscience},
abstract = {A person's capacity to process advice is an important aspect of decision making in the real world. For example, in decisions about treatment, the way patients respond to the advice of family, friends and medical professionals may be used (intentionally or otherwise) as a marker of the “use or weigh” requirement of decision-making capacity. Here we explore neuroscientific research on decision-making to identify features of advice-taking that help conceptualize this requirement. We focus on studies of the neural and computational basis of decision-making in laboratory settings. These studies originally investigated simple perceptual decisions about ambiguous stimuli, but have more recently been extended to more complex “value-based” decisions involving the comparison of subjective preferences. Value-based decisions are a useful model system for capacity-related decision-making as they do not have an objectively ‘correct’ answer and are instead based on subjective preferences. In this context, advice-taking can be seen as a process in which new evidence for one or other option is integrated, leading to altered behaviour or choices. We use this framework to distinguish between different types of advice-taking: private compliance consists of updating one's privately held beliefs based on new evidence, whereas in the case of public compliance, people change their behaviour at a surface level without shifting their privately-held beliefs. Importantly, both types of advice-taking may lead to similar outcomes but rely on different decision processes. We suggest that understanding how multiple mechanisms drive advice-taking holds promise for targeting decision-making support and improving our understanding of the use and weigh requirement in cases of contested capacity.}
}
@article{HYSMITH2024621,
title = {The future of self-driving laboratories: from human in the loop interactive AI to gamification},
journal = {Digital Discovery},
volume = {3},
number = {4},
pages = {621-636},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d4dd00040d},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X24000548},
author = {Holland Hysmith and Elham Foadian and Shakti P. Padhy and Sergei V. Kalinin and Rob G. Moore and Olga S. Ovchinnikova and Mahshid Ahmadi},
abstract = {Recent developments in artificial intelligence (AI) and machine learning (ML), implemented through self-driving laboratories (SDLs), are rapidly creating unprecedented opportunities for the accelerated discovery and optimization of materials. This paper provides a joint analysis of SDLs from both academic and industry perspectives, highlighting the importance of integrating human intelligence in these systems. It discusses the necessity of careful planning in SDL design across physical, data, and workflow dimensions, including instrumental setup, experimental workflow, data management, and human–SDL interaction. The significance of integrating human input within SDLs, especially as the focus shifts from individual tools and tasks to the creation and management of complex workflows, is emphasized. The paper stresses on the crucial role of reward function design in developing forward-looking workflows and examines the interplay between hardware evolution, ML application across chemical processes, and the influence of reward systems in research. Ultimately, the article advocates for a future where SDLs blend human intuition in hypothesis formulation with AI's precision, speed, and data-handling capabilities.}
}
@article{HAPPLE2017283,
title = {Effects of air infiltration modeling approaches in urban building energy demand forecasts},
journal = {Energy Procedia},
volume = {122},
pages = {283-288},
year = {2017},
note = {CISBAT 2017 International ConferenceFuture Buildings & Districts – Energy Efficiency from Nano to Urban Scale},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2017.07.323},
url = {https://www.sciencedirect.com/science/article/pii/S1876610217329260},
author = {Gabriel Happle and Jimeno A. Fonseca and Arno Schlueter},
keywords = {Urban Building Energy Modeling, Air Infiltration, Ventilation, Heating Energy Demand, Cooling Energy Demand, Forecasting, City Energy Analyst (CEA)},
abstract = {The air infiltration rate is a highly sensitive variable that influences heating and cooling demand forecasts in urban building energy modeling. This paper analyses the effect of two different simplified modeling techniques of air infiltration - fixed air change rate vs. a model based on wind pressure and air temperatures - on the heating and cooling demand in a district. The urban energy simulation toolbox City Energy Analyst (CEA) is used to simulate a case study in Switzerland, comprising of 24 buildings of various functions. Results indicate that despite the large differences for individual buildings, a fixed infiltration rate model could be sufficient for early design studies of district energy systems, as the impact on the sizing of district energy systems remains relatively low. This comparison will contribute to the continued development of urban energy simulations that are robust, as well as computationally fast.}
}
@article{AGBO2024100385,
title = {A systematic literature review on software applications used to support curriculum development and delivery in primary and secondary education},
journal = {International Journal of Educational Research Open},
volume = {7},
pages = {100385},
year = {2024},
issn = {2666-3740},
doi = {https://doi.org/10.1016/j.ijedro.2024.100385},
url = {https://www.sciencedirect.com/science/article/pii/S2666374024000670},
author = {Benjamin Agbo and Ceris Morris and Mogdam Osman and Joe Basketts and Theocharis Kyriacou},
keywords = {Systematic literature review (SLR), Primary education, Secondary education, Software applications},
abstract = {The evolution of educational software applications has revolutionised teaching and learning methodologies in primary and secondary education over the past decade. This paper conducts a review of primary studies based on N = 21 papers published between 2013 and 2023, focusing on the diverse landscape of software applications designed for student learning, curriculum development, delivery, and assessment. Findings from this study showcases a range of software solutions ranging from assessment tools to tutoring applications. Distinctive features supporting various aspects of teaching and learning, including lesson planning, delivery, management, assessment, and self-directed learning, were also identified. Regarding the features of software solutions used in primary and secondary schools, some differences were identified in terms of complexity, interactivity, assessment methodologies, and the collaborative functionalities of these tools. While highlighting the potential benefits, findings from this study also showed that challenges such as deployment costs, user self-efficacy, and technology anxiety are influential factors affecting the adoption of these technologies in primary and secondary educational settings. The evidence presented in this study serves as a resource for educational leaders and practitioners, facilitating a deeper understanding of available educational tools and essential considerations in the design and adoption of future tools.}
}
@incollection{VANCOUVER2018203,
title = {Chapter Six - Self-Efficacy’s Role in Unifying Self-Regulation Theories},
editor = {Andrew J. Elliot},
series = {Advances in Motivation Science},
publisher = {Elsevier},
volume = {5},
pages = {203-230},
year = {2018},
issn = {2215-0919},
doi = {https://doi.org/10.1016/bs.adms.2018.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S2215091918300051},
author = {Jeffrey B. Vancouver},
keywords = {Self-efficacy, Self-regulation, Social cognitive theory, Perceptual control theory, Computational modeling},
abstract = {On the way to finding a comprehensive theory of human behavior, protagonists from two perspectives—social cognitive theory and perceptual control theory—engaged in a debate over the roles of self-efficacy in determining human behavior. The debate represents a larger question of the role of cognitive constructs in explaining human behavior and the patience needed in elaborating a comprehensive, paradigm-level theory. Specifically, social cognitive theory could be seen as overemphasizing cognitive constructs, whereas perceptual control theory could be seen as underemphasizing them. Careful empirical and theoretical work, much of which is described here, provides a reconciliation and unification of these perspectives within a single, self-regulatory synthesis. This research and reconciliation demonstrates that self-efficacy can play multiple roles, some that negatively affect effort and performance and some that positively affect them. Yet, these roles are all useful for facilitating self-regulation. Moreover, the reconciliation demonstrates the usefulness of formally (e.g., with computational models) representing the processes the theory is meant to explain. Such a rendition makes clear the overlap and the contributions of each perspective to the larger goal of a paradigm for psychology.}
}
@article{HAN2014106,
title = {Toward an understanding of the impact of production pressure on safety performance in construction operations},
journal = {Accident Analysis & Prevention},
volume = {68},
pages = {106-116},
year = {2014},
note = {Systems thinking in workplace safety and health},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2013.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0001457513004041},
author = {SangUk Han and Farzaneh Saba and SangHyun Lee and Yasser Mohamed and Feniosky Peña-Mora},
keywords = {Safety, Systems thinking, Accident prevention, Simulation, Causal loop analysis},
abstract = {It is not unusual to observe that actual schedule and quality performances are different from planned performances (e.g., schedule delay and rework) during a construction project. Such differences often result in production pressure (e.g., being pressed to work faster). Previous studies demonstrated that such production pressure negatively affects safety performance. However, the process by which production pressure influences safety performance, and to what extent, has not been fully investigated. As a result, the impact of production pressure has not been incorporated much into safety management in practice. In an effort to address this issue, this paper examines how production pressure relates to safety performance over time by identifying their feedback processes. A conceptual causal loop diagram is created to identify the relationship between schedule and quality performances (e.g., schedule delays and rework) and the components related to a safety program (e.g., workers’ perceptions of safety, safety training, safety supervision, and crew size). A case study is then experimentally undertaken to investigate this relationship with accident occurrence with the use of data collected from a construction site; the case study is used to build a System Dynamics (SD) model. The SD model, then, is validated through inequality statistics analysis. Sensitivity analysis and statistical screening techniques further permit an evaluation of the impact of the managerial components on accident occurrence. The results of the case study indicate that schedule delays and rework are the critical factors affecting accident occurrence for the monitored project.}
}
@incollection{DILLIWAR2020137,
title = {Chapter 8 - Cognitive and brain function analysis of sleeping stage electroencephalogram wave using parallelization},
editor = {G.R. Sinha and Jasjit S. Suri},
booktitle = {Cognitive Informatics, Computer Modelling, and Cognitive Science},
publisher = {Academic Press},
pages = {137-160},
year = {2020},
isbn = {978-0-12-819443-0},
doi = {https://doi.org/10.1016/B978-0-12-819443-0.00008-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128194430000088},
author = {Vikas Dilliwar and Mridu Sahu},
keywords = {Cognitive neuroscience, electroencephalogram (EEG), channel selection, EEG waves, coherence estimation, sleeping stages, parallel, distributed processing},
abstract = {Cognitive neuroscience is an imperative division of cognitive science. In cognitive neuroscience different types of brain signaling and imaging techniques are used. An electroencephalogram (EEG) is the effective noninvasive brain signaling technique to understand the mechanism of brain activity. This is helpful to neurologists, scientists, and researchers to understand the causes of brain activity and diagnosis of brain disorders. Due to large amount of intensives and complex data owned by EEG, it requires a high-power computing device or a cluster of computers for computation. In this chapter, the fundamental details of EEG signal processing and coherence estimation–based channel selection model as a case study for sleep stages classification problem using parallel and distributed computing for minimizing the execution cost in terms of time and resource utilization are described.}
}
@incollection{SANFT20201,
title = {Unit 1 - Preliminaries: models, R, and lab techniques},
editor = {Rebecca Sanft and Anne Walter},
booktitle = {Exploring Mathematical Modeling in Biology Through Case Studies and Experimental Activities},
publisher = {Academic Press},
pages = {1-27},
year = {2020},
isbn = {978-0-12-819595-6},
doi = {https://doi.org/10.1016/B978-0-12-819595-6.00007-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128195956000074},
author = {Rebecca Sanft and Anne Walter},
keywords = {data structures, plotting, linear regression, for loops, micropipettes, absorbance spectroscopy, serial dilution},
abstract = {This section provides a brief introduction to mathematical modeling in biology and basic programming and lab skills. The purpose of this section is to build the reader's confidence and skill set in R and the lab before they engage in the modeling process. The R Basics section is for those who are new or relative novices in the use of R or other programming languages and introduces data structures, plotting, importing data, and linear regression. Exercises engage students with the material and reinforce the concepts presented. This section of the book may be extremely helpful on its own or to any course or team project that requires R. The Prelab Lab introduces pipettes, units, spectrophotometry, dilutions, and data analysis. Again, exercises are embedded to help users practice using the equipment and thinking about the methods they are using to collect data.}
}
@article{BOLINSKA2023107,
title = {Epistemic expression in the determination of biomolecular structure},
journal = {Studies in History and Philosophy of Science},
volume = {100},
pages = {107-115},
year = {2023},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2023.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0039368123000894},
author = {Agnes Bolinska},
keywords = {Scientific representation, Modeling, Efficiency, DNA, Protein structure},
abstract = {Scientific research is constrained by limited resources, so it is imperative that it be conducted efficiently. This paper introduces the notion of epistemic expression, a kind of representation that expedites the solution of research problems. Epistemic expressions are representations that (i) contain information in a way that enables more reliable information to place the most stringent constraints on possible solutions and (ii) make new information readily extractible by biasing the search through that space. I illustrate these conditions using historical and contemporary examples of biomolecular structure determination. Then, I argue that the notion of epistemic expression parts ways with pragmatic accounts of scientific representation and an understanding of models as artifacts, neither of which require models to accurately represent. Explicating epistemic expression thus fills a gap in our understanding of scientific practice, extending Morrison and Morgan's (1999) conception of models as investigative instruments.}
}
@article{CALDERON2022600,
title = {A conceptual framework for modeling the supply side of mobility services within large-scale agent-based travel demand models},
journal = {Transportation Letters},
volume = {14},
number = {6},
pages = {600-609},
year = {2022},
issn = {1942-7867},
doi = {https://doi.org/10.1080/19427867.2021.1913303},
url = {https://www.sciencedirect.com/science/article/pii/S1942786722004295},
author = {Francisco Calderón and Eric J. Miller},
keywords = {conceptual framework, supply, mobility services, agent-based models, mobility as a service},
abstract = {ABSTRACT
The state-of-the-art of mobility services among large-scale agent-based travel demand models has focused predominantly on the demand side, whereas complexities of the supply side have been given much less attention. Conventional definitions must be revisited to establish solid foundations for model implementations. First, several mobility services are characterized from abstract attributes, by all agents involved, and by operational tasks involved in service provision. Then, a high-level generic service provision process is developed to allow instantiation of various services. Extensions for conventional travel demand model systems are also developed, including: a mobility services component containing service provider agentsand their operational tasks integrated logically into service provision processes; a fleet component containing vehicle agents of each service and a driver activity model when human-driven; a MaaS component containing mobility provider agents that coordinate and integrate mobility services. Generic modeling issues are also discussed, including treatment of time, data structures, and computational efficiency.}
}
@article{SUN2020227964,
title = {Data-driven reinforcement-learning-based hierarchical energy management strategy for fuel cell/battery/ultracapacitor hybrid electric vehicles},
journal = {Journal of Power Sources},
volume = {455},
pages = {227964},
year = {2020},
issn = {0378-7753},
doi = {https://doi.org/10.1016/j.jpowsour.2020.227964},
url = {https://www.sciencedirect.com/science/article/pii/S0378775320302676},
author = {Haochen Sun and Zhumu Fu and Fazhan Tao and Longlong Zhu and Pengju Si},
keywords = {Fuel cell hybrid electric vehicle, Energy management strategy, Reinforcement learning, Data driven, Hierarchical power splitting},
abstract = {A reinforcement-learning-based energy management strategy is proposed in this paper for managing energy system of Fuel Cell Hybrid Electric Vehicles (FCHEV) equipped with three power sources. A hierarchical power splitting structure is employed to shrink large state-action space based on an adaptive fuzzy filter. Then, the reinforcement-learning-based algorithm using Equivalent Consumption Minimization Strategy (ECMS) is proposed for tackling high-dimensional state-action space, and finding a trade-off between global learning and real-time implementation. The power splitting policy based on experimental data is obtained by using reinforcement learning algorithm, which allows for many different driving cycles and traffic conditions. The proposed energy management strategy can achieve low computation cost, optimal fuel cell efficiency and energy consumption economy. Simulation results confirm that, compared with existing learning algorithms and optimization methods, the proposed reinforcement-learning-based energy management strategy using ECMS can achieve high computation efficiency, lower power fluctuation of fuel cell and optimal fuel economy of FCHEV.}
}
@article{AVINERI2012512,
title = {On the use and potential of behavioural economics from the perspective of transport and climate change},
journal = {Journal of Transport Geography},
volume = {24},
pages = {512-521},
year = {2012},
note = {Special Section on Theoretical Perspectives on Climate Change Mitigation in Transport},
issn = {0966-6923},
doi = {https://doi.org/10.1016/j.jtrangeo.2012.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0966692312000646},
author = {Erel Avineri},
keywords = {Behavioural economics, Travel behaviour, Nudge},
abstract = {It can be argued that the main thinking in transport planning and policy making stem from neoclassical economics in which individuals are largely assumed to make rational, consistent, and efficient choices, and apply cognitive processes of decision making that maximise their economic utility. Research in behavioural sciences indicates that individuals’ choices in a wide range of contexts deviate from the predictions of the rational man paradigm inspired the research agenda in the field of travel behaviour. New concepts and practices of government aim to apply some behavioural economics insights in the design of behavioural change initiatives and measures, an approach recently advocated in the US and the UK. This paper provides a brief review on the use and potential of behavioural economics from the perspective of transport and climate change, in two main contexts: travel demand modelling and design of behaviour change measures. The discussion of limitations and knowledge gaps associated with the implementation of behavioural economics to a travel behaviour context might contribute to the debate and help in defining research agenda in this area.}
}
@article{LO2014358,
title = {Assembling the unexpected inspiration–from linking to jigsaw},
journal = {Frontiers of Architectural Research},
volume = {3},
number = {4},
pages = {358-367},
year = {2014},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2014.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S2095263514000247},
author = {Chia-Hui Nico Lo and Teng-Wen Chang and Ih-Cheng Lai},
keywords = {Idea linking, Puzzle solving, Design Jigsaw, Control strategy, Assembling, Early phrase of design},
abstract = {Linking pieces of design information for inspiration are an important part of the early phase of the design process. One key linking operation is assembling, wherein designers create new ideas by assembling partial or whole pieces of ideas together. How designers assemble the ideas reflect their design process. Hence, by developing a computational tool for assembling ideas, the underlying rules of design decision-making might be revealed. In this research, we employed a computational design method consisting of methodological mapping (jigsaw) and consequential analysis (Design Jigsaw system prototype) to create associations between varied types of information at different levels in the design information hierarchy. We then propose a system prototype called Design Jigsaw, based on the analysis of five representation schemes with network-like structures and sound delegation mechanisms. We also developed and explored the representation, components, and the control mechanisms involved in these components. The algorithm of the two main control strategies, grouping and matching/combining, is described in detail along with the procedural description of a jigsaw solving session. Furthermore, we conducted a design experiment to reify the process of the Design Jigsaw system prototype.}
}
@article{SARASSO202464,
title = {Nature heals: An informational entropy account of self-organization and change in field psychotherapy},
journal = {Physics of Life Reviews},
volume = {51},
pages = {64-84},
year = {2024},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2024.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S1571064524001167},
author = {Pietro Sarasso and Wolfgang Tschacher and Felix Schoeller and Gianni Francesetti and Jan Roubal and Michela Gecele and Katiuscia Sacco and Irene Ronga},
keywords = {Field-based psychotherapy, Free-energy principle, Synchronization, Biophysics, Entropy, Neurophenomenology},
abstract = {This paper reviews biophysical models of psychotherapeutic change based on synergetics and the free energy principle. These models suggest that introducing sensory surprise into the patient-therapist system can lead to self-organization and the formation of new attractor states, disrupting entrenched patterns of thoughts, emotions, and behaviours. We propose that the therapist can facilitate this process by cultivating epistemic trust and modulating embodied attention to allow surprising affective states to enter shared awareness. Transient increases in free energy enable the update of generative models, expanding the range of experiences available within the patient-therapist phenomenal field. We hypothesize that patterns of disorganization at behavioural and physiological levels, indexed by increased entropy, complexity, and lower determinism, are key markers and predictors of psychotherapeutic gains. Future research should investigate how the therapist's openness to novelty shapes therapeutic outcomes.}
}
@article{BORJI2023101027,
title = {On students' understanding of volumes of solids of revolution: An APOS analysis},
journal = {The Journal of Mathematical Behavior},
volume = {70},
pages = {101027},
year = {2023},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2022.101027},
url = {https://www.sciencedirect.com/science/article/pii/S0732312322000955},
author = {Vahid Borji and Rafael Martínez-Planell},
keywords = {Volumes of solids of revolution, Riemann sums, Visualization, APOS, Textbook, Integral},
abstract = {We apply Action-Process-Object-Schema theory (APOS) to (1) examine the textbook used by students in the study to infer mental constructions the book proposes that students could do to understand solids of revolution, (2) use semi-structured interviews with nine students to find out which of the textbook’s proposed constructions students actually do, and what unconjectured or unexpected constructions students do, and (3) use the results of the interviews and the research literature to inform and set forth an alternative proposal for constructing volumes of revolution. Results suggest that many students have not constructed processes to visualize solids of revolution and to relate Riemann sums to the corresponding definite integrals. Implications for curriculum and instruction are discussed.}
}
@article{HUANG2011183,
title = {On the intrinsic inevitability of cancer: From foetal to fatal attraction},
journal = {Seminars in Cancer Biology},
volume = {21},
number = {3},
pages = {183-199},
year = {2011},
note = {Why Systems Biology and Cancer?},
issn = {1044-579X},
doi = {https://doi.org/10.1016/j.semcancer.2011.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S1044579X11000320},
author = {Sui Huang},
keywords = {Tumorigenesis, Tumour progression, Epigenetic landscape, Gene regulatory network, Attractor, State space, Somatic mutation, Oncogene},
abstract = {The cracks in the paradigm of oncogenic mutations and somatic evolution as driving force of tumorigenesis, lucidly exposed by the dynamic heterogeneity of “cancer stem cells” or the diffuse results of cancer genome sequencing projects, indicate the need for a more encompassing theory of cancer that reaches beyond the current proximate explanations based on individual genetic pathways. One such integrative concept, derived from first principles of the dynamics of gene regulatory networks, is that cancerous cell states are attractor states, just like normal cell types are. Here we extend the concept of cancer attractors to illuminate a more profound property of cancer initiation: its inherent inevitability in the light of metazoan evolution. Using Waddington's Epigenetic Landscape as a conceptual aid, for which we present a mathematical and evolutionary foundation, we propose that cancer is intrinsically linked to ontogenesis and phylogenesis. This explanatory rather than enumerating review uses a formal argumentation structure that is atypical in modern experimental biology but may hopefully offer a new coherent perspective to reconcile many conflicts between new findings and the old thinking in the categories of linear oncogenic pathways.}
}
@incollection{LIANG2013147,
title = {7 - Conclusions},
editor = {Kung-Hao Liang},
booktitle = {Bioinformatics for Biomedical Science and Clinical Applications},
publisher = {Woodhead Publishing},
pages = {147-149},
year = {2013},
series = {Woodhead Publishing Series in Biomedicine},
isbn = {978-1-907568-44-2},
doi = {https://doi.org/10.1533/9781908818232.147},
url = {https://www.sciencedirect.com/science/article/pii/B9781907568442500073},
author = {Kung-Hao Liang},
keywords = {conceptual framework, adaptive model},
abstract = {Abstract:
Conducting contemporary biomedical science is an art. Akin to food preparation, the bioinformatician needs to minimize the alteration of data (the food ingredients), but maximize its potential new knowledge and the visual presentations to biology (taste, flavor and aroma). Conceptual frameworks and adaptive computational models are two resources for elevating biomedical science to new heights.}
}
@article{NAZEEM2010267,
title = {Optimal deadlock avoidance for complex resource allocation systems through classification theory},
journal = {IFAC Proceedings Volumes},
volume = {43},
number = {12},
pages = {267-274},
year = {2010},
note = {10th IFAC Workshop on Discrete Event Systems},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20100830-3-DE-4013.00045},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015324678},
author = {Ahmed Nazeem and Spyros A. Reveliotis and Yin Wang and Stéphane Lafortune},
keywords = {Deadlock, Supervisory Control, Discrete Event Systems, Classifiers, Optimality},
abstract = {Most of the past research on the problem of deadlock avoidance for sequential complex resource allocation systems (RAS) has acknowledged the fact that the maximally permissive deadlock avoidance policy (DAP) possesses super-polynomial complexity for most RAS classes, and it has resorted to solutions that trade off maximal permissiveness for computational tractability. In this work, we seek the effective implementation of the maximally permissive DAP for a broad spectrum of RAS, by distinguishing between the off-line and the on-line computation that is required for the specification of this policy, and developing a representation of the derived result that will require minimal on-line computation. The particular representation that we adopt is that of a compact classifier that will effect the underlying dichotomy of the reachable state space into safe and unsafe subspaces. Through a series of reductions of the posed classification problem, we are also able to attain extensive reductions in the computational complexity of the off-line task of the construction of the sought classifier. A series of computational experiments demonstrate the efficacy of the proposed approach and establish its ability to provide tractable implementations of the maximally permissive DAP for problem instances significantly beyond the capacity of any other approach currently available in the literature.}
}
@article{ALTUNTAS2023156,
title = {Some characterizations of Generalized Top Trading Cycles},
journal = {Games and Economic Behavior},
volume = {141},
pages = {156-181},
year = {2023},
issn = {0899-8256},
doi = {https://doi.org/10.1016/j.geb.2023.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0899825623000672},
author = {Açelya Altuntaş and William Phan and Yuki Tamura},
keywords = {Behavioral market design, Endowment manipulation, Heuristic manipulation, Strategy-proofness, Object exchange, Generalized Top Trading Cycles},
abstract = {Consider object exchange problems when each agent may be endowed with and consume more than one object. For most domains of preferences, no rule satisfies efficiency, the endowment lower bound, and strategy-proofness. Insisting on the first two properties, we explore the extent to which weaker incentive compatibility can be achieved. Motivated by behavioral and computational considerations as well as online mechanisms, we define several forms of manipulation. We consider the lexicographic domain of preferences, and provide several characterizations of Generalized Top Trading Cycles based on properties concerning immunity from heuristic and identity-splitting manipulations. We also show that this establishes a boundary with respect to incentive compatibility—minimal strengthening results in impossibility.}
}
@article{MOORE20229,
title = {Slow or sudden: Re-interpreting the learning curve for modern systems neuroscience},
journal = {IBRO Neuroscience Reports},
volume = {13},
pages = {9-14},
year = {2022},
issn = {2667-2421},
doi = {https://doi.org/10.1016/j.ibneur.2022.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S2667242122000367},
author = {Sharlen Moore and Kishore V. Kuchibhotla},
keywords = {Learning, Instrumental learning, Behavior, Systems neuroscience, Large-scale recordings, Big data, Goal-directed learning, circuit, Stimulus-response, Acquisition},
abstract = {Learning is fundamental to animal survival. Animals must learn to link sensory cues in the environment to actions that lead to reward or avoid punishment. Rapid learning can then be highly adaptive and the difference between life or death. To explore the neural dynamics and circuits that underlie learning, however, has typically required the use of laboratory paradigms with tight control of stimuli, action sets, and outcomes. Learning curves in such reward-based tasks are reported as slow and gradual, with animals often taking hundreds to thousands of trials to reach expert performance. The slow, highly variable, and incremental learning curve remains the largely unchallenged belief in modern systems neuroscience. Here, we provide historical and contemporary evidence that instrumental forms of reward-learning can be dissociated into two parallel processes: knowledge acquisition which is rapid with step-like improvements, and behavioral expression which is slower and more variable. We further propose that this conceptual distinction may allow us to isolate the associative (knowledge-related) and non-associative (performance-related) components that influence learning. We then discuss the implications that this revised understanding of the learning curve has for systems neuroscience.}
}
@article{MAKKAR2019381,
title = {Cognitive spammer: A Framework for PageRank analysis with Split by Over-sampling and Train by Under-fitting},
journal = {Future Generation Computer Systems},
volume = {90},
pages = {381-404},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.07.046},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18305703},
author = {Aaisha Makkar and Neeraj Kumar},
keywords = {Internet of Things(IoT), Cognitive IoT, Web spam, PageRank},
abstract = {From the past few years, there is an exponential increase in one of the most popular technologies of the modern era called as Internet of Things (IoT). In IoT, various objects perform the tasks of sensing, communication, and computation for providing uninterrupted services (e.g., e-health, e-transportation, security access, etc.) to the end users. In this era, Cognitive Internet of Things (CIoT) is an another paradigm of IoT developed to enhance the capabilities of intelligence in IoT objects where these objects can take independent decisions in any environment. IoT follows the service oriented architecture (SOA), in which the application layer is the topmost layer. It enables the IoT objects to interact with the other objects located across the globe. The power of learning, thinking, and understanding by these objects, can make the information access more accurate and reliable but Web spam is one of the challenges while accessing information from the web. It has been observed from the literature review that search engines are preferred mostly by the people for accessing information. The efficient ranking by the search engines can reduce the computational cost of information exchange by IoT objects. Search engines should be able to prevent the spam from being injected into the web. But, the existing techniques for this problem target in finding the spam after its occurrence in search engine result pages. So, in this proposal, we present an intelligent cognitive spammer framework, Cognitive spammer, which eliminates the spam pages during the web page rank score calculation by search engines. The framework update the Google’s ranking algorithm, PageRank in such a way that it automatically prevents link spam by considering the link structure of web for rank score computation. The updated PageRank algorithm provided the better ranking of web pages. The proposed framework is validated with the WEBSPAM-UK2007 dataset. Before processing, the dataset is preprocessed with a new technique, called as ‘Split by Over-sampling and Train by Under-fitting’ to remove the trade off between imbalanced instances of target class. After data cleaning, we applied machine learning techniques (Bagged model, Boosted linear model, etc) with the web page features to make accurate predictions. The detection classifiers only consider the link features of the web page irrespective of the page content. Out of the fifteen classifiers, best three are ensemble, which results in better performance with overall accuracy improvement. Ten-fold cross validation has also been applied with the resulted ensemble model, which results in getting the accuracy of 99.6% in the proposed scheme.}
}
@article{HAN2024122413,
title = {Walrus optimizer: A novel nature-inspired metaheuristic algorithm},
journal = {Expert Systems with Applications},
volume = {239},
pages = {122413},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122413},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423029159},
author = {Muxuan Han and Zunfeng Du and Kum Fai Yuen and Haitao Zhu and Yancang Li and Qiuyu Yuan},
keywords = {Walrus Optimizer (WO), Metaheuristic algorithm, Swarm intelligence, Exploration, Exploitation},
abstract = {Metaheuristic algorithms are intelligent optimization approaches that lead the searching procedure through utilizing exploitation and exploration. The increasing complexity of real-world optimization problem has prompted the development of more metaheuristic algorithms. Hence, this work proposes a novel swarm intelligence algorithm, Walrus optimizer (WO). It is inspired by the behaviors of walruses that choose to migrate, breed, roost, feed, gather and escape by receiving key signals (danger signals and safety signals). To test the capability of the proposed algorithm, 23 standard functions and the benchmark suite from the IEEE (Institute of Electrical and Electronics Engineers) Congress on Evolutionary Computation (CEC) 2021 are used. In addition, to evaluate the practicability of the proposed algorithm to solve various real-world optimization problems, 6 standard classical engineering optimization problems are examined and compared. For statistical purposes, 100 independent optimization runs are conducted to determine the statistical measurements, including the mean, standard deviation, and the computation time of the program, by considering a predefined stopping criterion. Some well-known statistical analyses are also used for comparative purposes, including the Friedman and Wilcoxon analysis. The results demonstrate that the proposed algorithm can provide special stability features and very competitive performance in dealing with high-dimensional benchmarks and real-world problems. The proposal of WO promotes the continuous development and application expansion of artificial intelligence, improves the efficiency of optimization calculation, and provides powerful tools for solving complex problems in the real world. The source code of WO is publicly availabe at https://ww2.mathworks.cn/matlabcentral/fileexchange/154702-walrus-optimizer-wo.}
}
@article{TURKHEIMER20193,
title = {Conflicting emergences. Weak vs. strong emergence for the modelling of brain function},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {99},
pages = {3-10},
year = {2019},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2019.01.023},
url = {https://www.sciencedirect.com/science/article/pii/S0149763418308315},
author = {Federico E. Turkheimer and Peter Hellyer and Angie A. Kehagia and Paul Expert and Louis-David Lord and Jakub Vohryzek and Jessica {De Faria Dafflon} and Mick Brammer and Robert Leech},
keywords = {Brain, Emergence, Weak emergence, Strong emergence, Computational models, Bayesian inference, Free energy principle, Integrated information theory, Oscillators, Multi-scale},
abstract = {The concept of “emergence” has become commonplace in the modelling of complex systems, both natural and man-made; a functional property” emerges” from a system when it cannot be readily explained by the properties of the system’s sub-units. A bewildering array of adaptive and sophisticated behaviours can be observed from large ensembles of elementary agents such as ant colonies, bird flocks or by the interactions of elementary material units such as molecules or weather elements. Ultimately, emergence has been adopted as the ontological support of a number of attempts to model brain function. This manuscript aims to clarify the ontology of emergence and delve into its many facets, particularly into its “strong” and “weak” versions that underpin two different approaches to the modelling of behaviour. The first group of models is here represented by the “free energy” principle of brain function and the “integrated information theory” of consciousness. The second group is instead represented by computational models such as oscillatory networks that use mathematical scalable representations to generate emergent behaviours and are then able to bridge neurobiology with higher mental functions. Drawing on the epistemological literature, we observe that due to their loose mechanistic links with the underlying biology, models based on strong forms of emergence are at risk of metaphysical implausibility. This, in practical terms, translates into the over determination that occurs when the proposed model becomes only one of a large set of possible explanations for the observable phenomena. On the other hand, computational models that start from biologically plausible elementary units, hence are weakly emergent, are not limited by ontological faults and, if scalable and able to realistically simulate the hierarchies of brain output, represent a powerful vehicle for future neuroscientific research programmes.}
}
@article{GOODMAN2016818,
title = {Pragmatic Language Interpretation as Probabilistic Inference},
journal = {Trends in Cognitive Sciences},
volume = {20},
number = {11},
pages = {818-829},
year = {2016},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2016.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S136466131630122X},
author = {Noah D. Goodman and Michael C. Frank},
abstract = {Understanding language requires more than the use of fixed conventions and more than decoding combinatorial structure. Instead, comprehenders make exquisitely sensitive inferences about what utterances mean given their knowledge of the speaker, language, and context. Building on developments in game theory and probabilistic modeling, we describe the rational speech act (RSA) framework for pragmatic reasoning. RSA models provide a principled way to formalize inferences about meaning in context; they have been used to make successful quantitative predictions about human behavior in a variety of different tasks and situations, and they explain why complex phenomena, such as hyperbole and vagueness, occur. More generally, they provide a computational framework for integrating linguistic structure, world knowledge, and context in pragmatic language understanding.}
}
@article{FLEMING201462,
title = {Visual perception of materials and their properties},
journal = {Vision Research},
volume = {94},
pages = {62-75},
year = {2014},
issn = {0042-6989},
doi = {https://doi.org/10.1016/j.visres.2013.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0042698913002782},
author = {Roland W. Fleming},
keywords = {Materials, Surface perception, Computational models, Theory},
abstract = {Misidentifying materials—such as mistaking soap for pâté, or vice versa—could lead to some pretty messy mishaps. Fortunately, we rarely suffer such indignities, thanks largely to our outstanding ability to recognize materials—and identify their properties—by sight. In everyday life, we encounter an enormous variety of materials, which we usually distinguish effortlessly and without error. However, despite its subjective ease, material perception poses the visual system with some unique and significant challenges, because a given material can take on many different appearances depending on the lighting, viewpoint and shape. Here, I use observations from recent research on material perception to outline a general theory of material perception, in which I suggest that the visual system does not actually estimate physical parameters of materials and objects. Instead—I argue—the brain is remarkably adept at building ‘statistical generative models’ that capture the natural degrees of variation in appearance between samples. For example, when determining perceived glossiness, the brain does not estimate parameters of the BRDF. Instead, it uses a constellation of low- and mid-level image measurements to characterize the extent to which the surface manifests specular reflections. I argue that these ‘statistical appearance models’ are both more expressive and easier to compute than physical parameters, and therefore represent a powerful middle way between a ‘bag of tricks’ and ‘inverse optics’.}
}
@article{NIEDERBRUCKER2011126,
title = {Effcient Solution of Evolution Models for Virus Populations},
journal = {Procedia Computer Science},
volume = {4},
pages = {126-135},
year = {2011},
note = {Proceedings of the International Conference on Computational Science, ICCS 2011},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2011.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S187705091100072X},
author = {Gerhard Niederbrucker and Wilfried N. Gansterer},
keywords = {Evolution models for virus populations, Quasispecies, Eigenvector computation, Structured eigenvalue problem},
abstract = {The computation of the quasispecies in Eigen's quasispecies model requires the solution of a very large scale eigenvalue problem. Since the problem dimension is of an exponential growing nature the well known methods for dealing with such a problem run out of resources already far away from practically relevant cases. We propose the use of an implicit matrix vector product using the special problem structure as building block for eigenvalue solvers to partially overcome the exponential growth, which let us reach unexpected large problem sizes. As we will show our implicit matrix vector product is a prime example for an algorithm perfectly matching the requirements of GPU computing since it has low space and high parallel computation requirements. Therefore we will also present an GPU implementation delivering a speedup factor of about 100 compared to a standard implementation.}
}
@article{MOGILNER2011692,
title = {Modeling cellular processes in 3D},
journal = {Trends in Cell Biology},
volume = {21},
number = {12},
pages = {692-700},
year = {2011},
issn = {0962-8924},
doi = {https://doi.org/10.1016/j.tcb.2011.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0962892411001978},
author = {Alex Mogilner and David Odde},
abstract = {Recent advances in photonic imaging and fluorescent protein technology offer unprecedented views of molecular space–time dynamics in living cells. At the same time, advances in computing hardware and software enable modeling of ever more complex systems, from global climate to cell division. As modeling and experiment become more closely integrated we must address the issue of modeling cellular processes in 3D. Here, we highlight recent advances related to 3D modeling in cell biology. While some processes require full 3D analysis, we suggest that others are more naturally described in 2D or 1D. Keeping the dimensionality as low as possible reduces computational time and makes models more intuitively comprehensible; however, the ability to test full 3D models will build greater confidence in models generally and remains an important emerging area of cell biological modeling.}
}
@article{PACKWOOD2022100265,
title = {Machine Learning in Materials Chemistry: An Invitation},
journal = {Machine Learning with Applications},
volume = {8},
pages = {100265},
year = {2022},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2022.100265},
url = {https://www.sciencedirect.com/science/article/pii/S2666827022000093},
author = {Daniel Packwood and Linh Thi Hoai Nguyen and Pierluigi Cesana and Guoxi Zhang and Aleksandar Staykov and Yasuhide Fukumoto and Dinh Hoa Nguyen},
keywords = {Materials chemistry, Kernelized machine learning, Density functional theory, Bayesian optimization, Ensemble methods, Reinforcement learning, Federated learning},
abstract = {Materials chemistry is being profoundly influenced by the uptake of machine learning methodologies. Machine learning techniques, in combination with established techniques from computational physics, promise to accelerate the discovery of new materials by elucidating complex structure–property relationships from massive material databases. Despite exciting possibilities, further methodological developments call for a greater synergism between materials chemists, physicists, and engineers on one side, with computer science and math majors on the other. In this review, we provide a non-exhaustive account of machine learning in materials chemistry for computer scientists and applied mathematicians, with an emphasis on molecule datasets and materials chemistry problems. The first part of this review provides a tutorial on how to prepare such datasets for subsequent model building, with an emphasis on the construction of feature vectors. We also provide a self-contained introduction to density functional theory, a method from computational physics which is widely used to generate datasets and compute response variables. The second part reviews two machine learning methodologies which represent the status quo in materials chemistry at present – kernelized machine learning and Bayesian machine learning – and discusses their application to real datasets. In the third part of the review, we introduce some emerging machine learning techniques which have not been widely adopted by materials scientists and therefore present potential avenues for computer science and applied math majors. In the final concluding section, we discuss some recent machine learning-based approaches to real materials discovery problems and speculate on some promising future directions.}
}
@incollection{DOEWES2024255,
title = {Chapter Thirteen - Human AI: Social robot decision-making using emotional AI and neuroscience},
editor = {Muskan Garg and Deepika Koundal},
booktitle = {Emotional AI and Human-AI Interactions in Social Networking},
publisher = {Academic Press},
pages = {255-286},
year = {2024},
isbn = {978-0-443-19096-4},
doi = {https://doi.org/10.1016/B978-0-443-19096-4.00013-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443190964000134},
author = {Rumi Iqbal Doewes and Sapta Kunta Purnama and Islahuzzaman Nuryadin and Nughthoh Arfawi Kurdhi},
keywords = {Decision-making, Emotional artificial intelligence, HumanResource interview, Neuroscience, Social robot},
abstract = {A socially intelligent robot can analyze information, is continually aware of its social environment, and can respond in ways that are consistent with human social norms. A socially intelligent robot is one who can converse with other socially intelligent robots. It should also be able to assimilate information, think abstractly about it, generate new ideas, and automatically influence decisions in its favor. The conscious mind is required at this stage for the development of an emotional body. A feeling-capable body is required because emotions are important for distinguishing oneself from others and inspiring action. The goal was to move social robotics forward. Our research intends to develop social robots with human-like qualities. We created social robots using our own design that could converse with people more naturally. One of our ultimate goals was to create social robots with more human-like interaction abilities. The social emotional artificial intelligence (SEAI) is a mind designed specifically for sentient robots with emotions and a sense of self. The goal of the combined system was to accurately portray the users' emotional states and logical cognitive processes. Its development was largely influenced by biological research. It employs a deliberative/reactive paradigm, with a reactive paradigm handling low-level processing and control and a knowledge-based expert system handling high-level symbolic reasoning. These systems work in tandem to ensure flawless performance. Before presenting the scientific foundations of the SEAI framework and a computational formalization of those foundations, an overview of numerous cognitive systems with biological inspiration will be provided. The design is then thoroughly technical, with major parallels to the human cognitive system highlighted. This research is being conducted to assist researchers in better understanding how artificial emotions influence the robot's decisions and judgments.}
}