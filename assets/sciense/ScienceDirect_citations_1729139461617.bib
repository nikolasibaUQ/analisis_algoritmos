@article{SCHAEFER2024108796,
title = {GPT-4 as a biomedical simulator},
journal = {Computers in Biology and Medicine},
volume = {178},
pages = {108796},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108796},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524008813},
author = {Moritz Schaefer and Stephan Reichl and Rob {ter Horst} and Adele M. Nicolas and Thomas Krausgruber and Francesco Piras and Peter Stepper and Christoph Bock and Matthias Samwald},
keywords = {Biomedical simulation, Large language models, GPT-4, Computational biology, Artificial intelligence},
abstract = {Background
Computational simulation of biological processes can be a valuable tool for accelerating biomedical research, but usually requires extensive domain knowledge and manual adaptation. Large language models (LLMs) such as GPT-4 have proven surprisingly successful for a wide range of tasks. This study provides proof-of-concept for the use of GPT-4 as a versatile simulator of biological systems.
Methods
We introduce SimulateGPT, a proof-of-concept for knowledge-driven simulation across levels of biological organization through structured prompting of GPT-4. We benchmarked our approach against direct GPT-4 inference in blinded qualitative evaluations by domain experts in four scenarios and in two quantitative scenarios with experimental ground truth. The qualitative scenarios included mouse experiments with known outcomes and treatment decision support in sepsis. The quantitative scenarios included prediction of gene essentiality in cancer cells and progression-free survival in cancer patients.
Results
In qualitative experiments, biomedical scientists rated SimulateGPT's predictions favorably over direct GPT-4 inference. In quantitative experiments, SimulateGPT substantially improved classification accuracy for predicting the essentiality of individual genes and increased correlation coefficients and precision in the regression task of predicting progression-free survival.
Conclusion
This proof-of-concept study suggests that LLMs may enable a new class of biomedical simulators. Such text-based simulations appear well suited for modeling and understanding complex living systems that are difficult to describe with physics-based first-principles simulations, but for which extensive knowledge is available as written text. Finally, we propose several directions for further development of LLM-based biomedical simulators, including augmentation through web search retrieval, integrated mathematical modeling, and fine-tuning on experimental data.}
}
@article{GAO20222707,
title = {Similarity reductions for a generalized (3+1)-dimensional variable-coefficient B-type Kadomtsev–Petviashvili equation in fluid dynamics},
journal = {Chinese Journal of Physics},
volume = {77},
pages = {2707-2712},
year = {2022},
issn = {0577-9073},
doi = {https://doi.org/10.1016/j.cjph.2022.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S0577907322001228},
author = {Xin-Yi Gao and Yong-Jiang Guo and Wen-Rui Shan},
keywords = {Fluid dynamics, Generalized (3+1)-dimensional variable-coefficient B-type Kadomtsev–Petviashvili equation, Similarity reductions, Symbolic computation},
abstract = {Rather intriguing, the paper Chin. J. Phys. 73 (2021) 600-612 has studied a (3+1)-dimensional B-type Kadomtsev–Petviashvili equation in fluid dynamics, while fluid dynamics has a wide range of applications, including those for geophysics, mechanical engineering, civil engineering, chemical engineering, astrophysics and biology. In this paper, taking into consideration certain nonlinear waves in fluid dynamics, we investigate a generalized variable-coefficient version of the aforementioned equation. Making use of symbolic computation, with respect to the amplitude or elevation of the relevant wave, we construct out two sets of the similarity reductions, which rely on the variable coefficients in the generalized equation.}
}
@article{LOMBARDI2022100601,
title = {Understanding emerging patterns and dynamics through the lenses of the cyber-physical universe},
journal = {Patterns},
volume = {3},
number = {11},
pages = {100601},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100601},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922002264},
author = {Mauro Lombardi and Simone Vannuccini},
keywords = {cyber-physical universe, ubiquitous computing, information technology, artificial intelligence, decision making},
abstract = {Summary
The complex interaction among contemporary techno- and socio-economic processes has set the stage for the emergence of a cyber-physical universe, the novel landscape in which agents behave and interact, and which is centered on the fundamental role played by information and computation at all levels. In this paper, we weave into a single analysis the different threads that lead to (and characterize) the cyber-physical universe and outline a map of its building blocks and the complex dynamics at work in the new environment. The resulting description is used to assess how decision-making processes should evolve in order to be able to address the opportunities and challenges of the current era of deep and extended changes. The analysis offers an encompassing interpretative grid to understand and unpack patterns in the contemporary socio-technical systems that experience a fundamental informational turn; this can inform new research trajectories and help open up new areas for scientific inquiry.}
}
@article{EDELMANN20181,
title = {Formal studies of culture: Issues, challenges, and current trends},
journal = {Poetics},
volume = {68},
pages = {1-9},
year = {2018},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2018.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X18301323},
author = {Achim Edelmann and John W. Mohr},
keywords = {Formal study of culture, Cultural matrix approach, Measuring duality, Formalist theorization of culture, Computational hermeneutics},
abstract = {Over the last two decades, the formal study of culture has grown into one of the most exciting, systematic, and dynamic sub-fields in sociology. In this essay, we take stock of recent developments in this field. We highlight four emerging themes: (1) the maturation of the field that has occurred over the last two decades, (2) the rise and formalization of the “cultural matrix” approach to studying culture, (3) the development of various efforts to advance a more formal theory of culture, and (4) the proliferation of Big Data and the development of new kinds of quantitative and computational approaches to the study of culture, including the emergence of a new area focused on “computational hermeneutics.” We conclude by discussing future opportunities, challenges, and questions in formalizing culture.}
}
@article{FISCHER199721,
title = {Computational environments supporting creativity in the context of lifelong learning and design},
journal = {Knowledge-Based Systems},
volume = {10},
number = {1},
pages = {21-28},
year = {1997},
note = {Information Technology Support for Creativity},
issn = {0950-7051},
doi = {https://doi.org/10.1016/S0950-7051(97)00010-5},
url = {https://www.sciencedirect.com/science/article/pii/S0950705197000105},
author = {Gerhard Fischer and Kumiyo Nakakoji},
keywords = {Creativity support, Domain-oriented design environments (DODEs), Lifelong-learning},
abstract = {Much of our intelligence and creativity results from the collective memory of communities of practice and of the artifacts and technology surrounding them. Rather than studying individual creativity in isolation, we have developed a conceptual framework of creativity in the context of everyday practice — where design activities prevail and learning is constantly required. The conceptual framework explores new role distributions between people and computers based on theories that view design as reflection-in-action and breakdowns as opportunities for learning and creativity. We use an example from the domain of multimedia information design to illustrate how creativity is supported by domain-oriented design environments. The paper describes the mechanisms, architectures and processes underlying these environments.}
}
@incollection{LUND202435,
title = {3 - Choice Awareness strategies},
editor = {Henrik Lund},
booktitle = {Renewable Energy Systems (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
pages = {35-50},
year = {2024},
isbn = {978-0-443-14137-9},
doi = {https://doi.org/10.1016/B978-0-443-14137-9.00003-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443141379000036},
author = {Henrik Lund},
keywords = {Alternatives assessment, Choice Awareness, Feasibility studies, Institutional barriers, Market barriers, Path dependency, Public regulation, Radical technological change, Renewable energy systems, Technical alternatives},
abstract = {This chapter introduces strategies to raise the awareness of how to implement smart energy systems in fully decarbonized societies using the Choice Awareness theory. Choice Awareness is based on the understanding that existing institutional perceptions and organizational interests will often seek to eliminate certain choices from the political decision-making process, when the introduction of radical technological change is discussed. The counterstrategy is to raise public awareness that alternatives do exist and that it is possible to make a choice. Counterstrategies may involve the design of technical alternatives, feasibility studies based on institutional economic thinking, and the design of public regulation measures seen in the light of conflicting interests as well as changes in the democratic decision-making infrastructure. Each of the strategies is elaborated on in this chapter.}
}
@article{EDELSON2021100986,
title = {How fuzzy-trace theory predicts development of risky decision making, with novel extensions to culture and reward sensitivity},
journal = {Developmental Review},
volume = {62},
pages = {100986},
year = {2021},
issn = {0273-2297},
doi = {https://doi.org/10.1016/j.dr.2021.100986},
url = {https://www.sciencedirect.com/science/article/pii/S0273229721000411},
author = {Sarah M. Edelson and Valerie F. Reyna},
keywords = {Risk-taking, Risky decision making, Reward sensitivity, COVID-19, Fuzzy-trace theory, Adolescence},
abstract = {Comprehensive meta-analyses of risky decision making in children, adolescents, and adults have revealed that age trends in disambiguated laboratory tasks confirmed fuzzy-trace theory’s prediction that preference for risk decreases monotonically from childhood to adulthood. These findings are contrary to predictions of dual systems or neurobiological imbalance models. Assumptions about increasing developmental reliance on mental representations of the gist of risky options are essential to account for this developmental trend. However, dual systems theory appropriately emphasizes how cultural context changes behavioral manifestation of risk preferences across age and neurobiological imbalance models appropriately emphasize developmental changes in reward sensitivity. All of the major theories include the assumption of increasing behavioral inhibition. Here, we integrate these theoretical constructs—representation, cultural context, reward sensitivity, and behavioral inhibition—to provide a novel framework for understanding and improving risky decision making in youth. We also discuss the roles of critical tests, scientific falsification, disambiguating assessments of psychological and neurological processes, and the misuse of such concepts as ecological validity and reverse inference. We illustrate these concepts by extending fuzzy-trace theory to explain why youth are a major conduit of viral infections, including the virus that causes COVID-19. We conclude by encouraging behavioral scientists to embrace new ways of thinking about risky decision making that go beyond traditional stereotypes about adolescents and that go beyond conceptualizing ideal decision making as trading off degrees of risk and reward.}
}
@article{GABRIEL2008330,
title = {A friend is a present you give to your “Self”: Avoidance of intimacy moderates the effects of friends on self-liking},
journal = {Journal of Experimental Social Psychology},
volume = {44},
number = {2},
pages = {330-343},
year = {2008},
issn = {0022-1031},
doi = {https://doi.org/10.1016/j.jesp.2007.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0022103107001126},
author = {Shira Gabriel and Mauricio Carvallo and Lisa M. Jaremka and Brooke Tippin},
keywords = {The self, Social comparison, Friendship, Avoidance of intimacy, Attachment style},
abstract = {The current research proposes that thinking about friends improves feelings about the self and does so differentially depending on avoidance of intimacy. Based on previous findings that individuals who avoid intimacy in relationships (avoidant individuals) contrast their self-concepts with primed friends whereas those who pursue intimacy in relationships (non-avoidant individuals) assimilate their self-concepts to primed friends [Gabriel, S., Carvallo, M., Dean, K., Tippin, B. D., & Renaud, J. (2005). How I see “Me” depends on how I see “We”: The role of avoidance of intimacy in social comparison. Personality and Social Psychology Bulletin, 31, 156–157], we predicted that friends who embody negative aspects of self would lead avoidant individuals to like themselves more, whereas friends who embody positive aspects of self would lead non-avoidant individuals to like themselves more. A pretest determined that good friends were seen as more similar to positive and ideal aspects of the self, whereas friends about whom participants had more mixed feelings (ambivalent friends) were seen as more similar to disliked and feared aspects of the self. Four experiments supported the main hypotheses. In Experiment 1, non-avoidant individuals like themselves more when good friends were primed. In Experiment 2, avoidant individuals like themselves more when ambivalent friends were primed. In Experiment 3, non-avoidant individuals liked themselves better after thinking about a friend’s positive traits, whereas avoidant individuals liked themselves better after thinking about a friend’s negative traits. In Experiment 4, all individuals under self-esteem threat strategically brought friends to mind who would help them like themselves more.}
}
@article{CEGIELSKI2016283,
title = {Rethinking the role of Agent-Based Modeling in archaeology},
journal = {Journal of Anthropological Archaeology},
volume = {41},
pages = {283-298},
year = {2016},
issn = {0278-4165},
doi = {https://doi.org/10.1016/j.jaa.2016.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0278416516000118},
author = {Wendy H. Cegielski and J. Daniel Rogers},
keywords = {ABM, Agent-Based Modeling, Archaeological methods, Simulation, Computational modeling},
abstract = {Agent-Based Modeling (ABM) represents a methodology with significant potential for altering archaeological analytical practice. The continued growth in the number of publications that use ABM provides evidence for the significance of this emerging approach. However, the scope of the research topics investigated has not increased accordingly. A consensus exists among ABM practitioners, that once generally accepted by the field, ABM can make revolutionary advances within the overall archaeological research paradigm. Unresolved concerns within the archaeological community center on whether ABMs are sufficiently grounded in empirical data, are aligned with theoretical trajectories, and on the difficult task of mastering the computational systems. It is worth exploring these aspects of the disjuncture between the mainstream and ABM practitioners for two reasons – to frame a discussion of qualities of ABM that make it transformative and to provide guidelines for broadening ABM’s applicability. With capacity-building in mind, offered here is a practical reference for the non-practitioner archaeologist considering ABM. A glossary is included of key terms used in the text to describe ABM methods and theory.}
}
@article{CHASTAIN200083,
title = {Cultivating design competence: online support for beginning design studio},
journal = {Automation in Construction},
volume = {9},
number = {1},
pages = {83-91},
year = {2000},
issn = {0926-5805},
doi = {https://doi.org/10.1016/S0926-5805(99)00053-9},
url = {https://www.sciencedirect.com/science/article/pii/S0926580599000539},
author = {Thomas Chastain and Ame Elliott},
abstract = {A primary lesson of a beginning design studio is the development of a fundamental design competence. This entails acquiring skills of integration, projection, exploration, as well as critical thinking—forming the basis of thinking “like a designer”. Plaguing the beginning architectural design student as she develops this competence are three typical problems: a lagging visual intelligence, a linking of originality with creativity, and the belief that design is an act of an individual author instead of a collaborative activity. We believe that computation support for design learning has particular attributes for helping students overcome these problems. These attributes include its inherent qualities for visualization, for explicitness, and for sharing. This paper describes five interactive multi-media exercises exploiting these attributes which were developed to support a beginning design studio. The paper also reports how they have been integrated into the course curriculum. Le développement des compétences en design: support on-line pour le studio de design élémentaire Une des premières leçons lors du studio de design est le développement d’une compétence fondamentale en conception. Ceci implique l’acquisition des habiletés d’intégration, de projection, d’exploration ainsi que la pensée critique—antérieurement les bases de la façon de penser nommée “comme un concepteur”. Il y a trois problèmes fondamentaux qui pèsent sur l’étudiant débutant en architecture lors du développement cette compétence: une intelligence visuelle insuffisante, le fait de lier l’originalité à la créativité, et la croyance que le processus de conception est une activité individuelle, plutôt que collaborative. Nous sommes de l’avis que le soutien en informatique lors de l’apprentissage de la conception architecturale posséde des attributs bien particuliers pour aider les étudiants à surmonter ces difficultés. Ces attributs comprennent des qualités inhérentes pour la visualisation, pour être explicite, et pour le partage. Ce papier décrit cinq exercices de médias interactifs qui exploitent ces attributs, et qui ont été développés pour supporter un studio de design élémentaire. Il présente aussi un reportage sur la façon dont ces exercices ont été intégrés dans le curriculum du cours.}
}
@incollection{CARPENDALE2013125,
title = {Chapter Six - A Relational Developmental Systems Approach to Moral Development},
editor = {Richard M. Lerner and Janette B. Benson},
series = {Advances in Child Development and Behavior},
publisher = {JAI},
volume = {45},
pages = {125-153},
year = {2013},
booktitle = {Embodiment and Epigenesis: Theoretical and Methodological Issues in Understanding the Role of Biology within the Relational Developmental System},
issn = {0065-2407},
doi = {https://doi.org/10.1016/B978-0-12-397946-9.00006-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780123979469000063},
author = {Jeremy I.M. Carpendale and Stuart I. Hammond and Sherrie Atwood},
keywords = {Developmental systems theory, Moral development, Moral norms, Nativism, Social interaction},
abstract = {Morality and cooperation are central to human life. Psychological explanations for moral development and cooperative behavior will have biological and evolutionary dimensions, but they can differ radically in their approach to biology. In particular, many recent proposals have pursued the view that aspects of morality are innate. We briefly review and critique two of these claims. In contrast to these nativist assumptions about the role of biology in morality, we present an alternative approach based on a relational developmental systems view of moral development. The role for biology in this approach is in setting up the conditions—the developmental system—in which forms of interaction and later forms of thinking emerge.}
}
@article{20213341,
title = {Tim Behrens},
journal = {Neuron},
volume = {109},
number = {21},
pages = {3341-3343},
year = {2021},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2021.09.047},
url = {https://www.sciencedirect.com/science/article/pii/S0896627321007200},
abstract = {Summary
Tim Behrens discusses with Neuron creative ways to facilitate virtual meetings, the multiple ways that the pandemic has affected different people, and his advice for the younger generation of neuroscientists in general and computational scientists in particular.}
}
@article{BOURDAKOU2023102881,
title = {Drug repurposing on Alzheimer's disease through modulation of NRF2 neighborhood},
journal = {Redox Biology},
volume = {67},
pages = {102881},
year = {2023},
issn = {2213-2317},
doi = {https://doi.org/10.1016/j.redox.2023.102881},
url = {https://www.sciencedirect.com/science/article/pii/S2213231723002823},
author = {Marilena M. Bourdakou and Raquel Fernández-Ginés and Antonio Cuadrado and George M. Spyrou},
keywords = {Alzheimer's disease, NRF2, , Association network,  drug repurposing, Differentially expressed genes},
abstract = {Alzheimer's disease (AD) is an age-dependent neurodegenerative disorder and the most common cause of cognitive decline. The alarming epidemiological features of Alzheimer's disease, combined with the high failure rate of candidate drugs tested in the preclinical phase, impose more intense investigations for new curative treatments. NRF2 (Nuclear factor-erythroid factor 2-related factor 2) plays a critical role in the inflammatory response and in the cellular redox homeostasis and provides cytoprotection in several diseases including those in the neurodegeneration spectrum. These roles suggest that NRF2 and its directly associated proteins may be novel attractive therapeutic targets in the fight against AD. In this study, through a systemics perspective, we propose an in silico drug repurposing approach for AD, based on the NRF2 interactome and regulome, with the aim of highlighting possible repurposed drugs for AD. Using publicly available information based on differential expressions of the NRF2-neighborhood in AD and through a computational drug repurposing pipeline, we derived to a short list of candidate repurposed drugs and small molecules that affect the expression levels of the majority of NRF2-partners. The relevance of these findings was assessed in a four-step computational meta-analysis including i) structural similarity comparisons with currently ongoing NRF2-related drugs in clinical trials ii) evaluation based on the NRF2-diseasome iii) comparison of relevance between targeted pathways of shortlisted drugs and NRF2-related drugs in clinical trials and iv) further comparison with existing knowledge on AD and NRF2-related drugs in clinical trials based on their known modes of action. Overall, our analysis yielded in 5 candidate repurposed drugs for AD. In cell culture, these 5 candidates activated a luciferase reporter for NRF2 activity and in hippocampus derived TH22 cells they increased NRF2 protein levels and the NRF2 transcriptional signatures as determined by increased expression of its downstream target heme oxygenase 1. We expect that our proposed candidate repurposed drugs will be useful for further research and clinical translation for AD.}
}
@article{BIDERMAN2020542,
title = {What Are Memories For? The Hippocampus Bridges Past Experience with Future Decisions},
journal = {Trends in Cognitive Sciences},
volume = {24},
number = {7},
pages = {542-556},
year = {2020},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2020.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S1364661320301066},
author = {Natalie Biderman and Akram Bakkour and Daphna Shohamy},
keywords = {memory, decision-making, amnesia, hippocampus, value},
abstract = {Many decisions require flexible reasoning that depends on inference, generalization, and deliberation. Here, we review emerging findings indicating that the hippocampus, known for its role in long-term memory, contributes to these flexible aspects of value-based decision-making. This work offers new insights into the role of memory in decision-making and suggests that memory may shape decisions even in situations that do not appear, at first glance, to depend on memory at all. Uncovering the pervasive role of memory in decision-making challenges the way we define what memory is and what it does, suggesting that memory’s primary purpose may be to guide future behavior and that storing a record of the past is just one way to do so.}
}
@article{LIU2022121968,
title = {Comparison of coal-to-ethanol product separation strategies},
journal = {Separation and Purification Technology},
volume = {301},
pages = {121968},
year = {2022},
issn = {1383-5866},
doi = {https://doi.org/10.1016/j.seppur.2022.121968},
url = {https://www.sciencedirect.com/science/article/pii/S1383586622015234},
author = {Daoyan Liu and Hao Lyu and Jiahao Wang and Chengtian Cui and Jinsheng Sun},
keywords = {Coal-to-ethanol, Separation strategy, Differential evolution algorithm, Parallel computation, Heat integration},
abstract = {Given China's energy structure and the limitations of bioethanol, the coal-to-ethanol (CTE) pathway, from dimethyl ether to ethanol (DMTE) via carbonylation and hydrogenation, is highly anticipated. Ethanol, methanol, methyl acetate, and ethyl acetate are the crude hydrogenation products that need to be purified, requiring at least an eight-column scheme. However, the optimization of the existing separation strategy with ethanol as the priority is unfavorable in the following aspects: it is usually plagued by tedious rules of thumb and, due to the large scale of the process, is prone to falling into local minima; pre-designed heat integration inevitably neglects the interaction of parameter optimization and heat integration; reports on alternative feasible distillation sequences are scarce in publications, let alone comparisons amongst these counterparts. Therefore, four viable separation strategies are proposed in this paper to compare with this faulted separation strategy. A self-adapting dynamic differential evolution (SADDE) algorithm, which is accelerated by parallel computation, is used to search for optimal column parameters of all the configuration options and facilitates simultaneous heat integration structure synthesis. Two strategies stand out after 3000 generations of evolution. Splitting methanol outperforms in specific steam consumption (SSC) of ethanol (1.8177), much better than the benchmark (2.4840), and splitting ethyl acetate with ethyl acetate priority has the most competitive total annual cost (TAC), 23.98% lower than the benchmark. In summary, this paper provides a reference for optimizing complex distillation systems like CTE product separation, or more specifically, the DMTE route, before the appearance of the most suitable separation strategy in demand. Furthermore, it will also serve for the CTE superstructure to further explore the optimal distillation sequence.}
}
@article{BANERJEE2015143,
title = {Z*-numbers: Augmented Z-numbers for machine-subjectivity representation},
journal = {Information Sciences},
volume = {323},
pages = {143-178},
year = {2015},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2015.06.026},
url = {https://www.sciencedirect.com/science/article/pii/S0020025515004582},
author = {Romi Banerjee and Sankar K. Pal},
keywords = {Artificial-mindfulness, Machine-consciousness, Machine-qualia, Machine-self, Perception-operators, Thinking machine},
abstract = {Envisaging a futuristic environment of man–machine and machine–machine synergy, this article documents our research on the augmented Z-numbers, the Z*-numbers, for machine-perception encapsulation. The Z*-numbers have been envisioned as operands of endogenous machine-mind processes underlying bespoke comprehension of the real world. Besides information-certainty, as in a Z-number, a Z*-number incorporates context, time and affects as essential factors of subjectivity representation. We have proposed: (a) definitions for certainty and affect parameters—arising out of socio-cultural influences on machine-knowledge, (b) a Z*-number based rudimentary procedure for natural-language comprehension emulation, and (c) primitive perception-operators for ‘machine-mentalese’ simulation using Z*-number information-equivalents. Our work draws from non-symbolic theories of cognition and ‘mindfulness’, human-mind processes—studied through behavioral experiments, and theories of the ‘self’ and ‘qualia’. The article includes detailed discussions of these experiments and consequent insights, analysis of a theoretical run-through of the defined procedure, and correspondence-studies between the Z*-number paradigm and philosophies of the self. Our research raises questions on cognitive biases and autogenous mind-processes that highlight crucial practical challenges in the current realization of a synthetic-mind. All ideas herein aim to contribute to studies on the ‘self’ and its machine-embodiment for the synthesis of an empathetic machine-mind.}
}
@article{GREENE201766,
title = {The rat-a-gorical imperative: Moral intuition and the limits of affective learning},
journal = {Cognition},
volume = {167},
pages = {66-77},
year = {2017},
note = {Moral Learning},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2017.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0010027717300690},
author = {Joshua D. Greene},
keywords = {Deontology, Utilitarianism, Consequentialism, Reinforcement learning, Model-free learning, Machine learning, Ethics, Normative ethics, Moral judgment},
abstract = {Decades of psychological research have demonstrated that intuitive judgments are often unreliable, thanks to their inflexible reliance on limited information (Kahneman, 2003, 2011). Research on the computational underpinnings of learning, however, indicates that intuitions may be acquired by sophisticated learning mechanisms that are highly sensitive and integrative. With this in mind, Railton (2014) urges a more optimistic view of moral intuition. Is such optimism warranted? Elsewhere (Greene, 2013) I’ve argued that moral intuitions offer reasonably good advice concerning the give-and-take of everyday social life, addressing the basic problem of cooperation within a “tribe” (“Me vs. Us”), but that moral intuitions offer unreliable advice concerning disagreements between tribes with competing interests and values (“Us vs. Them”). Here I argue that a computational perspective on moral learning underscores these conclusions. The acquisition of good moral intuitions requires both good (representative) data and good (value-aligned) training. In the case of inter-tribal disagreement (public moral controversy), the problem of bad training looms large, as training processes may simply reinforce tribal differences. With respect to moral philosophy and the paradoxical problems it addresses, the problem of bad data looms large, as theorists seek principles that minimize counter-intuitive implications, not only in typical real-world cases, but in unusual, often hypothetical, cases such as some trolley dilemmas. In such cases the prevailing real-world relationships between actions and consequences are severed or reversed, yielding intuitions that give the right answers to the wrong questions. Such intuitions—which we may experience as the voice of duty or virtue—may simply reflect the computational limitations inherent in affective learning. I conclude, in optimistic agreement with Railton, that progress in moral philosophy depends on our having a better understanding of the mechanisms behind our moral intuitions.}
}
@article{RODRIGUEZMENDEZ2024103804,
title = {UK net-zero policy design – from optimisation to robustness},
journal = {Environmental Science & Policy},
volume = {158},
pages = {103804},
year = {2024},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2024.103804},
url = {https://www.sciencedirect.com/science/article/pii/S1462901124001382},
author = {Quirina {Rodriguez Mendez} and Mark Workman and Geoff Darch},
keywords = {Robust Decision Making, Deep Uncertainty, Greenhouse Gas Removal, Climate modelling, United Kingdom Net-Zero target},
abstract = {The need to deal with the deep uncertainty and system complexity associated to Net-Zero pathways, especially those relying on emergent greenhouse gas removal (GGR) technologies, has resulted in a growing body of literature on alternative decision-support approaches. Exploratory modelling, and specifically Robust Decision Making (RDM), are potential approaches capable of addressing these challenges: by exploring a wide range of conceivable futures, they explicitly embrace deep uncertainties while seeking to reduce system vulnerabilities. However, though RDM methods have been well documented, there is little insight as to how such approach might be integrated into Net-Zero policy design processes. By means of a workshop (n=17) and interviews (n=13) with the UK climate policy and energy modelling communities, this contribution provides insights into the role and potential of RDM in explicitly dealing with the deep uncertainties that pervade in the establishment of a 60–100 MtCO2 UK GGR sector within three decades. The consultation process revealed that there is an appetite from the decision-making and analytical communities in integrating exploratory modelling concepts into UK policy design processes. It is recommended that to bridge the gap between theoretical RDM constructs and their broader adoption, the analytical process should include a broader set of disciplines and expertise. Specifically for the modelling community, this work suggests that in-use computational models should be adapted, rather than new tools developed. Key challenges also arise from the time and resources required, suggesting small scale place-based pilots could promote the acceptability and foster the adoption of the RDM methodology.}
}
@article{GOBERT201581,
title = {Using educational data mining to assess students’ skills at designing and conducting experiments within a complex systems microworld},
journal = {Thinking Skills and Creativity},
volume = {18},
pages = {81-90},
year = {2015},
note = {21st Century Skills: International Advancements and Recent Developments},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2015.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S1871187115300067},
author = {Janice D. Gobert and Yoon Jeon Kim and Michael A. {Sao Pedro} and Michael Kennedy and Cameron G. Betts},
keywords = {Complex systems, Inquiry assessment, Performance assessment, Educational data mining, 21st century skills},
abstract = {Many national policy documents underscore the importance of 21st century skills, including critical thinking. In parallel, recent American frameworks for K-12 science education call for the development of critical thinking skills in science, also referred to as science inquiry skills/practices. Assessment of these skills is necessary, as indicated in policy documents; however, this has posed a great challenge for assessment researchers. Recently, some science learning environments seek to assess these science skills. These systems log all students’ interactions within the given system, and if fully leveraged, these logs provide rich assessments of inquiry skills. Here, we describe our environment Inq-ITS (inquiry intelligent tutoring system), that uses educational data mining to assess science inquiry skills, as described as 21st century skills. Additionally, here, we describe how we measure students’ skills at designing controlled experiments, a lynchpin skill of inquiry, in the context of complex systems. In doing so, our work addresses 21st century skill assessment in two ways, namely of inquiry (designing and conducting experiments), and in the context of complex systems, a key topic area of 21st century skills. We use educational data mining to develop our assessment of this skill for complex systems.}
}
@article{MIHAI20221082,
title = {Multimodal emotion detection from multiple data streams for improved decision making},
journal = {Procedia Computer Science},
volume = {214},
pages = {1082-1089},
year = {2022},
note = {9th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.281},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922019937},
author = {Neghina Mihai and Matei Alexandru and Zamfirescu Bala-Constantin},
keywords = {emotion detection, sensor fusion, multimodal, affect},
abstract = {Recent neurological studies shows that emotions are tightly connected to the thinking and cognitive actions, being part of the decision-making process. Considering this, having a way to help decision making processes based on current emotion of the user or to consider the potential emotional impact if a decision is made, would be beneficial. This paper introduces a novel method for fusing multiple emotional signals, using a weighted average, where each weight value adapts to real time conditions, based on signal type, presence, and quality. In the context of a training station for manual operation, we implemented and tested separately several emotion detection methods, each based on a different signal acquired from audio, video, and galvanic skin response data streams. The final goal is to include the proposed method together with state of the art emotion detection machine learning algorithms as part of the digital twin training station for manual operation.}
}
@article{OREILLY2016547,
title = {Creative Engineers: Is Abductive Reasoning Encouraged enough in Degree Project Work?},
journal = {Procedia CIRP},
volume = {50},
pages = {547-552},
year = {2016},
note = {26th CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2016.04.155},
url = {https://www.sciencedirect.com/science/article/pii/S221282711630395X},
author = {Ciarán J. O’Reilly},
keywords = {Creative design, abductive reasoning, education, degree project},
abstract = {Creativity is considered to be an important ability for an engineer to have, and it is therefore important that the development of this ability is structured into the education of engineering students, along with the ability to apply, analyse and evaluate based on existent knowledge. In this paper, the importance of abduction in creative engineering processes is briefly reviewed. It has been shown that abductive reasoning plays a key role in design as it is the only logical operation that introduces new ideas. Its encouragement within the KTH Royal Institute of Technology's degree projects at the Department of Aeronautical and Vehicle Engineering is analysed by examining the stated intended learning outcomes, and through interviewing students. It is found that abductive reasoning is not explicitly encouraged within the intended learning outcomes of these degree project courses, despite its importance in creative thinking. Although, it is very likely that at least some abduction takes place in the project work, its absence from the intended learning outcomes means that students may not have a felt need to demonstrate their abductive reasoning, and supervisors may encourage only non-creative deductive or inductive reasoning. A more explicit inclusion of abductive reasoning in the intended learning outcomes would help both students and supervisors to include creative thinking in the degree project courses.}
}
@article{MEINTZ1994273,
title = {Future directions in computational nursing sciences},
journal = {Mathematical and Computer Modelling},
volume = {19},
number = {6},
pages = {273-288},
year = {1994},
issn = {0895-7177},
doi = {https://doi.org/10.1016/0895-7177(94)90199-6},
url = {https://www.sciencedirect.com/science/article/pii/0895717794901996},
author = {S.L. Meintz and E.A. Yfantis and W.P. Graebel},
keywords = {Computational nursing, Health care data sets, Interdisciplinary, Nurmetrics, Nursing informatics, Nursing science, Supercomputers},
abstract = {The advent of the supercomputer and its capabilities for dealing with terabyte-sized data bases has provided a unique opportunity for nursing sciences to enhance and add to its theories. An interdisciplinary team has formed at the University of Nevada, Las Vegas (UNLV), to provide new tools and methodologies for analyzing large-scale data bases. Their first project is a study of infant mortality. The strategy and goals for this project are presented, along with an assessment of the present state of health care data base analysis.}
}
@article{SHAFFER199795,
title = {Learning mathematics through design: The anatomy of Escher's world},
journal = {The Journal of Mathematical Behavior},
volume = {16},
number = {2},
pages = {95-112},
year = {1997},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(97)90019-5},
url = {https://www.sciencedirect.com/science/article/pii/S0732312397900195},
author = {David Williamson Shaffer},
abstract = {This article explores one example of an open learning environment created by combining mathematics and design activities in a “mathematics studio”. Two iterations of the mathematics studio experiment in a project at the MIT Media Laboratory known as Escher's World suggest that: (a) students can learn about the mathematical concept of symmetry in a studio learning environment, (b) students learn to use visual thinking to solve mathematical problems in a studio learning environment, and (c) students develop a more positive attitude towards mathematics as a result of working in a studio learning environment. This article uses a qualitative research model to explore the specific characteristics of the mathematics studio that were influential in creating a successful learning environment—in particular, how expressive mathematics activities and expressive computational media give students a sense of control over their learning.}
}
@article{WANG20071997,
title = {DIANA: A computer-supported heterogeneous grouping system for teachers to conduct successful small learning groups},
journal = {Computers in Human Behavior},
volume = {23},
number = {4},
pages = {1997-2010},
year = {2007},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2006.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0747563206000094},
author = {Dai-Yi Wang and Sunny S.J. Lin and Chuen-Tsai Sun},
keywords = {Cooperative learning, Small-group learning, Computer assisted grouping system, Group composition, Thinking styles, University students},
abstract = {Teachers interested in small-group learning can benefit from using psychological factors to create heterogeneous groups. In this paper we describe a computer-supported grouping system named DIANA that uses genetic algorithms to achieve fairness, equity, flexibility, and easy implementation. Grouping was performed so as to avoid the creation of exceptionally weak groups. We tested DIANA with 66 undergraduate computer science students assigned to groups of three either randomly (10 groups) or using an algorithm reflecting [Sternberg, R. J. (1994). Thinking styles: theory and assessment at the interface between intelligence and personality. In R. J. Sterberg, & P. Ruzgis (Eds.), Personality and Intelligence (pp. 169–187). New York: Cambridge University Press.] three thinking styles (12 groups). The results indicate that: (a) the algorithm-determined groups were more capable of completing whatever they were “required to do” at a statistically significant level, (b) both groups were equally capable of solving approximately 80% of what they “chose to do,” and (c) the algorithm-determined groups had smaller inter-group variation in performance. Levels of satisfaction with fellow group member attitudes, the cooperative process, and group outcomes were also higher among members of the algorithm-determined groups. Suggestions for applying computer-supported group composition systems are offered.}
}
@article{KUNZE2024249,
title = {Bioinspired approaches for resource-efficient material flow in production – an innovative actuator concept for peristaltic-based transport},
journal = {Procedia CIRP},
volume = {125},
pages = {249-254},
year = {2024},
note = {CIRP BioM 2024},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.08.043},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124003974},
author = {Henriette Kunze and Marcel Lorenz},
keywords = {tensegrity, biotensegrity, assembly technologies},
abstract = {In automated material flow, in a wide variety of areas, the primary goal is usually to handle a wide spectrum of components as time- and cost-efficiently as possible. In view of the current and future challenges in industrial production, it is becoming apparent that ecological requirements are becoming increasingly important in automation solutions. For example, in form of resource efficiency, transformability and material efficiency. In this context, especially materials handling technology is subject of various optimization approaches, as no value is added to the part handled. The question: "How does material flow occur in nature?" thus offers biologically inspired approaches to thinking about transport in the industrial sector. This paper first presents a selection of concepts or existing mechanisms that are adaptable in materials- handling technology and have been developed based on a biological model. In the second part of this paper, a new concept is presented that is modeled on peristalsis as a transport mechanism. The approach presented here uses tensegrity-structures for assembly, which are characterized by their high material efficiency and flexibility. The transport movement is achieved by peristaltic typical contraction or relaxation of the respective structure parts.}
}
@article{ROMAN1992161,
title = {Pavane: a system for declarative visualization of concurrent computations},
journal = {Journal of Visual Languages & Computing},
volume = {3},
number = {2},
pages = {161-193},
year = {1992},
issn = {1045-926X},
doi = {https://doi.org/10.1016/1045-926X(92)90014-D},
url = {https://www.sciencedirect.com/science/article/pii/1045926X9290014D},
author = {Gruia-Catalin Roman and Kenneth C Cox and C.Donald Wilcox and Jerome Y Plun},
abstract = {This paper describes the conceptual model, specification method and visualization methodology for Pavane—a visualization environment concerned with exploring, monitoring and presenting concurrent computations. The underlying visualization model is declarative in the sense that visualization is treated as a mapping from program states to a three-dimensional world of geometric objects. The latter is rendered in full color and may be examined freely by a viewer who is allowed to navigate through the geometric world. The state-to-geometry mapping is defined as a composition of several simpler mappings. The choice is determined by methodological and architectural considerations. This paper shows how this decomposition was molded by two methodological objectives: (1) the desire visually to capture abstract formal properties of programs (e.g. safety and progress) rather than operational details; and (2) the need to support complex animations of atomic computational events. All mappings are specified using a rule-based notation; rules may be added, deleted and modified at any time during the visualization. An algorithm for termination detection in diffusing computations is used to illustrate the specification method and to demonstrate its conceptual elegance and flexibility. A concurrent version of a popular artificial intelligence program provides a vehicle for demonstrating how we derive graphical representations and animation scenarios from key formal properties of the program, i.e. from those safety and progress assertions about the program which turn out to be important in verifying its correctness.}
}
@incollection{ZHUGE2016149,
title = {15 - Limitations and challenges},
editor = {Hai Zhuge},
booktitle = {Multi-Dimensional Summarization in Cyber-Physical Society},
publisher = {Morgan Kaufmann},
pages = {149-151},
year = {2016},
series = {Computer Science Reviews and Trends},
isbn = {978-0-12-803455-2},
doi = {https://doi.org/10.1016/B978-0-12-803455-2.00015-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128034552000159},
author = {Hai Zhuge},
keywords = {Summarization, limitations, challenges, representations, computing},
abstract = {The limitation of summarisation lies in the natural differences between human and machine, between languages, and between the ways of observation and thinking of authors and those of readers. The significant evolution of documents in form and function in cyber-physical society challenges the paradigm of summarization research.}
}
@article{BAMBINI2022106196,
title = {It is time to address language disorders in schizophrenia: A RCT on the efficacy of a novel training targeting the pragmatics of communication (PragmaCom)},
journal = {Journal of Communication Disorders},
volume = {97},
pages = {106196},
year = {2022},
issn = {0021-9924},
doi = {https://doi.org/10.1016/j.jcomdis.2022.106196},
url = {https://www.sciencedirect.com/science/article/pii/S0021992422000156},
author = {Valentina Bambini and Giulia Agostoni and Mariachiara Buonocore and Elisabetta Tonini and Margherita Bechi and Ilaria Ferri and Jacopo Sapienza and Francesca Martini and Federica Cuoco and Federica Cocchi and Luca Bischetti and Roberto Cavallaro and Marta Bosia},
keywords = {Pragmatics, Schizophrenia, Rehabilitation, Concretism, Metaphor, Functioning},
abstract = {Introduction: Language and communication disruptions in schizophrenia are at the center of a large body of investigation. Yet, the remediation of such disruptions is still in its infancy. Here we targeted what is known to be one of the most damaged language domains in schizophrenia, namely pragmatics, by conducting a pragmatics-centered intervention with a randomized controlled trial design and assessing also durability and generalization. To the best of our knowledge, this is the first study with these characteristics. Methods: Inspired by the Gricean account of natural language use, we tailored a novel treatment addressing the pragmatics of communication (PragmaCom) and we tested its efficacy in a sample of individuals with schizophrenia randomized to the experimental group or to an active control group. The primary outcome with respect to the efficacy of the PragmaCom was measured by changes in pragmatic abilities (as evaluated with the global score of the Assessment of Pragmatic Abilities and Cognitive Substrates test) from baseline to 12 weeks and at 3-month follow-up. The secondary outcome was measured by changes in metaphor comprehension, abstract thinking, and global functioning from baseline to 12 weeks and at 3-month follow-up. Results: Relative to the control group, at post-test the PragmaCom group showed greater and enduring improvement in global pragmatic skills and in metaphor comprehension. At follow-up, these improvements persisted and the PragmaCom exerted beneficial effects also on functioning. Conclusions: Despite the limited sample size, we believe that these findings offer initial yet encouraging evidence of the possibility to improve pragmatic skills with a theoretically grounded approach and to obtain durable and clinically relevant benefits. We argue that it is time that therapeutic efforts embrace communicative dysfunctions in order to improve illness outcome.}
}
@article{GARG2024101391,
title = {Molecular Mechanics Demonstrate S-COMT as promising therapeutic receptor when analyzed with secondary plant metabolites},
journal = {Journal of the Indian Chemical Society},
volume = {101},
number = {11},
pages = {101391},
year = {2024},
issn = {0019-4522},
doi = {https://doi.org/10.1016/j.jics.2024.101391},
url = {https://www.sciencedirect.com/science/article/pii/S0019452224002711},
author = {Deepanshu Garg and Aarya Vashishth and Maharsh Jayadeep Jayawant and Virupaksha A. Bastikar},
keywords = {S-COMT receptor, Depression, Plant secondary metabolites, Molecular docking, Molecular dynamic simulation},
abstract = {Major depressive disorder (MDD) and other psychiatric conditions are debilitating illnesses affecting millions globally. Catechol-O-methyltransferase (COMT), an enzyme that regulates dopamine and norepinephrine breakdown in the brain, has emerged as a potential therapeutic target for these disorders. This study explores the inhibitory potential of plant secondary metabolites against S-COMT using computational techniques. COMT exists in two isoforms: membrane-bound COMT (MB-COMT), primarily found in brain neurons, and soluble COMT (S-COMT), present in peripheral tissues. S-COMT, particularly in the prefrontal cortex, is crucial for regulating neurotransmitters and maintaining cognitive function. Studies suggest S-COMT variants might be linked to the development of depression, schizophrenia, and other psychiatric disorders. Current COMT inhibitors often suffer from limitations, necessitating the exploration of novel therapeutic strategies. This study employed in-silico methods to investigate plant secondary metabolites as potential S-COMT inhibitors. Here, we describe the S-COMT protein structure retrieval and validation, followed by molecular docking simulations to identify plant compounds with the strongest binding affinity to the receptor's active site. Key amino acid residues involved in these interactions were also analyzed. Furthermore, molecular dynamics simulations were conducted to assess the stability of the top-scoring protein-ligand complexes over a 100-ns timeframe. The results explored the stability of ligand binding within the active site and its impact on the overall conformation of the S-COMT receptor. Our findings highlight promising therapeutic potential for these plant-derived compounds. Further in vitro and in vivo studies are warranted to validate their efficacy and safety for potential clinical applications in treating S-COMT-related disorders.
Subjects
Bioinformatics and Computational Biology, Proteomics, Neurogenerative Diseases.}
}
@article{YAO2022107747,
title = {Regional attention reinforcement learning for rapid object detection},
journal = {Computers & Electrical Engineering},
volume = {98},
pages = {107747},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.107747},
url = {https://www.sciencedirect.com/science/article/pii/S004579062200057X},
author = {Hongge Yao and Peng Dong and Siyi Cheng and Jun Yu},
keywords = {Regional attention, Reinforcement learning, Object detection, Information fusion, Location and recognition},
abstract = {When people observe a picture, they first pay attention to local areas of the picture, rather than the whole areas, then combine them with previous experience in the brain, and finally make judgments through thinking. This is human visual logic. In this paper, we propose a regional attention reinforcement learning model for object detection. The proposed model uses human visual logical to solve the detection problem of small and complex targets in the picture. The model uses a recurrent network structure as the main framework to extract historical information, and fuse the historical information with the current concerned information. At each recurrent time step, it can pay attention to the fused information, especially pay more attention to the information that may have objects. Experimental results show that the proposed method has more than 5% improved in recognition accuracy to the conventional methods. In terms of FLOPs, the conventional methods normally require 170 M, while the proposed method only needs 25.4M This means that the proposed method has higher detection efficiency.}
}
@article{DELI2021784,
title = {The thermodynamics of cognition: A mathematical treatment},
journal = {Computational and Structural Biotechnology Journal},
volume = {19},
pages = {784-793},
year = {2021},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2021.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S200103702100012X},
author = {Eva Deli and James Peters and Zoltán Kisvárday},
keywords = {Consciousness, Free will, Mental energy, Intellect, Emotional regulation, Fermionic mind hypothesis, Carnot cycle, Landauer's principle},
abstract = {There is a general expectation that the laws of classical physics must apply to biology, particularly the neural system. The evoked cycle represents the brain's energy/information exchange with the physical environment through stimulus. Therefore, the thermodynamics of emotions might elucidate the neurological origin of intellectual evolution, and explain the psychological and health consequences of positive and negative emotional states based on their energy profiles. We utilized the Carnot cycle and Landauer's principle to analyze the energetic consequences of the brain's resting and evoked states during and after various cognitive states. Namely, positive emotional states can be represented by the reversed Carnot cycle, whereas negative emotional reactions trigger the Carnot cycle. The two conditions have contrasting energetic and entropic aftereffects with consequences for mental energy. The mathematics of the Carnot and reversed Carnot cycles, which can explain recent findings in human psychology, might be constructive in the scientific endeavor in turning psychology into hard science.}
}
@article{SCHREIBER20142544,
title = {A few bad ideas on the way to the triumph of parallel computing},
journal = {Journal of Parallel and Distributed Computing},
volume = {74},
number = {7},
pages = {2544-2547},
year = {2014},
note = {Special Issue on Perspectives on Parallel and Distributed Processing},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2013.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0743731513002177},
author = {Robert Schreiber},
keywords = {Parallelism, Amdahl, Automatic parallelization, Accelerators, Exascale},
abstract = {Parallelism has become mainstream, in the multicore chip, the GPU, and the internet datacenter running MapReduce. In my field, large-scale scientific computing, parallelism now reigns triumphant. It was no simple, direct route that led to this triumph. Along the way, we were confused by ideas that, in retrospect, turned out to be distractions and errors. The thinking behind them was reasonable, but wrong. One can learn from a dissection of mistakes, so I will retell part of the story here.}
}
@article{NISHANT2020102104,
title = {Artificial intelligence for sustainability: Challenges, opportunities, and a research agenda},
journal = {International Journal of Information Management},
volume = {53},
pages = {102104},
year = {2020},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2020.102104},
url = {https://www.sciencedirect.com/science/article/pii/S0268401220300967},
author = {Rohit Nishant and Mike Kennedy and Jacqueline Corbett},
keywords = {Agenda for practice, AI, Artificial intelligence, Climate change, Environmental governance, Natural environment, Research agenda, Sustainability},
abstract = {Artificial intelligence (AI) will transform business practices and industries and has the potential to address major societal problems, including sustainability. Degradation of the natural environment and the climate crisis are exceedingly complex phenomena requiring the most advanced and innovative solutions. Aiming to spur groundbreaking research and practical solutions of AI for environmental sustainability, we argue that AI can support the derivation of culturally appropriate organizational processes and individual practices to reduce the natural resource and energy intensity of human activities. The true value of AI will not be in how it enables society to reduce its energy, water, and land use intensities, but rather, at a higher level, how it facilitates and fosters environmental governance. A comprehensive review of the literature indicates that research regarding AI for sustainability is challenged by (1) overreliance on historical data in machine learning models, (2) uncertain human behavioral responses to AI-based interventions, (3) increased cybersecurity risks, (4) adverse impacts of AI applications, and (5) difficulties in measuring effects of intervention strategies. The review indicates that future studies of AI for sustainability should incorporate (1) multilevel views, (2) systems dynamics approaches, (3) design thinking, (4) psychological and sociological considerations, and (5) economic value considerations to show how AI can deliver immediate solutions without introducing long-term threats to environmental sustainability.}
}
@article{BEATY2017189,
title = {Creative constraints: Brain activity and network dynamics underlying semantic interference during idea production},
journal = {NeuroImage},
volume = {148},
pages = {189-196},
year = {2017},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2017.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S1053811917300125},
author = {Roger E. Beaty and Alexander P. Christensen and Mathias Benedek and Paul J. Silvia and Daniel L. Schacter},
keywords = {Creativity, Divergent thinking, Cognitive control, Functional connectivity, Default network, Executive control network},
abstract = {Functional neuroimaging research has recently revealed brain network interactions during performance on creative thinking tasks—particularly among regions of the default and executive control networks—but the cognitive mechanisms related to these interactions remain poorly understood. Here we test the hypothesis that the executive control network can interact with the default network to inhibit salient conceptual knowledge (i.e., pre-potent responses) elicited from memory during creative idea production. Participants studied common noun-verb pairs and were given a cued-recall test with corrective feedback to strengthen the paired association in memory. They then completed a verb generation task that presented either a previously studied noun (high-constraint) or an unstudied noun (low-constraint), and were asked to “think creatively” while searching for a novel verb to relate to the presented noun. Latent Semantic Analysis of verbal responses showed decreased semantic distance values in the high-constraint (i.e., interference) condition, which corresponded to increased neural activity within regions of the default (posterior cingulate cortex and bilateral angular gyri), salience (right anterior insula), and executive control (left dorsolateral prefrontal cortex) networks. Independent component analysis of intrinsic functional connectivity networks extended this finding by revealing differential interactions among these large-scale networks across the task conditions. The results suggest that interactions between the default and executive control networks underlie response inhibition during constrained idea production, providing insight into specific neurocognitive mechanisms supporting creative cognition.}
}
@article{NADOLSKI2019210,
title = {Complex systems analysis of hybrid warfare},
journal = {Procedia Computer Science},
volume = {153},
pages = {210-217},
year = {2019},
note = {17th Annual Conference on Systems Engineering Research (CSER)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.05.072},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919307318},
author = {Molly Nadolski and James Fairbanks},
keywords = {Multi-level modelling, Sociotechnical systems, Complex Systems, Toolsets, Unstructured Spaces, Conceptual Modeling, Quantitative Modeling},
abstract = {Being empowered with the appropriate toolset will enable decision-makers to analyze how best to intervene in ever-changing complex systems. This research project explored deconstructing qualitative methods to identify and document requirements to connect the models to computational social science approaches. Previous efforts from our research provided a customizable toolset that assesses the current and future impact that decisions, policies, or strategies can deliver in a system to tackle particularly complex problems. This paper presents a portion of the research effort that developed a threat analysis framework by establishing formally documented research methods to effectively combine conceptual and computational tools. This enables more accurate, efficient, and foresightful knowledge capture and depictions of a particular problem space. The case that the tools and approach are tested against is Russia’s application of grey zone warfare tools in Moldova.}
}
@article{JACKSON2012370,
title = {Information technology use and creativity: Findings from the Children and Technology Project},
journal = {Computers in Human Behavior},
volume = {28},
number = {2},
pages = {370-376},
year = {2012},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2011.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0747563211002147},
author = {Linda A. Jackson and Edward A. Witt and Alexander Ivan Games and Hiram E. Fitzgerald and Alexander {von Eye} and Yong Zhao},
keywords = {Videogames, Creativity, Children, Technology use},
abstract = {This research examined relationships between children’s information technology (IT) use and their creativity. Four types of information technology were considered: computer use, Internet use, videogame playing and cell phone use. A multidimensional measure of creativity was developed based on Sternberg and Lubart, 1999, Subrahmanyam et al., 2006 test of creative thinking. Participants were 491 12-year olds; 53% were female, 34% were African American and 66% were Caucasian American. Results indicated that videogame playing predicted of all measures of creativity. Regardless of gender or race, greater videogame playing was associated with greater creativity. Type of videogame (e.g., violent, interpersonal) was unrelated to videogame effects on creativity. Gender but not race differences were obtained in the amount and type of videogame playing, but not in creativity. Implications of the findings for future research to test the causal relationship between videogame playing and creativity and to identify mediator and moderator variables are discussed.}
}
@article{MONRO199293,
title = {Parallel computation of ECG fields},
journal = {Journal of Electrocardiology},
volume = {25},
pages = {93-100},
year = {1992},
note = {Research and Applications in Computerized Electrocardiology},
issn = {0022-0736},
doi = {https://doi.org/10.1016/0022-0736(92)90068-B},
url = {https://www.sciencedirect.com/science/article/pii/002207369290068B},
author = {D.M. Monro and D.M. Budgett},
keywords = {electrocardiogram, modeling, forward solution, magnetic resonance imaging, inverse solution, parallel computers},
abstract = {A parallel implementation of a finite difference model for computing the electric field of cardiac sources is presented. On a relatively inexpensive SIMD parallel computer, a full-forward solution is obtained in minutes, using accurate thoracic detail including anisotropy if required. Because the computation is based on a volume grid with constant size voxels, it readily accepts anatomical data from classified magnetic resonance imaging scans. By using a variation of the colored successive over-relaxation iteration, our finite difference model takes full advantage of the performance of massively parallel computers. Evaluations of the accuracy and performance of the model show the practicality of using specific anatomical models to recover the electrocardiographic field distributions for individual subjects. A relatively modest parallel machine is capable of assembling and computing a specific direct inverse solution from body surface potentials within an hour of measurement, assuming the magnetic resonance imaging classification has been previously completed.}
}
@article{NIKIFORIDOU2010795,
title = {Statistical literacy at university level: the current trends},
journal = {Procedia - Social and Behavioral Sciences},
volume = {9},
pages = {795-799},
year = {2010},
note = {World Conference on Learning, Teaching and Administration Papers},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2010.12.236},
url = {https://www.sciencedirect.com/science/article/pii/S1877042810023414},
author = {Zoi Nikiforidou and Aspasia Lekka and Jenny Pange},
keywords = {statistical literacy, Statistics Education},
abstract = {Active and critical citizens, in contemporary information-driven societies, are considered to possess capacities and skills of statistical literacy. There are numerous definitions and descriptions concerning statistical literacy, statistical reasoning and statistical thinking. Thus, all these terms converge to the principle that statistical citizenship develops from school settings and relates mainly to the processes of evaluating, interpreting and communicating data. If these are not acquired on time, then students build up errors and misunderstandings. In the current paper general issues concerning Statistics Education at the University level are addressed and aspects for future research are stressed in terms of technology use, content and pedagogic approaches.}
}
@article{SZYJEWSKI20203476,
title = {Future management},
journal = {Procedia Computer Science},
volume = {176},
pages = {3476-3485},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.049},
url = {https://www.sciencedirect.com/science/article/pii/S187705092031944X},
author = {Zdzisław Szyjewski},
keywords = {Future management, forecasting the future, new technologies},
abstract = {Accurate forecasting, good identification of trends is the basis of business success. Strategic management methods and techniques that use experience and historical economic data are not adequate to the rapidly changing business environment. In particular, technological changes, and in particular the widespread use of ICT, forces a new approach to management style and changes in the way data is acquired on the basis of which future decisions are made. Innovation thinking, a flexible and dynamic approach to making future-oriented decisions using new technologies are the foundations of future management. Therefore, the aim of the paper is to show the role and position of technology in creating the future.}
}
@article{GUO2017677,
title = {Research on Element Importance of Shafting Installation Based on QFD and FMEA},
journal = {Procedia Engineering},
volume = {174},
pages = {677-685},
year = {2017},
note = {13th Global Congress on Manufacturing and Management Zhengzhou, China 28-30 November, 2016},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.01.205},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817302059},
author = {Qi Guo and Kuangjie Sheng and Zheng Wang and Xilin Zhang and hengyi Yang and Rui Miao},
keywords = {Quality Function Deployment, Failure Mode and Effects Analysis, Marine Shafting, Comprehensive Importance},
abstract = {Development in today's shipbuilding economy is transforming from the quantitative growth to the quality growth. Quality function deployment (QFD) and failure mode and effects analysis (FMEA) adopt different ways of thinking, they remedy their respective limitations for each other, that can effectively guide the quality control. This paper is combined of HuDong ZhongHua Shipbuilding (group) co. LTD.’s shafting installation process, starting from the QFD customer requirements for finding the importance of production process elements and correction by FMEA, ultimately acquire the comprehensive importance of shafting installation process elements.}
}
@incollection{RIVELA202293,
title = {Chapter 6 - Life Cycle Sustainability Assessment-based tools},
editor = {Carmen Teodosiu and Silvia Fiore and Almudena Hospido},
booktitle = {Assessing Progress Towards Sustainability},
publisher = {Elsevier},
pages = {93-118},
year = {2022},
isbn = {978-0-323-85851-9},
doi = {https://doi.org/10.1016/B978-0-323-85851-9.00018-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323858519000183},
author = {Beatriz Rivela and Brandon Kuczenski and Dolores Sucozhañay},
keywords = {Life Cycle Thinking, Life Cycle Assessment, Life Cycle Costing, Social Life Cycle Assessment, Life Cycle Sustainability Assessment},
abstract = {This chapter establishes a baseline of ideas of what Life Cycle Thinking means: going beyond the traditional focus, understanding and including the whole environmental, social, and economic implications of decision-making processes to identify potential conflicts, synergies, and trade-offs. The life cycle methodologies for sustainability assessment are described, providing an overview of the tools and criteria currently applied, available software and databases, and ongoing challenges. While Environmental Life Cycle Assessment (LCA) is a consolidated tool, based on the ISO standards, Life Cycle Costing (LCC), the tool aimed at the assessment of the economic domain using a life cycle perspective, has not been widely integrated into sustainability assessment until the last decade. Concerning the social dimension, Social Life Cycle Assessment (S-LCA) is still at an early stage of development, but it is a promising methodology to face the challenge of integrating the social aspects towards a holistic approach to sustainable development.}
}
@article{GRAF2021100836,
title = {A cycle for validating a learning progression illustrated with an example from the concept of function},
journal = {The Journal of Mathematical Behavior},
volume = {62},
pages = {100836},
year = {2021},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2020.100836},
url = {https://www.sciencedirect.com/science/article/pii/S0732312320301000},
author = {Edith Aurora Graf and Peter W. {van Rijn} and Cheryl L. Eames},
keywords = {Learning progressions, Learning trajectories, Validation, Empirical recovery, Mathematics assessment},
abstract = {A learning progression, or learning trajectory, describes the evolution of student thinking from early conceptions to the target understanding within a particular domain. As a complex theory of development, it requires conceptual and empirical support. In earlier work, we proposed a cycle for the validation of a learning progression with four steps: 1) Theory Development, 2) Examination of Empirical Recovery, 3) Comparison to Competing Models, and 4) Evaluation of Instructional Efficacy. A group of experts met to discuss the application of learning sciences to the design, use, and validation of classroom assessment. Learning progressions, learning trajectories, and how they can support classroom assessment were the main focuses. Revisions to the cycle were suggested. We describe the adapted cycle and illustrate how the first third of it has been applied towards the validation of a learning progression for the concept of function.}
}
@article{TRAENKLE1994197,
title = {Solving microstructure electrostatistics with MIMD parallel supercomputers and Split-C},
journal = {Journal of Non-Newtonian Fluid Mechanics},
volume = {53},
pages = {197-213},
year = {1994},
issn = {0377-0257},
doi = {https://doi.org/10.1016/0377-0257(94)85049-6},
url = {https://www.sciencedirect.com/science/article/pii/0377025794850496},
author = {Frank Traenkle and Matthew I. Frank and Mary K. Vernon and Sangtae Kirn},
keywords = {Microstructure electrostatics, Multiple Instruction Multiple Data (MIMD) model, Parallel computational algorithms, Split-C},
abstract = {We consider parallel computational algorithms for the boundary integral solutions of the Laplace equation for use in the simulation of electrorheological fluids and as a model study of a class of elliptic partial differential equations that appear in basic microscopic descriptions of heterogeneous structured continua. The viewpoint is that of constructing large scale simulations that bridge micro- and macro-length and time scales on state-of-the-art parallel supercomputers. Because of long range interactions, fast communications are the key to scalable N-Body algorithms. The communication scheduling strategies of Fuentes and Kim are examined in two contexts on the Thinking Machines CM-5 parallel computer. First, an N-Body simulation implementation in C using the standard send-receive message passing primitives in the CMMD 2.0 library shows that communication scheduling leads to dramatic improvements in computational performance. Second, an implementation in Split-C, which uses highly efficient activemessages to implement shared memory communication, reduces communication overhead by an order of magnitude. Taken together, these two developments portend great promise for the development of efficient large scale simulations using portable, high level parallel programming languages.}
}
@article{PEYRACHE2024255,
title = {A homothetic data generated technology},
journal = {European Journal of Operational Research},
volume = {316},
number = {1},
pages = {255-267},
year = {2024},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2024.01.031},
url = {https://www.sciencedirect.com/science/article/pii/S037722172400050X},
author = {Antonio Peyrache},
keywords = {Data envelopment analysis, Input homotheticity, Free disposal hull, Efficiency},
abstract = {I propose a method for constructing an enlargement of a variable returns to scale production technology that will satisfy homotheticity. The method can be used both with DEA and FDH single output (or single input) technologies and it is computationally fast. The method is constructed by adding a restriction to the axiomatically delineated homothetic reference technologies which requires these reference technologies to be subsets of the minimal reference technology that satisfies constant returns to scale. Within this set it is possible to identify a homothetic technology that satisfies the property of minimum extrapolation.}
}
@incollection{HLAVAEEK20041,
title = {Chapter I - Reality, Mathematics, and Computation},
editor = {Ivan Hlaváěek and Jan Chleboun and Ivo Babuška},
series = {North-Holland Series in Applied Mathematics and Mechanics},
publisher = {North-Holland},
volume = {46},
pages = {1-49},
year = {2004},
booktitle = {Uncertain Input Data Problems and the Worst Scenario Method},
issn = {0167-5931},
doi = {https://doi.org/10.1016/S0167-5931(04)80005-6},
url = {https://www.sciencedirect.com/science/article/pii/S0167593104800056},
author = {Ivan Hlaváěek and Jan Chleboun and Ivo Babuška}
}
@incollection{KARALIS2024215,
title = {Chapter 6 - Artificial intelligence in drug discovery and clinical practice},
editor = {Natassa Pippa and Costas Demetzos and Maria Chountoulesi},
booktitle = {From Current to Future Trends in Pharmaceutical Technology},
publisher = {Academic Press},
pages = {215-255},
year = {2024},
isbn = {978-0-323-91111-5},
doi = {https://doi.org/10.1016/B978-0-323-91111-5.00006-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323911115000068},
author = {Vangelis D. Karalis},
keywords = {Artificial intelligence, Drug discovery, Clinical practice, Machine learning},
abstract = {Artificial intelligence (AI) is the imitation of human intelligence by computers. It is the science of creating intelligent machines capable of performing tasks equivalent or superior to those performed by humans. The process involves collecting data, formulating rules for its use, making approximate or definitive determinations, and self-correcting. In particular, artificial intelligence and machine learning (ML) have attracted considerable attention in a variety of sectors, including pharmaceutical sciences, and have led to a rapid rise in new applications for machine learning in numerous areas of pharmaceutical sciences. In computational chemistry, deep learning models have been used to predict drug-target interactions, develop new compounds, and predict pharmacokinetics. AI, robotics, and advanced computing have applications in drug repurposing, quality-by-design, 3D printing, and nanomedicine. When used properly, AI techniques can improve patient treatment, detection and reduction of risk factors, and identification of complications.}
}
@article{SUYOTO2015328,
title = {Parametric Approach as a Tool for Decision-making in Planning and Design Process. Case study: Office Tower in Kebayoran Lama},
journal = {Procedia - Social and Behavioral Sciences},
volume = {184},
pages = {328-337},
year = {2015},
note = {REFLECTIONS ON CREATIVITY: PUBLIC ENGAGEMENT AND THE MAKING OF PLACE},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.05.098},
url = {https://www.sciencedirect.com/science/article/pii/S1877042815033479},
author = {William Suyoto and Aswin Indraprastha and Heru W. Purbo},
keywords = {parametric design, discrete method, office tower, building modeling},
abstract = {This study offers discrete method in parametric design to solve problems during design process (programming, site planning, massing, structure planning, and facade planning). This study is applied in the design of office tower in Kebayoran Lama, Jakarta. The objective of the study is to explore the uses of parametric design method, yet, maintains its time feasibility. The result of the study is a method for planning and design that is more advantageous than the conventional ones in terms of simultaneous, coordinated and accountable. This method enables designer to do many iterations and monitor changes during the design process. However, the method needs a higher skill in logical thinking during the process, which demands time.}
}
@article{CHAUHAN2023107757,
title = {Personalized optimal room temperature and illuminance for maximizing occupant's mental task performance using physiological data},
journal = {Journal of Building Engineering},
volume = {78},
pages = {107757},
year = {2023},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2023.107757},
url = {https://www.sciencedirect.com/science/article/pii/S235271022301937X},
author = {Hardik Chauhan and Youjin Jang and Surakshya Pradhan and Hyosoo Moon},
keywords = {Indoor environment quality, Physiological response, Occupant performance, Machine learning, Particle swarm optimization},
abstract = {Indoor room temperature and illuminance level are critical factors of indoor environment quality (IEQ), affecting human mental task performance. These effects are reflected in their physiological responses such as heart rate, electrodermal activity, and skin temperature. Occupants' individual preferences, sensitivity, and physiological responses to different combinations of room temperature and illuminance level can differ among individuals. Despite previous studies investigating the individual and combined effects of different IEQ parameters, the limited research on the cross-modal relationship between room temperature and illuminance level and its impact on mental task performance highlights its significance. Moreover, to achieve personalized insights, it is essential to incorporate individual physiological responses, and this necessitates the development of an optimization model to comprehensively examine their impact. To address these issues, this study proposes a personalized model that optimizes room temperature and illuminance levels to enhance mental task performance using occupants' physiological data. Having the random forest algorithm, this study first predicted mental task performance, which includes four mental abilities such as attention, perception, working memory, and thinking ability using the occupant's physiological data. Then, the particle swarm optimization algorithm was employed to optimize room temperature and illuminance level to maximize the predicted mental task performance. The results of the proposed model align with observed values of room temperature and illuminance level during experiments, validating the adoption of a personalized approach. The findings contribute to future insights and guidelines for the design and management of indoor environments to maximize occupants' performance.}
}
@article{GROUT2014680,
title = {Taking Computer Science and Programming into Schools: The Glyndŵr/BCS Turing Project},
journal = {Procedia - Social and Behavioral Sciences},
volume = {141},
pages = {680-685},
year = {2014},
note = {4th World Conference on Learning Teaching and Educational Leadership (WCLTA-2013)},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2014.05.119},
url = {https://www.sciencedirect.com/science/article/pii/S1877042814035435},
author = {Vic Grout and Nigel Houlden},
keywords = {Computer science, programming, computing curriculum, teacher training, British Computer Society (BCS) Academy, Computing At School (CAS), Council of Professors and Heads of Computing (CPHC), Lego NXT Mindstorm, Raspberry Pi, Robot C, Scratch, Picoboards ;},
abstract = {2012 and 2013 have been challenging years for Computer Science (CS) education in the UK. After decades of national neglect, there has been a sudden impetus to reintroduce CS into the 11-16 age school curriculums. Immediate obstacles include a generation of children with no CS background and an estimated need for 20,000 new CS teachers - existing UK IT teachers being insufficiently qualified and experienced. The Computing at School (CAS) movement has been instrumental in this quantum transition from an IT to Computing syllabus, as have the British Computer Society (BCS), leading UK universities and a number of major international technology companies, including Microsoft, Google, IBM, British Telecom and Facebook.This paper discusses the background to this position and the progress being made to address these challenges. It describes, in particular, the work of the BCS-funded Glyndwr University ‘Turing Project’ in introducing Welsh high-school students and staff to high-level programming and ‘computational thinking’. The Turing Project uses an innovative combination of Lego NXT Mindstorm robots, Raspberry Pi computers and PicoBoard hardware together with the Robot C and Scratch programming platforms. The paper discusses initial objectives and the general approach, describes focused delivery across different age groups and ability ranges and presents results and analysis demonstrating the effectiveness of the programme. Lessons learnt and future directions are considered in conclusion.}
}
@incollection{LI2014249,
title = {Chapter 8 - Image Processing at Your Fingertips: The New Horizon of Mobile Imaging},
editor = {Joel Trussell and Anuj Srivastava and Amit K. Roy-Chowdhury and Ankur Srivastava and Patrick A. Naylor and Rama Chellappa and Sergios Theodoridis},
series = {Academic Press Library in Signal Processing},
publisher = {Elsevier},
volume = {4},
pages = {249-264},
year = {2014},
booktitle = {Academic Press Library in Signal Processing: Volume 4},
issn = {2351-9819},
doi = {https://doi.org/10.1016/B978-0-12-396501-1.00008-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012396501100008X},
author = {Xin Li},
keywords = {Mobile imaging, Mobile computing, Interactive image processing, Human network interaction},
abstract = {In this chapter, we briefly review the history of mobile imaging and current trend of mobile computing—namely interacting with a computer without an interface. Specifically, we highlight a list of image processing problems at fingertips: intelligent image acquisition, interactive image matting, dynamic image mosaicing and supervised image restoration. The unifying theme is how human interaction can reshape our thinking of conventional image processing algorithms. Several promising applications related to fingertip image processing are discussed at the end.}
}
@article{DUENASDIEZ2019514,
title = {How Chemistry Computes: Language Recognition by Non-Biochemical Chemical Automata. From Finite Automata to Turing Machines},
journal = {iScience},
volume = {19},
pages = {514-526},
year = {2019},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2019.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S2589004219302858},
author = {Marta Dueñas-Díez and Juan Pérez-Mercader},
keywords = {Chemistry, Chemical Reaction, Computer Science, Theory of Computation},
abstract = {Summary
Every problem in computing can be cast as decision problems of whether strings are in a language or not. Computations and language recognition are carried out by three classes of automata, the most complex of which is the Turing machine. Living systems compute using biochemistry; in the artificial, computation today is mostly electronic. Thinking of chemical reactions as molecular recognition machines, and without using biochemistry, we realize one automaton in each class by means of one-pot, table top chemical reactors: from the simplest, Finite automata, to the most complex, Turing machines. Language acceptance/rejection criteria by automata can be formulated using energy considerations. Our Turing machine uses the Belousov-Zhabotinsky chemical reaction and checks the same symbol in an Avogadro′s number of processors. Our findings have implications for chemical and general computing, artificial intelligence, bioengineering, the study of the origin and presence of life on other planets, and for artificial biology.}
}
@article{CAI2024118870,
title = {An efficient Meta-VSW method for ship behaviors recognition and application},
journal = {Ocean Engineering},
volume = {311},
pages = {118870},
year = {2024},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2024.118870},
url = {https://www.sciencedirect.com/science/article/pii/S002980182402208X},
author = {Zhiyuan Cai and Qidong Fan and Lecheng Li and Long Yu and Congbo Li},
keywords = {Ship behavior recognition, Unsupervised algorithm, Massive unknown data, Meta-trajectory, Operational efficiency, Fuel consumption},
abstract = {Ship behaviors refer to the operational process such as sailing, entering into port/departure, etc., which indicate by their position, speed, and so on. The collected big data normally have been treated by unsupervised Machine Learning methods. However, the process is time consuming and lacks consideration of time continuity. From the unknown data to recognize and recur the ship behaviors is still a complex problem. Hence, this study proposes a universal Meta-trajectory Variable Sliding Window (Meta-VSW) method to provide an efficient and high-fidelity solution. In this method, the ship data were connected into the smallest units by the meta-trajectory coding, and combines with variable sliding windows to achieve fast, continuous and accurate recognition of ship behaviors. Taking an inland-water ship and a marine transport ship as examples, the validity of the method was fulfilled and compared with two commonly used algorithms, Affinity Propagation (AP) and Density-Based Spatial Clustering of Applications with Noise (DBSCAN). It has the fastest computational speed and can effectively classify the behaviors of massive unknown data from different ships. And it has good performance in capturing behavior boundaries, with the recognition accuracy up to 0.9. Then, the method was applied to analyze the operational effectively and fuel consumption.}
}
@article{CORTI19942717,
title = {A computational study of metastability in vapor—liquid equilibrium},
journal = {Chemical Engineering Science},
volume = {49},
number = {17},
pages = {2717-2734},
year = {1994},
issn = {0009-2509},
doi = {https://doi.org/10.1016/0009-2509(94)E0093-6},
url = {https://www.sciencedirect.com/science/article/pii/0009250994E00936},
author = {David S. Corti and Pablo G. Debenedetti},
abstract = {Computer simulations are ideally suited to study systems under arbitrary constraints; hence they are useful for the investigation of metastability. Different types of constraints were applied to the three-dimensional Lennard—Jones fluid in the vapor—liquid coexistence region. Constraining the magnitude of allowed density fluctuations (restricted ensemble) has little effect on the equation of state and on phase equilibrium predictions for reduced temperatures lower than 0.95. Thermodynamic integrations along constrained and unstable paths are in good agreement with chemical potential calculations, indicating that imposing the density constraint does not violate microscopic reversibility. Restricted ensemble calculations were also used to calculate the width of the transition region where the mechanism of phase separation in the superheated liquid changes from nucleation to spinodal decomposition. The width of this region decreases as the temperature is reduced away from criticality. Free energy barriers to isotropic compression were used to determine the width of the transition region from nucleation to spinodal decomposition in the supercooled vapor. This transition region also becomes narrower as the distance from the critical point increases. The pressure of the deeply superheated liquid was found to be sensitive to the maximum size of voids that are allowed to form.}
}
@article{GUPTA20062290,
title = {Towards a new paradigm for innovative training methods for capacity building in remote sensing},
journal = {Advances in Space Research},
volume = {38},
number = {10},
pages = {2290-2298},
year = {2006},
note = {Remote Sensing of Oceanographic Processes and Land Surfaces; Space Science Education and Outreach},
issn = {0273-1177},
doi = {https://doi.org/10.1016/j.asr.2006.06.017},
url = {https://www.sciencedirect.com/science/article/pii/S0273117706004285},
author = {R.K. Gupta and P.M. Bala Manikavelu and D. Vijayan and T.S. Prasad},
keywords = {Thinking curricula, Innovative training methods, Capacity building, Remote sensing},
abstract = {Everybody uses a bulb to illustrate an idea but nobody shows where the current comes from. Majority of remote sensing user community comes from natural and social sciences domain while remote sensing technology evolves from physical and engineering sciences. To ensure inculcation and internalization of remote sensing technology by application/resource scientists, trainer needs to transfer physical and engineering concepts in geometric manner. Here, the steering for the transfer of knowledge (facts, procedures, concepts and principles) and skills (thinking, acting, reacting and interacting) needs to take the trainees from Known to Unknown, Concrete to Abstract, Observation to Theory and Simple to Complex. In the initial stage of training/education, experiential learning by instructor led exploring of thematic details in false colour composite (FCC) as well as in individual black and white spectral band(s) imagery by trainees not only creates interest, confidence build-up and orientation towards purposeful learning but also helps them to overcome their inhibitions towards the physical and engineering basal. The methodology to be adopted has to inculcate productive learning, emphasizing more on thinking and trial and error aspects as opposed to reproductive learning based dominantly on being told and imitation. The delivery by trainer needs to ensure dynamic, stimulating and effective discussions through deluging questions pertaining to analysis, synthesis and evaluation nature. This would ensure proactive participation from trainees. Hands-on module leads to creative concretization of concepts. To keep the trainees inspired to learn in an auto mode during post-training period, they need to consciously swim in the current and emerging knowledge pool during training programme. This is achieved through assignment of seminar delivery task to the trainees. During the delivery of seminar, peers and co-trainees drive the trainee to communicate the seminar content not only in what but also in how and why mode. The interest culminated in this manner keeps the entropy of the trainee minimized even during post-training professional life. So, such germinated trainee would always generate positive induction among colleagues; thus, helping in realizing multiplier effect. Based upon above thought process(es), the paper discusses the concept of “thinking curricula” and associated cares needed in training deliveries.}
}
@article{ZALL2024101285,
title = {Towards emotion-aware intelligent agents by utilizing knowledge graphs of experiences},
journal = {Cognitive Systems Research},
volume = {88},
pages = {101285},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101285},
url = {https://www.sciencedirect.com/science/article/pii/S1389041724000792},
author = {Raziyeh Zall and Mohammad Reza Kangavari},
keywords = {Affective computing, Intelligent agent, Cognitive architecture, Appraisal theory, Emotional mental state},
abstract = {Because of the increasing presence of intelligent agents in various aspects of human social life, social skills play a vital role in ensuring these systems exhibit acceptable and realistic behavior in social communication. The importance of emotional intelligence in social capabilities is noteworthy, so incorporating emotions into the behaviors of intelligent agents is essential. Therefore, some computational models of emotions have been presented to develop intelligent agents that exhibit emotional human-like behaviors. However, most current computational models of emotions neglect the dynamic learning of the affective meaning of events based on agents’ experiences. Such models evaluate the events in the environment according to emotional aspects without considering the context of the situations. Also, these models capture the emotional states of agents by using predefined rules determined according to psychological theories. Therefore, they disregard the data-driven methods that can obtain the relationships between appraisal variables and emotions based on natural human data with fewer assumptions on the nature of such relationships. To address these issues, we proposed a novel and unified affective-cognitive framework (EIAEC) to facilitate the development of emotion-aware intelligent agents. EIAEC uses appraisal theories to acquire the emotional states of the agent in various situations. This paper contains four main contributions: 1- We have designed an efficient episodic memory that uses events and their conditional contexts to store and retrieve knowledge and experiences. This memory facilitates emotional expressions and decision-making adapted to the situations of the agent. 2- A novel method has been proposed that learns context-dependent affective values associated with events by using the agent’s experiences in various contexts. Subsequently, we acquired appraisal variables using the elements and related meta-data in episodic memory. 3- We have proposed a new data-driven method that maps appraisal variables to emotional states. 4- Moreover, a method has been developed to update the activation values regarding actions by using the emotional states of the agent. This method models the influence of emotions on the agent’s decision-making. Finally, we simulate a driving scenarios in our proposed framework to manifest the generated emotions in different situations and conditions. Moreover, we show how the proposed method learns the affective meaning of events and actions used in appraisal computing.}
}
@article{PUDANE2017517,
title = {Human Emotional Behavior Simulation in Intelligent Agents: Processes and Architecture},
journal = {Procedia Computer Science},
volume = {104},
pages = {517-524},
year = {2017},
note = {ICTE 2016, Riga Technical University, Latvia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.01.167},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917301680},
author = {Mara Pudane and Egons Lavendelis and Michael A. Radin},
keywords = {Affective agents, Emotive agents, Human behavior simulation, Agent internal architecture},
abstract = {The paper describes and discusses processes needed for human emotional behaviour simulation, in particular, emotion incorporation into rational thinking, as well as presents corresponding agent architecture. Such system would enable various application fields, perhaps one of the most important being enhancing smart devices with emotions. Decreasing frequency of social contact has become an urgent issue, particularly among young people. Emotional and social intelligence are however highly desired set of skills which is impossible to develop without interacting with others. Although this problem has been acknowledged, and there are some efforts to facilitate social contact, e.g., by augmented virtual reality games, that is still not enough. There is a need to develop environment that would allow learning exactly social and emotional skills. This on-going research aims at developing intelligent agents that are able to express and incorporate affects into rational processes.}
}
@article{USKOKOVIC2023e15015,
title = {Natural sciences and chess: A romantic relationship missing from higher education curricula},
journal = {Heliyon},
volume = {9},
number = {4},
pages = {e15015},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e15015},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023022223},
author = {Vuk Uskoković},
keywords = {Chemistry, Chess, Creativity, Culture, Education, Instruction, Science},
abstract = {Chess is a game that delicately weaves analytical thinking around artistic experience, yet recent conversions of STEM (Science-Technology-Engineering-Mathematics) to STEAM (Science-Technology-Engineering-Art-Mathematics) have omitted adding chess as an elementary coursework to K-12 and higher education curricula. Chess, as per arguments presented in this essay, can be considered as a language and a tool for furthering the development of artistic skills among scientists and analytical, pattern-recognition skills among artists. It can also serve as a missing link between science and art in STEAM curricula thanks to its finding itself halfway between the two. A handful of analogies are drawn here from chess, illustrated sporadically with positions from real-life chess games and converted to lessons in creativity for students in natural sciences. The discussion centered around these analogies is reinforced by a literature review of studies conducted over the past 80 years to assess the effect of exposing students to lessons in chess on their learning in distant domains. Overall, great benefits can emerge from complementing science education with chess and it is hoped that chess will become an integral part of basic education in primary schools and universities worldwide in the near future.}
}
@incollection{FAVERO2023509,
title = {Chapter 25 - Raster objects},
editor = {Luiz Paulo Fávero and Patrícia Belfiore and Rafael {de Freitas Souza}},
booktitle = {Data Science, Analytics and Machine Learning with R},
publisher = {Academic Press},
pages = {509-519},
year = {2023},
isbn = {978-0-12-824271-1},
doi = {https://doi.org/10.1016/B978-0-12-824271-1.00011-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128242711000111},
author = {Luiz Paulo Fávero and Patrícia Belfiore and Rafael {de Freitas Souza}},
keywords = {Spatial analysis, Maps, Raster objects, R},
abstract = {At the end of this chapter, you will be able to:•Understand what a raster object is;•Load and use raster objects;•Combine raster objects with shapefiles;•Manipulate and cut out raster objects;•Use the raster objects in such a way as to demand less computational time for the execution of tasks;•View raster objects in R language.}
}
@article{BENTON2023105626,
title = {Associative learning or Bayesian inference? Revisiting backwards blocking reasoning in adults},
journal = {Cognition},
volume = {241},
pages = {105626},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2023.105626},
url = {https://www.sciencedirect.com/science/article/pii/S0010027723002603},
author = {Deon T. Benton and David H. Rakison},
keywords = {Causal reasoning, Causal mechanisms, Computational models, Analytical models, Associative learning, Bayesian inference},
abstract = {Causal reasoning is a fundamental cognitive ability that enables humans to learn about the complex interactions in the world around them. However, the cognitive mechanisms that underpin causal reasoning are not well understood. For instance, there is debate over whether Bayesian inference or associative learning best captures causal reasoning in human adults. The two experiments and computational models reported here were designed to examine whether adults engage in one form of causal inference called backwards blocking reasoning, whether the presence of potential distractors affects performance, and how adults' ratings align with the predictions of different computational models. The results revealed that adults engaged in backwards blocking reasoning regardless of whether distractor objects are present and that their causal judgements supported the predictions of a Bayesian model but not the predictions of two different associative learning models. Implications of these results are discussed.}
}
@incollection{BATCHELDER2015808,
title = {Mathematical Psychology: History},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {808-815},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.43059-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008097086843059X},
author = {William H. Batchelder},
keywords = {Bayesian theory, Choice, Correlation coefficient, Decision making, Differential psychology, Experimental psychology, Factor analysis, Game theory, Information science, Learning theory, Marginal utility, Measurement theory, Memory, Paired-comparison scaling, Psychometrics, Psychophysics, Response time, Scaling, Signal detection theory, Stochastic processes, Theory of grammar, Utility theory},
abstract = {The application of mathematics to certain problems within the field of psychology dates back to at least the seventeenth century. This article reviews some of these early applications, most of which either involve theories for experimental phenomena or statistical methods for measuring individual differences. These later applications led to the field of psychometrics in the 1930s; and the former led to the field of mathematical psychology in the 1950s, and both fields are active today. Early mathematical psychology was characterized by testable, formal theories in the areas of learning and memory, perception and psychophysics, choice and decision making, language and thinking, and measurement and scaling; and these areas still characterize the field today.}
}
@article{MCGOWEN2010169,
title = {Metaphor or Met-Before? The effects of previouos experience on practice and theory of learning mathematics},
journal = {The Journal of Mathematical Behavior},
volume = {29},
number = {3},
pages = {169-179},
year = {2010},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2010.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0732312310000404},
author = {Mercedes A. McGowen and David O. Tall},
keywords = {Metaphor, Met-before, Epistemological obstacle, Embodiment, Local straightness},
abstract = {While the general notion of ‘metaphor’ may offer a thoughtful analysis of the nature of mathematical thinking, this paper suggests that it is even more important to take into account the particular mental structures available to the individual that have been built from experience that the individual has ‘met-before.’ The notion of ‘met-before’ offers not only a principle to analyse the changing meanings in mathematics and the difficulties faced by the learner—which we illustrate by the problematic case of the minus sign—it can also be used to analyse the met-befores of mathematicians, mathematics educators and those who develop theories of learning to reveal implicit assumptions that support our thinking in some ways and act as impediments in others.}
}
@article{URECH2022101731,
title = {A simulation-based design framework to iteratively analyze and shape urban landscapes using point cloud modeling},
journal = {Computers, Environment and Urban Systems},
volume = {91},
pages = {101731},
year = {2022},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2021.101731},
url = {https://www.sciencedirect.com/science/article/pii/S0198971521001381},
author = {Philipp R.W. Urech and Muhammad Omer Mughal and Carlos Bartesaghi-Koc},
keywords = {Point-cloud modeling, Computational fluid dynamics, Laser-scanned data, Urban landscape design, Design performance},
abstract = {The topic of this paper evolves on the discourse of digital modeling in landscape design. Current design methods stagger to address physical forms and dynamics present in the environment. This status quo limits possibilities to integrate scientific evidence when developing spatial and aesthetic configurations in urban landscapes. Remote sensing technology such as laser scanning measures physical forms to reproduce them as geo-specific digital 3D models, while dynamic simulation is widely used to predict how scenarios will perform under given conditions. However, there is still a need for a holistic design process that is capable of integrating both the measured physical forms and physical dynamics. This paper presents a novel framework using point cloud modeling to shape design scenarios that are iteratively evaluated for their performance. The proposed framework is demonstrated through a case study in Singapore. New spatial configurations are tested for the site through an iterative and comparative analysis of the design performance. The case study exposes (1) a site-specific design approach by iteratively modeling a laser-scanned point cloud model, (2) a workflow to convert the geometric data from the point cloud models into voxels and meshes, (3) an integration of computational fluid dynamics (CFD) simulation during design development as per-point attributes, and (4) a comparison of the configurations to identify best performing scenarios. This design framework can support city managers, planners, urban and landscape designers to better inform their decision-making process by relying on accurate scientific feedback. By guiding the design process with the consideration of the built environment as a complex adaptive system, it will be possible to improve how open spaces and ecosystem services perform in cities, and to design landscapes that can mitigate dynamic events such as urban heat islands.}
}
@article{HASELI2023184,
title = {HECON: Weight assessment of the product loyalty criteria considering the customer decision's halo effect using the convolutional neural networks},
journal = {Information Sciences},
volume = {623},
pages = {184-205},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.12.027},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522015201},
author = {Gholamreza Haseli and Ramin Ranjbarzadeh and Mostafa Hajiaghaei-Keshteli and Saeid {Jafarzadeh Ghoushchi} and Aliakbar Hasani and Muhammet Deveci and Weiping Ding},
keywords = {Customer loyalty, Deep learning, Convolutional neural networks, Multi-criteria decision-making, Halo effect},
abstract = {The economic pressures and increasing competition in markets have led to the CEOs of companies being forced to make the right strategic decisions in the development of products for selling the right products to the right customers. To achieve this goal, companies need to know which criteria of their products lead to customer loyalty to that product. In the past, various methods have been introduced to obtain the importance (weight) of criteria that use the opinions of experts or customers. There is a halo effect in human decisions that leads to biases in evaluating the criteria by influencing human emotions. This study introduces a new method for weight assessment of the product loyalty criteria by considering the customer's decisions halo effect using the convolutional neural network (CNN) called the halo effect using the convolutional neural networks (HECON) method. In the HECON method to consider the halo effect of the customer decisions, a CNN model is proposed as the deep learning pipeline to obtain more accurate weights of the criteria. The HECON method to obtain the weight of the criteria and identify criteria that lead to product loyalty needs to collect the feedback of a large number of customers based on the net promoter score (NPS) scale. The innovation of the HECON method is to obtain the effect level of each product criterion on selection and loyalty to the product through the feedback of a large number of customers by considering the halo effect on the customers' thinking. To date, the analyzing methods have often not been able to identify the halo effect in evaluating the reasons for customer loyalty to the product. The halo effect indicates sometimes some of the product criteria secretly affect the customers' opinions that require deep neural networks to analyze them. By using the deep CNN model of the HECON method to evaluate product criteria for understanding customer behavior, companies will be able to identify customers' behavior and develop their products exactly following the customer's desires. To evaluate the performance and demonstrate the applicability of the HECON method, presented two case studies. It is presented that there are challenging differences between the results of the HECON method with the other methods because the HECON method considers the halo effect on the customer decisions and demonstrates better performance.}
}
@article{SUN20112118,
title = {How digital scaffolds in games direct problem-solving behaviors},
journal = {Computers & Education},
volume = {57},
number = {3},
pages = {2118-2125},
year = {2011},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2011.05.022},
url = {https://www.sciencedirect.com/science/article/pii/S036013151100128X},
author = {Chuen-Tsai Sun and Dai-Yi Wang and Hui-Ling Chan},
keywords = {Human–computer interface, Interactive learning environments, Secondary education, Teaching/learning strategies},
abstract = {Digital systems offer computational power and instant feedback. Game designers are using these features to create scaffolding tools to reduce player frustration. However, researchers are finding some unexpected effects of scaffolding on strategy development and problem-solving behaviors. We used a digital Sudoku game named Professor Sudoku to classify built-in critical features, frustration control and demonstration scaffolds, and to investigate their effects on player/learner behaviors. Our data indicate that scaffolding support increased the level at which puzzles could be solved, and decreased frustration resulting from excessive numbers of retries. However, it also reduced the number of unassisted placements (i.e., independently filled cells), and increased reliance on scaffolding tools, both of which are considered disadvantageous for learning. Among the three scaffold types, frustration control reduced the potential for players to feel stuck at certain levels, but also reduced the frequency of use of critical feature-making tools, which are thought to have greater heuristic value. We conclude that the simultaneous provision of critical feature and frustration control scaffolds may increase player reliance on available support, thereby reducing learning opportunities. Providing players with critical features and demonstration scaffolds at the same time increases reliance on available support for some players, but for most it encourages the development of solving strategies.}
}
@incollection{RUNCO200771,
title = {Chapter 3 - Biological Perspectives on Creativity},
editor = {Mark A. Runco},
booktitle = {Creativity},
publisher = {Academic Press},
address = {Burlington},
pages = {71-113},
year = {2007},
isbn = {978-0-12-602400-5},
doi = {https://doi.org/10.1016/B978-012602400-5/50003-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780126024005500034},
author = {Mark A. Runco},
abstract = {Publisher Summary
This chapter discusses various aspects of biological perspectives on creativity. Some of the research on creativity as of late involves the brain and biological correlates of originality, novelty, and insight. Handedness is sometimes used as an indication of hemispheric dominance or hemisphericity, with right-handed people being compared to left-handed people. There are several reports of left-handed persons outnumbering the right-handed in creative and eminent samples. Hemisphericity and other important brain structures and processes contributing to creative thinking and behavior have been studied with EEG, PET, cerebral blood flow, and MRI techniques. Numerous EEG studies suggest that there are particular brain-wave patterns and brain structures that are associated with creative problem solving, or at least specific phases within the problem solving process. EEGs suggest a complex kind of activity while individuals work on divergent thinking tasks. The complexity disappears when those same individuals work on convergent thinking tasks. It is found that the role of the prefrontal cortex in creative thinking and behavior comes from several sources and uses different methodologies.}
}
@incollection{ZHUGE201655,
title = {4 - The think lens},
editor = {Hai Zhuge},
booktitle = {Multi-Dimensional Summarization in Cyber-Physical Society},
publisher = {Morgan Kaufmann},
pages = {55-65},
year = {2016},
series = {Computer Science Reviews and Trends},
isbn = {978-0-12-803455-2},
doi = {https://doi.org/10.1016/B978-0-12-803455-2.00004-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128034552000044},
author = {Hai Zhuge},
keywords = {Think lens, Semantic lens, models, semantic images, multi-dimensional, semantic link network},
abstract = {The nature of many research problems is about scale and dimension of observation and thinking. Whether the patterns and rules on one scale still hold on the other scale? Whether the patterns and rules on one dimension or some dimensions still hold on the other dimension or some other dimensions? Summarization is also about the scale and the dimension of motivation, representation and thinking. Human eyes can focus on not only a part of a representation but also the whole from a certain distance like the lens of camera. The think lens is a mechanism that can zoom in and out while observing, searching, mapping, analysing, planning, predicting, calculating, reasoning, imaging, and representing patterns through semantic computing on various representations according to some principles and rules. This section presents a concept model of the think lens for realising general summarisation in cyber-physical society.}
}
@article{KOWALSKI2020103693,
title = {Effects of attention training technique on brain function in high- and low-cognitive-attentional syndrome individuals: Regional dynamics before, during, and after a single session of ATT},
journal = {Behaviour Research and Therapy},
volume = {132},
pages = {103693},
year = {2020},
issn = {0005-7967},
doi = {https://doi.org/10.1016/j.brat.2020.103693},
url = {https://www.sciencedirect.com/science/article/pii/S0005796720301479},
author = {Joachim Kowalski and Małgorzata Wierzba and Marek Wypych and Artur Marchewka and Małgorzata Dragan},
keywords = {Attention training technique, Metacognitve therapy, Attention, fMRI, S-REF},
abstract = {Objective
Attention Training Technique (ATT) is a key therapeutic tool in metacognitive therapy. There are numerous studies on the behavioral effects of ATT, however the neural mechanisms at work in the training are yet to be uncovered. To date there have been no controlled fMRI studies of ATT.
Method
We conducted a randomized double-blind controlled study of two groups with varying levels of cognitive-attentional syndrome (CAS). Groups with high (n = 43) and low (n = 46) levels of CAS underwent a single session of ATT or a control condition (CON) in an MRI scanner. Participants underwent resting state functional MRI (rsfMRI) sessions and rumination induction sessions both pre- and post-intervention Functional connectivity analyses and inter-subject correlations analyses were computed. We also collected data on emotion and attention functioning pre- and post-intervention.
Results
We did not observe any behavioral effects of ATT. However, direct comparison between ATT and CON sessions revealed greater inter-subject correlations in almost all hubs belonging to the studied functional networks. Moreover, subjects who received ATT showed diminished connectivity in the fronto-parietal network during ruminations and diminished connectivity of the precuneus with lateral occipital cortices and the intraparietal sulcus in abstract thinking and rsfMRI, respectively. Furthermore, some of the observed effects in functional connectivity and inter-subject correlations were specific to different levels of CAS.
Conclusions
Our results may support a proposed neural mechanism for ATT: disengagement of attention from CAS-type processing in either low- or high-CAS individuals. It is also possible that some neural effects of ATT are specific to individuals with different levels of CAS.}
}
@incollection{PAGEL201749,
title = {Chapter Four - Testing for Machine Consciousness},
editor = {J.F. Pagel and Philip Kirshtein},
booktitle = {Machine Dreaming and Consciousness},
publisher = {Academic Press},
address = {San Diego},
pages = {49-65},
year = {2017},
isbn = {978-0-12-803720-1},
doi = {https://doi.org/10.1016/B978-0-12-803720-1.00004-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128037201000049},
author = {J.F. Pagel and Philip Kirshtein},
keywords = {Thinking, intelligence, attention, intentionality, volition, self-awareness, artificial intelligence, AI, autonomous entity, Turing Test, Chinese Room Test},
abstract = {Thinking, intelligence, data integration, and attention are aspects of consciousness for which tests have been designed. A short history of the Computer Science field, a description, and an assessment of results obtained to this point for the Turing Test and Chinese Room Test are part of this chapter. Alternative definitions of artificial intelligence are presented. Applied tests for consciousness including those for intelligence, attention, intentionality, volition, and self-awareness are discussed as applied to the assessment of machine systems. Strong AI and the concept of autonomous entities are defined and addressed. The presence of dream-equivalent states is discussed as a potential marker for human-equivalent consciousness.}
}
@article{MENG201248,
title = {Extracting linguistic rules from data sets using fuzzy logic and genetic algorithms},
journal = {Neurocomputing},
volume = {78},
number = {1},
pages = {48-54},
year = {2012},
note = {Selected papers from the 8th International Symposium on Neural Networks (ISNN 2011)},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2011.05.029},
url = {https://www.sciencedirect.com/science/article/pii/S0925231211004711},
author = {Dan Meng and Zheng Pei},
keywords = {Computing with Words, Linguistic rules, Fuzzy logic, Genetic algorithms},
abstract = {Linguistic rules in natural language are useful and consistent with human way of thinking. They are very important in multi-criteria decision making due to their interpretability. In this paper, our discussions concentrate on extracting linguistic rules from data sets. In the end, we firstly analyze how to extract complex linguistic data summaries based on fuzzy logic. Then, we formalize linguistic rules based on complex linguistic data summaries, in which, the degree of confidence of linguistic rules from a data set can be explained by linguistic quantifiers and its linguistic truth from the fuzzy logical point of view. In order to obtain a linguistic rule with a higher degree of linguistic truth, a genetic algorithm is used to optimize the number and parameters of membership functions of linguistic values. Computational results show that the proposed method is an alternative method for extracting linguistic rules with linguistic truth from data sets.}
}
@article{CHANG2017160,
title = {Dynamic modeling approaches to characterize the functioning of health systems: A systematic review of the literature},
journal = {Social Science & Medicine},
volume = {194},
pages = {160-167},
year = {2017},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2017.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0277953617305300},
author = {Angela Y. Chang and Osondu Ogbuoji and Rifat Atun and Stéphane Verguet},
keywords = {Health systems, Dynamic modeling, Systems thinking, System dynamics},
abstract = {Universal Health Coverage (UHC) is one of the targets for the United Nations Sustainable Development Goal 3. The impetus for UHC has led to an increased demand for time-sensitive tools to enhance our knowledge of how health systems function and to evaluate impact of system interventions. We define the field of “health system modeling” (HSM) as an area of research where dynamic mathematical models can be designed in order to describe, predict, and quantitatively capture the functioning of health systems. HSM can be used to explore the dynamic relationships among different system components, including organizational design, financing and other resources (such as investments in resources and supply chain management systems) – what we call “inputs” – on access, coverage, and quality of care – what we call “outputs”, toward improved health system “outcomes”, namely increased levels and fairer distributions of population health and financial risk protection. We undertook a systematic review to identify the existing approaches used in HSM. We identified “systems thinking” – a conceptual and qualitative description of the critical interactions within a health system – as an important underlying precursor to HSM, and collated a critical collection of such articles. We then reviewed and categorized articles from two schools of thoughts: “system dynamics” (SD)” and “susceptible-infected-recovered-plus” (SIR+). SD emphasizes the notion of accumulations of stocks in the system, inflows and outflows, and causal feedback structure to predict intended and unintended consequences of policy interventions. The SIR + models link a typical disease transmission model with another that captures certain aspects of the system that impact the outcomes of the main model. These existing methods provide critical insights in informing the design of HSM, and provide a departure point to extend this research agenda. We highlight the opportunity to advance modeling methods to further understand the dynamics between health system inputs and outputs.}
}
@incollection{VALLERO202151,
title = {Chapter 3 - Transitional and translational sciences},
editor = {Daniel A. Vallero},
booktitle = {Environmental Systems Science},
publisher = {Elsevier},
pages = {51-87},
year = {2021},
isbn = {978-0-12-821953-9},
doi = {https://doi.org/10.1016/B978-0-12-821953-9.00012-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012821953900012X},
author = {Daniel A. Vallero},
keywords = {Translational science, Geodesign, Landfill fires, Tire fires, Coal mine fires, Nuclear accidents, “As low as reasonably practicable” (ALARP), Rational methods, Modularity, Interoperability},
abstract = {This chapter introduces two aspects critical to environmental systems science. The first is attention to ways to move from reductionism to systems thinking. The second is the need to translate the methods, results, and meaning of scientific discoveries from one discipline to all those needed to address an environmental or public health problem. To aid in this discussion, several examples of environmental episodes are discussed with an eye toward root causes. Knowledgebase needs to support the transition to systems thinking are discussed, including modularity and interoperability of models and methods}
}
@article{BRENTDANIEL2023105630,
title = {Extremely rapid, Lagrangian modeling of 2D flooding: A rivulet-based approach},
journal = {Environmental Modelling & Software},
volume = {161},
pages = {105630},
year = {2023},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2023.105630},
url = {https://www.sciencedirect.com/science/article/pii/S1364815223000166},
author = {W. {Brent Daniel} and Corinne Roth and Xue Li and Cindy Rakowski and Tim McPherson and David Judi},
keywords = {Modeling, Fluid flow, Flood modeling, Hydrodynamic model, Rivulet, Lagrangian},
abstract = {Estimates of potential flood inundation areas and depths are critical to informing the preparedness, response, and investment decisions of many government agencies and private sector organizations, especially under a changing climate. The standard modeling approaches, however, are often either computationally intensive or constrained in their accuracy or applicability. A novel, rivulet-based, 2D model of flooding is described in this article that is 10,000 to 10 million times less computationally complex than the full solution of the shallow water equations, yet achieves inundation area hit rates of between 0.8 and 0.9, relative absolute mean errors of 10%–20% across a wide range of flow depths, and comparable accuracy at forecasting empirical high-water marks. This combination of accuracy and efficiency will significantly enhance real-time depth estimates during flood events, support detailed sensitivity analyses, and allow for the generation of large ensembles to enable complex uncertainty analyses.}
}
@article{FUKAI2021145,
title = {Neural mechanisms for learning hierarchical structures of information},
journal = {Current Opinion in Neurobiology},
volume = {70},
pages = {145-153},
year = {2021},
note = {Computational Neuroscience},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2021.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S0959438821001252},
author = {Tomoki Fukai and Toshitake Asabuki and Tatsuya Haga},
abstract = {Spatial and temporal information from the environment is often hierarchically organized, so is our knowledge formed about the environment. Identifying the meaningful segments embedded in hierarchically structured information is crucial for cognitive functions, including visual, auditory, motor, memory, and language processing. Segmentation enables the grasping of the links between isolated entities, offering the basis for reasoning and thinking. Importantly, the brain learns such segmentation without external instructions. Here, we review the underlying computational mechanisms implemented at the single-cell and network levels. The network-level mechanism has an interesting similarity to machine-learning methods for graph segmentation. The brain possibly implements methods for the analysis of the hierarchical structures of the environment at multiple levels of its processing hierarchy.}
}
@article{OXMAN1994141,
title = {Precedents in design: a computational model for the organization of precedent knowledge},
journal = {Design Studies},
volume = {15},
number = {2},
pages = {141-157},
year = {1994},
issn = {0142-694X},
doi = {https://doi.org/10.1016/0142-694X(94)90021-3},
url = {https://www.sciencedirect.com/science/article/pii/0142694X94900213},
author = {Rivka E Oxman},
keywords = {case-based reasoning, design precedents, memory organization},
abstract = {A computational model for the organization of design precedent knowledge is developed. The model is composed of distinct chunks of knowledge called design stories. A formalism for the design story is proposed which represents the linkage between design issue, concept and form in designs. Stories are structured in memory according to a semantic network. The lexicon of the semantic network acts as a memory index. The memory structure and indexing system are demonstrated to enhance search and to support cross-contextual browsing and exploration in the precedent library. The approach is demonstrated in a pilot design aid system in the task domain of early conceptual design in architecture.}
}
@article{FLAHERTY2022114546,
title = {The conspiracy of Covid-19 and 5G: Spatial analysis fallacies in the age of data democratization},
journal = {Social Science & Medicine},
volume = {293},
pages = {114546},
year = {2022},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2021.114546},
url = {https://www.sciencedirect.com/science/article/pii/S0277953621008789},
author = {Eoin Flaherty and Tristan Sturm and Elizabeth Farries},
keywords = {Conspiracy theories, Spatial data, Health geography, Public data, COVID-19, 5G},
abstract = {In a context of mistrust in public health institutions and practices, anti-COVID/vaccination protests and the storming of Congress have illustrated that conspiracy theories are real and immanent threat to health and wellbeing, democracy, and public understanding of science. One manifestation of this is the suggested correlation of COVID-19 with 5G mobile technology. Throughout 2020, this alleged correlation was promoted and distributed widely on social media, often in the form of maps overlaying the distribution of COVID-19 cases with the instillation of 5G towers. These conspiracy theories are not fringe phenomena, and they form part of a growing repertoire for conspiracist activist groups with capacities for organised violence. In this paper, we outline how spatial data have been co-opted, and spatial correlations asserted by conspiracy theorists. We consider the basis of their claims of causal association with reference to three key areas of geographical explanation: (1) how social properties are constituted and how they exert complex causal forces, (2) the pitfalls of correlation with spatial and ecological data, and (3) the challenges of specifying and interpreting causal effects with spatial data. For each, we consider the unique theoretical and technical challenges involved in specifying meaningful correlation, and how their discarding facilitates conspiracist attribution. In doing so, we offer a basis both to interrogate conspiracists’ uses and interpretation of data from elementary principles and offer some cautionary notes on the potential for their future misuse in an age of data democratization. Finally, this paper contributes to work on the basis of conspiracy theories in general, by asserting how – absent an appreciation of these key methodological principles – spatial health data may be especially prone to co-option by conspiracist groups.}
}
@article{MEHRYAR2024109812,
title = {AI and climate resilience governance},
journal = {iScience},
volume = {27},
number = {6},
pages = {109812},
year = {2024},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2024.109812},
url = {https://www.sciencedirect.com/science/article/pii/S2589004224010344},
author = {Sara Mehryar and Vahid Yazdanpanah and Jeffrey Tong},
keywords = {Natural sciences, Earth sciences, Environmental science, Environmental policy, Social sciences},
abstract = {Summary
While artificial intelligence (AI) offers promising solutions to address climate change impacts, it also raises many application limitations and challenges. A risk governance perspective is used to analyze the role of AI in supporting decision-making for climate adaptation, spanning risk assessment, policy analysis, and implementation. This comprehensive review combines expert insights and systematic literature review. The study’s findings indicate a large emphasis on applying AI to climate “risk assessments,” particularly regarding hazard and exposure assessment, but a lack of innovative approaches and tools to evaluate resilience and vulnerability as well as prioritization and implementation process, all of which involve subjective, qualitative, and context-specific elements. Additionally, the study points out challenges such as difficulty of simulating complex long-term changes, and evolving policies and human behavior, reliance on data quality and computational resources, and the need for improved interpretability of results as areas requiring further development.}
}
@article{CHUNG201356,
title = {Table-top role playing game and creativity},
journal = {Thinking Skills and Creativity},
volume = {8},
pages = {56-71},
year = {2013},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2012.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1871187112000478},
author = {Tsui-shan Chung},
keywords = {Creativity, Role playing game, Divergent thinking test, Priming},
abstract = {The current study aims to observe whether individuals who engaged in table-top role playing game (TRPG) were more creative. Participants total 170 (52 TRPG players, 54 electronic role playing game (ERPG) players and 64 Non-players) aged from 19 to 63. In the current study, an online questionnaire is used, adopting the verbal subtests of Wallach–Kogan Creativity Tests and the McCrae and Costa Big Five Personality Inventory. It is found that TRPG players score higher in divergent thinking tests. Priming and instruction giving methods lower the performance of all participants, in particular, when the instruction is memory provoking. ERPG players score lowest among the three groups. TRPG could be regarded as a form of improvisation. It could also be a preferable activity for the promotion of creativity. It is low cost and no formal setting is required to play. Many ERPGs are originated from TRPGs, therefore, with the popularity of ERPG, there should be advantages in promoting TRPG.}
}
@article{HUANG2022108818,
title = {Hippocampus-heuristic character recognition network for zero-shot learning in Chinese character recognition},
journal = {Pattern Recognition},
volume = {130},
pages = {108818},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108818},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002990},
author = {Guanjie Huang and Xiangyu Luo and Shaowei Wang and Tianlong Gu and Kaile Su},
keywords = {Chinese character recognition, Hippocampus thinking, Radical analysis, Zero-shot learning, Label embedding},
abstract = {The recognition of Chinese characters has always been a challenging task due to their huge variety and complex structures. The current radical-based methods fail to recognize Chinese characters without learning all of their radicals in the training stage. To this end, we propose a novel Hippocampus-heuristic Character Recognition Network (HCRN), which can recognize unseen Chinese characters only by training part of radicals. More specifically, the network architecture of HCRN is a new pseudo-siamese network designed by us, which can learn features from pairs of input samples and use them to predict unseen characters. The experimental results on the recognition of printed and handwritten characters show that HCRN is robust and effective on zero/few-shot learning tasks. For the printed characters, the mean accuracy of HCRN outperforms the state-of-the-art approach by 23.93% on recognizing unseen characters. For the handwritten characters, HCRN improves the mean accuracy by 11.25% on recognizing unseen characters.}
}
@article{CHEN2010573,
title = {Generating ontologies with basic level concepts from folksonomies},
journal = {Procedia Computer Science},
volume = {1},
number = {1},
pages = {573-581},
year = {2010},
note = {ICCS 2010},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2010.04.061},
url = {https://www.sciencedirect.com/science/article/pii/S1877050910000621},
author = {Wen-hao Chen and Yi Cai and Ho-fung Leung and Qing Li},
keywords = {Folksonomy, Ontology, Basic level categories, Category utility},
abstract = {This paper deals with the problem of ontology generation. Ontology plays an important role in knowledge representation, and it is an artifact describing a certain reality with specific vocabulary. Recently many researchers have realized that folksonomy is a potential knowledge source for generating ontologies. Although some results have already been reported on generating ontologies from folksonomies, most of them do not consider what a more acceptable and applicable ontology for users should be, nor do they take human thinking into consideration. Cognitive psychologists find that most human knowledge is represented by basic level concepts which is a family of concepts frequently used by people in daily life. Taking cognitive psychology into consideration, we propose a method to generate ontologies with basic level concepts from folksonomies. Using Open Directory Project (ODP) as the benchmark, we demonstrate that the ontology generated by our method is reasonable and consistent with human thinking.}
}
@incollection{GALLISTEL199235,
title = {Classical Conditioning as an Adaptive Specialization: A Computational Model},
editor = {Douglas L. Medin},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {28},
pages = {35-67},
year = {1992},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(08)60487-9},
url = {https://www.sciencedirect.com/science/article/pii/S0079742108604879},
author = {C.R. Gallistel},
abstract = {Publisher Summary
This chapter analyzes the results of some modern classical conditioning experiments from the perspective of a computational model based on the assumption that the underlying learning process is specifically adapted to the domain of multivariate, nonstationary time series. It focuses on the quantitative results from experiments on the effects of partial reinforcement on the rate of acquisition and extinction because the other predictions of the model have been discussed and associative models are conspicuously unsuccessful at making quantitative predictions in this area. The model gives a mathematical characterization of the learning process from which one can derive the results of conditioning experiments. It is unlike these models in the sense that it is not in the associative tradition. The model replaces the associative explanatory framework with a framework that treats the conditioning process as a computational mechanism adapted through evolution to the peculiarities of one domain-a mechanism that solves one and only one of the several fundamentally distinct learning problems that confront mobile, multicellular organisms.}
}
@incollection{ROWLAND2003341,
title = {Chapter 16 - Interpreting Analytical Spectra with Evolutionary Computation},
editor = {Gary B. Fogel and David W. Corne},
booktitle = {Evolutionary Computation in Bioinformatics},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {341-365},
year = {2003},
series = {The Morgan Kaufmann Series in Artificial Intelligence},
isbn = {978-1-55860-797-2},
doi = {https://doi.org/10.1016/B978-155860797-2/50018-4},
url = {https://www.sciencedirect.com/science/article/pii/B9781558607972500184},
author = {Jem J. Rowland},
abstract = {Publisher Summary
This chapter deals with analytical techniques that are used to probe the activity and chemical makeup of cells. Metabolomics, the study of the entire biochemical constituents of a cell at any one time, is found to provide a rich means of monitoring organism activity. It can reveal explanations for different characteristics of seemingly similar organisms and can be used to relate function with gene. Spectroscopies are well suited to the study and interpretation of the metabolome in functional genomics. Another important technique in functional genomics is the measurement of gene expression via transcriptome arrays. This chapter outlines the various ways in which evolutionary computation (EC) can provide the basis for powerful tools for spectral interpretation and thus for functional genomics. It mentions various methods of forming predictive models from multivariate, often quasi-continuous data. It also discusses ways in which the effectiveness of such conventional techniques may be enhanced by combining them with evolutionary techniques.}
}
@incollection{TIRAPELLE20233465,
title = {Practical learning activities to increase the interest of university applicants in STEM careers in the era of Industry 4.0},
editor = {Antonios C. Kokossis and Michael C. Georgiadis and Efstratios Pistikopoulos},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {52},
pages = {3465-3470},
year = {2023},
booktitle = {33rd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-443-15274-0.50553-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443152740505539},
author = {Monica Tirapelle and Dian Ning Chia and Fanyi Duanmu and Konstantinos Katsoulas and Alberto Marchetto and Eva Sorensen},
keywords = {Engineering Education, Hands-on activities, Active learning, Orientation to university, PSE computational tools},
abstract = {Inspiring young students, especially young girls, about STEM disciplines is crucial to address the current shortage of engineers. Since the engineering skills that are required by graduates are evolving in line with technological progress, there is now an even stronger need for graduates with strong Process Systems Engineering skills. In this work, we describe an effective way to promote the chemical engineering curriculum, with particular emphasis on computational tools, to a group of Year 12 high school students during a one-week course in our department. The course was designed to engage students in active learning through interactive sessions and practical hands-on activities. Through the course, the students gained a better understanding of the importance of STEM subjects and, in particular, of the challenges and opportunities that engineers encounter in the era of Industry 4.0 with ever-increasing use of digitalization in process design and operation.}
}
@article{GALBUSERA2022103109,
title = {Game-based training in critical infrastructure protection and resilience},
journal = {International Journal of Disaster Risk Reduction},
volume = {78},
pages = {103109},
year = {2022},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2022.103109},
url = {https://www.sciencedirect.com/science/article/pii/S2212420922003284},
author = {Luca Galbusera and Monica Cardarilli and Marina {Gómez Lara} and Georgios Giannopoulos},
keywords = {Critical infrastructure, Resilience, Preparedness, Training, Exercises, Serious games, Gamification},
abstract = {Several institutions worldwide are reflecting on the relevance of training and exercises to critical infrastructure protection and resilience. This is witnessed, for instance, by Council Directive 2008/114/EC in the EU and the Homeland Security Exercise and Evaluation Program in the US. Contributing to the research actions in the field, the present article discusses methodological approaches, tools, techniques, and technologies relevant to this domain. In particular, we report on a recent training initiative elaborated by the authors and involving a game-based, modelling-and-simulation-backed, computer-assisted exercise for critical infrastructure expert audiences. This was developed taking advantage of JRC's Geospatial Risk and Resilience Assessment Platform (GRRASP) and critical infrastructure analysis methodologies integrated therein. The overarching objective was to enhance system thinking and raise awareness of resilience aspects while familiarizing participants with specific analysis tools and scientific models.}
}
@article{MININA2022104684,
title = {Neuron quantum computers and a way to unification of science: A compendium of Efim Liberman's scientific work},
journal = {Biosystems},
volume = {217},
pages = {104684},
year = {2022},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2022.104684},
url = {https://www.sciencedirect.com/science/article/pii/S0303264722000727},
author = {Svetlana V. Minina and Nikita E. Shklovskiy-Kordi},
keywords = {Efim liberman, cAMP, Biological computation, Biophysics, Chaimatics, Quantum biology, Unity of science, Quantum computation, Molecular cell computer, Quantum regulator},
abstract = {In 1972, Efim Liberman, a Soviet biophysicist, pioneered a brand-new approach to studying the operation of the brain, the live cell and the human mind by publishing a paper titled “Cell as a molecular computer” (1972). In this paper, Liberman posited that a consecutive/parallel stochastic molecular computer (MCC) controls a living cell. An MCC operates with molecule-words (DNA, RNA, proteins) according to the program recorded in DNA and RNA. Computational operations are implemented by molecular operators acting as enzymes. An MCC is present in each live cell. A neuron cell MCC can be involved in solving tasks for the entire organism. Neuron MCC investigation was started with studying an impact of an intracellular injection of cyclic AMP on electric activity of a neuron. Cyclic nucleotides were considered as input words for an MCC, which are generated inside a neuron as a result of synaptic activity. This led Efim Liberman to the idea that, in order to solve complex physical problems, which are encountered by a neuron and require rapid solutions, the molecular computer adjusts the operation of the quantum molecular regulator, which uses the “computational environment” of the cytoskeleton and quantum properties of the elementary hypersound quasiparticles for completing mathematical operations for the minimum price of action. Efim Liberman suggested that the human self-consciousness is a quantum computer of even a higher level and designated it as an extreme quantum regulator. In order to describe such systems, he suggested to join biology, physics and mathematics into a unified science, and formulated its four fundamental principles. Results of Efim Liberman’s theoretical and experimental studies on the topic of biological computation are summarized in this review.}
}
@article{MUGHAL2020159,
title = {Goals of the national mathematics curriculum of Pakistan: educators’ perceptions and challenges toward achievement},
journal = {International Journal of Educational Management},
volume = {35},
number = {1},
pages = {159-172},
year = {2020},
issn = {0951-354X},
doi = {https://doi.org/10.1108/IJEM-04-2020-0203},
url = {https://www.sciencedirect.com/science/article/pii/S0951354X20000678},
author = {Shahid Hussain Mughal and Muhammad Mujtaba Asad and Donnie Adams},
keywords = {Mathematics, Curriculum design, Pedagogy, Content knowledge, National plan},
abstract = {Purpose
The national mathematics curriculum of Pakistan has emphasized on improving content knowledge, reasoning abilities and problem-solving skills of students about thinking, communicating and solving mathematics (national mathematics curriculum of Pakistan, 2006). Whereas, there is a need to understand the point of view of teachers about the challenges they face in achieving the goals of national mathematics curriculum. This will help leading teacher training institutions to revisit their math teacher continuous professional development (CPD) programs and facilitate school leadership in improving the quality of math education in rural schools of the province. However, the purpose of this research study is to figure out the challenges that teachers are facing while achieving the goals of the national curriculum by teaching mathematics at the primary level in educational institutes of Pakistan.
Design/methodology/approach
In this research study qualitative research approaches have been utilized, in which focus group discussions (FGDs) were used as data collection techniques. Furthermore, thematic analysis of the data led toward the development of four overarching themes such as teachers' knowledge about mathematics curriculum, challenges relating to mathematics content and pedagogy, difficulties in developing conceptual understanding and designing lesson plans to address students' diversity.
Findings
The overall findings of this research study suggested that the majority of teachers are facing difficulties in mathematics content teaching such as decimal fraction, unitary method, measurement principles, practical geometry and data handling. Moreover, teachers are also facing challenges and difficulties in developing hands-on and minds-on activities in the teaching of mathematical concepts to the students of primary level in educational institutes of Pakistan.
Practical implications
This research study will facilitate the teachers and stakeholders to address the problematic issues in the domain of content delivery of mathematics. Whereas, this study recommends educating teachers about national mathematics curriculum and to develop a CPD framework for mathematics teachers for the enhancement of their pedagogical content knowledge. The study also recommends orientating school heads about the different aspects of math curriculum so that they can mentor math teachers in achieving math curriculum goals.
Originality/value
This is the first research study of its nature, which targets and highlights the teacher's perceptions toward the achieving the goals of national mathematics curriculum of Pakistan and addressing the pedagogical challenges faced in mathematics teachers. There is a dearth of studies in mathematics education in Sindh province. The issue is of immense importance, the findings will help teachers to improve mathematics instructions at primary level.}
}
@article{DO2020110730,
title = {Capturing creative requirements via requirements reuse: A machine learning-based approach},
journal = {Journal of Systems and Software},
volume = {170},
pages = {110730},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110730},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220301631},
author = {Quoc Anh Do and Tanmay Bhowmik and Gary L. Bradshaw},
keywords = {Requirements reuse, Requirements engineering, Creativity in RE, Boilerplate, Natural language processing, Machine learning},
abstract = {The software industry has become increasingly competitive as we see multiple software serving the same domain and striving for customers. To that end, modern software needs to provide creative features to improve sustainability. To advance software creativity, research has proposed several techniques, including multi-day workshops involving experienced requirements analysts, and semi-automated tools to support creative thinking in a limited scope. Such approaches are either useful only for software with already rich issue tracking systems, or require substantial engagement from analysts with creative minds. In a recent work, we have demonstrated a novel framework that is beneficial for both novel and existing software and allows end-to-end automation promoting creativity. The framework reuses requirements from similar software freely available online, utilizes advanced natural language processing and machine learning techniques, and leverages the concept of requirement boilerplate to generate candidate creative requirements. An application of our framework on software domains: Antivirus, Web Browser, and File Sharing followed by a human subject evaluation have shown promising results. In this invited extension, we present further analysis for our research questions and report an additional evaluation by human subjects. The results exhibit the framework’s ability in generating creative features even for a relatively matured application domain, such as Web Browser, and provoking creative thinking among developers irrespective of their experience levels.}
}
@article{ROSSITER202017604,
title = {Using interactive tools to facilitate student self-testing of dynamics and PI compensation},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {17604-17609},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2677},
url = {https://www.sciencedirect.com/science/article/pii/S240589632033439X},
author = {J.A. Rossiter},
keywords = {Virtual laboratories, staff efficiency, student engagement, independent learning},
abstract = {Virtual laboratories have become a common tool in recent years for supporting student learning and engagement. This paper presents a new tool for helping students self-assess their competence in basic dynamics for 1st and 2nd order systems alongside simple PI compensation techniques. The tools provide a supported environment for helping students work towards the correct answer by providing succinct feedback on incorrect responses and opportunities to try again, while displaying relevant information. A partner interactive tool is also provided which focuses solely on assessment with no feedback, so that students can assess their ability to get correct answers in a scenario that only the first attempt counts. This paper gives the thinking behind the tools, their coding and also accessibility for students.}
}
@article{ADRIAENSEN2023106294,
title = {Systems-theoretic interdependence analysis in robot-assisted warehouse management},
journal = {Safety Science},
volume = {168},
pages = {106294},
year = {2023},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2023.106294},
url = {https://www.sciencedirect.com/science/article/pii/S0925753523002369},
author = {Arie Adriaensen and Liliane Pintelon and Francesco Costantino and Giulio {Di Gravio} and Riccardo Patriarca},
keywords = {FRAM, Human-machine interaction, Industry 4.0, Industry 5.0, Cobots},
abstract = {The safe and efficient application of collaborative robots requires an understanding of actual work practices transformation, emerging from the adoption of new technological instruments. Functional systems-thinking is largely absent in literature about collaborative robot applications. In this context, this study proposes a framework that combines two safety analysis methods, being the Functional Resonance Analysis Method and Interdependence Analysis. Both safety and efficiency are examined by selected case study highlights to gain an in-depth understanding of human operators’ role as the central driver of human–machine (eco)systems in a warehouse distribution system, in which warehouse robot assistance is provided. Whereas the Functional Resonance Analysis Method first maps the work system interactions as a whole, Interdependence Analysis is subsequently applied to investigate individual inter-agent exchanges by the principles of Observability, Predictability, and Directability as a core principle for goal coordination between multiple agents, including warehouse robot agents. The case study examples reveal the combined effects of the working system environment and the robot application but also demonstrate possible operational solutions to deal with socio-technical complexity.}
}
@article{REIS2024103184,
title = {Machine learning methods in physical therapy: A scoping review of applications in clinical context},
journal = {Musculoskeletal Science and Practice},
volume = {74},
pages = {103184},
year = {2024},
issn = {2468-7812},
doi = {https://doi.org/10.1016/j.msksp.2024.103184},
url = {https://www.sciencedirect.com/science/article/pii/S2468781224002790},
author = {Felipe J.J. Reis and Matheus Bartholazzi Lugão de Carvalho and Gabriela de Assis Neves and Leandro Calazans Nogueira and Ney Meziat-Filho},
keywords = {Artificial intelligence, Computational intelligence, Machine intelligence, Computer reason, Physical therapy modalities},
abstract = {Background
Machine learning (ML) efficiently processes large datasets, showing promise in enhancing clinical practice within physical therapy.
Objective
The aim of this scoping review is to provide an overview of studies using ML approaches in clinical settings of physical therapy.
Data sources
A scoping review was performed in PubMed, EMBASE, PEDro, Cochrane, Web of Science, and Scopus.
Selection criteria
We included studies utilizing ML methods. ML was defined as the utilization of computational systems to encode patterns and relationships, enabling predictions or classifications with minimal human interference.
Data extraction and data synthesis
Data were extracted regarding methods, data types, performance metrics, and model availability.
Results
Forty-two studies were included. The majority were published after 2020 (n = 25). Fourteen studies (33.3%) were in the musculoskeletal physical therapy field, nine (21.4%) in neurological, and eight (19%) in sports physical therapy. We identified 44 different ML models, with random forest being the most used. Three studies reported on model availability. We identified several clinical applications for ML-based tools, including diagnosis (n = 14), prognosis (n = 7), treatment outcomes prediction (n = 7), clinical decision support (n = 5), movement analysis (n = 4), patient monitoring (n = 3), and personalized care plan (n = 2).
Limitation
Model performance metrics, costs, model interpretability, and explainability were not reported.
Conclusion
This scope review mapped the emerging landscape of machine learning applications in physical therapy. Despite the growing interest, the field still lacks high-quality studies on validation, model availability, and acceptability to advance from research to clinical practice.}
}
@article{XUE2024108224,
title = {Interaction dynamics of social support expressions predict future support-seeking behaviors in online support groups},
journal = {Computers in Human Behavior},
volume = {156},
pages = {108224},
year = {2024},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2024.108224},
url = {https://www.sciencedirect.com/science/article/pii/S074756322400092X},
author = {Haoning Xue and Wang Liao and Jingwen Zhang},
keywords = {Compensation, Computational methods, Interaction dynamics, Online support groups, Reciprocity, Support-seeking},
abstract = {Maintaining the sustainability of online support groups (OSGs) presents a significant challenge. Integrating the literature on interaction dynamics and supportive communication, this study investigated how interaction dynamics in supportive communication foster long-term support-seeking behaviors that are crucial to sustaining continuous support exchanges in OSGs. Using a large-scale dataset of 48,868 posts and 468,243 comments over ten years from an OSG, this study examined how reciprocity and compensation of emotional and informational support, signaled by emotional expressions and analytical expressions, predicted a poster's future support-seeking behaviors in the OSG. Results showed that a poster's future support-seeking behaviors were positively associated with receiving (a) reciprocity of analytical expressions and (b) compensation of negative emotional expressions with positive emotional expressions in their past posts. However, reciprocity of negative emotional expressions was negatively associated with a poster's future support-seeking behaviors. This study emphasizes social support as an ongoing interactive process and its importance in motivating support-seeking behaviors and fostering a thriving OSG.}
}
@article{BROWN201511,
title = {On unifiers, diversifiers, and the nature of pattern recognition},
journal = {Pattern Recognition Letters},
volume = {64},
pages = {11-20},
year = {2015},
note = {Philosophical Aspects of Pattern Recognition},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2015.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167865515001312},
author = {Gavin Brown},
keywords = {Nature of pattern recognition, Unifying, Diversifying, Dyson},
abstract = {We study a dichotomy of scientific styles, unifying and diversifying, as proposed by Freeman J. Dyson. We discuss the extent to which the dichotomy transfers from the natural sciences (where Dyson proposed it) to the field of Pattern Recognition. To address this we must firstly ask what it means to be a “unifier” or “diversifier” in a field, and what are the relative merits of each style of thinking. Secondly, given that Dyson applied this to the sciences, does it also apply in a field known to be a blend of science and engineering? Parallels are drawn to Platonic/Aristotelian views, and to Cartesian/Baconian science, and questions are asked on what drives the Kuhnian paradigm shifts of our field. This article is intended not to marginalise individuals into categories (unifier/diversifier) but instead to demonstrate the utility of philosophical reflection on our field, showing the depth and complexities a seemingly simple idea can unearth.}
}
@article{OFFENHUBER2023264,
title = {Reconsidering Representation in College Design Curricula},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {9},
number = {2},
pages = {264-282},
year = {2023},
note = {The Future of Design Education: Rethinking Design Education for the 21st Century},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2023.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S2405872623000394},
author = {Dietmar Offenhuber and Joy Mountford},
keywords = {Representation, Data, Models, Maps, Visualization, Sensory modalities},
abstract = {The Future of Design Education working group on representation addressed the roles of data, maps, models, and interfaces as a continuum from representation to action. The article traces historical ideas of representation grounded by a linguistic paradigm to more recent approaches based on performance, embodiment, and sensory modalities other than vision. Discussions include the use of representations in the design process. Designers are able to use traditional forms of representation in the design of artifacts, such as sketches. These forms of representation are not sufficient for the design of systems. System design requires models that allow stakeholders to negotiate their view of a situation and design teams to iterate how things might work. Core ideas in the working group recommendations address issues of, substitution, formal rules, motivation, context dependency, materiality, provisionality, latency, performance, externalization, facilitation and negotiation, mediation, and measurement and evaluation. Discussions address the socio-political implications of representation and the expanding role of computing and data that call for a systems view.}
}
@article{NEVES2009834,
title = {Structuring an MCDA model using SSM: A case study in energy efficiency},
journal = {European Journal of Operational Research},
volume = {199},
number = {3},
pages = {834-845},
year = {2009},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2009.01.053},
url = {https://www.sciencedirect.com/science/article/pii/S0377221709002033},
author = {L.P. Neves and L.C. Dias and C.H. Antunes and A.G. Martins},
keywords = {Problem structuring methods, Multiple criteria analysis, SSM, Value Focused Thinking, Energy efficiency},
abstract = {This work presents the use of a problem structuring method, Soft Systems Methodology (SSM), to structure a Multi-Criteria Decision Analysis (MCDA) model, aimed at appraising energy efficiency initiatives. SSM was useful to help defining clearly the decision problem context and the main actors involved, as well as to unveil the relevant objectives for each stakeholder. Keeney’s Value Focused Thinking approach was then used to refine and structure the list of objectives according to the perspective of the main evaluators identified. In addition to describing this particular case study, this paper aims at providing some general guidelines on how SSM may facilitate the emergence of objectives for MCDA models.}
}
@article{ROBERTSON2009136,
title = {Impact of CAD tools on creative problem solving in engineering design},
journal = {Computer-Aided Design},
volume = {41},
number = {3},
pages = {136-146},
year = {2009},
note = {Computer Support for Conceptual Design},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2008.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0010448508001334},
author = {B.F. Robertson and D.F. Radcliffe},
keywords = {CAD, Creativity, Conceptual design},
abstract = {This paper presents the results of a survey of CAD users that examined the ways in which their computational environment may influence their ability to design creatively. This extensive online survey builds upon the findings of an earlier observational case study of the use of computer tools by a small engineering team. The case study was conducted during the conceptual and detailed stages of the design of a first-to-world product. Four mechanisms by which CAD tools may influence the creative problem solving process were investigated: enhanced visualisation and communication, circumscribed thinking, premature design fixation and bounded ideation. The prevalence of these mechanisms was examined via a series of questions that probed the user’s mode of working, attitudes, and responses to hypothetical situations. The survey showed good support for the first three mechanisms and moderate support for the fourth. The results have important implications for both the users and designers of CAD tools.}
}
@incollection{MARTIGNON2001382,
title = {Algorithms},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {382-385},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/00549-0},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767005490},
author = {L. Martignon},
abstract = {The concept of algorithm is central to the modern view of a thinking machine, be it the human mind or the modern computer. An algorithm is a well-defined mathematical recipe for the solution of a well-defined task. It is presented as a finite set of steps or instructions that can be applied to unlimited sets of possibilities. There is a clear-cut rule for the operation to be performed at each step, as well as a clear-cut specification of the conditions under which to terminate the process. An algorithm may contain loops, that is, there may be steps that return to previous steps. Algorithms can be sequential or parallel. An algorithm that produces a ‘yes’ or ‘no’ answer is, decision algorithm. An algorithm that constructs or determines a specific solution to a given problem is a computation algorithm.}
}
@article{NYSTROM201077,
title = {Ontological musings on how nature computes},
journal = {Procedia Computer Science},
volume = {1},
number = {1},
pages = {77-86},
year = {2010},
note = {ICCS 2010},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2010.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S1877050910000116},
author = {J.F. Nystrom},
keywords = {Universe as computation, Quantum vacuum, Computational cosmography},
abstract = {Modern physical theory and modern computational techniques are used to provide conjecture on how nature computes. I utilize time-domain simulation of physical phenomena and build analogies between elements of computation and the “things” of Universe computation, resulting, for example, in the identification of the quantum vacuum as the power source for Universe computation. While reviewing how Universe can be viewed as a computation, we find the need for Negative Universe (which is a part of the quantum vacuum mechanism). This idea is compared with Penrose’s current model which utilizes a separate Platonic world outside of physical Universe. Lastly, in the Discussion, I present an updated version of computational cosmography as a model for Universe as computation.}
}
@article{OLIVIER2024103956,
title = {DynBioSketch: A tool for sketching dynamic visual summaries in biology, and its application to infection phenomena},
journal = {Computers & Graphics},
volume = {122},
pages = {103956},
year = {2024},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2024.103956},
url = {https://www.sciencedirect.com/science/article/pii/S0097849324000918},
author = {Pauline Olivier and Tara Butler and Pascal Guehl and Jean-Luc Coll and Renaud Chabrier and Pooran Memari and Marie-Paule Cani},
keywords = {Sketch-based modeling, Interactive geometric modeling, Sketch-based animation, Narration},
abstract = {Having simple methods of illustration is essential to scientific thinking. To complement the abstract sketches regularly used in cell biology, we propose DynBioSketch, an easy-to-use digital modeling and animation tool, enabling biologists to resort to less simplified representations when necessary without having to call professional artists. DynBioSketch is an interactive sketching system dedicated to the design and communication of biological phenomena at the cellular scale that can be illustrated in a few minutes of animation. Our model integrates 3D modeling, pattern-based design of 3D shape distributions, and sketch-based animation. These elements can be combined to create complex scenarios such as the infection phenomenon on which we focus, allowing a narrative design adapted to communication between researchers or educational applications in biology. Our results, along with a user study conducted with biology researchers, highlight the potential of DynBioSketch in enabling the direct design of dynamic visual summaries that convey relevant information, as shown in our infection case study. By bridging the gap between abstract representations used by experts and more illustrative depictions, DynBioSketch opens a new avenue for communicating biological concepts.}
}
@article{WANG2023120782,
title = {A closed-loop analysis approach for ensuring stormwater source control design solution to achieve the intended goals},
journal = {Water Research},
volume = {247},
pages = {120782},
year = {2023},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2023.120782},
url = {https://www.sciencedirect.com/science/article/pii/S0043135423012228},
author = {Sheng Wang and Lidan Feng and Yezi Yuan},
keywords = {Sponge city, Stormwater source control, Closed-loop analysis, Bioretention, Runoff frequency spectrum},
abstract = {Stormwater source controls have been adopted worldwide to address hydrological and environmental impairments caused by the spread of impervious surfaces in cities. Current design method in China uses 30-year daily rainfall records to generate relationship of rainfall volume capture ratio (αg) and daily design storm, and then uses design storm to propose design solution. However, source control performance differs from rain to rain, and hence the design solution's actual effect may deviate from αg. Borrowing closed-loop feedback concept from business domain, this study proposes closed-loop analysis (CLA) which uses design solution's 30-year simulated result as data feedback to check design solution's effectiveness and then make improvements if necessary. It consists of four methods: 1) hourly design storm statistical method, for addressing the weakness of current daily design storm; 2) design solution model credibility examination method, for guaranteeing credibility of 30-year simulated results for CLA; 3) appropriate design storms determination method for source control without underdrain; 4) additional design parameters optimization method for source control with underdrain. Taking Xiamen city for example, case study results shows that design solution's 30-year simulated results were consistent/comparable with sizing calculation formula that was used to propose design solution, and therefore they were credible for CLA. Appropriate design storms ensured design solutions without underdrain to achieve the intended αg±3 %. Optimal design parameters combinations ensured design solutions with underdrain to achieve αg but also restore natural runoff events with pre- and post-development runoff frequency spectra similarity being 0.670–0.691. Based on stormwater mathematical model, CLA can drive source control design computation to a new methodological stage.}
}
@article{PIVIK2012548,
title = {Eating breakfast enhances the efficiency of neural networks engaged during mental arithmetic in school-aged children},
journal = {Physiology & Behavior},
volume = {106},
number = {4},
pages = {548-555},
year = {2012},
issn = {0031-9384},
doi = {https://doi.org/10.1016/j.physbeh.2012.03.034},
url = {https://www.sciencedirect.com/science/article/pii/S0031938412001394},
author = {R.T. Pivik and Kevin B. Tennal and Stephen D. Chapman and Yuyuan Gu},
keywords = {Morning nutrition, Mental arithmetic, Preadolescents, Time–frequency analysis},
abstract = {To determine the influence of a morning meal on complex mental functions in children (8–11y), time–frequency analyses were applied to electroencephalographic (EEG) activity recorded while children solved simple addition problems after an overnight fast and again after having either eaten or skipped breakfast. Power of low frequency EEG activity [2Hertz (Hz) bands in the 2–12Hz range] was determined from recordings over frontal and parietal brain regions associated with mathematical thinking during mental calculation of correctly answered problems. Analyses were adjusted for background variables known to influence or reflect the development of mathematical skills, i.e., age and measures of math competence and math fluency. Relative to fed children, those who continued to fast showed greater power increases in upper theta (6–8Hz) and both alpha bands (8–10Hz; 10–12Hz) across sites. Increased theta suggests greater demands on working memory. Increased alpha may facilitate task-essential activity by suppressing non-task-essential activity. Fasting children also had greater delta (2–4Hz) and greater lower-theta (4–6Hz) power in left frontal recordings—indicating a region-specific emphasis on both working memory for mental calculation (theta) and activation of processes that suppress interfering activity (delta). Fed children also showed a significant increase in correct responses while children who continued to fast did not. Taken together the findings suggest that neural network activity involved in processing numerical information is functionally enhanced and performance is improved in children who have eaten breakfast, whereas greater mental effort is required for this mathematical thinking in children who skip breakfast.}
}
@article{BIRJALI201765,
title = {Machine Learning and Semantic Sentiment Analysis based Algorithms for Suicide Sentiment Prediction in Social Networks},
journal = {Procedia Computer Science},
volume = {113},
pages = {65-72},
year = {2017},
note = {The 8th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2017) / The 7th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2017) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.08.290},
url = {https://www.sciencedirect.com/science/article/pii/S187705091731699X},
author = {Marouane Birjali and Abderrahim Beni-Hssane and Mohammed Erritali},
keywords = {Sentiment Analysis, Machine Learning, Suicide, Social Networks, Tweets, Semantic Sentiment Analysis},
abstract = {Sentiment analysis is one of the new challenges appeared in automatic language processing with the advent of social networks. Taking advantage of the amount of information is now available, research and industry have sought ways to automatically analyze sentiments and user opinions expressed in social networks. In this paper, we place ourselves in a difficult context, on the sentiments that could thinking of suicide. In particular, we propose to address the lack of terminological resources related to suicide by a method of constructing a vocabulary associated with suicide. We then propose, for a better analysis, to investigate Weka as a tool of data mining based on machine learning algorithms that can extract useful information from Twitter data collected by Twitter4J. Therefore, an algorithm of computing semantic analysis between tweets in training set and tweets in data set based on WordNet is proposed. Experimental results demonstrate that our method based on machine learning algorithms and semantic sentiment analysis can extract predictions of suicidal ideation using Twitter Data. In addition, this work verify the effectiveness of performance in term of accuracy and precision on semantic sentiment analysis that could thinking of suicide.}
}