@article{GUHA2023100014,
title = {Cultural mapping and SARS-CoV-2 vaccination: An ethnic perspective},
journal = {Societal Impacts},
volume = {1},
number = {1},
pages = {100014},
year = {2023},
issn = {2949-6977},
doi = {https://doi.org/10.1016/j.socimp.2023.100014},
url = {https://www.sciencedirect.com/science/article/pii/S2949697723000140},
author = {Soumya Kanti Guha and Sougata Niyogi},
keywords = {COVID-19, Culture, Vaccination, Immunization, Global},
abstract = {The COVID-19 pandemic has forced a comprehensive vaccination program that spans across social, geographical, and political boundaries. Our study aims to evaluate the program's effectiveness and explore the social factors that can influence its success globally. By analyzing data on immunization coverage and COVID-19 vaccination status for 196 and 187 countries, respectively, and taking into account the cultural values of 110 countries, we identified a correlation between vaccination success and societal openness to change. Our findings suggest that a history of good immunization coverage alone may not guarantee successful vaccine rollout; an optimistic ecosystem is also necessary. People must be willing to accept and apply the solution, and the rollout must address their psychological need for 360-degree success. Our study thus highlights the importance of understanding the social outlook to calibrate the attainment scale of global vaccination programs.}
}
@article{DELIMA2023100590,
title = {Managing the plot structure of character-based interactive narratives in games},
journal = {Entertainment Computing},
volume = {47},
pages = {100590},
year = {2023},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2023.100590},
url = {https://www.sciencedirect.com/science/article/pii/S1875952123000459},
author = {Edirlei Soares {de Lima} and Bruno Feijó and Antonio L. Furtado},
keywords = {Interactive storytelling, Narrative generation, Drama management, Plot structure, Automated planning},
abstract = {The use of narrative generation methods in games is a complex challenge that involves multiple problems of plot-based processes integrated with character-based methods. Examples of these problems are the high computational complexity of many story generation algorithms, the difficulties associated with the generation of interactive narratives that are compelling and emotionally impactful, the complex interactions among characters, and the need for tools and methods to support story writers in the process of creating and managing the narrative structure of interactive stories. In this work, we present and evaluate a new approach to generate and manage the plot structure of character-based interactive narratives in games, which combines multi-agent planning with a drama management strategy based on narrative structures. The proposed method is supported by an authoring tool that allows authors to create and test interactive narratives using graphical interfaces and intuitive diagrams. The results of our study suggest the effectiveness of our approach in generating interactive narratives for highly interactive game environments. In addition, a user study of the proposed authoring tool indicates that it can successfully support the development of character-based interactive narratives without requiring programming knowledge.}
}
@article{SAKAI2024100064,
title = {Human divergent exploration capacity for material design: A comparison with artificial intelligence},
journal = {Computers in Human Behavior: Artificial Humans},
volume = {2},
number = {1},
pages = {100064},
year = {2024},
issn = {2949-8821},
doi = {https://doi.org/10.1016/j.chbah.2024.100064},
url = {https://www.sciencedirect.com/science/article/pii/S2949882124000240},
author = {Hiroyuki Sakai and Kenroh Matsuda and Nobuaki Kikkawa and Seiji Kajita},
keywords = {Creativity, Divergent thinking, Lubricant molecule, Collaborative intelligence},
abstract = {Applications of artificial intelligence (AI) to material design have attracted increasing attention in recent years. Although AI-aided material design holds great promise for some applications, whether it has surpassed human creativity remains uncertain. The aim of the current study was to compare the divergent exploration capacity of AI with that of humans on a material design task. Human participants were asked to find a high-performance lubricant molecule under searching conditions comparable to a state-of-the-art AI system. Results indicated that, on average, AI was able to find significantly better lubricant molecules. However, the best molecule found by AI fell short of the best molecule found by a human participant. Furthermore, the structural characteristics of the molecules found by AI and human participants differed significantly. These findings suggest that a state-of-the-art AI system is capable of surpassing human divergent exploration capacity in material design, as in other fields in which AI has advanced. Nevertheless, our results also demonstrate that human intelligence and AI can play complementary roles in covering a broader search space. This investigation opens up new possibilities for collaborative systems involving both AI agents and humans in material design.}
}
@article{LATIF2023726,
title = {Design and Development a Virtual Planetarium Learning Media Using Augmented Reality},
journal = {Procedia Computer Science},
volume = {227},
pages = {726-733},
year = {2023},
note = {8th International Conference on Computer Science and Computational Intelligence (ICCSCI 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.577},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923017441},
author = {Jazlyn Jan Keyla Latif and Augustinus Adrian Triputra and Michael Awarsa Kesuma and Fairuz Iqbal Maulana},
keywords = {Augmented Reality, Planetarium, Virtual, Virtual Planetarium, Application},
abstract = {The solar system has always been a mystery to many. If not for the advanced technologies, most humans would not have the opportunity to gain knowledge about the planets. Although Earth is a part of the solar system, the solar system is simply too dangerous and expensive for humans to explore casually. Humans do not interact with the Sun or planets actively. This is especially concerning for children who often require visual aids in studying. An Augmented Reality (AR) based application can solve that problem. Through Virtual Planetarium, children may interact with the Sun or the planets and gain information. This will help aid children's guardians in studying the solar system. The application is made by Systems Development Life Cycle (SDLC) method. Through the making of this application, it is expected that children will have a better understanding of the solar system.}
}
@incollection{BLACKBURN19941,
title = {1 - Structures, Languages and Translations: the Structural Approach to Feature Logic},
editor = {C.J. Rupp and M.A. Rosner and R.L. Johnson},
booktitle = {Constraints, Language and Computation},
publisher = {Academic Press},
address = {San Diego},
pages = {1-27},
year = {1994},
series = {Cognitive Science},
isbn = {978-0-08-050296-0},
doi = {https://doi.org/10.1016/B978-0-08-050296-0.50008-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780080502960500085},
author = {Patrick Blackburn},
abstract = {Publisher Summary
This chapter reviews methodological issues involved in the structural approach used in feature logic. A direct consequence of the systematic approach to a variety of feature logics is that it clarifies the relationships between them. This is most explicit in the presentation of translations between various existing and putative feature logics which draws heavily on the correspondence theory that relates modal and classical languages. The chapter describes a general approach to the subject called the structural approach, and explains that thinking in structural terms is a useful way of thinking about unification formalisms and their interrelationships. At present, in contemporary unification-based linguistic frameworks, linguistic data is modelled by certain kinds of decorated labelled (directed) graphs. Perhaps the most prevalent way of thinking about unification-based grammar formalisms is that they are languages for expressing constraints on feature structures. Two basic ideas drive modal logic: one syntactic, and the other semantic. Modal languages are interpreted on Kripke models, which are set theoretic entities providing the following information. Propositional Dynamic Logic (PDL) is an extension of modal logic; PDL and some of its extensions are natural constraint languages for dealing with feature structures. Subsequently, Attribute Value Matrices (AVMs) are one of the most widely used methods of describing feature structures. A general setting for feature logic is the space of relational structures of model theory, together with the various languages for describing these structures, and the satisfiability preserving translations that exist among these languages. The basic ideas are very simple: feature structures are certain sorts of relational structures, and while there is a vast range of languages for talking about these structures, these languages are interrelated by satisfiability preserving translations.}
}
@article{AKBAR2023114290,
title = {Investigation of structural, electronic, and optical properties of zintl phase of Ba3In2As4: A DFT study for optoelectronic application},
journal = {Computational and Theoretical Chemistry},
volume = {1228},
pages = {114290},
year = {2023},
issn = {2210-271X},
doi = {https://doi.org/10.1016/j.comptc.2023.114290},
url = {https://www.sciencedirect.com/science/article/pii/S2210271X23002724},
author = {Seher Akbar and Muhammad Usman and Jalil {Ur Rehman} and M. {Bilal Tahir} and Altaf Hussain},
keywords = {Zintl phase, CASTEP code, BaInAs, Narrow band gap},
abstract = {The physical properties of Zintl phase of Ba3In2As4 are investigated by employing first-principles calculation. The optimized lattice parameters are found as, a = 13.79 Å, b = 11.09 Å, and c = 7.20 Å. The material possesses a band gap of 0.66 and 1.58 eV as investigated by using GGA-PBE and HSE06 functional, respectively. Both functional show that the material possesses the direct band gap nature. The density of states was investigated to identify the orbital participation in the valence and conduction band. The PDOS shows that the contribution of the p-orbital is greater than s and d-orbitals. The optical properties of the material are also investigated in detail. The reflectivity and the refractive index have been observed to have a static value of 0.36 and 4.03, respectively. The material can be utilized in optoelectronic devices due to the direct band gap nature.}
}
@article{VARDOULI2015137,
title = {Making use: Attitudes to human-artifact engagements},
journal = {Design Studies},
volume = {41},
pages = {137-161},
year = {2015},
note = {Special Issue: Computational Making},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2015.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X15000563},
author = {Theodora Vardouli},
keywords = {design theory, philosophy of design, user behavior, function theory, computational models},
abstract = {‘Function’ and ‘use’ are keywords that design researchers customarily employ when referring to human-artifact engagements. However, there is little consensus about how the concepts of function and use relate to each other, to the intentions of ‘designers’ and ‘users’, or to their actions and encompassing contexts. In this paper, I synthesize literature from design research, material culture studies, design anthropology, and function theory in order to critically compare different attitudes to human-artifact engagements, implicit in characterizations of function and use. I identify design-centric, communicative, and use-centric attitudes, and discuss their assumptions and implications for design theory. I conclude by outlining principles for theoretically and computationally approaching use as an embodied and temporally contingent process – as a form of ‘making’.}
}
@article{WU2023101906,
title = {Human–machine hybrid intelligence for the generation of car frontal forms},
journal = {Advanced Engineering Informatics},
volume = {55},
pages = {101906},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.101906},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623000344},
author = {Yu Wu and Lisha Ma and Xiaofang Yuan and Qingnan Li},
keywords = {Car frontal form, Creative generation, Human–machine hybrid intelligence, Human–machine shared knowledge base, generative adversarial network (GAN)},
abstract = {With the acceleration of the upgrading of the automobile consumption market, artificial intelligence has become an increasingly effective means of enhancing the creative design of automobile appearance modeling. However, when artificial intelligence processes specific design tasks, creativity is primarily based on data drive, resulting in machine-generated design schemes that do not match human-specific psychological intentions. Due to the absence of design knowledge in the process of machine design, there is a data gap between human cognitive thought and machine information processing. This paper aims to structure the human's complex cognitive knowledge of car frontal form, establish the consistency between human and machine cognitive structures, and reduce communication barriers in the process of human–machine hybrid creative design. To achieve this objective, a human–machine hybrid intelligence methodology – a combination of human cognitive mental model, human–machine shared knowledge base, and Generative Adversarial Networks (GAN) – was developed to generate a large number of car frontal forms that are consistent with the design intent. First, we constructeda mental model of human cognition based on three dimensions: design intent, drawing behavior, and functional structure. Second, we created a shared human–machine knowledge base with design Knowledge. This knowledge base contains 12,560 images of car frontal form designs with corresponding morphological semantic labels and 3,140 sketches of car frontal forms drawn by hand. Human–machine shared knowledge base datawasutilized in a machine learning training network. In addition, a conditional cross-domain generative adversarial network was developed to investigate the implicit relationship between sketch characteristics, morphological semantics, and image visual effects. Using the suggested method, a large number of images with the specified morphological semantic category and resembling the hand-drawn sketch of a car frontal form can be generated rapidly. In terms of the quality of car frontal form generation, our research is superior to the baseline model according to qualitative and quantitative assessments. In comparison to the designer's output, the human–machine hybrid intelligent generation also demonstrates excellent creative performance.}
}
@article{NYBERG2022394,
title = {Spatial goal coding in the hippocampal formation},
journal = {Neuron},
volume = {110},
number = {3},
pages = {394-422},
year = {2022},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2021.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S0896627321010291},
author = {Nils Nyberg and Éléonore Duvelle and Caswell Barry and Hugo J. Spiers},
keywords = {hippocampus, entorhinal cortex, navigation, goal, wayfinding, spatial memory, reinforcement learning, rodent, human},
abstract = {Summary
The mammalian hippocampal formation contains several distinct populations of neurons involved in representing self-position and orientation. These neurons, which include place, grid, head direction, and boundary cells, are thought to collectively instantiate cognitive maps supporting flexible navigation. However, to flexibly navigate, it is necessary to also maintain internal representations of goal locations, such that goal-directed routes can be planned and executed. Although it has remained unclear how the mammalian brain represents goal locations, multiple neural candidates have recently been uncovered during different phases of navigation. For example, during planning, sequential activation of spatial cells may enable simulation of future routes toward the goal. During travel, modulation of spatial cells by the prospective route, or by distance and direction to the goal, may allow maintenance of route and goal-location information, supporting navigation on an ongoing basis. As the goal is approached, an increased activation of spatial cells may enable the goal location to become distinctly represented within cognitive maps, aiding goal localization. Lastly, after arrival at the goal, sequential activation of spatial cells may represent the just-taken route, enabling route learning and evaluation. Here, we review and synthesize these and other evidence for goal coding in mammalian brains, relate the experimental findings to predictions from computational models, and discuss outstanding questions and future challenges.}
}
@article{BAHK2013298,
title = {Analytical investigation of tooth profile modification effects on planetary gear dynamics},
journal = {Mechanism and Machine Theory},
volume = {70},
pages = {298-319},
year = {2013},
issn = {0094-114X},
doi = {https://doi.org/10.1016/j.mechmachtheory.2013.07.018},
url = {https://www.sciencedirect.com/science/article/pii/S0094114X13001584},
author = {Cheon-Jae Bahk and Robert G. Parker},
keywords = {Tooth profile modification, Planetary gear, Vibration, Nonlinear, Perturbation method},
abstract = {This study investigates the impact of tooth profile modification on spur planetary gear vibration. An analytical model is proposed to capture the excitation from tooth profile modifications at the sun–planet and ring–planet meshes. The accuracy of the proposed model for dynamic analysis is correlated against a benchmark finite element analysis. Perturbation analysis yields a closed-form approximation of the vibration response with tooth profile modifications. The perturbation solution is used to investigate the effects of tooth profile modification. The tooth profile modification parameters that minimize response are readily obtained. Static transmission error and dynamic response are minimized at different amounts of profile modification, which contradicts common practical thinking regarding the correlation between static transmission error and dynamic response. Contrary to expectations, the optimal sun–planet and ring–planet tooth profile modifications that minimize response when applied individually may increase dynamic response when applied simultaneously. System parameters such as mesh stiffness and mesh phase significantly affect the influence of tooth profile modification.}
}
@article{ARGYROUDIS2022100387,
title = {Digital technologies can enhance climate resilience of critical infrastructure},
journal = {Climate Risk Management},
volume = {35},
pages = {100387},
year = {2022},
issn = {2212-0963},
doi = {https://doi.org/10.1016/j.crm.2021.100387},
url = {https://www.sciencedirect.com/science/article/pii/S2212096321001169},
author = {Sotirios A. Argyroudis and Stergios Aristoteles Mitoulis and Eleni Chatzi and Jack W. Baker and Ioannis Brilakis and Konstantinos Gkoumas and Michalis Vousdoukas and William Hynes and Savina Carluccio and Oceane Keou and Dan M. Frangopol and Igor Linkov},
keywords = {Emerging digital technologies, Data-driven, Critical infrastructure, Climate change, Sustainable development goals (SDGs)},
abstract = {Delivering infrastructure, resilient to multiple natural hazards and climate change, is fundamental to continued economic prosperity and social coherence. This is a strategic priority of the United Nations Sustainable Development Goals (SDGs), the World Bank, the Organisation for Economic Co-operation and Development (OECD), public policies and global initiatives. The operability and functionality of critical infrastructure are continuously challenged by multiple stressors, increasing demands and ageing, whilst their interconnectedness and dependencies pose additional challenges. Emerging and disruptive digital technologies have the potential to enhance climate resilience of critical infrastructure, by providing rapid and accurate assessment of asset condition and support decision-making and adaptation. In this pursuit, it is imperative to adopt multidisciplinary roadmaps and deploy computational, communication and other digital technologies, tools and monitoring systems. Nevertheless, the potential of these emerging technologies remains largely unexploited, as there is a lack of consensus, integrated approaches and legislation in support of their use. In this perspective paper, we discuss the main challenges and enablers of climate-resilient infrastructure and we identify how available roadmaps, tools and emerging digital technologies, e.g. Internet of Things, digital twins, point clouds, Artificial Intelligence, Building Information Modelling, can be placed at the service of a safer world. We show how digital technologies will lead to infrastructure of enhanced resilience, by delivering efficient and reliable decision-making, in a proactive and/or reactive manner, prior, during and after hazard occurrences. In this respect, we discuss how emerging technologies significantly reduce the uncertainties in all phases of infrastructure resilience evaluations. Thus, building climate-resilient infrastructure, aided by digital technologies, will underpin critical activities globally, contribute to Net Zero target and hence safeguard our societies and economies. To achieve this we set an agenda, which is aligned with the relevant SDGs and highlights the urgent need to deliver holistic and inclusive standards and legislation, supported by coordinated alliances, to fully utilise emerging digital technologies.}
}
@article{RAJPUT2021104270,
title = {VLSI implementation of transcendental function hyperbolic tangent for deep neural network accelerators},
journal = {Microprocessors and Microsystems},
volume = {84},
pages = {104270},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2021.104270},
url = {https://www.sciencedirect.com/science/article/pii/S014193312100435X},
author = {Gunjan Rajput and Gopal Raut and Mahesh Chandra and Santosh Kumar Vishvakarma},
keywords = {Activation function, Artificial neural network, Hyperbolic tangent (tanh), Digital implementation, Combinational logic},
abstract = {Extensive use of neural network applications prompted researchers to customize a design to speed up their computation based on ASIC implementation. The choice of activation function (AF) in a neural network is an essential requirement. Accurate design architecture of an AF in a digital network faces various challenges as these AF require more hardware resources because of its non-linear nature. This paper proposed an efficient approximation scheme for hyperbolic tangent (tanh) function which purely based on combinational design architecture. The approximation is based on mathematical analysis by considering maximum allowable error in a neural network. The results prove that the proposed combinational design of an AF is efficient in terms of area, power and delay with negligible accuracy loss on MNIST and CIFAR-10 benchmark datasets. Post synthesis results show that the proposed design area is reduced by 66% and delay is reduced by nearly 16% compared to state-of-the-art.}
}
@article{ASAHIRO202016,
title = {Graph orientation with splits},
journal = {Theoretical Computer Science},
volume = {844},
pages = {16-25},
year = {2020},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2020.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S030439752030387X},
author = {Yuichi Asahiro and Jesper Jansson and Eiji Miyano and Hesam Nikpey and Hirotaka Ono},
keywords = {Graph orientation, Maximum flow, Vertex cover, Partition, Algorithm, Computational complexity},
abstract = {The Minimum Maximum Outdegree Problem (MMO) is to assign a direction to every edge in an input undirected, edge-weighted graph so that the maximum weighted outdegree taken over all vertices becomes as small as possible. In this paper, we introduce a new variant of MMO called the p-Split Minimum Maximum Outdegree Problem (p-Split-MMO) in which one is allowed to perform a sequence of p split operations on the vertices before orienting the edges, for some specified non-negative integer p, and study its computational complexity.}
}
@incollection{CANAVERO202221,
title = {Chapter 2 - Cerebral: surface},
editor = {Jeffrey E. Arle and Jay L. Shils},
booktitle = {Essential Neuromodulation (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {21-48},
year = {2022},
isbn = {978-0-12-817000-7},
doi = {https://doi.org/10.1016/B978-0-12-817000-7.00002-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128170007000028},
author = {Sergio Canavero},
keywords = {Cortical Stimulation, Functional Neurosurgery, Pain, Psychiatry, TDCS, TMS},
abstract = {Cortical stimulation (CS) is a recent addition to electrical stimulation of the nervous system for therapeutic purposes. It can be administered both invasively (functional neurosurgery) or noninvasively mainly via transcranial magnetic stimulation or transcranial direct current stimulation. The primary indications for CS include neuropathic pain, and psychiatric disorders such as depression, but movement disorders, tinnitus, epilepsy, and coma rehabilitation are part of its purview. Neurosurgically implanted stimulators appear to achieve better results on a larger number of patients, but noninvasive stimulation has clear advantages in terms of safety and cost. Mixed results can be chalked up to individual anatomical and connectomic variations: the cortex is by far more complex than other neural, deeper targets, and current computational modeling efforts are underway to improve results. The next generation of CS will come with closed-loop capability and newer electrodes, in addition to refinements in personalized neuroimaging-guided targeting.}
}
@article{MCQUEEN2021120575,
title = {Do we really understand how drug eluted from stents modulates arterial healing?},
journal = {International Journal of Pharmaceutics},
volume = {601},
pages = {120575},
year = {2021},
issn = {0378-5173},
doi = {https://doi.org/10.1016/j.ijpharm.2021.120575},
url = {https://www.sciencedirect.com/science/article/pii/S037851732100380X},
author = {Alistair McQueen and Javier Escuer and Ankush Aggarwal and Simon Kennedy and Christopher McCormick and Keith Oldroyd and Sean McGinty},
keywords = {Pharmacodynamics, Ligand-receptor interactions, Drug-eluting stents, Smooth Muscle Cells, Cell proliferation, Mathematical Modelling},
abstract = {The advent of drug-eluting stents (DES) has revolutionised the treatment of coronary artery disease. These devices, coated with anti-proliferative drugs, are deployed into stenosed or occluded vessels, compressing the plaque to restore natural blood flow, whilst simultaneously combating the evolution of restenotic tissue. Since the development of the first stent, extensive research has investigated how further advancements in stent technology can improve patient outcome. Mathematical and computational modelling has featured heavily, with models focussing on structural mechanics, computational fluid dynamics, drug elution kinetics and subsequent binding within the arterial wall; often considered separately. Smooth Muscle Cell (SMC) proliferation and neointimal growth are key features of the healing process following stent deployment. However, models which depict the action of drug on these processes are lacking. In this article, we start by reviewing current models of cell growth, which predominantly emanate from cancer research, and available published data on SMC proliferation, before presenting a series of mathematical models of varying complexity to detail the action of drug on SMC growth in vitro. Our results highlight that, at least for Sodium Salicylate and Paclitaxel, the current state-of-the-art nonlinear saturable binding model is incapable of capturing the proliferative response of SMCs across a range of drug doses and exposure times. Our findings potentially have important implications on the interpretation of current computational models and their future use to optimise and control drug release from DES and drug-coated balloons.}
}
@article{ZHOU2019104484,
title = {Long-term forecasts for energy commodities price: What the experts think},
journal = {Energy Economics},
volume = {84},
pages = {104484},
year = {2019},
issn = {0140-9883},
doi = {https://doi.org/10.1016/j.eneco.2019.104484},
url = {https://www.sciencedirect.com/science/article/pii/S0140988319302658},
author = {Fan Zhou and Lionel Page and Robert K. Perrons and Zuduo Zheng and Simon Washington},
keywords = {Crude oil prices, Natural gas prices, Expert elicitation, Bayesian Truth Serum, Surprisingly popular},
abstract = {The ability to forecast energy prices in the long-term is important for a wide range of reasons, from the formulation of countries' energy and transportation policies to the defensive strategies of nations to investment decisions within the private sector. Despite the importance of these predictions, however, forecasters and market pundits face a difficult challenge when trying to forecast over the long-term. While statistical models can credibly rely on assumptions about the relationship between variables in the short-term, they are frequently less reliable in the long-term as political and technological transformations profoundly change how the economy works over time. Towards improving long-term predictions for energy commodities, this paper uses the elicitation and aggregation of experts' beliefs to put forward forecasts for crude oil and natural gas prices by incentivizing experts to tell the truth and minimising their own biases through the application of the Bayesian Truth Serum. With this approach, we generated both short-term and long-term forecasts, and used the short-term forecast to validate the quality of the experts' predictions.}
}
@article{SCHNASE2017198,
title = {MERRA Analytic Services: Meeting the Big Data challenges of climate science through cloud-enabled Climate Analytics-as-a-Service},
journal = {Computers, Environment and Urban Systems},
volume = {61},
pages = {198-211},
year = {2017},
note = {Geospatial Cloud Computing and Big Data},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2013.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S019897151300118X},
author = {John L. Schnase and Daniel Q. Duffy and Glenn S. Tamkin and Denis Nadeau and John H. Thompson and Cristina M. Grieg and Mark A. McInerney and William P. Webster},
keywords = {MapReduce, Hadoop, Data analytics, Data services, Cloud Computing, Generativity, iRODS, MERRA, ESGF, BAER},
abstract = {Climate science is a Big Data domain that is experiencing unprecedented growth. In our efforts to address the Big Data challenges of climate science, we are moving toward a notion of Climate Analytics-as-a-Service (CAaaS). We focus on analytics, because it is the knowledge gained from our interactions with Big Data that ultimately produce societal benefits. We focus on CAaaS because we believe it provides a useful way of thinking about the problem: a specialization of the concept of business process-as-a-service, which is an evolving extension of IaaS, PaaS, and SaaS enabled by Cloud Computing. Within this framework, Cloud Computing plays an important role; however, we see it as only one element in a constellation of capabilities that are essential to delivering climate analytics as a service. These elements are essential because in the aggregate they lead to generativity, a capacity for self-assembly that we feel is the key to solving many of the Big Data challenges in this domain. MERRA Analytic Services (MERRA/AS) is an example of cloud-enabled CAaaS built on this principle. MERRA/AS enables MapReduce analytics over NASA’s Modern-Era Retrospective Analysis for Research and Applications (MERRA) data collection. The MERRA reanalysis integrates observational data with numerical models to produce a global temporally and spatially consistent synthesis of 26 key climate variables. It represents a type of data product that is of growing importance to scientists doing climate change research and a wide range of decision support applications. MERRA/AS brings together the following generative elements in a full, end-to-end demonstration of CAaaS capabilities: (1) high-performance, data proximal analytics, (2) scalable data management, (3) software appliance virtualization, (4) adaptive analytics, and (5) a domain-harmonized API. The effectiveness of MERRA/AS has been demonstrated in several applications. In our experience, Cloud Computing lowers the barriers and risk to organizational change, fosters innovation and experimentation, facilitates technology transfer, and provides the agility required to meet our customers’ increasing and changing needs. Cloud Computing is providing a new tier in the data services stack that helps connect earthbound, enterprise-level data and computational resources to new customers and new mobility-driven applications and modes of work. For climate science, Cloud Computing’s capacity to engage communities in the construction of new capabilities is perhaps the most important link between Cloud Computing and Big Data.}
}
@article{MARSELLA200970,
title = {EMA: A process model of appraisal dynamics},
journal = {Cognitive Systems Research},
volume = {10},
number = {1},
pages = {70-90},
year = {2009},
note = {Modeling the Cognitive Antecedents and Consequences of Emotion},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2008.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S1389041708000314},
author = {Stacy C. Marsella and Jonathan Gratch},
keywords = {Emotion, Cognitive models, Appraisal theory, Coping},
abstract = {A computational model of emotion must explain both the rapid dynamics of some emotional reactions as well as the slower responses that follow deliberation. This is often addressed by positing multiple levels of appraisal processes such as fast pattern directed vs. slower deliberative appraisals. In our view, this confuses appraisal with inference. Rather, we argue for a single and automatic appraisal process that operates over a person’s interpretation of their relationship to the environment. Dynamics arise from perceptual and inferential processes operating on this interpretation (including deliberative and reactive processes). This article discusses current developments in a computational model of emotion processes and illustrates how a single-level model of appraisal obviates a multi-level approach within the context of modeling a naturalistic emotional situation.}
}
@article{YI2023101423,
title = {Os(II)-catalyzed γ-C(sp3)–H amidation and meta-C(sp2)–H alkylation by fine-tuning the characteristics of in-situ-generated C–Os σ bonds},
journal = {Cell Reports Physical Science},
volume = {4},
number = {6},
pages = {101423},
year = {2023},
issn = {2666-3864},
doi = {https://doi.org/10.1016/j.xcrp.2023.101423},
url = {https://www.sciencedirect.com/science/article/pii/S2666386423001972},
author = {Wei Yi and Weijie Chen and Huiying Xu and Kaifeng Chen and Xiuhua Zhong and Zhi Zhou},
keywords = {Os(II) catalysis, γ-C(sp)–H amidation, -C(sp)–H alkylation, C–H functionalization, theoretical calculations},
abstract = {Summary
Transition metal-catalyzed C–H functionalization has recently emerged as a powerful synthetic tool for accessing various value-added structural motifs. However, addressing the remote C–H activation including γ-C(sp3)–H and meta-C(sp2)–H versions is far less investigated and more challenging than that of ortho-C(sp2)–H activation. By fine-tuning the characteristics of the C–osmium (Os) σ bond in situ generated by the phenylpyridine scaffold-mediated ortho-C–H activation/C,N-bidentate coordination, either Os(II)-catalyzed intramolecular γ-C(sp3)–H amidation or intermolecular meta-C(sp2)–H alkylation has been realized in a highly efficient and chemo-/site-/region-selective manner. Through integrated experimental and computational mechanistic studies, the principle of the observed selectivity and two distinct reaction pathways, involving the Os(IV)-nitrenoid-mediated Os(II)-Os(IV)-Os(II) catalytic cycle and the Os(III)-radical-enabled Os(II)-Os(III)-Os(II) SET process, respectively, have been also clarified. Taken together, it provides an insightful reference benchmark for the future development of more wonderful C–H functionalizations mediated by transition metals, especially by less-explored Os.}
}
@incollection{KEENAN2015394,
title = {Psychology of Inferences},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {394-399},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.57011-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868570111},
author = {Janice M. Keenan},
keywords = {Backward inferences, Coherence, Cortical networks, Explicit inferences, Forward inferences, Implicit inferences, Individual differences, Inferences, Inferencing, computational processes of, Inferencing, methodological issues, Inferencing, neural basis of, Knowledge, Language, Right hemisphere language},
abstract = {The goal of comprehension is to understand what the speaker intended the text to mean. Inferences are driven by a desire to make the interpretation both more coherent and more elaborate than the text itself. The goal of research on inferencing is to specify how the computational processes involved in making inferences vary with the comprehender's knowledge, the conditions that promote inferencing, the various types of inferences and methodological problems involved in assessing them, and, most recently, the neural bases of inferencing.}
}
@article{SHANAHAN2005157,
title = {Applying global workspace theory to the frame problem},
journal = {Cognition},
volume = {98},
number = {2},
pages = {157-176},
year = {2005},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2004.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0010027704002288},
author = {Murray Shanahan and Bernard Baars},
keywords = {Frame problem, Relevance problem, Global workspace theory, Consciousness, Analogical reasoning},
abstract = {The subject of this article is the frame problem, as conceived by certain cognitive scientists and philosophers of mind, notably Fodor for whom it stands as a fundamental obstacle to progress in cognitive science. The challenge is to explain the capacity of so-called informationally unencapsulated cognitive processes to deal effectively with information from potentially any cognitive domain without the burden of having to explicitly sift the relevant from the irrelevant. The paper advocates a global workspace architecture, with its ability to manage massively parallel resources in the context of a serial thread of computation, as an answer to this challenge. Analogical reasoning is given particular attention, since it exemplifies informational unencapsulation in its most extreme form. Because global workspace theory also purports to account for the distinction between conscious and unconscious information processing, the paper advances the tentative conclusion that consciousness may go hand-in-hand with a solution to the frame problem in the biological brain.}
}
@article{CHENG20115100,
title = {Equilibrium Conditions In Service Supply Chain},
journal = {Procedia Engineering},
volume = {15},
pages = {5100-5104},
year = {2011},
note = {CEIS 2011},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2011.08.946},
url = {https://www.sciencedirect.com/science/article/pii/S1877705811024477},
author = {Fei Cheng and Shanlin Yang and Xijun Ma},
keywords = {service supply chain, service volume, equilibrium},
abstract = {Service supply chain features human players as service vendor, service integrator, customer and service resource. It tends to be digitally connected, such as consulting, e-business and integrated enterprises. Our study uses a formal model and simulations to develop the effect of a service supply chain on equilibrium computation. Two insights arise on how a network can obtain equilibrium computation: forming the network structure of service supply chain; exploring entities behavior and equilibrium conditions. These results highlight the importance for service supply chain of adapting its network structure to equilibrium and application.}
}
@article{SWARTZ2004773,
title = {A multimethod approach to the combat air forces mix and deployment problem},
journal = {Mathematical and Computer Modelling},
volume = {39},
number = {6},
pages = {773-797},
year = {2004},
note = {Defense transportation: Algorithms, models, and applications for the 21st century},
issn = {0895-7177},
doi = {https://doi.org/10.1016/S0895-7177(04)90554-7},
url = {https://www.sciencedirect.com/science/article/pii/S0895717704905547},
author = {S.M Swartz and A.W Johnson},
keywords = {Multiattribute decision analysis, Ranking and selection, Heuristics},
abstract = {The purpose of military logistics is to ensure that the material elements of combat capability come together at the right place and time and in the right configuration to be useful to the supported commander. These material elements are constrained in both quantity and location. The usefulness of any element to a commander is dependent upon both its extrinsic (qualitative; situation dependent) and intrinsic (quantitative; inherent) characteristics. Our research provides a methodology for rationally assigning relative value to material resources over time, in order to improve the linkage between what arrives (becomes available for use) in theater at any given time, and what is actually needed at that time. A blend of qualitative (value focused thinking and hierarchical weighting) and quantitative (a greedy matching algorithm) methods were used against the lift-constrained combat forces material selection/movement problem. The intent is to provide a decision support tool for the formulation of force mixes that best support desired time-phased battlefield objectives, given constraints on available transportation resources. This methodology is applicable to general crisis response planning, such as for disaster relief.}
}
@article{FORTINI2023107058,
title = {An experimental and numerical study of the solid particle erosion damage in an industrial cement large-sized fan},
journal = {Engineering Failure Analysis},
volume = {146},
pages = {107058},
year = {2023},
issn = {1350-6307},
doi = {https://doi.org/10.1016/j.engfailanal.2023.107058},
url = {https://www.sciencedirect.com/science/article/pii/S1350630723000122},
author = {Annalisa Fortini and Alessio Suman and Nicola Zanini},
keywords = {Wear damage, Hardfacing, Centrifugal fan, Computational fluid dynamics, Solid particle erosion, Metallographic analysis},
abstract = {The present paper addresses the wear failure analysis of a large-sized centrifugal fan operating in a cement clinker grinding plant. Within cement production, the calcination at middle and high-temperature values (from 120 °C to 400 °C depending on the process parameters) of the raw material requires such a process fan, which also ensures the draft and feed of the flue gases and combustion air needed for the operation of the main equipment of the cement factory. To detect and analyze the impact conditions within the heavy-duty fan, Computational Fluid Dynamics (CFD) analyses were performed. The analysis of the numerical results shows that the relevant fan surfaces are affected by different impact velocities and angles, generating non-uniform erosion patterns similar to the on-field detections. Besides, the obtained comprehensive description of the flow and contaminants behaviors through the entire flow path enables setting up the subsequent experimental investigation. The erosive wear behavior of a Fe-Cr-C hardfacing cast iron and wear-resistant steel was tested through a test rig constructed for the purpose of being in accordance with the ASTM G76 standard. The test bench was adapted to manage the raw meal powder used in the cement factory to reproduce the actual operating conditions. The results show a greater capability of Fe-Cr-C hardfacing cast iron to face the erosion phenomenon in terms of lower values of material loss over the exposure time. These findings, coupled with the metallographic analysis to detect the erosion mechanisms (ductile and/or brittle), help a better prediction of the fan operating life. The investigation showed the reliability of the numerical/experimental coupled approach in assessing the actual erosion magnitude and the influence of the impact angle on the erosion phenomena. This coupled approach gains a further understanding of the proper design of manufacturing and maintenance activities, covering several project steps from material selections to the scheduled and overhaul operations. A reliable operating-life prediction allows manufacturers and operators to obtain production and economic goals.}
}
@article{AMMAR2024100839,
title = {Role of pedagogical approaches in fostering innovation among K-12 students in STEM education},
journal = {Social Sciences & Humanities Open},
volume = {9},
pages = {100839},
year = {2024},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2024.100839},
url = {https://www.sciencedirect.com/science/article/pii/S2590291124000366},
author = {Mohammad Ammar and Noora J. Al-Thani and Zubair Ahmad},
keywords = {Educational reform, STEM, Pedagogy, K-12, Early education, Innovation, Technology},
abstract = {The intricate challenges of the modern world demand students to be equipped with advanced skills and knowledge to thrive in an increasingly competitive global landscape. Science, technology, engineering, and mathematics (STEM) practices can help develop these capabilities in students from an early age. However, as technology continues to advance rapidly, STEM education has experienced a rapid transformation with seamless integration of various technologies. Students in the K-12 education is required to keep up with the growing innovation and to bridge this gap, pedagogical approaches play a crucial role. Therefore, this review presents the current landscape, developmental trends, and future directions of the various pedagogical practices used to integrate innovation in K-12 STEM. The characteristics and environmental perceptions that influence the development of innovation in students using such approaches are examined. Results from 42 systematically shortlisted studies indicate positive correlations of personalized pedagogical approaches in promoting innovation in students, thereby increasing STEM literacy in K-12 education. However, limitations that remain with teacher competencies and school facilities to cope with various pedagogical approaches are also discussed. Finally, we conclude with our recommendations on effective and efficient approaches that can be implemented in K-12 STEM education to develop the skills and mindset in students necessary to become innovative thinkers and prepare them for a technology-driven society.}
}
@article{WANG2023e131,
title = {Interbody Fusion Cage Design Driven by Topology Optimization},
journal = {World Neurosurgery},
volume = {174},
pages = {e131-e143},
year = {2023},
issn = {1878-8750},
doi = {https://doi.org/10.1016/j.wneu.2023.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S1878875023003042},
author = {Zuowei Wang and Jun Jiang and Fengzeng Jian and Zan Chen and Xingwen Wang and Wanru Duan and Weisheng Zhang},
keywords = {Fusion cage design, Interbody fusion, Moving morphable void approach, Topology optimization},
abstract = {Objective
We used topology optimization technology to explore the new theory and method of interbody fusion cage design and realized an innovative design of interbody cages.
Methods
The lumbar spine of a normal healthy volunteer was scanned to perform reverse modeling. Based on the scan data for the L1-L2 segments of the lumbar spine, a three dimensional model was reconstructed to obtain the complete simulation model of the L1-L2 segment. The boundary inversion method was used to obtain approximately isotropic material parameters that can effectively characterize the mechanical behavior of vertebrae, thereby reducing the computational complexity. The topology description function was used to model the clinically used traditional fusion cage to obtain Cage A. The moving morphable void-based topology optimization method was used for the integrated design of size, shape, and topology to obtain the optimized fusion cage, Cage B.
Results
The volume fraction of the bone graft window in Cage B was 74.02%, which was 60.67% higher than that (46.07%) in Cage A. Additionally, the structural strain energy in the design domain of Cage B was 1.48 mJ, which was lower than that of Cage A (satisfying the constraints). The maximum stress in the design domain of Cage B was 5.336 Mpa, which was 35.6% lower than that (8.286 Mpa) of Cage A. In addition, the surface stress distribution of Cage B was more uniform than that of Cage A.
Conclusions
This study proposed a new innovative design method for interbody fusion cages, which not only provides new insights into the innovative design of interbody fusion cages but may also guide the customized design of interbody fusion cages in different pathological environments.}
}
@article{SUSPERREGUY2024101441,
title = {The role of mathematical vocabulary in the development of mathematical skills for Spanish-speaking students},
journal = {Cognitive Development},
volume = {70},
pages = {101441},
year = {2024},
issn = {0885-2014},
doi = {https://doi.org/10.1016/j.cogdev.2024.101441},
url = {https://www.sciencedirect.com/science/article/pii/S0885201424000261},
author = {María Inés Susperreguy and Sabrina M. {Di Lonardo Burr} and Chang Xu and Heather P. Douglas and Taeko Bourque and M. Francisca {del Río} and Viviana Salinas and Jo-Anne LeFevre},
keywords = {Mathematical language, Mathematical vocabulary, Mathematics, Math growth, Chile},
abstract = {Does mathematical vocabulary predict the change in students’ performance on mathematical tasks from one academic year to the next? Chilean Spanish-speaking students (N = 87) completed measures of mathematical vocabulary, mathematical skills (i.e., arithmetic fluency, calculation, and applied problems), receptive vocabulary, and working memory in Grade 2 (T1, Mage = 7:11 years:months, SD = 0:5, 46% girls). One year later (T2) they completed the same mathematical measures. Concurrent relations were found between mathematical vocabulary and the three mathematical skills at both time points. Together, general and mathematical vocabulary at T1 explained significant unique variance in the change in applied problems and calculation from T1 to T2. For calculation however, only mathematical vocabulary predicted significant unique variance in the change from T1 to T2. Change in arithmetic fluency was only predicted by working memory. These results address the roles of general and mathematical vocabulary in students’ mathematical development in elementary school.}
}
@article{COSTABILE2021126306,
title = {A 2D-SWEs framework for efficient catchment-scale simulations: Hydrodynamic scaling properties of river networks and implications for non-uniform grids generation},
journal = {Journal of Hydrology},
volume = {599},
pages = {126306},
year = {2021},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2021.126306},
url = {https://www.sciencedirect.com/science/article/pii/S002216942100353X},
author = {Pierfranco Costabile and Carmelina Costanzo},
keywords = {2D shallow water equations, Surface runoff, River networks, Non-uniform grids, Channel heads, Scaling laws},
abstract = {The application of two-dimensional shallow-water equations models (2D-SWEs) for the description of hydrodynamic-based surface runoff computations is becoming a reference approach in rainfall-runoff simulations at the catchment scale. Due to their ability in generation of flow patterns throughout the basin, they can be used not only as an advanced method for flood mapping studies and hazard assessment but also as an innovative tool for the analysis of river drainage networks, opening new perspectives for several environmental processes. In particular, in this work we put the river networks in a 2D-SWEs framework, meaning that the traditional tree-like fluvial structure, represented by a skeleton composed of a set of lines, is replaced by a collection of points discretizing the 2-D geometry of the river structure itself, for which the values of the hydrodynamic values are provided by the numerical simulations. This approach is used here to derive a new scaling property that relates the specific discharge threshold, used to identify the river network cells, to the total areas of the network cells themselves. The hydrodynamic and geomorphological interpretation of this power law function and the influence of grid resolution, on some relevant parameters of this curve, have inspired the development of a heuristic procedure for non-uniform grid generation, able to detect the most hydrodynamically active areas of the basins for which the grid refinement process makes sense. Moreover, information related to how much grid refinement is needed is provided as well. The performances of this procedure are very promising in terms of accuracy of simulated discharges, hydrodynamic behaviour of the river network and flooded areas, reducing significantly the computational times in respect to the use of fine uniform grids.}
}
@article{KRICHEN2024104034,
title = {Performance enhancement of artificial intelligence: A survey},
journal = {Journal of Network and Computer Applications},
volume = {232},
pages = {104034},
year = {2024},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2024.104034},
url = {https://www.sciencedirect.com/science/article/pii/S108480452400211X},
author = {Moez Krichen and Mohamed S. Abdalzaher},
keywords = {Performance evaluation, Optimization techniques, Machine learning, Artificial intelligence, Data processing approaches},
abstract = {The advent of machine learning (ML) and Artificial intelligence (AI) has brought about a significant transformation across multiple industries, as it has facilitated the automation of jobs, extraction of valuable insights from extensive datasets, and facilitation of sophisticated decision-making processes. Nevertheless, optimizing efficiency has become a critical research field due to AI systems’ increasing complexity and resource requirements. This paper provides an extensive examination of several techniques and methodologies aimed at improving the efficiency of ML and artificial intelligence. In this study, we investigate many areas of research about AI. These areas include algorithmic improvements, hardware acceleration techniques, data pretreatment methods, model compression approaches, distributed computing frameworks, energy-efficient strategies, fundamental concepts related to AI, AI efficiency evaluation, and formal methodologies. Furthermore, we engage in an examination of the obstacles and prospective avenues in this particular domain. This paper offers a deep analysis of many subjects to equip researchers and practitioners with sufficient strategies to enhance efficiency within ML and AI systems. More particularly, the paper provides an extensive analysis of efficiency-enhancing techniques across multiple dimensions: algorithmic advancements, hardware acceleration, data processing, model compression, distributed computing, and energy consumption.}
}
@article{ZHAO2024100691,
title = {Artificial intelligence for geoscience: Progress, challenges, and perspectives},
journal = {The Innovation},
volume = {5},
number = {5},
pages = {100691},
year = {2024},
issn = {2666-6758},
doi = {https://doi.org/10.1016/j.xinn.2024.100691},
url = {https://www.sciencedirect.com/science/article/pii/S2666675824001292},
author = {Tianjie Zhao and Sheng Wang and Chaojun Ouyang and Min Chen and Chenying Liu and Jin Zhang and Long Yu and Fei Wang and Yong Xie and Jun Li and Fang Wang and Sabine Grunwald and Bryan M. Wong and Fan Zhang and Zhen Qian and Yongjun Xu and Chengqing Yu and Wei Han and Tao Sun and Zezhi Shao and Tangwen Qian and Zhao Chen and Jiangyuan Zeng and Huai Zhang and Husi Letu and Bing Zhang and Li Wang and Lei Luo and Chong Shi and Hongjun Su and Hongsheng Zhang and Shuai Yin and Ni Huang and Wei Zhao and Nan Li and Chaolei Zheng and Yang Zhou and Changping Huang and Defeng Feng and Qingsong Xu and Yan Wu and Danfeng Hong and Zhenyu Wang and Yinyi Lin and Tangtang Zhang and Prashant Kumar and Antonio Plaza and Jocelyn Chanussot and Jiabao Zhang and Jiancheng Shi and Lizhe Wang},
keywords = {artificial intelligence, machine learning, deep learning, geoscience},
abstract = {This paper explores the evolution of geoscientific inquiry, tracing the progression from traditional physics-based models to modern data-driven approaches facilitated by significant advancements in artificial intelligence (AI) and data collection techniques. Traditional models, which are grounded in physical and numerical frameworks, provide robust explanations by explicitly reconstructing underlying physical processes. However, their limitations in comprehensively capturing Earth’s complexities and uncertainties pose challenges in optimization and real-world applicability. In contrast, contemporary data-driven models, particularly those utilizing machine learning (ML) and deep learning (DL), leverage extensive geoscience data to glean insights without requiring exhaustive theoretical knowledge. ML techniques have shown promise in addressing Earth science-related questions. Nevertheless, challenges such as data scarcity, computational demands, data privacy concerns, and the “black-box” nature of AI models hinder their seamless integration into geoscience. The integration of physics-based and data-driven methodologies into hybrid models presents an alternative paradigm. These models, which incorporate domain knowledge to guide AI methodologies, demonstrate enhanced efficiency and performance with reduced training data requirements. This review provides a comprehensive overview of geoscientific research paradigms, emphasizing untapped opportunities at the intersection of advanced AI techniques and geoscience. It examines major methodologies, showcases advances in large-scale models, and discusses the challenges and prospects that will shape the future landscape of AI in geoscience. The paper outlines a dynamic field ripe with possibilities, poised to unlock new understandings of Earth’s complexities and further advance geoscience exploration.}
}
@article{SCHUH20181,
title = {Exact satisfiability of linear CNF formulas},
journal = {Discrete Applied Mathematics},
volume = {251},
pages = {1-4},
year = {2018},
issn = {0166-218X},
doi = {https://doi.org/10.1016/j.dam.2018.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S0166218X18302762},
author = {Bernd R. Schuh},
keywords = {Complexity, XSAT, Exact linear formula, l-regularity, k-uniformity, NP-completeness},
abstract = {Open questions with respect to the computational complexity of linear CNF (LCNF) formulas are addressed. Focus lies on exact linear CNF formulas (XLCNF), in which any two clauses have exactly one variable in common. It is shown that l-regularity, i.e. each variable occurs exactly l times in the formula, imposes severe restrictions on the structure of XLCNF formulas. In particular it is proven that l-regularity in XLCNF implies k-uniformity, i.e. all clauses have the same number k of literals. Allowed k- values obey k (k−1)=0 (mod l), and the number of clauses m is given by m =kl-(k−1). Then the computational complexity of monotone l-regular XLCNF formulas with respect to exact satisfiability (XSAT) is determined. XSAT turns out to be either trivial, if m is not a multiple of l, or it can be decided in sub-exponential time, namely O(nn).}
}
@article{LOW2020e03083,
title = {Induction approach via P-Graph to rank clean technologies},
journal = {Heliyon},
volume = {6},
number = {1},
pages = {e03083},
year = {2020},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2019.e03083},
url = {https://www.sciencedirect.com/science/article/pii/S2405844019367428},
author = {C.X. Low and W.Y. Ng and Z.A. Putra and K.B. Aviso and M.A.B. Promentilla and R.R. Tan},
keywords = {Chemical engineering, Optimal selection, Simple additive weighting, Clean technologies, Induction, Decision analysis, P-Graph},
abstract = {Identification of appropriate clean technologies for industrial implementation requires systematic evaluation based on a set of criteria that normally reflect economic, technical, environmental and other aspects. Such multiple attribute decision-making (MADM) problems involve rating a finite set of alternatives with respect to multiple potentially conflicting criteria. Conventional MADM approaches often involve explicit trade-offs in between criteria based on the expert's or decision maker's priorities. In practice, many experts arrive at decisions based on their tacit knowledge. This paper presents a new induction approach, wherein the implicit preference rules that estimate the expert's thinking pathways can be induced. P-graph framework is applied to the induction approach as it adds the advantage of being able to determine both optimal and near-optimal solutions that best approximate the decision structure of an expert. The method elicits the knowledge of experts from their ranking of a small set of sample alternatives. Then, the information is processed to induce implicit rules which are subsequently used to rank new alternatives. Hence, the expert's preferences are approximated by the new rankings. The proposed induction approach is demonstrated in the case study on the ranking of Negative Emission Technologies (NETs) viability for industry implementation.}
}
@incollection{PASCAL2022231,
title = {10 - A practical guide to paleostress analysis},
editor = {Christophe Pascal},
booktitle = {Paleostress Inversion Techniques},
publisher = {Elsevier},
pages = {231-245},
year = {2022},
isbn = {978-0-12-811910-5},
doi = {https://doi.org/10.1016/B978-0-12-811910-5.00008-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128119105000087},
author = {Christophe Pascal},
keywords = {Fieldwork, Measuring, Processing, Plotting, Interpretation, Reporting},
abstract = {After having presented extensively different theoretical and methodological aspects of paleostress reconstruction methods, the purpose of the final chapter is to introduce recommendations for their practical use in tectonic problems. The discussion focuses on paleostress inversion of fault slip data, the latter being both the most elaborated and the most employed method. Detailed practical advice is given to conduct a paleostress study efficiently, starting with data acquisition in the field, proceeding with subsequent computation of paleostress tensors and ending with the reporting of the results and of their subsequent interpretations.}
}
@article{HUANG202219,
title = {Numerical simulation and comparative study for the zinc smelting furnaces at the Tongmuling site in Qing Dynasty, Hunan Province, China},
journal = {Advances in Archaeomaterials},
volume = {3},
number = {1},
pages = {19-27},
year = {2022},
issn = {2667-1360},
doi = {https://doi.org/10.1016/j.aia.2022.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S2667136022000061},
author = {Xing Huang and Linheng Mo and Wenli Zhou and Shengqiang Luo and Ya Xiao and Jianli Chen},
keywords = {Zinc smelting furnace, Furnace profile, Retort, Numerical simulation, Tongmuling site},
abstract = {Brass, which appears golden in color, used to be a valuable alloy in ancient times. During the Ming and Qing Dynasties, the Chinese used special furnaces to smelt zinc for minting and exporting to overseas in large quantities. Archeological findings have revealed the overall structure of the zinc smelting furnaces at the Tongmuling site during the Qing Dynasty. In this study, computational fluid dynamics software was employed to simulate airflow fields within a furnace. Consequently, we observed that airflows were concentrated at the center of the lower chamber, after which they dispersed into the upper chamber through ceramic pads and finally were evenly distributed between the retorts. Increasing furnace height and improving thermal convection in the lower chamber helped increase the furnace temperature. The ceramic pads adjusted the airflow to ensure that temperature distribution in the upper chamber was uniform, and they supported burning in the upper chamber by preventing collapse. Compared with the heap smelting process recorded in Heavenly Creations and the large crucible furnaces used in modern times, zinc smelting furnaces at the Tongmuling site possess a unique structure. They serve as a link between preceding and subsequent technologies, offering important evidence for exploring the development of ancient Chinese zinc smelting technologies.}
}
@article{ANDERSON1998159,
title = {Mental retardation general intelligence and modularity},
journal = {Learning and Individual Differences},
volume = {10},
number = {3},
pages = {159-178},
year = {1998},
issn = {1041-6080},
doi = {https://doi.org/10.1016/S1041-6080(99)80128-9},
url = {https://www.sciencedirect.com/science/article/pii/S1041608099801289},
author = {Mike Anderson},
abstract = {This article presents a case for distinguishing between mental retardation as a general deficit of thinking and mental retardation that might result from the global effects of a specific deficit in a cognitive module. Using Anderson's (1992a) theory of the minimal cognitive architecture of intelligence and developmental, I show how this distinction can explain the pattern of intellectual strengths and weaknesses in Savant syndrome, Williams syndrome, Down syndrome, and autism. In addition, I discuss the developmental versus difference view and the distinction between organic and cultural familial mental retardation in the light of this theory. I conclude that not only is there no inherent incompatibility between the constructs of general intelligence and modularity of mind but that both are essential to understanding the different patterns of abilities and developmental profiles found in individuals with low IQ.}
}
@article{BARTH201937,
title = {Progressive Circuit Changes during Learning and Disease},
journal = {Neuron},
volume = {104},
number = {1},
pages = {37-46},
year = {2019},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2019.09.032},
url = {https://www.sciencedirect.com/science/article/pii/S0896627319308347},
author = {Alison L. Barth and Ajit Ray},
abstract = {A critical step toward understanding cognition, learning, and brain dysfunction will be identification of the underlying cellular computations that occur in and across discrete brain areas, as well as how they are progressively altered by experience or disease. These computations will be revealed by targeted analyses of the neurons that perform these calculations, defined not only by their firing properties but also by their molecular identity and how they are wired within the local and broad-scale network of the brain. New studies that take advantage of sophisticated genetic tools for cell-type-specific identification and control are revealing how learning and neurological disorders initiate and successively change the properties of defined neural circuits. Understanding the temporal sequence of adaptive or pathological synaptic changes across multiple synapses within a network will shed light into how small-scale neural circuits contribute to higher cognitive functions during learning and disease.}
}
@article{PRATO201888,
title = {Considering built environment and spatial correlation in modeling pedestrian injury severity},
journal = {Traffic Injury Prevention},
volume = {19},
number = {1},
pages = {88-93},
year = {2018},
issn = {1538-9588},
doi = {https://doi.org/10.1080/15389588.2017.1329535},
url = {https://www.sciencedirect.com/science/article/pii/S1538958822003630},
author = {Carlo G. Prato and Sigal Kaplan and Alexandre Patrier and Thomas K. Rasmussen},
keywords = {Pedestrian crashes, injury severity models, built environment, spatial correlation},
abstract = {ABSTRACT
Objective: This study looks at mitigating and aggravating factors that are associated with the injury severity of pedestrians when they have crashes with another road user and overcomes existing limitations in the literature by focusing attention on the built environment and considering spatial correlation across crashes. Method: Reports for 6,539 pedestrian crashes occurred in Denmark between 2006 and 2015 were merged with geographic information system resources containing detailed information about the built environment and exposure at the crash locations. A linearized spatial logit model estimated the probability of pedestrians sustaining a severe or fatal injury conditional on the occurrence of a crash with another road user. Results: This study confirms previous findings about older pedestrians and intoxicated pedestrians being the most vulnerable road users and crashes with heavy vehicles and in roads with higher speed limits being related to the most severe outcomes. This study provides novel perspectives by showing positive spatial correlations of crashes with the same severity outcomes and emphasizing the role of the built environment in the proximity of the crash. Conclusions: This study emphasizes the need for thinking about traffic calming measures, illumination solutions, road maintenance programs, and speed limit reductions. Moreover, this study emphasizes the role of the built environment, because shopping areas, residential areas, and walking traffic density are positively related to a reduction in pedestrian injury severity. Often, these areas have in common a larger pedestrian mass that is more likely to make other road users more aware and attentive, whereas the same does not seem to apply to areas with lower pedestrian density.}
}
@article{SRINIVAS199799,
title = {Strategic decision-making processes: network-based representation and stochastic simulation},
journal = {Decision Support Systems},
volume = {21},
number = {2},
pages = {99-110},
year = {1997},
note = {Special Issue: Expertise and Modeling Expert Decision Making},
issn = {0167-9236},
doi = {https://doi.org/10.1016/S0167-9236(97)00023-7},
url = {https://www.sciencedirect.com/science/article/pii/S0167923697000237},
author = {V. Srinivas and B. Shekar},
keywords = {Qualitative probabilistic networks, Stochastic simulation, Cognitive maps, Strategic thinking, Decision-making process, Network-based representation},
abstract = {Representation of decision-making in organizations is an intricate process. Qualitative Probabilistic Network (QPN)-based approach offers a scheme which is useful for representing processes involved in decision-making. This paper demonstrates the usefulness of QPN-based scheme with an illustrative case study. The focus of the case study is on understanding the strategic behavior of a key player in the Indian Automobile Industry. This is done by transforming Cognitive Maps developed into QPN-based formalisms and analyzing them. In addition to this, stochastic simulation experiment is performed on the QPN-based networks to generate hypothetical scenarios.}
}
@article{PEZZULO2014647,
title = {Internally generated sequences in learning and executing goal-directed behavior},
journal = {Trends in Cognitive Sciences},
volume = {18},
number = {12},
pages = {647-657},
year = {2014},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2014.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S1364661314001570},
author = {Giovanni Pezzulo and Matthijs A.A. {van der Meer} and Carien S. Lansink and Cyriel M.A. Pennartz},
keywords = {forward sweep, generative models, hippocampus, decision making, reinforcement learning, spatial navigation, replay, inference, prospection, theta rhythm, ventral striatum},
abstract = {A network of brain structures including hippocampus (HC), prefrontal cortex, and striatum controls goal-directed behavior and decision making. However, the neural mechanisms underlying these functions are unknown. Here, we review the role of ‘internally generated sequences’: structured, multi-neuron firing patterns in the network that are not confined to signaling the current state or location of an agent, but are generated on the basis of internal brain dynamics. Neurophysiological studies suggest that such sequences fulfill functions in memory consolidation, augmentation of representations, internal simulation, and recombination of acquired information. Using computational modeling, we propose that internally generated sequences may be productively considered a component of goal-directed decision systems, implementing a sampling-based inference engine that optimizes goal acquisition at multiple timescales of on-line choice, action control, and learning.}
}
@article{MCINTOSH2024172,
title = {Intricacies of mega and complex projects: a survey of AAMI Park, Australia},
journal = {Infrastructure Asset Management},
volume = {11},
number = {3},
pages = {172-188},
year = {2024},
issn = {2053-0250},
doi = {https://doi.org/10.1680/jinam.24.00003},
url = {https://www.sciencedirect.com/science/article/pii/S2053025024000071},
author = {Jennifer McIntosh and Sophia Harris and Josh Scott Taylor and Koorosh Gharehbaghi and Neville Hurst and Kong Fah Tee},
keywords = {design complexity, environmental considerations, infrastructure asset management, mapping complexity science, mega & complex projects, UN SDG 11: Sustainable cities and communities, UN SDG 13: Climate action},
abstract = {Complexity science brings together differing paradigms and thoughts. It has attracted academic interest as a means of understanding complex social and technological phenomena. The emergence of increasingly complex building design, construction and technologies is posing numerous challenges for professionals. This paper utilises complexity science, and the practical application of complexity theories, to evaluate holistically the development of a unique major sporting stadium, known as AAMI Park, in Melbourne, Australia. The authors assess the complexity of this project and rank it on a complexity scale to illustrate the level of knowledge and expertise required to deliver such projects. The most dominant and complex feature of AAMI Park, the bio-frame roof, which was innovative at the time, posed unique challenges. Therefore, the authors also evaluate the constraints and specific project delivery methods developed to manage best its complications. Since the adaptive management approach was used for this project, this paper assessed this approach to recommend enhanced delivery methodologies for similar future projects. Further, since the design of this project was the first of a kind, the authors seek to identify the level of project complexity and its success through the lens of complexity theory.}
}
@article{RUPNOW2024101173,
title = {Abstract algebra students’ conceptual metaphors for isomorphism and homomorphism},
journal = {The Journal of Mathematical Behavior},
volume = {75},
pages = {101173},
year = {2024},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2024.101173},
url = {https://www.sciencedirect.com/science/article/pii/S0732312324000506},
author = {Rachel Rupnow and Brooke Randazzo},
keywords = {Isomorphism, Homomorphism, Conceptual metaphors, Abstract algebra, Sameness},
abstract = {Group isomorphism and homomorphism are core concepts in abstract algebra, and student understanding of isomorphism has received extensive attention in line with the centrality of this topic. However, limited work has directly examined student conceptions of homomorphism or what metaphors students use to express their thought processes while problem solving. Based on interviews with four students, we contrast two students who used predominantly formal definition and mapping-centered metaphors for homomorphism with two who additionally used sameness-centered metaphors and note that the usage or non-usage of sameness-centered metaphors was not indicative of successful problem solving. Implications include the alignment between students’ metaphors and those used in instruction, indicating the importance of attending to metaphors when teaching, and the importance of discussing what is intended by some sameness-based metaphors, such as operation-preservation.}
}
@article{COLOMBINI2022104631,
title = {Safety evaluations on unignited high-pressure methane jets impacting a spherical obstacle},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {74},
pages = {104631},
year = {2022},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2021.104631},
url = {https://www.sciencedirect.com/science/article/pii/S0950423021002400},
author = {Cristian Colombini and Edoardo Carminati and Andrea Parisi and Renato Rota and Valentina Busini},
keywords = {High-pressure release, Methane, Spherical obstacle influence, Risk assessment, CFD, Analytical correlation},
abstract = {Nowadays methane is a fossil fuel widely used both in industries and in civil appliances. From the safety point of view, due to its flammability, its use implies hazards for people and assets. The hazardous area related to a high-pressure jet of methane arising from an accidental loss of containment requires the estimation of the distance at which the methane concentration falls below the Lower Flammability limit. Such a topic is well covered in the literature when considering free jet conditions, i.e., jets that do not interact with any equipment or surface. The same cannot be said for high pressure jets impacting an obstacle. In this context, the present work focuses on studying high pressure methane jets impacting spherical obstacles by means of Computational Fluid Dynamics with the aim of giving some insights about such a jet-obstacle interaction, possibly providing a brief by-hand procedure that, only based on known scenario information, allows to estimate the maximum extent of the unignited high-pressure jet when interacting with a spherical obstacle.}
}
@article{STENNING1988143,
title = {Knowledge-rich solutions to the binding problem: a simulation of some human computational mechanisms},
journal = {Knowledge-Based Systems},
volume = {1},
number = {3},
pages = {143-152},
year = {1988},
issn = {0950-7051},
doi = {https://doi.org/10.1016/0950-7051(88)90072-X},
url = {https://www.sciencedirect.com/science/article/pii/095070518890072X},
author = {Keith Stenning and Joe Levy},
keywords = {binding, memory, PDP system, knowledge-rich, human memory, representations},
abstract = {The binding problem, how properties are represented as belonging to individuals, is identified as a severe problem for human memory, for which the memory adopts knowledge-rich solutions. It is argued that it is the nature of these solutions that endows human memory with many of its positive properties, particularly rapid retrieval on the basis of unreliable search clues. Parallel Distributed Processing (PDP) systems offer some insight into how human memory systems may work, as they also have to solve the binding problem by knowledge-rich methods. Experimental analysis and statistical models of Memory for Individuals Task (MIT) are presented, which provide evidence that the memory representations underlying human performance consist of sets of existential facts containing no referential terms. It is shown that the proposed representations can be incorporated directly into a PDP simulation of the inference from representation to response, and that the resulting system produces human-like errors when subjected to noisy input. The PDP simulation captures some of the asymmetries between stimulus and response which the statistical model cannot.}
}
@article{ERIOLI2011729,
title = {Interwoven landscape},
journal = {Procedia Engineering},
volume = {21},
pages = {729-736},
year = {2011},
note = {2011 International Conference on Green Buildings and Sustainable Cities},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2011.11.2071},
url = {https://www.sciencedirect.com/science/article/pii/S1877705811049058},
author = {Alessio Erioli and Mirco Bianchini and Piero Bruschi and Andrea Baschieri},
keywords = {architecture, ecology, infrastructure, highway, photocatalysis, dazzle, new materials ;},
abstract = {Human specie has always engineered the environment to set the conditions for its own settlement, producing in its evolutionary development superorganisms (cities) and the necessary networks of connections among them. Instead of rejecting cars as an extraneous object to a picturesque nature, this project starts from a perspective in which cities and technology are the metabolic extension of human specie and therefore a necessary part of its own nature; the vessels (vehicles) for human transportation, or better, the vehicle-host symbiotic system thus becomes a necessary part of human ecology, and so the network of connections upon which they live, operate and interact with: infrastructures. The project of an environmental enhancer for the Nogara mare highway in Veneto (Italy) provides the unique chance to bring together ecological thinking, host interaction and active materials. Its location (an open country planar area among cultivated fields) enucleates as critical variables the impact of pollutants and the phenomenon of dazzling. With respect to such criticalities, the project uses digital generative and parametric strategies to generate a performative structure in which densification and rarefaction of elements is a local morphological response to dazzle. The structure itself acts as a scaffold for a photo catalytic PET based material that, mimicking the behavior of coccoluti (marine microorganisms) is able to reduce CO2 (and potentially other pollutants) to salts and nitrates that are then naturally deployed to the neighboring cultivated fields as fertilizers. The material has been tested for photo catalytic integration and is currently under development. Present building and production techniques privilege the industrial assembly of inert materials, with a one-way flow of energy and process from raw material to finished product. Instead of this mono-directional energy consumption the project promotes the continuous exchange of information (as code and matter-energy) at all levels and from the digital to the material domains: use of dazzle information, morphogenetic rules and structural behavior to generate the scaffold, a photo catalytic material that responds to pollutants and produces fertilizers, making the structure symbiotic with their hosts and the environment.}
}
@article{IVCEVIC2024100079,
title = {Artificial intelligence as a tool for creativity},
journal = {Journal of Creativity},
volume = {34},
number = {2},
pages = {100079},
year = {2024},
issn = {2713-3745},
doi = {https://doi.org/10.1016/j.yjoc.2024.100079},
url = {https://www.sciencedirect.com/science/article/pii/S2713374524000050},
author = {Zorana Ivcevic and Mike Grandinetti},
keywords = {Artificial intelligence, Creativity, AI as tool, 4c's model of creativity},
abstract = {The release of ChatGPT has sparked quite a bit of interest about creativity in the context of artificial intelligence (AI), with theorizing and empirical research asking questions about the nature of creativity (both human and artificially-produced) and the valuing of work produced by humans and artificial means. In this article, we discuss one specific scenario identified in the creativity research community – co-creation, or use of AI as a tool that could augment human creativity. We present emerging research relevant to how AI can be used on a continuum of four levels of creativity, from mini-c/creativity in learning to little-c/everyday creativity to Pro-C/professional creativity and Big-C/eminent creativity. In this discussion, AI is defined broadly, not to include only large language models (e.g., ChatGPT) which might approach general AI, but also other computer programs that perform tasks typically understood as requiring human intelligence. We conclude by considering future directions for research on AI as a tool for creativity across the four c's.}
}
@incollection{HORRIGAN2004317,
title = {A Study in the Process of Planning, Designing and Executing a Survey Program: The BLS American Time-Use Survey},
series = {Contributions to Economic Analysis},
publisher = {Elsevier},
volume = {271},
pages = {317-350},
year = {2004},
booktitle = {The Economics of Time Use},
issn = {0573-8555},
doi = {https://doi.org/10.1016/S0573-8555(04)71012-3},
url = {https://www.sciencedirect.com/science/article/pii/S0573855504710123},
author = {Michael Horrigan and Diane Herz},
keywords = {US, time use, survey, American time-use survey},
abstract = {In this study, we describe the evolution of the American time-use survey (ATUS) from its inception as an issue of statistical policy interest in 1991 to its implementation in January 2003 as an ongoing monthly survey sponsored by the US Bureau of Labor Statistics. This 12-year process included four developmental phases. Each successive phase represented a deeper level of agency commitment and outside statistical support. The resulting reports referenced in the text reflect an evolution in our thinking on survey estimation objectives, units of measurement, universe frame and sampling plan, and data collection and coding protocols.}
}
@article{PALANIYAPPAN2020109911,
title = {Cortical thickness and formal thought disorder in schizophrenia: An ultra high-field network-based morphometry study},
journal = {Progress in Neuro-Psychopharmacology and Biological Psychiatry},
volume = {101},
pages = {109911},
year = {2020},
issn = {0278-5846},
doi = {https://doi.org/10.1016/j.pnpbp.2020.109911},
url = {https://www.sciencedirect.com/science/article/pii/S0278584619310309},
author = {Lena Palaniyappan and Ali Al-Radaideh and Penny A. Gowland and Peter F. Liddle},
keywords = {Disorganisation, Thought disorder, Salience network, Cognitive control, Language network, Coherence},
abstract = {Background
Persistent formal thought disorder (FTD) is a core feature of schizophrenia. Recent cognitive and neuroimaging studies indicate a distinct mechanistic pathway underlying the persistent positive FTD (pFTD or disorganized thinking), though its structural determinants are still elusive. Using network-based cortical thickness estimates from ultra-high field 7-Tesla Magnetic Resonance Imaging (7T MRI), we investigated the structural correlates of pFTD.
Methods
We obtained speech samples and 7T MRI anatomical scans from medicated clinically stable patients with schizophrenia (n = 19) and healthy controls (n = 20). Network-based morphometry was used to estimate the mean cortical thickness of 17 functional networks covering the entire cortical surface from each subject. We also quantified the vertexwise variability of thickness within each network to quantify the spatial coherence of the 17 networks, estimated patients vs. controls differences, and related the thickness of the affected networks to the severity of pFTD.
Results
Patients had reduced thickness of the frontoparietal and default mode networks, and reduced spatial coherence affecting the salience and the frontoparietal control network. A higher burden of positive FTD related to reduced frontoparietal thickness and reduced spatial coherence of the salience network. The presence of positive FTD, but not its severity, related to the reduced thickness of the language network comprising of the superior temporal cortex.
Conclusions
These results suggest that cortical thickness of both cognitive control and language networks underlie the positive FTD in schizophrenia. The structural integrity of cognitive control networks is a critical determinant of the expressed severity of persistent FTD in schizophrenia.}
}
@article{ACHARJYA2022100647,
title = {A rough set, formal concept analysis and SEM-PLS integrated approach towards sustainable wearable computing in the adoption of smartwatch},
journal = {Sustainable Computing: Informatics and Systems},
volume = {33},
pages = {100647},
year = {2022},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2021.100647},
url = {https://www.sciencedirect.com/science/article/pii/S221053792100130X},
author = {D.P. Acharjya and Gladys Gnana {Kiruba B}},
keywords = {Rough set, Wearable computing, Path diagram, Composite reliability, Convergence, Discriminant validity},
abstract = {The rapid growth of sustainable computing towards the energy, power and environment seize an immense attention from bigger organizations to an individual life. Besides the world is advancing towards the digital mode and smartwatch is gaining its popularity because of additional importance to improve lifestyle. Moreover, it is not restricted to only time viewer rather paves a way in user's daily life. Therefore, it is highly cardinal in identifying the factors among consumers influencing the adoption of smartwatch in sustainable wearable computing. Traditional data modelling tools limited to technology acceptance model is used to this end. After all the study deals with user's behaviour that includes uncertainties and thus studying such problems using computational intelligence techniques is pivotal. In this research work we hybridize rough set, partial least square, and formal concept analysis to study smartwatch users adoption in wearable computing. Initially, the reliability and validity of the proposed model is analysed using structural equation modelling along with partial least square. Further, decision rules are generated using the rough set. Finally, important factors affecting the user's behavioural adoption towards sustainable wearable computing is discovered using formal concept analysis.}
}
@article{WEI202238,
title = {Promoter prediction in nannochloropsis based on densely connected convolutional neural networks},
journal = {Methods},
volume = {204},
pages = {38-46},
year = {2022},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2022.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S1046202322000846},
author = {Pi-Jing Wei and Zhen-Zhen Pang and Lin-Jie Jiang and Da-Yu Tan and Yan-Sen Su and Chun-Hou Zheng},
keywords = {Nannochloropsis, Promoter, Deep learning, Densely connected convolutional neural networks, Within-group scrambling},
abstract = {Promoter is a key DNA element located near the transcription start site, which regulates gene transcription by binding RNA polymerase. Thus, the identification of promoters is an important research field in synthetic biology. Nannochloropsis is an important unicellular industrial oleaginous microalgae, and at present, some studies have identified some promoters with specific functions by biological methods in Nannochloropsis, whereas few studies used computational methods. Here, we propose a method called DNPPro (DenseNet-Predict-Promoter) based on densely connected convolutional neural networks to predict the promoter of Nannochloropsis. First, we collected promoter sequences from six Nannochloropsis strains and removed 80% similarity using CD-HIT for each strain to yield a reliable set of positive datasets. Then, in order to construct a robust classifier, within-group scrambling method was used to generate negative dataset which overcomes the limitation of randomly selecting a non-promoter region from the same genome as a negative sample. Finally, we constructed a densely connected convolutional neural network, with the sequence one-hot encoding as the input. Compared with commonly used sequence processing methods, DNPPro can extract long sequence features to a greater extent. The cross-strain experiment on independent dataset verifies the generalization of our method. At the same time, T-SNE visualization analysis shows that our method can effectively distinguish promoters from non-promoters.}
}
@article{RAMAZAN2023101254,
title = {Students’ 2018 PISA reading self-concept: Identifying predictors and examining model generalizability for emergent bilinguals},
journal = {Journal of School Psychology},
volume = {101},
pages = {101254},
year = {2023},
issn = {0022-4405},
doi = {https://doi.org/10.1016/j.jsp.2023.101254},
url = {https://www.sciencedirect.com/science/article/pii/S0022440523000821},
author = {Onur Ramazan and Shenghai Dai and Robert William Danielson and Yuliya Ardasheva and Tao Hao and Bruce W. Austin},
keywords = {Reading self-concept, Perception of competence, PISA 2018, Machine learning, Multilevel modeling},
abstract = {Decades of research have indicated that reading self-concept is an important predictor of reading achievement. During this period, the population of emergent bilinguals has continued to increase within United States' schools. However, the existing literature has tended to examine native English speakers' and emergent bilinguals' reading self-concept in the aggregate, thereby potentially obfuscating the unique pathways through which reading self-concept predicts reading achievement. Furthermore, due to the overreliance of native English speakers in samples relating to theory development, researchers attempting to examine predictors of reading achievement may a priori select variables that are more aligned with native English speakers' experiences. To address this issue, we adopted Elastic Net, which is a theoretically agnostic methodology and machine learning approach to variable selection to identify the proximal and distal predictors of reading self-concept for the entire population; in our study, participants from the United States who participated in PISA 2018 served as the baseline group to determine significant predictors of reading self-concept with the intent of identifying potential new directions for future researchers. Based on Elastic Net analysis, 20 variables at the student level, three variables at the teacher level, and 12 variables at the school level were identified as the most salient predictors of reading self-concept. We then utilized a multilevel modeling approach to test model generalizability of the identified predictors of reading self-concept for emergent bilinguals and native English speakers. We disaggregated and compared findings for both emergent bilinguals and native English speakers. Our results indicate that although some predictors were important for both groups (e.g., perceived information and communications technologies competence), other predictors were not (e.g., competitiveness). Suggestions for future directions and implications of the present study are examined.}
}
@article{ZHANG2022108760,
title = {Causal discovery and inference-based fault detection and diagnosis method for heating, ventilation and air conditioning systems},
journal = {Building and Environment},
volume = {212},
pages = {108760},
year = {2022},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2022.108760},
url = {https://www.sciencedirect.com/science/article/pii/S0360132322000099},
author = {Chaobo Zhang and Yazhou Zhao and Yang Zhao and Tingting Li and Xuejun Zhang},
keywords = {Fault detection and diagnosis, Heating, Ventilation and air conditioning systems, Building energy conservation, Causal discovery and inference, Individual average causal effect estimation, Backward structural causal model},
abstract = {Data driven-based methods have aroused wide attention in the domain of fault detection and diagnosis of heating, ventilation and air conditioning systems. However, they are good at learning statistical relationships between faults and symptoms rather than their causal relationships, resulting in poor interpretability. This paper proposes a causal discovery and inference-based fault detection and diagnosis method to address this challenge. It applies a do-calculus-based individual average causal effect estimation approach to reveal causal relationships between faults and symptoms. Based on the causal relationships discovered, a backward structural causal model is developed for fault detection and diagnosis. Regression coefficients in the model are visualized using heatmaps to explain the model reasoning processes. The method is validated using the experimental data collected by the ASHARE project RP-1312. The individual average causal effect estimation approach reveals the causal relationships between eleven air handing unit faults and twenty-eight symptoms successfully. The diagnosis accuracy of the backward structural causal model (99.58%) is almost as high as that of k-nearest neighbors, support vector machine, classification and regression trees, deep neural networks and convolutional neural networks. Its hyper-parameter optimization time is reduced by 81.64% and 99.91%, respectively, compared with deep neural networks and convolutional neural networks. And its model training time is reduced by 38.61% and 92.01%, respectively. Based on the heatmap of regression coefficients of the model, it is demonstrated that the decision-making processes of the model are understandable and consistent with the domain knowledge in most cases.}
}
@article{PRATAMA2023338,
title = {WizardOfMath: A top-down puzzle game with RPG elements to hone the player's arithmetic skills},
journal = {Procedia Computer Science},
volume = {216},
pages = {338-345},
year = {2023},
note = {7th International Conference on Computer Science and Computational Intelligence 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.144},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922022220},
author = {Mulia Pratama and Yanfi Yanfi and Pualam Dipa Nusantara},
keywords = {Game, math, Game Development Life Cycle, Game Experience Questionnaire},
abstract = {As one of the important education subjects’ mathematics difficulties can lead to tension and be described as the most hated or feared subject. This study aims to create a puzzle game application with RPG elements called WizardOfMath to increase a user's interest in mathematics subject. The research method includes a method called Game Development Life Cycle (GDLC), which has a pre-production stage that is suitable for game development rather than the Waterfall method. A game application is built based on the prior requirement gathering. The evaluation was arranged using the Game Experience Questionnaire (GEQ) survey which is performed by providing an online form to the public. Reliability test of GEQ modules meets Cronbach's Alpha value above 0.7 and the validity test of the r table is greater than 0.05. The results calculation of the Game Experience Questionnaire (GEQ) from a total of 55 participants and 3 modular structures, which are the Core module, In-game Module, and Post-game Module obtained an average score of 4.06, 3.88, and 3.57 for positive aspects and 2,72, 2.67, and 2,61 for the negative aspect. The contribution of this study shows this puzzle game application with RPG elements decreased user tension and the negative effect of being involved with mathematics subjects.}
}
@incollection{NI2016239,
title = {Chapter 17 - More Intelligent Models},
editor = {Daiheng Ni},
booktitle = {Traffic Flow Theory},
publisher = {Butterworth-Heinemann},
pages = {239-251},
year = {2016},
isbn = {978-0-12-804134-5},
doi = {https://doi.org/10.1016/B978-0-12-804134-5.00017-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128041345000179},
author = {Daiheng Ni},
keywords = {Car-following models, Psycho-physical model, Carsim model, Rule-based model, Neural network model},
abstract = {Along the lines of car-following models, single-regime models stand at one end and use one equation to handle all driving situations. Models can become increasingly intelligent if they include more and more equations to represent different regimes, such as start-up, speedup, free-flow, approaching, following, and stopping. Even more intelligent models can mimic the way of human thinking—for example, using rules and reasoning based on neural networks.}
}
@article{KOHLER2016212,
title = {On GPU acceleration of common solvers for (quasi-) triangular generalized Lyapunov equations},
journal = {Parallel Computing},
volume = {57},
pages = {212-221},
year = {2016},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2016.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167819116300436},
author = {Martin Köhler and Jens Saak},
keywords = {Lyapunov equations, BLAS level-3, Accelerator device},
abstract = {The solutions of Lyapunov and generalized Lyapunov equations are a key player in many applications in systems and control theory. Their stable numerical computation, when the full solution is sought, is considered solved since the seminal work of Bartels and Stewart [R. H. Bartels, G. W. Stewart, Solution of the matrix equation AX+XB=C: Algorithm 432, Comm. ACM 15 (1972) 820–826.]. A number of variants of their algorithm have been proposed, but none of them goes beyond BLAS level-2 style implementation. On modern computers, however, the formulation of BLAS level-3 type implementations is crucial to enable optimal usage of cache hierarchies and modern block scheduling methods based on directed acyclic graphs describing the interdependence of single block computations. In this contribution, we present the port of our recent BLAS level-3 algorithm [M. Köhler, J. Saak, On BLAS Level-3 implementations of common solvers for (quasi-) triangular generalized Lyapunov equations, SLICOT Working Note 2014-1, NICONET e.V., available from www.slicot.org (Sep. 2014).] to a GPU accelerator device.}
}
@article{MA2024111918,
title = {Aggregate-aware model with bidirectional edge generation for medical image segmentation},
journal = {Applied Soft Computing},
volume = {163},
pages = {111918},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111918},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624006926},
author = {Shiqiang Ma and Xuejian Li and Jijun Tang and Fei Guo},
keywords = {Medical image segmentation, Multi-task learning, Edge generation, Aggregate-aware, Ensemble learning},
abstract = {Accurate segmentation of lesion areas plays an important role in medical imaging-assisted diagnosis and treatment. Accurate boundary information can help doctors develop precise surgical plans and improve patient prognosis. However, automatic segmentation methods often struggle to accurately segment edges due to the random shape, size, and location of regions of interest (ROI). This problem is compounded in medical images, where the difference in pixel intensity between foreground and background is significantly smaller than in natural images. In this study, we propose an aggregate-aware model with bidirectional edge generation (Ambeg) for medical image segmentation. To overcome the problem of blurred edges between foreground and background in medical images, we design a deep learning model via a multi-task learning strategy and obtain richer visual features to guide segmentation. Furthermore, an Edge Feature Fusion (EFF) module is developed to combine spatial correlation information of lesion edges between adjacent images for more accurate edge segmentation. Finally, we design a new evaluation metric, the Boundary DSC segmentation consistency measure, to evaluate the edge segmentation accuracy of medical image segmentation methods. We utilize dilation and erosion operations in morphological methods to construct lesion edge labels. In addition, we use expansion and erosion rates to regulate the dimensions of the edge region to assess the requirements of different diseases for edge segmentation accuracy. The proposed approach is particularly noteworthy for achieving state-of-the-art results on medical image segmentation datasets, including BraTS 2022 (MRI), BraTS 2020 (MRI) and COVID-19–20 (CT), which have different modalities of datasets. It has an impressive Hausdorff distance of 4.62 mm and a sensitivity score of 92.45 % on BraTS 2020. Compared with existing assessment methods such as Dice score, Boundary DSC segmentation consistency measure focuses on the hard-to-segment lesion edge region rather than the easy-to-segment lesion center region, which provides a more comprehensive reference for physicians to choose automatic segmentation methods. In addition, since the boundary shapes of medical images are complex and diverse, we utilize morphological methods to obtain the boundary labels to ensure the smoothness of the boundary. Moreover, the approach is easy to implement and has a low computational cost, making it an attractive option for practical medical imaging applications.}
}
@incollection{DOLIVEIRACOELHO2020259,
title = {Chapter 5.1 - Osteomics: Decision support systems for forensic anthropologists},
editor = {Zuzana Obertová and Alistair Stewart and Cristina Cattaneo},
booktitle = {Statistics and Probability in Forensic Anthropology},
publisher = {Academic Press},
pages = {259-273},
year = {2020},
isbn = {978-0-12-815764-0},
doi = {https://doi.org/10.1016/B978-0-12-815764-0.00005-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128157640000058},
author = {João {d’Oliveira Coelho} and Francisco Curate and David Navega},
keywords = {Biological profile, Machine learning, Population data, Web-based applications, Age at death, Sex diagnosis, Biogeographic origins, Body parameters, Medicolegal contexts, Cross validation},
abstract = {The popularity of web-based analytical tools with an emphasis on improved statistical analyses within the landscape of forensic anthropology is increasing. Osteomics is a web-based platform composed of a suite of forensic decision support systems designed to contend with the challenges posed by the estimation of the biological profile of human skeletal remains and particularly the estimation of age at death, the diagnosis of sex, the calculation of body parameters, and the prediction of biogeographic origin. The web applications designed at Osteomics intend to make innovative and reliable statistical models freely available. The suggested models are grounded around traditional and advanced statistical thinking, data visualization and processing, and predictive modeling under the machine learning paradigm. This paper aims to introduce the potential of the web platforms as forensic decision support systems and to give a detailed description of the statistical techniques used in the web-based applications available at Osteomics.}
}
@article{CHEEMA2022100123,
title = {Augmented Intelligence to Identify Patients With Advanced Heart Failure in an Integrated Health System},
journal = {JACC: Advances},
volume = {1},
number = {4},
pages = {100123},
year = {2022},
issn = {2772-963X},
doi = {https://doi.org/10.1016/j.jacadv.2022.100123},
url = {https://www.sciencedirect.com/science/article/pii/S2772963X22001739},
author = {Baljash Cheema and R. Kannan Mutharasan and Aditya Sharma and Maia Jacobs and Kaleigh Powers and Susan Lehrer and Firas H. Wehbe and Jason Ronald and Lindsay Pifer and Jonathan D. Rich and Kambiz Ghafourian and Anjan Tibrewala and Patrick McCarthy and Yuan Luo and Duc T. Pham and Jane E. Wilcox and Faraz S. Ahmad},
keywords = {advanced heart failure, artificial intelligence, augmented intelligence, electronic health record, integrated healthcare system, machine learning},
abstract = {Background
Timely referral for specialist evaluation in patients with advanced heart failure (HF) is a Class 1 recommendation. However, the transition from stage C HF to advanced or stage D HF often goes undetected in routine care, resulting in delayed referral and higher mortality rates.
Objectives
The authors sought to develop an augmented intelligence-enabled workflow using machine learning to identify patients with stage D HF and streamline referral.
Methods
We extracted data on HF patients with encounters from January 1, 2007, to November 30, 2020, from a HF registry within a regional, integrated health system. We created an ensemble machine learning model to predict stage C or stage D HF and integrated the results within the electronic health record.
Results
In a retrospective data set of 14,846 patients, the model had a good positive predictive value (60%) and low sensitivity (25%) for identifying stage D HF in a 100-person, physician-reviewed, holdout test set. During prospective implementation of the workflow from April 1, 2021, to February 15, 2022, 416 patients were reviewed by a clinical coordinator, with agreement between the model and the coordinator in 50.3% of stage D predictions. Twenty-four patients have been scheduled for evaluation in a HF clinic, 4 patients started an evaluation for advanced therapies, and 1 patient received a left ventricular assist device.
Conclusions
An augmented intelligence-enabled workflow was integrated into clinical operations to identify patients with advanced HF. Endeavors such as this require a multidisciplinary team with experience in design thinking, informatics, quality improvement, operations, and health information technology, as well as dedicated resources to monitor and improve performance over time.}
}
@article{RICH2018110,
title = {Participatory systems approaches for urban and peri-urban agriculture planning: The role of system dynamics and spatial group model building},
journal = {Agricultural Systems},
volume = {160},
pages = {110-123},
year = {2018},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2016.09.022},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X16305959},
author = {Karl M. Rich and Magda Rich and Kanar Dizyee},
keywords = {Urban agriculture, System dynamics, Spatial group model building, Participatory processes, Planning, Christchurch},
abstract = {Urban agriculture has become an important research theme in recent years. Over the past decade, a number of different, diverse value chains have been established in the urban areas of developed and developing countries alike, with increasing convergence in their motivations related to food security and livelihoods development, particularly for poor and disadvantaged segments of society. However, for urban agriculture to be sustainable as a livelihoods and resilience strategy will require decision-support tools that allow planners and participants alike to jointly develop strategies and assess potential leverage points within urban food value chains. In this paper, we argue that system dynamics (SD) models combined with participatory approaches have important roles in bridging this gap, though these will need to be adapted to the spatial influences that exist in urban settings. We first review elements of urban agriculture and some of the policy challenges faced in this growing phenomenon. We follow this by motivating the role of SD models in the context of urban agriculture and note their potential utility in overlaying quantitative models of urban food value chains alongside their land-use characteristics, highlighting the dynamic feedbacks between intensive processes within changing urban food systems and extensive processes associated with land-use and planning. From this background, we introduce the concept of spatial group model building (SGMB), which adapts standard group model building concepts to account for both the spatial context of urban agriculture and enables a spatially sensitive, participatory approach to qualitative and quantitative model building. We provide a qualitative proof-of-concept of SGMB principles and techniques in the context of describing the setting and dynamic issues facing organic urban agriculture value chains in Christchurch, New Zealand. Our approach fills an important space between participatory GIS practices and the development of complex spatial system dynamics models, infusing systems thinking principles to participatory processes, while showing a way to enhance the future development of quantitative spatial system dynamics models more generally.}
}
@article{LAWSON2013284,
title = {Sensory connection, interest/attention and gamma synchrony in autism or autism, brain connections and preoccupation},
journal = {Medical Hypotheses},
volume = {80},
number = {3},
pages = {284-288},
year = {2013},
issn = {0306-9877},
doi = {https://doi.org/10.1016/j.mehy.2012.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0306987712005415},
author = {Wendy Lawson},
abstract = {Does motivational interest increase gamma synchrony across neuronal networking to enable computation of related sensory inputs that might lead to greater social understanding in autism spectrum conditions (ASC)? Meaning, is it possible/likely that in autism because individuals process one aspect of sensory input at any one time (therefore missing the wider picture in general) when they are motivated/interested or attending to particular stimuli their attention window is widened due to increased gamma synchrony and they might be enabled to connect in ways that do not occur when they are not motivated? This is my current research question. If gamma synchrony is helping with the binding of information from collective sensory inputs, in ASC, when and only if the individual is motivated, then this has huge potential for how learning might be encouraged for individuals with an ASC.}
}
@article{URSINO2015234,
title = {A neural network for learning the meaning of objects and words from a featural representation},
journal = {Neural Networks},
volume = {63},
pages = {234-253},
year = {2015},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2014.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608014002639},
author = {Mauro Ursino and Cristiano Cuppini and Elisa Magosso},
keywords = {Semantic memory, Lexical memory, Conceptual representation, Hebb rule, Dominant features, Category formation},
abstract = {The present work investigates how complex semantics can be extracted from the statistics of input features, using an attractor neural network. The study is focused on how feature dominance and feature distinctiveness can be naturally coded using Hebbian training, and how similarity among objects can be managed. The model includes a lexical network (which represents word-forms) and a semantic network composed of several areas: each area is topologically organized (similarity) and codes for a different feature. Synapses in the model are created using Hebb rules with different values for pre-synaptic and post-synaptic thresholds, producing patterns of asymmetrical synapses. This work uses a simple taxonomy of schematic objects (i.e., a vector of features), with shared features (to realize categories) and distinctive features (to have individual members) with different frequency of occurrence. The trained network can solve simple object recognition tasks and object naming tasks by maintaining a distinction between categories and their members, and providing a different role for dominant features vs. marginal features. Marginal features are not evoked in memory when thinking of objects, but they facilitate the reconstruction of objects when provided as input. Finally, the topological organization of features allows the recognition of objects with some modified features.}
}
@article{LAFORCADE2010347,
title = {A Domain-Specific Modeling approach for supporting the specification of Visual Instructional Design Languages and the building of dedicated editors},
journal = {Journal of Visual Languages & Computing},
volume = {21},
number = {6},
pages = {347-358},
year = {2010},
note = {Special Issue on Visual Instructional Design Languages},
issn = {1045-926X},
doi = {https://doi.org/10.1016/j.jvlc.2010.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X10000492},
author = {Pierre Laforcade},
keywords = {Visual Instructional Design Languages Domain Specific Modeling, Visual and executable models},
abstract = {This paper presents, illustrates and discusses theories and practices about the application of a domain-specific modeling (DSM) approach to facilitate the specification of Visual Instructional Design Languages (VIDLs) and the development of dedicated graphical editors. Although this approach still requires software engineering skills, it tackles the need of building VIDLs allowing both visual models for human-interpretation purposes (explicit designs, communication, thinking, etc.) and machine-readable notations for deployment or other instructional design activities. This article proposes a theoretical application and a categorization, based on a domain-oriented separation of concerns of instructional design. It also presents some practical illustrations from experiments of specific DSM tooling. Key lessons learned as well as observed obstacles and challenges to deal with are discussed in order to further develop such an approach.}
}
@article{DURKIN2024108584,
title = {Surrogate-based optimisation of process systems to recover resources from wastewater},
journal = {Computers & Chemical Engineering},
volume = {182},
pages = {108584},
year = {2024},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2024.108584},
url = {https://www.sciencedirect.com/science/article/pii/S0098135424000024},
author = {Alex Durkin and Lennart Otte and Miao Guo},
keywords = {Surrogate modelling, Derivative-free optimisation, Resource recovery from wastewater},
abstract = {Wastewater systems are transitioning towards integrative process systems to recover multiple resources whilst simultaneously satisfying regulations on final effluent quality. This work contributes to the literature by bringing a systems-thinking approach to resource recovery from wastewater, harnessing surrogate modelling and mathematical optimisation techniques to highlight holistic process systems. A surrogate-based process synthesis methodology was presented to harness high-fidelity data from black box process simulations, embedding first principles models, within a superstructure optimisation framework. Modelling tools were developed to facilitate tailored derivative-free optimisation solutions widely applicable to black box optimisation problems. The optimisation of a process system to recover energy and nutrients from a brewery wastewater reveals significant scope to reduce the environmental impacts of food and beverage production systems. Additionally, the application demonstrates the capabilities of the modelling methodology to highlight optimal processes to recover carbon, nitrogen, and phosphorous resources whilst also accounting for uncertainties inherent to wastewater systems.}
}
@article{WANG2022103414,
title = {Cross-layer progressive attention bilinear fusion method for fine-grained visual classification},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103414},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103414},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002789},
author = {Chaoqing Wang and Yurong Qian and Weijun Gong and Junjong Cheng and Yongqiang Wang and Yuefei Wang},
keywords = {Fine-grained visual classification, Feature fusion, Attention, Progressive},
abstract = {Fine-grained visual classification (FGVC) is a critical task in the field of computer vision. However, FGVC is full of challenges due to the large intra-class variation and small inter-class variation of the classes to be classified on an image. The key in dealing with the problem is to capture subtle visual differences from the image and effectively represent the discriminative features. Existing methods are often limited by insufficient localization accuracy and insufficient feature representation capabilities. In this paper, we propose a cross-layer progressive attention bilinear fusion (CPABF in short) method, which can efficiently express the characteristics of discriminative regions. The CPABF method involves three components: 1) Cross-Layer Attention (CLA) locates and reinforces the discriminative region with low computational costs; 2) The Cross-Layer Bilinear Fusion Module (CBFM) effectively integrates the semantic information from the low-level to the high-level 3) Progressive Training optimizes the parameters in the network to the best state in a delicate way. The CPABF shows excellent performance on the four FGVC datasets and outperforms some state-of-the-art methods.}
}
@article{PHILLIPS200930,
title = {Fiber tractography reveals disruption of temporal lobe white matter tracts in schizophrenia},
journal = {Schizophrenia Research},
volume = {107},
number = {1},
pages = {30-38},
year = {2009},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2008.10.019},
url = {https://www.sciencedirect.com/science/article/pii/S0920996408004854},
author = {Owen R. Phillips and Keith H. Nuechterlein and Kristi A. Clark and Liberty S. Hamilton and Robert F. Asarnow and Nathan S. Hageman and Arthur W. Toga and Katherine L. Narr},
keywords = {Diffusion tensor imaging, White matter, Uncinate fasciculus, Inferior longitudinal fasciculus, Arcuate fasciculus, Fractional anisotropy},
abstract = {Diffusion tensor imaging (DTI) studies have demonstrated abnormal anisotropic diffusion in schizophrenia. However, examining data with low spatial resolution and/or a low number of gradient directions and limitations associated with analysis approaches sensitive to registration confounds may have contributed to mixed findings concerning the regional specificity and direction of results. This study examined three major white matter tracts connecting lateral and medial temporal lobe regions with neocortical association regions widely implicated in systems-level functional and structural disturbances in schizophrenia. Using DTIstudio, a previously validated regions of interest tractography method was applied to 30 direction diffusion weighted imaging data collected from demographically similar schizophrenia (n=23) and healthy control subjects (n=22). The diffusion tensor was computed at each voxel after intra-subject registration of diffusion-weighted images. Three-dimensional tract reconstruction was performed using the Fiber Assignment by Continuous Tracking (FACT) algorithm. Tractography results showed reduced fractional anisotropy (FA) of the arcuate fasciculi (AF) and inferior longitudinal fasciculi (ILF) in patients compared to controls. FA changes within the right ILF were negatively correlated with measures of thinking disorder. Reduced volume of the left AF was also observed in patients. These results, which avoid registration issues associated with voxel-based analyses of DTI data, support that fiber pathways connecting lateral and medial temporal lobe regions with neocortical regions are compromised in schizophrenia. Disruptions of connectivity within these pathways may potentially contribute to the disturbances of memory, language, and social cognitive processing that characterize the disorder.}
}
@article{PILLAY2024113656,
title = {Performance of Softcup® menstrual cup and vulvovaginal swab samples for detection and quantification of genital cytokines},
journal = {Journal of Immunological Methods},
volume = {528},
pages = {113656},
year = {2024},
issn = {0022-1759},
doi = {https://doi.org/10.1016/j.jim.2024.113656},
url = {https://www.sciencedirect.com/science/article/pii/S0022175924000413},
author = {Nashlin Pillay and Gugulethu Favourate Mzobe and Marothi Letsoalo and Asavela Olona Kama and Andile Mtshali and Stanley Nzuzo Magini and Nikkishia Singh and Vani Govender and Natasha Samsunder and Megeshinee Naidoo and Dhayendre Moodley and Cheryl Baxter and Derseree Archary and Sinaye Ngcapu},
keywords = {Cytokines, Softcup® menstrual cup, Vulvovaginal swab, Detection, Genital inflammation},
abstract = {Cytokines are important mediators of immunity in the female genital tract, and their levels may be associated with various reproductive health outcomes. However, the measurement of cytokines and chemokines in vaginal fluid samples may be influenced by a variety of factors, each with the potential to affect the sensitivity and accuracy of the assay, including the interpretation and comparison of data. We measured and compared cytokine milieu in samples collected via Softcup® menstrual cup versus vulvovaginal swabs. One hundred and eighty vulvovaginal swabs from CAPRISA 088 and 42 Softcup supernatants from CAPRISA 016 cohorts of pregnant women were used to measure the concentrations of 28 cytokines through multiplexing. Cytokines measured in this study were detectable in each of the methods however, SoftCup supernatants showed consistently, higher detectability, expression ratios, and mean concentration of cytokines than vulvovaginal swabs. While mean concentrations differed, the majority of cytokines correlated between SoftCup supernatants and vulvovaginal swabs. Additionally, there were no significant differences in a number of participants between the two sampling methods for the classification of genital inflammation. Our findings suggest that SoftCup supernatants and vulvovaginal swab samples are suitable for the collection of genital specimens to study biological markers of genital inflammatory response. However, the Softcup menstrual cup performs better for the detection and quantification of soluble biomarkers that are found in low concentrations in cervicovaginal fluid.}
}
@article{BARKE2023618,
title = {Linking life cycle sustainability assessment and the sustainable development goals – Calculation of goal achievement},
journal = {Procedia CIRP},
volume = {116},
pages = {618-623},
year = {2023},
note = {30th CIRP Life Cycle Engineering Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.02.104},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123000999},
author = {Alexander Barke and Manbir S. Sodhi and Christian Thies and Thomas S. Spengler},
keywords = {Sustainable development goals (SDGs), SDG quantification, Sustainable development, Life cycle sustainability assessment},
abstract = {In 2015, the United Nations General Assembly proposed seventeen Sustainable Development Goals (SDGs) intended to ensure sustainable development worldwide at the economic, environmental, and social levels. SDGs are now being used by some corporations in formulating and expressing business strategies. However, assessing the effects of corporate activities and products regarding their contribution to SDGs is difficult. In this paper, we have developed a method for linking life cycle sustainability assessment (LCSA) with the SDGs and calculating the contribution to SDG achievement. An essential part of this approach is the weighting of LCSA impact categories, which is typically done using equal weighting. This weighting method enables compensation of negative contributions by positive contributions in different impact categories but results in ambiguity in the results. This article identifies alternative weighting methods, integrates them into a computational approach, and determines their influence on the SDG contribution scores. The analysis shows that the use of alternative weights changes SDG contribution scores. However, the same product always has the highest SDG contribution score, regardless of the weighting method used. Nonetheless, the recommendations for action with regard to the total product alternatives would change depending on the weighting method.}
}
@article{OSCARIDO2023539,
title = {The impact of competitive FPS video games on human's decision-making skills},
journal = {Procedia Computer Science},
volume = {216},
pages = {539-546},
year = {2023},
note = {7th International Conference on Computer Science and Computational Intelligence 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.167},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922022451},
author = {Juan Oscarido and Zulfikar Airlangga Siswanto and Devin Akwila Maleke and Alexander Agung Santoso Gunawan},
keywords = {decision making, comparison strategy, video games, cognitive skill, influence-of-games, game-based learning},
abstract = {The problem we face today is that many people think that playing games only has a negative impact on a person's brain and behavior. but the fact is that playing games has a positive impact in many ways. The aim of this document is to prove whether video games can really influence human behavior on their decision-making skills. We will test 22 respondents directly who are teenagers and adults around 17 - 25 years old, and we will score them after they have finished playing games with the genre that we decided. The results proved that competitive First-person shooter (FPS) games increase human ability to make decisions quickly and correctly. Many of our participants agree that after playing competitive FPS games, they feel a positive impact on their cognitive skills. Our participants said that they can quickly compare the impact of the decisions they make and choose exactly which is the best course of action.}
}
@incollection{YACKINOUS2015193,
title = {Chapter 11 - Cellular Automata Investigations and Emerging Complex System Principles},
editor = {William S. Yackinous},
booktitle = {Understanding Complex Ecosystem Dynamics},
publisher = {Academic Press},
address = {Boston},
pages = {193-212},
year = {2015},
isbn = {978-0-12-802031-9},
doi = {https://doi.org/10.1016/B978-0-12-802031-9.00011-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128020319000115},
author = {William S. Yackinous},
keywords = {Cellular automata, Cellular automata investigations, Explicit experimentation, Cellular automata classes, Simple programs/simple rules, Complex system principles, Computational view of systems, Computational universality, Principle of Computational Equivalence},
abstract = {This chapter is primarily about Stephen Wolfram's innovative cellular automata investigations and his associated ideas on emerging complex system principles. The chapter begins with some cellular automata history and background, and then provides a description of Wolfram's cellular automata “explicit experimentation” work. The experimentation work shows that simple programs with simple rules, repeated over and over, can yield highly complex behavior. Wolfram has identified four classes of cellular automata. Those classes and their characteristics are discussed. The correspondence between cellular automata classes and the attractors of nonlinear dynamics theory is also discussed. Another of Wolfram's important insights is that the behavior of cellular automata is indicative of the behavior of systems in general. That idea is addressed in some detail. The latter part of the chapter addresses Wolfram's computational view of systems. The topics covered include computation as a framework for system principles, the concept of computational universality, and the identification of computationally universal cellular automata. Wolfram's Principle of Computational Equivalence is then described and discussed. The chapter concludes with a summary of my perspectives on the emerging complex system principles.}
}
@article{ZHANG2023103277,
title = {A security optimization scheme for data security transmission in UAV-assisted edge networks based on federal learning},
journal = {Ad Hoc Networks},
volume = {150},
pages = {103277},
year = {2023},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2023.103277},
url = {https://www.sciencedirect.com/science/article/pii/S157087052300197X},
author = {Han Zhang and Jianbin Xue and Qi Wang and Ye Li},
keywords = {Communication security, Mobile edge computing, Data offloading, Artificial intelligence, UAV communication},
abstract = {To solve the problem that mobile users cannot handle all data tasks by themselves due to the huge amount of data at present, this paper proposes a four-layer model of efficient and secure multiuser multitask computing offload. Since the user needs to offload the data to the server for processing through the wireless communication link, and users have mobility, it is difficult to achieve dynamic updates of user status and data information based on traditional cloud computing data processing methods, and it is difficult to ensure security during data unloading. In this paper, we studied how to realize the efficient and safe transmission of mobile users' data tasks when deploying Mobile Edge Computing (MEC) servers with artificial intelligence to assist in data processing on Unmanned Aerial vehicles (UAVs). First of all, to ensure the efficient use of computing resources, we use compression algorithms to reduce transmission overhead. Secondly, to enhance the security of the data transmission process, we have designed a security layer in the system model, which mainly uses secure multiparty technology to encrypt sensitive data, and a data transmission algorithm based on artificial intelligence is designed to assure the efficient and secure transmission of data in the network. Finally, through the analysis of the simulation results, we can know that the proposed strategy in this study has better performance than other existing strategies.}
}
@incollection{LUND202542,
title = {Boson Sampling},
editor = {Richard Szabo and Martin Bojowald},
booktitle = {Encyclopedia of Mathematical Physics (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {42-56},
year = {2025},
isbn = {978-0-323-95706-9},
doi = {https://doi.org/10.1016/B978-0-323-95703-8.00111-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323957038001117},
author = {A.P. Lund and T.C. Ralph},
keywords = {Boson Sampling, Complexity theory, Gaussian Boson sampling, Photons, Quantum Computing, Quantum Optics, Squeezing.},
abstract = {Boson sampling is an algorithm which produces samples from the probability distribution describing the linear scattering of multiple indistinguishable quantum particles exhibiting bosonic statistics. We outline why it is expected that a classical computer cannot efficiently run a boson sampling algorithm and describe the types of optical quantum devices that can. We review experimental progress and describe devices that have demonstrated a quantum advantage by producing samples faster than can be achieved with current super-computers.}
}
@article{RAVISHANKAR20211,
title = {Time dependent network resource optimization in cyber–physical systems using game theory},
journal = {Computer Communications},
volume = {176},
pages = {1-12},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.04.034},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421001857},
author = {Monica Ravishankar and Thompson Stephan and Thinagaran Perumal},
keywords = {Critical infrastructures, Cyber–physical systems, Game theory, Reinforcement learning technique, Linguistic fuzzy variables},
abstract = {The social and economic stability of a country is dependent on critical infrastructures (CIs) whose services range from financial to healthcare and power to transportation and communications. Most of these CIs are cyber–physical systems (CPSs), which integrate the network’s computational and communication capabilities to facilitate the monitoring and controlling of physical processes. Such systems are vulnerable to damage due to natural disasters, physical incidents, or cyber-attacks impacting the CPS organizations managing complex industrial control systems and data acquisition systems. When these CPSs are exposed to systemic cyber risks and cascaded network failures, network administrators need to recover from the compromise under limited resources. This is formulated as an attacker-defender game model to emulate the decision-making process in choosing an appropriate attack/defence mechanism in response to cybersecurity incidents using game theory. To further improve the assumptions made in the pure game-theoretic model, we relax the constraints on the rationality of the players, monetary payoff, and completeness of information by incorporating learning in games using reinforcement learning technique and compute the expected payoff using linguistic fuzzy variables.}
}
@article{SINGH2023103044,
title = {A survey of mobility-aware Multi-access Edge Computing: Challenges, use cases and future directions},
journal = {Ad Hoc Networks},
volume = {140},
pages = {103044},
year = {2023},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2022.103044},
url = {https://www.sciencedirect.com/science/article/pii/S1570870522002165},
author = {Ramesh Singh and Radhika Sukapuram and Suchetana Chakraborty},
keywords = {Mobility, Multi-access Edge Computing, Task offloading, Service migration, Content caching, Resource allocation},
abstract = {Many mobile and pervasive applications avail cloud services to reduce overheads in on-device computation. The performance of these services depends on the available bandwidth of the underlying network, the physical proximity of the cloud server and the end devices, the volume of data, the computational capacity of the server, and, importantly, the mobility of the devices hosting the applications. Edge computing promises to provide better performance by bringing services (e.g., a video streaming service) from the cloud to servers near the user. It also enables partial or full offloading of the computation (tasks) and storage functionalities from the User Equipment (UE) to the edge of the network. This saves power and benefits from relatively more powerful devices at the edge. Multi-access Edge Computing (MEC), which supports wireless and wired access technologies, has gained significant research interest. When UEs move, services must continue to operate, tasks may need to be offloaded again, and states related to tasks and services may need to be migrated. In this paper, we focus on four functional components (task/service offloading, resource allocation, content/task caching, and service/task migration) of MEC. We survey the challenges to these and their solutions in the context of UE mobility. Mobility creates challenges during offloading resource-intensive tasks as the user may move while the task is being offloaded. Some of the other challenges are how to jointly allocate computing and communication resources, minimize service down time during migration, and share the backhaul network if the same MEC host must continue to be used. Some key research areas include intelligent task offloading and service migration algorithms, exploiting group mobility to improve task migration time, studying the interplay of MEC parameters such as capabilities of the target MEC host, etc. In addition, predicting the mobile trajectory through intelligent methods and implementations with datasets from real-world scenarios are required. We compare this paper on 11 parameters (service migration, task offloading, resource allocation, content caching, mobility, use cases, architecture, computing paradigm, mobility model, system model, virtualization/Software Defined Networks) with 31 other survey papers from 2018 to April 2022 in MEC and related domains. We discuss the Edge Computing paradigm, the system architecture and model descriptions, and use cases. We briefly explain the relevant challenges and future directions in emerging domains, such as the Internet of drones and Digital twins. We also discuss future research directions in task/service migration, offloading, resource management, distributed computing, reliability, and Quality of Service, all related to mobility in MEC.}
}
@article{WERNER200782,
title = {Perspectives on the Neuroscience of Cognition and Consciousness},
journal = {Biosystems},
volume = {87},
number = {1},
pages = {82-95},
year = {2007},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2006.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0303264706000608},
author = {Gerhard Werner},
keywords = {Cognition, Consciousness, Metastability, Phase space, Coordination dynamics, Computation, Representation, Information},
abstract = {The origin and current use of the concepts of computation, representation and information in Neuroscience are examined and conceptual flaws are identified which vitiate their usefulness for addressing the problem of the neural basis of Cognition and Consciousness. In contrast, a convergence of views is presented to support the characterization of the Nervous System as a complex dynamical system operating in a metastable regime, and capable of evolving to configurations and transitions in phase space with potential relevance for Cognition and Consciousness.}
}
@article{RAY2020106679,
title = {A framework for probabilistic model-based engineering and data synthesis},
journal = {Reliability Engineering & System Safety},
volume = {193},
pages = {106679},
year = {2020},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2019.106679},
url = {https://www.sciencedirect.com/science/article/pii/S0951832018312754},
author = {Douglas Ray and Jose Ramirez-Marquez},
keywords = {Modeling and Simulation (M&S), Design of experiments (DOE), Deterministic computer experiments, Space filling designs, Uncertainty Quantification (UQ), Probabilistic optimization, Verification, Validation, Calibration, Trade space, Sensitivity analysis, Statistical engineering},
abstract = {Modern computing resources provide scientists, engineers, and system design teams the ability to study phenomena, such as system behavior, in a virtual setting. Computational modeling and simulation (M&S) enables engineers to avoid many of the challenges encountered in traditional design engineering, including the design, manufacture, and testing of expensive prototypes prior to having an optimized design. However, the use of M&S carries its own challenges, such as the computational time and resources required to execute effective studies, and uncertainties arising from simplifying assumptions inherent to computer models, which are intended to be an approximate representation of reality. In recent year advances have been made in a number of areas related to the efficient and reliable use of M&S for system evaluations, including design & analysis of computer experiments, uncertainty quantification, probabilistic analysis, response optimization, and data synthesis techniques. In this review paper, a general framework for systematically executing efficient M&S studies at the component-level, product-level, system-level, and system-of-systems-level is described. A case study is used to demonstrate how statistical and probabilistic techniques can be integrated with M&S to address those challenges inherent to model-based engineering, and how this aligns with the proposed workflow. The example is a gun-launch dynamics model of an artillery projectile developed by US Army engineers, and illustrates the application of this workflow in the study of subsystem system reliability, performance, and end-to-end system-level characterization.}
}
@article{ARNOLD2018581,
title = {Combining conscious and unconscious knowledge within human-machine-interfaces to foster sustainability with decision-making concerning production processes},
journal = {Journal of Cleaner Production},
volume = {179},
pages = {581-592},
year = {2018},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2018.01.070},
url = {https://www.sciencedirect.com/science/article/pii/S0959652618300787},
author = {Marlen Gabriele Arnold},
keywords = {Exploratory design, Structural systemic constellations, Cognitive human biases, HMI, Sustainable production contexts},
abstract = {At present, sustainability science is mainly based on conscious information and strongly focused on analytical tools or strategies. Neuroscience has made obvious that human decisions are prepared by the unconsciousness. Intuition plays an important role in early and late stages of learning processes and has a crucial impact on decision-making. Thus, intuitive and unconscious thinking is crucial for management processes in general and production planning processes in the main. However, unconscious knowledge and human behaviour is predominantly neglected in production research. Especially the addressing of human machine interfaces (HMI), human cognitive biases have a crucial impact on decision making processes. Constellation work is based on unconscious knowledge and intuition. Thus, systemic structural constellations are an innovative tool to integrate unconscious knowledge in a research context. In systemic structural constellations specific foci of complex systems, such as a production system, can be simulated and represented through spatial arrangements of persons or symbols. So, the method was used to reveal relevant patterns of relationships, structures, interaction, implicit knowledge, including hidden or underlying dynamics and influences that are relevant to and within a production system to understand how the raised problems in HMI can be better solved. The guiding research question is: How can the use of structural systemic constellations improve decision-making processes in HMI contexts in production environments in order to increase sustainability? Results show sustainability seems to be a matter of consciousness and is closely linked to the bias group not enough meaning. Sustainability and complexity resemble more than being linked by trade-offs. The recognition of human biases can be trained to improve human-machine-interfaces and sustainability. Constellation work contributes to decision theory by supporting effectuation.}
}
@article{WANG2023107152,
title = {scASGC: An adaptive simplified graph convolution model for clustering single-cell RNA-seq data},
journal = {Computers in Biology and Medicine},
volume = {163},
pages = {107152},
year = {2023},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2023.107152},
url = {https://www.sciencedirect.com/science/article/pii/S0010482523006170},
author = {Shudong Wang and Yu Zhang and Yulin Zhang and Wenhao Wu and Lan Ye and YunYin Li and Jionglong Su and Shanchen Pang},
keywords = {ScRNA-seq, Clustering, Bioinformatics, Graph convolution, Computational biology, Machine learning},
abstract = {Single-cell RNA sequencing (scRNA-seq) is now a successful technique for identifying cellular heterogeneity, revealing novel cell subpopulations, and forecasting developmental trajectories. A crucial component of the processing of scRNA-seq data is the precise identification of cell subpopulations. Although many unsupervised clustering methods have been developed to cluster cell subpopulations, the performance of these methods is vulnerable to dropouts and high dimensionality. In addition, most existing methods are time-consuming and fail to adequately account for potential associations between cells. In the manuscript, we present an unsupervised clustering method based on an adaptive simplified graph convolution model called scASGC. The proposed method builds plausible cell graphs, aggregates neighbor information using a simplified graph convolution model, and adaptively determines the most optimal number of convolution layers for various graphs. Experiments on 12 public datasets show that scASGC outperforms both classical and state-of-the-art clustering methods. In addition, in a study of mouse intestinal muscle containing 15,983 cells, we identified distinct marker genes based on the clustering results of scASGC. The source code of scASGC is available at https://github.com/ZzzOctopus/scASGC.}
}
@article{ZHANG202240,
title = {FPFS: Filter-level pruning via distance weight measuring filter similarity},
journal = {Neurocomputing},
volume = {512},
pages = {40-51},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.09.049},
url = {https://www.sciencedirect.com/science/article/pii/S092523122201164X},
author = {Wei Zhang and Zhiming Wang},
keywords = {Model compression, Neural network pruning, Distance and similarity, Deep convolutional neural network (DCNN)},
abstract = {Deep Neural Networks (DNNs) enjoy the welfare of convolution, while also bearing huge computational pressure. Therefore, model compression techniques are used to alleviate this problem, where filter-based neural network has received extensive attention as the research object of this paper. Common approaches treat filters as independent individuals and choose retrained filters by evaluating their performance, while more complex macro methods consider relationship between filters. Therefore, we propose a facile distance-based filter selection method, called FPFS, to visualize the similarity between filters from a global perspective. We calculate and sum the distance between filters to get filters’ “Distance Weight” which is applied as a metric to assess filters. We use four common and appropriate distances for filters evaluation. To verify the performance of our algorithm, we introduce FPFS to classical DCNNs and test it on general classification datasets CIFAR-10, CIFAR-100 and mageNet. For example, FPFS reduces Parameters and FLOPs of the lightweight model DenseNet-40 to about half of the original while maintain accuracy on CIFAR-10 by 94.40% (the original model is 94.80%). To ResNet-56 on CIFAR-100, FPFS compresses FLOPs to less than half of the original, while model accuracy reaches 71.46% (the original model is 71.44%). About ResNet-50 on ImageNet, FPFS achieves 60.3% FLOPs pruning rate accompanied by 0.96% top-1 accuracy loss. We also compare the experimental results with state-of-the-art filter pruning algorithms to highlight the effectiveness of FPFS.}
}
@article{CHEN2025100821,
title = {Analysis of facial recognition attendance technology based on artificial intelligence algorithms in political course e-learning teaching},
journal = {Entertainment Computing},
volume = {52},
pages = {100821},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100821},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124001897},
author = {Lu Chen},
keywords = {Artificial intelligence algorithms, Face recognition, Network Teaching, Attendance System},
abstract = {The lag in course construction has led to some teaching content and methods not being well adapted to the characteristics of online teaching. Collected student face data and advanced facial recognition algorithms to automatically recognize student avatars, ensuring the accuracy of each student’s identity. During the course, facial recognition attendance technology will automatically recognize students’ attendance status and record it, thereby obtaining attendance data at any time during the teaching process. The attendance records of students are generated in real-time and easily imported into the academic affairs system for management and statistics. By applying facial recognition attendance technology, teachers can understand students’ attendance in real-time and take timely measures to improve their learning enthusiasm. Students also use this technology to more conveniently sign in, reducing potential omissions and errors in the attendance process.}
}
@article{GAO201464,
title = {Unconscious processing modulates creative problem solving: Evidence from an electrophysiological study},
journal = {Consciousness and Cognition},
volume = {26},
pages = {64-73},
year = {2014},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2014.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1053810014000464},
author = {Ying Gao and Hao Zhang},
keywords = {Unconscious processing, Divergent thinking, Creative problem solving, Creativity, Event-related potential},
abstract = {Previous behavioral studies have identified the significant role of subliminal cues in creative problem solving. However, neural mechanisms of such unconscious processing remain poorly understood. Here we utilized an event-related potential (ERP) approach and sandwich mask technique to investigate cerebral activities underlying the unconscious processing of cues in creative problem solving. College students were instructed to solve divergent problems under three different conditions (conscious cue, unconscious cue and no-cue conditions). Our data showed that creative problem solving can benefit from unconscious cues, although not as much as from conscious cues. More importantly, we found that there are crucial ERP components associated with unconscious processing of cues in solving divergent problems. Similar to the processing of conscious cues, processing unconscious cues in problem solving involves the semantic activation of unconscious cues (N280–340) in the right inferior parietal lobule (BA 40), new association formation (P350–450) in the right parahippocampal gyrus (BA 36), and mental representation transformation (P500–760) in the right superior temporal gyrus (BA 22). The present results suggest that creative problem solving can be modulated by unconscious processing of enlightening information that is weakly diffused in the semantic network beyond our conscious awareness.}
}
@article{J2023105690,
title = {Deep learning based multi-labelled soil classification and empirical estimation toward sustainable agriculture},
journal = {Engineering Applications of Artificial Intelligence},
volume = {119},
pages = {105690},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.105690},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622006807},
author = {Padmapriya J. and Sasilatha T.},
keywords = {Soil classification, Q-HOG, SVM, Deep Neural Network, VGG16},
abstract = {Agriculture is the underlying occupation of the vast people in India and it is a major economic contribution. Soil is prime for the vital nutrient supply to the crops and its yield. Determination of the type of soil which comprises of the clay, sand and silt particles in the respective proportion is indeed significant for the suitable crop selection and to identify the weeds growth. The most commonly utilized soil determination methods were International Pipette method and Pressure-plate apparatus method. In this research work, multiclass soil classification using machine learning and deep learning models for the appropriate determination of the soil type as Multi-Stacking ensemble model and a novel feature selection algorithm Q-HOG is proposed; since the Artificial Intelligence has led to furtherance in the smart agriculture. Besides, the images are collected from the exploration site vriddhachalam along with the soil datasets will increase the classification accuracy. The deep learning models Recurrent Neural Network(RNN), Long Short Term Memory(LSTM), Gated Recurrent Unit(GRU) and VGG16 are considered and the comprehensive evaluation of these different deep learning architectures and also the machine learning algorithms such as Naïve-bayes, KNN, SVM are carried out and the obtained results are tabulated. Multi-stacking ensemble model for multi-classification is proposed with the Machine learning and deep learning algorithms and evaluated the performance with increased computation time. Among these models the proposed model outperformed in soil classification in-terms of accuracy as 98.96 percent, achieved precision as 96.14 percent, recall as 99.65 percent and the achieved F1-Score is 97.87 percent.}
}
@article{DEBOER2010502,
title = {Frame-based guide to situated decision-making on climate change},
journal = {Global Environmental Change},
volume = {20},
number = {3},
pages = {502-510},
year = {2010},
note = {Governance, Complexity and Resilience},
issn = {0959-3780},
doi = {https://doi.org/10.1016/j.gloenvcha.2010.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0959378010000245},
author = {Joop {de Boer} and J. Arjan Wardekker and Jeroen P. {van der Sluijs}},
keywords = {Climate change, Adaptation, Decision-making, Frames},
abstract = {The present paper describes a frame-based approach to situated-decision-making on climate change. Building on the multidisciplinary literature on the relationship between frames and decision-making, it argues that decision-makers may gain from making frames more explicit and using them for generating different visions about the central issues. Frames act as organizing principles that shape in a “hidden” and taken-for-granted way how people conceptualize an issue. Science-related issues, such as climate change, are often linked to only a few frames, which consistently appear across different policy areas. Indeed, it appears that there are some very contrasting ways in which climate change may be framed. These frames can be characterized in terms of a simple framework that highlights specific interpretations of climate issues. A second framework clarifies the built-in frames of decision tools. Using Thompson's two basic dimensions of decision, it identifies the main uncertainties that should be considered in developing a decision strategy. The paper characterizes four types of decision strategy, focusing on (1) computation, (2) compromise, (3) judgment, or (4) inspiration, and links each strategy to the appropriate methods and tools, as well as the appropriate social structures. Our experiences show that the frame-based guide can work as an eye-opener for decision-makers, particularly where it demonstrates how to add more perspectives to the decision.}
}
@article{MILLER2016102,
title = {Provision for income tax expense ASC 740: A teaching note},
journal = {Journal of Accounting Education},
volume = {35},
pages = {102-126},
year = {2016},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2015.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0748575115000858},
author = {Tad Miller and Lindsay Miller and Jeffrey Tolin},
keywords = {Provision for income tax expense, Accounting Standards Codification, Deferred tax assets, Deferred tax liabilities, Deductible temporary differences, Taxable temporary differences},
abstract = {This project requires students to think critically to synthesize concepts they learned in their financial reporting and tax classes. They will use and interpret accounting standards to prepare tax provisions, comparative financial statements and the appropriate footnote disclosures. Even a simple tax provision results in a challenging project.}
}
@article{BONSIGNORE2017298,
title = {Present and future approaches to lifetime prediction of superelastic nitinol},
journal = {Theoretical and Applied Fracture Mechanics},
volume = {92},
pages = {298-305},
year = {2017},
issn = {0167-8442},
doi = {https://doi.org/10.1016/j.tafmec.2017.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167844217300587},
author = {Craig Bonsignore}
}
@incollection{ZOHURI2022121,
title = {Chapter 5 - Mathematical modeling driven predication},
editor = {Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia},
booktitle = {Knowledge is Power in Four Dimensions: Models to Forecast Future Paradigm},
publisher = {Academic Press},
pages = {121-163},
year = {2022},
isbn = {978-0-323-95112-8},
doi = {https://doi.org/10.1016/B978-0-323-95112-8.00005-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323951128000052},
author = {Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia},
keywords = {Data mining and data analytics, Forecasting and prediction, Modeling and mathematics},
abstract = {During the past decade, there has been a tremendous blast and progress in computation technology, and with it comes vast amounts of data in a variety of fields such as the economy, medicine, biology, banking services such as customer relation management and credit card fraud, finance, demographic population growth from a demographical point of view nationwide and worldwide, and the need for new lifestyles and growth in term of continuous renewable sources of energy and its production, as well as marketing are among the fields that can be mentioned. The challenge of understanding these data has led to the development of new tools such as predictive analytics in the field of statistics and spawned new areas such as data mining, machine learning, and bioinformatics to process these data and determine the integrity of their information for prediction analysis. Many of these tools have common underpinnings but are often expressed with different terminology. This chapter will summarize the important ideas in these areas in a common conceptual framework.}
}
@article{SZYMANSKI201284,
title = {Information retrieval with semantic memory model},
journal = {Cognitive Systems Research},
volume = {14},
number = {1},
pages = {84-100},
year = {2012},
note = {Cognitive Systems Research: Special Issue on Modeling and Application of Cognitive Systems},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2011.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S1389041711000179},
author = {Julian Szymański and Włodzisław Duch},
abstract = {Psycholinguistic theories of semantic memory form the basis of understanding of natural language concepts. These theories are used here as an inspiration for implementing a computational model of semantic memory in the form of semantic network. Combining this network with a vector-based object-relation-feature value representation of concepts that includes also weights for confidence and support, allows for recognition of concepts by referring to their features, enabling a semantic search algorithm. This algorithm has been used for word games, in particular the 20-question game in which the program tries to guess a concept that a human player thinks about. The game facilitates lexical knowledge validation and acquisition through the interaction with humans via supervised dialog templates. The elementary linguistic competencies of the proposed model have been evaluated assessing how well it can represent the meaning of linguistic concepts. To study properties of information retrieval based on this type of semantic representation in contexts derived from on-going dialogs experiments in limited domains have been performed. Several similarity measures have been used to compare the completeness of knowledge retrieved automatically and corrected through active dialogs to a “golden standard”. Comparison of semantic search with human performance has been made in a series of 20-question games. On average results achieved by human players were better than those obtained by semantic search, but not by a wide margin.}
}
@article{SHIPLEY201948,
title = {Collaboration, cyberinfrastructure, and cognitive science: The role of databases and dataguides in 21st century structural geology},
journal = {Journal of Structural Geology},
volume = {125},
pages = {48-54},
year = {2019},
note = {Back to the future},
issn = {0191-8141},
doi = {https://doi.org/10.1016/j.jsg.2018.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0191814117303164},
author = {Thomas F. Shipley and Basil Tikoff},
keywords = {Spatial cognition, Cyberinfrastructure, Expert training},
abstract = {Structural geologists support their mind with tools, and these tools are increasingly computer based. The advent of Intelligent Systems will allow creation of research teams that combine the strengths of the human mind and computer processing to produce new research results. The efficacy of these approaches will require a solid grounding in cognitive science. Critical to this approach are databases, which are potentially transformative solely in their ability to allow access to data, in a primary form. Emerging more recently, however, is the concept of a dataguide, in which computer-aided analysis informs ongoing decisions about where and what data to collect. The creation of human and computer teams can expand the types of questions that can be addressed in structural geology and tectonics research, but it will take a community-based effort to understand the value of data to experts and how computers might aid an expert in the field.}
}
@article{WANG2023458,
title = {The Hutong neighbourhood grammar: A procedural modelling approach to unravel the rationale of historical Beijing urban structure},
journal = {Frontiers of Architectural Research},
volume = {12},
number = {3},
pages = {458-476},
year = {2023},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2022.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S2095263523000031},
author = {Yuyang Wang and Andrew Crompton and Asterios Agkathidis},
keywords = {Urban morphology, Siheyuan, Hutong neighbourhood, Procedural modelling, Shape grammar},
abstract = {Hutong neighbourhoods, composed of Chinese courtyard dwellings (Siheyuan), are historically and socially significant urban spaces that embody the traditional Chinese way of life and philosophy. As part of the national heritage, there is an increasing research interest in Hutong neighbourhoods, many of which are facing oblivion. This study presents a formal grammar for Hutong neighbourhood generation. This research investigates traditional principles of urban planning of ancient Beijing, based on examples on the historical map Qianlong Jingcheng Quantu, to derive the lost design rules. These rules are used to build up a procedural modelling framework, which reveals the development of Beijing's urban structure from the Yuan (1271–1368) to the Qing (1644–1911) dynasty. Our findings present a grammar incorporated into the procedural modelling framework to parametrically generate Hutong neighbourhoods, which replicates the morphological characteristics of historic cases. It contributes to the understanding of the generation of Hutong neighbourhoods. In support of heritage sustainability, this grammar can be implemented in a computational environment by visual scripting that enables the generation of new instances of Hutong neighbourhoods, both real and virtual.}
}
@article{LARSON201129,
title = {Interdisciplinary research training in a school of nursing},
journal = {Nursing Outlook},
volume = {59},
number = {1},
pages = {29-36},
year = {2011},
issn = {0029-6554},
doi = {https://doi.org/10.1016/j.outlook.2010.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0029655410004379},
author = {Elaine L. Larson and Bevin Cohen and Kristine Gebbie and Sarah Clock and Lisa Saiman},
abstract = {Although interdisciplinarity has become a favored model of scholarly inquiry, the assumption that interdisciplinary work is intuitive and can be performed without training is short-sighted. This article describes the implementation of an interdisciplinary research training program within a school of nursing. We describe the key elements of the program and the challenges we encountered. From 2007-2010, eleven trainees from 6 disciplines have been accepted into the program and 7 have completed the program; the trainees have published 12 manuscripts and presented at 10 regional or national meetings. The major challenge has been to sustain and “push the envelope” toward interdisciplinary thinking among the trainees and their mentors, and to assure that they do not revert to their “safer” disciplinary silos. This training program, funded by National Institute of Nursing Research (NINR), has become well-established within the school of nursing and across the entire University campus, and is recognized as a high quality research training program across disciplines, as exemplified by excellent applicants from a number of disciplines.}
}
@article{DELEON2021281,
title = {Assessing the Efficacy of Tier 2 Mathematics Intervention for Spanish Primary School Students},
journal = {Early Childhood Research Quarterly},
volume = {56},
pages = {281-293},
year = {2021},
issn = {0885-2006},
doi = {https://doi.org/10.1016/j.ecresq.2021.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0885200621000508},
author = {Sara C. {de León} and Juan E. Jiménez and Nuria Gutiérrez and Juan Andrés Hernández-Cabrera},
keywords = {RtI model, math, early grades, Tier 2, at-risk},
abstract = {This study explored the efficacy of a Tier 2 intervention within the context of the Response to Intervention (RtI) model implemented by Spanish first- to third-grade primary school teachers to improve at-risk students’ early math skills. Teachers were instructed in the administration of a math curriculum-based measure composed of 5 isolated measures (quantity discrimination, missing number, single-digit computation, multidigit computation, and place value) to identify at-risk students and to monitor their progress; and in the implementation of a systematic and explicit instructional program to improve basic math skills in at-risk students. Implementation fidelity was analyzed using direct observations and self-reports. The intervention was conducted with adequate fidelity and had a significant positive impact on all grades. Significant differences were found between experimental and control students at risk of math failure in the improvement rate of quantity discrimination, missing number, and place value in all grades. Experimental at-risk students showed a monthly improvement, assessed using a combination of screening and progress monitoring measures. In conclusion, Spanish first to third graders at risk of math failure benefited from a Tier 2 intervention based on basic math skills, implemented by in-service teachers.}
}
@article{WEIGL20231,
title = {Modelling learning for a better safety culture within an organization using a virtual safety coach: Reducing the risk of postpartum depression via improved communication with parents},
journal = {Cognitive Systems Research},
volume = {80},
pages = {1-36},
year = {2023},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723000153},
author = {Linn-Marie Weigl and Fakhra Jabeen and Jan Treur and H. Rob Taal and Peter H.M.P. Roelofsma},
keywords = {Shared mental models, virtual AI Coach in healthcare, Fathers/psychology, Depressive disorders/complications, Postpartum depression},
abstract = {This paper describes an extension of a safety culture within hospital organizations providing more transparency and acknowledgement of all actors, and in particular the parents. It contributes a model architecture to support a hospital to develop such an extended safety culture. It is illustrated for prevention of postpartum depression. Postpartum depression is a commonly known consequence of childbirth for both mothers and fathers. In this research, we computationally analyze the risk factors and lack of support received by fathers. Therefore, we use shared mental models to model the effects of poor and additional communication by healthcare practitioners to mitigate the development of postpartum depression in both the mother and the father. Both individual mental models and shared mental models are considered in the design of the computational model. The paper illustrates the benefits of simple support in terms of communication during childbirth, which has lasting effects, even outside the hospital. For the impact of additional communication, a Virtual Safety Coach is designed that intervenes when necessary to provide support, i.e., when a health care practitioner doesn’t. Moreover, organizational learning is also modelled to improve the mental models of both the Safety Coach and the Health Care Practitioner.}
}
@article{LU2017138,
title = {Quasi-generalized least squares regression estimation with spatial data},
journal = {Economics Letters},
volume = {156},
pages = {138-141},
year = {2017},
issn = {0165-1765},
doi = {https://doi.org/10.1016/j.econlet.2017.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0165176517301441},
author = {Cuicui Lu and Jeffrey M. Wooldridge},
keywords = {Quasi-GLS, Spatial correlation, Covariance tapering, Spatial HAC estimator},
abstract = {We use a particular quasi-generalized least squares (QGLS) approach to study a linear regression model with spatially correlated error terms. The QGLS estimator is consistent, asymptotically normal, computationally easier than GLS, and it appears to not lose much efficiency. A variance–covariance estimator for QGLS, which is robust to heteroskedasticity, spatial correlation and general variance–covariance misspecification is provided.}
}
@article{LEGLEITER20131,
title = {Introduction to the special issue: The field tradition in geomorphology},
journal = {Geomorphology},
volume = {200},
pages = {1-8},
year = {2013},
note = {The Field Tradition in Geomorphology 43rd Annual Binghamton Geomorphology Symposium, held 21-23 September 2012 in Jackson, Wyoming USA},
issn = {0169-555X},
doi = {https://doi.org/10.1016/j.geomorph.2013.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0169555X13003140},
author = {Carl J. Legleiter and Richard A. Marston},
keywords = {Binghamton Geomorphology Symposium, Preface, Field work, Jackson Hole, Wyoming},
abstract = {In recognition of the critical role of field observations in the ongoing development of our discipline, the 43rd annual Binghamton Geomorphology Symposium (BGS) celebrated The Field Tradition in Geomorphology. By organizing a conference devoted to this theme, we sought to honor the contributions of pioneering, field-based geomorphologists and to encourage our community to contemplate how field work might continue to provide unique insight into a new, more technologically-driven era. For example, given recent advances in remote sensing methods such as LiDAR, what kind of added value can field work provide? Similarly, how can field-based studies contribute to societally relevant, large-scale questions related to climate change and sustainable management of the Earth system? Motivated by such questions, the 2012 BGS was convened in Jackson Hole, WY, a new, Western location that enabled participation by Rocky Mountain and west coast research groups underrepresented at previous Binghamton symposia. Also, in keeping with the field tradition theme, the 2012 BGS emphasized field trips, including a rafting excursion down the Snake River and an overview of the tectonic and glacial history of Jackson Hole. The on-site portion of the symposium consisted of invited oral and poster presentations and contributed posters, including many by graduate students. Topics ranged from an historical overview of the development of geomorphic thinking to long-term sediment tracer studies to a commentary on the synergy between LiDAR and field mapping. This special issue of Geomorphology consists of papers by invited authors from the 2012 BGS, and this overview provides some context for these contributions. Looking forward, we hope that the 43rd annual BGS will stimulate further discussion of the role of field work as the discipline of geomorphology continues to evolve, carrying on the field tradition into the future.}
}
@article{MURAI2024100631,
title = {Making as an opportunity for classroom assessment: Canadian maker educators’ views on assessment},
journal = {International Journal of Child-Computer Interaction},
volume = {39},
pages = {100631},
year = {2024},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2023.100631},
url = {https://www.sciencedirect.com/science/article/pii/S2212868923000685},
author = {Yumiko Murai and A. {Yulis San Juan}},
keywords = {Maker education, School-based making, Maker classrooms, Classroom assessment, Curriculum change, Case study},
abstract = {A growing number of studies have shown that the exploratory, collaborative, and contextualized nature of maker activities create opportunities for learners to engage with knowledge in a uniquely different way from traditional education which largely relies on de-contextualized instructions. The increased integration of making into K-12 curricula has enormous implications not only for instructional design but also for assessment practices. Maker-oriented activities have the potential to shed light on types of learning that previous assessment systems have not captured and examined. Nevertheless, little is discussed on how making can contribute to the assessment and instructional practices at large. This case study investigated educators' experiences with assessment in classrooms integrating maker activities. Through a qualitative analysis of interviews with six K-12 educators in Canada, the researchers examined: (1) in what ways does making activities create opportunities for assessment and instruction in K-12 classrooms? (2) in what ways does maker learning become a challenge for assessment and instruction in K-12 classrooms? Our analysis revealed several ways in which teachers experienced the advantages of the making approach for understanding student learning and for helping students become further aware of their own progress. The results also revealed challenges to conducting assessments for maker learning, including administrative challenges like continuing to gain support from the administration, and literacy challenges such as students’ obsession with letter grades. This study provides insights into how making may help improve assessment and instructional practices in K-12 classrooms.}
}
@article{ALIANOFILHO2024122437,
title = {An effective approach for bi-objective multi-period touristic itinerary planning},
journal = {Expert Systems with Applications},
volume = {240},
pages = {122437},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122437},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423029391},
author = {Angelo {Aliano Filho} and Reinaldo Morabito},
keywords = {Touristic itinerary planning, Multi-objective optimization, Routing and scheduling problem, MIP-heuristic, Trade-off analysis},
abstract = {Planning effective itineraries for tourists is a major problem that has been gaining attention over the last years. This paper proposes a new bi-objective integer linear programming model for this problem. Decisions include the choice of the best itinerary to be performed considering multi-period routing, time windows for the visited attractions and the choice of restaurants and hotels. The conflicting objectives considered are: (i) maximizing the level of service offered by the itinerary, and (ii) minimizing the total distance traveled. The problem resolution, even for small instances by exact methods, is limited. This motivated the proposition of a new customized MIP-heuristic based on decomposition, fix-and-optimize and MIP-start, to produce good-quality solutions with moderate computational effort. Tchebycheff’s scalarization method was coupled to this heuristic and multiple compromise solutions were obtained. Extensive results with problem instances of different sizes and characteristics showed a good performance of this approach, capable of producing effective solutions within short runtimes. The analysis of the solutions indicated a strong conflict between the objectives, allowing the user to quantify the losses and gains when one criterion is prioritized over the other. A brief sensitivity analysis of some model parameters revealed interesting managerial insights. Some examples include quantifying the negative impacts in terms of the level of service offered by concentrating hotels and restaurants in the center of tourist attractions, increasing visit and transfer times between attractions and reducing the planning horizon for the entire itinerary. These aspects validate the potential of using this MIP model and applying this MIP-heuristic in real situations.}
}
@article{MONTESERRAT202130,
title = {Interpretability in neural networks towards universal consistency},
journal = {International Journal of Cognitive Computing in Engineering},
volume = {2},
pages = {30-39},
year = {2021},
issn = {2666-3074},
doi = {https://doi.org/10.1016/j.ijcce.2021.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S266630742100005X},
author = {Dionéia Motta Monte-Serrat and Carlo Cattani},
keywords = {Intelligent systems, Interpretability, Language semantics, Universal consistency},
abstract = {In the challenge of Artificial Intelligence in processing semantically evaluable information, the application of deep learning techniques depends not only on the algorithms, but also on the principles that explain how they work. The malfunction of a machine learning system, ML, can occur due lack of knowledge of the algorithm intended behavior. The difficulty in debugging ML can be overcome by using strategies based on the universal structure of language that overlaps in the cognitive architecture of biological and intelligent systems. The appropriate choice of an algorithm inspired by the functioning of human language offers the computational scientist methodological strategies to clarify its performance analysis to optimize the interpretative activity under the good instrumentation of the system and to reach the performance level of an application considered safe. Neurolinguistic principles that link interpretation to language and cognition; the semantic dimension that arises not only from the linguistic system, but also from the context in which the information is produced; and the theoretical bases for understanding language as a 'form' (process) and not as a substance (set of signs) provide the groundwork for the intelligent systems’ improvement so that they have universal consistency and lessen the effects of the ‘curse of dimensionality’ or of the bias in the interpretation by the system. Semantics and statistics are considered to understand universal consistency as opposed to ideal consistency when evaluating a data set, since training alone is not sufficient to avoid data manipulation. We conclude that the 'key' for a good information classifier to achieve an acceptable performance of neural networks is in the dynamic aspect of language (language as a form / process) that: Guides the apprehension of how neural networks have access to weights (values); replicates this for intelligent systems making them invariant to many input transformations and guarantees an infinite amount of finite sample information, avoiding semantic distortion.}
}
@article{GAO2019242,
title = {Expert knowledge recommendation systems based on conceptual similarity and space mapping},
journal = {Expert Systems with Applications},
volume = {136},
pages = {242-251},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419304130},
author = {Li Gao and Kun Dai and Liping Gao and Tao Jin},
keywords = {Conceptual similarity, Space mapping, Core resource database (CRD), Institutional repository (IR), Expert Knowledge Recommendation System (EKRS)},
abstract = {The semantic analysis method of structured big data generated based on human knowledge is important in expert recommendation systems and scientific and technological information analysis. In these fields, the most important problem is the calculation of concept similarity. The study aims to explore the spatial mapping relationship between the general knowledge base and the professional knowledge base for the application of the general knowledge map in professional fields. With the core resource database (CRD) as the main body of the general knowledge and the institutional repository (IR) as the main body of the professional knowledge, the conceptual features of institutional expert knowledge were firstly abstracted from IR and inferred from small-scale datasets and the mathematical model was established based on the similarity of text concepts and related ranking results. Then, a two-set concept space mapping algorithm between CRD and IR was designed. In the algorithm, the more granular concept nodes were extracted from the information on the shortest paths among concepts to obtain a new knowledge set, the Expert Knowledge Recommendation System (EKRS). Finally, the simulation experiment was carried out with open datasets to verify the algorithm. The simulation results showed that the algorithm reduced the structural complexity in the calculation of large datasets. The proposed system model had a clear knowledge structure and the recommended accuracy of the text similarity was high. For small-scale knowledge base datasets with different sparsity, the system showed the stable performance, indicating the better convergence and robustness of the algorithm.}
}
@article{LI2024102009,
title = {BERT-based transfer learning in tacit knowledge externalization: A study case of history teachers},
journal = {Learning and Motivation},
volume = {87},
pages = {102009},
year = {2024},
issn = {0023-9690},
doi = {https://doi.org/10.1016/j.lmot.2024.102009},
url = {https://www.sciencedirect.com/science/article/pii/S0023969024000511},
author = {Guang Li and Linkai Zhu and Fangfang Liu and Zhiming Cai and Yiyun Wang and Ruichen Gao},
keywords = {Transfer Learning, Classification, Tacit Knowledge Externalization},
abstract = {There has been significant progress in the field of transfer learning. However, there are still issues with inconsistent results in professional domain applications, with low-resource learning being a considerable problem. This paper proposes a language processing model for historical education built using BERT's pre-training techniques. Two experiments were conducted to obtain comparative results and choose the appropriate model method for explicating implicit expertise in secondary school history teaching. It compares traditional methods, represented by naive Bayes, to popular continuation pre-processing techniques such as domain adaptive learning and task adaptive learning to improve the effectiveness of transfer learning. Finally, this study builds targeted models based on real application needs and selects professional rules consistent with the scene application. The use of continued pre-training helps to enhance the accuracy of the professional domain model.}
}
@article{BIENVENU202049,
title = {On low for speed oracles},
journal = {Journal of Computer and System Sciences},
volume = {108},
pages = {49-63},
year = {2020},
issn = {0022-0000},
doi = {https://doi.org/10.1016/j.jcss.2019.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0022000018305828},
author = {Laurent Bienvenu and Rod Downey},
keywords = {Oracle computations, Lowness for speed},
abstract = {Relativizing computations of Turing machines to an oracle is a central concept in the theory of computation, both in complexity theory and in computability theory(!). Inspired by lowness notions from computability theory, Allender introduced the concept of “low for speed” oracles. An oracle A is low for speed if relativizing to A has essentially no effect on computational complexity, meaning that if a decidable language can be decided in time f(n) with access to oracle A, then it can be decided in time poly(f(n)) without any oracle. The existence of non-computable such A's was later proven by Bayer and Slaman, who even constructed a computably enumerable one, and exhibited a number of properties of these oracles. In this paper, we pursue this line of research, answering the questions left by Bayer and Slaman and give further evidence that the class of low for speed oracles is a very rich one.}
}
@article{WITTKUHN2021367,
title = {Replay in minds and machines},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {129},
pages = {367-388},
year = {2021},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2021.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0149763421003444},
author = {Lennart Wittkuhn and Samson Chien and Sam Hall-McMaster and Nicolas W. Schuck},
keywords = {Replay, Reinforcement learning, Machine learning, Representation learning, Decision-making},
abstract = {Experience-related brain activity patterns reactivate during sleep, wakeful rest, and brief pauses from active behavior. In parallel, machine learning research has found that experience replay can lead to substantial performance improvements in artificial agents. Together, these lines of research suggest that replay has a variety of computational benefits for decision-making and learning. Here, we provide an overview of putative computational functions of replay as suggested by machine learning and neuroscientific research. We show that replay can lead to faster learning, less forgetting, reorganization or augmentation of experiences, and support planning and generalization. In addition, we highlight the benefits of reactivating abstracted internal representations rather than veridical memories, and discuss how replay could provide a mechanism to build internal representations that improve learning and decision-making.}
}
@article{RUPNOW2024101193,
title = {Snapshots of sameness: Characterizations of mathematical sameness across student groups},
journal = {The Journal of Mathematical Behavior},
volume = {76},
pages = {101193},
year = {2024},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2024.101193},
url = {https://www.sciencedirect.com/science/article/pii/S0732312324000701},
author = {Rachel Rupnow and Rosaura Uscanga and Anna Marie Bergman and Cassandra Mohr},
keywords = {Sameness, Big Ideas, Undergraduate Students, Graduate Students},
abstract = {Sameness is foundational to mathematics but has only recently become an area of focus in mathematics education research. In this paper, we describe characterizations of sameness generated by four student groups: discrete mathematics students, linear algebra students, abstract algebra students, and graduate students. Based on qualitative analysis of open response surveys, we compare these groups’ characterizations of sameness; note the subcomponents discussed and variation within each dimension; and highlight experiences influential to students’ perceptions of sameness. Findings include interpretability of sameness as a big idea, nascent development of thematic connections across courses, emphases on current course material rather than connections to prior courses for students solicited from a particular course, greater reflectiveness from the graduate student group, and abstract algebra as an impactful course. Implications include a need for thoughtful examinations of how “big ideas” develop among students and what experiences might support such development.}
}