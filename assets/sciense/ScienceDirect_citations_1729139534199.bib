@article{FRANZ1994433,
title = {A critical framework for methodological research in architecture},
journal = {Design Studies},
volume = {15},
number = {4},
pages = {433-447},
year = {1994},
issn = {0142-694X},
doi = {https://doi.org/10.1016/0142-694X(94)90006-X},
url = {https://www.sciencedirect.com/science/article/pii/0142694X9490006X},
author = {Jill M. Franz},
keywords = {methodological research, critical framework, architecture, design},
abstract = {This paper reviews a cross-section of methodological studies undertaken in architecture since the Second World War. Despite a variety of orientations, technically, conceptually and philosophically, most studies reflect an understanding of people and objects as discrete entities interacting in an passive and unilateral manner. This dominant dualist understanding is concluded to be the essential cause of the ‘implementation gap' between architectural research and practice. For the gap to close, the development and institution of a critical framework is needed which encourages researchers to acknowledge explicitly the ontological and epistomological issues associated with architectural practice, education and research. Underlying this recommendation is a dialectic appreciation of person-world interaction; one which accepts as a holistic theme for inquiry, the experiential and interpretative quality of human thinking, feeling and action.}
}
@incollection{BIR202197,
title = {Chapter Four - Generic quantum hardware accelerators for conventional systems},
editor = {Shiho Kim and Ganesh Chandra Deka},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {122},
pages = {97-133},
year = {2021},
booktitle = {Hardware Accelerator Systems for Artificial Intelligence and Machine Learning},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2021.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0065245821000322},
author = {Parth Bir},
keywords = {Quantum mechanics, Computational basis, State space, Deterministic model, Probabilistic model, QA, GQHA},
abstract = {Quantum mechanics proposes, universe is a sum of a generic building block. Different orientation (i.e., angle, phase, amplitude, etc.) and summation of blocks forms entities. When differentiated, building blocks used for formation of entity are termed as basis. Following computational theory, these basis are termed as computational basis. Classical computers possess binary basis. Quantum system possess exponential computational power because of infinite computational basis. When computing solution to a problem, it's found in state space. Deterministic model (Conventional) requires both correct and incorrect solution set. For problems of probabilistic nature with plenty of variables (NP and P problems), computing solution requires exponential time, as entire state space is scanned. Furthermore, if solution is incomputable, the computation will never complete as solution is missing from both sets. Probabilistic model (Quantum) conducts a guided state space search and possess greater information carrying capacity per bit. Therefore, Quantum Accelerators (QA) are ideal for solving such problems. Resulting implementation of a Generic Quantum Hardware Accelerator (GQHA) is described via algorithms, mathematical models and microarchitecture. Next, a competitive industrial analysis and virtual implementation in a cloud environment is defined. Finally, it's proven that GQHA can replace conventional accelerators to produce faster and reliable results.}
}
@article{TANG2019101065,
title = {Addressing cascading effects of earthquakes in urban areas from network perspective to improve disaster mitigation},
journal = {International Journal of Disaster Risk Reduction},
volume = {35},
pages = {101065},
year = {2019},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2019.101065},
url = {https://www.sciencedirect.com/science/article/pii/S2212420918308720},
author = {Pan Tang and Qi Xia and Yueyao Wang},
keywords = {Earthquakes, Urban areas, Cascading effects, Disaster mitigation, Disaster chains, Social network analysis},
abstract = {Given the rising size and complexity of urban areas, the city governments are faced to the challenges of cascading effects triggered by devastating earthquakes, in which the disastrous consequences are amplified significantly by combined effects of the occurred secondary events with interrelationships on the elements at risks. As a low-probability and high impact natural disaster, the escalation of secondary events are guided by the vulnerability paths, as well as their interconnections should be considered from system perspectives during the preparedness and mitigation process. This research aims to develop, model and analyze cascading effects scenario of earthquakes in urban areas for supporting decision making in disaster risk reduction. A framework for addressing cascading effects of earthquakes in urban area is presented. The procedure for developing cascading effects scenario of such highly complex and uncertain disasters by identifying the triggered disaster chains is introduced. A directed network was built to model and visualize the secondary events with interrelationships involving in the cascading effects scenario. In particular, a range of network metrics are developed to examine the relational patterns of hazardous events based on Social Network Analysis. Together with, how to design disaster mitigation strategies according to network analysis results is introduced, such as disaster chains with priorities to be blocked, hazardous events to be mitigated firstly, and essential collaborative relationships among the responsible organizations. Furthermore, a case study in an urban area in Shenzhen City, China was conducted to highlight the application of the proposed framework. This research presents an innovative approach to address cascading effects in urban areas of earthquakes by developing the triggered worst case scenario, as well as understanding secondary events with interrelationships using network analysis method for providing insights to design disaster mitigation strategies from system thinking perspectives.}
}
@article{BALE2015150,
title = {Energy and complexity: New ways forward},
journal = {Applied Energy},
volume = {138},
pages = {150-159},
year = {2015},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2014.10.057},
url = {https://www.sciencedirect.com/science/article/pii/S0306261914011076},
author = {Catherine S.E. Bale and Liz Varga and Timothy J. Foxon},
keywords = {Complexity science, Energy systems, Modelling, Complex adaptive systems, Agent-based modelling, Energy policy},
abstract = {The purpose of this paper is to review the application of complexity science methods in understanding energy systems and system change. The challenge of moving to sustainable energy systems which provide secure, affordable and low-carbon energy services requires the application of methods which recognise the complexity of energy systems in relation to social, technological, economic and environmental aspects. Energy systems consist of many actors, interacting through networks, leading to emergent properties and adaptive and learning processes. Insights on these type of phenomena have been investigated in other contexts by complex systems theory. However, these insights are only recently beginning to be applied to understanding energy systems and systems transitions. The paper discusses the aspects of energy systems (in terms of technologies, ecosystems, users, institutions, business models) that lend themselves to the application of complexity science and its characteristics of emergence and coevolution. Complex-systems modelling differs from standard (e.g. economic) modelling and offers capabilities beyond those of conventional models, yet these methods are only beginning to realize anything like their full potential to address the most critical energy challenges. In particular there is significant potential for progress in understanding those challenges that reside at the interface of technology and behaviour. Some of the computational methods that are currently available are reviewed: agent-based and network modelling. The advantages and limitations of these modelling techniques are discussed. Finally, the paper considers the emerging themes of transport, energy behaviour and physical infrastructure systems in recent research from complex-systems energy modelling. Although complexity science is not well understood by practitioners in the energy domain (and is often difficult to communicate), models can be used to aid decision-making at multiple levels e.g. national and local, and to aid understanding and allow decision making. The techniques and tools of complexity science, therefore, offer a powerful means of understanding the complex decision-making processes that are needed to realise a low-carbon energy system. We conclude with recommendations for future areas of research and application.}
}
@article{HEIRDSFIELD200257,
title = {Flexibility and inflexibility in accurate mental addition and subtraction: two case studies},
journal = {The Journal of Mathematical Behavior},
volume = {21},
number = {1},
pages = {57-74},
year = {2002},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(02)00103-7},
url = {https://www.sciencedirect.com/science/article/pii/S0732312302001037},
author = {Ann M Heirdsfield and Tom J Cooper},
keywords = {Mental computation, Addition, Subtraction, Accuracy},
abstract = {This paper reports on a study of two children’s mental computation in addition and subtraction, and compares their mental architecture. Both students were identified as being accurate, however, one student used a variety of mental strategies (was flexible) while the other student used only one strategy that reflected the written procedure for each of the addition and subtraction algorithms taught in the classroom. Interviews were used to identify both children’s knowledge and ability with respect to number sense (including numeration, number and operations, basic facts, estimation), metacognition and affects. Frameworks were developed to show how these factors interacted to explain the two types of accuracy in mental addition and subtraction. Flexible accuracy was related to the presence of strong number sense knowledge integrated with metacognitive strategies and beliefs and beliefs about self and teaching; while inflexible accuracy was a result of compensation of inadequate knowledge supported by beliefs about self and teaching.}
}
@article{DESAFERREIRA2013135,
title = {Promoting integrative medicine by computerization of traditional Chinese medicine for scientific research and clinical practice: The SuiteTCM Project},
journal = {Journal of Integrative Medicine},
volume = {11},
number = {2},
pages = {135-139},
year = {2013},
issn = {2095-4964},
doi = {https://doi.org/10.3736/jintegrmed2013013},
url = {https://www.sciencedirect.com/science/article/pii/S2095496414601096},
author = {Arthur {de Sá Ferreira}},
keywords = {traditional Chinese medicine, evidence-based practice, computer-assisted decision making},
abstract = {Background
Chinese and contemporary Western medical practices evolved on different cultures and historical contexts and, therefore, their medical knowledge represents this cultural divergence. Computerization of traditional Chinese medicine (TCM) is being used to promote the integrative medicine to manage, process and integrate the knowledge related to TCM anatomy, physiology, semiology, pathophysiology, and therapy.
Methods
We proposed the development of the SuiteTCM software, a collection of integrated computational models mainly derived from epidemiology and statistical sciences for computerization of Chinese medicine scientific research and clinical practice in all levels of prevention. The software includes components for data management (DataTCM), simulation of cases (SimTCM), analyses and validation of datasets (SciTCM), clinical examination and pattern differentiation (DiagTCM, TongueTCM, and PulseTCM), intervention selection (AcuTCM, HerbsTCM, and DietTCM), management of medical records (ProntTCM), epidemiologic investigation of sampled data (ResearchTCM), and medical education, training, and assessment (StudentTCM).
Discussion
The SuiteTCM project is expected to contribute to the ongoing development of integrative medicine and the applicability of TCM in worldwide scientific research and health care. The SuiteTCM 1.0 runs on Windows XP or later and is freely available for download as an executable application.}
}
@article{MEHREGAN2012426,
title = {An application of Soft System Methodology},
journal = {Procedia - Social and Behavioral Sciences},
volume = {41},
pages = {426-433},
year = {2012},
note = {The First International Conference on Leadership, Technology and Innovation Management},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2012.04.051},
url = {https://www.sciencedirect.com/science/article/pii/S1877042812009317},
author = {M. Reza Mehregan and Mahnaz Hosseinzadeh and Aliyeh Kazemi},
keywords = {Soft System Methodology (SSM), University course timetabling, rich picture, conceptual model},
abstract = {The typical course timetabling problem is assigning Classes of students to appropriate faculty members, suitable classrooms and available timeslots. Hence, it involves a large number of stakeholders including students, teachers and institutional administrators. Different kinds of Hard Operational Research techniques have been employed over the years to address such problems. Due to the computational difficulties of this NP complete problem as well as the size and the complexity of the real world instances, an efficient optimal solution cannot be found easily.As an alternative strategy, this paper investigates the application of Checkland‘s Soft System Methodology (SSM) to the course timetabling problem. Besides giving an ideal course timetable, even to large and complex real problems, application of SSM, generates debate, learning, and understanding; enables key changes; facilitates negotiating the actions to be taken and makes possible the meaningful collaboration among concerned stakeholders. This paper also provides an appropriate course timetable for the management faculty at University of Tehran to show the potential of this application to real problems.}
}
@article{KASONGO2023113,
title = {A deep learning technique for intrusion detection system using a Recurrent Neural Networks based framework},
journal = {Computer Communications},
volume = {199},
pages = {113-125},
year = {2023},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0140366422004601},
author = {Sydney Mambwe Kasongo},
keywords = {Machine learning, Feature selection, Intrusion detection, Feature extraction},
abstract = {In recent years, the spike in the amount of information transmitted through communication infrastructures has increased due to the advances in technologies such as cloud computing, vehicular networks systems, the Internet of Things (IoT), etc. As a result, attackers have multiplied their efforts for the purpose of rendering network systems vulnerable. Therefore, it is of utmost importance to improve the security of those network systems. In this study, an IDS framework using Machine Learning (ML) techniques is implemented. This framework uses different types of Recurrent Neural Networks (RNNs), namely, Long-Short Term Memory (LSTM), Gated Recurrent Unit (GRU) and Simple RNN. To assess the performance of the proposed IDS framework, the NSL-KDD and the UNSW-NB15 benchmark datasets are considered. Moreover, existing IDSs suffer from low test accuracy scores in detecting new attacks as the feature dimension grows. In this study, an XGBoost-based feature selection algorithm was implemented to reduce the feature space of each dataset. Following that process, 17 and 22 relevant attributes were picked from the UNSW-NB15 and NSL-KDD, respectively. The accuracy obtained through the test subsets was used as the main performance metric in conjunction with the F1-Score, the validation accuracy, and the training time (in seconds). The results showed that for the binary classification tasks using the NSL-KDD, the XGBoost-LSTM achieved the best performance with a test accuracy (TAC) of 88.13%, a validation accuracy (VAC) of 99.49% and a training time of 225.46 s. For the UNSW-NB15, the XGBoost-Simple-RNN was the most efficient model with a TAC of 87.07%. For the multiclass classification scheme, the XGBoost-LSTM achieved a TAC of 86.93% over the NSL-KDD and the XGBoost-GRU obtained a TAC of 78.40% over the UNSW-NB15 dataset. These results demonstrated that our proposed IDS framework performed optimally in comparison to existing methods.}
}
@article{MEJIA20113964,
title = {On the complexity of sandpile critical avalanches},
journal = {Theoretical Computer Science},
volume = {412},
number = {30},
pages = {3964-3974},
year = {2011},
note = {Cellular Automata and Discrete Complex Systems},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2011.02.029},
url = {https://www.sciencedirect.com/science/article/pii/S0304397511001496},
author = {Carolina Mejia and J. {Andres Montoya}},
keywords = {Abelian sandpile model, Self-organized criticality,  complete problems, Parallel algorithms},
abstract = {In this work, we study The Abelian Sandpile Model from the point of view of computational complexity. We begin by studying the length distribution of sandpile avalanches triggered by the addition of two critical configurations: we prove that those avalanches are long on average, their length is bounded below by a constant fraction of the length of the longest critical avalanche which is, in most of the cases, superlinear. At the end of the paper we take the point of view of computational complexity, we analyze the algorithmic hardness of the problem consisting in computing the addition of two critical configurations, we prove that this problem is P complete, and we prove that most algorithmic problems related to The Abelian Sandpile Model are NC reducible to it.}
}
@article{PINCETL2012S32,
title = {Nature, urban development and sustainability – What new elements are needed for a more comprehensive understanding?},
journal = {Cities},
volume = {29},
pages = {S32-S37},
year = {2012},
note = {Current Research on Cities},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2012.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S0264275112001059},
author = {Stephanie Pincetl},
keywords = {Urban metabolism, Sustainability, Political ecology, Urban ecosystem services},
abstract = {With the rise of interest in urban sustainability, the question of nature is front and center. This review suggests bridging between three distinct research paths concerned with urban areas and nature: urban ecosystem services, urban metabolism and urban political ecology to forge new thinking to transition from the sanitary city of the twentieth century to the sustainable city of the twenty-first. Cities are anthropogenic creations, sourcing their materials from nearby and far-off places, transforming those materials into products, goods and the physical infrastructure of cities. Tracking that flow of nature into the built environment, and the other flows such as water, needs to be accounted for as part of nature in the city. Cities – having entirely transformed the place they are located through building – have a unique nature, a nature planted by people, and made up of plants and animals that are often different than what had existed in the first place. The services of this new assemblage of species in the city, need to be studied critically. But ultimately, cities are the product of human volition, driven by economics, culture, politics and history. Understanding those drivers – the political ecology of place – provides an interpretive framework for reconsidering the nature of cities and its place in moving from a modernist sanitary city to a gray/green sustainable city.}
}
@article{PIHLAJAMAKI2020101103,
title = {Subjective cognitive complaints and sickness absence: A prospective cohort study of 7059 employees in primarily knowledge-intensive occupations},
journal = {Preventive Medicine Reports},
volume = {19},
pages = {101103},
year = {2020},
issn = {2211-3355},
doi = {https://doi.org/10.1016/j.pmedr.2020.101103},
url = {https://www.sciencedirect.com/science/article/pii/S2211335520300632},
author = {Minna Pihlajamäki and Heikki Arola and Heini Ahveninen and Jyrki Ollikainen and Mikko Korhonen and Tapio Nummi and Jukka Uitti and Simo Taimela},
keywords = {Subjective cognitive complaints, Screening questionnaire, Occupational healthcare, Self-reported data, Sickness allowance, Register data},
abstract = {Knowledge-intensive work requires capabilities like monitoring multiple sources of information, prioritizing between competing tasks, switching between tasks, and resisting distraction from the primary task(s). We assessed whether subjective cognitive complaints (SCC), presenting as self-rated problems with difficulties of concentration, memory, clear thinking and decision making predict sickness absence (SA) in knowledge-intensive occupations. We combined SCC questionnaire results with reliable registry data on SA of 7743 professional/managerial employees (47% female). We excluded employees who were not active in working life, on long-term SA, and those on a work disability benefit at baseline. The exposure variable was the presence of SCC. Age and SA before the questionnaire as a proxy measure of general health were treated as confounders and the analyses were conducted by gender. The outcome measure was the accumulated SA days during a 12-month follow-up. We used a hurdle model to analyse the SA data. SCC predicted the number of SA days during the 12-month follow-up. The ratio of the means of SA days was higher than 2.8 as compared to the reference group, irrespective of gender, with the lowest limit of 95% confidence interval 2.2. In the Hurdle model, SCC, SA days prior to the questionnaire, and age were additive predictors of the likelihood of SA and accumulated SA days, if any. Subjective cognitive complaints predict sickness absence in knowledge-intensive occupations, irrespective of gender, age, or general health. This finding has implications for supporting work ability (productivity) among employees with cognitively demanding tasks.}
}
@article{WALTER2016597,
title = {The financial Logos: The framing of financial decision-making by mathematical modelling},
journal = {Research in International Business and Finance},
volume = {37},
pages = {597-604},
year = {2016},
issn = {0275-5319},
doi = {https://doi.org/10.1016/j.ribaf.2016.01.022},
url = {https://www.sciencedirect.com/science/article/pii/S0275531916300228},
author = {Christian Walter},
keywords = {Performativity, Mathematisation, Mathematical modelling, Financialisation, Ethics, Finance},
abstract = {This paper introduces the notion of “financial Logos”, defined as a structuring discourse embedded in management tools and beliefs of financial practices. I hypothesize that this discourse contains a specific representation of risk mathematically modelled by probability measures. Next I use a performativity based approach to describe the concrete action of the financial Logos on financial practices: the framing of financial decision-making by mathematical modelling. I argue that it is not possible to think of a given financial practice without epistemologically and sociologically thinking of the contribution of the mathematical modelling to this practice. I conclude with consequences for ethics of finance: extending ethics of action to epistemic ethics, I suggest that, in finance, any preference in mathematical modelling is also a preference in ethics.}
}
@incollection{MAXWELL19961,
title = {Driving forces for innovation in applied catalysis},
editor = {Joe W. Hightower and W. {Nicholas Delgass} and Enrique Iglesia and Alexis T. Bell},
series = {Studies in Surface Science and Catalysis},
publisher = {Elsevier},
volume = {101},
pages = {1-9},
year = {1996},
booktitle = {11th International Congress On Catalysis - 40th Anniversary},
issn = {0167-2991},
doi = {https://doi.org/10.1016/S0167-2991(96)80210-2},
url = {https://www.sciencedirect.com/science/article/pii/S0167299196802102},
author = {Ian E. Maxwell},
abstract = {Publisher Summary
Catalytic environmental technologies such as automobile exhaust catalysts and the selective catalytic reduction (SCR) DeNOx systems in power plants have significantly contributed to the reduction of environmentally harmful emissions into the lower atmosphere. Some studies have identified catalysis as not only being pervasive but also offering significant scope for the innovative development of new and improved technologies for environmentally acceptable processes and products in the future. The spectrum of process industries that are directly impacted by catalysis include, for example, oil refining, natural gas conversion, petrochemicals, fine chemicals, and pharmaceuticals. Environmental catalytic technologies also play an important role in emission control systems for power generation, fossil fuel driven transportation, oil refining, and chemical industries. Catalytic technologies typically embrace a wide range of disciplines, such as heterogeneous and homogeneous catalysis, materials science, process technology, reactor engineering, separation technology, surface science, computational chemistry, and analytic chemistry. Innovation in this field is, therefore, often achieved by lateral thinking across these different disciplines. This chapter attempts to develop this theme by means of examples from recent commercial successes and from this platform provides some guidelines for multi-disciplinary approaches at the academic and industrial interface to enhanced innovation in catalytic technologies in the future.}
}
@article{FESSAKIS201387,
title = {Problem solving by 5–6 years old kindergarten children in a computer programming environment: A case study},
journal = {Computers & Education},
volume = {63},
pages = {87-97},
year = {2013},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2012.11.016},
url = {https://www.sciencedirect.com/science/article/pii/S0360131512002813},
author = {G. Fessakis and E. Gouli and E. Mavroudi},
keywords = {Programming and programming languages, Kindergarten, Improving classroom teaching, Teaching/learning strategies},
abstract = {Computer programming is considered an important competence for the development of higher-order thinking in addition to algorithmic problem solving skills. Its horizontal integration throughout all educational levels is considered worthwhile and attracts the attention of researchers. Towards this direction, an exploratory case study is presented concerning dimensions of problem solving using computer programming by 5–6 years old kindergarten children. After a short introductory experiential game the children were involved in solving a series of analogous computer programming problems, using a Logo-based environment on an Interactive White Board. The intervention was designed as a part of the structured learning activities of the kindergarten which are teacher-guided and are conducted in a whole-class social mode. The observation of the video recording of the intervention along with the analysis of teacher's interview and the researcher's notes allow for a realistic evaluation of the feasibility, the appropriateness and the learning value of integrating computer programming in such a context. The research evidence supports the view that children enjoyed the engaging learning activities and had opportunities to develop mathematical concepts, problem solving and social skills. Interesting results about children learning, difficulties, interactions, problem solving strategies and the teacher's role are reported. The study also provides proposals for the design of future research.}
}
@article{DRAGO2011361,
title = {Cyclic alternating pattern in sleep and its relationship to creativity},
journal = {Sleep Medicine},
volume = {12},
number = {4},
pages = {361-366},
year = {2011},
issn = {1389-9457},
doi = {https://doi.org/10.1016/j.sleep.2010.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S1389945711000578},
author = {Valeria Drago and Paul S. Foster and Kenneth M. Heilman and Debora Aricò and John Williamson and Pasquale Montagna and Raffaele Ferri},
keywords = {Sleep, Creativity, Cyclic alternating pattern, Torrance, Frontal lobe functions, Arousal},
abstract = {Background/objectives
Sleep has been shown to enhance creativity, but the reason for this enhancement is not entirely known. There are several different physiologic states associated with sleep. In addition to rapid (REM) and non-rapid eye movement (NREM) sleep, NREM sleep can be broken down into Stages (1–4) that are characterized by the degree of EEG slow-wave activity. In addition, during NREM sleep the cyclic alternating pattern (CAPs) of EEG activity has been described which can also be divided into three subtypes (A1–A3) according to the frequency of the EEG waves. Differences in CAP subtype ratios have been previously linked to cognitive performances. The purpose of this study was to asses the relationship between CAP activity during sleep and creativity.
Methods
The participants were eight healthy young adults (four women) who underwent three consecutive nights of polysomnographic recording and took the Abbreviated Torrance Test for Adults (ATTA) on the second and third mornings after the recordings.
Results
There were positive correlations between Stage 1 of NREM sleep and some measures of creativity such as fluency (R=.797; p=.029) and flexibility (R=.43; p=.002), between Stage 4 of NREM sleep and originality (R=.779; p=.034) and a global measure of figural creativity (R=.758; p=.040). There was also a negative correlation between REM sleep and originality (R=−.827; p=.042). During NREM sleep the CAP rate, which in young people reflects primarily the A1 subtype, also correlated with originality (R=.765; p=.038).
Conclusions
NREM sleep is associated with low levels of cortical arousal, and low cortical arousal may enhance the ability of people to access to the remote associations that are critical for creative innovations. In addition, A1 CAP subtypes reflect frontal activity, and the frontal lobes are important for divergent thinking, also a critical aspect of creativity.}
}
@article{ARCK201954,
title = {When 3 Rs meet a forth R: Replacement, reduction and refinement of animals in research on reproduction},
journal = {Journal of Reproductive Immunology},
volume = {132},
pages = {54-59},
year = {2019},
issn = {0165-0378},
doi = {https://doi.org/10.1016/j.jri.2019.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0165037819300385},
author = {Petra Clara Arck},
keywords = {Reproduction, Mouse models, 3R principle, Immunology},
abstract = {Research endeavors aiming to understand the maternal immune adaptation to pregnancy significantly rely on the use of animal models, such as mice and rats. These models have provided important insights into the pathophysiology of a number of pregnancy disorders in humans. However, the use of animal models in scientific research is a vividly debated and emotive topic. The 3R principles – replacement, reduction and refinement of research animals – have been propagated a few decades ago. The present review advocates a forward-thinking consciousness to address the 3R principles in research projects in the field of reproductive biology and immunology. Specific measures and alternative methods are being proposed to replace research animals by using e.g. tissue engineering approaches, biobank-derived tissue, ‘placenta-on-a-chip’ devices or in silico methods. The latter may involve data queries from repositories now available to provide single cell sequencing information on reproductive tissues. Reduction of research animals by gestational imaging and a wealth of suggestions for refinement are proposed. Taken together, the measures and guidelines introduced in this review are expected to spark a reconsideration of experimental designs in the area of reproductive biology and immunology in order to implement 3R principle where applicable.}
}
@article{LUO2011384,
title = {Application of Improved EAHP on Stability Evaluation of Coal Seam Roof},
journal = {Procedia Earth and Planetary Science},
volume = {3},
pages = {384-393},
year = {2011},
note = {2011 Xi'an International Conference on Fine Exploration and Control of Water & Gas in Coal Mines},
issn = {1878-5220},
doi = {https://doi.org/10.1016/j.proeps.2011.09.110},
url = {https://www.sciencedirect.com/science/article/pii/S1878522011001111},
author = {Donghai Luo and Shunxin Sun and Dunhu Zhang and Yuqing Wan and Guangchao Zhang and Junqiang Niu},
keywords = {Coal Seam Roof, Stability Evaluation, EAHP, Model},
abstract = {Stability of coal seam roof is one of the important factors to ensure safe and efficient coal production. Stability result is the complex interaction subjected to a larger number of geological factors. Only taking comprehensive consideration into evaluation can the result be in line with the actual complex geological environment. Main factors of coal seam roof stability are divided into four major factors and eight secondary factors. Major factors are sedimentary environment, structural feature, rock mechanics property and so on. Secondary factors are the combination of roof rock, lithology difference, bedding changes, and so on. Stability rank is divided into four grades: super stability, stability, basically stability and instability. EAHP model of stability evaluation of coal seam roof and the extension comparison matrix are established by means of the improved EAHP (Extension Analytical Hierarchy Process) method. Using the method based on judgment by possibility degree matrix can get the Sorting order. Evaluation results show that: the stability grade of main coal seam 5# roof of the mine is stable. It is true and credible. The method not only has the merits of “Extension to consider fuzziness of human thinking to judge”, but also eliminates a lot of spreadsheet work in traditional AHP. These studies are useful experiment and explore to study on comprehensive evaluation of coal seam roof stability.}
}
@article{FORRY2013634,
title = {Ready or not: Associations between participation in subsidized child care arrangements, pre-kindergarten, and Head Start and children’s school readiness},
journal = {Early Childhood Research Quarterly},
volume = {28},
number = {3},
pages = {634-644},
year = {2013},
issn = {0885-2006},
doi = {https://doi.org/10.1016/j.ecresq.2013.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0885200613000367},
author = {Nicole D. Forry and Elizabeth E. Davis and Kate Welti},
keywords = {Low-income, School readiness, Pre-kindergarten, Head Start, Child care subsidies},
abstract = {Research has found disparities in young children’s development across income groups. A positive association between high-quality early care and education and the school readiness of children in low-income families has also been demonstrated. This study uses linked administrative data from Maryland to examine the variations in school readiness associated with different types of subsidized child care, and with dual enrollment in subsidized child care and state pre-kindergarten or Head Start. Using multivariate methods, we analyze linked subsidy administrative data and portfolio-based kindergarten school readiness assessment data to estimate the probability of children’s school readiness in three domains: personal and social development, language and literacy, and mathematical thinking. Compared to children in subsidized family child care or informal care, those in subsidized center care are more likely to be rated as fully ready to learn on the two pre-academic domains. Regardless of type of subsidized care used, enrollment in pre-kindergarten, but not Head Start, during the year prior to kindergarten is strongly associated with being academically ready for kindergarten. No statistically significant associations are found between type of subsidized care, pre-kindergarten enrollment, or Head Start and assessments of children’s personal/social development.}
}
@incollection{HADAP2023319,
title = {Chapter 16 - Theories methods and the parameters of quantitative structure–activity relationships and artificial neural network},
editor = {Dakeshwar Kumar Verma and Chandrabhan Verma and Jeenat Aslam},
booktitle = {Computational Modelling and Simulations for Designing of Corrosion Inhibitors},
publisher = {Elsevier},
pages = {319-335},
year = {2023},
isbn = {978-0-323-95161-6},
doi = {https://doi.org/10.1016/B978-0-323-95161-6.00019-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323951616000199},
author = {Arti Hadap and Ashutosh Pandey and Bhawana Jain and Reena Rawat},
keywords = {Corrosion, QSAR (quantitative structure and activity relationship), ANN (artificial neural network), inhibitor adsorption, metal surface},
abstract = {Quantitative Structure–Activity Relationships (QSAR) is a computational model used to describe and anticipate the interaction and surface interactions of substances. In addition, Artificial Neural Network (ANN) has been used for the development of linear and sigmoidal functionals, aiming to predict low-carbon steel, copper, and aluminum corrosion rates corresponding to environmental parameters. Thus this chapter explains the theory behind the QSAR/ANN and demonstrates its effectiveness related to corrosion. QSAR aims to draw an attention to the link between the effectiveness of prevention (any function) and the structural features (adjectives). It involves finding one or more items that, by mathematical equation, link these definitions to their blocking function. ANN (artificial neural network) shows excellent performance in the prediction (output) for various complex characteristic data (input). The obtained results give deep insight into the corrosion systems by analyzing the point of surface corrosion attack, the more stable site of the inhibitor adsorption, and the binding power of the adsorbed coating.}
}
@article{LAMB2023101031,
title = {Flexibility across and flexibility within: The domain of integer addition and subtraction},
journal = {The Journal of Mathematical Behavior},
volume = {70},
pages = {101031},
year = {2023},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2023.101031},
url = {https://www.sciencedirect.com/science/article/pii/S0732312323000019},
author = {Lisa Lamb and Jessica Bishop and Ian Whitacre and Randolph Philipp},
keywords = {Number concepts and operations, Cognition, Flexibility, Adaptive expertise, Strategy variability, Integers},
abstract = {To better understand the role that flexibility plays in students’ success on integer addition and subtraction problems, we examined students’ flexibility when solving open number sentences. We define flexibility as the degree to which a learner uses more than one strategy to solve a single task when prompted, as well as the degree to which a learner changes strategies when solving a range of tasks to accommodate task differences. We introduce the categorizations of flexibility within and flexibility across to distinguish these two ways of operationalizing flexibility. We examined flexibility and performance within and among three groups of students — 2nd and 4th graders who had negative numbers in their numerical domains, 7th graders, and college-track 11th graders. Profiles of five students are shared to provide insight in relation to the quantitative findings.}
}
@article{ABDELAZIZ2024100615,
title = {A scoping review of artificial intelligence within pharmacy education},
journal = {American Journal of Pharmaceutical Education},
volume = {88},
number = {1},
pages = {100615},
year = {2024},
issn = {0002-9459},
doi = {https://doi.org/10.1016/j.ajpe.2023.100615},
url = {https://www.sciencedirect.com/science/article/pii/S0002945923045539},
author = {May H. {Abdel Aziz} and Casey Rowe and Robin Southwood and Anna Nogid and Sarah Berman and Kyle Gustafson},
keywords = {Pharmacy education, Artificial intelligence, Deep learning, Machine learning},
abstract = {Objectives
This scoping review aimed to summarize the available literature on the use of artificial intelligence (AI) in pharmacy education and identify gaps where additional research is needed.
Findings
Seven studies specifically addressing the use of AI in pharmacy education were identified. Of these 7 studies, 5 focused on AI use in the context of teaching and learning, 1 on the prediction of academic performance for admissions, and the final study focused on using AI text generation to elucidate the benefits and limitations of ChatGPT use in pharmacy education.
Summary
There are currently a limited number of available publications that describe AI use in pharmacy education. Several challenges exist regarding the use of AI in pharmacy education, including the need for faculty expertise and time, limited generalizability of tools, limited outcomes data, and several legal and ethical concerns. As AI use increases and implementation becomes more standardized, opportunities will be created for the inclusion of AI in pharmacy education.}
}
@article{LIU2023115384,
title = {Trajectory planning for unmanned surface vehicles in multi-ship encounter situations},
journal = {Ocean Engineering},
volume = {285},
pages = {115384},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.115384},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823017687},
author = {Jianjian Liu and Huizi Chen and Shaorong Xie and Yan Peng and Dan Zhang and Huayan Pu},
keywords = {Tordsdrajectory planning, Collision avoidance, Velocity obstacle, Multiship encounters, COLREGS},
abstract = {Unmanned surface vehicles (USVs) can encounter traffic ships while navigating toward the target location. For the USVs, collision avoidance (CA) trajectories need to be planned according to the international regulations for preventing collisions at sea (COLREGS). A novel trajectory planning approach is proposed for the collision-free trajectories planning of USVs in the case of multiship encounters. Unlike the existing trajectory planning approaches, the proposed approach uses the holistic thinking to simplify the analysis of encounter situations. Ships approaching from all sides of the USV are treated as one or two equivalent obstacles based on consistent offset velocity direction (COVD) method. Furthermore, planned velocity is designed using the proposed CA strategy and kinematic constraints. This strategy is compliant with COLREGS and includes an emergency CA module to further ensure a safe distance between the USV and traffic ships. The performance of the proposed trajectory planning approach is verified through physical simulations using an existing simulator. The simulation results show that the proposed trajectory planning approach can implement multiple USVs to simultaneously avoid collisions and reach their respective target positions. Moreover, the approach remains effective when other USVs do not follow the COLREGS protocols.}
}
@article{DAVIES2023100692,
title = {Idea generation and knowledge creation through maker practices in an artifact-mediated collaborative invention project},
journal = {Learning, Culture and Social Interaction},
volume = {39},
pages = {100692},
year = {2023},
issn = {2210-6561},
doi = {https://doi.org/10.1016/j.lcsi.2023.100692},
url = {https://www.sciencedirect.com/science/article/pii/S2210656123000089},
author = {Sini Davies and Pirita Seitamaa-Hakkarainen and Kai Hakkarainen},
keywords = {Constructionism, Design practices, Engineering practices, Epistemic object, Epistemic practices, Knowledge creation, Learning by making, Material mediation},
abstract = {This investigation involved carrying out interventions that engaged teams of lower-secondary (13–14-year-old) Finnish students in using traditional and digital fabrication technologies to make materially embodied collaborative inventions. By relying on video data and ethnographic observations of the student teams' collaborative invention processes, the investigation focused on investigating 1) how the teams generated and developed their design ideas in their materially anchored making process and 2) what kinds of maker practices they relied on during open-ended invention projects. The study focused on a microanalytic study of three teams of students, and we utilized and developed visual data analysis methods. Our findings reveal the complex nature of the student teams' materially contextualized ideation and the knowledge creation activities that took place within their projects. The findings suggest that open-ended, materially mediated co-invention projects offer ample opportunities for creative cultural participation and practice-based knowledge creation in schools.}
}
@article{YOUSIF20241342,
title = {Safety 4.0: Harnessing computer vision for advanced industrial protection},
journal = {Manufacturing Letters},
volume = {41},
pages = {1342-1356},
year = {2024},
note = {52nd SME North American Manufacturing Research Conference (NAMRC 52)},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2024.09.161},
url = {https://www.sciencedirect.com/science/article/pii/S2213846324002451},
author = {Ibrahim Yousif and Jad Samaha and JuHyeong Ryu and Ramy Harik},
keywords = {Smart Manufacturing, Safety 4.0, Computer Vision, Industrial Protection, Musculoskeletal disorders (MSD), Effective Functional Training},
abstract = {In the pursuit of enhanced productivity, reduced costs, and minimized lead times, manufacturers are transitioning from traditional systems to autonomous systems. This shift, driven by the emergence of smart manufacturing and technological advancements such as robotics, collaborative robots (Cobots), automation, and digitalization, necessitates a parallel evolution in safety protocols—termed Safety 4.0—to mitigate the risks associated with such dynamic environments. The integration of smart technologies within manufacturing significantly transforms traditional workflows and intensifies the need for comprehensive safety training and guidelines. Innovations like smart personal protective equipment (PPE) and wearable sensors are pivotal in this transition, yet they often prove financially burdensome for manufacturers due to high costs and the scale of workforce deployment. Moreover, the effective use of these technologies requires continuous monitoring and data analysis, further straining resources. To address these challenges, this paper proposes the adoption of computer vision technology to enhance safety measures within manufacturing facilities, focusing on human and PPE detection. It details a holistic methodology encompassing data collection, preprocessing, training, and execution. The discussion extends to the implementation framework of this technology, emphasizing its role in enabling autonomous decision-making—a crucial step beyond mere detection. Furthermore, the paper explores the utilization of the accumulated data to develop immersive training modules employing Mixed Reality, thereby reinforcing safety protocols and fostering an environment of continuous learning and adaptation. This approach not only contributes to safeguarding personnel but also aligns with the financial and reputational interests of forward-thinking manufacturers.}
}
@article{GAO2023101631,
title = {Key nodes identification in complex networks based on subnetwork feature extraction},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {7},
pages = {101631},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101631},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823001854},
author = {Luyuan Gao and Xiaoyang Liu and Chao Liu and Yihao Zhang and Giacomo Fiumara and Pasquale De Meo},
keywords = {Key nodes identification, Complex network, Subnetwork feature extraction, Graph convolutional networks},
abstract = {The problem of detecting key nodes in a network (i.e. nodes with the greatest ability to spread an infection) has been studied extensively in the past. Some approaches to key node detection compute node centrality, but there is no formal proof that central nodes also have the greatest spreading capacity. Other methods use epidemiological models (e.g., the SIR model) to describe the spread of an infection and rely on numerical simulations to find out key nodes; these methods are highly accurate but computationally expensive. To efficiently but accurately detect key nodes, we propose a novel deep learning method called Rank by Graph Convolutional Network, RGCN. Our method constructs a subnetwork around each node to estimate its spreading power; then RGCN applies a graph convolutional network to each subnetwork and the adjacency matrix of the network to learn node embeddings. Finally, a neural network is applied to the node embeddings to detect key nodes. Our RGCN method outperforms state-of-the-art approaches such as RCNN and MRCNN by 11.84% and 13.99%, respectively, when we compare the Kendall’s τ coefficient between the node ranking produced by each method with the true ranking obtained by SIR simulations.}
}
@incollection{ROSENBERG2023157,
title = {Chapter 9 - Machine learning and precision medicine},
editor = {Gary A. Rosenberg},
booktitle = {Neuroinflammation in Vascular Dementia},
publisher = {Academic Press},
pages = {157-173},
year = {2023},
isbn = {978-0-12-823455-6},
doi = {https://doi.org/10.1016/B978-0-12-823455-6.00005-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234556000055},
author = {Gary A. Rosenberg},
keywords = {Principal component analysis (PCA), exploratory factor analysis (EFA), Binswanger’s disease score (BDS), The Alzheimer Disease Neuroimaging Initiative (ADNI), hierarchical clustering analysis (HCA)},
abstract = {Clinical medicine is experiencing a massive increase in the amount of information available to the physician caring for a patient. Certain medical fields have incorporated this deluge of information into patient care while others are lagging behind. Neurology has been slow to adopt the new methods to use the large amount of information, but it is rapidly learning from other fields. Cancer diagnosis and treatment has been at the forefront of this revolution; not only have there been an extensive number of genes associated with different cancers discovered, but this information has been used to formulate treatment plans. Other fields such as radiology and dermatology are using computer-aided imaging to diagnose illness by analysis of radiographs and to automate diagnoses of skin cancers. The concepts behind the use of machine learning in diagnosis originated from early work in the field of “cybernetics,” which is a transdisciplinary approach for exploring regulatory systems – their structures, constraints, and possibilities. Norbert Wiener defined cybernetics in 1948 as “the scientific study of control and communication in the animal and the machine.” From the early work on control theory by Wiener and others has slowly evolved the modern concepts of artificial intelligence (AI) and machine learning. There are various definitions of AI or machine learning. The term is used to describe computers that perform cognitive functions that we associate with the human mind; these “thinking machines” can beat experts in chess and the Chinese game of Go. In medicine, there are capable of analyzing large amounts of data to arrive at a diagnosis through pattern recognition. Antibiotic drugs have been designed by AI in ways that were unavailable to humans, pointing to the future of molecular discovery in medicine.}
}
@article{HOLZER20163,
title = {Design exploration supported by digital tool ecologies},
journal = {Automation in Construction},
volume = {72},
pages = {3-8},
year = {2016},
note = {Computational and generative design for digital fabrication: Computer-Aided Architectural Design Research in Asia (CAADRIA)},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2016.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0926580516301467},
author = {Dominik Holzer},
keywords = {Parametric design, Environmental performance optimization, Interoperability, multidisciplinary design, Convergence, Optioneering, Multi-criteria},
abstract = {Designers take advantage of tool ecologies in order to find the most purposeful way of connecting often distinct processes that inform morphological design and associated building performance feedback. The ability to set up logical connections of design parameters across different digital applications becomes ever more relevant in a time where the proliferation of computational tools has led to a fundamental transformation in architectural education. Morphological exploration and form-finding get increasingly enriched by environmental performance feedback. This paper points out a major step forward in software interoperability and the alignment of digital design applications, allowing users to engage with morphological form-finding enriched by real-time physical building performance feedback. The key innovation presented here relates to tools available to designers who neither possess in-depth programming skills, nor need to rely on custom-developed scripts in order to advance their concepts. A recent architectural design studio serves as a testbed to interrogate the level of convergence among tools for morphological design and performance optimization.}
}
@incollection{HERLIHY20211,
title = {Chapter 1 - Introduction},
editor = {Maurice Herlihy and Nir Shavit and Victor Luchangco and Michael Spear},
booktitle = {The Art of Multiprocessor Programming (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {1-18},
year = {2021},
isbn = {978-0-12-415950-1},
doi = {https://doi.org/10.1016/B978-0-12-415950-1.00009-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780124159501000094},
author = {Maurice Herlihy and Nir Shavit and Victor Luchangco and Michael Spear},
keywords = {parallelism, concurrent programming, shared-memory multiprocessors, safety, liveness, mutual exclusion, coordination protocol, producer—consumer problem, readers–writers problem, deadlock-freedom, starvation-freedom, Amdahl's law},
abstract = {This chapter introduces and motivates the study of shared-memory multiprocessor programming, or concurrent programming. It describes the overall plan of the book, and then presents some basic concepts of concurrent computation, and presents some of the fundamental problems—mutual exclusion, the producer–consumer problem, and the readers–writers problem—and some simple approaches to solve these problems. It ends with a brief discussion of Amdahl's law.}
}
@article{OMIZO2020102578,
title = {Machining Topoi: Tracking Premising in Online Discussion Forums with Automated Rhetorical Move Analysis},
journal = {Computers and Composition},
volume = {57},
pages = {102578},
year = {2020},
note = {Composing Algorithms: Writing (with) Rhetorical Machines},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2020.102578},
url = {https://www.sciencedirect.com/science/article/pii/S8755461520300396},
author = {Ryan M. Omizo},
keywords = {computational rhetoric, Docuscope, Faciloscope, discussion forums, topoi},
abstract = {This article interrogates recent computational work on discovering and analyzing topoi through the use of topic modeling in the discipline of the literary digital humanities against the long history of topical research and pedagogy in rhetoric and composition. While significant work has been done in the literary digital humanities to advance the study of texts through topic modeling, this article argues that the emphasis on the textuality of topoi in computational research neglects situated rhetorical actions and the dynamics of audience interaction. In response to this deemphasis, this article proposes an algorithmic alternative to the identification and explanation of the rhetorical topoi through the integrated use of a computational rhetorical move classifier called the Faciloscope (Omizo et al., 2016) and the pattern-matching program, Docuscope (Kaufer and Ishizaki, 1998).}
}
@incollection{SEN201693,
title = {5 - Conclusions},
editor = {Syamal K. Sen and Ravi P. Agarwal},
booktitle = {Zero},
publisher = {Academic Press},
pages = {93-142},
year = {2016},
isbn = {978-0-08-100774-7},
doi = {https://doi.org/10.1016/B978-0-08-100774-7.00005-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780081007747000053},
author = {Syamal K. Sen and Ravi P. Agarwal},
keywords = {Black holes, computational zero versus absolute zero, dark matter, Epoch, error in error-free computation, existence of year zero, fast computation, fundamental particle, glory of zero, intense concentration, irrational number without zero, numerical zero consciousness, place-value system, Pythagoras theorem, quantum universe, quantum zero, storage capacity and computational power, Vedic-Hindu-Buddhist legacy, zero in natural mathematics, zero space},
abstract = {We have stressed, among others, on the globally accepted Indian zero along with its place-value system. Pointed out are (i) the impossibility of a zero-free irrational number, (ii) the requirement of a minimum two symbols to represent any information, (iii) the reason for the survival of radix-10 number system, (iv) the distinction of a numerical zero and the absolute zero and the consequent eternal nonremovable error, (v) existence/nonexistence of a building block of matter beyond a fundamental particle, (vi) the importance of intense concentration for innovation, (vii) the nonexactness of zero in physics, (viii) our continuing scientific quest for the origin of the universe, (ix) the existence of pure consciousness eternally everywhere as well as the distinction between artificial and natural consciousness, (x) the pervasiveness of computational mathematics/science in all sciences, engineering, and technologies, (xi) the distinction between living and nonliving computers in terms of storage and computational power, (xii) the extraordinary living computers among human beings, and (xiii) the solution of Y2K problem.}
}
@article{MOTA2023985,
title = {Speech as a Graph: Developmental Perspectives on the Organization of Spoken Language},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {8},
number = {10},
pages = {985-993},
year = {2023},
note = {Natural Language Processing in Psychiatry and Clinical Neuroscience Research},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2023.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S2451902223000988},
author = {Natália Bezerra Mota and Janaina Weissheimer and Ingrid Finger and Marina Ribeiro and Bárbara Malcorra and Lilian Hübner},
keywords = {Clinical high risk, Cognitive development, Computational psychiatry, Dementia, Diagnosis, Psychosis},
abstract = {Language has been used as a privileged window to investigate mental processes. More recently, descriptions of psychopathological symptoms have been analyzed with the help of natural language processing tools. An example is the study of speech organization using graph theoretical approaches that began approximately 10 years ago. After its application in different areas, there is a need to better characterize what aspects can be associated with typical and atypical behavior throughout the lifespan, given the variables related to aging as well as biological and social contexts. The precise quantification of mental processes assessed through language may allow us to disentangle biological/social markers by looking at naturalistic protocols in different contexts. In this review, we discuss 10 years of studies in which word recurrence graphs were adopted to characterize the chain of thoughts expressed by individuals while producing discourse. Initially developed to understand formal thought disorder in the context of psychotic syndromes, this line of research has been expanded to understand the atypical development in different stages of psychosis and differential diagnosis (such as dementia) as well as the typical development of thought organization in school-age children/teenagers in naturalistic and school-based protocols. We comment on the effects of environmental factors, such as education and reading habits (in monolingual and bilingual contexts), in clinical and nonclinical populations at different developmental stages (from childhood to older adulthood, considering aging effects on cognition). Looking toward the future, there is an opportunity to use word recurrence graphs to address complex questions that consider biological/social factors within a developmental perspective in typical and atypical contexts.}
}
@article{KUMARSHRIVASTAVA20217025,
title = {Bike clutch plate thermal analysis with using different materials},
journal = {Materials Today: Proceedings},
volume = {47},
pages = {7025-7029},
year = {2021},
note = {International Conference on Advances in Design, Materials and Manufacturing},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.05.300},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321038943},
author = {Ashish {Kumar Shrivastava} and Rohit Pandey and Rahaman {Khan Pathan} and Yogesh {Kumar Tembhurne} and T {Ravi Kiran}},
keywords = {ANSYS, CATIA, Wet-Clutch plat, Thermal stress, Total deformation, Heat flux},
abstract = {In this present paper, wet clutch is analysed by Computational Modeling. 2-D drawings are designed for multi plate clutch from computational calculations. 3D model is created in the Solid Edge modeling software for bike clutch. FEM analysis of clutch is carried out by varying friction materials with some non metals and composite materials. The best material suited for the lining of friction surfaces is found. Thermal transient investigation will be done on the wet grating plates to check the quality. Modeling of multi plate clutch is done by using Solid edge Software and then the model is imported into ANSYS Software for Structural, Thermal analysis to check the quality and temperature contours. Results show that friction materials like Al alloy, Al oxide, Cast iron, Carbon fiber and Al nitride generate 146.31 °C, 136.77 °C, 146.44 °C, 149.99 °C and 144.54 °C respectively. Heat flux results with the above friction materials are respectively 1.18 w/mm2, 1.04 w/mm2, 0.56 w/mm2, 0.0085 w/mm2 and 1.25 w/mm2. A twin grasp transmission is very like the traditional programmed, the primary distinction being the twofold grip structure contrasted with the single programmed grasp utilized in automatics. Automatics utilize a torque converter to move motor torque from the motor to the transmission. Basic investigation for grip plate has done utilizing the properties of three materials which are utilized for liner (for example carbon–carbon composites, Kevlar, Ceramic composites). A twin grasp transmission is very like the traditional programmed, the primary distinction being the twofold grip structure contrasted with the single programmed grasp utilized in automatics.}
}
@article{JOHNSON2022105743,
title = {Metacognition for artificial intelligence system safety – An approach to safe and desired behavior},
journal = {Safety Science},
volume = {151},
pages = {105743},
year = {2022},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2022.105743},
url = {https://www.sciencedirect.com/science/article/pii/S0925753522000832},
author = {Bonnie Johnson},
keywords = {Metacognition, Artificial intelligence systems, Machine learning, System safety, Complexity},
abstract = {Advances in computational thinking and data science have led to a new era of artificial intelligence systems being engineered to adapt to complex situations and develop actionable knowledge. These learning systems are meant to reliably understand the essence of a situation and construct critical decision recommendations to support autonomous and human–machine teaming operations. In parallel, the increasing volume, velocity, variety, veracity, value, and variability of data is confounding the complexity of these new systems – creating challenges in terms of their development and implementation. For artificial systems supporting critical decisions with higher consequences, safety has become an important concern. Methods are needed to avoid failure modes and ensure that only desired behavior is permitted. This paper discusses an approach that promotes self-awareness, or metacognition, within the artificial intelligence systems to understand their external and internal operational environments and use this knowledge to identify potential failures and enable self-healing and self-management for safe and desired behavior.}
}
@article{HE2021117140,
title = {Understanding chemical short-range ordering/demixing coupled with lattice distortion in solid solution high entropy alloys},
journal = {Acta Materialia},
volume = {216},
pages = {117140},
year = {2021},
issn = {1359-6454},
doi = {https://doi.org/10.1016/j.actamat.2021.117140},
url = {https://www.sciencedirect.com/science/article/pii/S1359645421005206},
author = {Q.F. He and P.H. Tang and H.A. Chen and S. Lan and J.G. Wang and J.H. Luan and M. Du and Y. Liu and C.T. Liu and C.W. Pao and Y. Yang},
keywords = {Chemical Short Rang Order, High entropy alloy, Solid solution, Lattice distortion},
abstract = {Chemical short-range ordering (CSRO) or demixing in solid solution high entropy alloys (HEAs) is a fundamental issue yet to be fully understood. In this work, we first developed a generalized quasi-chemical solid solution model that enables quantitative computation of the local chemical ordering or demixing in solid solution HEAs. After that, we performed synchrotron diffraction experiments, extensive Reverse Monte Carlo (RMC) simulations, and first principles calculations on the CoCrFeNi model alloy to study the development of local chemical environments after long time thermal annealing. The outcome of the combined research demonstrates that the development of local chemical ordering or demixing in CoCrFeNi is not only affected by the heat of mixing between dislike atoms but also coupled with local lattice distortion.}
}
@article{DELIGKAS2022103784,
title = {Two's company, three's a crowd: Consensus-halving for a constant number of agents},
journal = {Artificial Intelligence},
volume = {313},
pages = {103784},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103784},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222001242},
author = {Argyrios Deligkas and Aris Filos-Ratsikas and Alexandros Hollender},
keywords = {Consensus-halving, Fair division, Computational complexity, Query complexity, Robertson-Webb},
abstract = {We consider the ε-Consensus-Halving problem, in which a set of heterogeneous agents aim at dividing a continuous resource into two (not necessarily contiguous) portions that all of them simultaneously consider to be of approximately the same value (up to ε). This problem was recently shown to be PPA-complete, for n agents and n cuts, even for very simple valuation functions. In a quest to understand the root of the complexity of the problem, we consider the setting where there is only a constant number of agents, and we consider both the computational complexity and the query complexity of the problem. For agents with monotone valuation functions, we show a dichotomy: for two agents the problem is polynomial-time solvable, whereas for three or more agents it becomes PPA-complete. Similarly, we show that for two monotone agents the problem can be solved with polynomially-many queries, whereas for three or more agents, we provide exponential query complexity lower bounds. These results are enabled via an interesting connection to a monotone Borsuk-Ulam problem, which may be of independent interest. For agents with general valuations, we show that the problem is PPA-complete and admits exponential query complexity lower bounds, even for two agents.}
}
@article{LI2024101038,
title = {One-stop multi-sensor fusion and multimodal precise quantified traditional Chinese medicine imaging health examination technology},
journal = {Journal of Radiation Research and Applied Sciences},
pages = {101038},
year = {2024},
issn = {1687-8507},
doi = {https://doi.org/10.1016/j.jrras.2024.101038},
url = {https://www.sciencedirect.com/science/article/pii/S168785072400222X},
author = {Chuanxue Li and Ping Wang and Meifang Zheng and Wenxiang Li and Jun Zhou and Lin Fu},
keywords = {Traditional Chinese medicine imaging, Large language model, Knowledge graphs, Multimodal imaging, Health examination technology, Image fusion, Imaging agent, Deep learning},
abstract = {Except for single-mode traditional Chinese medicine imaging techniques such as infrared thermal imaging, the one-stop multimodal whole-body imaging health examination technology and device is still blank. We focus on infrared thermal imaging as the main modality, integrated various modalities of medical imaging intelligent sensing agents such as terahertz imaging. The upper and lower computer and virtual instrument architecture are used, and the imaging data are collected by the lower computers that each is an intelligent sensing agent. The upper computer is used for image reconstruction with intelligent algorithms. Based on the core theory of traditional Chinese medicine, intelligent fusion imaging is achieved through various modalities to achieve the ‘observation, hearing, questioning, and palpation’ four diagnostic integration. We use fractional Fourier transform to filter imaging data, Laplacian pyramid for image fusion. We have proposed an implementation method and process for combining traditional Chinese medicine imaging large language model with knowledge graph, and based on deep learning, we have studied the image and report generation algorithm that combines traditional Chinese medicine pathology and four diagnostic methods with knowledge graph fusion, as well as the traditional Chinese medicine human physiological and pathological interpretation and evaluation system. We have achieved some results, and through further research and development, we can achieve commercial applications.}
}
@incollection{VALLERO2025ix,
title = {Preface},
editor = {Daniel A. Vallero},
booktitle = {Fundamentals of Water Pollution},
publisher = {Elsevier},
pages = {ix-xi},
year = {2025},
isbn = {978-0-443-28987-3},
doi = {https://doi.org/10.1016/B978-0-443-28987-3.10000-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780443289873100001},
author = {Daniel A. Vallero}
}
@article{HESTER2019187,
title = {Simulation of integrative physiology for medical education},
journal = {Morphologie},
volume = {103},
number = {343},
pages = {187-193},
year = {2019},
issn = {1286-0115},
doi = {https://doi.org/10.1016/j.morpho.2019.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1286011519300554},
author = {R.L. Hester and W. Pruett and J. Clemmer and A. Ruckdeschel},
keywords = {VPH, Simulation, Physiology, Healthcare, Electronic health record},
abstract = {Summary
Medical education is founded on the understanding of physiology. While lecture materials and reading contribute to the learning of physiology, the richness and complexity of the subject suggest that more active learning methods may provide a richer introduction to the science as it applies to the practice of medicine. Simulation has been previously used in basic science to better understand the interaction of physiological systems. In the current context, simulation generally refers to interactive case studies performed with a manikin or anatomic device. More recently, simulation has grown to encompass computational simulation: virtual models of physiology and pathophysiology where students can see in a mechanistic setting how tissues and organs interact with one another to respond to changes in their environment. In this manuscript, we discuss how simulation fits into the overall history of medical education, and detail two computational simulation products designed for medical education. The first of these is an acute simulator, JustPhysiology, which reduces the scope of a large model, HumMod, down to a more focused interface. The second is Sycamore, an electronic health record-delivered, real time simulator of patients designed to teach chronic patient care to students. These products represent a new type of tool for medical and allied health students to encourage active learning and integration of basic science knowledge into clinical situations.
Résumé
L’étude de la médecine est fondée entre autres sur la compréhension de la physiologie. Bien que l’apprentissage de la physiologie puisse se faire au moyen de cours magistraux et la lecture de contenus spécialisés, la richesse et la complexité du sujet laissent supposer que des méthodes d’apprentissage plus interactives puissent susciter une initiation plus élaborée de cette science et de son application à la pratique de la médecine. La simulation a précédemment été appliquée aux sciences fondamentales afin de mieux comprendre l’interaction entre systèmes physiologiques. Dans le contexte actuel, la simulation réfère en général à des études de cas interactives réalisées à l’aide d’un mannequin ou tout autre modèle anatomique. Plus récemment, la simulation s’est étendue à la simulation informatique incluant des modèles virtuels de physiologie et de physiopathologie à partir desquels les étudiants peuvent apprécier dans un contexte mécanistique comment les tissus et organes interagissent dans leur réponse à tout changement environnemental. Dans cet article nous présentons comment la simulation s’intègre dans l’histoire de l’éducation de la médecine et détaillons deux modèles de simulation informatique adaptés à l’éducation médicale. Le premier modèle, JustPhysiology, est un simulateur de courte durée qui réduit le champ d’action d’un simulateur plus complexe, HumMod, à une interface plus spécialisée. Le second outil est Sycamore, un dossier de santé électronique généré en temps réel et conçu pour un apprentissage de la pratique de soins médicaux en continu. Ces simulateurs informatiques représentent un nouvel outil pour les étudiants en médecine et autres professions de santé afin d’encourager un apprentissage actif et l’intégration de concepts scientifiques fondamentaux aux conditions cliniques.}
}
@article{NAWAZ2024121481,
title = {CoffeeNet: A deep learning approach for coffee plant leaves diseases recognition},
journal = {Expert Systems with Applications},
volume = {237},
pages = {121481},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121481},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423019838},
author = {Marriam Nawaz and Tahira Nazir and Ali Javed and Sherif {Tawfik Amin} and Fathe Jeribi and Ali Tahir},
keywords = {CenterNet, Coffee plant disease, Classification, Deep learning, ResNet},
abstract = {Coffee is regarded as the highest consumed drink around the globe and has accounted as a major source of income in the regions where it is cultivated. To meet the coffee marketplace's requirements around the globe, cultivators must boost and analyze its cultivation and quality. Several factors like environmental changes and plant diseases are the major hindrance to increasing the yield of coffee. The development in the field of computer vision has facilitated the earliest diagnostic of diseased plant samples, however, the incidence of various image distortions i.e., color, light, size, orientation changes, and similarity in the healthy and diseased portions of examined samples are the major challenges in the effective recognition of various coffee plant leaf infections. The proposed work is focused to overwhelm the mentioned limitations by proposing a novel and effective DL model called the CoffeeNet. Explicitly, an improved CenterNet approach is proposed by introducing spatial-channel attention strategy-based ResNet-50 model for the computation of deep and disease-specific sample characteristics which are then classified by the 1-step detector of the CenterNet framework. We investigated the localization and cataloging outcomes of the suggested method on the Arabica coffee leaf repository which contains the images captured in the more realistic and complicated environmental constraints. The CoffeeNet model acquires a classification accuracy number of 98.54%, along with an mAP of 0.97 that is presenting the usefulness of our technique in localizing and categorizing various sorts of coffee plant leaf disorders.}
}
@article{KELLER2011174,
title = {Towards a science of informed matter},
journal = {Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences},
volume = {42},
number = {2},
pages = {174-179},
year = {2011},
note = {When Physics Meets Biology},
issn = {1369-8486},
doi = {https://doi.org/10.1016/j.shpsc.2010.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S1369848610001172},
author = {Evelyn Fox Keller},
keywords = {Information, Self-assembly, Evolution, Selection, Embodiment, Supramolecular chemistry},
abstract = {Over the last couple of decades, a call has begun to resound in a number of distinct fields of inquiry for a reattachment of form to matter, for an understanding of ‘information’ as inherently embodied, or, as Jean-Marie Lehn calls it, for a “science of informed matter.” We hear this call most clearly in chemistry, in cognitive science, in molecular computation, and in robotics—all fields looking to biological processes to ground a new epistemology. The departure from the values of a more traditional epistemological culture can be seen most clearly in changing representations of biological development. Where for many years now, biological discourse has accepted a sharp distinction (borrowed directly from classical computer science) between information and matter, software and hardware, data and program, encoding and enactment, a new discourse has now begun to emerge in which these distinctions have little meaning. Perhaps ironically, much of this shift depends on drawing inspiration from just those biological processes which the discourse of disembodied information was intended to describe.}
}
@article{DURSTEWITZ2008739,
title = {The Dual-State Theory of Prefrontal Cortex Dopamine Function with Relevance to Catechol-O-Methyltransferase Genotypes and Schizophrenia},
journal = {Biological Psychiatry},
volume = {64},
number = {9},
pages = {739-749},
year = {2008},
note = {Neurodevelopment and the Transition from Schizophrenia Prodrome to Schizophrenia},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2008.05.015},
url = {https://www.sciencedirect.com/science/article/pii/S000632230800646X},
author = {Daniel Durstewitz and Jeremy K. Seamans},
keywords = {Attractor dynamics, computational model, dopamine, GABA currents, NMDA currents, prefrontal cortex, schizophrenia},
abstract = {There is now general consensus that at least some of the cognitive deficits in schizophrenia are related to dysfunctions in the prefrontal cortex (PFC) dopamine (DA) system. At the cellular and synaptic level, the effects of DA in PFC via D1- and D2-class receptors are highly complex, often apparently opposing, and hence difficult to understand with regard to their functional implications. Biophysically realistic computational models have provided valuable insights into how the effects of DA on PFC neurons and synaptic currents as measured in vitro link up to the neural network and cognitive levels. They suggest the existence of two discrete dynamical regimes, a D1-dominated state characterized by a high energy barrier among different network patterns that favors robust online maintenance of information and a D2-dominated state characterized by a low energy barrier that is beneficial for flexible and fast switching among representational states. These predictions are consistent with a variety of electrophysiological, neuroimaging, and behavioral results in humans and nonhuman species. Moreover, these biophysically based models predict that imbalanced D1:D2 receptor activation causing extremely low or extremely high energy barriers among activity states could lead to the emergence of cognitive, positive, and negative symptoms observed in schizophrenia. Thus, combined experimental and computational approaches hold the promise of allowing a detailed mechanistic understanding of how DA alters information processing in normal and pathological conditions, thereby potentially providing new routes for the development of pharmacological treatments for schizophrenia.}
}
@article{KARSTEN2020104512,
title = {Closing the gap: Merging engineering and anthropology in holistic fire safety assessments in the maritime and offshore industries},
journal = {Safety Science},
volume = {122},
pages = {104512},
year = {2020},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2019.104512},
url = {https://www.sciencedirect.com/science/article/pii/S0925753518316175},
author = {Mette Marie Vad Karsten and Aqqalu Thorbjørn Ruge and Thomas Hulin},
keywords = {Anthropology, Fire safety engineering, Transdisciplinary, Risk, Maritime, Offshore},
abstract = {This article reports on the endeavor to merge the fields of anthropology and fire safety engineering in holistic fire safety assessments within the maritime and offshore industries. The article suggests a combination of the two disciplines to transition from an interdisciplinary approach towards transdisciplinarity. The approach has been developed and adjusted during three cases of risk analyses and prevention strategies on fire safety. The article presents two methodological insights illustrating the necessary attitude of interdisciplinarity as a foundation towards transdisciplinarity. It advocates for the need of willingness in organizations and project teams to consider both disciplines as equally valid, integrate them in research definition, and create a base for common understanding. Subsequently, it is proposed that transdisciplinary work requires the creation of a group of core members acting as guarantors of transdisciplinarity, thus becoming themselves transdisciplinary humans working in a joined framework of thinking and methods. The article also presents two operational findings integrating the two disciplines within the area of fire safety. The first finding concerns including ‘daily operations’ in fire safety design, as daily practices and perceptions among crew can have a high impact on fire safety. The second finding concerns ‘reclassification of space and place’. It highlights mixing and shifting between work- and leisure-related practices within the same physical space, leading to the identification of new fire scenarios. It also explores the shifts between work, leisure, and emergency places, and their link to the shifts in professional roles of crew.}
}
@article{QI2021338821,
title = {Accurate diagnosis of lung tissues for 2D Raman spectrogram by deep learning based on short-time Fourier transform},
journal = {Analytica Chimica Acta},
volume = {1179},
pages = {338821},
year = {2021},
issn = {0003-2670},
doi = {https://doi.org/10.1016/j.aca.2021.338821},
url = {https://www.sciencedirect.com/science/article/pii/S0003267021006474},
author = {Yafeng Qi and Lin Yang and Bangxu Liu and Li Liu and Yuhong Liu and Qingfeng Zheng and Dameng Liu and Jianbin Luo},
keywords = {Raman spectrogram, Lung cancer, Short-time Fourier transform, Deep learning},
abstract = {Multivariate statistical analysis methods have an important role in spectrochemical analyses to rapidly identify and diagnose cancer and the subtype. However, utilizing these methods to analyze lager amount spectral data is challenging, and poses a major bottleneck toward achieving high accuracy. Here, a new convolutional neural networks (CNN) method based on short-time Fourier transform (STFT) to diagnose lung tissues via Raman spectra readily is proposed. The models yield that the accuracies of the new method are higher than the conventional methods (principal components analysis -linear discriminant analysis and support vector machine) for validation group (95.2% vs 85.5%, 94.4%) and test group (96.5% vs 90.4%, 93.9%) after cross-validation. The results illustrate that the new method which converts one-dimensional Raman data into two-dimensional Raman spectrograms improve the discriminatory ability of lung tissues and can achieve automatically accurate diagnosis of lung tissues.}
}
@article{ZHU2024102509,
title = {Developing a fast and accurate collision detection strategy for crane-lift path planning in high-rise modular integrated construction},
journal = {Advanced Engineering Informatics},
volume = {61},
pages = {102509},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102509},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624001575},
author = {Aimin Zhu and Zhiqian Zhang and Wei Pan},
keywords = {Crane-lift, Path planning, Collision detection, Modular integrated construction},
abstract = {Crane-lift path planning (CLPP) ensures the safe and efficient installation of hefty modules in high-rise modular integrated construction (MiC). The implementation of CLPP requires effective collision detection strategies. However, existing collision detection strategies suffer from limitations in terms of computational intensity or insufficient accuracy. This paper aims to develop a fast and accurate collision detection strategy for CLPP in high-rise MiC projects using a single tower crane, thereby achieving safe and efficient module installation. It is executed with the assumptions that the geometry of the building remains unchanged, the positions and orientations of the lifted module and the tower crane are monitored, and no external loads act on the lifted module. Based on the research scope and assumptions, an octree and bounding box (Oct-Box) integrated strategy is developed. The strategy operates in two stages, the pre-execution and execution stages, supported by two critical technical components: (1) an optimized octree for lifting space division and encoding, and (2) an integrated bounding box algorithm for construction object collision detection. The strategy was evaluated using a real-life MiC project in Hong Kong. The results show that the developed strategy minimized the CLPP time by about 95 %, while ensuring continuous and accurate collision detection. In addition, the strategy was significantly affected by the depth of octree, the encoding method of octree, the bounding box algorithm and the configuration density. The developed Oct-Box strategy for CLPP is novel as it addresses temporal efficiency and spatial tightness in tandem, and marks a breakthrough for collision detection in modular construction.}
}
@article{DISESSA198067,
title = {Computation as a physical and intellectual environment for learning physics},
journal = {Computers & Education},
volume = {4},
number = {1},
pages = {67-75},
year = {1980},
issn = {0360-1315},
doi = {https://doi.org/10.1016/0360-1315(80)90009-3},
url = {https://www.sciencedirect.com/science/article/pii/0360131580900093},
author = {A.A. DiSessa}
}
@incollection{BAREISS1993157,
title = {The Evolution of a Case-Based Computational Approach to Knowledge Representation, Classification, and Learning},
editor = {Glenn V. Nakamura and Roman Taraban and Douglas L. Medin},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {29},
pages = {157-186},
year = {1993},
booktitle = {Categorization by Humans and Machines},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(08)60139-5},
url = {https://www.sciencedirect.com/science/article/pii/S0079742108601395},
author = {Ray Bareiss and Brian M. Slator}
}
@article{ISMAILOVA2020291,
title = {Hereditary information processes with semantic modeling structures},
journal = {Procedia Computer Science},
volume = {169},
pages = {291-296},
year = {2020},
note = {Postproceedings of the 10th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2019 (Tenth Annual Meeting of the BICA Society), held August 15-19, 2019 in Seattle, Washington, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.181},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920303045},
author = {Larisa Ismailova and Viacheslav Wolfengagen and Sergey Kosikov},
keywords = {semantic information processing, computational model, variable domains},
abstract = {In practice, when developing an information model, inheritance and composition mechanisms are used, which allows the model developer to extend the properties of the class. In this paper, we establish and use the difference between these two closely related representations when applied in aspect-oriented modeling. In particular, when an aspect is applied to extend the base class of the original model, the designer must choose to use composition. Depending on the composition order, indexing occurs, which can lead to the expansion of the base class by dynamic effects. With a different compositional order, a class narrowing occurs, since it becomes necessary to take into account an additional property. If you intend to define an alternative to a base class with advanced functionality, then inheritance should be used. The work demonstrates the power of the combined use of inheritance and composition, which allows us to develop an aspect-oriented modeling of a family of property transformations, in which a line of intermediate models of the working information process arises.}
}
@incollection{YU2018177,
title = {Chapter 11 - Geospatial Data Discovery, Management, and Analysis at National Aeronautics and Space Administration (NASA)},
editor = {Feras A. Batarseh and Ruixin Yang},
booktitle = {Federal Data Science},
publisher = {Academic Press},
pages = {177-191},
year = {2018},
isbn = {978-0-12-812443-7},
doi = {https://doi.org/10.1016/B978-0-12-812443-7.00011-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128124437000119},
author = {Manzhu Yu and Min Sun},
keywords = {Big data, Big data management, Climate, Data discovery, Large-scale scientific simulation, Natural phenomena, Spatiotemporal},
abstract = {The 21st century is experiencing simultaneous changes in global population, urbanization, and climate. These changes, along with the rapid growth of geospatial data and increasing popularity of data discovery, access, and analytics techniques, lead to the promising future of innovation and achievements in geospatial and spatiotemporal thinking, computing, and application. However, big geospatial data bring forth challenges that require the cutting-edge science and technology to address. In this chapter, we highlight some of the characteristics and challenges in geospatial and spatiotemporal data discovery, management, processing, and analytics, and provide solutions based on recent research achievements as case studies. These study cases provide concrete examples of challenges faced when handling geospatial and spatiotemporal big data and the potential solutions in high level of accuracy, interoperability, and scalability.}
}
@article{FENG2022127434,
title = {Parallel cooperation search algorithm and artificial intelligence method for streamflow time series forecasting},
journal = {Journal of Hydrology},
volume = {606},
pages = {127434},
year = {2022},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2022.127434},
url = {https://www.sciencedirect.com/science/article/pii/S0022169422000099},
author = {Zhong-kai Feng and Peng-fei Shi and Tao Yang and Wen-jing Niu and Jian-zhong Zhou and Chun-tian Cheng},
keywords = {Hydrological time series forecasting, Artificial intelligence, Evolutionary computation, Parallel computing},
abstract = {Reliable streamflow prediction is an important productive information in the hydrology and water resources management fields. As used to forecast the nonlinear streamflow time series, the conventional artificial intelligence model may suffer from local convergence defect and fail to track the dynamic changes of the hydrological process when the model parameters and network structure are not well identified. Thus, this research develops a practical hydrological forecasting model based on parallel cooperation search algorithm (PCSA) and extreme learning machine (ELM), where the standard ELM method is chosen as the basic forecasting model, and then the PCSA method using several smaller and independent subswarms for parallel computation is used to determine satisfying input-hidden weights and hidden biases of the ELM model. The proposed model is used to forecast the nonlinear streamflow time series of several real-world hydrological stations in China. The results demonstrate that the proposed model outperforms the standard ELM model in various evaluation indicators. Thus, the key contributions of this study lie in two aspects: (1) for the first time, the parallel computing technique is developed to improve the global search ability and resources utilization efficiency of the emerging cooperation search algorithm; (2) an artificial intelligence model coupled with parallel evolutionary optimizer is proposed to improve the prediction accuracy of hydrological time series.}
}
@article{R2023100760,
title = {Stellar parameter estimation in O-type stars using artificial neural networks},
journal = {Astronomy and Computing},
volume = {45},
pages = {100760},
year = {2023},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2023.100760},
url = {https://www.sciencedirect.com/science/article/pii/S2213133723000756},
author = {M. Flores R. and L.J. Corral and C.R. Fierro-Santillán and S.G. Navarro},
keywords = {Methods: Data analysis, Deep learning, Stars: Fundamental parameters, Astronomical databases: Miscellaneous},
abstract = {This work presents the results of the implementation of a deep learning system capable of estimating the effective temperature and surface gravity of O-type stars. The proposed system was trained with a database of 5,557 synthetic spectra computed with the stellar atmosphere code CMFGEN that covers stars with Teff from ∼20,000 K to ∼58,000 K, log(L/L⊙) from 4.3 to 6.3 dex, logg from 2.4 to 4.2 dex, and mass from 9 to 120 M⊙. Important advantages proposed in this paper include using a set of equivalent width measurements over the optical region of the stellar spectra, which avoids processing the full spectra with the inherent computational cost and allows it to apply the same trained system over different spectra resolutions. The validation of the system was performed by processing a sample of twenty O-type stars taken from the IACOB database, and a subgroup of eleven stars of those twenty taken from The Galactic O-Star Spectroscopic Catalog (GOSC) with lower resolution. As complementary work, we show the results of a synthetic spectra fitting process with the aim of simplifying the comparison with other estimations and parameter fitting from the literature.}
}
@article{DAHL20231039,
title = {A Learning Approach for Future Competencies in Manufacturing using a Learning Factory},
journal = {Procedia CIRP},
volume = {118},
pages = {1039-1043},
year = {2023},
note = {16th CIRP Conference on Intelligent Computation in Manufacturing Engineering},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.06.178},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123004055},
author = {Håkon Dahl and Nina Tvenge and Carla Susana A Assuad and Kristian Martinsen},
keywords = {Learning factory, Work Related Learning, Industry 4.0, Learning Method, Manufacturing, Future Work Competencies},
abstract = {This paper describes a study on future competence needs in manufacturing and how a learning factory utilising a Connective Model for Didactic Design can be used in teaching and learning of these competencies. The paper briefly reports on a literature study, and a set of interviews in Norwegian manufacturing companies to get a better understanding on the expected future competence needs. This was used to design a learning process with four steps: 1: Exploration, 2: Product and process design, 3: Problem solving and 4: Debriefing. The method was tested in a case study where undergraduate students are learners following the 4-step method. The approach was evaluated through feedback from the learners. The case utilised a Festo CP-Factory learning factory at NTNU.}
}
@article{OXMAN2000337,
title = {Design media for the cognitive designer1This paper is based on the keynote speech on `The Challenge of Design Computation' given by the author at ECAADE '97 in Vienna.1},
journal = {Automation in Construction},
volume = {9},
number = {4},
pages = {337-346},
year = {2000},
issn = {0926-5805},
doi = {https://doi.org/10.1016/S0926-5805(99)00017-5},
url = {https://www.sciencedirect.com/science/article/pii/S0926580599000175},
author = {Rivka Oxman},
keywords = {Generic design knowledge, Design collaboration, Re-representation, Typology, },
abstract = {Work on media for design which are responsive to the cognitive processes of the human designer are introduced as a paradigm for research and development. Design media are intended to support the cognitive nature of design and, particularly, the exploitation of design knowledge in computational environments. Basic theoretical assumptions are presented which underlie the development of design media. A central assumption is that designers share common forms of design knowledge which can be formalized, represented, and employed in computational environments. Generic knowledge is proposed as one such seminal form of design knowledge. We then develop a cognitive model which relates to the internal mental representations, strategies and mechanisms of generic design. The paper emphasizes the theoretical foundations of design media. This theoretical discussion is then exemplified through case studies presenting current research for the support of visual cognition in design. We introduce an approach to design schema as a visual form of generic design knowledge. Secondly we present a conceptual framework for the support of schema emergence in visual reasoning in design media. Finally, some implications of schema emergence in design collaboration are presented and discussed.}
}
@incollection{MANIKTALA2008247,
title = {Chapter 12 - Discussion Forums, Datasheets, and Other Real-World Issues},
editor = {Sanjaya Maniktala},
booktitle = {Troubleshooting Switching Power Converters},
publisher = {Newnes},
address = {Burlington},
pages = {247-289},
year = {2008},
isbn = {978-0-7506-8421-7},
doi = {https://doi.org/10.1016/B978-075068421-7.50014-X},
url = {https://www.sciencedirect.com/science/article/pii/B978075068421750014X},
author = {Sanjaya Maniktala},
abstract = {Publisher Summary
This chapter explains a few concepts such as thinking is the key, one needs to cross check everything, and product liability concerns. The chapter describes that the company's online tools can be used to discover design problems and correct them as long as thinking is applied as well. But what about “errors” in the online tools themselves? The chapter deals in and highlights that if the thinking process is done assiduously, sometimes one might arrive at the opposite conclusion that one initially foresaw. Anyone can even suddenly realize that he/she can be a part of the very problem that they are trying to fix; it could be in his/her own backyard. The chapter thinks about the customers and highlights that the very idea of a company starting a forum such as this one is essentially brilliant and thoroughly laudable. It also imparts a perception of transparency to their operations from the get-go. One should not outrightly believe anything and put in front of everyone, even if it is on semiglossy paper or in high-definition video or Flash HTML format. As engineers, one is required to put pen to paper, and at least do a sanity check.}
}
@article{LIN2024122254,
title = {Reinforcement learning and bandits for speech and language processing: Tutorial, review and outlook},
journal = {Expert Systems with Applications},
volume = {238},
pages = {122254},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122254},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423027562},
author = {Baihan Lin},
keywords = {Reinforcement learning, Bandits, Speech processing, Natural language processing, Speech recognition, Large language models, Survey, Perspective},
abstract = {In recent years, reinforcement learning and bandits have transformed a wide range of real-world applications including healthcare, finance, recommendation systems, robotics, and last but not least, the speech and natural language processing. While most speech and language applications of reinforcement learning algorithms are centered around improving the training of deep neural networks with its flexible optimization properties, there are still many grounds to explore to utilize the benefits of reinforcement learning, such as its reward-driven adaptability, state representations, temporal structures and generalizability. In this survey, we present an overview of recent advancements of reinforcement learning and bandits including those in the large language models, and discuss how they can be effectively employed to solve speech and natural language processing problems with models that are adaptive, interactive and scalable.}
}
@article{MORISHITA2023102079,
title = {Data assimilation and control system for adaptive model predictive control},
journal = {Journal of Computational Science},
volume = {72},
pages = {102079},
year = {2023},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2023.102079},
url = {https://www.sciencedirect.com/science/article/pii/S1877750323001394},
author = {Y. Morishita and S. Murakami and M. Yokoyama and G. Ueno},
keywords = {Data assimilation, Model-based control, Fusion plasma, ASTI},
abstract = {Model-based control of complex systems is a challenging task, particularly when the system model involves many uncertain elements. To achieve model predictive control of complex systems, we require a method that sequentially reduces uncertainties in the system model using observations and estimates control inputs under the model uncertainties. In this work, we propose an extended data assimilation framework, named data assimilation and control system (DACS), to integrate data assimilation and optimal control-input estimation. The DACS framework comprises a prediction step and three filtering steps and provides adaptive model predictive control algorithms. Since the DACS framework does not require additional prediction steps, the framework can even be applied to a large system in which iterative model prediction is prohibitive due to computational burden. Through numerical experiments in controlling virtual (numerically created) fusion plasma, we demonstrate the effectiveness of DACS and reveal the characteristics of the control performance related to the choice of hyper parameters and the discrepancies between the system model and the real system.}
}
@article{YU2022158,
title = {Bioinspired interactive neuromorphic devices},
journal = {Materials Today},
volume = {60},
pages = {158-182},
year = {2022},
issn = {1369-7021},
doi = {https://doi.org/10.1016/j.mattod.2022.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S1369702122002413},
author = {Jinran Yu and Yifei Wang and Shanshan Qin and Guoyun Gao and Chong Xu and Zhong {Lin Wang} and Qijun Sun},
keywords = {Neuromorphic devices, Synaptic transistors, Interactive, Neuromorphic computing, Bioinspired},
abstract = {The performance of conventional computer based on von Neumann architecture is limited due to the physical separation of memory and processor. By synergistically integrating various sensors with synaptic devices, recently emerging interactive neuromorphic devices can directly sense/store/process various stimuli information from external environments and implement functions of perception, learning, memory, and computation. In this review, we present the basic model of bioinspired interactive neuromorphic devices and discuss the performance metrics. Next, we summarize the recent progress and development of bioinspired interactive neuromorphic devices, which are classified into neuromorphic tactile systems, visual systems, auditory systems, and multisensory system. They are discussed in detail from the aspects of materials, device architectures, operating mechanisms, synaptic plasticity, and potential applications. Additionally, the bioinspired interactive neuromorphic devices that can fuse multiple/mixed sensing signals are proposed to address more realistic and sophisticated problems. Finally, we discuss the pros and cons regarding to the computing neurons and integrating sensory neurons and deliver the perspectives on interactive neuromorphic devices at the material, device, network, and system levels. It is believed the neuromorphic devices can provide promising solutions to next generation of interactive sensation/memory/computation toward the development of multimodal, low-power, and large-scale intelligent systems endowed with neuromorphic features.}
}
@article{LORENZODUS202015,
title = {The communicative modus operandi of online child sexual groomers: Recurring patterns in their language use},
journal = {Journal of Pragmatics},
volume = {155},
pages = {15-27},
year = {2020},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2019.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S0378216619306162},
author = {Nuria Lorenzo-Dus and Anina Kinzel and Matteo {Di Cristofaro}},
keywords = {Child sexual abuse, Online grooming, Linguistic patterns, Corpus assisted discourse studies},
abstract = {Online child sexual groomers manipulate their targets into partaking in sexual activity online and, in some cases, offline. To do so they use language (and other semiotic means, such as images) strategically. This study uses a Corpus-Assisted Discourse Studies methodology to identify recurring patterns in online groomers' language use, mapping them to the specific grooming goal that their use in context fulfils. The analysis of the groomers' language (c. 3.3 million words) within 622 conversations from the Perverted Justice website newly identifies 70 such recurring linguistic patterns (three-word collocations), as well as their relative strength of association to one or more grooming goals. The results can be used to inform computational models for detecting online child sexual grooming language. They can also support the development of training resources that raise awareness of typical language structures that characterise online sexual groomers’ communicative modus operandi.}
}
@article{PRABATHA2021116304,
title = {Community-level decentralized energy system planning under uncertainty: A comparison of mathematical models for strategy development},
journal = {Applied Energy},
volume = {283},
pages = {116304},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2020.116304},
url = {https://www.sciencedirect.com/science/article/pii/S0306261920316895},
author = {Tharindu Prabatha and Hirushie Karunathilake and Amin {Mohammadpour Shotorbani} and Rehan Sadiq and Kasun Hewage},
keywords = {Community energy planning, Renewable energy, Uncertainty modelling, Linear programming, Robust multi-objective optimization, Monte Carlo simulation},
abstract = {Distributed energy systems renewable energy are one solution to the environmental and economic concerns of energy use. While energy planning and optimization have been conducted mainly as a mathematical exercise, practical approaches that incorporate the engineering realities and uncertainties are limited. Decision makers find challenges in community energy planning due to the lack of expertise, planning tools, and information. While a multitude of models and tools are currently available, there are no means of identifying the most appropriate or accurate methods, especially considering uncertainty. The main objective of this study is to compare and identify the strengths and limitations of various mathematical modelling techniques used in energy planning for grid connected renewable energy systems. As a case study demonstration, different multi-objective optimization techniques with and without uncertainty consideration (i.e. robust optimization, linear optimization, Taguchi Orthogonal Array method, and Monte Carlo simulation) were applied on a selected neighborhood in British Columbia. The optimization outcomes and the time and effort for evaluation were compared for the different methods. The findings indicate that robust optimization can be used to develop an uncertainty-based decision model. It significantly reduces evaluation time compared to the other methods. Although the presence of uncertainties can change the optimal configuration of a planned energy system, the assessment method itself does not significantly impact the outcomes. The findings of this study will enable the energy planners and researchers to compare different multi-objective optimization techniques, and to select the best for planning renewable energy projects, especially during the pre-project planning stage.}
}
@article{LANDETE2024103034,
title = {Uncapacitated single-allocation hub median location with edge upgrading: Models and exact solution algorithms},
journal = {Transportation Research Part B: Methodological},
volume = {187},
pages = {103034},
year = {2024},
issn = {0191-2615},
doi = {https://doi.org/10.1016/j.trb.2024.103034},
url = {https://www.sciencedirect.com/science/article/pii/S0191261524001589},
author = {Mercedes Landete and Juan M. Muñoz-Ocaña and Antonio M. Rodríguez-Chía and Francisco Saldanha-da-Gama},
keywords = {Hub location, Hub-network design, Single allocation, Edge upgrading, MILP models, Branch-and-cut},
abstract = {In this paper, a class of single-allocation hub location problems is investigated from the perspective of upgrading. The latter is understood as an improvement of a set of edges to increase their individual performance, e.g., a decreased unit transportation cost. The goal is to obtain an improved optimal solution to the problem compared to that obtained if upgrading was not done. A budget constraint is assumed to limit the upgrading operations. A flow-based formulation is initially proposed that extends a classical model for uncapacitated single-allocation hub location with complete hub networks. Nevertheless, the fact that the unit costs after upgrading may violate the triangle inequality needs to be accounted for. Since the proposed formulation has a high computing burden, different possibilities are discussed for enhancing it. This leads to devising an efficient branch-and-cut algorithm with different variants. Additionally, a formulation based on the discrete ordered median function is also introduced that is also enhanced and embedded into a branch-and-cut algorithm again with several variants. All models and algorithms are also adapted to problems embedding hub network design decisions. Extensive computational tests were conducted to assess the methodological contributions proposed.}
}
@article{FOLLEY2003467,
title = {Psychoses and creativity: is the missing link a biological mechanism related to phospholipids turnover?},
journal = {Prostaglandins, Leukotrienes and Essential Fatty Acids},
volume = {69},
number = {6},
pages = {467-476},
year = {2003},
note = {Recent Advances of Membran e Pathology in Schizophrenia},
issn = {0952-3278},
doi = {https://doi.org/10.1016/j.plefa.2003.08.019},
url = {https://www.sciencedirect.com/science/article/pii/S0952327803001716},
author = {Bradley S Folley and Mikisha L Doop and Sohee Park},
keywords = {Creativity, Norepinephrine, Fatty acids, Schizophrenia, Psychoses},
abstract = {Recent evidence suggests that genetic and biochemical factors associated with psychoses may also provide an increased propensity to think creatively. The evolutionary theories linking brain growth and diet to the appearance of creative endeavors have been made recently, but they lack a direct link to research on the biological correlates of divergent and creative thought. Expanding upon Horrobin's theory that changes in brain size and in neural microconnectivity came about as a result of changes in dietary fat and phospholipid incorporation of highly unsaturated fatty acids, we propose a theory relating phospholipase A2 (PLA2) activity to the neuromodulatory effects of the noradrenergic system. This theory offers probable links between attention, divergent thinking, and arousal through a mechanism that emphasizes optimal individual functioning of the PLA2 and NE systems as they interact with structural and biochemical states of the brain. We hope that this theory will stimulate new research in the neural basis of creativity and its connection to psychoses.}
}
@article{MATT2013420,
title = {Implementation of Lean Production in Small Sized Enterprises},
journal = {Procedia CIRP},
volume = {12},
pages = {420-425},
year = {2013},
note = {Eighth CIRP Conference on Intelligent Computation in Manufacturing Engineering},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2013.09.072},
url = {https://www.sciencedirect.com/science/article/pii/S2212827113007130},
author = {D.T. Matt and E. Rauch},
keywords = {Manufacturing, Productivity, Small Enterprises},
abstract = {The introduction and implementation of Lean Production Principles over the last twenty years has had a notable impact on many manufacturing enterprises. The practice shows that lean production methods and instruments are not equally applicable to large and small companies. After the implementation in large enterprises belonging to the automotive sector the concept of lean thinking was introduced successfully in medium sized enterprises. Small enterprises have been ignored for a long time and special investigations about this topic are rarely. Considering statistical data and analysis about the economic importance of small enterprises we can see, that they are numerous and create a notable part of the total value added in the non-financial business economy. This paper analysis in a first step the role and potential of small enterprises – especially in Italy – and shows then a preliminary study of the suitability of existing lean methods for the application in this type of organization. The research was combined with an industrial case study in a small enterprise to analyse the difficulties in the implementation stage and to identify the critical success factors. The results of this preliminary study should illustrate the existing hidden potential in small enterprises as well as a selection of suitable methods for productivity improvements. This research will be the base for a further and more detailed research project.}
}
@article{DUFVA201697,
title = {Metaphors of code—Structuring and broadening the discussion on teaching children to code},
journal = {Thinking Skills and Creativity},
volume = {22},
pages = {97-110},
year = {2016},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2016.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1871187116301055},
author = {Tomi Dufva and Mikko Dufva},
keywords = {Code, Code literacy, Metaphors, Education, Programming, Teaching programming, Pedagogy, Media literacy},
abstract = {Digital technology has become embedded into our daily lives. Code is at the heart of this technology. The way code is perceived influences the way our everyday interaction with digital technologies is perceived: is it an objective exchange of ones and zeros, or a value- laden power struggle between white male programmers and those who think they are users, when they are, in fact, the product being sold. Understanding the nature of code thus enables the imagination and exploration of the present state and alternative future developments of digital technologies. A wider imagination is especially important for developing basic education so that it provides the capabilities for coping with these developments. Currently, the discussion has been mainly on the technical details of code. We study how to broaden this narrow view in order to support the design of more comprehensive and future-proof education around code and coding. We approach the concept of code through nine different metaphors from the existing literature on systems thinking and organisational studies. The metaphors we use are machine, organism, brain, flux and transformation, culture, political system, psychic prison, instrument of domination and carnival. We describe their epistemological backgrounds and give examples of how code is perceived through each of them. We then use the metaphors in order to suggest different complementary ways that ICT could be taught in schools. The metaphors illustrate different contexts and help to interpret the discussions related to developments in digital technologies such as free software movement, democratization of information and internet of things. They also help to identify the dominant views and the tensions between the views. We propose that the systematic use of metaphors described in this paper would be a useful tool for broadening and structuring the dialogue about teaching children to code.}
}
@article{LIN2023103217,
title = {A novel personality detection method based on high-dimensional psycholinguistic features and improved distributed Gray Wolf Optimizer for feature selection},
journal = {Information Processing & Management},
volume = {60},
number = {2},
pages = {103217},
year = {2023},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.103217},
url = {https://www.sciencedirect.com/science/article/pii/S0306457322003181},
author = {Hao Lin and Chundong Wang and Qingbo Hao},
keywords = {Personality detection, Feature selection, Symmetric uncertainty, Grey Wolf Optimizer, Spark},
abstract = {Existing personality detection methods based on user-generated text have two major limitations. First, they rely too much on pre-trained language models to ignore the sentiment information in psycholinguistic features. Secondly, they have no consensus on the psycholinguistic feature selection, resulting in the insufficient analysis of sentiment information. To tackle these issues, we propose a novel personality detection method based on high-dimensional psycholinguistic features and improved distributed Gray Wolf Optimizer (GWO) for feature selection (IDGWOFS). Specifically, we introduced the Gaussian Chaos Map-based initialization and neighbor search strategy into the original GWO to improve the performance of feature selection. To eliminate the bias generated when using mutual information to select features, we adopt symmetric uncertainty (SU) instead of mutual information as the evaluation for correlation and redundancy to construct the fitness function, which can balance the correlation between features–labels and the redundancy between features–features. Finally, we improve the common Spark-based parallelization design of GWO by parallelizing only the fitness computation steps to improve the efficiency of IDGWOFS. The experiments indicate that our proposed method obtains average accuracy improvements of 3.81% and 2.19%, and average F1 improvements of 5.17% and 5.8% on Essays and Kaggle MBTI dataset, respectively. Furthermore, IDGWOFS has good convergence and scalability.}
}
@article{TSOTSOS2021305,
title = {On the control of attentional processes in vision},
journal = {Cortex},
volume = {137},
pages = {305-329},
year = {2021},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2021.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0010945221000150},
author = {John K. Tsotsos and Omar Abid and Iuliia Kotseruba and Markus D. Solbach},
keywords = {Vision, Attention, Control, Cognitive program, Selective tuning},
abstract = {The study of attentional processing in vision has a long and deep history. Recently, several papers have presented insightful perspectives into how the coordination of multiple attentional functions in the brain might occur. These begin with experimental observations and the authors propose structures, processes, and computations that might explain those observations. Here, we consider a perspective that past works have not, as a complementary approach to the experimentally-grounded ones. We approach the same problem as past authors but from the other end of the computational spectrum, from the problem nature, as Marr's Computational Level would prescribe. What problem must the brain solve when orchestrating attentional processes in order to successfully complete one of the myriad possible visuospatial tasks at which we as humans excel? The hope, of course, is for the approaches to eventually meet and thus form a complete theory, but this is likely not soon. We make the first steps towards this by addressing the necessity of attentional control, examining the breadth and computational difficulty of the visuospatial and attentional tasks seen in human behavior, and suggesting a sketch of how attentional control might arise in the brain. The key conclusions of this paper are that an executive controller is necessary for human attentional function in vision, and that there is a 'first principles' computational approach to its understanding that is complementary to the previous approaches that focus on modelling or learning from experimental observations directly.}
}
@article{DAN2025100814,
title = {Social robot assisted music course based on speech sensing and deep learning algorithms},
journal = {Entertainment Computing},
volume = {52},
pages = {100814},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100814},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124001824},
author = {Xiao Dan},
keywords = {Speech sensing, Deep learning algorithms, Social robots, Course in music},
abstract = {In the field of social robot teaching, research has focused on how to use technological means to provide better learning support and personalized interactive experiences. Social robots can interact with students and provide personalized learning support, thereby improving their learning effectiveness and engagement. The speech sensing model of social robots can perceive students’ emotions and feedback in real-time through technologies such as speech recognition and sentiment analysis, thereby providing intelligent responses and guidance. The deep learning recommendation model for music course resources extracts music features through deep learning techniques, and combines session interest extraction techniques to personalized recommend music resources suitable for students’ interests and abilities. By analyzing students’ interests and learning goals, robots can provide music learning resources that meet their needs based on recommendation algorithms, further stimulating their learning interest and enthusiasm. The experimental results show that the use of social robots in the learning environment significantly improves the learning effectiveness and participation of students. Through personalized interaction and intelligent response guidance, students are more likely to understand and master music knowledge, while experiencing joyful and positive learning emotions. The study validated the effectiveness of social robot assisted music courses based on speech sensing and deep learning algorithms, demonstrating its advantages in improving student learning effectiveness and engagement.}
}
@article{BARTELL20051283,
title = {How hydrides misled chemists},
journal = {Spectrochimica Acta Part A: Molecular and Biomolecular Spectroscopy},
volume = {61},
number = {7},
pages = {1283-1286},
year = {2005},
note = {Honour Issue - Jim Durig},
issn = {1386-1425},
doi = {https://doi.org/10.1016/j.saa.2004.11.056},
url = {https://www.sciencedirect.com/science/article/pii/S1386142504006481},
author = {Lawrence S. Bartell},
keywords = {Models of molecular structure and force fields, Simple hydrides, Ligand-close-packing, Cautionary stories},
abstract = {Hydrogen-containing molecules are simple enough to be attractive subjects in experimental diffraction and spectroscopic studies and in quantum computations. Yet, the inferences about molecular structure and force fields originally drawn from studies of these subjects were significantly flawed. In recent developments the original models of structure invoked, such as hybridization, have been superseded. The reasons for this are briefly reviewed. What has emerged to account for molecular geometry, prevailing even over the popular VSEPR theory, is a model of geminal nonbonded interactions.}
}
@incollection{HASKELL200195,
title = {Chapter 6 - Knowledge Base and Transfer: On the Usefulness of Useless Knowledge},
editor = {Robert E. Haskell},
booktitle = {Transfer of Learning},
publisher = {Academic Press},
address = {San Diego},
pages = {95-113},
year = {2001},
series = {Educational Psychology},
issn = {18716148},
doi = {https://doi.org/10.1016/B978-012330595-4/50007-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012330595450007X},
author = {Robert E. Haskell},
abstract = {Publisher Summary
Knowledge base broadly includes knowledge acquired by reading, personal experience, careful listening, and astute observing. It also includes thinking and it is the absolute requirement not only for transfer but for thinking and reasoning. This chapter presents the general knowledge-base conditions necessary for optimal transfer to occur and explain its importance. Transfer of learning requires more than quick-fix strategies and learners must have a knowledge base in a subject in order to even know enough to ask questions about it. Without a sufficient knowledge base, novices do not have a framework within which to formulate adequate questions. Possessing a large knowledge base enables a learner to think about the subject in depth. To achieve general transfer, one often requires much more than immediately useful knowledge. It requires learning that may be considered useless knowledge. There are several examples of knowledge that appeared to have absolutely no use, but years later, these "useless" knowledge was applied to other areas, which later had major applied importance.}
}
@article{CHIU2024100171,
title = {What are artificial intelligence literacy and competency? A comprehensive framework to support them},
journal = {Computers and Education Open},
volume = {6},
pages = {100171},
year = {2024},
issn = {2666-5573},
doi = {https://doi.org/10.1016/j.caeo.2024.100171},
url = {https://www.sciencedirect.com/science/article/pii/S2666557324000120},
author = {Thomas K.F. Chiu and Zubair Ahmad and Murod Ismailov and Ismaila Temitayo Sanusi},
keywords = {AI literacy, AI competency, K-12 education, Machine learning, Data literacy, Generative AI},
abstract = {Artificial intelligence (AI) education in K–12 schools is a global initiative, yet planning and executing AI education is challenging. The major frameworks are focused on identifying content and technical knowledge (AI literacy). Most of the current definitions of AI literacy for a non-technical audience are developed from an engineering perspective and may not be appropriate for K–12 education. Teacher perspectives are essential to making sense of this initiative. Literacy is about knowing (knowledge, what skills); competency is about applying the knowledge in a beneficial way (confidence, how well). They are strongly related. This study goes beyond knowledge (AI literacy), and its two main goals are to (i) define AI literacy and competency by adding the aspects of confidence and self-reflective mindsets, and (ii) propose a more comprehensive framework for K–12 AI education. These definitions are needed for this emerging and disruptive technology (e.g., ChatGPT and Sora, generative AI). We used the definitions and the basic curriculum design approaches as the analytical framework and teacher perspectives. Participants included 30 experienced AI teachers from 15 middle schools. We employed an iterative co-design cycle to discuss and revise the framework throughout four cycles. The definition of AI competency has five abilities that take confidence into account, and the proposed framework comprises five key components: technology, impact, ethics, collaboration, and self-reflection. We also identify five effective learning experiences to foster abilities and confidences, and suggest five future research directions: prompt engineering, data literacy, algorithmic literacy, self-reflective mindset, and empirical research.}
}
@incollection{SCHONER202471,
title = {Chapter 4 - Toward a neural theory of goal-directed reaching movements},
editor = {Mindy F. Levin and Maurizio Petrarca and Daniele Piscitelli and Susanna Summa},
booktitle = {Progress in Motor Control},
publisher = {Academic Press},
pages = {71-102},
year = {2024},
isbn = {978-0-443-23987-8},
doi = {https://doi.org/10.1016/B978-0-443-23987-8.00008-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443239878000080},
author = {Gregor Schöner and Lukas Bildheim and Lei Zhang},
keywords = {Dynamic field theory, Neural dynamics, Neural timers, Target selection, Movement initiation, Mouse-tracking paradigm, Degrees of freedom problem, Posture movement problem},
abstract = {How do we bring about goal-directed motor acts? Reaching for an object that offers a useful exemplary case around which the processes underlying human movement behavior can be studied. Such reaching entails processes from scene and object perception, target selection, and movement initiation, to timing and control. These processes are typically studied in different subdisciplines, using different methods based on different theoretical concepts. Yet they are continuously coupled online and evolve in a closed loop. Understanding how they work together thus requires an integrative theoretical framework. While abstract computational ideas are often invoked for such integration, we argue for a theoretical account that is grounded in neural principles. We review the key concepts of a neural theory of goal-directed reaching movements that draw on neural dynamic models of population activation in which recurrent connectivity provides stability. For each component process, we discuss the key issues and empirical constraints for a neural dynamic account. Although a complete neural architecture of goal-directed movement behavior is still under development, the outline we provide interfaces with a large set of empirical findings.}
}
@article{PITKOW2017943,
title = {Inference in the Brain: Statistics Flowing in Redundant Population Codes},
journal = {Neuron},
volume = {94},
number = {5},
pages = {943-953},
year = {2017},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2017.05.028},
url = {https://www.sciencedirect.com/science/article/pii/S089662731730466X},
author = {Xaq Pitkow and Dora E. Angelaki},
keywords = {brain, inference, theory, population code, message-passing, redundant, coding, nuisance, nonlinear},
abstract = {It is widely believed that the brain performs approximate probabilistic inference to estimate causal variables in the world from ambiguous sensory data. To understand these computations, we need to analyze how information is represented and transformed by the actions of nonlinear recurrent neural networks. We propose that these probabilistic computations function by a message-passing algorithm operating at the level of redundant neural populations. To explain this framework, we review its underlying concepts, including graphical models, sufficient statistics, and message-passing, and then describe how these concepts could be implemented by recurrently connected probabilistic population codes. The relevant information flow in these networks will be most interpretable at the population level, particularly for redundant neural codes. We therefore outline a general approach to identify the essential features of a neural message-passing algorithm. Finally, we argue that to reveal the most important aspects of these neural computations, we must study large-scale activity patterns during moderately complex, naturalistic behaviors.}
}
@article{TEY2021153,
title = {The Impact of Concession Patterns on Negotiations: When and Why Decreasing Concessions Lead to a Distributive Disadvantage},
journal = {Organizational Behavior and Human Decision Processes},
volume = {165},
pages = {153-166},
year = {2021},
issn = {0749-5978},
doi = {https://doi.org/10.1016/j.obhdp.2021.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0749597821000613},
author = {Kian Siong Tey and Michael Schaerer and Nikhil Madan and Roderick I. Swaab},
keywords = {Negotiations, Concessions, Reservation price, Offers, Signaling, Distributive},
abstract = {We propose that making a series of decreasing concessions (e.g., $1,500–1,210–1,180–1,170) signals that negotiators are reaching their limit and that this results in a negotiation disadvantage for offer recipients. Although we find that most negotiators do not use this strategy naturally, seven studies (N = 2,311) demonstrate that decreasing concessions causes recipients to make less ambitious counteroffers (Studies 1–5) and reach worse deals (Study 2) in distributive negotiations. We find that this disadvantage occurs because decreasing concessions shape recipients’ expectations of the subsequent offers that will be made, which results in inflated perceptions of the counterparts’ reservation price relative to the other concession strategies (Study 3). In addition, we find that this disadvantage is particularly large when concessions decrease at a moderate rate (Study 4a) and when decreasing concessions takes place over more (vs. fewer) rounds (Study 4b). Finally, we find that recipients can protect themselves against the deleterious effects of decreasing concession by thinking of a target before they enter the negotiation (Study 5).}
}
@article{PERRIN20227006,
title = {Malonic Anhydrides, Challenges from a Simple Structure},
journal = {The Journal of Organic Chemistry},
volume = {87},
number = {11},
pages = {7006-7012},
year = {2022},
issn = {0022-3263},
doi = {https://doi.org/10.1021/acs.joc.2c00453},
url = {https://www.sciencedirect.com/science/article/pii/S0022326322022642},
author = {Charles L. Perrin},
abstract = {ABSTRACT
After many years of unsuccessful attempts, monomeric malonic anhydrides were prepared by ozonolysis of ketene dimers, a procedure validated by model studies. The structure proof relied most heavily on IR absorption at 1820 cm–1 and a Raman band at 1947 cm–1. Malonic anhydrides are unstable, decomposing below room temperature to a ketene plus carbon dioxide. Surprisingly, according to kinetic studies, the dimethyl derivative is slightly less unstable than the parent, and the monomethyl is the fastest to decompose, with an enthalpy of activation of only 12.6 kcal/mol. Computations rationalize this behavior in terms of a concerted [2s + 2a] cycloreversion that requires a more highly organized transition state, as also manifested by a negative entropy of activation.}
}
@article{LI2013262,
title = {Improved Particle Filter for Target Tracing Application based on ChinaGrid},
journal = {AASRI Procedia},
volume = {5},
pages = {262-267},
year = {2013},
note = {2013 AASRI Conference on Parallel and Distributed Computing and Systems},
issn = {2212-6716},
doi = {https://doi.org/10.1016/j.aasri.2013.10.087},
url = {https://www.sciencedirect.com/science/article/pii/S2212671613000887},
author = {Yuqiang Li and Xixu He and Haitao Jia},
keywords = {Target tracing, Particle filter, ChinaGrid},
abstract = {Most practical target tracking are usually maneuvering, while most target tracking algorithm are linear filter. More estimation error is introduced from linear filter. Nowadays more and more researchers pay their attention in Maneuvering Target Tracking algorithm. Particle filter has been developed for estimation of nonlinear system states. This paper presents an improved particle filter, which can apply the maneuvering target tracking problem. In practice, the particle filter would take abundant computation for estimate the maneuvering target tracking. The ChinaGrid system use the agile and distributed federations to reduce the computing time, which achieve to fast resolution for particle filter computation of target tracing application. Lastly the simulation proves it.}
}
@article{THACKER2023101782,
title = {Climate change by the numbers: Leveraging mathematical skills for science learning online},
journal = {Learning and Instruction},
volume = {86},
pages = {101782},
year = {2023},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2023.101782},
url = {https://www.sciencedirect.com/science/article/pii/S0959475223000518},
author = {Ian Thacker},
keywords = {Climate change, Conceptual change, Epistemic dispositions, Numerical estimation, Plausibility judgments, Learning technology},
abstract = {The purpose of this preregistered study was to test an online intervention that presents participants with novel numbers about climate change after they estimate those numbers. An experimental study design was used to investigate the impact of the intervention on undergraduate students’ climate change understanding and perceptions that human caused climate change is plausible. Findings revealed that posttest climate change knowledge and plausibility perceptions were higher among those randomly assigned to use the intervention compared with those assigned to a control condition, and that supplementing this experience with numeracy instruction was linked with the use of more explicit estimation strategies and greater learning gains for people with adaptive epistemic dispositions. Findings from this study replicate and extend prior research, support the idea that novel data can support knowledge revision, identify estimation strategies used in this context, and offer an open-source online intervention for sharing surprising data with students and teachers.}
}
@article{BENBOW19841643,
title = {Computational analysis of polymer processing: (Edited by J.R.A. Pearson and S.M. Richardson). Applied Science, 1983. £36.00. 343pp},
journal = {Chemical Engineering Science},
volume = {39},
number = {11},
pages = {1643},
year = {1984},
issn = {0009-2509},
doi = {https://doi.org/10.1016/0009-2509(84)80093-7},
url = {https://www.sciencedirect.com/science/article/pii/0009250984800937},
author = {J.J. Benbow}
}
@article{RINGE2023101268,
title = {Cation effects on electrocatalytic reduction processes at the example of the hydrogen evolution reaction},
journal = {Current Opinion in Electrochemistry},
volume = {39},
pages = {101268},
year = {2023},
issn = {2451-9103},
doi = {https://doi.org/10.1016/j.coelec.2023.101268},
url = {https://www.sciencedirect.com/science/article/pii/S2451910323000613},
author = {Stefan Ringe},
keywords = {Cation effects, Hydrogen evolution reaction, Hydrogen underpotential deposition, CO reduction, Electric double layer, Solid-liquid interface},
abstract = {Cation effects provide invaluable insights into electrochemistry. In this review, I discuss them with a main focus on the hydrogen evolution reaction and a summary of recent in situ spectroscopic and electrochemical measurements as well as advanced computational simulation results conducted at varying cation identities, concentrations, and pH. According to these works, the interfacial cation concentration is the main descriptor to explain cation and pH effects. The detailed mechanism (such as e.g. water polarization, water structure changes, field-stabilization of intermediates) depends strongly on potential, pH, oxophilicity of the electrode, or the nature of the rate-limiting step and proton donor. With growing convergence in this field, cation effects remain a highly challenging and promising topic for research.}
}
@article{BRINK201339,
title = {Computing with networks of spiking neurons on a biophysically motivated floating-gate based neuromorphic integrated circuit},
journal = {Neural Networks},
volume = {45},
pages = {39-49},
year = {2013},
note = {Neuromorphic Engineering: From Neural Systems to Brain-Like Engineered Systems},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2013.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S089360801300052X},
author = {S. Brink and S. Nease and P. Hasler},
keywords = {Neuromorphic VLSI, Floating-gate transistor, Single transistor learning synapse, Spiking winner-take-all, Synfire chain},
abstract = {Results are presented from several spiking network experiments performed on a novel neuromorphic integrated circuit. The networks are discussed in terms of their computational significance, which includes applications such as arbitrary spatiotemporal pattern generation and recognition, winner-take-all competition, stable generation of rhythmic outputs, and volatile memory. Analogies to the behavior of real biological neural systems are also noted. The alternatives for implementing the same computations are discussed and compared from a computational efficiency standpoint, with the conclusion that implementing neural networks on neuromorphic hardware is significantly more power efficient than numerical integration of model equations on traditional digital hardware.}
}
@article{TIAN2016363,
title = {Identifying informative energy data in Bayesian calibration of building energy models},
journal = {Energy and Buildings},
volume = {119},
pages = {363-376},
year = {2016},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2016.03.042},
url = {https://www.sciencedirect.com/science/article/pii/S0378778816301967},
author = {Wei Tian and Song Yang and Zhanyong Li and Shen Wei and Wei Pan and Yunliang Liu},
keywords = {Bayesian computation, Cluster analysis, Model calibration, Building energy},
abstract = {Bayesian computation has received increasing attention in calibrating building energy models due to its flexibility and accuracy. However, there has been little research on how to determine informative energy data in Bayesian calibration in building energy models. Therefore, this study aims to determine and choose informative energy data using correlation analysis and hierarchical clustering method. A case study of retail building is used to demonstrate the proposed methods to infer four unknown input parameters using EnergyPlus program. The results indicate that the different combinations of energy data can provide various levels of accuracy in estimating unknown input variables in model calibration. This suggests that Bayesian computation is suitable for inferring the parameters when there are missing energy data that can be treated as uninformative output data. The proposed method can be also used to find the redundant information on energy data in order to improve computational efficiency in Bayesian calibration.}
}
@article{DARROCH2023105989,
title = {The rangeomorph Pectinifrons abyssalis: Hydrodynamic function at the dawn of animal life},
journal = {iScience},
volume = {26},
number = {2},
pages = {105989},
year = {2023},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2023.105989},
url = {https://www.sciencedirect.com/science/article/pii/S2589004223000664},
author = {Simon A.F. Darroch and Susana Gutarra and Hale Masaki and Andrei Olaru and Brandt M. Gibson and Frances S. Dunn and Emily G. Mitchell and Rachel A. Racicot and Gregory Burzynski and Imran A. Rahman},
keywords = {Zoology, Evolutionary biology, Paleobiology},
abstract = {Summary
Rangeomorphs are among the oldest putative eumetazoans known from the fossil record. Establishing how they fed is thus key to understanding the structure and function of the earliest animal ecosystems. Here, we use computational fluid dynamics to test hypothesized feeding modes for the fence-like rangeomorph Pectinifrons abyssalis, comparing this to the morphologically similar extant carnivorous sponge Chondrocladia lyra. Our results reveal complex patterns of flow around P. abyssalis unlike those previously reconstructed for any other Ediacaran taxon. Comparisons with C. lyra reveal substantial differences between the two organisms, suggesting they converged on a similar fence-like morphology for different functions. We argue that the flow patterns recovered for P. abyssalis do not support either a suspension feeding or osmotrophic feeding habit. Instead, our results indicate that rangeomorph fronds may represent organs adapted for gas exchange. If correct, this interpretation could require a dramatic reinterpretation of the oldest macroscopic animals.}
}
@article{FALCONE2020110269,
title = {Soft computing techniques in structural and earthquake engineering: a literature review},
journal = {Engineering Structures},
volume = {207},
pages = {110269},
year = {2020},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2020.110269},
url = {https://www.sciencedirect.com/science/article/pii/S0141029619322540},
author = {Roberto Falcone and Carmine Lima and Enzo Martinelli},
keywords = {Structural engineering, Earthquake engineering, Fuzzy logic, Neural network, Swarm intelligence, Evolutionary computing},
abstract = {Although civil engineering problems are often characterized by significant levels of complexity, they are generally approached and solved by combining several practitioners’ skills, such as intuition, past experience, logical reasoning, mathematical elaborations, and physical sense. This is also the case of problems in structural and earthquake engineering whose solution is generally based on the so-called “engineer’s judgment”. However, heuristic theories and algorithms within the framework of “soft computing” can provide a more rational and systematic way to approach and solve problems in these areas. As a matter of fact, the aforementioned algorithms have been recently utilized in several branches of engineering and applied sciences. This paper proposes a state-of-the-art review of the main applications of soft computing techniques to relevant structural and earthquake engineering problems. Specifically, the applications of fuzzy computing, evolutionary computing, swarm intelligence, and neural networks, as well as their hybrid combinations, are analyzed with the aim to examine their capability and limitations in modeling, simulation, and optimization problems.}
}
@article{ZHANG201313,
title = {A semantic representation model for design rationale of products},
journal = {Advanced Engineering Informatics},
volume = {27},
number = {1},
pages = {13-26},
year = {2013},
note = {Modeling, Extraction, and Transformation of Semantics in Computer Aided Engineering Systems},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2012.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S1474034612000985},
author = {Yingzhong Zhang and Xiaofang Luo and Jian Li and Jennifer J. Buis},
keywords = {Design rationale, Design semantics, Representation model, Ontology},
abstract = {Design rationale (DR) is crucial information in product design decision support, design analysis and design reuse. In this paper, based on the Issue-based Information System (IBIS) model, a new ontology-based semantic representation model for DR information; the integrated issue, solution, artifact and argument (ISAA) model; is proposed. The ISAA model introduces the ontology-based semantic representation mode to the DR representation and expands the concept elements of IBIS. The class of concept elements and the semantic relationships among them are defined by Web Ontology Language (OWL). The axioms and rules which are used to reason and analyze DR are defined and encoded with Semantic Web Rule Language (SWRL), which enrich the semantic relations defined by OWL. The ISAA model represents the directed graph of IBIS to the Resource Description Framework (RDF) graph and serializes to an RDF/XML document which lays the foundation for retrieving and reasoning semantic information of DR. A semantic annotator integrating with the visual product design model was developed, by which the discrete information of thinking is captured and abstracted to a conceptual representation of the ISAA model. Finally, an example of DR representation for the spring operating mechanism of a high-voltage circuit breaker product is given.}
}
@article{LO2022111357,
title = {Architectural patterns for the design of federated learning systems},
journal = {Journal of Systems and Software},
volume = {191},
pages = {111357},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111357},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222000899},
author = {Sin Kit Lo and Qinghua Lu and Liming Zhu and Hye-Young Paik and Xiwei Xu and Chen Wang},
keywords = {Federated learning, Pattern, Software architecture, Machine learning, Artificial intelligence},
abstract = {Federated learning has received fast-growing interests from academia and industry to tackle the challenges of data hungriness and privacy in machine learning. A federated learning system can be viewed as a large-scale distributed system with different components and stakeholders as numerous client devices participate in federated learning. Designing a federated learning system requires software system design thinking apart from the machine learning knowledge. Although much effort has been put into federated learning from the machine learning technique aspects, the software architecture design concerns in building federated learning systems have been largely ignored. Therefore, in this paper, we present a collection of architectural patterns to deal with the design challenges of federated learning systems. Architectural patterns present reusable solutions to a commonly occurring problem within a given context during software architecture design. The presented patterns are based on the results of a systematic literature review and include three client management patterns, four model management patterns, three model training patterns, four model aggregation patterns, and one configuration pattern. The patterns are associated to the particular state transitions in a federated learning model lifecycle, serving as a guidance for effective use of the patterns in the design of federated learning systems.}
}
@article{GRAYSON2022108844,
title = {R Markdown as a dynamic interface for teaching: Modules from math and biology classrooms},
journal = {Mathematical Biosciences},
volume = {349},
pages = {108844},
year = {2022},
issn = {0025-5564},
doi = {https://doi.org/10.1016/j.mbs.2022.108844},
url = {https://www.sciencedirect.com/science/article/pii/S0025556422000499},
author = {Kristine L. Grayson and Angela K. Hilliker and Joanna R. Wares},
keywords = {R markdown, Data visualization, Pedagogy, Herd immunity, Teaching programming},
abstract = {Advancing technologies, including interactive tools, are changing classroom pedagogy across academia. Here, we discuss the R Markdown interface, which allows for the creation of partial or complete interactive classroom modules for courses using the R programming language. R Markdown files mix sections of R code with formatted text, including LaTeX, which are rendered together to form complete reports and documents. These features allow instructors to create classroom modules that guide students through concepts, while providing areas for coding and text response by students. Students can also learn to create their own reports for more independent assignments. After presenting the features and uses of R Markdown to enhance teaching and learning, we present examples of materials from two courses. In a Computational Modeling course for math students, we used R Markdown to guide students through exploring mathematical models to understand the principle of herd immunity. In a Data Visualization and Communication course for biology students, we used R Markdown for teaching the fundamentals of R programming and graphing, and for students to learn to create reproducible data investigations. Through these examples, we demonstrate the benefits of R Markdown as a dynamic teaching and learning tool.}
}
@article{KAPUSTINA2024100072,
title = {User-friendly and industry-integrated AI for medicinal chemists and pharmaceuticals},
journal = {Artificial Intelligence Chemistry},
volume = {2},
number = {2},
pages = {100072},
year = {2024},
issn = {2949-7477},
doi = {https://doi.org/10.1016/j.aichem.2024.100072},
url = {https://www.sciencedirect.com/science/article/pii/S2949747724000307},
author = {Olga Kapustina and Polina Burmakina and Nina Gubina and Nikita Serov and Vladimir Vinogradov},
keywords = {Machine Learning, Medicinal Chemistry, Pharmaceutics, Data-Driven Drug Discovery},
abstract = {Artificial intelligence has brought crucial changes to the whole field of natural sciences. Myriads of machine learning algorithms have been developed to facilitate the work of experimental scientists. Molecular property prediction and drug synthesis planning become routine tasks. Moreover, inverse design of compounds with tunable properties as well as on-the-fly autonomous process optimization and chemical space exploration became possible in silico. Affordable robotic platforms exist able to perform thousands of experiments every day, analyzing the results and tuning the protocols. Despite this, most of these developments get trapped at the stage of code or overlooked, limiting their use by experimental scientists. Meanwhile, visibility and the number of user-friendly tools and technologies available to date is too low to compensate for this fact, rendering the development of novel therapeutic compounds inefficient. In this Review, we set the goal to bridge the gap between modern technologies and experimental scientists to improve drug development efficacy. Here we survey advanced and easy-to-use technologies able to help medical chemists at every stage of their research, including those integrated in technological processes during COVID-19 pandemic motivated by the need for fast yet precise solutions. Moreover, we review how these technologies are integrated by industry and clinics to streamline drug development and production. These technologies already transform the current paradigm of scientific thinking and revolutionize not only medicinal chemistry, but the whole field of natural sciences.}
}
@article{CRESPO201916,
title = {General solution procedures to compute the stored energy density of conservative solids directly from experimental data},
journal = {International Journal of Engineering Science},
volume = {141},
pages = {16-34},
year = {2019},
issn = {0020-7225},
doi = {https://doi.org/10.1016/j.ijengsci.2019.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0020722517327635},
author = {José Crespo and Francisco J. Montáns},
keywords = {Hyperelasticity, Soft materials, Classical invariants, Data-driven constitutive modelling},
abstract = {Energy-conservative, hyperelastic solids assume the existence of a stored energy density which relates stresses and strains for any deformation state. The usual approach to model such materials is to impose an analytical expression of the stored energy function as a function of some invariants and material parameters. These material parameters are best-fitted to available experimental data. This approach is good for analytical derivations but less optimal for data-driven computational approaches and for accurate and efficient finite element analyses. We show in this paper that the stored energy solution of a solid may be accurately obtained in a general case from suitable numerical procedures, regardless of the invariants being use, without using material parameters nor fitting any assumed analytical form. We explain two general, simple, computational procedures to solve the problem. The numerically computed stored energies may be used in general-purpose finite element programs, yielding more general procedures that have an efficiency equivalent to that of the classical approach, which uses pre-defined analytical “models” and fitting parameters.}
}
@article{POVALA2022114712,
title = {Variational Bayesian approximation of inverse problems using sparse precision matrices},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {393},
pages = {114712},
year = {2022},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2022.114712},
url = {https://www.sciencedirect.com/science/article/pii/S0045782522000822},
author = {Jan Povala and Ieva Kazlauskaite and Eky Febrianto and Fehmi Cirak and Mark Girolami},
keywords = {Inverse problems, Bayesian inference, Variational Bayes, Precision matrix, Uncertainty quantification},
abstract = {Inverse problems involving partial differential equations (PDEs) are widely used in science and engineering. Although such problems are generally ill-posed, different regularisation approaches have been developed to ameliorate this problem. Among them is the Bayesian formulation, where a prior probability measure is placed on the quantity of interest. The resulting posterior probability measure is usually analytically intractable. The Markov Chain Monte Carlo (MCMC) method has been the go-to method for sampling from those posterior measures. MCMC is computationally infeasible for large-scale problems that arise in engineering practice. Lately, Variational Bayes (VB) has been recognised as a more computationally tractable method for Bayesian inference, approximating a Bayesian posterior distribution with a simpler trial distribution by solving an optimisation problem. In this work, we argue, through an empirical assessment, that VB methods are a flexible and efficient alternative to MCMC for this class of problems. We propose a natural choice of a family of Gaussian trial distributions parametrised by precision matrices, thus taking advantage of the inherent sparsity of the inverse problem encoded in its finite element discretisation. We utilise stochastic optimisation to efficiently estimate the variational objective and assess not only the error in the solution mean but also the ability to quantify the uncertainty of the estimate. We test this on PDEs based on the Poisson equation in 1D and 2D. A Tensorflow implementation is made publicly available on GitHub.}
}
@article{PADILLARIVERA201979,
title = {A systematic review of the sustainability assessment of bioenergy: The case of gaseous biofuels},
journal = {Biomass and Bioenergy},
volume = {125},
pages = {79-94},
year = {2019},
issn = {0961-9534},
doi = {https://doi.org/10.1016/j.biombioe.2019.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0961953419301102},
author = {Alejandro Padilla-Rivera and María Guadalupe Paredes and Leonor Patricia Güereca},
keywords = {Sustainability assessment, Systematic review, Bioenergy, Gaseous biofuels, Renewable energy, Sustainability of bioenergy},
abstract = {In recent years, achieving sustainability in renewable energy systems has become important for achieving future economic prosperity and energy security all over the world; therefore, multiple attempts have been made to assess their sustainability. This means that in addition to considering the technological and economic factors, environmental and social aspects should also be considered. However, the wide-ranging concept of sustainability and the various methodological frameworks presented make their interpretation and correct implementation difficult. In this research, through a systematic literature review, we summarize and analyze the current research on the sustainability assessment of bioenergy production/use (also referred as gaseous biofuels) for electricity and heat generation. Sustainability approaches and their underlying factors from the three dimensions of sustainability were consolidated and structured in this systematic review. In addition, a set of indicators (environmental, social and economic) is provided based on the literature analyzed that decision makers can use to evaluate the sustainability performance of bioenergy systems. The main finding indicates that although there are various international efforts on measuring sustainability, only 32 of studies of the 8542 works initially screened (less than 1%) have an integrated approach that considers all three aspects of sustainability, i.e., environmental, economic and social aspects. In most cases, the focus is on one of the three aspects. Additionally, 50% of the studies evaluated included another dimension, i.e., a cultural, institutional or technical dimension. These results support the idea that a multidimensional sustainability assessment is feasible and facilitates decision-making processes towards a sustainable energy future.}
}
@article{AUGSBURGER2009270,
title = {Methodologies to assess blood flow in cerebral aneurysms: Current state of research and perspectives},
journal = {Journal of Neuroradiology},
volume = {36},
number = {5},
pages = {270-277},
year = {2009},
issn = {0150-9861},
doi = {https://doi.org/10.1016/j.neurad.2009.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0150986109000479},
author = {L. Augsburger and P. Reymond and E. Fonck and Z. Kulcsar and M. Farhat and M. Ohta and N. Stergiopulos and D. {A. Rüfenacht}},
keywords = {Cerebral aneurysms, Blood flow assessment, Particle image velocimetry, Computational fluid dynamics},
abstract = {Summary
With intracranial aneurysms disease bringing a weakened arterial wall segment to initiate, grow and potentially rupture an aneurysm, current understanding of vessel wall biology perceives the disease to follow the path of a dynamic evolution and increasingly recognizes blood flow as being one of the main stakeholders driving the process. Although currently mostly morphological information is used to decide on whether or not to treat a yet unruptured aneurysm, among other factors, knowledge of blood flow parameters may provide an advanced understanding of the mechanisms leading to further aneurismal growth and potential rupture. Flow patterns, velocities, pressure and their derived quantifications, such as shear and vorticity, are today accessible by direct measurements or can be calculated through computation. This paper reviews and puts into perspective current experimental methodologies and numerical approaches available for such purposes. In our view, the combination of current medical imaging standards, numerical simulation methods and endovascular treatment methods allow for thinking that flow conditions govern more than any other factor fate and treatment in cerebral aneurysms. Approaching aneurysms from this perspective improves understanding, and while requiring a personalized aneurysm management by flow assessment and flow correction, if indicated.}
}
@article{BRIERLEY2021107870,
title = {The dark art of interpretation in geomorphology},
journal = {Geomorphology},
volume = {390},
pages = {107870},
year = {2021},
issn = {0169-555X},
doi = {https://doi.org/10.1016/j.geomorph.2021.107870},
url = {https://www.sciencedirect.com/science/article/pii/S0169555X21002786},
author = {Gary Brierley and Kirstie Fryirs and Helen Reid and Richard Williams},
keywords = {Landform, Landscape, Explanation, Prediction, Big Data, Fieldwork, Modelling},
abstract = {The process of interpretation, and the ways in which knowledge builds upon interpretations, has profound implications in scientific and managerial terms. Despite the significance of these issues, geomorphologists typically give scant regard to such deliberations. Geomorphology is not a linear, cause-and-effect science. Inherent complexities and uncertainties prompt perceptions of the process of interpretation in geomorphology as a frustrating form of witchcraft or wizardry — a dark art. Alternatively, acknowledging such challenges recognises the fun to be had in puzzle-solving encounters that apply abductive reasoning to make sense of physical landscapes, seeking to generate knowledge with a reliable evidence base. Carefully crafted approaches to interpretation relate generalised understandings derived from analysis of remotely sensed data with field observations/measurements and local knowledge to support appropriately contextualised place-based applications. In this paper we develop a cognitive approach (Describe-Explain-Predict) to interpret landscapes. Explanation builds upon meaningful description, thereby supporting reliable predictions, in a multiple lines of evidence approach. Interpretation transforms data into knowledge to provide evidence that supports a particular argument. Examples from fluvial geomorphology demonstrate the data-interpretation-knowledge sequence used to analyse river character, behaviour and evolution. Although Big Data and machine learning applications present enormous potential to transform geomorphology into a data-rich, increasingly predictive science, we outline inherent dangers in allowing prescriptive and synthetic tools to do the thinking, as interpreting local differences is an important element of geomorphic enquiry.}
}
@article{SUZEN2020726,
title = {Automatic short answer grading and feedback using text mining methods},
journal = {Procedia Computer Science},
volume = {169},
pages = {726-743},
year = {2020},
note = {Postproceedings of the 10th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2019 (Tenth Annual Meeting of the BICA Society), held August 15-19, 2019 in Seattle, Washington, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.171},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920302945},
author = {Neslihan Süzen and Alexander N. Gorban and Jeremy Levesley and Evgeny M. Mirkes},
keywords = {Natural Language Processing, Information Extraction, Automatic Grading, Machine Learning, Text Mining, Similarity, Clustering, k-means},
abstract = {Automatic grading is not a new approach but the need to adapt the latest technology to automatic grading has become very important. As the technology has rapidly became more powerful on scoring exams and essays, especially from the 1990s onwards, partially or wholly automated grading systems using computational methods have evolved and have become a major area of research. In particular, the demand of scoring of natural language responses has created a need for tools that can be applied to automatically grade these responses. In this paper, we focus on the concept of automatic grading of short answer questions such as are typical in the UK GCSE system, and providing useful feedback on their answers to students. We present experimental results on a dataset provided from the introductory computer science class in the University of North Texas. We first apply standard data mining techniques to the corpus of student answers for the purpose of measuring similarity between the student answers and the model answer. This is based on the number of common words. We then evaluate the relation between these similarities and marks awarded by scorers. We consider an approach that groups student answers into clusters. Each cluster would be awarded the same mark, and the same feedback given to each answer in a cluster. In this manner, we demonstrate that clusters indicate the groups of students who are awarded the same or the similar scores. Words in each cluster are compared to show that clusters are constructed based on how many and which words of the model answer have been used. The main novelty in this paper is that we design a model to predict marks based on the similarities between the student answers and the model answer. We argue that computational methods be used to enhance the reliability of human scoring, and not replace it. Humans are required to calibrate the system, and to deal with situations that are challenging. Computational methods can provide insight into which student answers will be found challenging and thus be a place human judgement is required.}
}
@article{FOUGERES2021100025,
title = {Fuzzy engineering design semantics elaboration and application},
journal = {Soft Computing Letters},
volume = {3},
pages = {100025},
year = {2021},
issn = {2666-2221},
doi = {https://doi.org/10.1016/j.socl.2021.100025},
url = {https://www.sciencedirect.com/science/article/pii/S2666222121000149},
author = {Alain-Jérôme Fougères and Egon Ostrosi},
keywords = {Fuzzy collaborative design, Fuzzy collaborative system, Natural language processing, Fuzzy requirement engineering, Fuzzy engineering design platform},
abstract = {Product design activities are predicated on fuzzy modelling, given that verbalising and interpreting engineering requirements are inherently fuzzy processes. The aim of this paper is to present a method for fuzzy intelligent requirement engineering from natural language to Computer-Aided Design (CAD) models. The field exploring the dynamics of computational processes from fuzzy linguistic modelling to fuzzy design modelling is complex and remains under-explored. No existing research has been identified which focuses specifically on fuzzy requirements engineering from natural language to CAD modelling. This paper seeks to address this by providing a design formalisation system based on five key principles. These principles are used to set out a computing procedure which follows a method broken up into six phases. The results of these six phases are fuzzy semantic graphs, which provide engineering requirements according to reliable design information. The approach is put into practice using the fuzzy agent-based tool developed by the authors, called F-EGEON (Fuzzy Engineering desiGn sEmantics elabOration and applicatioN). The proposed method is illustrated through an application from the automotive industry.}
}
@article{MILLER200021,
title = {Representational Tools and Conceptual Change: The Young Scientist's Tool Kit},
journal = {Journal of Applied Developmental Psychology},
volume = {21},
number = {1},
pages = {21-25},
year = {2000},
issn = {0193-3973},
doi = {https://doi.org/10.1016/S0193-3973(99)00047-7},
url = {https://www.sciencedirect.com/science/article/pii/S0193397399000477},
author = {Kevin F Miller},
abstract = {We interpret the world and its regularities through representations and procedures that are a complex mélange of formal experience, rules of thumb, and naive concepts that precede formal education. These representational tools give us the language in which we can think about science. Three propositions are argued: (a) that such tools are fundamental to scientific reasoning and science education; (b) that cognitive science has a great deal to say about how cognitive tools affect thinking and conceptual change, particularly how the representations intrinsic to ordinary language relate to the symbol systems of formal science and mathematics; and finally, (c) that cognitive science may play a role in developing representational tools that make scientific information more accessible.}
}
@article{BRAMSON2023105397,
title = {Emotion regulation from an action-control perspective},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {153},
pages = {105397},
year = {2023},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2023.105397},
url = {https://www.sciencedirect.com/science/article/pii/S0149763423003664},
author = {Bob Bramson and Ivan Toni and Karin Roelofs},
keywords = {Emotion control, Emotion regulation, Emotional-action selection, Forward modelling},
abstract = {Despite increasing interest in emotional processes in cognitive science, theories on emotion regulation have remained rather isolated, predominantly focused on cognitive regulation strategies such as reappraisal. However, recent neurocognitive evidence suggests that early emotion regulation may involve sensorimotor control in addition to other emotion-regulation processes. We propose an action-oriented view of emotion regulation, in which feedforward predictions develop from action-selection mechanisms. Those can account for acute emotional-action control as well as more abstract instances of emotion regulation such as cognitive reappraisal. We argue the latter occurs in absence of overt motor output, yet in the presence of full-blown autonomic, visceral, and subjective changes. This provides an integrated framework with testable neuro-computational predictions and concrete starting points for intervention to improve emotion control in affective disorders.}
}
@article{MORIN1992371,
title = {From the concept of system to the paradigm of complexity},
journal = {Journal of Social and Evolutionary Systems},
volume = {15},
number = {4},
pages = {371-385},
year = {1992},
issn = {1061-7361},
doi = {https://doi.org/10.1016/1061-7361(92)90024-8},
url = {https://www.sciencedirect.com/science/article/pii/1061736192900248},
author = {Edgar Morin},
abstract = {This paper is an overview of the author's ongoing reflections on the need for a new paradigm of complexity capable of informing all theories, whatever their field of application or the phenomena in question. Beginning with a critique of General System Theory and the principle of holism with which it is associated, the author suggests that contemporary advances in our knowledge of organization call for a radical reformation in our organization of knowledge. This reformation involves the mobilization of recursive thinking, which is to say a manner of thinking capable of establishing a dynamic and generative feedback loop between terms or concepts (such as whole and part, order and disorder, observer and observed, system and ecosystem, etc.) that remain both complementary and antagonistic. The paradigm of complexity thus stands as a bold challenge to the fragmentary and reductionistic spirit that continues to dominate the scientific enterprise.}
}
@article{CHEN2019381,
title = {How Big Data and High-performance Computing Drive Brain Science},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {17},
number = {4},
pages = {381-392},
year = {2019},
note = {Big Data in Brain Science},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2019.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S1672022919301561},
author = {Shanyu Chen and Zhipeng He and Xinyin Han and Xiaoyu He and Ruilin Li and Haidong Zhu and Dan Zhao and Chuangchuang Dai and Yu Zhang and Zhonghua Lu and Xuebin Chi and Beifang Niu},
keywords = {Brain science, Big data, High-performance computing, Brain connectomes, Deep learning},
abstract = {Brain science accelerates the study of intelligence and behavior, contributes fundamental insights into human cognition, and offers prospective treatments for brain disease. Faced with the challenges posed by imaging technologies and deep learning computational models, big data and high-performance computing (HPC) play essential roles in studying brain function, brain diseases, and large-scale brain models or connectomes. We review the driving forces behind big data and HPC methods applied to brain science, including deep learning, powerful data analysis capabilities, and computational performance solutions, each of which can be used to improve diagnostic accuracy and research output. This work reinforces predictions that big data and HPC will continue to improve brain science by making ultrahigh-performance analysis possible, by improving data standardization and sharing, and by providing new neuromorphic insights.}
}
@article{CUMMING2012923,
title = {Better compounds faster: the development and exploitation of a desktop predictive chemistry toolkit},
journal = {Drug Discovery Today},
volume = {17},
number = {17},
pages = {923-927},
year = {2012},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2012.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S1359644612000840},
author = {John G. Cumming and Jon Winter and Andrew Poirrette},
abstract = {Today's drug designer has access to vast quantities of data and an impressive array of sophisticated computational methods. At the same time, there is increasing pressure on the pharmaceutical industry to improve its productivity and reduce candidate drug attrition. We set out to develop a highly integrated suite of design and data analysis tools underpinned by the best predictive chemistry methods and models, with the aim of enabling multi-disciplinary compound design teams to make better informed design decisions. In this article we address the challenges of developing a powerful, flexible and user-friendly toolkit, and of maximising its exploitation by the design community. We describe the impact the toolkit has had on drug discovery projects and give our perspective on the future direction of this activity.}
}
@article{DECARVALHOBOTEGA2022109893,
title = {A data-driven Machine Learning approach to creativity and innovation techniques selection in solution development},
journal = {Knowledge-Based Systems},
volume = {257},
pages = {109893},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109893},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122009868},
author = {Luiz Fernando {de Carvalho Botega} and Jonny Carlos {da Silva}},
keywords = {Decision support system, Creativity, Artificial intelligence, Design},
abstract = {The creation and refinement of new ideas is a strategic competence for teams and organization to innovate and prosper. This paper addresses the challenge of finding adequate creativity and innovation techniques (CITs) for improving individual or team creativity through the use of Machine Learning (ML). The process of choosing which CIT to use is complex and demanding, especially when taking into consideration the existence of hundreds of techniques and the plurality of different design contexts. This empiric knowledge, usually retained in an expert’s repertoire, can be extracted and implemented in a computational system, making it more available and permanent. This research focused on developing a Decision Support System embedded in an online application with a two-stage ML inference process able to evaluate users’ design scenario through an online form, and infer the most appropriate CITs from the database that would fit their needs. This paper presents two iterative development cycles of the prototype, first focused on core knowledge acquisition, representation, ML implementation, and verification; while second focused on system expansion, addition of web interface, and initial validation. After essaying 12 algorithms, the two-stage model achieved uses a Gradient Boosted Regression Trees algorithm using user provided information about the context to infer the required CITs characteristics; followed by a Logistic Regression classification-ranking algorithm that uses outputs from first model to define which CITs to present to users. To the best of our efforts, no other system was found to use ML approaches to address the problem of CIT selection.}
}
@article{CHRISLEY2008119,
title = {Philosophical foundations of artificial consciousness},
journal = {Artificial Intelligence in Medicine},
volume = {44},
number = {2},
pages = {119-137},
year = {2008},
note = {Artificial Consciousness},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2008.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0933365708001000},
author = {Ron Chrisley},
keywords = {Artificial consciousness, Machine consciousness, Prosthetic artificial intelligence, Synthetic phenomenology, Interactive empiricism, Heterophenomenology},
abstract = {Summary
Objective
Consciousness is often thought to be that aspect of mind that is least amenable to being understood or replicated by artificial intelligence (AI). The first-personal, subjective, what-it-is-like-to-be-something nature of consciousness is thought to be untouchable by the computations, algorithms, processing and functions of AI method. Since AI is the most promising avenue toward artificial consciousness (AC), the conclusion many draw is that AC is even more doomed than AI supposedly is. The objective of this paper is to evaluate the soundness of this inference.
Methods
The results are achieved by means of conceptual analysis and argumentation.
Results and conclusions
It is shown that pessimism concerning the theoretical possibility of artificial consciousness is unfounded, based as it is on misunderstandings of AI, and a lack of awareness of the possible roles AI might play in accounting for or reproducing consciousness. This is done by making some foundational distinctions relevant to AC, and using them to show that some common reasons given for AC scepticism do not touch some of the (usually neglected) possibilities for AC, such as prosthetic, discriminative, practically necessary, and lagom (necessary-but-not-sufficient) AC. Along the way three strands of the author's work in AC – interactive empiricism, synthetic phenomenology, and ontologically conservative heterophenomenology – are used to illustrate and motivate the distinctions and the defences of AC they make possible.}
}
@article{GAL2014246,
title = {Novel approaches to address challenges in modelling aquatic ecosystems},
journal = {Environmental Modelling & Software},
volume = {61},
pages = {246-248},
year = {2014},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2014.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S1364815214002321},
author = {Gideon Gal and Matthew Hipsey and Karsten Rinke and Barbara Robson},
keywords = {Ecological modelling, Freshwater, Marine, Biogeochemistry, Lake, Estuary},
abstract = {Aquatic ecosystems are under increasing stress due to direct and indirect human activities. In response to this increased stress, aquatic ecosystems models are increasingly used to simulate water quality responses to changes. The increasing use of these models has not come without challenges. This thematic issue brings together examples of the latest thinking and novel approaches addressing key areas across a range of aquatic ecosystems, from lakes to rivers to marine waters. Topics include approaches applied to cover the full range of activities from methodological and technical developments of model-driven research of aquatic ecosystem functioning to model applications in lake management and decision-making. This thematic issue will provide additional momentum towards the ongoing development and improvement of aquatic models and their application.}
}
@article{LI20103574,
title = {The process model to aid innovation of products conceptual design},
journal = {Expert Systems with Applications},
volume = {37},
number = {5},
pages = {3574-3587},
year = {2010},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2009.10.034},
url = {https://www.sciencedirect.com/science/article/pii/S0957417409009002},
author = {Wenqiang Li and Yan Li and Jian Wang and Xiaoying Liu},
keywords = {Conceptual design, Innovative strategies, Process mapping, Extension reasoning, Mathematical model},
abstract = {Currently, designers often pay little attention to integrated innovation during the design process of products. In addition, the product assistance design systems mainly focus on the detailed design phrase and the construction function of mathematics models are often been neglected. In order to solve these problems, this paper proposes a conceptual design process model to aid multi-stage innovation of product design based on the integration of the essential rules of the Axiomatic Design (AD) model, Function–Behaviour–Structure (FBS) model, and the guideline of functional creative thinking logics. By utilising the function tree and functional structure tree as the mediums to express the design information and by applying the conflict solving strategies of Extensic theory, the conceptual design process is defined as an integrated system with five stages and four mappings. The integrated logical processes of this model are described with mathematical language. Thus, the whole transformation from design experiences to design principles and to mathematical model finally to aided design system is realized perfectly in the proposed process model. The meaningful exploration on the nature and practical processes of product conceptual design is carried out in this research.}
}