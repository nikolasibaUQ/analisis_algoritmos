@incollection{WANG2022238,
title = {1.10 - CyberGIS and Geospatial Data Science for Advancing Geomorphology},
editor = {John (Jack) F. Shroder},
booktitle = {Treatise on Geomorphology (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {238-259},
year = {2022},
isbn = {978-0-12-818235-2},
doi = {https://doi.org/10.1016/B978-0-12-818234-5.00122-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012818234500122X},
author = {Shaowen Wang and Michael P. Bishop and Zhe Zhang and Brennan W. Young and Zewei Xu},
keywords = {Artificial intelligence, CyberGIS, Deep learning, Geomorphology, Geospatial data science, Land cover science, LiDAR, Uncertainty},
abstract = {Theoretical and practical issues in geomorphology have not been adequately addressed due to a lack of formalization and digital representation of spatial and temporal concepts, given the limitations associated with modern-day geographic information systems (GIS). Rapid advancements in geospatial technologies have resulted in new sensors and large volumes of geospatial data that have yet to be fully exploited given a variety of computational issues. Computational limitations involving storage, preprocessing, analysis, and modeling pose significant problems for Earth scientists. Consequently, advanced cyberinfrastructure is required to address geospatial data-science issues involving communication, representation, computation, information production, decision-making, and geovisualization. We identify and discuss important aspects of exploiting advances in cyberinfrastructure that involve computational scalability, artificial intelligence, and uncertainty characterization and analysis for addressing issues in the Earth sciences. Such developments can be termed cyber geographic information science and systems (cyberGIS). We discuss this important topic by addressing the significant overlap of concepts in GIS and geomorphology that can be formalized, digitally represented, implemented, and evaluated with cyberGIS. We then introduce the fundamentals of cyberinfrastructure and cyberGIS, including a discussion of the utilization of artificial intelligence and deep learning. We finally provide one case study demonstrating operational cyberGIS capabilities.}
}
@article{DAS2022104116,
title = {Role of non-motorized transportation and buses in meeting climate targets of urban regions},
journal = {Sustainable Cities and Society},
volume = {86},
pages = {104116},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2022.104116},
url = {https://www.sciencedirect.com/science/article/pii/S2210670722004292},
author = {Deepjyoti Das and Pradip P. Kalbar and Nagendra R. Velaga},
keywords = {Decarbonization, Life cycle thinking, Avoided trip and material, Carbon budget, Climate change, Sustainable transportation},
abstract = {Studies examining the potential of low-carbon modes of passenger transportation for achieving climate goals are limited. The study is one of the first to assess the potential of non-motorized transportation (NMT) and buses to meet regional climate targets representing 2 °C, 1.5 °C, and Intended Nationally Determined Contributions from 2018 to 2050. Also, the approach towards quantifying contribution from avoided trips and materials in holistically understanding the potential of NMT and buses is novel. Data from the transportation model of Mumbai Metropolitan Region's Comprehensive Mobility Plan is used to assess multiple scenarios of upgrading NMT and bus infrastructure to reduce cumulative carbon dioxide emissions (CCE) from passenger transportation. The assessment is based on three push levels, i.e., conservative, moderate, and aggressive. Results show that upgrading bus infrastructure contributes higher to reducing CCE than NMT. As NMT also contributes significantly to decreasing CCE, it is recommended that bus and NMT development should be integrated. However, their combined contribution will not meet the climate targets. Since avoided materials contribute considerably more than avoided trips, high emission materials such as aluminum used in light-weighting should be questioned. The results provide policy guidance to authorities in prioritizing buses and NMT infrastructure development during city planning.}
}
@incollection{HUNG2017227,
title = {Chapter 12 - Rationality and Escherichia Coli},
editor = {T.-W. Hung and T.J. Lane},
booktitle = {Rationality},
publisher = {Academic Press},
address = {San Diego},
pages = {227-240},
year = {2017},
isbn = {978-0-12-804600-5},
doi = {https://doi.org/10.1016/B978-0-12-804600-5.00012-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012804600500012X},
author = {T.-W. Hung},
keywords = {practical rationality, procedural rationality, , human uniqueness, nonhuman rationality},
abstract = {If rationality is the defining characteristic of the human species, as Aristotle asserts, why is this trait rarely noticed in Eastern traditions? How should we interpret the reasoning ability in problem solving that is increasingly reported in other animals? In this chapter, I focus on descriptive-practical-procedural rationality (one’s action is described as rational if it is determined by internal processes that conform to logical or Bayesian rules). I argue that this rationality can be found in all organisms with adaptive capacity, including unicellular bacteria. To this end, I first review three seemingly true claims and explain why they lead to an inconsistency: (1) Escherichia coli are computational systems in a nontrivial sense, (2) E. coli are not creatures that can be rational or irrational, and (3) rationality is a matter of computational facts in that nontrivial sense, and organisms of the same computation are the type of creatures that can be rational or irrational. I then suggest rejecting claim (2) by examining recent microbiological data on E. coli, explaining the extent to which they satisfy this type of rationality. I also discuss some objections to and implications of this view. Instead of concluding that humans and bacteria both evolve with rationality at the same level, I argue that organisms’ rationality capacities comes in degree.}
}
@incollection{SIEGEL20223,
title = {Chapter 1 - Introduction: Defining the Role of Statistics in Business},
editor = {Andrew F. Siegel and Michael R. Wagner},
booktitle = {Practical Business Statistics (Eighth Edition)},
publisher = {Academic Press},
edition = {Eighth Edition},
pages = {3-18},
year = {2022},
isbn = {978-0-12-820025-4},
doi = {https://doi.org/10.1016/B978-0-12-820025-4.00001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128200254000014},
author = {Andrew F. Siegel and Michael R. Wagner},
abstract = {We begin this chapter with an overview of the competitive advantage provided by a knowledge of statistical methods, followed by some basic facts about statistics and probability and their role in business. Statistical activities can be grouped into five main activities (designing, exploring, modeling, estimating, and hypothesis testing), and one way to clarify statistical thinking is to be able to match the business task at hand with the correct collection of statistical methods. This chapter sets the stage for the rest of the book, which follows up with many important detailed procedures for accomplishing business goals that involve these activities. Next follows an overview of data mining of Big Data (which involves these main activities) and its importance in business. Then we distinguish the field of probability (where, based on assumptions, we reach conclusions about what is likely to happen—a useful exercise in business where nobody knows for sure what will happen) from the field of statistics (where we know from the data what happened, from which we infer conclusions about the system that produced these data) while recognizing that probability and statistics will work well together in future chapters. The chapter concludes with some words of advice on how to integrate statistical thinking with other business viewpoints and activities.}
}
@article{JI2024109648,
title = {Research on 3D printed titanium alloy scaffold structure induced osteogenesis: Mechanics and in vitro testing},
journal = {Materials Today Communications},
volume = {40},
pages = {109648},
year = {2024},
issn = {2352-4928},
doi = {https://doi.org/10.1016/j.mtcomm.2024.109648},
url = {https://www.sciencedirect.com/science/article/pii/S2352492824016295},
author = {Yuchen Ji and Huiming Zhang and Zhixiu Jiang and Danyu Liu and Yuhao Yang and Chenxu Guan and Yucheng Su and Xinyu Wang and Feng Duan},
keywords = {Selective laser melting, Structural titanium alloy scaffold, Finite element analysis, Mechanical experiment, Osteogenesis},
abstract = {Several methods exist for repairing mandibular segmental bone defects, typically employing the implant method to accomplish the repair. Following a comparative analysis, the Ti6Al4V structural scaffold implant was chosen for bone defect repair. Triply periodic minimal surface (TPMS) is characterized by a high surface area to volume ratio, an average curvature of zero, and other notable advantages, providing a new line of thinking for bone tissue scaffolds. In this work, the in vitro osteogenesis of a cell unit measuring 4 mm was investigated. First, the finite element analysis (FEA) method and the mechanical experiment method were employed to screen the elasticity modulus of the cancellous bone of the mandible. Subsequently, the selective laser melting (SLM) technique was adopted to prepare three different structures precisely - Gyroid, octahedron, and cube - each with wall thicknesses of 0.3 mm, 0.4 mm, and 0.5 mm. In the in vitro osteogenic experiments, it was observed through confocal laser scanning microscopy (CLSM) that each scaffold displayed favorable cell spreading at 1 day and 3 days. Moreover, osteoblast cell adhesion and proliferation assays revealed improved cell adhesion and proliferation with prolonged co-cultivation time, signifying the excellent biocompatibility of the structural titanium alloy scaffolds. Furthermore, findings from cell differentiation and bioactivity assays indicated that the Gyroid structure exhibited superior osteogenesis compared to the cube and octahedron structures. However, no statistically significant difference was noted between varying wall thicknesses within the same structure.}
}
@article{ZHANG2022786,
title = {Research on Graph Neural Network in Stock Market},
journal = {Procedia Computer Science},
volume = {214},
pages = {786-792},
year = {2022},
note = {9th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.242},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922019524},
author = {Wenjun Zhang and Zhensong Chen and Jianyu Miao and Xueyong Liu},
keywords = {Graph neural networks, stocks, forecasts},
abstract = {The stock market is a very important part of the financial field, and the prediction of the stock market has a great relationship with the returns and risk safety of the entire financial field. With the continuous mature application of machine learning and deep learning in other fields, such as image processing and text analysis, people begin to focus on the use of different models so as to predict stock volatility. However, in view of the unique multi-source and heterogeneous characteristics of stock information, the artificial neural network relying on deep learning cannot make a good prediction on it. At this time, the graph neural network that can well analyze the graph structure data is gradually favored by scholars at home and abroad, and the research thinking is also expanding. This dissertation examines the purpose of deeply analyzing the methods of different graph neural network models on stock prediction through an inductive study of amount of relevant literature. In this paper, we not only classify the literature by various graph neural network models, but also describe objectively the models and ideas presented in each paper. By referring to literature, this paper summarizes the previous research results, analyzes the applicability and results of different methods, and lays a foundation for better stock prediction in the future.}
}
@article{YANG2024102469,
title = {Prosumer data center system construction and synergistic optimization of computing power, electricity and heat from a global perspective},
journal = {Thermal Science and Engineering Progress},
volume = {49},
pages = {102469},
year = {2024},
issn = {2451-9049},
doi = {https://doi.org/10.1016/j.tsep.2024.102469},
url = {https://www.sciencedirect.com/science/article/pii/S2451904924000878},
author = {Dongfang Yang and Xiaoyuan Wang and Rendong Shen and Yang Li and Lei Gu and Ruifan Zheng and Jun Zhao},
keywords = {Data center, Prosumer, Global optimization, Waste heat, Computing power},
abstract = {In the context of achieving carbon neutrality, the imperative to save energy and reduce emissions in data centers (DCs) has become paramount. Current research predominantly concentrates on internal systems of DCs, lacking the perspective of treating DCs as prosumers and subsequent global optimization, resulting in limited results. In this paper, a novel prosumer DC integrated energy system is constructed and a globally coordinated optimization strategy for the synergy of computing power, electricity, and heat is proposed. This is achieved through meticulous adjustments to the battery charge–discharge processes on the supply side, computational workloads within the DC's internal systems, and the heating temperature for waste heat utilization on the load side. The optimization objectives are centered around minimizing the renewable energy waste and operation cost. Compared to the non-optimized system, the optimized system exhibits reductions of 11.39 % in renewable energy waste, 6.96 % in operation cost, 8.89 % in grid electricity consumption, and 4.18 % in total electricity consumption. Furthermore, the strategy effectively reduces renewable energy waste and operation cost at different occupancy rates by 8.02 %–12.21 % and 6.61 %–10.44 %, respectively. Under varying battery capacities, the system demonstrates reductions in renewable energy waste and operation cost by 9.42 %–26.57 % and 6.89 %–10.70 %, confirming the effectiveness of the proposed strategy across different occupancy rates and battery capacities. The research findings further highlight the potential of globally coordinated optimization in the synergy of computing power, electricity, and heat, providing valuable insights for the sustainable development of DCs.}
}
@article{LI2022101701,
title = {A framework and method for Human-Robot cooperative safe control based on digital twin},
journal = {Advanced Engineering Informatics},
volume = {53},
pages = {101701},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101701},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622001604},
author = {Hao Li and Wenfeng Ma and Haoqi Wang and Gen Liu and Xiaoyu Wen and Yuyan Zhang and Miying Yang and Guofu Luo and Guizhong Xie and Chunya Sun},
keywords = {Human-robot collaboration, Digital twin, Safety control, Machine vision, Convolutional neural network},
abstract = {Human-robot collaboration (HRC) combines the robot’s mechanical properties and predictability with human experience, logical thinking, and strain capabilities to alleviate production efficiency. However, ensuring the safety of the HRC process in-real time has become an urgent issue. Digital twin extends functions of virtual models in the design phase of the physical counterpart in the production phase through virtual-real interactive feedback, data fusion analysis, advanced computational features, etc. This paper proposes an HRC safety control framework and corresponding method based on the digital twin. In the design phase, virtual simulation and virtual reality technology are integrated to construct virtual twins of various HRC scenarios for testing and analyzing potential safety hazards. In the production phase, the safety distance between humans and robots of the HRC scene is monitored and calculated by an iterative algorithm according to machine vision and a convolutional neural network. Finally, the virtual twin is driven based on real-scene data, real-time online visual monitoring, and optimization of the HRC’s overall process. A case study using ABB-IRB1600 is presented to verify the feasibility of the proposed approach.}
}
@article{SU2024121016,
title = {Detecting anomalies with granular-ball fuzzy rough sets},
journal = {Information Sciences},
volume = {678},
pages = {121016},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121016},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524009307},
author = {Xinyu Su and Zhong Yuan and Baiyang Chen and Dezhong Peng and Hongmei Chen and Yingke Chen},
keywords = {Granular computing, Fuzzy rough sets, Granular-ball, Anomaly detection, Outlier detection},
abstract = {Most of the existing anomaly detection methods are based on a single and fine granularity input pattern, which is susceptible to noisy data and inefficient for detecting anomalies. Granular-ball computing, as a novel multi-granularity representation and computation method, can effectively compensate for these shortcomings. We utilize the fuzzy rough sets to mine the potential uncertainty information in the data efficiently. The combination of granular-ball computing and fuzzy rough sets takes into account the benefits of both methods, providing great application and research value. However, this novel combination still needs to be explored, especially for unsupervised anomaly detection. In this study, we first propose the granular-ball fuzzy rough set model, and the relevant definitions in the model are given. Subsequently, we pioneeringly present an unsupervised anomaly detection method based on granular-ball fuzzy rough sets called granular-ball fuzzy rough sets-based anomaly detection (GBFRD). Our method introduces the granular-ball fuzzy rough granules-based outlier factor to characterize the outlier degree of an object effectively. The experimental results demonstrate that GBFRD exhibits superior performance compared to the state-of-the-art methods. The code is publicly available at https://github.com/Mxeron/GBFRD.}
}
@article{MONTEIRO2023100076,
title = {Environmental assessment in concrete pole industries},
journal = {CEMENT},
volume = {13},
pages = {100076},
year = {2023},
issn = {2666-5492},
doi = {https://doi.org/10.1016/j.cement.2023.100076},
url = {https://www.sciencedirect.com/science/article/pii/S2666549223000221},
author = {Nathalie Barbosa Reis Monteiro and José Machado {Moita Neto} and Elaine Aparecida {da Silva}},
keywords = {Concrete poles, Life cycle, Environmental impact},
abstract = {Purpose
Companies that manufacture poles generate several negative environmental impacts, whose extent needs to be assessed to find ways to mitigate them.
Methods
In this research, Life Cycle Assessment (LCA) was used as a methodology to measure the potential environmental impacts throughout the poles' life cycle. Primary data (amount of cement, gravel, sand, steel rebars, energy, water) were collected from industries located in Teresina, Piauí, Brazil, and information from the Ecoinvent 3.7.1 database (transport, solid waste, liquid effluents, particulate matter) was used.
Results and discussion
The literature addresses pole production from a different perspective, making this study relevant to disseminate the life cycle thinking in concrete pole production. However, the literature points to a correlation trend for ecotoxicity and human toxicity indicators, as well as the results found in this research. Waste disposal stands out as an important source of impact for these industries, confirming the necessity of efficient management of these materials at the end of their lifespan and during the production process. The scenario analysis showed that is possible to reduce the potential impacts of these industries.
Conclusion
The reuse of waste within the industry itself is feasible (using a shredder for this purpose) and can contribute to decreasing the extraction of natural deposits in various production processes related to the poles' life cycle and reducing their accumulation in the environment. The use of inputs from closer suppliers is a strategy that contributes to mitigating the potential impact of gaseous emissions, reducing the impact that generates global warming and climate change. In addition, other papers show viable alternatives in different scenarios, based on complex laboratory studies. Nevertheless, his approach shows how impacts can be mitigated with the adoption of simple actions such as the reuse of effluents and residues from these industries. It is possible to redefine the production process through a scenario close to the ideal, bringing environmental sustainability to the sector.}
}
@incollection{CANCES20033,
title = {Computational quantum chemistry: A primer},
series = {Handbook of Numerical Analysis},
publisher = {Elsevier},
volume = {10},
pages = {3-270},
year = {2003},
booktitle = {Special Volume, Computational Chemistry},
issn = {1570-8659},
doi = {https://doi.org/10.1016/S1570-8659(03)10003-8},
url = {https://www.sciencedirect.com/science/article/pii/S1570865903100038},
author = {Eric Cancès and Mireille Defranceschi and Werner Kutzelnigg and Claude {Le Bris} and Yvon Maday},
abstract = {Publisher Summary
This chapter discusses basic modeling. The chapter illustrates that quantum chemistry aims at understanding the properties of matter through the modeling of its behavior at a subatomic scale, where matter is described as an assembly of nuclei and electrons. At this scale, the equation that rules the interactions between these constitutive elements is the Schrödinger equation. It can be considered as a universal model for at least three reasons. First, it contains all the physical information of the system under consideration so that any of the properties of this system can be deduced in theory from the Schrödinger equation associated to it. Second, the Schrödinger equation does not involve any empirical parameter, except some fundamental constants of Physics; it can thus be written for any kind of molecular system provided its chemical composition, in terms of natures of nuclei and number of electrons, is known. Third, this model enjoys remarkable predictive capabilities, as confirmed by comparisons with a large amount of experimental data of various types.}
}
@article{BOWMAN2023107339,
title = {Desperately searching for something},
journal = {Communications in Nonlinear Science and Numerical Simulation},
volume = {125},
pages = {107339},
year = {2023},
issn = {1007-5704},
doi = {https://doi.org/10.1016/j.cnsns.2023.107339},
url = {https://www.sciencedirect.com/science/article/pii/S1007570423002575},
author = {Clive E. Bowman and Peter Grindrod},
abstract = {There is a growing interest in novelty search : that is, in sampling a parameter space to search for radical or unexpected behaviour(s), occurring as a consequence of parameter choice, being input to some downstream complex system, process, or service that will not yield to analysis, without imposing any specific pre-ordained objective function, or fitness function to be optimised. We mean “parameter” in the widest sense, including system learnables, non-autonomous forcing, sequencing and all inputs. Depending upon the nature of the underlying parameter space of interest one may adopt a rather wide range of search algorithms. We do consider that this search activity has meta-objectives, though: one is of achieving diversity (efficiently reaching out across the space in some way); and one is of achieving some minimum density (not leaving out large unexplored holes). These are in tension. In general, the computational costs of both of these qualities become restrictive as the dimension of the parameter spaces increase; and consequently their balance is harder to maintain. We may also wish for a substantial random element of search to provide some luck in discovery and to avoid any naive preset sampling patterns. We consider archive-based methods within a range of spaces: finite discrete spaces, where the problem is straightforward (provided we are patient with the random element); Euclidean spaces, of increasing dimension, that become very lonely places; and infinite dimensional spaces. Our aim is to discuss a raft of distinctive search concepts, that respond to identified challenges, and rely on a rather diverse range of mathematical ideas. This arms practitioners with a range of highly practical methods. However applications requiring novelty search arise, one should avoid rushing to code-up a standard evolving search algorithm and instead give some thought to the nature and requirements of the search: there is a range of effective options available. We give some considered advice.}
}
@article{EPIOTIS1989213,
title = {Lewis formulae for metallic systems: the Li tetramer paradigm},
journal = {Journal of Molecular Structure: THEOCHEM},
volume = {201},
pages = {213-238},
year = {1989},
issn = {0166-1280},
doi = {https://doi.org/10.1016/0166-1280(89)87077-0},
url = {https://www.sciencedirect.com/science/article/pii/0166128089870770},
author = {N.D. Epiotis},
abstract = {In previous works, we argued that metal atoms bind through a mechanism in which overlap is assisted by some other overlap-independent mode like dispersion or induction. The result is the formation of gas pairs (interstitial electron pairs). Unlike overlap, these mechanisms of bonding can be properly reproduced only at the MCSCF level. To draw the chemist away from one-electron thinking, we propose specific Lewis formulae for small metal clusters and we show how these change as we shift from a lower to a higher level of theory so as to project the key point. A qualitative (let alone quantitative) understanding of metallic bonding can only be achieved from examination of properly correlated wavefunctions. From the practical standpoint, we show how usage of these Lewis formulae can inspire analogies for explaining computational and experimental results.}
}
@article{HASSABIS2007299,
title = {Deconstructing episodic memory with construction},
journal = {Trends in Cognitive Sciences},
volume = {11},
number = {7},
pages = {299-306},
year = {2007},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2007.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S1364661307001258},
author = {Demis Hassabis and Eleanor A. Maguire},
abstract = {It has recently been observed that the brain network supporting recall of episodic memories shares much in common with other cognitive functions such as episodic future thinking, navigation and theory of mind. It has been speculated that ‘self-projection’ is the key common process. However, in this Opinion article, we note that other functions (e.g. imagining fictitious experiences) not explicitly connected to either the self or a subjective sense of time, activate a similar brain network. Hence, we argue that the process of ‘scene construction’ is better able to account for the commonalities in the brain areas engaged by an extended range of disparate functions. In light of this, we re-evaluate our understanding of episodic memory, the processes underpinning it and other related cognitive functions.}
}
@article{WELLMAN1991205,
title = {The ecology of computation: B.A. Huberman, ed.},
journal = {Artificial Intelligence},
volume = {52},
number = {2},
pages = {205-218},
year = {1991},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(91)90044-K},
url = {https://www.sciencedirect.com/science/article/pii/000437029190044K},
author = {Michael P. Wellman}
}
@article{ULRICH1988309,
title = {Computation and conceptual design},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {4},
number = {3},
pages = {309-315},
year = {1988},
note = {Special Issue Manufacturing Systems and Technology of the Future},
issn = {0736-5845},
doi = {https://doi.org/10.1016/0736-5845(88)90002-6},
url = {https://www.sciencedirect.com/science/article/pii/0736584588900026},
author = {Karl Ulrich and Warren Seering},
abstract = {Design is the transformation between a functional and a structural description of a device. Conceptual design is the initial stage of this transformation. We hypothesize that most new design are derived from knowledge of existing designs. We identify a special case of this process and call it novel combination. By describing an implemented program which designs novel mechanical fasteners, we explain how knowledge of existing devices can be represented and used. We highlight the issues arising from this implementation and propose four areas of future research. This work is important for establishing a fundamental understanding of conceptual design, leading to enhanced design teaching and better design tools.}
}
@article{GERSHMAN2020104394,
title = {Origin of perseveration in the trade-off between reward and complexity},
journal = {Cognition},
volume = {204},
pages = {104394},
year = {2020},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2020.104394},
url = {https://www.sciencedirect.com/science/article/pii/S0010027720302134},
author = {Samuel J. Gershman},
keywords = {Decision making, Information theory, Reinforcement learning},
abstract = {When humans and other animals make repeated choices, they tend to repeat previously chosen actions independently of their reward history. This paper locates the origin of perseveration in a trade-off between two computational goals: maximizing rewards and minimizing the complexity of the action policy. We develop an information-theoretic formalization of policy complexity and show how optimizing the trade-off leads to perseveration. Analysis of two data sets reveals that people attain close to optimal trade-offs. Parameter estimation and model comparison supports the claim that perseveration quantitatively agrees with the theoretically predicted functional form (a softmax function with a frequency-dependent action bias).}
}
@article{199064,
title = {Natural languages: Berwick, R ‘Natural language computational complexity and generative capacity’ Comput. Artif. Intell. Vol 8 No 5 (1989) pp 423–441},
journal = {Knowledge-Based Systems},
volume = {3},
number = {1},
pages = {64},
year = {1990},
issn = {0950-7051},
doi = {https://doi.org/10.1016/0950-7051(90)90091-U},
url = {https://www.sciencedirect.com/science/article/pii/095070519090091U}
}
@incollection{QIN2025775,
title = {Representation of others' beliefs},
editor = {Jordan Henry Grafman},
booktitle = {Encyclopedia of the Human Brain (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {775-792},
year = {2025},
isbn = {978-0-12-820481-8},
doi = {https://doi.org/10.1016/B978-0-12-820480-1.00159-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204801001595},
author = {Jingmin Qin and Haiyan Wu},
keywords = {Theory of mind, Mentalizing, Perspective-taking, Metacognition, Social inference, Social cognition, Developmental changes, Neural mechanisms, Psychopathology, Computational models},
abstract = {This article delves into the diverse aspects in which individuals make inferences about the beliefs and values held by others. By reviewing the psychological factors underlying representing the beliefs of others, as well as the individual differences for both the individuals being represented and those undertaking the representation, this article sheds light on the intricate nature of representation of other's belief. Furthermore, it discusses the ways and considerations involved in updating these beliefs (such as observation, active interaction, and Bayesian inference) and offers suggestions for future research.}
}
@article{COIERA2022100860,
title = {Evidence synthesis, digital scribes, and translational challenges for artificial intelligence in healthcare},
journal = {Cell Reports Medicine},
volume = {3},
number = {12},
pages = {100860},
year = {2022},
issn = {2666-3791},
doi = {https://doi.org/10.1016/j.xcrm.2022.100860},
url = {https://www.sciencedirect.com/science/article/pii/S2666379122004244},
author = {Enrico Coiera and Sidong Liu},
keywords = {evidence-based medicine, evidence synthesis, patient safety, research replication, machine learning, algorithmic transportability, deep learning, clinical trial registries},
abstract = {Summary
Healthcare has well-known challenges with safety, quality, and effectiveness, and many see artificial intelligence (AI) as essential to any solution. Emerging applications include the automated synthesis of best-practice research evidence including systematic reviews, which would ultimately see all clinical trial data published in a computational form for immediate synthesis. Digital scribes embed themselves in the process of care to detect, record, and summarize events and conversations for the electronic record. However, three persistent translational challenges must be addressed before AI is widely deployed. First, little effort is spent replicating AI trials, exposing patients to risks of methodological error and biases. Next, there is little reporting of patient harms from trials. Finally, AI built using machine learning may perform less effectively in different clinical settings.}
}
@incollection{CALVERT201369,
title = {Chapter 3 - Social Dimensions of Microbial Synthetic Biology},
editor = {Colin Harwood and Anil Wipat},
series = {Methods in Microbiology},
publisher = {Academic Press},
volume = {40},
pages = {69-86},
year = {2013},
booktitle = {Microbial Synthetic Biology},
issn = {0580-9517},
doi = {https://doi.org/10.1016/B978-0-12-417029-2.00003-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780124170292000030},
author = {Jane Calvert and Emma Frow},
keywords = {Anticipation, Governance, Public engagement, Public good, Responsible innovation, Social dimensions, Science and technology studies, Synthetic biology},
abstract = {In this chapter, we outline a number of foundational ideas that underpin our approach to the study of the social, ethical, legal and philosophical dimensions of synthetic biology. We describe these through a series of important shifts that have taken place over the past few decades of social science research. We suggest a move away from discussions centred around ethical ‘implications’, speculations about the future and concerns about risk, regulation and public acceptance, towards a conversation that talks in terms of social ‘dimensions’, anticipating the future, managing uncertainty, tools of governance and research for the public good. We argue that these seemingly subtle changes in vocabulary open up a new and productive space for thinking about the social dimensions of synthetic biology.}
}
@article{LAL2023100791,
title = {IOT-based cyber security identification model through machine learning technique},
journal = {Measurement: Sensors},
volume = {27},
pages = {100791},
year = {2023},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2023.100791},
url = {https://www.sciencedirect.com/science/article/pii/S2665917423001277},
author = {Bechoo Lal and S. Ravichandran and R. Kavin and N. {Anil Kumar} and Dibyahash Bordoloi and R. {Ganesh Kumar}},
keywords = {Cyber security, Machine learning algorithms, Security, Repositories, Meta-classifier methods, Internet of things},
abstract = {Manual vulnerability evaluation tools produce erroneous data and lead to difficult analytical thinking. Such security concerns are exacerbated by the variety, imperfection, and redundancies of modern security repositories. These problems were common traits of producers and public vulnerability disclosures, which make it more difficult to identify security flaws through direct analysis through the Internet of Things (IoT). Recent breakthroughs in Machine Learning (ML) methods promise new solutions to each of these infamous diversification and asymmetric information problems throughout the constantly increasing vulnerability reporting databases. Due to their varied methodologies, those procedures themselves display varying levels of performance. The authors provide a method for cognitive cybersecurity that enhances human cognitive capacity in two ways. To create trustworthy data sets, initially reconcile competing vulnerability reports and then pre-process advanced embedded indicators. This proposed methodology's full potential has yet to be fulfilled, both in terms of its execution and its significance for security evaluation in application software. The study shows that the recommended mental security methodology works better when addressing the above inadequacies and the constraints of variation among cybersecurity alert mechanisms. Intriguing trade-offs are presented by the experimental analysis of our program, in particular the ensemble method that detects tendencies of computational security defects on data sources.}
}
@article{MURTAGH201637,
title = {Direct Reading Algorithm for Hierarchical Clustering},
journal = {Electronic Notes in Discrete Mathematics},
volume = {56},
pages = {37-42},
year = {2016},
note = {TCDM 2016 - 1st IMA Conference on Theoretical and Computational Discrete Mathematics, University of Derby},
issn = {1571-0653},
doi = {https://doi.org/10.1016/j.endm.2016.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S157106531630213X},
author = {Fionn Murtagh and Pedro Contreras},
keywords = {Analytics, hierarchical clustering, ultrametric topology, p-adic and m-adic number representation, linear time computational complexity},
abstract = {Reading the clusters from a data set such that the overall computational complexity is linear in both data dimensionality and in the number of data elements has been carried out through filtering the data in wavelet transform space. This objective is also carried out after an initial transforming of the data to a canonical order. Including high dimensional, high cardinality data, such a canonical order is provided by row and column permutations of the data matrix. In our recent work, we induce a hierarchical clustering from seriation through unidimensional representation of our observations. This linear time hierarchical classification is directly derived from the use of the Baire metric, which is simultaneously an ultrametric. In our previous work, the linear time construction of a hierarchical clustering is studied from the following viewpoint: representing the hierarchy initially in an m-adic, m = 10, tree representation, followed by decreasing m to smaller valued representations that include p-adic representations, where p is prime and m is a non-prime positive integer. This has the advantage of facilitating a more direct visualization and hence interpretation of the hierarchy. In this work we present further case studies and examples of how this approach is very advantageous for such an ultrametric topological data mapping.}
}
@article{ROBSON2022101018,
title = {Searching for the principles of a less artificial A.I.},
journal = {Informatics in Medicine Unlocked},
volume = {32},
pages = {101018},
year = {2022},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2022.101018},
url = {https://www.sciencedirect.com/science/article/pii/S2352914822001617},
author = {B. Robson and G. Ochoa-Vargas},
keywords = {AI, Algorithms, X factor, Emergent properties, Consciousness, Quantum effects},
abstract = {What would it take to build a computer physician that can take its place amongst human peers? Currently, Neural Nets, especially as so-called “Deep Learning” nets, dominate what is popularly called “Artificial Intelligence”, but to many critics they seem to be little more than powerful data-analytic tools inspired by some of the more basic functions and regions of the human brain such as those involved in early processes in biological vision, classification, and categorization. The deeper nature of human intelligence as the term is normally meant, including relating to consciousness, has been the domain of philosophers, psychologists, and some neuroscientists. Now, attention is turning to neuronal mechanisms in humans and simpler organisms as a basis of a truer AI with far greater potential. Arguably, the approach required should be rooted in information theory and algorithmic science. But as discussed in this paper, caution is required: “just any old information” might not do. The information might need to be of a particular dynamical and actioning nature, and that might significantly impact the kind of computation and computer hardware required. Overall, however, the authors do not favor emergent properties such as those based on complexity and quantum effects. Despite the possible difficulties, such studies could, in return, have substantial benefits for biology and medicine beyond the computational tools that they produce to serve those disciplines.}
}
@incollection{KIM2009332,
title = {Spatial Data Mining, Geovisualization},
editor = {Rob Kitchin and Nigel Thrift},
booktitle = {International Encyclopedia of Human Geography},
publisher = {Elsevier},
address = {Oxford},
pages = {332-336},
year = {2009},
isbn = {978-0-08-044910-4},
doi = {https://doi.org/10.1016/B978-008044910-4.00526-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780080449104005265},
author = {C. Kim},
keywords = {Exploratory spatial data analysis, Geovisualization, Knowledge discovery, Spatial autocorrelation, Spatial data mining, Spatial outliers, Spatial uncertainty, Visual data mining},
abstract = {Geovisualization in spatial data mining is one of the main methods that has recently been the subject of knowledge discovery research in geographic information science. Geovisualization is often referred to as knowledge discovery in that it produces previously unseen patterns from a larger set of data. Due to the increase in geospatial data, any techniques that can shift through large sets of data quickly and efficiently are in high demand. Geovisualization uses visual representations to facilitate thinking, understanding, and knowledge construction about human and physic environments, at geographic scales of measurement. It augments human visual ability in perceiving high complex structures, and detecting, exploring, and exploiting significant patterns. It integrates scientific visualization with traditional cartography, and can be utilized at data pre-processing, spatial data mining, and knowledge construction. The main purpose of geovisualization, however, is on insight rather than maps. Research needs in geovisualization are extensive as follows: geovisulation in spatiotemporal databases, the automated discovery of spatial knowledge, geovisualization in remote-sensing data and spatial object-oriented databases, effective geovisualizations of spatial relationships, and efficient geocomputation.}
}
@article{RADTKE2022102355,
title = {Smart energy systems beyond the age of COVID-19: Towards a new order of monitoring, disciplining and sanctioning energy behavior?},
journal = {Energy Research & Social Science},
volume = {84},
pages = {102355},
year = {2022},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2021.102355},
url = {https://www.sciencedirect.com/science/article/pii/S2214629621004461},
author = {Jörg Radtke},
keywords = {Smart city, Smart energy governmentality, Social power framework, Energy transition conflict, Energy communities, Energy democracy, Michel Foucault},
abstract = {The Corona pandemic has led to the increased use of online tools throughout society, whether in business, education, or daily life. This shift to an online society has led social scientists to question the extent to which increased forms of control, surveillance and enforced conformity to ways of thinking, attitudes and behaviors can be promoted through online activities. This question arises overtly amidst a pandemic, but it also lurks behind the widespread diffusion of smart energy systems throughout the world and the increased use of smart meters in those systems. The extent to which forms of monitoring, disciplining and sanctioning of energy behavior and practices could come to reality is thus an important question to consider. This article does so using the ideas of Michel Foucault, together with research on smart energy systems and current trends in energy policy. The article closes with a discussion of energy democracy and democratic legitimacy in the context of possible effects of smart technologies on community energy systems.}
}
@incollection{ELYAS202357,
title = {Chapter 3 - Physical property estimation and phase behavior for process simulation∗},
editor = {Dominic C.Y. Foo},
booktitle = {Chemical Engineering Process Simulation (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
pages = {57-86},
year = {2023},
isbn = {978-0-323-90168-0},
doi = {https://doi.org/10.1016/B978-0-323-90168-0.00005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323901680000056},
author = {Rafil Elyas},
keywords = {Enthalpy, Entropy, Equations of state, Property estimation methods, Separator, Work},
abstract = {Like the foundation of a building, the methods used for physical property estimation determine the integrity of a chemical engineering computation. These days, most engineers rely on commercial simulators to perform their computations, and all commercial simulators these days come with a myriad of property packages, where various property estimation methods have been combined into property packages such as Peng–Robinson, Soave–Redlich–Kwong, BWRS, Grayson–Streed, Braun-K10, nonrandom two liquid, UNIQUAC, and the list goes on. It is critical to know which property package would be applicable for one's computation. The objective of this chapter is to provide some insight into the workings of those property packages and enable the reader to make the correct selection.}
}
@article{VANZUNDERT2010270,
title = {Effective peer assessment processes: Research findings and future directions},
journal = {Learning and Instruction},
volume = {20},
number = {4},
pages = {270-279},
year = {2010},
note = {Unravelling Peer Assessment},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2009.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0959475209000814},
author = {Marjo {van Zundert} and Dominique Sluijsmans and Jeroen {van Merriënboer}},
keywords = {Peer assessment, Development of peer assessment skills, Attitudes towards peer assessment, Training of peer assessment skills},
abstract = {Despite the popularity of peer assessment (PA), gaps in the literature make it difficult to describe exactly what constitutes effective PA. In a literature review, we divided PA into variables and then investigated their interrelatedness. We found that (a) PA's psychometric qualities are improved by the training and experience of peer assessors; (b) the development of domain-specific skills benefits from PA-based revision; (c) the development of PA skills benefits from training and is related to students' thinking style and academic achievement, and (d) student attitudes towards PA are positively influenced by training and experience. We conclude with recommendations for future research.}
}
@article{MOINGEON2021566,
title = {Applications de l’intelligence artificielle au développement de nouveaux médicaments},
journal = {Annales Pharmaceutiques Françaises},
volume = {79},
number = {5},
pages = {566-571},
year = {2021},
issn = {0003-4509},
doi = {https://doi.org/10.1016/j.pharma.2021.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0003450921000092},
author = {P. Moingeon},
keywords = {Biotechnologies, Développement médicamenteux, Intelligence artificielle, Machines intelligentes, Médecine computationnelle, Médecine de précision, Modèle de maladie, Artificial intelligence, Biotechnologies, Computational medicine, Disease model, Drug development, Intelligent machines, Precision medicine},
abstract = {Résumé
L’intelligence artificielle (IA) recouvre les technologies qui reproduisent, par la machine, quatre dimensions de l’intelligence humaine, à savoir la perception, l’analyse, l’action et l’apprentissage. Les progrès technologiques, combinés dans ces domaines, permettent de générer et d’analyser des données massives pour modéliser la réalité d’un phénomène. Ces modèles sont ensuite réactualisés par l’accumulation de nouvelles données afin d’aider à la prise de décision et prédire le futur. Appliquée à la problématique du développement d’un médicament, l’IA permet d’établir des modèles de maladies à partir de données de profilage moléculaire de patients. Par sa puissance de calcul, l’IA intègre ces données multimodales massives dans un modèle permettant : (i) de rendre compte de l’hétérogénéité des maladies ; et (ii) d’identifier des cibles thérapeutiques importantes dans la physiopathologie. D’autres analyses computationnelles sont utilisées pour identifier des molécules thérapeutiques interagissant avec ces cibles, optimiser ces molécules ou repositionner des molécules anciennes dans de nouvelles indications. La modélisation par l’IA aide également à identifier des biomarqueurs d’efficacité, définir des combinaisons de molécules thérapeutiques pertinentes, concevoir des études cliniques innovantes avec des groupes placebo virtuels… Cette convergence révolutionnaire entre les biotechnologies, les sciences du médicament et l’IA donne aujourd’hui naissance à une médecine computationnelle de précision applicable à toutes les maladies chroniques, qui offrira des traitements parfaitement ciblés prenant en compte les spécificités du patient quant à sa physiologie, sa maladie, sa relation à l’environnement.
Summary
Artificial intelligence (AI) encompasses technologies recapitulating four dimensions of human intelligence, i.e. sensing, thinking, acting and learning. The convergence of technological advances in those fields allows to integrate massive data and build probabilistic models of a problem. The latter can be continuously updated by incorporating new data to inform decision-making and predict the future. In support of drug discovery and development, AI allows to generate disease models using data obtained following extensive molecular profiling of patients. Given its superior computational power, AI can integrate those big multimodal data to generate models allowing: (i) to represent patient heterogeneity; and (ii) identify therapeutic targets with inferences of causality in the pathophysiology. Additional computational analyses can help identifying and optimizing drugs interacting with these targets, or even repurposing existing molecules for a new indication. AI-based modeling further supports the identification of biomarkers of efficacy, the selection of appropriate combination therapies and the design of innovative clinical studies with virtual placebo groups. The convergence of biotechnologies, drug sciences and AI is fostering the emergence of a computational precision medicine predicted to yield therapies or preventive measures precisely tailored to patient characteristics in terms of their physiology, disease features and environmental risk exposure.}
}
@article{PICCIONI2024114222,
title = {From layer to building: Multiscale modeling of thermo-optical properties in 3D-printed facades},
journal = {Energy and Buildings},
volume = {314},
pages = {114222},
year = {2024},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.114222},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824003384},
author = {Valeria Piccioni and Lars O. Grobe and Illias Hischier and Arno Schlueter},
keywords = {3D-printed facades, Thermo-optical properties, Multi-scale modeling, Experimental validation},
abstract = {The challenge of building sector decarbonization has driven an integral rethinking of the way we design and build facades. Recently, large scale 3D-printing has emerged as an alternative manufacturing technique for novel facade components aiming at high operational efficiency and low environmental impact. Focusing on translucent polymer 3DPFs, this study tackles the challenges of modeling thermal and optical effects in geometrically complex components where interactions across multiple domains and scales occur. In particular, we introduce a novel method for modeling the irregular thermo-optical properties of 3DPFs, capable of capturing relevant effects often out of the scope of traditional modeling approaches. Our model accounts for geometry-dependent physical effects ranging from millimeter-scale fabrication details that impact optical behavior to centimeter-scale geometric features influencing heat and radiation transfer, extending up to the meter-scale implications for the building application. By employing computational techniques such as ray-tracing, computational fluid dynamics, and finite element analysis, we establish a model that offers detailed thermal and optical analysis to support performance-driven design iterations. Finally, demonstrating this approach in an office building context, we show that 3DPFs can match the performance of double glazing with dynamic shading, providing effective solar and thermal management over the year. This is achieved in a single, mono-material component with no active control, suggesting 3DPFs are a promising direction for low-environmental impact facade design.}
}
@article{SREENATH1992121,
title = {A hybrid computation environment for multibody simulation},
journal = {Mathematics and Computers in Simulation},
volume = {34},
number = {2},
pages = {121-140},
year = {1992},
issn = {0378-4754},
doi = {https://doi.org/10.1016/0378-4754(92)90049-M},
url = {https://www.sciencedirect.com/science/article/pii/037847549290049M},
author = {N. Sreenath},
abstract = {A simulation architecture capable of generating the dynamical equations of a multibody system symbolically, automatically creating the computer code to simulate these equations numerically, run the simulation and display the results using animation and graphics is discussed. The power of object-oriented programming is used systematically to manipulate the symbolic, numeric and graphic modules and produce an effective tool for understanding the complicated motions of multibody systems. The architecture has been implemented in OOPSS (Object-Oriented Planar System Simulator) a software package written in a multilanguage (macsyma–fortran–lisp) environment. The package supports user interface capable of interactively modifying system parameters, change runtime initial conditions and introduce feedback control.}
}
@article{PALANIYAPPAN2023994,
title = {Studying Psychosis Using Natural Language Generation: A Review of Emerging Opportunities},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {8},
number = {10},
pages = {994-1004},
year = {2023},
note = {Natural Language Processing in Psychiatry and Clinical Neuroscience Research},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2023.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S2451902223001040},
author = {Lena Palaniyappan and David Benrimoh and Alban Voppel and Roberta Rocca},
keywords = {Computational psychiatry, Deep learning, Explainable models, Large language models, Neural networks, Neuroimaging},
abstract = {Disrupted language in psychotic disorders, such as schizophrenia, can manifest as false contents and formal deviations, often described as thought disorder. These features play a critical role in the social dysfunction associated with psychosis, but we continue to lack insights regarding how and why these symptoms develop. Natural language generation (NLG) is a field of computer science that focuses on generating human-like language for various applications. The theory that psychosis is related to the evolution of language in humans suggests that NLG systems that are sufficiently evolved to generate human-like language may also exhibit psychosis-like features. In this conceptual review, we propose using NLG systems that are at various stages of development as in silico tools to study linguistic features of psychosis. We argue that a program of in silico experimental research on the network architecture, function, learning rules, and training of NLG systems can help us understand better why thought disorder occurs in patients. This will allow us to gain a better understanding of the relationship between language and psychosis and potentially pave the way for new therapeutic approaches to address this vexing challenge.}
}
@incollection{GIGERENZER2015515,
title = {Computers: Impact on the Social Sciences},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {515-518},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.03202-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868032025},
author = {Gerd Gigerenzer},
keywords = {Charles Babbage, Cognitive revolution, Computer simulation, Division of labor, H.A. Simon, Metaphor, Statistics},
abstract = {The social organization of labor in the nineteenth century served as the model for Babbage's first computers. In the second half of the twentieth century, when working computers were finally constructed and invaded the offices of social scientists, they turned into theories of mind. This mutual inspiration first changed the meaning of calculation and then led to a new understanding of thought as computation based on hierarchically organized subroutines. The computer as a research tool has changed the social sciences in a fundamental way, from enabling large-scale simulations of cognitive and social systems to allowing mindless and mechanical use of statistics.}
}
@article{SULLIVAN2020246,
title = {Maritime 4.0 – Opportunities in Digitalization and Advanced Manufacturing for Vessel Development},
journal = {Procedia Manufacturing},
volume = {42},
pages = {246-253},
year = {2020},
note = {International Conference on Industry 4.0 and Smart Manufacturing (ISM 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.078},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920306430},
author = {Brendan P. Sullivan and Shantanoo Desai and Jordi Sole and Monica Rossi and Lucia Ramundo and Sergio Terzi},
keywords = {Maritime 4.0, Digitalization, Maritime Vessel Development, Industry 4.0},
abstract = {Maritime vessels are complex systems that generate and require the utilization of large amounts of data for maximum efficiency. The successful utilization of sensors and IoT in the industry requires a forward-thinking approach to leverage the benefits of Industry 4.0 in a more comprehensive manner. While processes and manufacturing processes can be improved and advanced through such efforts, in order the industry to be able to benefit from data generation, integrated approaches are necessary. In order to develop truly value-added vessels, we introduce a descriptive approach for understanding Maritime 4.0.}
}
@article{MIRTSOPOULOS2023103518,
title = {Structural topology exploration through policy-based generation of equilibrium representations},
journal = {Computer-Aided Design},
volume = {160},
pages = {103518},
year = {2023},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2023.103518},
url = {https://www.sciencedirect.com/science/article/pii/S0010448523000507},
author = {Ioannis Mirtsopoulos and Corentin Fivet},
keywords = {Design space exploration, Generative design, Rule-based design, Topology, Structural design, Strut-and-tie},
abstract = {Mainstream approaches to design spatial architectural forms that are structurally relevant consist either in adapting well-known and catalogued conventional types or in searching for close-to-optimum solutions of well-defined problems. Few means exist to explore structural forms detached from these routines. The approach in this paper generates diverse non-triangulated structural topologies that do not result from optimization procedures. The process incrementally transforms interim networks of bars and forces by means of a parametric policy (–) that maintains the static equilibrium of the network at every single step, (–) that ensures growth of the network within specified (non-)convex geometric boundaries, and (–) whose high-level abstract description controls all design parameters. The successive policy application aims at decreasing the number of interim forces while increasing the number of nodes and bars in compression or tension. The entire process ends when no interim force exists anymore, which is always achievable thanks to the permanence of the static equilibrium condition. From a designer perspective, the approach opens up the generative design black box by providing geometrical and topological control and partial automation of the generation process, while not resorting to common topology patterns – e.g. triangulated bar networks. This paper describes the conceptualization and its implementation into a computational framework, named Policy-based Exploration of Equilibrium Representations (PEER). It illustrates the potential of the approach to unveil unprecedented, unexpected, but statically-valid, structural topologies. Opportunities for further development are eventually discussed.}
}
@article{JI2023106379,
title = {Scalable incomplete multi-view clustering via tensor Schatten p-norm and tensorized bipartite graph},
journal = {Engineering Applications of Artificial Intelligence},
volume = {123},
pages = {106379},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106379},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623005638},
author = {Guangyan Ji and Gui-Fu Lu and Bing Cai},
keywords = {Scalable incomplete multi-view clustering, Tensorized bipartite graph, Graph completion, Tensor low-rank constraint},
abstract = {Graph-based incomplete multi-view clustering (IMVC) methods have drawn considerable attention due to their good performance in exploring the nonlinear structure of data. However, they still have the following shortcomings. First, graph construction and eigen decomposition of the Laplacian matrix included in the IMVC methods generally have high computational complexity. Second, most methods do not consider the impact of missing views and neglect the potential relationships between different views. Third, few algorithms consider both intra-view and inter-view information for clustering. Therefore, we innovatively propose a scalable incomplete multi-view clustering via the tensor Schatten p-norm and tensorized bipartite graph (SIMVC/TSTBG) method, which combines tensorized bipartite graph, graph completion, and tensor low-rank constraint into a joint framework. Concretely, we first construct bipartite graphs based on the selected m anchor points and the n data points, reducing the size of the graph from n×n to n×m(m<<n), which considerably reduces the computational complexity. Second, we adaptively complete the missing bipartite graph, which reduces the effect of missing view information on the clustering results. Third, to explore connections between missing views and mine high-order information between views, we splice the bipartite graphs into a tensor and impose a tensor low-rank constraint, i.e., the tensor Schatten p-norm, on it. At the same time, we also design an efficient algorithm to solve SIMVC/TSTBG. To our knowledge, we are the first successful practice to integrate the tensor technique with the scalable IMVC method. Compared with other IMVC methods, the results on seven datasets fully show the high efficiency and effectiveness of SIMVC/TSTBG.}
}
@article{ZHANG2020259,
title = {Self-blast state detection of glass insulators based on stochastic configuration networks and a feedback transfer learning mechanism},
journal = {Information Sciences},
volume = {522},
pages = {259-274},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.02.058},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520301419},
author = {Qian Zhang and Weitao Li and Hua Li and Jianping Wang},
keywords = {Insulator self-blast state, Deep learning, Feedback transfer learning mechanism, Semantic error entropy},
abstract = {The self-blast state of a glass insulator directly affects the safety and reliability of transmission lines. To address the insufficient generalization ability of existing detection methods for insulator self-blast states and the drawbacks of deep neural network structures, the theories of transfer learning and closed-loop control are drawn upon to provide an intelligent detection method for the self-blast states of glass insulators. The method proposed in this paper is based on stochastic configuration networks and a feedback transfer learning mechanism. First, to reduce the redundancy of convolutional kernels in the channel extent, the interleaved group convolution strategy is employed to reconstruct the convolutional layers of the Inception network. Second, in view of the different feature applicabilities of different glass insulator images and based on the adaptive convolution module groups, the data structure of the dynamic feature space of insulator images is built with a certain mapping relationship from global to local. Then, the discriminative measure index is used to evaluate the discriminative information of the feature space to enhance the interpretability of the compact feature spance. Third, the fully connected feature vector of the compact feature space is sent to stochastic configuration networks (SCNs), which have universal approximation property to establish the classification criteria of the self-blast states of insulator images with generalization ability. Finally, an imitation of human thinking patterns is employed that exhibits repeated deliberation and comparison. Consequently, based on generalized error and entropy theories, the evaluation index of the objective function is established to evaluate the uncertain detection results of the self-blast states of glass insulator images in real time. Then, the dynamic transfer learning mechanism is constructed based on the constraint of the measurement index of uncertain detection results to realize self-optimizing regulation of the feature space that exhibits multihierarchy and discrimination and reconstructed classification criteria. The experimental results show that compared with other algorithms, the proposed method enhances the generalization ability and detection accuracy of the model.}
}
@article{ANDROUTSOPOULOS2023141,
title = {Punctuating the other: Graphic cues, voice, and positioning in digital discourse},
journal = {Language & Communication},
volume = {88},
pages = {141-152},
year = {2023},
issn = {0271-5309},
doi = {https://doi.org/10.1016/j.langcom.2022.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0271530922000957},
author = {Jannis Androutsopoulos},
keywords = {Voice, Positioning, Graphic cues, <!!1!>, Reddit, Punctuation},
abstract = {This article investigates the nested relationship between graphic cues, voice, and positioning in digital discourse. The focus is on the ‘indignation mark’, or <!!1!>, an allographic sign used in German-language discussion boards on Reddit. The study's theoretical backdrop brings research on graphic practices in digitally-mediated communication into dialogue with sociolinguistic approaches to the enactment of group relations in discourse, in particular double-voicing, stylization, and positioning, thereby aiming to foster theory-building on both sides. Data is extracted from a large German-language forum (‘subreddit’) on Reddit and subjected to computational, sequential, and microlinguistic analysis. The findings show how participants in public online discussions use punctuation signs and other graphic cues to animate voices, i.e. ways of speaking that index recognizable social positions and ideologies; how these stylized voices provide a resource for positioning; and how participants display recognition of and alignment to this feature's indexical meaning. The findings also suggest that the ‘indignation mark’ is part of a wider ecology of graphic cues, which evolve constantly to enable multi-voicedness in public digital discourse. Overall, this paper aims to advance our understanding about how graphic elements of digital discourse are indexically and ideologically connected with positioning activities in online communities of practice.}
}
@article{PAIVIO2014141,
title = {Intelligence, dual coding theory, and the brain},
journal = {Intelligence},
volume = {47},
pages = {141-158},
year = {2014},
issn = {0160-2896},
doi = {https://doi.org/10.1016/j.intell.2014.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0160289614001305},
author = {Allan Paivio},
keywords = {IQ theories, IQ tests, Conceptual/empirical flaw, DCT a unified theory, IQ neuroscience},
abstract = {The distinction between verbal and nonverbal cognitive abilities has been a defining feature of psychometric theories of intelligence for the past century. Despite their popularity, however, these theories have not included functional connections between verbal and nonverbal systems that are necessary if they are to explain performance in intellectual tasks involving interactions between language and nonverbal knowledge. This functional gap limits the capacity of psychometric theories to explain and predict fundamental aspects of individual differences in cognitive abilities that have long been studied experimentally. This article summarizes the history, nature, and possible causes of the problem, and then concludes with a neuroscientifically-enhanced, multimodal dual coding approach to intelligence that focuses on the synergistic interactivity of verbal and nonverbal systems.}
}
@article{GAO2024104747,
title = {Representing and assessing distributed situation awareness in multi-agency disaster response: A hypergraph-based methodology},
journal = {International Journal of Disaster Risk Reduction},
volume = {111},
pages = {104747},
year = {2024},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2024.104747},
url = {https://www.sciencedirect.com/science/article/pii/S2212420924005090},
author = {Chong Gao and Hui Jiang and Xiaoling Guo},
keywords = {Distributed situation awareness, Systems thinking, Higher-order interactions, Multi-agency response},
abstract = {This paper introduces a novel hypergraph-based methodology for representing and assessing distributed situation awareness (DSA) in multi-agency disaster response. The fundamental ideas and motivations of our methodology stem from the following widely acknowledged understandings and phenomenons: (a) DSA’s representation should be approached from social, information and task dimensions; (b) DSA is one of the collective behaviors that emerge from the interactions; (c) the interactions in the real world are not pairwise. Our methodology delineates the collaboration, co-activation, and co-existence interactions among social, information, and task elements as higher-order interactions. We then construct these interaction systems using hypergraph-structured data derived from disaster response scenarios. Subsequently, these systems are encoded into hypergraphs, which are validated against our dataset and proven to be practical tools for depicting higher-order interaction patterns. Analytical techniques tailored to hypergraphs are applied, yielding insights intrinsic to hypergraphs regarding DSA in emergency response. Moreover, we integrate these interaction systems into a comprehensive framework that allows for the visualization and quantitative analyses of DSA evolution dynamics. We propose several indicators of evolution, discussing their trends and implications throughout the development of the emergency response. We locate the system deficiencies by revealing a mismatch between the positions of specific elements in the network and their functions. We also identify the saturation phase in the DSA evolution process.}
}
@article{HUHN201642,
title = {Cognitive framing in action},
journal = {Cognition},
volume = {151},
pages = {42-51},
year = {2016},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2016.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S0010027716300439},
author = {John M. Huhn and Cory Adam Potts and David A. Rosenbaum},
keywords = {Action, Cognitive framing, Heuristics, Object manipulation, Motor control, Bimanual actions},
abstract = {Cognitive framing effects have been widely reported in higher-level decision-making and have been ascribed to rules of thumb for quick thinking. No such demonstrations have been reported for physical action, as far as we know, but they would be expected if cognition for physical action is fundamentally similar to cognition for higher-level decision-making. To test for such effects, we asked participants to reach for a horizontally-oriented pipe to move it from one height to another while turning the pipe 180° to bring one end (the “business end”) to a target on the left or right. From a physical perspective, participants could have always rotated the pipe in the same angular direction no matter which end was the business end; a given participant could have always turned the pipe clockwise or counter-clockwise. Instead, our participants turned the business end counter-clockwise for left targets and clockwise for right targets. Thus, the way the identical physical task was framed altered the way it was performed. This finding is consistent with the hypothesis that cognition for physical action is fundamentally similar to cognition for higher-level decision-making. A tantalizing possibility is that higher-level decision heuristics have roots in the control of physical action, a hypothesis that accords with embodied views of cognition.}
}
@article{KRAUSE20211094,
title = {The challenge of ensuring affordability, sustainability, consistency, and adaptability in the common metrics agenda},
journal = {The Lancet Psychiatry},
volume = {8},
number = {12},
pages = {1094-1102},
year = {2021},
issn = {2215-0366},
doi = {https://doi.org/10.1016/S2215-0366(21)00122-X},
url = {https://www.sciencedirect.com/science/article/pii/S221503662100122X},
author = {Karolin Rose Krause and Sophie Chung and Maria da Luz {Sousa Fialho} and Peter Szatmari and Miranda Wolpert},
abstract = {Summary
Mental health research grapples with research waste and stunted field progression caused by inconsistent outcome measurement across studies and clinical settings, which means there is no common language for considering findings. Although recognising that no gold standard measures exist and that all existing measures are flawed in one way or another, anxiety and depression research is spearheading a common metrics movement to harmonise measurement, with several initiatives over the past 5 years recommending the consistent use of specific scales to allow read-across of measurements between studies. For this approach to flourish, however, common metrics must be acceptable and adaptable to a range of contexts and populations, and global access should be as easy and affordable as possible, including in low-income countries. Within a measurement landscape dominated by fixed proprietary measures and with competing views of what should be measured, achieving this goal poses a range of challenges. In this Personal View, we consider tensions between affordability, sustainability, consistency, and adaptability that, if not addressed, risk undermining the common metrics agenda. We outline a three-pronged way forward that involves funders taking more direct responsibility for measure development and dissemination; a move towards managing measure dissemination and adaptation via open-access measure hubs; and transitioning from fixed questionnaires to item banks. We argue that now is the time to start thinking of mental health metrics as 21st century tools to be co-owned and co-created by the mental health community, with support from dedicated infrastructure, coordinating bodies, and funders.}
}
@article{RAM2022100232,
title = {The role of ‘big data’ and ‘in silico’ New Approach Methodologies (NAMs) in ending animal use – A commentary on progress},
journal = {Computational Toxicology},
volume = {23},
pages = {100232},
year = {2022},
issn = {2468-1113},
doi = {https://doi.org/10.1016/j.comtox.2022.100232},
url = {https://www.sciencedirect.com/science/article/pii/S2468111322000202},
author = {Rebecca N. Ram and Domenico Gadaleta and Timothy E.H. Allen},
keywords = {Computational toxicology, In-silico, NAMs, New approach methodologies, Human relevant, QSAR, Read across, Chemical safety, High throughput, Adverse outcome pathways},
abstract = {In silico (computational) methods continue to evolve as part of a robust 21st century public health strategy in risk assessment, relevant to all sectors of chemical safety including preclinical drug discovery, industrial chemicals testing, food and cosmetics. Alongside in vitro methods as components of intelligent testing and pathway driven strategies, in silico models provide the potential for more human relevant solutions to the use of animals in safety testing and biomedical research. These are often termed ‘New Approach Methodologies’ (NAMs). Some NAMs incorporate the use of ‘big data’, for example the information provided from high throughput or high content in vitro screening assays or ‘omics’ technologies. Big data has increasing relevance to predictive toxicology but must be appropriately defined, particularly with regard to ‘quality vs quantity’. The purpose of this article is to provide a commentary on the progress of in silico human-based research methods within the context of NAMs, as well as discussion of the emerging use of big data with relevance to safety assessment. The current status of in silico methods is discussed, with input from researchers in the field. Scientific and legislative drivers for change are also considered, along with next steps to address challenges in funding and recognition, to achieve regulatory acceptance and uptake within the research community. To provide some wider context, the use of in silico methods alongside other relevant approaches (e.g., human-based in vitro) is also discussed.}
}
@article{HU2023100795,
title = {A cardiologist-like computer-aided interpretation framework to improve arrhythmia diagnosis from imbalanced training datasets},
journal = {Patterns},
volume = {4},
number = {9},
pages = {100795},
year = {2023},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2023.100795},
url = {https://www.sciencedirect.com/science/article/pii/S2666389923001502},
author = {Lianting Hu and Shuai Huang and Huazhang Liu and Yunmei Du and Junfei Zhao and Xiaoting Peng and Dantong Li and Xuanhui Chen and Huan Yang and Lingcong Kong and Jiajie Tang and Xin Li and Heng Liang and Huiying Liang},
keywords = {arrhythmia, inter-class bullying, waveform clustering, heartbeat splicing, Bayesian approach},
abstract = {Summary
Arrhythmias can pose a significant threat to cardiac health, potentially leading to serious consequences such as stroke, heart failure, cardiac arrest, shock, and sudden death. In computer-aided electrocardiogram interpretation systems, the inclusion of certain classes of arrhythmias, which we term “aggressive” or “bullying,” can lead to the underdiagnosis of other “vulnerable” classes. To address this issue, a method for arrhythmia diagnosis is proposed in this study. This method combines morphological-characteristic-based waveform clustering with Bayesian theory, drawing inspiration from the diagnostic reasoning of experienced cardiologists. The proposed method achieved optimal performance in macro-recall and macro-precision through hyperparameter optimization, including spliced heartbeats and clusters. In addition, with increasing bullying by aggressive arrhythmias, our model obtained the highest average recall and the lowest average drop in recall on the nine vulnerable arrhythmias. Furthermore, the maximum cluster characteristics were found to be consistent with established arrhythmia diagnostic criteria, lending interpretability to the proposed method.}
}
@article{MIRZAEI2021102839,
title = {CFD modeling of micro and urban climates: Problems to be solved in the new decade},
journal = {Sustainable Cities and Society},
volume = {69},
pages = {102839},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.102839},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721001293},
author = {Parham A. Mirzaei},
keywords = {Computational fluid dynamics, Urban climate, Microclimate, Data acquisition, Building energy simulation, Sustainable, Resilient, Smart cities},
abstract = {Despite the popularity of the micro/urban climate CFD modeling as a powerful approach to simulate convective exchanges in urban areas, yet its application faces three profound limitations, including (1) computational barriers, (2) data acquisition, and (3) over-simplifications of underlying physics. Computational resources are not qualitatively studied to be allocated to their best of performance in urban climate models. Moreover, bigdata of city components and inhabitants are sometimes inaccessible or difficult to be effectively interpreted to be fed into CFD models. Furthermore, commonly adopted oversimplifications, and misinterpretation of underlying physics of urban climate can substantially render falsified results, no matter if they look otherwise followed by extravagant visual reports. This paper, hence, aims to explore the capabilities and limitations of urban climate CFD modeling. It further scrutinizes the common oversimplifications in the modeling techniques, potentially resulting in CFD capacities to be lost in the translation. The paper describes the extend to which CFD tools can be the favourable options and otherwise, while it underpins the areas in which further research is needed to conform urban climate CFD models as practical design and decision-making tools. It also offers a brief overview in the recent advancements in response to the mentioned challenges.}
}
@article{LILWALL1989268,
title = {Seismological Algorithms, Computational Methods and Computer Programs: Durk J. Doornbos (Editor), Academic Press/Harcourt Brace Jovanovich, 1988, 469 pp., £39.50, ISBN 0-12-220770-X},
journal = {Physics of the Earth and Planetary Interiors},
volume = {58},
number = {2},
pages = {268-269},
year = {1989},
issn = {0031-9201},
doi = {https://doi.org/10.1016/0031-9201(89)90062-9},
url = {https://www.sciencedirect.com/science/article/pii/0031920189900629},
author = {R.C. Lilwall}
}
@article{IGAMBERDIEV2024105346,
title = {Reflexive neural circuits and the origin of language and music codes},
journal = {BioSystems},
volume = {246},
pages = {105346},
year = {2024},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2024.105346},
url = {https://www.sciencedirect.com/science/article/pii/S0303264724002314},
author = {Abir U. Igamberdiev},
keywords = {Aristotle, Language, Consciousness, Musical code, Reflexive psychology, Self-awareness},
abstract = {Conscious activity is grounded in the reflexive self-awareness in sense perception, through which the codes signifying sensual perceptive events operate and constrain human behavior. These codes grow via the creative generation of hypertextual statements. We apply the model of Vladimir Lefebvre (Lefebvre, V.A., 1987, J. Soc. Biol. Struct. 10, 129–175) to reveal the underlying structures on which the perception and creative development of language and music codes are based. According to this model, the reflexive structure of conscious subject is grounded in three thermodynamic cycles united by the control of the basic functional cycle by the second one, and resulting in the internal action that it turn is perceived by the third cycle evaluating this action. In this arrangement, the generative language structures are formed and the frequencies of sounds that form musical phrases and patterns are selected. We discuss the participation of certain neural brain structures and the establishment of reflexive neural circuits in the ad hoc transformation of perceptive signals, and show the similarities between the processes of perception and of biological self-maintenance and morphogenesis. We trace the peculiarities of the temporal encoding of emotions in music and musical creativity, as well as the principles of sharing musical information between the performing and the perceiving individuals.}
}
@article{SEISING2006237,
title = {From vagueness in medical thought to the foundations of fuzzy reasoning in medical diagnosis},
journal = {Artificial Intelligence in Medicine},
volume = {38},
number = {3},
pages = {237-256},
year = {2006},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2006.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0933365706001072},
author = {Rudolf Seising},
keywords = {History of science and technology, Fuzzy set theory, Vagueness, Medical diagnoses, Computer assistance, Medical philosophy, System theory},
abstract = {Summary
Objective
This article delineates a relatively unknown path in the history of medical philosophy and medical diagnosis. It is concerned with the phenomenon of vagueness in the physician's “style of thinking” and with the use of fuzzy sets, systems, and relations with a view to create a model of such reasoning when physicians make a diagnosis. It represents specific features of medical ways of thinking that were mentioned by the Polish physician and philosopher Ludwik Fleck in 1926. The paper links Lotfi Zadeh's work on system theory before the age of fuzzy sets with system-theory concepts in medical philosophy that were introduced by the philosopher Mario Bunge, and with the fuzzy-theoretical analysis of the notions of health, illness, and disease by the Iranian-German physician and philosopher Kazem Sadegh-Zadeh.
Material
Some proposals to apply fuzzy sets in medicine were based on a suggestion made by Zadeh: symptoms and diseases are fuzzy in nature and fuzzy sets are feasible to represent these entity classes of medical knowledge. Yet other attempts to use fuzzy sets in medicine were self-contained. The use of this approach contributed to medical decision-making and the development of computer-assisted diagnosis in medicine.
Conclusion
With regard to medical philosophy, decision-making, and diagnosis; the framework of fuzzy sets, systems, and relations is very useful to deal with the absence of sharp boundaries of the sets of symptoms, diagnoses, and phenomena of diseases. The foundations of reasoning and computer assistance in medicine were the result of a rapid accumulation of data from medical research. This explosion of knowledge in medicine gave rise to the speculation that computers could be used for the medical diagnosis. Medicine became, to a certain extent, a quantitative science. In the second half of the 20th century medical knowledge started to be stored in computer systems. To assist physicians in medical decision-making and patient care, medical expert systems using the theory of fuzzy sets and relations (such as the Viennese “fuzzy version” of the Computer-Assisted Diagnostic System, Cadiag, which was developed at the end of the 1970s) were constructed. The development of fuzzy relations in medicine and their application in computer-assisted diagnosis show that this fuzzy approach is a framework to deal with the “fuzzy mode of thinking” in medicine.}
}
@article{LIANG2022119384,
title = {Dynamic Causal Modelling of Hierarchical Planning},
journal = {NeuroImage},
volume = {258},
pages = {119384},
year = {2022},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2022.119384},
url = {https://www.sciencedirect.com/science/article/pii/S1053811922005031},
author = {Qunjun Liang and Jinhui Li and Senning Zheng and Jiajun Liao and Ruiwang Huang},
keywords = {Dynamic Causal Modelling (DCM), Parametric Empirical Bayes (PEB), fMRI, neural architecture, individual difference},
abstract = {Hierarchical planning (HP) is a strategy that optimizes the planning by storing the steps towards the goal (lower-level planning) into subgoals (higher-level planning). In the framework of model-based reinforcement learning, HP requires the computation through the transition value between higher-level hierarchies. Previous study identified the dmPFC, PMC and SPL were involved in the computation process of HP respectively. However, it is still unclear about how these regions interaction with each other to support the computation in HP, which could deepen our understanding about the implementation of plan algorithm in hierarchical environment. To address this question, we conducted an fMRI experiment using a virtual subway navigation task. We identified the activity of the dmPFC, premotor cortex (PMC) and superior parietal lobe (SPL) with general linear model (GLM) in HP. Then, Dynamic Causal Modelling (DCM) was performed to quantify the influence of the higher- and lower-planning on the connectivity between the brain areas identified by the GLM. The strongest modulation effect of the higher-level planning was found on the dmPFC→right PMC connection. Furthermore, using Parametric Empirical Bayes (PEB), we found the modulation of higher-level planning on the dmPFC→right PMC and right PMC→SPL connections could explain the individual difference of the response time. We conclude that the dmPFC-related connectivity takes the response to the higher-level planning, while the PMC acts as the bridge between the higher-level planning to behavior outcome.}
}
@article{JIAQI2024109752,
title = {Decomposed-coordinated framework with intelligent extremum network for operational reliability analysis of complex system},
journal = {Reliability Engineering & System Safety},
volume = {242},
pages = {109752},
year = {2024},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2023.109752},
url = {https://www.sciencedirect.com/science/article/pii/S095183202300666X},
author = {Liu Jia-Qi and Feng Yun-Wen and Lu Cheng and Pan Wei-Huang},
keywords = {Machine learning method, System operational reliability, Decomposed-coordinated surrogate model, Multi-failure mode, Approximate modeling},
abstract = {The analysis of operational reliability in complex systems, which involve numerous subsystems and multiple disciplines, presents significant computational challenges due to their highly nonlinear, transient nature and the presence of many hyperparameters. Although reliability analysis models have made progress, they are still inadequate for accurately modeling composite functions with multiple sublayers and sub-functions. To improve the performance of modeling composite functions, the decomposed-coordinated intelligent extremum network model (DC-IENM) is proposed in this paper. The present study employs the decomposed-coordinated (DC) strategy as a means to effectively address the coordination relationship among multiple analysis objectives. To assess the efficacy of the proposed approach, two illustrative examples are considered: (1) the approximate and probabilistic analysis of a nonlinear function with multiple responses, and (2) the reliability analysis of civil aircraft brake system temperature. These examples serve to demonstrate the effectiveness of the developed DC-IENM. Furthermore, the modeling and simulation properties are rigorously examined by means of a comparative analysis involving various methodologies. The obtained results unequivocally indicate that the proposed DC-IENM exhibits distinct advantages in terms of both computational efficiency and precision.}
}
@article{LAUTENBACHER2024129583,
title = {Petz recovery maps for qudit quantum channels},
journal = {Physics Letters A},
volume = {512},
pages = {129583},
year = {2024},
issn = {0375-9601},
doi = {https://doi.org/10.1016/j.physleta.2024.129583},
url = {https://www.sciencedirect.com/science/article/pii/S0375960124002779},
author = {Lea Lautenbacher and Vinayak Jagadish and Francesco Petruccione and Nadja K. Bernardes},
keywords = {Petz recovery map, Qudit channels, Non-unital maps},
abstract = {This study delves into the efficacy of the Petz recovery map within the context of two paradigmatic quantum channels: dephasing and amplitude-damping. While prior investigations have predominantly focused on qubits, our research extends this inquiry to higher-dimensional systems. We introduce a novel, state-independent framework based on the Choi-Jamiołkowski isomorphism to evaluate the performance of the Petz map. By analyzing different channels and the (non-)unital nature of these processes, we emphasize the pivotal role of the reference state selection in determining the map's effectiveness. Furthermore, our analysis underscores the considerable impact of suboptimal choices on performance, prompting a broader consideration of factors such as system dimensionality.}
}
@article{GUO2023100142,
title = {Advances in biorenewables-resource-waste systems and modelling},
journal = {Carbon Capture Science & Technology},
volume = {9},
pages = {100142},
year = {2023},
issn = {2772-6568},
doi = {https://doi.org/10.1016/j.ccst.2023.100142},
url = {https://www.sciencedirect.com/science/article/pii/S2772656823000465},
author = {Miao Guo and Chunfei Wu and Stephen Chapman and Xi Yu and Tom Vinestock and Astley Hastings and Pete Smith and Nilay Shah},
keywords = {Biomass, Biorenewable, Mathematical optimisation, Process design, Modelling advances},
abstract = {The transformation to a resource-circular bio-economy offers a mechanism to mitigate climate change and environmental degradation. As advanced bioeconomy components, biorenewables derived from terrestrial, aquatic biomass and waste resources are expected to play significant roles over the next decades. This study provides an overview of potential biomass resources ranging from higher plant species to phototrophic microbial cluster, and their fundamental photosynthesis processes as well as biogeochemical carbon cycles involved in ecosystems. The review reflects empirical advances in conversion technologies and processes to manufacture value-added biorenewables from biomass and waste resources. The nexus perspective of resource-biorenewable-waste has been analysed to understand their interdependency and wider interaction with environmental resources and ecosystems. We further discussed the systems perspectives of biorenewables to develop fundamental understanding of resource flows and carbon cycles across biorenewable subsystems and highlight their spatial and temporal variability. Our in-depth review suggested the system challenges of biorenewable, which are subject to nonlinearity, variability and complexity. To unlock such system complexity and address the challenges, a whole systems approach is necessary to develop fundamental understanding, design novel biorenewable solutions. Our review reflects recent advances and prospects of computational methods for biorenewable systems modelling. This covers the development and applications of first principle models, process design, quantitative evaluation of sustainability and ecosystem services and mathematical optimisation to improve design, operation and planning of processes and develop emerging biorenewable systems. Coupling these advanced computational methods, a whole systems approach enables a multi-scale modelling framework to inherently link the processes and subsystems involved in biomass ecosystems and biorenewable manufacturing. Reviewing modelling advances, our study provides insights into the emerging opportunities in biorenewable research and highlights the frontier research directions, which have the potential to impact biorenewable sector sustainability.}
}
@article{RABOY201736,
title = {An introductory microeconomics in-class experiment to reinforce the marginal utility/price maximization rule and the integration of modern theory},
journal = {International Review of Economics Education},
volume = {24},
pages = {36-49},
year = {2017},
issn = {1477-3880},
doi = {https://doi.org/10.1016/j.iree.2016.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S1477388016300494},
author = {David G. Raboy},
keywords = {Experimental economics, Modern microeconomics, Principles classes, Alternative pedagogy},
abstract = {This paper presents an in-class experiment used as a teaching tool in an introductory microeconomics class at the undergraduate college level. It is directed at a critical but challenging concept for principles students—constrained utility maximization and a methodology to intuit preferences. The experimental project is nested in the literature pertaining to the current transition in microeconomic theory motivated by contributions from behavioral economics and transactions-cost economics, among other elements; modern pedagogical models; experimental economics; and experiments as in-classroom teaching tools. While not dispositive as to the general efficacy of in-class experiments, the paper provides an example of an alternative instructional approach which is helpful to principles students under strictly defined protocols. The benefits to students include heightened understanding of the core subject topic, greater interest in the subject matter, a closer connection to real-world economics, and enhanced critical thinking capabilities.}
}
@article{ANDRADE2022102986,
title = {Writing styles and modes of engagement with the future},
journal = {Futures},
volume = {141},
pages = {102986},
year = {2022},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2022.102986},
url = {https://www.sciencedirect.com/science/article/pii/S0016328722000866},
author = {Stefan B. Andrade and Anneke Sools and Yashar Saghai},
keywords = {Anticipation, Writing style, Modes of engagement with the future, Anticipatory functions, narrative, Digital story grammar},
abstract = {This paper present a new approach to analyze how people anticipate the future in times of uncertainty. Our approach combines insights from narrative theory and the sociology of anticipatory modes of engagement with the future. We applied a mixed method approach to analyze 166 letters from a creative writing exercise where residents from five countries was asked to write retrospectively from the viewpoint of a desired post-corona future. Using the methodology of Digital Story Grammar, we first categorized the letters given their grammatical structure in terms of who are in stories (characters), what the stories are about (type of action), and to what or whom were the actions directed to (objects for the character’s actions). This resulted in four writing styles: (1) analytical-observational, (2) collective-moral, (3) dialogical-personal, and (4) sensory-emotional. Consequently, we interpreted the four writing styles qualitatively in relation to the theory of modes of engagement with the future (i.e., familiarity, plans, exploration, and justification). We conclude by reflecting on the relationships between writing styles and modes in a multi-paradigmatic approach to the study of anticipation and the relevance to scenario-building practices.}
}
@incollection{NICHELLI2016379,
title = {Chapter 23 - Consciousness and Aphasia},
editor = {Steven Laureys and Olivia Gosseries and Giulio Tononi},
booktitle = {The Neurology of Conciousness (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {379-391},
year = {2016},
isbn = {978-0-12-800948-2},
doi = {https://doi.org/10.1016/B978-0-12-800948-2.00023-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128009482000236},
author = {Paolo Nichelli},
keywords = {language impairment, anarthria, dynamic aphasia, fMRI, neurophysiological measures},
abstract = {Different language impairments allow us to investigate how much the use of language can influence the content of conscious awareness and therefore of thinking and reasoning. Pure anarthria (different from mutism) and verbal short-term memory deficits are associated with an impairment of the effect of covert speech on the content of working memory. Dynamic aphasia impairs the processes involved in the transition between thinking and speaking. However, even the most severe agrammatic patients can retain reasoning about others’ beliefs that according to some theories can only take place in explicit sentences of a natural language. Error monitoring is also impaired in many aphasic patients and in some of them is associated with complete lack of error awareness (anosognosia for aphasia). In patients with impaired consciousness whenever language examination is impossible or unreliable, fMRI and neurophysiological measures such as event-related potentials can provide a window for examining residual language capabilities.}
}
@incollection{SHAYNAROSENBAUM201787,
title = {2.06 - Episodic and Semantic Memory},
editor = {John H. Byrne},
booktitle = {Learning and Memory: A Comprehensive Reference (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {87-118},
year = {2017},
isbn = {978-0-12-805291-4},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.21037-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093245210377},
author = {R. {Shayna Rosenbaum} and Alice S.N. Kim and Stevenson Baker},
keywords = {Aging, Amnesia, Autobiographical memory, Autonoetic consciousness, Child development, Default mode network, Familiarity, fMRI, Future imagining, Hippocampus, Medial temporal lobe, Mental time travel, Patient K.C., Personal semantic memory, Recollection, Spatial memory, Temporal neocortex},
abstract = {Much of the richness in human life derives from episodic memory, mental representations of detailed experiences from our personal pasts. To make sense of those experiences, knowledge about the world and oneself must also exist in a form that is free of context – known as semantic memory. This chapter revisits and builds on Tulving's distinction between episodic and semantic memory, with a focus on their differences, similarities, and interactions, informed by cognitive, neuropsychological, and neuroimaging studies. Extensions of this distinction into spatial memory, and beyond memory into future thinking, are considered in the context of process views of memory organization.}
}
@incollection{CATTANEO2015220,
title = {Mental Imagery: Visual Cognition},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {220-227},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.57024-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008097086857024X},
author = {Zaira Cattaneo and Juha Silvanto},
keywords = {Brain stimulation, Creative thinking, Depictivist, Imagery, Imagery debate, Mathematics, Memory, Neuroimaging, Occipital cortex, Perception, Propositional, Reasoning, Vision, Visual cognition, Working memory},
abstract = {Mental imagery can be defined as a quasi-perceptual experience occurring in the absence of perceptual input. The present article provides a review of the key processes involved in mental imagery, the relationship of imagery to working memory, and of the debate on the underlying format of mental images. We also review the functional significance of imagery in a range of cognitive processes, such as memory, creative thinking, reasoning, and problem solving. Finally, the brain basis of mental imagery and its overlap with the cortical regions involved in visual perception are discussed.}
}
@article{WILKINSON2024168584,
title = {Environmental impacts of earth observation data in the constellation and cloud computing era},
journal = {Science of The Total Environment},
volume = {909},
pages = {168584},
year = {2024},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2023.168584},
url = {https://www.sciencedirect.com/science/article/pii/S0048969723072121},
author = {R. Wilkinson and M.M. Mleczko and R.J.W. Brewin and K.J. Gaston and M. Mueller and J.D. Shutler and X. Yan and K. Anderson},
keywords = {Cloud computing, Satellite, Data centre, Carbon intensity, Environmental impacts},
abstract = {Numbers of Earth Observation (EO) satellites have increased exponentially over the past decade reaching the current population of 1193 (January 2023). Consequently, EO data volumes have mushroomed and data storage and processing have migrated to the cloud. Whilst attention has been given to the launch and in-orbit environmental impacts of satellites, EO data environmental footprints have been overlooked. These issues require urgent attention given data centre water and energy consumption, high carbon emissions for computer component manufacture, and difficulty of recycling computer components. Doing so is essential if the environmental good of EO is to withstand scrutiny. We provide the first assessment of the EO data life-cycle and estimate that the current size of the global EO data collection is ~807 PB, increasing by ~100 PB/year. Storage of this data volume generates annual CO2 equivalent emissions of 4101 t. Major state-funded EO providers use 57 of their own data centres globally, and a further 178 private cloud services, with considerable duplication of datasets across repositories. We explore scenarios for the environmental cost of performing EO functions on the cloud compared to desktop machines. A simple band arithmetic function applied to a Landsat 9 scene using Google Earth Engine (GEE) generated CO2 equivalent (e) emissions of 0.042–0.69 g CO2e (locally) and 0.13–0.45 g CO2e (European data centre; values multiply by nine for Australian data centre). Computation-based emissions scale rapidly for more intense processes and when testing code. When using cloud services such as GEE, users have no choice about the data centre used and we push for EO providers to be more transparent about the location-specific impacts of EO work, and to provide tools for measuring the environmental cost of cloud computation. The EO community as a whole needs to critically consider the broad suite of EO data life-cycle impacts.}
}
@article{WHITACRE201747,
title = {Integer comparisons across the grades: Students’ justifications and ways of reasoning},
journal = {The Journal of Mathematical Behavior},
volume = {45},
pages = {47-62},
year = {2017},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2016.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732312316301742},
author = {Ian Whitacre and Beti Azuz and Lisa L.C. Lamb and Jessica Pierson Bishop and Bonnie P. Schappelle and Randolph A. Philipp},
keywords = {Integers, Negative numbers, Children’s mathematical thinking, Order, Magnitude},
abstract = {This study is an investigation of students’ reasoning about integer comparisons—a topic that is often counterintuitive for students because negative numbers of smaller absolute value are considered greater (e.g., −5>−6). We posed integer-comparison tasks to 40 students each in Grades 2, 4, and 7, as well as to 11th graders on a successful mathematics track. We coded for correctness and for students’ justifications, which we categorized in terms of 3 ways of reasoning: magnitude-based, order-based, and developmental/other. The 7th graders used order-based reasoning more often than did the younger students, and it more often led to correct answers; however, the college-track 11th graders, who responded correctly to almost every problem, used a more balanced distribution of order- and magnitude-based reasoning. We present a framework for students’ ways of reasoning about integer comparisons, report performance trends, rank integer-comparison tasks by relative difficulty, and discuss implications for integer instruction.}
}
@article{LESSARD20071754,
title = {Complexity and reflexivity: Two important issues for economic evaluation in health care},
journal = {Social Science & Medicine},
volume = {64},
number = {8},
pages = {1754-1765},
year = {2007},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2006.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0277953606006356},
author = {Chantale Lessard},
keywords = {Health economics, Complexity theory, Reflexivity, Economic evaluation in health care},
abstract = {Economic evaluations are analytic techniques to assess the relative costs and consequences of health care programmes and technologies. Their role is to provide rigorous data to inform the health care decision-making process. Economic evaluation may oversimplify complex health care decisions. These analyses often ignore important health consequences, contextual elements, relationships or other relevant modifying factors, which might not be appropriate in a multi-objective, multi-stakeholder issue. One solution would be to develop a new paradigm based on the issues of perspective and context. Complexity theory may provide a useful conceptual framework for economic evaluation in health care. Complexity thinking develops an awareness of issues including uncertainty, contextual issues, multiple perspectives, broader societal involvement, and transdisciplinarity. This points the economic evaluation field towards an accountability and epistemology based on pluralism and uncertainty, requiring new forms of lay-expert engagement and roles of lay knowledge into decision-making processes. This highlights the issue of reflexivity in economic evaluation in health care. A reflexive approach would allow economic evaluators to analyze how objective structures and subjective elements influence their practices. In return, this would point increase the integrity and reliability of economic evaluations. Reflexivity provides opportunities for critically thinking about the organization and activities of the intellectual field, and perhaps the potential of moving in new, creative directions. This paper argues for economic evaluators to have a less positivist attitude towards what is useful knowledge, and to use more imagination about the data and methodologies they use.}
}
@article{HOBBS2019100055,
title = {Estimating peak water demand: Literature review of current standing and research challenges},
journal = {Results in Engineering},
volume = {4},
pages = {100055},
year = {2019},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2019.100055},
url = {https://www.sciencedirect.com/science/article/pii/S2590123019300556},
author = {Ian Hobbs and Martin Anda and Parisa A. Bahri},
keywords = {Fixture unit, Peak water demand, Modified wistort method, Exhaustive enumeration method, Water demand calculator, Loading unit normalisation method},
abstract = {Since the 1940s, the models used to estimate peak water demand has been based largely upon variations and refinements of the probabilistic ‘fixture unit’ model. An approach originally advanced by Hunter (1940) in the United States of America (USA). Seeking an improved approach to the 'fixture unit' model, now widely recognised as outdated, is the key driving force behind the current work. Boosted by the development of computing power, the plumbing industry, researchers, and academics have, over the last decade, developed computational models as a means of estimating peak water demand. This paper builds on computational models embracing the estimation of peak water demand. A brief outline of the fixture unit and its limitations is provided with key developments in computational modeling comprising current developments from the USA and UK. A brief outline of computational models is presented: Modified Wistort Method (MWM); the Exhaustive Enumeration Method (EEM), and the Water Demand Calculator (WDC). Also presented, from the UK, is the Loading Unit Normalisation Assessment method (LUNA) aimed at an improved model to size domestic hot and cold-water systems. The analysis of the computational models suggests the WDC model is conceivably the most compatible with that of the plumbing industry's design requirements. Suggesting this model could easily be adapted to meet the requirements across international borders. Challenges for the international acceptance of the WDC are the field study requirements to determine p (probability of use) and q (fixture flow rate) values for all types of buildings.}
}
@incollection{CHIARENZA202317,
title = {Chapter 2 - The psychophysiology of “covert” goal-directed behavior},
editor = {Tal Dotan Ben-Soussan and Joseph Glicksohn and Narayanan Srinivasan},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {280},
pages = {17-42},
year = {2023},
booktitle = {Neurophysiology of Silence Part B: Theory and Review},
issn = {0079-6123},
doi = {https://doi.org/10.1016/bs.pbr.2023.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0079612323000067},
author = {Giuseppe Augusto Chiarenza},
keywords = {Covert behavior, Movement related potentials, Bereitschaftspotential, Skilled performance positivity, Development, Dyslexia, Education},
abstract = {Covert behavior is defined as behavior that is not directly visible and is thus comparable to a type of behavioral silence that requires modern psychophysiological techniques to reveal. Goal-directed behavior is teleologically purposive. Fundamentally, there are two approaches to accounting for purposeful behavior. One is the cybernetic approach, which views behavior as homeostatic and largely reflexive. The other one views behavior as a cognitive process that involves an interaction between neural events representing the previous experience, the present state of the individual, and the occurrence of particular features in the environment. This review, based on published data, presents a non-invasive psychophysiological method for investigating the electrical brain activity associated with those “silent” behaviors such as intention, evaluation of results, and memorization. Movement-related potentials (MRPs) are ideal for studying these processes. The MRPs are recorded during the execution of the skilled performance task (SPT). This task requires the execution of fast ballistic movements with the thumbs of both hands, learning a precise and short time interval between the two thumb presses, and scoring the highest number of target performances. The subject receives real-time feedback about the results of his performance. The MRPs associated with this task and present during covert behavior are the Bereitschaftspotential (BP) present before the onset of movement and the Skilled Performance Positivity (SPP) after movement, which coincides with the subject's awareness of the success or failure of his performance. These potentials show a maturational trend, reaching the adult form around the age of 10 when formal and abstract thinking progress. SPT and MRPs are particularly suitable to study neurodevelopmental disorders. Children with developmental dyslexia show abnormal MRPs, both in latency and amplitude, in different brain areas.}
}
@article{CUFFARO201235,
title = {Many worlds, the cluster-state quantum computer, and the problem of the preferred basis},
journal = {Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics},
volume = {43},
number = {1},
pages = {35-42},
year = {2012},
issn = {1355-2198},
doi = {https://doi.org/10.1016/j.shpsb.2011.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S1355219811000694},
author = {Michael E. Cuffaro},
keywords = {Quantum computation, Quantum mechanics, Many worlds, Everettian interpretation, Quantum parallelism, Quantum speed-up, Cluster state, Measurement-based, One-way, Preferred basis problem},
abstract = {I argue that the many worlds explanation of quantum computation is not licensed by, and in fact is conceptually inferior to, the many worlds interpretation of quantum mechanics from which it is derived. I argue that the many worlds explanation of quantum computation is incompatible with the recently developed cluster state model of quantum computation. Based on these considerations I conclude that we should reject the many worlds explanation of quantum computation.}
}
@article{BASHIRPOURBONAB2023104459,
title = {Urban quantum leap: A comprehensive review and analysis of quantum technologies for smart cities},
journal = {Cities},
volume = {140},
pages = {104459},
year = {2023},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2023.104459},
url = {https://www.sciencedirect.com/science/article/pii/S0264275123002718},
author = {Aysan {Bashirpour Bonab} and Maria Fedele and Vincenzo Formisano and Ihor Rudko},
keywords = {Smart city, Quantum city, Smart city technologies, Urban quantum technologies, Semi-systematic literature review, Thematic analysis},
abstract = {Contemporary smart city solutions rely on standardized von Neumann architecture, in which single data units are coded as “0” or “1.” Conversely, urban quantum technologies rely on the fundamental principles of quantum physics, transcending the conventions of the current computational paradigm. On the one hand, urban quantum technologies hold managerial relevance for future smart cities. On the other hand, they are often overlooked by smart city researchers. Accordingly, their value as a breakthrough technological paradigm is still largely unexplored. In this article, we look at how quantum technologies may contribute to existing smart city solutions, including the Internet of Things, cloud computing, big data, ICT, smart transportation, artificial intelligence, and blockchain. First, through a semi-systematic review of eighty articles on quantum computing within the social science domain, we identify two relevant classes of urban quantum technologies: quantum communication and quantum computing. Second, we establish a comprehensive taxonomy of conventional smart city solutions based on the automated content analysis of 567 abstracts of articles on the technological aspects of smart cities. Third, we investigate potential associations between two classes of technologies (conventional smart city solutions and urban quantum technologies) by analyzing the semantic relationships between eighty articles on quantum technologies according to the frequency of keywords denoting different types of conventional smart city solutions. Finally, we triangulate our findings through a thematic analysis of potential uses of quantum technologies within identified categories of smart city solutions.}
}
@article{GARBOCZI2001455,
title = {Elastic moduli of a material containing composite inclusions: effective medium theory and finite element computations},
journal = {Mechanics of Materials},
volume = {33},
number = {8},
pages = {455-470},
year = {2001},
issn = {0167-6636},
doi = {https://doi.org/10.1016/S0167-6636(01)00067-9},
url = {https://www.sciencedirect.com/science/article/pii/S0167663601000679},
author = {E.J. Garboczi and J.G. Berryman},
keywords = {Fnite element, Effective medium theory, Concrete, Microstructure, Random elastic},
abstract = {Concrete is a good example of a composite material in which the inclusions (rocks and sand) are surrounded by a thin shell of altered matrix material and embedded in the normal matrix material. Concrete, therefore, may be viewed as consisting of a matrix material containing composite inclusions. Assigning each of these phases different linear elastic moduli results in a complicated effective elastic moduli problem. A new kind of differential effective medium theory (D-EMT) is presented in this paper that is intended to address this problem. The key new idea is that each inclusion particle, surrounded by a shell of another phase, is mapped onto an effective particle of uniform elastic moduli. The resulting simpler composite, with a normal matrix, is then treated in usual D-EMT. Before use, however, the accuracy of this method must be determined, as effective medium theory of any kind is an uncertain approximation. One good way to assess the accuracy of effective medium theory is to compare to exact results for known microstructures and phase moduli. Exact results, however, only exist for certain microstructures (e.g., dilute limit of inclusions) or special choices of the moduli (e.g., equal shear moduli). Recently, a special finite element method has been developed that can compute the linear elastic moduli of an arbitrary digital image in 2D or 3D. If a random microstructure can be represented with enough resolution by a digital image, then its elastic moduli can be readily computed. This method is used, after proper error analysis, to provide stringent tests of the new D-EMT equations, which are found to compare favorably to numerically exact finite element simulations, in both 2D and 3D, with varying composite inclusion particle size distributions.}
}
@incollection{ADAMS2024593,
title = {Nature's novel materials: A review of quantum biology},
editor = {Tapash Chakraborty},
booktitle = {Encyclopedia of Condensed Matter Physics (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {593-604},
year = {2024},
isbn = {978-0-323-91408-6},
doi = {https://doi.org/10.1016/B978-0-323-90800-9.00268-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323908009002687},
author = {Betony Adams and Francesco Petruccione},
keywords = {Avian migration, Chirality, Coherent energy and charge transfer, Consciousness, DNA, Entanglement, Enzymes, Microtubules, Mitochondria, Nontrivial quantum effects, Photosynthesis, Posner molecules, Quantum biology, Quantum tunnelling, Reactive oxygen species, Receptors, Spin chemistry},
abstract = {Quantum biology is, most simply, the investigation of quantum effects in biological systems. While all matter is fundamentally quantum, quantum biology focuses on those quantum effects that result in properties of biological scale and relevance. It has long been argued that quantum effects are incompatible with biological systems, due to the likelihood of decoherence. But nature is a surprising and enterprising engineer. In this sense, quantum biology is the study of nature's materials and how they might sustain and enhance quantum effects, even in hot and wet environments. Quantum mechanics was developed in response to seemingly anomalous interactions between light and matter. It seems fitting that a great deal of the progress made in quantum biology in the last few decades has been on the topic of photosynthesis: the interaction of light with living matter. The interaction of electromagnetic radiation with matter is also at the heart of many other topics of quantum biology. This review is a broad overview of the current state of this research. While the underlying quantum effects are often similar—energy and charge transfer, tunnelling, spin dynamics—the biological contexts are varied and continue to grow. These contexts include enzyme catalysis, DNA mutation, receptor binding, photosynthesis, microtubule and mitochondrial function, magnetoreception, regulation of the production of reactive oxygen species, calcium ion storage and release and, potentially, consciousness. While there remain a number of reservations regarding quantum biology, progress made in this regard might further our understanding of both quantum and biological systems. With respect to the latter, advances in understanding biology might also contribute to a better understanding of physiology and the development of new therapeutics. But quantum biology might equally advance our understanding of quantum mechanics, by illustrating what quantum effects could look like in the novel materials out of which living systems are built.}
}
@article{TARI2023119635,
title = {Expansion-based Hill-climbing},
journal = {Information Sciences},
volume = {649},
pages = {119635},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.119635},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523012203},
author = {Sara Tari and Matthieu Basseur and Adrien Goëffon},
keywords = {Maximum expansion, Hill-climbing algorithm, Local search, Fitness landscapes, Landscape-aware heuristics, Combinatorial optimization},
abstract = {This paper investigates the influence of adaptive walks heuristics within local searches, by studying to what extent a wiser choice among improving neighbors influences the expected quality of the attained local optima. To this aim, we specifically focus on hill-climbers and introduce the maximum expansion pivoting rule, which selects the improving neighbor having the highest number of improving neighbors. Experiments show that having one step ahead of information allows for wiser neighbor choices, leading to better local optima. As the best improvement climber, a maximum expansion climber selects a particular search trajectory among all possible first improvement trajectories, however, it significantly increases the expected quality of the trajectory. On the other hand, the computational overhead makes this heuristic less valuable when included in an iterated search process, where repeating fast random hill-climbings from random starting points still allows a better exploration of the search space. This paper, therefore, extends previous studies on the relative efficiency of hill-climbing pivoting rules, by focusing on the original maximum expansion selection criterion. Results suggest that the achievement of good local optima combined with the use of approximation techniques and problem specificities could lead to the design of effective advanced metaheuristics that exploit the maximum expansion principle.}
}
@article{RICHEY2014857,
title = {A Complex Sociotechnical Systems Approach to Provisioning Educational Policies for Future Workforce},
journal = {Procedia Computer Science},
volume = {28},
pages = {857-864},
year = {2014},
note = {2014 Conference on Systems Engineering Research},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.03.102},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914001653},
author = {Michael Richey and Marcus Nance and Leroy Hanneman and William Hubbard and Azad M. Madni and Marc Spraragen},
keywords = {Socio-technical Systems, Systems Engineering Education, Engineering, Worldbuilding and Workforce Development},
abstract = {Reforming the U.S. educational system and workforce is a national challenge. Both industry leaders 30 and academics 2,28 concur that improving the quality, quantity [and alignment] of U.S STEM graduates are national imperatives. Models of the U.S. educational system, using complex sociotechnical systems’ approaches and tools that instill systems thinking, offer a holistic perspective to the educational and workforce challenges we face as a nation and allow us to identify and understand challenges associated with workforce preparedness, and increasing the number and technical excellence of STEM graduates 9,13,29. These models represent a sociotechnical system of systems with various sub-systems, each one representing an inherently complex and interdisciplinary problem of maintaining bi-directional, non-linear feedback relationships between one another. Each system involves multiple disparate stakeholders that need to collaborate within a time-and resource-intensive process while embedded in a larger sociotechnical system, aligned with the people, ideas, and support required to support desired global outcomes, of the system of systems, society and industry in particular11.}
}
@article{NIXON2024100992,
title = {Using machine learning to predict investors’ switching behaviour},
journal = {Journal of Behavioral and Experimental Finance},
volume = {44},
pages = {100992},
year = {2024},
issn = {2214-6350},
doi = {https://doi.org/10.1016/j.jbef.2024.100992},
url = {https://www.sciencedirect.com/science/article/pii/S2214635024001072},
author = {Paul Nixon and Evan Gilbert},
keywords = {Supervised machine learning, Random forest, Risk behaviour, Risk perception},
abstract = {Individual investors’ decisions to switch investments very often lead to significantly lower investment returns so having an effective predictive model of these switches would be of value to clients, advisors and investment managers. A random forest algorithm was applied to a new dataset of over 20 million observations relating to 95,685 clients on Momentum Investments’ platform between 2018 and 2024. It identified a combination of investor characteristics (number of holdings, past switching behaviour, total assets) and external features (past returns, macroeconomic variables) as the key features of investor switch behaviour. This model exceeds commercially accepted standards in respect of the AUC and Gini metrics showcasing the model’s strength in its ranking capability. It can thus provide a useful basis for client segmentation and engagement by financial advisors.}
}
@article{CHOI2023100069,
title = {Art and the artificial},
journal = {Journal of Creativity},
volume = {33},
number = {3},
pages = {100069},
year = {2023},
issn = {2713-3745},
doi = {https://doi.org/10.1016/j.yjoc.2023.100069},
url = {https://www.sciencedirect.com/science/article/pii/S2713374523000286},
author = {Suk Kyoung Choi and Steve DiPaola and Liane Gabora},
keywords = {Creativity systems, Anticipatory esthetics, AI art, Creative process, Arts-based research, Affective computing},
abstract = {This paper explores the philosophical implications of machine learning text-to-image synthesis in a practice-based phenomenology of the computational poetics of a visual art process. It is hypothesized that artificial intelligence (AI) facilitated reflective image development fosters an anticipatory esthetics in creative interactivity. The concept of AI-mediated “perspectival affordance” is introduced and its application to affective computing design emphasized. It is proposed that positioning intelligent systems as collaborative creativity tools promotes a dynamic interplay that envisions creativity as an anticipatory system, conceived of as those systems where exchange between artist and tool is mediated by future-oriented affective projection upon the system. The paper aims to establish a cognitive framework for AI-mediated creativity grounded in anticipatory interactivity, enhancing understanding of embodied cognition as mediated by AI in human-centered creativity support systems.}
}
@article{STOKLASA2021153,
title = {Possibilistic fuzzy pay-off method for real option valuation with application to research and development investment analysis},
journal = {Fuzzy Sets and Systems},
volume = {409},
pages = {153-169},
year = {2021},
note = {Games and Decision Analysis},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2020.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S0165011420302475},
author = {Jan Stoklasa and Pasi Luukka and Mikael Collan},
keywords = {Finance, Transformation, Possibility theory, Real option valuation, Fuzzy pay-off method},
abstract = {This paper presents the first fully possibilistic method for real option valuation of investment projects, a new possibilistic variant of the fuzzy pay-off method for real option valuation. The new variant is derived by using the Luukka-Stoklasa-Collan transformation and is proven to be consistent with financial theory. The new variant is comparatively analyzed with the original method and the previously presented probabilistic variant. Fast computation formulae for the new variant in all use-cases in the triangular context are presented and complete fast computation formulae also for the previously presented probabilistic variant of the method are presented for the first time. The use of the new variant is illustrated with a set of numerical examples including examples of Research and Development investment analysis.}
}
@article{VIJAYALAKSHMI2022103179,
title = {Predicting Hepatitis B to be acute or chronic in an infected person using machine learning algorithm},
journal = {Advances in Engineering Software},
volume = {172},
pages = {103179},
year = {2022},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2022.103179},
url = {https://www.sciencedirect.com/science/article/pii/S0965997822000898},
author = {C. Vijayalakshmi and S. Pakkir Mohideen},
keywords = {Hepatitis B, Machine learning, SVM, Stochastic gradient algorithm, Dataset},
abstract = {Hepatitis B is a viral infection which causes liver damage. It can lead to death. This hepatitis B along with Hepatitis C can cause hepatocellular carcinoma and liver cirrhosis. In this paper it is discussed about Hepatitis B found positive in a person's blood test is acute or chronic. This research work plans to code an endurance forecast model for the dataset which contains the boundaries or data of Hepatitis-B patients. At first the information will be pre-prepared, to improve fit for additional handling and for being in satisfactory configuration for the calculations. At that point, several calculations to indicate the forecast and draw out the precision of the model. What's more, further contrast those calculations with indicate the calculation with most adequacy. The precision is determined by contrasting the anticipated result and ongoing result of the patient. In light of thinking about different boundaries, the model will anticipate the danger of a patient of his endurance rate of acute or chronic infected person accuracy. In this paper we use Stochastic Gradient algorithm to find the Co-connection between boundaries of the date set, kernel approximation to finalise the resulting accuracy of the acute or choric prediction of patients and SVM method we use to clustering the kernel approximation calculation and connection analysis.}
}
@article{SCHIFFERKANE2024104659,
title = {Converting OMOP CDM to phenopackets: A model alignment and patient data representation evaluation},
journal = {Journal of Biomedical Informatics},
volume = {155},
pages = {104659},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104659},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000777},
author = {Kayla Schiffer-Kane and Cong Liu and Tiffany J. Callahan and Casey Ta and Jordan G. Nestor and Chunhua Weng},
keywords = {Phenopackets schema, OMOP-CDM, Health data standards, Interoperability, Phenotyping, Data model},
abstract = {Objective
This study aims to promote interoperability in precision medicine and translational research by aligning the Observational Medical Outcomes Partnership (OMOP) and Phenopackets data models. Phenopackets is an expert knowledge-driven schema designed to facilitate the storage and exchange of multimodal patient data, and support downstream analysis. The first goal of this paper is to explore model alignment by characterizing the common data models using a newly developed data transformation process and evaluation method. Second, using OMOP normalized clinical data, we evaluate the mapping of real-world patient data to Phenopackets. We evaluate the suitability of Phenopackets as a patient data representation for real-world clinical cases.
Methods
We identified mappings between OMOP and Phenopackets and applied them to a real patient dataset to assess the transformation’s success. We analyzed gaps between the models and identified key considerations for transforming data between them. Further, to improve ambiguous alignment, we incorporated Unified Medical Language System (UMLS) semantic type-based filtering to direct individual concepts to their most appropriate domain and conducted a domain-expert evaluation of the mapping’s clinical utility.
Results
The OMOP to Phenopacket transformation pipeline was executed for 1,000 Alzheimer’s disease patients and successfully mapped all required entities. However, due to missing values in OMOP for required Phenopacket attributes, 10.2 % of records were lost. The use of UMLS-semantic type filtering for ambiguous alignment of individual concepts resulted in 96 % agreement with clinical thinking, increased from 68 % when mapping exclusively by domain correspondence.
Conclusion
This study presents a pipeline to transform data from OMOP to Phenopackets. We identified considerations for the transformation to ensure data quality, handling restrictions for successful Phenopacket validation and discrepant data formats. We identified unmappable Phenopacket attributes that focus on specialty use cases, such as genomics or oncology, which OMOP does not currently support. We introduce UMLS semantic type filtering to resolve ambiguous alignment to Phenopacket entities to be most appropriate for real-world interpretation. We provide a systematic approach to align OMOP and Phenopackets schemas. Our work facilitates future use of Phenopackets in clinical applications by addressing key barriers to interoperability when deriving a Phenopacket from real-world patient data.}
}
@article{DODERO20221227,
title = {Ship design assessment through virtual prototypes},
journal = {Procedia Computer Science},
volume = {200},
pages = {1227-1236},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.323},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922003325},
author = {Matteo Dodero and Serena Bertagna and Luca Braidotti and Alberto Marinò and Vittorio Bucci},
keywords = {Ship design, early stage design, Virtual Prototype, Product Model Program},
abstract = {The traditional design process has been developed, through time, by trial and error, following an evolutive approach. By following this procedure, the design team focused its attention on only one conceptual design alternative at a time, which is perfected step by step until the expected outcome is obtained. Nevertheless nowadays, due to the high complexity of ships and increasingly stringent operational requirements, this approach appears to be obsolete in a market where cost and time reduction is a fundamental parameter. Indeed, to be competitive in the shipbuilding market, very accurate information should be available since the beginning of the process, to allow the design team a 360-degree exploration of a high number of alternatives and then identify the best design solution in no time. In this paper a new, rational, design process, necessary to raise efficiency and effectiveness of ship design, is presented. By using a multi-purpose design software, the authors were able to create a Virtual Prototype of a case study ship with ease and little training, obtaining, since early-stage design phases, some outputs of interest (such as longitudinally weight distribution of ship structures, preliminary midship section, GZ curves and powering curves) without great computational efforts. The most important benefit of using only one multipurpose software instead of multiple specific ones lies in the elimination of remarking activities for switching from one software to another, reducing loss of data’s risks during the process.}
}
@article{CELLERIER1990159,
title = {Psychology and computation: A response to Bunge},
journal = {New Ideas in Psychology},
volume = {8},
number = {2},
pages = {159-175},
year = {1990},
issn = {0732-118X},
doi = {https://doi.org/10.1016/0732-118X(90)90006-N},
url = {https://www.sciencedirect.com/science/article/pii/0732118X9090006N},
author = {G. Cellerier and J.-J. Ducret}
}
@article{GUEST1989560,
title = {An overview of vector and parallel processors in scientific computation},
journal = {Computer Physics Communications},
volume = {57},
number = {1},
pages = {560},
year = {1989},
issn = {0010-4655},
doi = {https://doi.org/10.1016/0010-4655(89)90285-3},
url = {https://www.sciencedirect.com/science/article/pii/0010465589902853},
author = {M. Guest}
}
@article{GILL2019556,
title = {Holons on the Horizon: Re-Understanding Automation and Control},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {25},
pages = {556-561},
year = {2019},
note = {19th IFAC Conference on Technology, Culture and International Stability TECIS 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.12.605},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319325261},
author = {Karamjit S Gill},
keywords = {data science, human-machine interaction, cybernetics, systems architecture, holon, symbiosis, valorisation, cultural architectures},
abstract = {In Re-Understanding Automation and Control in the era of digital automation of societal systems, we need to understand the inter-connected relations between knowledge, culture, technology and society. This in turn demands the exploration of social and cultural architectures, which facilitate them. Whilst computational model of data systems is built upon the bottom-up architecture, it is the top-down architecture of social and cultural contexts that synchronises the processing and outcomes of data systems, may they relate to organisational systems, heath and welfare systems, or institutional systems. It is this notion of the inter-dependence of the bottom-up and top-down architectures that makes us act beyond the linear gaze worldview of automation and control of production systems, and explore the multiplicity of interconnections between and across societal systems. In these horizons, we see the inter-connectedness between the unit and the whole, between the horizontal and vertical, and a symbiosis of hand and brain- an augmentation of the human and the machine. The ideas of inter-connectedness, augmentation and symbiosis lie at the core of holonic horizons. These horizons allow us to transcend the limit of the calculation and control model of automation, and enable the design of human-centred systems that valorise differences whilst utilising the richness and diversity of human-machine collaborations. When we envision these interactions and collaborations as a systems developmental process, we begin to visualise systems design from an interdependent perspective, which goes beyond the linear gaze of “utility”. The paper explores the ways holonic architectures engage us in the design process.}
}
@article{MILANEZ200793,
title = {A new method for real time computation of power quality indices based on instantaneous space phasors},
journal = {Electric Power Systems Research},
volume = {77},
number = {1},
pages = {93-98},
year = {2007},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2006.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0378779606000265},
author = {Dalgerti L. Milanez and Rade M. Ciric},
keywords = {Instantaneous space phasors, Instantaneous complex power, Buchholz–Goodhue effective apparent power, Power definitions, Unbalance, Dispersed generators},
abstract = {One of the important issues about using renewable energy is the integration of dispersed generation in the distribution networks. Previous experience has shown that the integration of dispersed generation can improve voltage profile in the network, decrease loss, etc. but can create safety and technical problems as well. This work report the application of the instantaneous space phasors and the instantaneous complex power in observing performances of the distribution networks with dispersed generators in steady state. New IEEE apparent power definition, the so-called Buchholz–Goodhue effective apparent power, as well as new proposed power quality (oscillation) index in the three-phase distribution systems with unbalanced loads and dispersed generators, are applied. Results obtained from several case studies using IEEE 34 nodes test network are presented and discussed.}
}
@article{GUERRERO2023107817,
title = {Forming We-intentions under breakdown situations in human-robot interactions},
journal = {Computer Methods and Programs in Biomedicine},
volume = {242},
pages = {107817},
year = {2023},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2023.107817},
url = {https://www.sciencedirect.com/science/article/pii/S0169260723004832},
author = {Esteban Guerrero and Maitreyee Tewari and Panu Kalmi and Helena Lindgren},
keywords = {We-intentions, Breakdown situations, Conflict of intentions, Repairing conflicts, Human-robot interaction, Answer set programming, Logic programming, Shared intentions, Social robots, Healthcare scenarios},
abstract = {Background and Objective: When agents (e.g. a person and a social robot) perform a joint activity to achieve a joint goal, they require sharing a relevant group intention, which has been defined as a We-intention. In forming We-intentions, breakdown situations due to conflicts between internal and “external” intentions are unavoidable, particularly in healthcare scenarios. To study such We-intention formation and “reparation” of conflicts, this paper has a two-fold objective: introduce a general computational mechanism allowing We-intention formation and reparation in interactions between a social robot and a person; and exemplify how the formal framework can be applied to facilitate interaction between a person and a social robot for healthcare scenarios. Method: The formal computational framework for managing We-intentions was defined in terms of Answer set programming and a Belief-Desire-Intention control loop. We exemplify the formal framework based on earlier theory-based user studies consisting of human-robot dialogue scenarios conducted in a Wizard of Oz setup, video-recorded and evaluated with 20 participants. Data was collected through semi-structured interviews, which were analyzed qualitatively using thematic analysis. N=20 participants (women n=12, men=8, age range 23-72) were part of the study. Two age groups were established for the analysis: younger participants (ages 23-40) and older participants (ages 41-72). Results: We proved four theoretical propositions, which are well-desired characteristics of any rational social robot. In our study, most participants suggested that people were the cause of breakdown situations. Over half of the young participants perceived the social robot's avoidant behavior in the scenarios. Conclusions: This work covered in depth the challenge of aligning the intentions of two agents (for example, in a person-robot interaction) when they try to achieve a joint goal. Our framework provides a novel formalization of the We-intentions theory from social science. The framework is supported by formal properties proving that our computational mechanism generates consistent potential plans. At the same time, the agent can handle incomplete and inconsistent intentions shared by another agent (for example, a person). Finally, our qualitative results suggested that this approach could provide an acceptable level of action/intention agreement generation and reparation from a person-centric perspective.}
}
@article{ALI2023e14993,
title = {Small hydropower generation using pump as turbine; a smart solution for the development of Pakistan's energy},
journal = {Heliyon},
volume = {9},
number = {4},
pages = {e14993},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e14993},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023022004},
author = {Asad Ali and Jianping Yuan and Hamza Javed and Qiaorui Si and Ibra Fall and Israel Enema Ohiemi and Fareed Konadu Osman and Rice ul Islam},
keywords = {Pump-as-turbine, Small hydropower, Renewable energy, Environment friendly, Energy resources, Energy in developing countries},
abstract = {Energy supply that is sustainable, effective, and economical has a strong association with socio-economic growth, particularly in developing countries such as Pakistan. Due to the ever-increasing gap between supply and demand, Pakistan has become an energy-deficient nation, with most people having no-to-limited access to power. Pakistan has been suffering from power shortages and an energy crisis because of its strong reliance on fossil-fuels to provide expensive electricity. Therefore, this paper offers a novel concept for developing Pakistan's energy by producing small-hydropower using Pump-As-Turbine (PAT), which is a form of Renewable-energy with lower environmental-impact and has not been used in Pakistan previously. PATs have shown several advantages over traditional hydro-turbines, such as minimum expenses, low-complexity, short delivery time, ease of spare parts, easy installation, availability in a large number of standard sizes, and massive production for broad-range of heads and flow rates. According to technical standards, any sort of pump could be used as PAT, including radial, mixed, single-stage, multi-stage etc. for power generation, which are capable of producing 5kW–1000kW of power, depending on their usage. However, Pakistan has shown little to no interest in exploring small/micro hydropower generation (PATs technology). Thus, this study offers public awareness and forward thinking regarding the use of advanced SHPs and draws the interests of legislators and different investors via solid recommendations about the cost-effective and environmental-friendly technology (PAT).}
}
@article{TUCKER199223,
title = {Deterministic and nondeterministic computation, and horn programs, on abstract data types},
journal = {The Journal of Logic Programming},
volume = {13},
number = {1},
pages = {23-55},
year = {1992},
issn = {0743-1066},
doi = {https://doi.org/10.1016/0743-1066(92)90020-4},
url = {https://www.sciencedirect.com/science/article/pii/0743106692900204},
author = {J.V. Tucker and J.I. Zucker},
abstract = {We investigate the notion of “semicomputability,” intended to generalize the notion of recursive enumerability of relations to abstract structures. Two characterizations are considered and shown to be equivalent: one in terms of “partial computable functions” (for a suitable notion of computability over abstract structures) and one in terms of definability by means of Horn programs over such structures. This leads to the formulation of a “Generalized Church-Turing Thesis” for definability of relations on abstract structures.}
}
@article{SMIRNOV20142507,
title = {Domain Ontologies Integration for Virtual Modelling and Simulation Environments},
journal = {Procedia Computer Science},
volume = {29},
pages = {2507-2514},
year = {2014},
note = {2014 International Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.05.234},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914004116},
author = {Pavel A. Smirnov and Sergey V. Kovalchuk and Alexey V. Dukhanov},
keywords = {Virtual simulation objects, semantic technologies, computational experiment, knowledge base},
abstract = {This paper presents a model of semantic ontologies integration into workflow co mposition design process via Virtual Simu lation Objects (VSO) concept and technology. Doma in knowledge distributed over open linked data sources may be usefully applied for new VSO-images design and used for organization co mputational-intensive simulation e xpe riments. In this paper we describe the VSO- architecture e xtended with novel functionality regarding integration with lin ked open data sources. We also provide a computational-scientific e xa mp le of do ma in-specific use-case offering a solution for some public-transportation domain problem.}
}
@article{TSUTSUI201856,
title = {A Bayesian network model for supporting the formation of PSS design knowledge},
journal = {Procedia CIRP},
volume = {73},
pages = {56-60},
year = {2018},
note = {10th CIRP Conference on Industrial Product-Service Systems, IPS2 2018, 29-31 May 2018, Linköping, Sweden},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118305171},
author = {Yusuke Tsutsui and Yosuke Kubota and Yoshiki Shimomura},
keywords = {Product-service systems, Design knowledge, Bayesian network},
abstract = {Recently, product-service systems (PSS) have drawn the interest of the manufacturing industry. Designing PSS to enhance the value of their core products, manufacturers should assume that their products are their strength or constraint and also derive the service solution logically. However, PSS design knowledge to determine the services suitable for manufacturers’ core products is unclear. As a result, determining a service solution that is compatible with their core products is difficult. This difficulty consequently prevents the manufacturing industry from realising high-quality PSS. To form PSS design knowledge efficiently, this study aims to support the analysis of the complicated and diverse relationships between product characteristics and service contents. Specifically, a Bayesian network model that represents the logical structure between the product characteristics and service contents common among existing PSS cases is constructed through computational learning based on statistical data on PSS cases.}
}
@article{CARLSON201888,
title = {Ghosts in machine learning for cognitive neuroscience: Moving from data to theory},
journal = {NeuroImage},
volume = {180},
pages = {88-100},
year = {2018},
note = {New advances in encoding and decoding of brain signals},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2017.08.019},
url = {https://www.sciencedirect.com/science/article/pii/S1053811917306663},
author = {Thomas Carlson and Erin Goddard and David M. Kaplan and Colin Klein and J. Brendan Ritchie},
keywords = {Multivariate pattern analysis, Brain decoding, Exploratory methods, fMRI, Magnetoencephalography},
abstract = {The application of machine learning methods to neuroimaging data has fundamentally altered the field of cognitive neuroscience. Future progress in understanding brain function using these methods will require addressing a number of key methodological and interpretive challenges. Because these challenges often remain unseen and metaphorically “haunt” our efforts to use these methods to understand the brain, we refer to them as “ghosts”. In this paper, we describe three such ghosts, situate them within a more general framework from philosophy of science, and then describe steps to address them. The first ghost arises from difficulties in determining what information machine learning classifiers use for decoding. The second ghost arises from the interplay of experimental design and the structure of information in the brain – that is, our methods embody implicit assumptions about information processing in the brain, and it is often difficult to determine if those assumptions are satisfied. The third ghost emerges from our limited ability to distinguish information that is merely decodable from the brain from information that is represented and used by the brain. Each of the three ghosts place limits on the interpretability of decoding research in cognitive neuroscience. There are no easy solutions, but facing these issues squarely will provide a clearer path to understanding the nature of representation and computation in the human brain.}
}
@article{TERAN2017384,
title = {Dynamic Profiles Using Sentiment Analysis for VAA's Recommendation Design},
journal = {Procedia Computer Science},
volume = {108},
pages = {384-393},
year = {2017},
note = {International Conference on Computational Science, ICCS 2017, 12-14 June 2017, Zurich, Switzerland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.05.265},
url = {https://www.sciencedirect.com/science/article/pii/S187705091730902X},
author = {Luis Terán and Jose Mancera},
keywords = {Voting Advice Applications, Dynamic Profiles, Recommender Systems, Decision-Making, Elections},
abstract = {In the context of elections, the Internet opens new and promising possibilities for parties and candidates looking for a better political strategy and visibility. In this way they can also organize their election campaign to gather funds, to mobilize support, and to enter into a direct dialogue with the electorate. This paper presents an ongoing research of recommender systems applied on e-government, particularly it is an extension of so-called voting advice applications (VAA’s). VAA’s are Web applications that support voters, providing relevant information on candidates and political parties by comparing their political interests with parties or candidates on different political issues. Traditional VAA’s provide recommendations of political parties and candidates focusing on static profiles of users. The goal of this work is to develop a candidate profile based on different parameters, such as the perspective of voters, social network activities, and expert opinions, to construct a more accurate dynamic profile of candidates. Understanding the elements that compose a candidate profile will help citizens in the decision-making process when facing a lack of information related to the behavior and thinking of future public authorities. At the end of this work, a fuzzy-based visualization approach for a VAA design is given using as a case study the National Elections of Ecuador in 2013.}
}
@article{MAHMOUD202263,
title = {Where to from here? On the future development of autonomous vehicles from a cognitive systems perspective},
journal = {Cognitive Systems Research},
volume = {76},
pages = {63-77},
year = {2022},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2022.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S1389041722000444},
author = {Sara Mahmoud and Erik Billing and Henrik Svensson and Serge Thill},
keywords = {Artificial cognition, Self-driving cars, Cognitive paradigms},
abstract = {Self-driving cars not only solve the problem of navigating safely from location A to location B; they also have to deal with an abundance of (sometimes unpredictable) factors, such as traffic rules, weather conditions, and interactions with humans. Over the last decades, different approaches have been proposed to design intelligent driving systems for self-driving cars that can deal with an uncontrolled environment. Some of them are derived from computationalist paradigms, formulating mathematical models that define the driving agent, while other approaches take inspiration from biological cognition. However, despite the extensive work in the field of self-driving cars, many open questions remain. Here, we discuss the different approaches for implementing driving systems for self-driving cars, as well as the computational paradigms from which they originate. In doing so, we highlight two key messages: First, further progress in the field might depend on adapting new paradigms as opposed to pushing technical innovations in those currently used. Specifically, we discuss how paradigms from cognitive systems research can be a source of inspiration for further development in modelling driving systems, highlighting emergent approaches as a possible starting point. Second, self-driving cars can themselves be considered cognitive systems in a meaningful sense, and are therefore a relevant, yet underutilized resource in the study of cognitive mechanisms. Overall, we argue for a stronger synergy between the fields of cognitive systems and self-driving vehicles.}
}
@article{BARBIAN2024122160,
title = {Flow and mass transfer prediction in anisotropic TPMS-structures as extracorporeal oxygenator membranes using reduced order modeling},
journal = {Journal of Membrane Science},
volume = {690},
pages = {122160},
year = {2024},
issn = {0376-7388},
doi = {https://doi.org/10.1016/j.memsci.2023.122160},
url = {https://www.sciencedirect.com/science/article/pii/S0376738823008165},
author = {Kai P. Barbian and Lukas T. Hirschwald and John Linkhorst and Michael Neidlin and Ulrich Steinseifer and Matthias Wessling and Bettina Wiegmann and Sebastian V. Jansen},
keywords = {Membrane oxygenator, TPMS, Reduced order modeling, Gas transfer simulation, Anisotropic},
abstract = {Currently, hollow fiber membranes are the standard technology for extracorporeal membrane oxygenators. Apart from the inevitable contact of the circulating blood with the artificial material, a suboptimal flow distribution within the oxygenator favors thrombus formation which leads to a rapid loss of gas exchange capacity. The current advancement in additive manufacturing allows the design of three-dimensional membrane-structures, based on triply periodic minimal surfaces (TPMS). One of their unique advantages is local geometry variation to manipulate the flow distribution. But how this anisotropy influences the overall device performance is non-trivial and requires numerical simulation. The aim of this study was to develop a reduced order model (ROM) that is able to efficiently predict three-dimensional flow distribution and gas transfer inside TPMS-structures. We performed a parametric study using a validated micro scale computational fluid dynamics (CFD) model. Afterwards, two different modeling approaches of Sherwood-correlations and artificial neural networks (NN) were compared to characterize flow and mass transfer from the simulated data. To create the ROM, the NN modeling strategy was then implemented into a porous medium CFD model. The developed ROM was also validated. Finally, an anisotropic TPMS-membrane-structure was compared numerically with an isotropic predicate. The NN fitting strategy showed superior accuracy over the Sherwood-correlations for characterizing mass transfer in TPMS-structures. With the ROM, the gas transfer rates of oxygen and carbon dioxide from the micro CFD model could be predicted within relative root mean squared errors (RRMSE) of 2.7% and 5.1%. The pressure drop in the experiments was predicted with an RRMSE accuracy of 7.6%. In comparison with the isotropic TPMS-structure, the anisotropic structure showed a homogenized flow distribution and increased gas transfer rates of 4% to 5% for oxygen and 2.3% to 7.4% for carbon dioxide at the simulated flow rates.}
}
@article{SMITH2020108208,
title = {Imprecise action selection in substance use disorder: Evidence for active learning impairments when solving the explore-exploit dilemma},
journal = {Drug and Alcohol Dependence},
volume = {215},
pages = {108208},
year = {2020},
issn = {0376-8716},
doi = {https://doi.org/10.1016/j.drugalcdep.2020.108208},
url = {https://www.sciencedirect.com/science/article/pii/S0376871620303732},
author = {Ryan Smith and Philipp Schwartenbeck and Jennifer L. Stewart and Rayus Kuplicki and Hamed Ekhtiari and Martin P. Paulus},
keywords = {Substance use disorders, Computational modeling, Active inference, Learning rate, Explore-exploit dilemma, Directed exploration},
abstract = {Background
Substance use disorders (SUDs) are a major public health risk. However, mechanisms accounting for continued patterns of poor choices in the face of negative life consequences remain poorly understood.
Methods
We use a computational (active inference) modeling approach, combined with multiple regression and hierarchical Bayesian group analyses, to examine how treatment-seeking individuals with one or more SUDs (alcohol, cannabis, sedatives, stimulants, hallucinogens, and/or opioids; N = 147) and healthy controls (HCs; N = 54) make choices to resolve uncertainty within a gambling task. A subset of SUDs (N = 49) and HCs (N = 51) propensity-matched on age, sex, and verbal IQ were also compared to replicate larger group findings.
Results
Results indicate that: (a) SUDs show poorer task performance than HCs (p = 0.03, Cohen’s d = 0.33), with model estimates revealing less precise action selection mechanisms (p = 0.004, d = 0.43), a lower learning rate from losses (p = 0.02, d = 0.36), and a greater learning rate from gains (p = 0.04, d = 0.31); and (b) groups do not differ significantly in goal-directed information seeking.
Conclusions
Findings suggest a pattern of inconsistent behavior in response to positive outcomes in SUDs combined with a tendency to attribute negative outcomes to chance. Specifically, individuals with SUDs fail to settle on a behavior strategy despite sufficient evidence of its success. These learning impairments could help account for difficulties in adjusting behavior and maintaining optimal decision-making during and after treatment.}
}
@article{KELLYPITOU201723,
title = {Microgrids and resilience: Using a systems approach to achieve climate adaptation and mitigation goals},
journal = {The Electricity Journal},
volume = {30},
number = {10},
pages = {23-31},
year = {2017},
issn = {1040-6190},
doi = {https://doi.org/10.1016/j.tej.2017.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S1040619017303007},
author = {Katrina M. Kelly-Pitou and Anais Ostroski and Brandon Contino and Brandon Grainger and Alexis Kwasinski and Gregory Reed},
keywords = {Sustainable energy, Microgrid, Sustainable development, Energy policy, Climate change policy, Climate change adaptation, Adaptive capacity, Ecological modernization},
abstract = {Although energy resource sustainability has been researched extensively, the understanding of how we use and interact with electricity sustainably is less understood. New electrical designs, like microgrids, provide opportunities to better address the immediate needs of electrical sustainability and urban development. This paper analyzes the role of microgrids in urban development and examines how greater systemic thinking between infrastructure planning and energy policymaking can increase a city’s resilience.}
}
@article{CARUSO1996135,
title = {Reported earliest memory age: Relationships with personality and coping variables},
journal = {Personality and Individual Differences},
volume = {21},
number = {1},
pages = {135-142},
year = {1996},
issn = {0191-8869},
doi = {https://doi.org/10.1016/0191-8869(96)00021-9},
url = {https://www.sciencedirect.com/science/article/pii/0191886996000219},
author = {John C. Caruso and Charles L. Spirrison},
abstract = {The present study examined the viability of motivated retrieval failure as a mechanism for childhood amnesia. A total of 115 undergraduates completed the Constructive Thinking Inventory, the NEO Personality Inventory, and answered questions regarding their earliest memory in order to assess the relationships between personality and coping variables and the subject's reported age of earliest memory. The personality and coping inventories were divided into four groups defined by the stated age of each subject's earliest memory. Analyses indicated that scores on two Constructive Thinking Inventory scales (Emotional Coping and Categorical Thinking) and two NEO Personality Inventory scales (Neuroticism and Openness to Experience) were significantly different between groups. These differences were consistent with the motivated retrieval failure hypothesis. Subscales of these four scales were then analyzed individually to further examine the differences between groups. Results are discussed in the context of personality assessment and the repression of early memories.}
}
@article{OUDMAN2018214,
title = {Effects of different cue types on the accuracy of primary school teachers' judgments of students' mathematical understanding},
journal = {Teaching and Teacher Education},
volume = {76},
pages = {214-226},
year = {2018},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2018.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X17302305},
author = {Sophie Oudman and Janneke {van de Pol} and Arthur Bakker and Mirjam Moerbeek and Tamara {van Gog}},
keywords = {Teacher judgment, Judgment accuracy, Cue utilization, Primary education, Mathematics education, Decimals},
abstract = {To gain insight into how teachers' judgment accuracy can be improved, we investigated effects of cue-type availability. While thinking aloud, 21 teachers judged their fourth grade students' (n = 176) decimal magnitude understanding. Sensitivity (correctly judging what students did understand) did not improve from availability of both answer cues (students' answers to prior practice problems) and student cues (knowledge of students triggered by knowing their names), and was lower when only answer cues were available, compared to only student cues. Specificity (correctly judging what students did not understand) was higher when only answer cues were available, compared to only student cues or both student and answer cues.}
}
@article{SEOW2021436,
title = {How Local and Global Metacognition Shape Mental Health},
journal = {Biological Psychiatry},
volume = {90},
number = {7},
pages = {436-446},
year = {2021},
note = {BPS 90/7Pharmacologic Prevention and Treatment of Posttraumatic Stress Disorder},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2021.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0006322321013299},
author = {Tricia X.F. Seow and Marion Rouault and Claire M. Gillan and Stephen M. Fleming},
keywords = {Confidence, Mental health, Metacognition, Self-beliefs, Self-efficacy, Transdiagnostic psychiatry},
abstract = {Metacognition is the ability to reflect on our own cognition and mental states. It is a critical aspect of human subjective experience and operates across many hierarchical levels of abstraction—encompassing local confidence in isolated decisions and global self-beliefs about our abilities and skills. Alterations in metacognition are considered foundational to neurologic and psychiatric disorders, but research has mostly focused on local metacognitive computations, missing out on the role of global aspects of metacognition. Here, we first review current behavioral and neural metrics of local metacognition that lay the foundation for this research. We then address the neurocognitive underpinnings of global metacognition uncovered by recent studies. Finally, we outline a theoretical framework in which higher hierarchical levels of metacognition may help identify the role of maladaptive metacognitive evaluation in mental health conditions, particularly when combined with transdiagnostic methods.}
}
@article{MITTAL2021102927,
title = {Modified-MaMeMi filter bank for efficient extraction of brainwaves from electroencephalograms},
journal = {Biomedical Signal Processing and Control},
volume = {69},
pages = {102927},
year = {2021},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2021.102927},
url = {https://www.sciencedirect.com/science/article/pii/S1746809421005243},
author = {Rakshit Mittal and A. Amalin Prince and Saif Nalband and Femi Robert and Agastinose Ronickom Jac Fredo},
keywords = {Electroencephalogram, Band-pass filter, Brainwaves, MaMeMi filter},
abstract = {Electroencephalography (EEG) is an important tool for characterizing the functioning of the brain. Studies based on EEG involve the extraction of different spectra from EEG signals. Traditional methods of extracting these brainwaves (commonly δ, θ, α, β, γ) from EEG signals, like impulse-response filtering or wavelet decomposition, are computationally inefficient or unsuitable for real-time implementation. The Maximum-Mean-Minimum (MaMeMi) filter is a signal processing algorithm that is computationally efficient for signal filtering. The response of the MaMeMi filter is dependent on pre-decided filter coefficients. An obstacle to its implementation is that the filter coefficients have to be tuned to the sampling frequency. We propose the Modified-MaMeMi (MoMaMeMi) filter, in which the choice of coefficients is independent from the sampling frequency. Furthermore, we develop a band-pass MoMaMeMi filter which is duplicated in a filter bank, to decompose EEG signals into five common brainwaves. We validate the efficiency of the proposed filter bank by the increase in Signal-to-Noise Ratio (SNR). The maximum average increase in SNR is 19.68 dB. To prove utility of the filter-bank, we statistically compare the values of windowed average power extracted from the MoMaMeMi-filtered signals, between seizure and non-seizure components of the EEG data-set. A significant difference between the distributions suggests utility for classification problems. Since EEG-signal processing algorithms are highly customised and not limited to the 5 common brainwaves reported in this paper, we also develop a program to determine filter parameters for extraction of unique frequency bands in a bespoke MoMaMeMi filter.}
}
@article{YEAP1988297,
title = {Towards a computational theory of cognitive maps},
journal = {Artificial Intelligence},
volume = {34},
number = {3},
pages = {297-360},
year = {1988},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(88)90064-1},
url = {https://www.sciencedirect.com/science/article/pii/0004370288900641},
author = {Wai K. Yeap},
abstract = {A computational theory of cognitive maps is developed which can explain some of the current findings about cognitive maps in the psychological literature and which provides a coherent framework for future development. The theory is tested with several computer implementations which demonstrate how the shape of the environment is computed and how one's conceptual representation of the environment is derived. We begin with the idea that the cognitive mapping process should be studied as two loosely coupled modules: The first module, known as the raw cognitive map, is computed from information made explicit in Marr's 212-D sketch and not from high-level descriptions of what we perceive. The second module, known as the full cognitive map, takes the raw cognitive map as input and produces different “abstract representations” for solving high-level spatial tasks faced by the individual.}
}
@article{KRAJNAK2021132976,
title = {Reactive islands for three degrees-of-freedom Hamiltonian systems},
journal = {Physica D: Nonlinear Phenomena},
volume = {425},
pages = {132976},
year = {2021},
issn = {0167-2789},
doi = {https://doi.org/10.1016/j.physd.2021.132976},
url = {https://www.sciencedirect.com/science/article/pii/S0167278921001330},
author = {Vladimír Krajňák and Víctor J. García-Garrido and Stephen Wiggins},
keywords = {Phase space of Hamiltonian systems, Stable and unstable manifolds, Normally hyperbolic invariant manifolds, Reactive islands, Spherinders, Lagrangian descriptors},
abstract = {We develop the geometrical, analytical, and computational framework for reactive island theory for three degrees-of-freedom time-independent Hamiltonian systems. In this setting, the dynamics occurs in a 5-dimensional energy surface in phase space and is governed by four-dimensional stable and unstable manifolds of a three-dimensional normally hyperbolic invariant sphere. The stable and unstable manifolds have the geometrical structure of spherinders and we provide the means to investigate the ways in which these spherinders and their intersections determine the dynamical evolution of trajectories. This geometrical picture is realized through the computational technique of Lagrangian descriptors. In a set of trajectories, Lagrangian descriptors allow us to identify the ones closest to a stable or unstable manifold. Using an approximation of the manifold on a surface of section we are able to calculate the flux between two regions of the energy surface.}
}
@article{STRYCKER2020e04358,
title = {K-12 art teacher technology use and preparation},
journal = {Heliyon},
volume = {6},
number = {7},
pages = {e04358},
year = {2020},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2020.e04358},
url = {https://www.sciencedirect.com/science/article/pii/S2405844020312020},
author = {Jesse Strycker},
keywords = {Applications in the subject area, Art education, Educational technology, Elementary education, Instructional technology, Post-secondary education, Secondary education, Teaching/learning strategies, Educational development, Evaluation in education, Media education, Pedagogy, Teaching research, Education},
abstract = {Largely absent from educational/instructional technology journals, this study focused on how K-12 art teachers in a southern state used technology to support teaching and learning, uses they found to be the best, and what kinds of technology training they received as part of their initial teacher preparation. Findings indicated that presentation and resource access technologies had transformed the way art teachers in the study work with students and materials. They also had little use of technology to support students with special needs and had limited technology experiences in their own training. Elementary art teachers were found to have more examples of student higher-order thinking skills promoting technology use, while secondary art teachers had more student media creation and a desire to implement digital portfolios. Additional findings and interpretations are offered.}
}
@article{CARDONAVASQUEZ2024110267,
title = {Enhancing time series aggregation for power system optimization models: Incorporating network and ramping constraints},
journal = {Electric Power Systems Research},
volume = {230},
pages = {110267},
year = {2024},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2024.110267},
url = {https://www.sciencedirect.com/science/article/pii/S037877962400155X},
author = {David Cardona-Vasquez and Thomas Klatzer and Bettina Klinz and Sonja Wogrin},
keywords = {Power systems optimization, Mathematical modeling, Dimensionality reduction, Renewable energy sources, Time series aggregation, Linear programming},
abstract = {In this paper, we extend a recently developed Basis-Oriented time series aggregation approach for aggregating input-data in power system optimization models which has proven to be exact in simple economic dispatch problems. We extend this methodology to include network and ramping constraints, for the latter, to handle temporal linking, we developed a heuristic that, in its current version, relies on the dual solution to find a partition of the input data, which is then aggregated. Our numerical results, for a 3-bus system, show that with network constraints only, we reduced the number of hours needed for an exact approximation by a factor of 1747, and a factor of 12 with network and ramping constraints. Moreover, our findings suggest that in the presence of temporal linking, aggregations of variable length must be employed to obtain an exact result (i.e., the same objective function value in the aggregated model) while maintaining the computational tractability. Our findings also imply that better performing aggregations do not necessarily correspond to commonly used lengths like days or weeks; additionally, we also prove that this input-data partition, based on the dual information, is always possible for these models independent of their size.}
}
@article{GROSS2019116125,
title = {Is perception the missing link between creativity, curiosity and schizotypy? Evidence from spontaneous eye-movements and responses to auditory oddball stimuli},
journal = {NeuroImage},
volume = {202},
pages = {116125},
year = {2019},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2019.116125},
url = {https://www.sciencedirect.com/science/article/pii/S1053811919307165},
author = {Madeleine E. Gross and Draulio B. Araujo and Claire M. Zedelius and Jonathan W. Schooler},
keywords = {Creativity, Curiosity, Schizotypy, Eye-tracking, Eye-gaze, Salience, Perception},
abstract = {What is the relationship between creativity, curiosity, and schizotypy? Schizophrenia-spectrum conditions and creativity have been linked to deficits in filtering sensory information, and curiosity is associated with information-seeking. This raises the possibility of a perception-based link between all three concepts. Here, we investigated whether the same individual differences in perceptual encoding explain variance in creativity, curiosity, and schizotypy. We administered an active auditory oddball task and a free viewing eye-tracking paradigm (N = 88). Creativity was measured with the figural portion of the Torrance Tests of Creative Thinking (TTCT) and two self-report scales. Schizotypy and curiosity were measured with self-reports. We found that creativity was associated with increased reaction time to the rare tone in the oddball task and was positively associated with the number and duration of fixations in the free viewing task. Schizotypy, on the other hand, showed a negative trend with the number and duration of fixations. Both creativity and curiosity were positively associated with explorative eye movements (unique number of regions visited) and Shannon entropy, while schizotypy was negatively associated with entropy. We further compared saliency maps finding that individuals high versus low in creativity and curiosity, respectively, exhibit differences in where they look. These findings may suggest a perception-based link between creativity and curiosity, but not schizotypy. Implications and limitations of these findings are discussed.}
}
@article{GUPTA2024114777,
title = {Replicated multistage interconnection networks: QoS evaluation for parallel and distributed computing},
journal = {Theoretical Computer Science},
volume = {1016},
pages = {114777},
year = {2024},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2024.114777},
url = {https://www.sciencedirect.com/science/article/pii/S0304397524003943},
author = {Shilpa Gupta and G.L. Pahuja},
keywords = {Sensor networks, Distributed system, Real-time data, Efficient computing, Multistage interconnection network (MIN), Shuffle exchange network (SEN), Reliability, Cost},
abstract = {Introduction to big data becomes very important with dealing in high-performance parallel distributed computing, especially in those systems where communication among a number of processors is required. The current paper takes into consideration different topologies for the Shuffle Exchange Network (SEN) in order to attain optimal data transfer among such scenarios. SEN topologies are a key feature in connecting several processors and implementing the data transfer among them where a single processor fails to handle the load. The study hereby reports in the performance, reliability, and cost analysis of these topologies. These topologies, some of which are advocated to have better performance in many studies, include replicated networks. Our research demystifies claims made in earlier papers that replicated networks have inflated reliability and higher costs due to their additional links. Researchers are therefore guided on accurate performance data that will lead them to make optimum choices of SEN topologies for targeted applications. These findings further highlight that the trade-offs between reliability and cost must be carefully considered during network design so as to arrive at better results for big data communication and computation in parallel and distributed systems. This work provides important insights into the correct evaluation of SEN topologies, helping to correct the misleading facts and to have better network selection for various real-time application realizations.}
}
@article{OSMAN2013188,
title = {21st Century Biology: An Interdisciplinary Approach of Biology, Technology, Engineering and Mathematics Education},
journal = {Procedia - Social and Behavioral Sciences},
volume = {102},
pages = {188-194},
year = {2013},
note = {6th International Forum on Engineering Education (IFEE 2012)},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2013.10.732},
url = {https://www.sciencedirect.com/science/article/pii/S1877042813042687},
author = {Kamisah Osman and Lee Chuo Hiong and Rian Vebrianto},
keywords = {interdisciplinary, BTEM (Biology, Technology, Engineering, Mathematics), inquiry-discovery, 21st century skills},
abstract = {The principal goal of interdisciplinary approach for Biology, Technology, Engineering and Mathematics (BTEM) is to cultivate scientific inquiry that requires coordination of both knowledge and skills simultaneously. The dominant activity for BTEM is inquiry-discovery on the authentic problems. This is intended to enhance the students’ abilities to construct their own knowledge through the relevant hands-on and minds-on activities. The essence of engineering is inventive problem solving. The Integration of advanced information communication technologies believed to be able to fulfill current Net Generation learning styles. Mathematics plays an important role as computational tools. The expected outcome of BTEM implementation is the inculcation of 21st century skills.}
}