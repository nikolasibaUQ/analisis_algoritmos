@article{LEESON2008630,
title = {Cognitive ability, personality, and academic performance in adolescence},
journal = {Personality and Individual Differences},
volume = {45},
number = {7},
pages = {630-635},
year = {2008},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2008.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0191886908002444},
author = {Peter Leeson and Joseph Ciarrochi and Patrick C.L. Heaven},
keywords = {Cognitive ability, Personality, Academic performance, Adolescents, Hope, Self-esteem, Attributional style, Psychometric },
abstract = {Does positive thinking predict variance in school grades over and above that predicted by cognitive ability? Six hundred and thirty nine high school students participated in a three-year longitudinal study that predicted grades using cognitive ability and three positive thinking variables – self-esteem, hope, and attributional style. Hope, positive attributional style and cognitive ability predicted higher grades, whilst self-esteem was a less consistent predictor of academic performance. Structural equation modelling revealed significant paths from cognitive ability, gender, and a second order positive thinking factor to grades. The results suggest that intelligence, gender, and positive thinking each play a unique role in predicting academic performance in youth. Some suggestions for further research are made.}
}
@article{WANG2016747,
title = {Towards felicitous decision making: An overview on challenges and trends of Big Data},
journal = {Information Sciences},
volume = {367-368},
pages = {747-765},
year = {2016},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2016.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0020025516304868},
author = {Hai Wang and Zeshui Xu and Hamido Fujita and Shousheng Liu},
keywords = {Big Data, Data deluge, Decision making, Data analysis, Data-intensive applications, Computational social science},
abstract = {The era of Big Data has arrived along with large volume, complex and growing data generated by many distinct sources. Nowadays, nearly every aspect of the modern society is impacted by Big Data, involving medical, health care, business, management and government. It has been receiving growing attention of researches from many disciplines including natural sciences, life sciences, engineering and even art & humanities. It also leads to new research paradigms and ways of thinking on the path of development. Lots of developed and under-developing tools improve our ability to make more felicitous decisions than what we have made ever before. This paper presents an overview on Big Data including four issues, namely: (i) concepts, characteristics and processing paradigms of Big Data; (ii) the state-of-the-art techniques for decision making in Big Data; (iii) felicitous decision making applications of Big Data in social science; and (iv) the current challenges of Big Data as well as possible future directions.}
}
@article{BATT2002185,
title = {Lateral thinking: 2-D interpretation of thermochronology in convergent orogenic settings},
journal = {Tectonophysics},
volume = {349},
number = {1},
pages = {185-201},
year = {2002},
note = {Low Temperature Thermochronology: From Tectonics to Landscape Evolution},
issn = {0040-1951},
doi = {https://doi.org/10.1016/S0040-1951(02)00053-7},
url = {https://www.sciencedirect.com/science/article/pii/S0040195102000537},
author = {Geoffrey E. Batt and Mark T. Brandon},
keywords = {Lateral motion, Thermochronology, Orogenic regions},
abstract = {Lateral motion of material relative to the regional thermal and kinematic frameworks is important in the interpretation of thermochronology in convergent orogens. Although cooling ages in denuded settings are commonly linked to exhumation, such data are not related to instantaneous behavior but rather to an integration of the exhumation rates experienced between the thermochronological ‘closure’ at depth and subsequent exposure at the surface. The short spatial wavelength variation of thermal structure and denudation rate typical of orogenic regions thus renders thermochronometers sensitive to lateral motion during exhumation. The significance of this lateral motion varies in proportion with closure temperature, which controls the depth at which isotopic closure occurs, and hence, the range of time and length scales over which such data integrate sample histories. Different chronometers thus vary in the fundamental aspects of the orogenic character to which they are sensitive. Isotopic systems with high closure temperature are more sensitive to exhumation paths and the variation in denudation and thermal structure across a region, while those of lower closure temperature constrain shorter-term behaviour and more local conditions. Discounting lateral motion through an orogenic region and interpreting cooling ages purely in terms of vertical exhumation can produce ambiguous results because variation in the cooling rate can result from either change in kinematics over time or the translation of samples through spatially varying conditions. Resolving this ambiguity requires explicit consideration of the physical and thermal framework experienced by samples during their exhumation. This can be best achieved through numerical simulations coupling kinematic deformation to thermal evolution. Such an approach allows the thermochronological implications of different kinematic scenarios to be tested, and thus provides an important means of assessing the contribution of lateral motion to orogenic evolution.}
}
@article{JACKSON201386,
title = {Airflow reversal and alternating corkscrew vortices in foredune wake zones during perpendicular and oblique offshore winds},
journal = {Geomorphology},
volume = {187},
pages = {86-93},
year = {2013},
issn = {0169-555X},
doi = {https://doi.org/10.1016/j.geomorph.2012.12.037},
url = {https://www.sciencedirect.com/science/article/pii/S0169555X13000081},
author = {Derek W.T. Jackson and Meiring Beyers and Irene Delgado-Fernandez and Andreas C.W. Baas and Andrew J. Cooper and Kevin Lynch},
keywords = {Computational fluid dynamics, Aeolian, Foredunes, Transport, Airflow modelling, Lee side eddies},
abstract = {On all sandy coastlines fringed by dunes, understanding localised air flow allows us to examine the potential sand transfer between the beach and dunes by wind-blown (Aeolian) action. Traditional thinking into this phenomenon had previously included only onshore winds as effective drivers of this transfer. Recent research by the authors, however, has shown that offshore air-flow too can contribute significantly, through lee-side back eddies, to the overall windblown sediment budget to coastal dunes. Under rising sea levels and increased erosion scenarios, this is an important process in any post-storm recovery of sandy beaches. Until now though, full visualisation in 3D of this newly recognised mechanism in offshore flows has not been achieved. Here, we show for the first time, this return flow eddy system using 3D computational fluid dynamics modelling, and reveal the presence of complex corkscrew vortices and other phenomena. The work highlights the importance of relatively small surface undulations in the dune crest which act to induce the spatial patterns of airflow (and transport) found on the adjacent beach.}
}
@article{RIVAS20011369,
title = {Computational identification of noncoding RNAs in E. coli by comparative genomics},
journal = {Current Biology},
volume = {11},
number = {17},
pages = {1369-1373},
year = {2001},
issn = {0960-9822},
doi = {https://doi.org/10.1016/S0960-9822(01)00401-8},
url = {https://www.sciencedirect.com/science/article/pii/S0960982201004018},
author = {Elena Rivas and Robert J. Klein and Thomas A. Jones and Sean R. Eddy},
abstract = {Some genes produce noncoding transcripts that function directly as structural, regulatory, or even catalytic RNAs 1, 2. Unlike protein-coding genes, which can be detected as open reading frames with distinctive statistical biases, noncoding RNA (ncRNA) gene sequences have no obvious inherent statistical biases [3]. Thus, genome sequence analyses reveal novel protein-coding genes, but any novel ncRNA genes remain invisible. Here, we describe a computational comparative genomic screen for ncRNA genes. The key idea is to distinguish conserved RNA secondary structures from a background of other conserved sequences using probabilistic models of expected mutational patterns in pairwise sequence alignments. We report the first whole-genome screen for ncRNA genes done with this method, in which we applied it to the “intergenic” spacers of Escherichia coli using comparative sequence data from four related bacteria. Starting from >23,000 conserved interspecies pairwise alignments, the screen predicted 275 candidate structural RNA loci. A sample of 49 candidate loci was assayed experimentally. At least 11 loci expressed small, apparently noncoding RNA transcripts of unknown function. Our computational approach may be used to discover structural ncRNA genes in any genome for which appropriate comparative genome sequence data are available.}
}
@article{ZENASNI2009353,
title = {Perception of emotion, alexithymia and creative potential},
journal = {Personality and Individual Differences},
volume = {46},
number = {3},
pages = {353-358},
year = {2009},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2008.10.030},
url = {https://www.sciencedirect.com/science/article/pii/S0191886908004108},
author = {F. Zenasni and T.I. Lubart},
keywords = {Creativity, Ability EI, Alexythimia, Emotional creativity, Divergent thinking},
abstract = {Theoretical proposals suggest that emotional intelligence (EI) may favor creativity. In the present paper, two studies are reported with French adults to examine the degree to which the ability to identify emotion is related to creative performance. This component of ability EI was hypothesized to be positively associated with a divergent thinking task involving emotional information. Contrary to our expectations, the first study (n=95) indicated that ability to identify emotions in faces and images was negatively related to idea generation ability. The second study (n=100) including a measure of alexithymia confirmed this relation. Moreover, evaluating emotional creativity, we observed a significant negative link between the ability to identify emotions and the tendency to experience emotions differently from those of others. We discuss these results suggesting an opposition between consensual/convergent thinking concerning emotions (ability EI) and divergent thinking.}
}
@article{TURNQUIST2024112726,
title = {Adaptive mesh methods on compact manifolds via Optimal Transport and Optimal Information Transport},
journal = {Journal of Computational Physics},
volume = {500},
pages = {112726},
year = {2024},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2023.112726},
url = {https://www.sciencedirect.com/science/article/pii/S0021999123008215},
author = {Axel G.R. Turnquist},
keywords = {Optimal transport, Optimal information transport, Diffeomorphic density matching, Moving mesh methods, Convergent numerical methods, Compact manifolds},
abstract = {Moving mesh methods were devised to redistribute a mesh in a smooth way, while keeping the number of vertices of the mesh and their connectivity unchanged. A fruitful theoretical point-of-view is to take such moving mesh methods and think of them as an application of the diffeomorphic density matching problem. Given two probability measures μ0 and μ1, the diffeomorphic density matching problem consists of finding a diffeomorphic pushforward map T such that T#μ0=μ1. Moving mesh methods are seen to be an instance of the diffeomorphic density matching problem by treating the probability density as the local density of nodes in the mesh. It is preferable that the restructuring of the mesh be done in a smooth way that avoids tangling the connections between nodes, which would lead to numerical instability when the mesh is used in computational applications. This then suggests that a diffeomorphic map T is desirable to avoid tangling. The first tool employed to solve the moving mesh problem between source and target probability densities on the sphere was Optimal Transport (OT). Recently Optimal Information Transport (OIT) was rigorously derived and developed allowing for the computation of a diffeomorphic mapping by simply solving a Poisson equation. Not only is the equation simpler to solve numerically in OIT, but with Optimal Transport there is no guarantee that the mapping between probability density functions defines a diffeomorphism for general 2D compact manifolds. In this manuscript, we perform a side-by-side comparison of using Optimal Transport and Optimal Information Transport on the sphere for adaptive mesh problems. We choose to perform this comparison with recently developed provably convergent solvers, but these are, of course, not the only numerical methods that may be used. We believe that Optimal Information Transport is preferable in computations due to the fact that the partial differential equation (PDE) solve step is simply a Poisson equation. For more general surfaces M, we show how the Optimal Transport and Optimal Information Transport problems can be reduced to solving on the sphere, provided that there exists a diffeomorphic mapping Φ:M→S2. This implies that the Optimal Transport problem on M with a special cost function can be solved with regularity guarantees, while computations for the problem are performed on the unit sphere.}
}
@article{ARUN2009S1115,
title = {P03-116 Damage to object oriented programming in the brain explains many of the psychopathological features of schizophrenia},
journal = {European Psychiatry},
volume = {24},
pages = {S1115},
year = {2009},
note = {17th EPA Congress - Lisbon, Portugal, January 2009, Abstract book},
issn = {0924-9338},
doi = {https://doi.org/10.1016/S0924-9338(09)71348-3},
url = {https://www.sciencedirect.com/science/article/pii/S0924933809713483},
author = {C.P. Arun},
abstract = {Introduction
Modern computers often use programs that incorporate a programming technique called Object Oriented Programming (OOP), allowing users to manipulate complex ‘computational objects’ such as menus, screen windows, etc with very little effort, say the click of a mouse. OOP deals with structures called objects and allows time and computational effort saving devices such as inheritance, polymorphism and encapsulation. We examine whether the brain itself may use OOP and if representation of objects suffers a breakdown in schizophrenia.
Review of literature
Previous models fail to provide a unifying explanation with a computational basis that could explain the psychopathology in schizophrenia. METHODS Using the object oriented programming language JavaTM we designed a system of self-objects named ‘hand’, ‘action monitor’ etc interacting with non-self objects ‘scissors’, ‘hammer’, ‘wall’, etc. In computational experiments, we allow the ‘action monitor’ to fail; the features of disparate objects are allowed to merge, some features of an object are allowed to be shared with other objects, etc.
Results
By transposing only a few lines of code, it is possible to duplicate various features of the psychopathology of schizophrenia.
Discussion
Our model can demonstrate overinclusion (overabstraction), concrete thinking (underabstraction), loss of ego boundaries (conjoining of disparate objects), delusions (misattribution of object function), lack of insight (poor monitoring of object activity) and passivity (loss of monitoring and misattribution of object activity).
Conclusion
The brain must use the OOP model in its computations. Failure of object representation and manipulation must lie at the core of the psychopathology of schizophrenia.}
}
@article{ZHU2023126915,
title = {SPAR: An efficient self-attention network using Switching Partition Strategy for skeleton-based action recognition},
journal = {Neurocomputing},
volume = {562},
pages = {126915},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126915},
url = {https://www.sciencedirect.com/science/article/pii/S092523122301038X},
author = {ZiJie Zhu and RenDong Ying and Fei Wen and PeiLin Liu},
keywords = {Action recognition, Self-attention, 3D-skeleton, Graph convolutional networks},
abstract = {Graph convolutional networks (GCN) have become the mainstream in skeleton-based action recognition. For further performance improvement, existing methods propose to utilize self-attention to model long-range features of joints. However, these methods cannot balance accuracy with computational efficiency. In this paper, we propose the Switching Partition Strategy (SPAR) Network that uses the self-attention mechanism for the simultaneous and efficient extraction of spatial–temporal long-range information from the skeleton. We design two partition strategies that reduce the computational cost and improve the efficiency of the computation of self-attention. Extensive experiments are conducted on two large-scale datasets, i.e. NTU RGB+D 60 and NTU RGB+D 120, to evaluate the performance of the proposed SPAR network. The results demonstrate that our method outperforms the state-of-the-art on accuracy as well as computational cost.}
}
@incollection{WARE2021425,
title = {Chapter Twelve - Designing Cognitively Efficient Visualizations},
editor = {Colin Ware},
booktitle = {Information Visualization (Fourth Edition)},
publisher = {Morgan Kaufmann},
edition = {Fourth Edition},
pages = {425-456},
year = {2021},
series = {Interactive Technologies},
isbn = {978-0-12-812875-6},
doi = {https://doi.org/10.1016/B978-0-12-812875-6.00012-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128128756000128},
author = {Colin Ware},
keywords = {Interactive visualization design, Visual thinking, Visual thinking design patterns, Visual working memory, Visualization design, Visualization types},
abstract = {A design methodology for producing cognitively efficient visualizations is introduced. The method involves seven steps (1) a high-level cognitive task description; (2) a data inventory; (3) cognitive task refinement; (4) identification of appropriate visualization types; (5) applying visual thinking design patterns (VTDPs); (6) prototype development; and (7) evaluation and design refinement. Most of the chapter is devoted to a set of VTDPs. These are descriptions of interactive visualization methods that have demonstrated value, together with a description of perceptual and cognitive issues relating to their use and guidelines for applicability. VDTPs provide a method for taking into account perceptual and cognitive issues in designing interaction especially with respect to key bottlenecks in the visual thinking process, such as limited visual working memory capacity. They also provide a way of reasoning about semiotic issues in perceptual terms via the concept of the visual query.}
}
@article{NEWMAN20031668,
title = {Frontal and parietal participation in problem solving in the Tower of London: fMRI and computational modeling of planning and high-level perception},
journal = {Neuropsychologia},
volume = {41},
number = {12},
pages = {1668-1682},
year = {2003},
issn = {0028-3932},
doi = {https://doi.org/10.1016/S0028-3932(03)00091-5},
url = {https://www.sciencedirect.com/science/article/pii/S0028393203000915},
author = {Sharlene D Newman and Patricia A Carpenter and Sashank Varma and Marcel Adam Just},
keywords = {Planning, fMRI, Spatial working memory, Problem solving, Tower of London, Computational modeling, 4CAPS},
abstract = {This study triangulates executive planning and visuo-spatial reasoning in the context of the Tower of London (TOL) task by using a variety of methodological approaches. These approaches include functional magnetic resonance imaging (fMRI), functional connectivity analysis, individual difference analysis, and computational modeling. A graded fMRI paradigm compared the brain activation during the solution of problems with varying path lengths: easy (1 and 2 moves), moderate (3 and 4 moves) and difficult (5 and 6 moves). There were three central findings regarding the prefrontal cortex: (1) while both the left and right prefrontal cortices were equally involved during the solution of moderate and difficult problems, the activation on the right was differentially attenuated during the solution of the easy problems; (2) the activation observed in the right prefrontal cortex was highly correlated with individual differences in working memory (measured independently by the reading span task); and (3) different patterns of functional connectivity were observed in the left and right prefrontal cortices. Results obtained from the superior parietal region also revealed left/right differences; only the left superior parietal region revealed an effect of difficulty. These fMRI results converged upon two hypotheses: (1) the right prefrontal area may be more involved in the generation of a plan, whereas the left prefrontal area may be more involved in plan execution; and (2) the right superior parietal region is more involved in attention processes while the left homologue is more of a visuo-spatial workspace. A 4CAPS computational model of the cognitive processes and brain activation in the TOL task integrated these hypothesized mechanisms, and provided a reasonably good fit to the observed behavioral and brain activation data. The multiple research approaches presented here converge on a deepening understanding of the combination of perceptual and conceptual processes in this type of visual problem solving.}
}
@article{ROSS2021100069,
title = {Kinenoetic analysis: Unveiling the material traces of insight},
journal = {Methods in Psychology},
volume = {5},
pages = {100069},
year = {2021},
issn = {2590-2601},
doi = {https://doi.org/10.1016/j.metip.2021.100069},
url = {https://www.sciencedirect.com/science/article/pii/S2590260121000266},
author = {Wendy Ross and Frédéric Vallée-Tourangeau},
keywords = {Insight, Case study, Observation},
abstract = {Research on insight problem solving sets itself a challenging goal: How to explain the origin of a new idea. It compounds the difficulty of this challenge by traditionally seeking to explain the phenomenon in strictly mental terms. Rather, we suggest that thoughts and actions are bound to objects, inviting a granular description of the world within which thinking proceeds. As the reasoner transforms the world, the physical traces of these changes can be mapped in space and time. Not only can the reasoner see these changes, and act upon them, the researcher can develop new inscription devices that captures the trajectory of the creative arc along spatial and temporal coordinates. Kinenoetic is a term we employ to capture the idea that knowledge comes from the movement of objects and that this knowledge is both at the level of the problem-solver and at the level of the researcher. This form of knowledge can only be constructed in problem solving environments where reasoners can manipulate physical elements. A kinenoetic analysis tracks and maps the changes to the object-qua-models of proto solutions, and in the process unveils the physical genesis of new ideas and creativity. Our aim here is to lay out a method for using the objects commonly employed in interactive problem-solving research, tracing the process of thought to elucidate underlying cognitive mechanisms. Thus, the focus turns from the effects of objects on thoughts, to tracing object-thought mutualities as they are enacted and made visible.}
}
@article{HAWES201560,
title = {Effects of mental rotation training on children’s spatial and mathematics performance: A randomized controlled study},
journal = {Trends in Neuroscience and Education},
volume = {4},
number = {3},
pages = {60-68},
year = {2015},
issn = {2211-9493},
doi = {https://doi.org/10.1016/j.tine.2015.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S2211949315000083},
author = {Zachary Hawes and Joan Moss and Beverly Caswell and Daniel Poliszczuk},
keywords = {Spatial thinking, Mental rotation, Spatial training, Computerized cognitive training, Mathematics education, STEM},
abstract = {The purpose of the current study was to (i) investigate the malleability of children’s spatial thinking, and (ii) the extent to which training-related gains in spatial thinking generalize to mathematics performance. Sixty-one 6- to 8-year-olds were randomly assigned to either computerized mental rotation training or literacy training. Training took place on iPad devices over a 6-week period as part of regular classroom activity. Results revealed that in comparison to the control group, children who received spatial training demonstrated significant gains on two measures of mental rotation and marginally significant improvements on an untrained mental transformation task; a finding that suggests that training may have had a general effect on children’s spatial ability. However, contrary to theoretical claims and prior empirical findings, there was no evidence that spatial training transferred to mathematics performance.}
}
@article{GRIFFITHS2020873,
title = {Understanding Human Intelligence through Human Limitations},
journal = {Trends in Cognitive Sciences},
volume = {24},
number = {11},
pages = {873-883},
year = {2020},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2020.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1364661320302151},
author = {Thomas L. Griffiths},
keywords = {artificial intelligence, inductive bias, meta-learning, rational meta-reasoning, cultural evolution},
abstract = {Recent progress in artificial intelligence provides the opportunity to ask the question of what is unique about human intelligence, but with a new comparison class. I argue that we can understand human intelligence, and the ways in which it may differ from artificial intelligence, by considering the characteristics of the kind of computational problems that human minds have to solve. I claim that these problems acquire their structure from three fundamental limitations that apply to human beings: limited time, limited computation, and limited communication. From these limitations we can derive many of the properties we associate with human intelligence, such as rapid learning, the ability to break down problems into parts, and the capacity for cumulative cultural evolution.}
}
@article{LOPEZORTEGA20133459,
title = {Computer-assisted creativity: Emulation of cognitive processes on a multi-agent system},
journal = {Expert Systems with Applications},
volume = {40},
number = {9},
pages = {3459-3470},
year = {2013},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2012.12.054},
url = {https://www.sciencedirect.com/science/article/pii/S095741741201295X},
author = {Omar López-Ortega},
keywords = {Computer-assisted creativity, Cognitive processes, Agent-oriented programming, Recursive systems},
abstract = {For creativity to be computed, it is paramount to understand the cognitive processes involved, which have been elucidated by either surveying creative people or discovering regions of the human brain that activate during creative endeavors. From this scattering, the author proposes a holistic framework to describe them and their interaction. Hence, creativity can be regarded as a meta process which coordinates autonomous cognitive processes such as planning or divergent thinking. To represent the interplay of cognitive processes around creativity, models are developed in the Agent Unified Modeling Language (AUML). Then, the execution of each process is delegated to autonomous agents and a global coordination protocol is devised. The implementation of the MAS is done on the JADE platform. Two modules of the resultant system are exemplified: opus planning and divergent exploration. The coordination protocol is also presented. The domain in which the software system is tested is the creation of musical pieces.}
}
@article{LI201985,
title = {Government accounting optimization based on computational linguistics},
journal = {Cognitive Systems Research},
volume = {57},
pages = {85-91},
year = {2019},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2018.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S1389041718304650},
author = {Jiyou Li},
keywords = {Computational linguistics, Accounting optimization, Research},
abstract = {The level of moral development and moral intensity in cognitive psychology will not only affect the ethical behavior of accountants, but also have a direct impact on the quality and level of accounting work. Therefore, in this paper, the ethical behavior of accountants was analyzed from the perspective of cognitive psychology. Computer-aided data mining techniques were introduced, and government accounting risk assessment management of financial accountants was studied. In this paper, the principle of cognitive psychology to measure the ethical level of accountants was first described. The predicament of moral judgments was analyzed and an optimization plan to improve the ethical intention of accountants was proposed. Support Vector Machine classification technology in data mining was studied to explore how to conduct effective and reliable evaluation, so as to provide a scientific basis for decision-making in improving accounting management. After the simulation experiment, it is proved that continuously improving the ethical standards of accountants and strengthening the forecast of accounting risks can continue to optimize the accounting office management.}
}
@article{LYU2023366,
title = {Application of Deep Learning in the Search and a Certain Celestial Body},
journal = {Procedia Computer Science},
volume = {228},
pages = {366-372},
year = {2023},
note = {3rd International Conference on Machine Learning and Big Data Analytics for IoT Security and Privacy},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.11.042},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923018665},
author = {Yang Lyu and Donglin Su},
keywords = {Deep Learning, Search Analysis, Research Applications, a Certain Celestial Body},
abstract = {With the development of the times, celestial body search has become increasingly common, and people want to enhance their understanding of the universe through further search of celestial bodies. Nowadays, many software and hardware have been invented to assist in celestial body search, but the computational efficiency and efficiency of current software are still insufficient. So this article focuses on the research and application of Deep Learning (DL) in the search and analysis of "a certain celestial body", aiming to improve celestial search through DL. Through experiments, this article uses DL to achieve a maximum computational rate of 78% and a minimum of 70% for celestial search. The computational efficiency of celestial search without DL can reach up to 68% and 57%, respectively. After using DL, the efficiency of celestial search reaches up to 82% and 70%, while before using DL, the efficiency of celestial search reaches up to 62% and 50%, respectively. From this data, it can be seen that DL can achieve good results in celestial search.}
}
@article{XU2024473,
title = {Review of the continuous catalytic ortho-para hydrogen conversion technology for hydrogen liquefaction},
journal = {International Journal of Hydrogen Energy},
volume = {62},
pages = {473-487},
year = {2024},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2024.03.085},
url = {https://www.sciencedirect.com/science/article/pii/S0360319924009170},
author = {Pan Xu and Jian Wen and Ke Li and Simin Wang and Yanzhong Li},
keywords = {Hydrogen liquefaction, Continuous catalytic ortho-para hydrogen conversion technology, Ortho-para hydrogen conversion catalyst, Packing layer, Plate-fin heat exchanger},
abstract = {In order to meet rapidly growing demand of liquid hydrogen in the future hydrogen industry and energy structure, the continuous catalytic ortho-para hydrogen conversion technology (CCOPHCT) has been once again proposed and has become an important choice to improve the hydrogen liquefaction units (HLUs). The origin, concept, and research progress of the CCOPHCT are systematically reviewed for the first time in this paper. However, the research depth and breadth of the CCOPHCT are insufficient to support its current application. To solve it, for the continuous catalytic ortho-para hydrogen conversion plate-fin heat exchanger (CCOPHC-PFHE) with better comprehensive performances, this paper comprehensively summarizes the research achievements in the related fields from the perspective of the unit analysis, including the ortho-para hydrogen conversion (OPHC), packing layer and plate-fin heat exchanger (PFHE), which to provide further thinking for the development of the CCOPHC-PFHE. Further, some suggestions for the CCOPHCT are proposed based on the existed research foundations, including preparing the effective ortho-para hydrogen conversion catalyst (OPHCC), developing the accurate OPHC dynamical model, revealing the coupling mechanism in the packing layer filled with the OPHCC and establishing an effective design method and standard of the CCOPHC-PFHE. In addition, considering the special conditions on the CCOPHC-PFHE, the importance of the experimental research is emphasized. And based on the established hydrogen experimental platform with the comprehensive supporting implementations, the experimental device of the CCOPHC-PFHE has been completed and a series of experimental tests are currently in progressing.}
}
@article{YANG202075,
title = {Local temporal-spatial multi-granularity learning for sequential three-way granular computing},
journal = {Information Sciences},
volume = {541},
pages = {75-97},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.06.020},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520306009},
author = {Xin Yang and Yingying Zhang and Hamido Fujita and Dun Liu and Tianrui Li},
keywords = {Three-way granular computing, Sequential three-way decision, Local neighborhood, Temporal-spatial, Multi-granularity},
abstract = {Based on multiple levels of granularity, the notion of sequential three-way granular computing focuses on a multiple stages of thinking, problem-solving, and information processing in threes. This paper interprets, represents, and implements sequential three-way granular computing by a framework of temporal-spatial multi-granularity learning, which is described with the temporality of data and the spatiality of parameters. In real-world decision-making, such a sequential approach is useful to make faster decisions for some objects with the lower cost of decision process and the acceptable accuracy when information is insufficient or unavailable. However, the cost of time-consuming computation for hierarchical multilevel granularity is our concern. To address this issue, we utilize a local strategy to accelerate a sequence of neighborhood-based granulation induced by Gaussian kernel function. Subsequently, local three-way decision rules are investigated based on the Bayesian minimum risk criterion. Moreover, by the construction of a novel local trisection model, we propose a local sequential approach of three-way granular computing under a temporal-spatial multilevel granular structure. Finally, a series of comparative experiments between global and local perspectives is carried out to verify the effectiveness of our proposed models.}
}
@article{CAO2024101200,
title = {Explanatory models in neuroscience, Part 2: Functional intelligibility and the contravariance principle},
journal = {Cognitive Systems Research},
volume = {85},
pages = {101200},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.101200},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723001341},
author = {Rosa Cao and Daniel Yamins},
keywords = {Evolution, Contravariance, Intelligibility, Function, Optimization, No-miracles, Instrumentalism, Realism, Philosophy, Constraints, Evolutionary landscape, Models, Explanation, Evo-devo, Development, Learning, Deep learning, Abstraction},
abstract = {Computational modeling plays an increasingly important role in neuroscience, highlighting the philosophical question of how computational models explain. In the particular case of neural network models, concerns have been raised about their intelligibility, and how these models relate (if at all) to what is found in the brain. We claim that what makes a system intelligible is an understanding of the dependencies between its behavior and the factors that are responsible for that behavior. In biology, many of these dependencies are naturally “top-down”, as ethological imperatives interact with evolutionary and developmental constraints under natural selection to produce systems with capabilities and behaviors appropriate to their evolutionary needs. We describe how the optimization techniques used to construct neural network models capture some key aspects of these dependencies, and thus help explain why brain systems are as they are — because when a challenging ecologically-relevant goal is shared by a neural network and the brain, it places constraints on the possible mechanisms exhibited in both kinds of systems. The presence and strength of these constraints explain why some outcomes are more likely than others. By combining two familiar modes of explanation — one based on bottom-up mechanistic description (whose relation to neural network models we address in a companion paper) and the other based on top-down constraints, these models have the potential to illuminate brain function.}
}
@article{MUST20167,
title = {Predicting the Flynn Effect through word abstractness : Results from the National Intelligence Tests support Flynn's explanation},
journal = {Intelligence},
volume = {57},
pages = {7-14},
year = {2016},
issn = {0160-2896},
doi = {https://doi.org/10.1016/j.intell.2016.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0160289616300253},
author = {Olev Must and Aasa Must and Jaan Mikk},
keywords = {Flynn Effect, National Intelligence Tests, Abstract thinking, Guessing, Tork, Estonia},
abstract = {The current study investigates the Flynn Effect (FE) and its relation to abstract thinking ability. We compare two cohorts of Estonian students (1933/36, n=888; 2006, n=912) using the Concepts (Logical Selection) subtest of the Estonian adaptation of the National Intelligence Tests (NIT). The item presentation order of the subtest correlates with the abstractness of the words used in the items (r=.609) of the subtest. The different test results (right, wrong and missing answers) were analysed in order to make an estimate of the FE magnitude. The FE for abstract thinking ability of those samples was 1.06 Hedges' g (adjusted for guessing). The magnitude of the FE is dependent upon the degree of difficulty of the items (an item's difficulty is estimated by determining its abstractness and its familiarity to students). The more difficult part of the subtest (the second half) showed a FE=1.80 whereas the easier part (the first half) of the subtest showed a FE=.72. Word abstractness was a strong predictor of all the testing results in both cohorts (Beta=.700). The familiarity of words used in the test items has no correlation with the test results if word abstractness is controlled in both cohorts. Our findings support Flynn's explanation that the FE is primarily an indicator of the rise in abstract thinking ability.}
}
@article{SUDHESHWAR2024108305,
title = {Learning from Safe-by-Design for Safe-and-Sustainable-by-Design: Mapping the current landscape of Safe-by-Design reviews, case studies, and frameworks},
journal = {Environment International},
volume = {183},
pages = {108305},
year = {2024},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2023.108305},
url = {https://www.sciencedirect.com/science/article/pii/S0160412023005780},
author = {Akshat Sudheshwar and Christina Apel and Klaus Kümmerer and Zhanyun Wang and Lya G. Soeteman-Hernández and Eugenia Valsami-Jones and Claudia Som and Bernd Nowack},
keywords = {Safe-by-Design (SbD), Safe and Sustainable-by-Design (SSbD), Literature mapping, SSbD implementation},
abstract = {With the introduction of the European Commission's “Safe and Sustainable-by-Design” (SSbD) framework, the interest in understanding the implications of safety and sustainability assessments of chemicals, materials, and processes at early-innovation stages has skyrocketed. Our study focuses on the “Safe-by-Design” (SbD) approach from the nanomaterials sector, which predates the SSbD framework. In this assessment, SbD studies have been compiled and categorized into reviews, case studies, and frameworks. Reviews of SbD tools have been further classified as quantitative, qualitative, or toolboxes and repositories. We assessed the SbD case studies and classified them into three categories: safe(r)-by-modeling, safe(r)-by-selection, or safe(r)-by-redesign. This classification enabled us to understand past SbD work and subsequently use it to define future SSbD work so as to avoid confusion and possibilities of “SSbD-washing” (similar to greenwashing). Finally, the preexisting SbD frameworks have been studied and contextualized against the SSbD framework. Several key recommendations for SSbD based on our analysis can be made. Knowledge gained from existing approaches such as SbD, green and sustainable chemistry, and benign-by-design approaches needs to be preserved and effectively transferred to SSbD. Better incorporation of chemical and material functionality into the SSbD framework is required. The concept of lifecycle thinking and the stage-gate innovation model need to be reconciled for SSbD. The development of high-throughput screening models is critical for the operationalization of SSbD. We conclude that the rapid pace of both SbD and SSbD development necessitates a regular mapping of the newly published literature that is relevant to this field.}
}
@article{RONAYNE2021318,
title = {Evaluating the sunk cost effect},
journal = {Journal of Economic Behavior & Organization},
volume = {186},
pages = {318-327},
year = {2021},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2021.03.029},
url = {https://www.sciencedirect.com/science/article/pii/S0167268121001293},
author = {David Ronayne and Daniel Sgroi and Anthony Tuckwell},
keywords = {Sunk cost effect, Sunk cost fallacy, Endowment effect, Cognitive ability, Psychological scales, Scale validation},
abstract = {We provide experimental evidence of behavior consistent with the sunk cost effect. Subjects who earned a lottery via a real-effort task were given an opportunity to switch to a dominant lottery; 23% chose to stick with their dominated lottery. The endowment effect accounts for roughly only one third of the effect. Subjects’ capacity for cognitive reflection is a significant determinant of sunk cost behavior. We also find stocks of knowledge or experience (crystallized intelligence) predict sunk cost behavior, rather than algorithmic thinking (fluid intelligence) or the personality trait of openness. We construct and validate a scale, the “SCE-8”, which encompasses many resources individuals can spend, and offers researchers an efficient way to measure susceptibility to the sunk cost effect.}
}
@incollection{HERLIHY201469,
title = {Chapter 4 - Colorless Wait-Free Computation},
editor = {Maurice Herlihy and Dmitry Kozlov and Sergio Rajsbaum},
booktitle = {Distributed Computing Through Combinatorial Topology},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {69-95},
year = {2014},
isbn = {978-0-12-404578-1},
doi = {https://doi.org/10.1016/B978-0-12-404578-1.00004-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780124045781000048},
author = {Maurice Herlihy and Dmitry Kozlov and Sergio Rajsbaum},
keywords = {Configurations, Executions, Layered executions, Layered protocols, Processes, Protocols, Schedules, Tasks},
abstract = {We outline the basic connection between distributed computing and combinatorial topology in terms of two formal models: a conventional operational model, in which systems consist of communicating state machines whose behaviors unfold over time, and the combinatorial model, in which all possible behaviors are captured statically using topological notions. We start with one particular system model (shared memory) and focus on a restricted (but important) class of problems (so-called “colorless” tasks).}
}
@article{OMHOLT2003107,
title = {Eberhard O. Voit, Computational Analysis of Biochemical Systems. A Practical Guide for Biochemists and Molecular Biologists, Cambridge University Press, 2000, 531 pages (ISBN 0-521-78579-0; paperback)},
journal = {Mathematical Biosciences},
volume = {181},
number = {1},
pages = {107-109},
year = {2003},
issn = {0025-5564},
doi = {https://doi.org/10.1016/S0025-5564(02)00153-0},
url = {https://www.sciencedirect.com/science/article/pii/S0025556402001530},
author = {Stig W Omholt}
}
@article{GAN2024102786,
title = {Large models for intelligent transportation systems and autonomous vehicles: A survey},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102786},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102786},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624004348},
author = {Lu Gan and Wenbo Chu and Guofa Li and Xiaolin Tang and Keqiang Li},
keywords = {Intelligent transportation systems, Autonomous vehicles, Survey, Large models, Efficient deployment techniques},
abstract = {Large models are widely used in intelligent transportation systems (ITS) and autonomous vehicles (AV) due to their excellent new capabilities such as intelligence emergence, domain application adaptive, and multi-task learning. By integrating massive amounts of data, vehicles and transportation systems based on large models can understand the real-world environment, simulate the reasoning process of human drivers, optimize traffic flow, and improve driving safety and efficiency. However, in the practical implementation of large models, there are three key research questions: (1) How can model reasoning be consistent with the target task? (2) How to improve the redundancy of structures and weights caused by complex models? (3) How to solve the problems of supercomputing power, high latency, and large throughput of large models in practical deployment? These challenges have stimulated the development of deployment techniques for large models. Although existing review articles discuss large model technologies from singular or partial perspectives, there remains a lack of comprehensive systematic investigations into the application and deployment of large models for ITS and AV. Therefore, this paper conducted a quantitative analysis of scientific literature, demonstrating the necessity and significance of studying large models for ITS and AV. Subsequently, it outlined the concept and characteristics of large models and provided a detailed summary of the frontier progress of large models for ITS and AV. Moreover, to bridge the gap between large model inference and target tasks, address structural and weight redundancies, and tackle challenges in practical deployment such as high computational power, latency, and throughput, it explored efficient deployment techniques to accelerate the rapid deployment of large models. Finally, it discussed the challenges and future trends. This paper aims to provide researchers and engineers with an understanding of the forefront advancements and future trends of large models to facilitate the rapid implementation of large models and accelerate their development in ITS and AV.}
}
@article{WILLIAMS2023145,
title = {Stabilizing expectations when shifting from analytical to intuitive reasoning: The role of prediction errors in reasoning},
journal = {Cortex},
volume = {161},
pages = {145-153},
year = {2023},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2023.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0010945223000412},
author = {Chad C. Williams and Cameron D. Hassall and Olave E. Krigolson},
keywords = {Reasoning, Prediction errors, Cognitive control, Theta, EEG},
abstract = {As humans, we rely on intuitive reasoning for most of our decisions. However, when there is a novel or atypical decision to be made, we must rely on a slower and more deliberative thought process—analytical reasoning. As we gain experience with these novel or atypical decisions, our reasoning shifts from analytical to intuitive, which parallels a reduction in the need for cognitive control. Here, we sought to confirm this claim by employing electroencephalographic (EEG) measures of cognitive control as participants performed a simple perceptual decision-making task. Specifically, we had participants categorize “blobs” into families based on their visual attributes so we could examine how their reasoning changed with learning. In a key manipulation, halfway through the experiment we introduced novel blob families to categorize, thus temporarily increasing the need for analytical reasoning (i.e., cognitive control). Congruent with past research, we focused our EEG analyses on frontal theta activity as it has been linked to cognitive control and analytical thinking. As hypothesized, we found a transition from analytical to intuitive decision-making systems with learning as indexed by a decrease in frontal theta power. Further, when the novel blobs were introduced at the midpoint of the experiment, we found that decisions about these stimuli recruited analytical reasoning as indicated by increased theta power in comparison to decisions about well-practiced stimuli. We propose our findings to reflect prediction errors to decision demands—a monitoring process that determines whether our expectations of demands are met. Shifting from analytical to intuitive reasoning thus reflects the stabilization of our expectations of decision demands, which can be violated with unexpected demands when encountering novel stimuli.}
}
@article{TAY20211,
title = {Modelability across time as a signature of identity construction on YouTube},
journal = {Journal of Pragmatics},
volume = {182},
pages = {1-15},
year = {2021},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2021.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0378216621002307},
author = {Dennis Tay},
keywords = {Identity construction, Social media, LIWC, Modelability, ARIMA, Time series analysis},
abstract = {Linguistic self-representation and identity construction on social media have attracted much scholarly attention. However, relevant studies tend to overlook the temporal dimension of social media, potentially systematic patterning of linguistic behavior across time, and the attendant implications of such a temporal perspective on identity. Combining an automated lexical tool (LIWC) and the Box–Jenkins method of statistical time series analysis, this paper shows how the ‘modelability’ of linguistic choices across time can be interpreted as signatures of identity construction and complement existing frameworks for identity analysis. Two levels of modelability are discussed—the availability of a well-fitting time series model as evidence of temporal patterning, and specific parameters of that model interpreted in context. These are demonstrated with a case study of the construction of ‘amateur expertise’ over 109 consecutive makeup tutorial videos on the popular YouTube channel ‘Nikkiestutorials’. Results show that the linguistic display of ‘analytical thinking’ reflects a strategy of ‘short term momentum’, the display of ‘clout’ and ‘authenticity’ a strategy of ‘short term restoration’, while the display of ‘emotional tone’ fluctuates randomly across time. The approach is further discussed in terms of its general principles and potential applications in other contexts of identity and related research.}
}
@article{ANDERSON2023102027,
title = {Nurse scholars of the Robert Wood Johnson Foundation Harold Amos Medical Faculty Development Program},
journal = {Nursing Outlook},
volume = {71},
number = {5},
pages = {102027},
year = {2023},
issn = {0029-6554},
doi = {https://doi.org/10.1016/j.outlook.2023.102027},
url = {https://www.sciencedirect.com/science/article/pii/S002965542300132X},
author = {Cindy M. Anderson and Nina Ardery and Daniel Pesut and Carmen Alvarez and Tamryn F. Gray and Karen M. Rose and Jasmine L. Travers and Janiece Taylor and Kathy D. Wright},
keywords = {Health professions diversity, Equity, Inclusive excellence, Robert Wood Johnson Foundation, Nurse faculty scholars, Harold Amos, Medical faculty development, Legacy leadership},
abstract = {Background
The challenge to increase the diversity, inclusivity, and equity of nurse scientists is a critical issue to enhance nursing knowledge development, health care, health equity, and health outcomes in the United States.
Purpose
The purpose of this paper is to highlight the current nurse scholars in the Robert Wood Johnson Foundation (RWJF) Harold Amos Medical Faculty Development Program (AMFDP).
Discussion
Profiles and the programs of research and scholarship of the current AMFDP nurse scholars are described and discussed. Scholars share lessons learned, and how the AMFDP program has influenced their thinking and commitments to future action in service of nursing science, diversity efforts, legacy leadership, issues of health equity.
Conclusion
RWJF has a history of supporting the development of nursing scholars. AMFDP is an example of legacy leadership program that contributes to a culture of health and the development of next-generation nursing science scholars.}
}
@article{CROSSLEY2024100865,
title = {A large-scale corpus for assessing written argumentation: PERSUADE 2.0},
journal = {Assessing Writing},
volume = {61},
pages = {100865},
year = {2024},
issn = {1075-2935},
doi = {https://doi.org/10.1016/j.asw.2024.100865},
url = {https://www.sciencedirect.com/science/article/pii/S1075293524000588},
author = {S.A. Crossley and Y. Tian and P. Baffour and A. Franklin and M. Benner and U. Boser},
keywords = {Corpus linguistics, Writing assessment, Argumentation, Individual differences},
abstract = {This research methods article introduces the open source PERSUADE 2.0 corpus. The PERSUADE 2.0 corpus comprises over 25,000 argumentative essays produced by 6th-12th grade students in the United States for 15 prompts on two writing tasks: independent and source-based writing. The PERSUADE 2.0 corpus also provides detailed individual and demographic information for each writer. The goal of the PERSUADE 2.0 corpus is to advance research into relationships between discourse elements, their effectiveness, writing quality, writing tasks and prompts, and demographic and individual differences.}
}
@article{JIANG201814,
title = {Computational intelligence techniques for maximum power point tracking in PV systems: A review},
journal = {Renewable and Sustainable Energy Reviews},
volume = {85},
pages = {14-45},
year = {2018},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2018.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S1364032118300054},
author = {Lian L. Jiang and R. Srivatsan and Douglas L. Maskell},
keywords = {Maximum power point tracking, PV system, Computational intelligence algorithm, Heuristic algorithm, Global tracking, Partial shading},
abstract = {Maximum power point (MPP) tracking (MPPT) is an important technique for maximizing the power extraction from photovoltaic (PV) systems under varying climatic conditions. In an array of PV modules it is possible to observe multiple peaks in the power versus voltage (P-V) curve due to the current versus voltage (I–V) PV cell mismatch caused by differences in the received irradiance, such as occurs during partial shading. In these circumstances, the ability of the MPPT devices to track the global MPP of the PV array directly influences the system efficiency. In the literature, various MPPT techniques have been proposed. Among them, computational intelligence (CI) algorithm based MPPT methods have demonstrated the ability to find the global MPP. This paper presents a detailed and specific review of CI- based MPPT techniques. Each method type is classified into one of several subcategories according to its application strategy. The various ways of applying CI into MPPTs are analyzed in detail. The advantages and disadvantages of each method are discussed and compared. The purpose of this study is to provide a compendium on CI-based MPPT techniques for users to understand and select an appropriate method based on application requirements and system constraints.}
}
@article{ECONOMOU1994131,
title = {Activities, issues and perspectives in computational physics: a view from Greece},
journal = {Computational Materials Science},
volume = {2},
number = {1},
pages = {131-136},
year = {1994},
issn = {0927-0256},
doi = {https://doi.org/10.1016/0927-0256(94)90055-8},
url = {https://www.sciencedirect.com/science/article/pii/0927025694900558},
author = {E.N. Economou}
}
@article{ZHOU20241018,
title = {A 21st Century View of Allowed and Forbidden Electrocyclic Reactions},
journal = {The Journal of Organic Chemistry},
volume = {89},
number = {2},
pages = {1018-1034},
year = {2024},
issn = {0022-3263},
doi = {https://doi.org/10.1021/acs.joc.3c02103},
url = {https://www.sciencedirect.com/science/article/pii/S0022326324000720},
author = {Qingyang Zhou and Garrett Kukier and Igor Gordiy and Roald Hoffmann and Jeffrey I. Seeman and K. N. Houk},
abstract = {In 1965, Woodward and Hoffmann proposed a theory to predict the stereochemistry of electrocyclic reactions, which, after expansion and generalization, became known as the Woodward–Hoffmann Rules. Subsequently, Longuet-Higgins and Abrahamson used correlation diagrams to propose that the stereoselectivity of electrocyclizations could be explained by the correlation of reactant and product orbitals with the same symmetry. Immediately thereafter, Hoffmann and Woodward applied correlation diagrams to explain the mechanism of cycloadditions. We describe these discoveries and their evolution. We now report an investigation of various electrocyclic reactions using DFT and CASSCF. We track the frontier molecular orbitals along the intrinsic reaction coordinate and modeled trajectories and examine the correlation between HOMO and LUMO for thermally forbidden systems. We also investigate the electrocyclizations of several highly polarized systems for which the Houk group had predicted that donor–acceptor substitution can induce zwitterionic character, thereby providing low-energy pathways for formally forbidden reactions. We conclude with perspectives on the field of pericyclic reactions, including a refinement as the meaning of Woodward and Hoffmann’s “Violations. There are none!” Lastly, we comment on the burgeoning influence of computations on all fields of chemistry.}
}
@article{PARK2023101271,
title = {The impact of research and representation of site analysis for creative design approach in architectural design studio},
journal = {Thinking Skills and Creativity},
volume = {48},
pages = {101271},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101271},
url = {https://www.sciencedirect.com/science/article/pii/S187118712300041X},
author = {Eun Joo Park and Keunhye Lee and Eunki Kang},
keywords = {Architectural design studio, Creative thinking, Design education, Design methodology, Site analysis},
abstract = {In the context of architectural education, design studio projects generally begin with the research of design themes and contexts; however, few attempts have been made to appreciate site analysis as reliable architectural design research. This study aims to explore strategies that link site analysis and design application to bridge the gap between research and representation as a framework for applying architectural design education. To bridge this knowledge gap, this study describes four phases of site analysis—(1) site selection, (2) site survey, (3) problem identification, and (4) suggestion for the design approach —in which visual expression of the representation technique was explored to develop a creative design approach through observation and analysis of students’ work. A curriculum that adopts the four phases of site analysis was developed based on the SPC and expanded for second-year architecture students in the university. The results showed that there are differences between architecture students in schematizing specific ideas and analysis methods, and that a substantial change in the process occurs when including visual expression in site analysis. In addition, the combination of group-based work and site analysis led to problem-solving that showed co-evolution. Finally, the study describes how research shapes site analysis and explains how representation can contribute to understanding the research. Site analysis consists of an initial attempt to explore research related to creative approaches, and may benefit both architecture educators and students.}
}
@incollection{BUNGE1980155,
title = {CHAPTER 7 - Thinking and Knowing},
editor = {MARIO BUNGE},
booktitle = {The Mind–Body Problem},
publisher = {Pergamon},
pages = {155-173},
year = {1980},
isbn = {978-0-08-024720-5},
doi = {https://doi.org/10.1016/B978-0-08-024720-5.50012-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780080247205500128},
author = {MARIO BUNGE},
abstract = {Publisher Summary
This chapter discusses the process of thinking and knowing. Forming concepts, propositions, problems, and directions are examples of thinking. Thinking can be visual, verbal, or abstract. It can be chaotic or orderly, creative, or routine. Thinking of any kind is an activity of some plastic neural systems. Of all mental activities, thinking is probably the one most affected by chemical changes and changes in basic properties of neurons. For example, humans unable to oxidize the amino acid phenylalanine cannot think, thyroid hypofunction produces cretinism, and normal subjects cannot think straight when in states of extreme stress, which are often states of hormonal imbalance, or when under the action of psychotropic drugs. The chapter also discusses the concept of cognition. All cognition is learned but not every learned item is of a cognitive nature. All cognition is cognition of some object, concrete or conceptual, and it consists in some information about its object—complete or partial, true or false. Cognition can be behavioral, perceptual, or conceptual.}
}
@article{CEKIRGE199465,
title = {An appropriate algorithm in parallel computations for three-dimensional hydrodynamics},
journal = {Mathematical and Computer Modelling},
volume = {20},
number = {1},
pages = {65-84},
year = {1994},
issn = {0895-7177},
doi = {https://doi.org/10.1016/0895-7177(94)90219-4},
url = {https://www.sciencedirect.com/science/article/pii/0895717794902194},
author = {H.M. Cekirge and J. Berlin and R.A. Bernatz and M. Koch},
keywords = {Methods of characteristics, Tidal currents, Parallel computations, Three-dimensional hydrodynamics, Tidal currents in the Arabian Gulf},
abstract = {There are a number of numerical methods for solving three-dimensional hydrodynamical models. An important aspect of any method is its efficient use of parallel computer architectures in an effort to minimize the clock time requirements in certain simulations such as oil spill modeling which uses three-dimensional hydrodynamics. The vertical-horizontal splitting (VHS) algorithm, using the method of characteristics for the two-dimensional horizontal plane and a generalization of the Crank-Nicholson method for vertical integration, is well-suited for the parallel architecture of the CM-2 machine.}
}
@article{GALTIER201683,
title = {Radiative transfer and spectroscopic databases: A line-sampling Monte Carlo approach},
journal = {Journal of Quantitative Spectroscopy and Radiative Transfer},
volume = {172},
pages = {83-97},
year = {2016},
note = {Eurotherm Conference No. 105: Computational Thermal Radiation in Participating Media V},
issn = {0022-4073},
doi = {https://doi.org/10.1016/j.jqsrt.2015.10.016},
url = {https://www.sciencedirect.com/science/article/pii/S0022407315003192},
author = {Mathieu Galtier and Stéphane Blanco and Jérémi Dauchet and Mouna {El Hafi} and Vincent Eymet and Richard Fournier and Maxime Roger and Christophe Spiesser and Guillaume Terrée},
keywords = {Radiative transfer, Monte Carlo method, Null-collision, Line sampling, Statistical approach, Spectroscopic databases},
abstract = {Dealing with molecular-state transitions for radiative transfer purposes involves two successive steps that both reach the complexity level at which physicists start thinking about statistical approaches: (1) constructing line-shaped absorption spectra as the result of very numerous state-transitions, (2) integrating over optical-path domains. For the first time, we show here how these steps can be addressed simultaneously using the null-collision concept. This opens the door to the design of Monte Carlo codes directly estimating radiative transfer observables from spectroscopic databases. The intermediate step of producing accurate high-resolution absorption spectra is no longer required. A Monte Carlo algorithm is proposed and applied to six one-dimensional test cases. It allows the computation of spectrally integrated intensities (over 25cm−1 bands or the full IR range) in a few seconds, regardless of the retained database and line model. But free parameters need to be selected and they impact the convergence. A first possible selection is provided in full detail. We observe that this selection is highly satisfactory for quite distinct atmospheric and combustion configurations, but a more systematic exploration is still in progress.}
}
@article{AKCAOGLU201472,
title = {Cognitive outcomes from the Game-Design and Learning (GDL) after-school program},
journal = {Computers & Education},
volume = {75},
pages = {72-81},
year = {2014},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2014.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0360131514000372},
author = {Mete Akcaoglu and Matthew J. Koehler},
keywords = {Game-design, Problem-solving, Quasi-experimental, Constructionism},
abstract = {The Game-Design and Learning (GDL) initiative engages middle school students in the process of game-design in a variety of in-school, after-school, and summer camp settings. The goal of the GDL initiative is to leverage students' interests in games and design to foster their problem-solving and critical reasoning skills. The present study examines the effectiveness of an after-school version of the GDL program using a quasi-experimental design. Students enrolled in the GDL program were guided in the process of designing games aimed at solving problems. Compared to students in a control group who did not attend the program (n = 24), the children who attended the GDL program (n = 20) showed a significant increase in their problem-solving skills. The results provide empirical support for the hypothesis that participation in the GDL program leads to measurable cognitive changes in children's problem-solving skills. This study bears important implications for educators and theory.}
}
@article{CHU20181,
title = {Supporting scientific modeling through curriculum-based making in elementary school science classes},
journal = {International Journal of Child-Computer Interaction},
volume = {16},
pages = {1-8},
year = {2018},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2017.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212868917300545},
author = {Sharon Lynn Chu and Elizabeth Deuermeyer and Francis Quek},
keywords = {Making, Maker movement, Children, Science, Science models, Scientific modeling, Model thinking, Electronics, Programming},
abstract = {Our work investigates how Making may be used in the context of scientific modeling in formal elementary school science classes. This paper presents an investigation of fourth- and fifth-grade students engaging in Making activities to create simulation, concept-process, and illustrative models in the science classroom. Based on video analyses of the Making-based class sessions, a generalized process model was developed for each type of science model. In addition, cross-cutting themes were found in Making-based science modeling: first, there are two loops that intersect and interact with each other (modeling for Making and modeling for Science content), and they interrelate in various ways depending on science model type; and second, showcasing Making products (sharing with peers, teachers, or helpers) is a primary factor that determines students’ overall engagement with science in the activity. We suggest that Making-based science kit and lesson design needs to support students to showcase their Making output, on top of science-related reflections, and to consider the balance between Making and science activity. We conclude that Making has the potential to support the development of scientific model thinking in the elementary science classroom, but much further research is needed in this area.}
}
@article{KAZANINA2023996,
title = {The neural ingredients for a language of thought are available},
journal = {Trends in Cognitive Sciences},
volume = {27},
number = {11},
pages = {996-1007},
year = {2023},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2023.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S1364661323001936},
author = {Nina Kazanina and David Poeppel},
keywords = {language-of-thought, symbolic representation, computational theory of mind, spatial navigation, compositionality},
abstract = {The classical notion of a ‘language of thought’ (LoT), advanced prominently by the philosopher Jerry Fodor, is an influential position in cognitive science whereby the mental representations underpinning thought are considered to be compositional and productive, enabling the construction of new complex thoughts from more primitive symbolic concepts. LoT theory has been challenged because a neural implementation has been deemed implausible. We disagree. Examples of critical computational ingredients needed for a neural implementation of a LoT have in fact been demonstrated, in particular in the hippocampal spatial navigation system of rodents. Here, we show that cell types found in spatial navigation (border cells, object cells, head-direction cells, etc.) provide key types of representation and computation required for the LoT, underscoring its neurobiological viability.}
}
@incollection{GALLICCHIO201127,
title = {Recent theoretical and computational advances for modeling protein–ligand binding affinities},
editor = {Christo Christov},
series = {Advances in Protein Chemistry and Structural Biology},
publisher = {Academic Press},
volume = {85},
pages = {27-80},
year = {2011},
booktitle = {Computational chemistry methods in structural biology},
issn = {1876-1623},
doi = {https://doi.org/10.1016/B978-0-12-386485-7.00002-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780123864857000028},
author = {Emilio Gallicchio and Ronald M. Levy},
keywords = {Quasi-chemical description, Statistical mechanics, Potential of mean force, PDT, MM/PBSA, free energy perturbation, BEDAM, double decoupling},
abstract = {We review recent theoretical and algorithmic advances for the modeling of protein ligand binding free energies. We first describe a statistical mechanics theory of noncovalent association, with particular focus on deriving the fundamental formulas on which computational methods are based. The second part reviews the main computational models and algorithms in current use or development, pointing out the relations with each other and with the theory developed in the first part. Particular emphasis is given to the modeling of conformational reorganization and entropic effect. The methods reviewed are free energy perturbation, double decoupling, the Binding Energy Distribution Analysis Method, the potential of mean force method, mining minima and MM/PBSA. These models have different features and limitations, and their ranges of applicability vary correspondingly. Yet their origins can all be traced back to a single fundamental theory.}
}
@article{PELAEZ2025109363,
title = {Universally Adaptable Multiscale Molecular Dynamics (UAMMD). A native-GPU software ecosystem for complex fluids, soft matter, and beyond},
journal = {Computer Physics Communications},
volume = {306},
pages = {109363},
year = {2025},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2024.109363},
url = {https://www.sciencedirect.com/science/article/pii/S0010465524002868},
author = {Raúl P. Peláez and Pablo Ibáñez-Freire and Pablo Palacios-Alonso and Aleksandar Donev and Rafael Delgado-Buscalioni},
keywords = {Molecular dynamics, Hydrodynamics, C++, CUDA, Soft matter},
abstract = {We introduce UAMMD (Universally Adaptable Multiscale Molecular Dynamics), a novel software infrastructure tailored for mesoscale complex fluid simulations on GPUs. The UAMMD library encompasses a comprehensive range of computational schemes optimized for the GPU, spanning from molecular dynamics to immersed boundary fluctuating-hydrodynamics. Developed in CUDA/C++14, this header-only open-source software serves both as a simulation engine and as a library with a modular architecture, offering a vast array of independent modules, categorized as interactors (neighbor search, bonded, non-bonded and electrostatic interactions, etc.) and integrators (molecular dynamics, dissipative particle dynamics, smooth particle hydrodynamics, Brownian hydrodynamics and a rather complete array of Immersed Boundary -IB- schemes). UAMMD excels in schemes that couple particle-based elastic structures with continuum fields in different regions of the mesoscale. To that end, thermal fluctuations can be added in physically consistent ways, and fast modes can be eliminated to adapt UAMMD to different regimes (compressible or incompressible flow, inertial or Stokesian dynamics, etc.). Thus, UAMMD is extremely useful for coarse-grained simulations of nanoparticles, and soft and biological matter (from proteins to viruses and micro-swimmers). Importantly, all UAMMD developments are hand-to-hand validated against experimental techniques, and it has proven to quantitatively reproduce experimental signals from quartz-crystal microbalance, atomic force microscopy, magnetic sensors, optic-matter interaction and ultrasound.
Program summary
Program Title: UAMMD CPC Library link to program files: https://doi.org/10.17632/srrt2y5s4m.1 Developer's repository link: https://github.com/RaulPPelaez/UAMMD/ Licensing provisions: GPLv3 Programming language: C++/CUDA Nature of problem: The key problem addressed in computational physics is simulating the behavior of matter at various scales, encompassing both discrete (particle-based) and continuum (field-based) approaches. The challenge lies in accurately and efficiently modeling interactions at different spatio-temporal scales, ranging from atomic (microscopic) to fluid dynamics (macroscopic). This complexity is further amplified in mesoscale regions, where different physics domains intersect, necessitating advanced computational techniques to capture the nuanced dynamics of systems such as colloids, polymers, and biological structures. Solution method: The present solution consists in the creation of UAMMD (Universally Adaptable Multiscale Molecular Dynamics), a CUDA/C++14 library designed for GPU-accelerated complex fluid simulations. UAMMD offers a flexible platform that integrates discrete particle dynamics with continuum fluid dynamics. It supports a variety of computational schemes, each tailored for specific spatio-temporal regimes. The library's modular architecture allows for the seamless introduction of new algorithms and easy integration into existing codebases. Additional comments including restrictions and unusual features: UAMMD's design emphasizes modularity and GPU-native architecture, optimizing computational efficiency and flexibility. However, its focus on GPU acceleration and low level nature means it requires compatible hardware and familiarity with CUDA programming. While UAMMD is versatile in handling various physical regimes, it currently lacks certain standard force field potentials and multi-GPU support. Nonetheless, its ongoing development and open-source nature promise continual enhancements.}
}
@article{EBERBACH2007200,
title = {The $-calculus process algebra for problem solving: A paradigmatic shift in handling hard computational problems},
journal = {Theoretical Computer Science},
volume = {383},
number = {2},
pages = {200-243},
year = {2007},
note = {Complexity of Algorithms and Computations},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2007.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S0304397507003192},
author = {Eugene Eberbach},
keywords = {Problem solving, Process algebras, Anytime algorithms, SuperTuring models of computation, Bounded rational agents, $-calculus, Intractability, Undecidability, Completeness, Optimality, Search optimality, Total optimality},
abstract = {The $-calculus is the extension of the π-calculus, built around the central notion of cost and allowing infinity in its operators. We propose the $-calculus as a more complete model for problem solving to provide a support to handle intractability and undecidability. It goes beyond the Turing Machine model. We define the semantics of the $-calculus using a novel optimization method (the kΩ-optimization), which approximates a nonexisting universal search algorithm and allows the simulation of many other search methods. In particular, the notion of total optimality has been utilized to provide an automatic way to deal with intractability of problem solving by optimizing together the quality of solutions and search costs. The sufficient conditions needed for completeness, optimality and total optimality of problem solving search are defined. A very flexible classification scheme of problem solving methods into easy, hard and solvable in the limit classes has been proposed. In particular, the third class deals with non-recursive solutions of undecidable problems. The approach is illustrated by solutions of some intractable and undecidable problems. We also briefly overview two possible implementations of the $-calculus.}
}
@article{BERGER2016337,
title = {Cognitive hierarchies in the minimizer game},
journal = {Journal of Economic Behavior & Organization},
volume = {130},
pages = {337-348},
year = {2016},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2016.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167268116301639},
author = {Ulrich Berger and Hannelore {De Silva} and Gerlinde Fellner-Röhling},
keywords = {Behavioral game theory, Experimental games, Poisson cognitive hierarchy, Level- model, Minimizer game},
abstract = {Experimental tests of choice predictions in one-shot games show only little support for Nash equilibrium (NE). Poisson Cognitive Hierarchy (PCH) and level-k (LK) are behavioral models of the thinking-steps variety where subjects differ in the number of levels of iterated reasoning they perform. Camerer et al. (2004) claim that substituting the Poisson parameter τ=1.5 yields a parameter-free PCH model (pfPCH) which predicts experimental data considerably better than NE. We design a new multi-person game, the Minimizer Game, as a testbed to compare initial choice predictions of NE, pfPCH and LK. Data obtained from two large-scale online experiments strongly reject NE and LK, but are well in line with the point-prediction of pfPCH.}
}
@article{VEITAS201716,
title = {Living Cognitive Society: A ‘digital’ World of Views},
journal = {Technological Forecasting and Social Change},
volume = {114},
pages = {16-26},
year = {2017},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2016.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0040162516300610},
author = {Viktoras Veitas and David Weinbaum},
keywords = {Cognitive system, Living society, Information and communication technologies, Future social governance, Individuation, Cognitive development},
abstract = {The current social reality is characterized by all-encompassing change, which disrupts existing social structures at all levels. Yet the approach based on the ontological primacy of stable and often hierarchical structures is still prevalent in theoretical and, most importantly, practical thinking about social systems. We propose a conceptual framework for thinking about a dynamically changing social system: the Living Cognitive Society. Importantly, we show how it follows from a much broader philosophical framework, guided by the theory of individuation, which emphasizes the importance of relationships and interactive processes in the evolution of a system. The framework addresses society as a living cognitive system – an ecology of interacting social subsystems – each of which is also a living cognitive system. We argue that this approach can help us to conceive sustainable social systems that will thrive in the circumstances of accelerating change. The Living Cognitive Society is explained in terms of its fluid structure, dynamics and the mechanisms at work. We then discuss the disruptive effects of Information and Communication Technologies on the mechanisms at work. We conclude by delineating a major topic for future research – distributed social governance – which focuses on processes of coordination rather than on stable structures within global society.}
}
@article{LOCK2023102310,
title = {Conserving complexity: A complex systems paradigm and framework to study public relations’ contribution to grand challenges},
journal = {Public Relations Review},
volume = {49},
number = {2},
pages = {102310},
year = {2023},
issn = {0363-8111},
doi = {https://doi.org/10.1016/j.pubrev.2023.102310},
url = {https://www.sciencedirect.com/science/article/pii/S0363811123000255},
author = {Irina Lock},
keywords = {Grand challenges, Public issues, Public relations, Strategic communication, Complex adaptive systems, Digital communication, Complexity},
abstract = {Sustainable development poses a grand challenge for society, addressed by organisations through their public relations activities. Grand challenges are complex by nature and call for nontrivial solutions whose effects show at the level of society. That is why studying public relations’ contribution to grand challenges requires a macro perspective that accounts for the dynamic interaction between individual, organisational, and system levels in a digital communication environment. This paper offers a new paradigm to analyse organisations’ significant and at times undue impact on grand challenges through public relations. It develops a framework inspired by complex adaptive systems thinking and adopts its ten properties for public relations: emergence, adaptivity, heterogeneous actors, nonlinear effects, feedback mechanisms, self-organisation, phase transitions, networks, scaling, and cooperation. The paper applies the framework to the example of sustainable development. It shows why research on grand challenges requires a holistic perspective and how it can help study digitally born communication phenomena. The proposed complex systems paradigm provides space for critical, social scientific, and interpretative research lines in public relations. Inquiries start from the grand challenge and study the communicative interactions between organisations and other actors from existing theory while accounting for the ten properties of complex adaptive systems. The paper outlines how future research can enrich the study of public relations and discusses its limits.}
}
@article{JU2022101062,
title = {Proposal for a STEAM education program for creativity exploring the roofline of a hanok using GeoGebra and 4Dframe},
journal = {Thinking Skills and Creativity},
volume = {45},
pages = {101062},
year = {2022},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2022.101062},
url = {https://www.sciencedirect.com/science/article/pii/S1871187122000657},
author = {Hyunshik Ju and Hogul Park and Eun Young Jung and Seoung-Hey Paik},
keywords = {STEAM education program, Creativity, Catenary curve, Korean traditional architecture, Modelling, GeoGebra, 4Dframe},
abstract = {This research was conducted to confirm the feasibility of a STEAM education program in which the mathematics, physics, and Korean traditional arts underlying the hanok roofline are investigated using educational tools of GeoGebra and 4Dframe. This paper contends that this program has the potential to engage students in knowledge restructuring regarding the perception of the hanok’s architectural beauty, Newton's concept of gravity, and mathematical functions. The Octagonal Pavilion in Tapgol Park in Seoul, South Korea, a representative hanok, was used as an educational resource. GeoGebra is used to determine that the roofline of the Octagonal Pavilion generally follows the formula of the catenary curve and then the roofline is modelled using 4Dframe. The catenary form of the roofline of the hanok is linked to the Korean sense of beauty in the pursuit of naturalness under the influence of gravity and organically harmonizes with the environment. The class described in this study, in which the curve of the roofline of the Octagonal Pavilion is explored using GeoGebra and 4Dframe, can help develop creative and critical thinking in students in the context of STEAM education. The findings of this study have the potential to expand the scope of STEAM education to include content for creative education.}
}
@article{WANG2016357,
title = {Research on Application of Abduction to Fire Investigation},
journal = {Procedia Engineering},
volume = {135},
pages = {357-362},
year = {2016},
note = {2015 International Conference on Performance-based Fire and Fire Protection Engineering (ICPFFPE 2015)},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2016.01.142},
url = {https://www.sciencedirect.com/science/article/pii/S1877705816001466},
author = {Shi Wang and Zhong-jun Shu},
keywords = {Fire investigation, Abduction, Logical thinking},
abstract = {To solve the problem of fire investigation caused by lack of exacting logical reasoning, it is of significance in helping that abduction, an important logical thinking should be introduced to the field of fire investigation. This paper first analyzes the fundamental reasoning forms of abduction as well as its general situation of application. Combined with practical work experience, the mode of application of abduction to fire investigation is put forward. The author shows it in detail by analyzing a real fire case. It is north noting that some matters needing attention in application are presented in the end. This paper will be conductive to constructing the right logical reasoning model in fire investigation.}
}
@article{MARCHETTI20121517,
title = {Mindwandering heightens the accessibility of negative relative to positive thought},
journal = {Consciousness and Cognition},
volume = {21},
number = {3},
pages = {1517-1525},
year = {2012},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2012.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S1053810012001420},
author = {Igor Marchetti and Ernst H.W. Koster and Rudi {De Raedt}},
keywords = {Mindwandering, Negative cognitions, Mood, Depression, Individual differences},
abstract = {Mindwandering (MW) is associated with both positive and negative outcomes. Among the latter, negative mood and negative cognitions have been reported. However, the underlying mechanisms linking mindwandering to negative mood and cognition are still unclear. We hypothesized that MW could either directly enhance negative thinking or indirectly heighten the accessibility of negative thoughts. In an undergraduate sample (n=79) we measured emotional thoughts during the Sustained Attention on Response Task (SART) which induces MW, and accessibility of negative cognitions by means of the Scrambled Sentences Task (SST) after the task. We also measured depressive symptoms and rumination. Results show that in individuals with elevated levels of depressive symptoms MW during SART predicts higher accessibility of negative thoughts after the task, rather than negative thinking during the task. These findings contribute to our understanding of the underlying mechanisms of MW and provide insight into the relationship between task-involvement and affect.}
}
@article{HU2022116276,
title = {Multi granularity based label propagation with active learning for semi-supervised classification},
journal = {Expert Systems with Applications},
volume = {192},
pages = {116276},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.116276},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421015840},
author = {Shengdan Hu and Duoqian Miao and Witold Pedrycz},
keywords = {Semi-supervised learning, Granular computing, Multi granularity, Label propagation, Active learning, Three-way decision},
abstract = {Semi-supervised learning (SSL) methods, which exploit both the labeled and unlabeled data, have attracted a lot of attention. One of the major categories of SSL methods, graph-based semi-supervised learning (GBSSL) learns labels of unlabeled data on an adjacency graph, where neighborhood sparse graph is often used to reduce computational complexity. However, the neighborhood size is difficult to set. Instead of assigning a concrete value of neighborhood size, we propose a new label propagation algorithm called multi granularity based label propagation (MGLP) and developed from the view of granular computing. In MGLP, labels of unlabeled data are learned by two classic label propagation processes with diverse neighborhood size k, where granular computing delivers a guiding strategy to leverage multiple level neighborhood information granules, and three-way decision acts as an active learning strategy to select the unlabeled data for further annotating. Through the iterative procedures of label propagating, data annotating and data subset updating, the ultimate pseudo label accuracy of unlabeled data may be higher. Theoretically, the accuracy of pseudo labels is enhanced in some scenarios. Experimentally, the results of simulation studies on ten benchmark datasets, show that the proposed method MGLP can rise pseudo labels accuracy by 8.6% than LP (label propagation), 6.5% than LNP (linear neighborhood propagation), 6.4% than LPSN (label propagation through sparse neighborhood), 4.5% than Adaptive-NP (adaptive neighborhood propagation) and 4.6% than CRLP (consensus rate-based label propagation). It also provides a novel way to annotate data.}
}
@article{BEHR199399,
title = {Computation of incompressible flows with implicit finite element implementations on the Connection Machine},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {108},
number = {1},
pages = {99-118},
year = {1993},
issn = {0045-7825},
doi = {https://doi.org/10.1016/0045-7825(93)90155-Q},
url = {https://www.sciencedirect.com/science/article/pii/004578259390155Q},
author = {M. Behr and A. Johnson and J. Kennedy and S. Mittal and T. Tezduyar},
abstract = {Two implicit finite element formulations for incompressible flows have been implemented on the Connection Machine supercomputers and successfully applied to a set of time-dependent problems. The stabilized space-time formulation for moving boundaries and interfaces, and a new stabilized velocity-pressure-stress formulation are both described, and significant aspects of the implementation of these methods on massively parallel architectures are discussed. Several numerical results for flow problems involving moving as well as fixed cylinders and airfoils are reported. The parallel implementation, taking full advantage of the computational speed of the new generation of supercomputers, is found to be a significant asset in fluid dynamics research. Its current capability to solve large-scale problems, especially when coupled with the potential for growth enjoyed by massively parallel computers, make the implementation a worthwhile enterprise.}
}
@article{GLASSMAN20101412,
title = {Pragmatism, connectionism and the internet: A mind’s perfect storm},
journal = {Computers in Human Behavior},
volume = {26},
number = {6},
pages = {1412-1418},
year = {2010},
note = {Online Interactivity: Role of Technology in Behavior Change},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2010.04.019},
url = {https://www.sciencedirect.com/science/article/pii/S0747563210000956},
author = {Michael Glassman and Min Ju Kang},
keywords = {Internet, Dewey, Connectionism, Democracy},
abstract = {This paper explores that natural relationships between Pragmatic theory of knowing, the dynamic structuring of the mind and thinking suggested by connectionist theory, and the way information is distributed and organized through the world wide web (www). We suggest that these three “innovations” can be brought together to offer a better understanding of the way the human mind works. The internet and the information revolution may finally offer the opportunity to use and develop inductive learning practices and information based social inquiry in ways Pragmatic philosophers envisioned a hundred years ago, while the recent rise of connectionist and cognitive architecture works provides a concrete context for such developments. This confluence of process represents the type of synergy that only history can offer. The information revolution – exemplified by both the rise of connectionism and the internet – is the apotheosis of the Pragmatic revolution – bringing together radical empiricism and democratization of information in community practice. We offer three important realizations in our understanding of how information is organized and thinking progresses made possible by burgeoning virtual communities on the internet – open source thinking, scale-free networks, and interrelationships in the development of blogs to illustrate our thesis.}
}
@article{LI2024109502,
title = {Numerical study on heat transfer performance of printed circuit heat exchanger with anisotropic thermal conductivity},
journal = {International Journal of Heat and Fluid Flow},
volume = {109},
pages = {109502},
year = {2024},
issn = {0142-727X},
doi = {https://doi.org/10.1016/j.ijheatfluidflow.2024.109502},
url = {https://www.sciencedirect.com/science/article/pii/S0142727X24002273},
author = {Libo Li and Jiyuan Bi and Jingkai Ma and Xiaoxu Zhang and Qiuwang Wang and Ting Ma},
keywords = {Printed circuit heat exchanger, Anisotropic thermal conductivity, Numerical simulation, Thermal resistance, Heat exchanger efficiency},
abstract = {Printed Circuit Heat Exchangers are compact and efficient heat exchangers, widely used in nuclear engineering, very high-temperature reactors, and aerospace systems. This study investigates the heat transfer performance of a heat exchanger with anisotropic thermal conductivity, such as fiber reinforced composites. Numerical simulations were conducted to examine the synergistic effect of three-dimensional thermal resistance on heat exchanger performance. The most significant impact on performance is the z-direction thermal resistance, followed by the y-direction, while the x-direction has the least impact. Contrary to traditional design thinking, increasing the overall heat exchanger thermal resistance under the same thermal resistance ratio improves heat transfer efficiency at the studied conditions. The results suggest that it is necessary to design the lowest thermal conductivity direction as the z-direction and increase the y-direction thermal conductivity to enhance heat exchanger performance. In the numerical investigation presented in this study, the efficiency of the heat exchanger was improved by approximately 23 % under specific operating conditions by adjusting the thermal conductivity of anisotropic materials to control the thermal resistance in the x, y and z directions. It is evident that the manipulation of anisotropic material properties has a substantial influence on the performance of heat exchangers.}
}
@article{20213190,
title = {Eunji Cheong},
journal = {Neuron},
volume = {109},
number = {20},
pages = {3190-3192},
year = {2021},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2021.09.027},
url = {https://www.sciencedirect.com/science/article/pii/S0896627321007005},
abstract = {In Korea, the pandemic has elevated scientists as trusted sources for both policy decisions and dinner table conversation. In an interview with Neuron, Eunji Cheong discusses how we need to support future generations by fostering scientific thinking, patience, and flexibility.}
}
@article{DEKKER2017554,
title = {Rasmussen's legacy and the long arm of rational choice},
journal = {Applied Ergonomics},
volume = {59},
pages = {554-557},
year = {2017},
note = {The Legacy of Jens Rasmussen},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2016.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0003687016300254},
author = {Sidney W.A. Dekker},
keywords = {Rasmussen, Rational choice, Human error, Second victim, Incidents},
abstract = {Rational choice theory says that operators and others make decisions by systematically and consciously weighing all possible outcomes along all relevant criteria. This paper first traces the long historical arm of rational choice thinking in the West to Judeo-Christian thinking, Calvin and Weber. It then presents a case study that illustrates the consequences of the ethic of rational choice and individual responsibility. It subsequently examines and contextualizes Rasmussen's legacy of pushing back against the long historical arm of rational choice, showing that bad outcomes are not the result of human immoral choice, but the product of normal interactions between people and systems. If we don't understand why people did what they did, Rasmussen suggested, it is not because people behaved inexplicably, but because we took the wrong perspective.}
}
@incollection{KAMAREDDINE2012801,
title = {Russell's Orders in Kripke's Theory of Truth and Computational Type Theory},
editor = {Dov M. Gabbay and Akihiro Kanamori and John Woods},
series = {Handbook of the History of Logic},
publisher = {North-Holland},
volume = {6},
pages = {801-845},
year = {2012},
booktitle = {Sets and Extensions in the Twentieth Century},
issn = {1874-5857},
doi = {https://doi.org/10.1016/B978-0-444-51621-3.50011-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780444516213500116},
author = {Fairouz Kamareddine and Twan Laan and Robert Constable}
}
@article{HONDA201718,
title = {The difference in foresight using the scanning method between experts and non-experts},
journal = {Technological Forecasting and Social Change},
volume = {119},
pages = {18-26},
year = {2017},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2017.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S004016251730313X},
author = {Hidehito Honda and Yuichi Washida and Akihito Sudo and Yuichiro Wajima and Keigo Awata and Kazuhiro Ueda},
keywords = {Foresight, Scanning method, Divergent thinking, Difference between experts and non-experts, Creativity},
abstract = {We examined the factors that produce differences in generating scenarios on the near future using the scanning method. Participants were asked to briefly read (scan) 151 articles about new technology, the latest customs, fashion, social change, value system transition, or emerging social problems, and then to generate three scenarios about the near future based on the articles. We compared the generated scenarios between scanning method experts and non-experts with no prior experience with the scanning method. We found that experts generated more unique scenarios than non-experts did, and that experts and non-experts differed in the diversity of articles referenced when generating scenarios. We discuss the relationship between the present findings and previous findings on divergent thinking.}
}
@incollection{CORMACK2005325,
title = {4.1 - Computational Models of Early Human Vision},
editor = {AL BOVIK},
booktitle = {Handbook of Image and Video Processing (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Burlington},
pages = {325-IX},
year = {2005},
series = {Communications, Networking and Multimedia},
isbn = {978-0-12-119792-6},
doi = {https://doi.org/10.1016/B978-012119792-6/50083-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780121197926500838},
author = {Lawrence K. Cormack}
}
@article{VANDUN2023113880,
title = {ProcessGAN: Supporting the creation of business process improvement ideas through generative machine learning},
journal = {Decision Support Systems},
volume = {165},
pages = {113880},
year = {2023},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2022.113880},
url = {https://www.sciencedirect.com/science/article/pii/S0167923622001518},
author = {Christopher {van Dun} and Linda Moder and Wolfgang Kratsch and Maximilian Röglinger},
keywords = {Business process improvement, Business process redesign, Generative adversarial networks, Generative machine learning, Process mining},
abstract = {Business processes are a key driver of organizational success, which is why business process improvement (BPI) is a central activity of business process management. Despite an abundance of approaches, BPI as a creative task is time-consuming and labour-intensive. Most importantly, its level of computational support is low. The few computational BPI approaches hardly leverage the opportunities brought about by computational creativity, neglect process data, and rely on rather rigid improvement patterns. Given the increasing amount of process data in the form of event logs and the uptake of generative machine learning for automating creative tasks in various domains, there is huge potential for BPI. Hence, following the design science research paradigm, we specified, implemented, and evaluated ProcessGAN, a novel computational BPI approach based on generative adversarial networks that supports the creation of BPI ideas. Our evaluation shows that ProcessGAN improves the creativity of process designers, particularly the originality of BPI ideas, and shapes up useful in real-world settings. Moreover, ProcessGAN is the first approach to combine BPI and computational creativity.}
}
@article{SYCHEV2024101261,
title = {Educational models for cognition: Methodology of modeling intellectual skills for intelligent tutoring systems},
journal = {Cognitive Systems Research},
volume = {87},
pages = {101261},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101261},
url = {https://www.sciencedirect.com/science/article/pii/S138904172400055X},
author = {Oleg Sychev},
keywords = {Reasoning modeling, Constraint-based modeling, Intelligent tutoring systems},
abstract = {Automation of teaching people new skills requires modeling of human reasoning because human cognition involves active reasoning over the new subject domain to acquire skills that will later become automatic. The article presents Thought Process Trees — a language for modeling human reasoning that was created to facilitate the development of intelligent tutoring systems, which can perform the same reasoning that is expected of a student and find deficiencies in their line of thinking, providing explanatory messages and allowing them to learn from performance errors. The methodology of building trees which better reflect human learning is discussed, with examples of design choices during the modeling process and their consequences. The characteristics of educational modeling that impact building subject-domain models for intelligent tutoring systems are discussed. The trees were formalized and served as a basis for developing a framework for constructing intelligent tutoring systems. This significantly lowered the time required to build and debug a constraint-based subject-domain model. The framework has already been used to develop five intelligent tutoring systems and their prototypes and is being used to develop more of them.}
}
@article{REDKO2016105,
title = {Epistemological foundations of investigation of cognitive evolution},
journal = {Biologically Inspired Cognitive Architectures},
volume = {18},
pages = {105-115},
year = {2016},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2016.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X16300597},
author = {Vladimir G. Red’ko},
keywords = {Modeling of cognitive evolution, Cognitive agents, Animal cognitive features, Epistemological foundations},
abstract = {Epistemological foundations for modeling of cognitive evolution are characterized. Cognitive evolution is the evolution of cognitive abilities of biological organisms. The important result of this evolution is the human thinking, which is used at scientific cognition of nature. The related epistemological viewpoints of David Hume, Immanuel Kant, Konrad Lorenz, and Eugene Wigner are outlined. The sketch program for future investigations of cognitive evolution is proposed; initial models of these studies are outlined. According to the presented analysis, it is possible to believe the following. Investigations of cognitive evolution are directed to analyze the fundamental problems: “Why is human thinking applicable to cognition of nature?”, “How did human thinking origin in the process of biological evolution?” There are powerful backgrounds for considered investigations: (1) models of autonomous cognitive agents, (2) biological investigations of animal cognitive features. Studies of cognitive evolution would have broad interdisciplinary relations. These studies should contribute significantly to the development of the scientific point of view.}
}
@article{PEARSON1994203,
title = {Report on University of Wales Institute of non-Newtonian Fluid Mechanics Mini-Symposium on “Continuum and Microstructural Modelling in Computational Rheology” Seiont Manor, Gwynedd, 11–12 April 1994},
journal = {Journal of Non-Newtonian Fluid Mechanics},
volume = {55},
number = {2},
pages = {203-205},
year = {1994},
issn = {0377-0257},
doi = {https://doi.org/10.1016/0377-0257(94)80006-5},
url = {https://www.sciencedirect.com/science/article/pii/0377025794800065},
author = {J.R.A. Pearson}
}
@incollection{TOPLAK202253,
title = {3 - Development of the ability to detect and override miserly information processing},
editor = {Maggie E. Toplak},
booktitle = {Cognitive Sophistication and the Development of Judgment and Decision-Making},
publisher = {Academic Press},
pages = {53-87},
year = {2022},
isbn = {978-0-12-816636-9},
doi = {https://doi.org/10.1016/B978-0-12-816636-9.00011-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128166369000116},
author = {Maggie E. Toplak},
keywords = {Miserly information processing, Dual process models, Children and youth, Development, Ratio bias, Belief bias syllogisms, Cognitive reflection},
abstract = {Several judgment and decision-making tasks require overriding an incorrect response that is signaled by miserly information processes. The successful detection and override of conflict between heuristic and analytic processes has been a focus of dual processes models, especially in adult samples. These miserly processing tendencies have also been described in developmental samples. The measurement of resistance to miserly information processing has been assessed using several tasks, including ratio bias, belief bias syllogisms, cognitive reflection, and disjunctive thinking tasks. Several of these tasks have been studied in developmental samples, including in the longitudinal study described in this volume. There is evidence to suggest that resistance to miserly information processing is measurable in children and youth. While judgment and decision-making tasks vary in the degree to which override of miserly processing is required, individuals also vary in their ability to resist miserly processing tendencies. Individual differences in resistance to miserly information processing serve as an additional foundation to support rational thinking performance.}
}
@article{FUXJAGER2023105340,
title = {Systems biology as a framework to understand the physiological and endocrine bases of behavior and its evolution—From concepts to a case study in birds},
journal = {Hormones and Behavior},
volume = {151},
pages = {105340},
year = {2023},
issn = {0018-506X},
doi = {https://doi.org/10.1016/j.yhbeh.2023.105340},
url = {https://www.sciencedirect.com/science/article/pii/S0018506X23000387},
author = {Matthew J. Fuxjager and T. Brandt Ryder and Nicole M. Moody and Camilo Alfonso and Christopher N. Balakrishnan and Julia Barske and Mariane Bosholn and W. Alice Boyle and Edward L. Braun and Ioana Chiver and Roslyn Dakin and Lainy B. Day and Robert Driver and Leonida Fusani and Brent M. Horton and Rebecca T. Kimball and Sara Lipshutz and Claudio V. Mello and Eliot T. Miller and Michael S. Webster and Morgan Wirthlin and Roy Wollman and Ignacio T. Moore and Barney A. Schlinger},
keywords = {Systems biology, Animal behavior, Organismal physiology, Adaptive evolution, Manakin birds, Androgenic hormones, Robustness},
abstract = {Organismal behavior, with its tremendous complexity and diversity, is generated by numerous physiological systems acting in coordination. Understanding how these systems evolve to support differences in behavior within and among species is a longstanding goal in biology that has captured the imagination of researchers who work on a multitude of taxa, including humans. Of particular importance are the physiological determinants of behavioral evolution, which are sometimes overlooked because we lack a robust conceptual framework to study mechanisms underlying adaptation and diversification of behavior. Here, we discuss a framework for such an analysis that applies a “systems view” to our understanding of behavioral control. This approach involves linking separate models that consider behavior and physiology as their own networks into a singular vertically integrated behavioral control system. In doing so, hormones commonly stand out as the links, or edges, among nodes within this system. To ground our discussion, we focus on studies of manakins (Pipridae), a family of Neotropical birds. These species have numerous physiological and endocrine specializations that support their elaborate reproductive displays. As a result, manakins provide a useful example to help imagine and visualize the way systems concepts can inform our appreciation of behavioral evolution. In particular, manakins help clarify how connectedness among physiological systems—which is maintained through endocrine signaling—potentiate and/or constrain the evolution of complex behavior to yield behavioral differences across taxa. Ultimately, we hope this review will continue to stimulate thought, discussion, and the emergence of research focused on integrated phenotypes in behavioral ecology and endocrinology.}
}
@article{MASOUD201293,
title = {Synthesis, computational, spectroscopic, thermal and antimicrobial activity studies on some metal–urate complexes},
journal = {Spectrochimica Acta Part A: Molecular and Biomolecular Spectroscopy},
volume = {90},
pages = {93-108},
year = {2012},
issn = {1386-1425},
doi = {https://doi.org/10.1016/j.saa.2012.01.028},
url = {https://www.sciencedirect.com/science/article/pii/S1386142512000418},
author = {Mamdouh S. Masoud and Alaa E. Ali and Medhat A. Shaker and Gehan S. Elasala},
keywords = {Uric, Complexes, Synthesis, Spectroscopy, Thermal analysis, Computational},
abstract = {New sixteen uric acid metal complexes of different stoichiometry, stereo-chemistries and modes of interactions were synthesized using different metals Cr, Mn, Fe, Co, Ni, Cu, Cd, UO2, Na and K. The synthesized complexes were characterized by elemental analysis, spectral (IR, UV–Vis and ESR) methods, thermal analysis (TG, DTA and DSC) and magnetic susceptibility studies. Molecular modeling calculations were used to characterize the ligation sites of the free ligand. Furthermore, quantum chemical parameters of uric acid such as the energies of highest occupied molecular orbital (EHOMO), energies of lowest unoccupied molecular orbital (ELUMO), the separation energy (ΔE=ELUMO−EHOMO), the absolute electronegativity, χ, the chemical potential, Pi, the absolute hardness, η and the softness (σ) were obtained for uric acid. Eight different microbial categories were used to study the antimicrobial activity of the free ligand and ten of its complexes. The results indicate that the ligand and its metal complexes possess antimicrobial properties. The stoichiometry of iron–uric acid complex was studied by using different spectrophotometric methods.}
}
@article{BAYNE201332,
title = {Thought},
journal = {New Scientist},
volume = {219},
number = {2935},
pages = {32-39},
year = {2013},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(13)62293-9},
url = {https://www.sciencedirect.com/science/article/pii/S0262407913622939},
author = {Tim Bayne},
abstract = {Conscious or unbidden, thoughts fill our heads from morning to night. But what are they, and what exactly is thinking? Join philosopher Tim Bayne on a journey into the fantastic, elusive and ceaseless world our minds create}
}
@article{VANDENHURK2023106030,
title = {Consideration of compound drivers and impacts in the disaster risk reduction cycle},
journal = {iScience},
volume = {26},
number = {3},
pages = {106030},
year = {2023},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2023.106030},
url = {https://www.sciencedirect.com/science/article/pii/S2589004223001074},
author = {Bart J.J.M. {van den Hurk} and Christopher J. White and Alexandre M. Ramos and Philip J. Ward and Olivia Martius and Indiana Olbert and Kathryn Roscoe and Henrique M.D. Goulart and Jakob Zscheischler},
keywords = {Earth sciences, Social sciences, Decision science},
abstract = {Summary
Consideration of compound drivers and impacts are often missing from applications within the Disaster Risk Reduction (DRR) cycle, leading to poorer understanding of risk and benefits of actions. The need to include compound considerations is known, but lack of guidance is prohibiting practitioners from including these considerations. This article makes a step toward practitioner guidance by providing examples where consideration of compound drivers, hazards, and impacts may affect different application domains within disaster risk management. We discern five DRR categories and provide illustrative examples of studies that highlight the role of “compound thinking” in early warning, emergency response, infrastructure management, long-term planning, and capacity building. We conclude with a number of common elements that may contribute to the development of practical guidelines to develop appropriate applications for risk management.}
}
@article{NISHI2022314,
title = {Health and landscape approaches: A comparative review of integrated approaches to health and landscape management},
journal = {Environmental Science & Policy},
volume = {136},
pages = {314-325},
year = {2022},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2022.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S1462901122002027},
author = {Maiko Nishi and Shizuka Hashimoto},
keywords = {Landscape approaches, One Health, Ecohealth, Planetary Health, Social-ecological systems, Sustainability transformation},
abstract = {Landscape approaches are integrated place-based approaches and provide cross-sectoral opportunities to facilitate sustainability transformations. The COVID-19 outbreak has profound ramifications for multiple dimensions of landscapes, ranging from mobility and lifestyle to value to environment and society. Therefore, integrated approaches to “health” have been more vigorously promoted in the policy arena dealing with human–nature interactions. The ecosystem principles of the Convention on Biological Diversity, which resonate with landscape approaches, are generally aligned with integrated approaches to health. However, commonalities and distinctions between these integrated approaches in both political and scientific domains have not been clarified. Drawing on a narrative review of the literature on “One Health,” “Ecohealth,” and “Planetary Health” as major health-oriented approaches in comparison with landscape approaches, the aspects of landscape approaches to be complemented in addressing health-related challenges were examined in this study. In addition to the review on the intellectual roots and evolutionary pathways, a comparative analysis of these relevant approaches was conducted in terms of three realms including theoretical assumptions, knowledge bases, and research paradigms. The results of the comparative review show that all approaches share systems thinking, interdisciplinarity, cross-sectoral collaboration, and holistic paradigm but differ with respect to their focused management problems, disciplines, and sectors as well as ontological and epistemological underpinnings. Pointing to the recent theoretical and methodological development in integrating health in placemaking, the results of this study suggest that pragmatic landscape approaches could be strengthened by using health-related research paradigms to achieve better constructivism–positivism meeting grounds regarding health–landscape intersections.}
}
@article{SAVIN2024108324,
title = {Reviewing studies of degrowth: Are claims matched by data, methods and policy analysis?},
journal = {Ecological Economics},
volume = {226},
pages = {108324},
year = {2024},
issn = {0921-8009},
doi = {https://doi.org/10.1016/j.ecolecon.2024.108324},
url = {https://www.sciencedirect.com/science/article/pii/S0921800924002210},
author = {Ivan Savin and Jeroen {van den Bergh}},
keywords = {Economic growth, Environmental policy, GDP, Political feasibility, Post-growth},
abstract = {In the last decade many publications have appeared on degrowth as a strategy to confront environmental and social problems. We undertake a systematic review of their content, data and methods. This involves the use of computational linguistics to identify main topics investigated. Based on a sample of 561 studies we conclude that: (1) content covers 11 main topics; (2) the large majority (almost 90%) of studies are opinions rather than analysis; (3) few studies use quantitative or qualitative data, and even fewer ones use formal modelling; (4) the first and second type tend to include small samples or focus on non-representative cases; (5) most studies offer ad hoc and subjective policy advice, lacking policy evaluation and integration with insights from the literature on environmental/climate policies; (6) of the few studies on public support, a majority concludes that degrowth strategies and policies are socially-politically infeasible; (7) various studies represent a “reverse causality” confusion, i.e. use the term degrowth not for a deliberate strategy but to denote economic decline (in GDP terms) resulting from exogenous factors or public policies; (8) few studies adopt a system-wide perspective – instead most focus on small, local cases without a clear implication for the economy as a whole. We illustrate each of these findings for concrete studies.}
}
@article{DENNER2012240,
title = {Computer games created by middle school girls: Can they be used to measure understanding of computer science concepts?},
journal = {Computers & Education},
volume = {58},
number = {1},
pages = {240-249},
year = {2012},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2011.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0360131511001849},
author = {Jill Denner and Linda Werner and Eloy Ortiz},
keywords = {Construction of computer games, Secondary education, Programming, After-school},
abstract = {Computer game programming has been touted as a promising strategy for engaging children in the kinds of thinking that will prepare them to be producers, not just users of technology. But little is known about what they learn when programming a game. In this article, we present a strategy for coding student games, and summarize the results of an analysis of 108 games created by middle school girls using Stagecast Creator in an after school class. The findings show that students engaged in moderate levels of complex programming activity, created games with moderate levels of usability, and that the games were characterized by low levels of code organization and documentation. These results provide evidence that game construction involving both design and programming activities can support the learning of computer science concepts.}
}
@article{SPINU2022100205,
title = {A matter of trust: Learning lessons about causality will make qAOPs credible},
journal = {Computational Toxicology},
volume = {21},
pages = {100205},
year = {2022},
issn = {2468-1113},
doi = {https://doi.org/10.1016/j.comtox.2021.100205},
url = {https://www.sciencedirect.com/science/article/pii/S2468111321000517},
author = {Nicoleta Spînu and Mark T.D. Cronin and Judith C. Madden and Andrew P. Worth},
keywords = {Model credibility, Adverse Outcome Pathway, qAOP, Causality, Next Generation Risk Assessment},
abstract = {Toxicology in the 21st Century has seen a shift from chemical risk assessment based on traditional animal tests, identifying apical endpoints and doses that are “safe”, to the prospect of Next Generation Risk Assessment based on non-animal methods. Increasingly, large and high throughput in vitro datasets are being generated and exploited to develop computational models. This is accompanied by an increased use of machine learning approaches in the model building process. A potential problem, however, is that such models, while robust and predictive, may still lack credibility from the perspective of the end-user. In this commentary, we argue that the science of causal inference and reasoning, as proposed by Judea Pearl, will facilitate the development, use and acceptance of quantitative AOP models. Our hope is that by importing established concepts of causality from outside the field of toxicology, we can be “constructively disruptive” to the current toxicological paradigm, using the “Causal Revolution” to bring about a “Toxicological Revolution” more rapidly.}
}
@article{FLANIGAN2017179,
title = {Implicit intelligence beliefs of computer science students: Exploring change across the semester},
journal = {Contemporary Educational Psychology},
volume = {48},
pages = {179-196},
year = {2017},
issn = {0361-476X},
doi = {https://doi.org/10.1016/j.cedpsych.2016.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0361476X16300479},
author = {Abraham E. Flanigan and Markeya S. Peteranetz and Duane F. Shell and Leen-Kiat Soh},
keywords = {Motivation, Implicit intelligence beliefs, Computer science, Self-regulation, Engagement},
abstract = {This study investigated introductory computer science (CS1) students’ implicit beliefs of intelligence. Referencing Dweck and Leggett’s (1988) framework for implicit beliefs of intelligence, we examined how (1) students’ implicit beliefs changed over the course of a semester, (2) these changes differed as a function of course enrollment and students’ motivated self-regulated engagement profile, and (3) implicit beliefs predicted student learning based on standardized course grades and performance on a computational thinking knowledge test. For all students, there were significant increases in entity beliefs and significant decreases in incremental beliefs across the semester. However, examination of effect sizes suggests that significant findings for change across time were driven by changes in specific subpopulations of students. Moreover, results showed that students endorsed incremental belief more strongly than entity belief at both the beginning and end of the semester. Furthermore, the magnitude of changes differed based on students’ motivated self-regulated engagement profiles. Additionally, students’ achievement outcomes were weakly predicted by their implicit beliefs of intelligence. Finally, results showed that the relationship between changes in implicit intelligence beliefs and student achievement varied across different CS1 courses. Theoretical implications for implicit intelligence beliefs and recommendations for STEM educators are discussed.}
}
@article{KULIK20242338,
title = {Reaction: The challenge of open-shell transition metal catalysis in “systems chemistry”},
journal = {Chem},
volume = {10},
number = {8},
pages = {2338-2339},
year = {2024},
issn = {2451-9294},
doi = {https://doi.org/10.1016/j.chempr.2024.06.026},
url = {https://www.sciencedirect.com/science/article/pii/S245192942400305X},
author = {Heather J. Kulik},
abstract = {Professor Heather J. Kulik is a professor in chemical engineering and chemistry at MIT. She received her BE in chemical engineering from the Cooper Union in 2004 and her PhD from the Department of Materials Science and Engineering at MIT in 2009. She completed postdocs at Lawrence Livermore and Stanford prior to joining MIT as a faculty member in 2013. Her research in computational inorganic chemistry has been recognized by an ONR YIP, a DARPA Director’s fellowship, an NSF CAREER Award, a Sloan Fellowship, an AIChE CoMSEF Impact Award, and a Hans Fischer Senior Fellowship from TU Munich, among others.}
}
@article{FRANTZ2003265,
title = {Herbert Simon. Artificial intelligence as a framework for understanding intuition},
journal = {Journal of Economic Psychology},
volume = {24},
number = {2},
pages = {265-277},
year = {2003},
note = {The Economic Psychology of Herbert A. Simon},
issn = {0167-4870},
doi = {https://doi.org/10.1016/S0167-4870(02)00207-6},
url = {https://www.sciencedirect.com/science/article/pii/S0167487002002076},
author = {Roger Frantz},
keywords = {Herbert Simon, Intuition, Artificial intelligence, Bounded rationality, Economics and psychology},
abstract = {Herbert Simon made overlapping substantive contributions to the fields of economics, psychology, cognitive science, artificial intelligence, decision theory, and organization theory. Simon’s work was motivated by the belief that neither the human mind, human thinking and decision making, nor human creativity need be mysterious. It was after he helped create “thinking” machines that Simon came to understand human intuition as subconscious pattern recognition. In doing so he showed that intuition need not be associated with magic and mysticism, and that it is complementary with analytical thinking. This paper will show how the overlaps in his work and especially his work on AI affected his view towards intuition.}
}
@article{FU20012567,
title = {Analytical and computational description of effect of grain size on yield stress of metals},
journal = {Acta Materialia},
volume = {49},
number = {13},
pages = {2567-2582},
year = {2001},
issn = {1359-6454},
doi = {https://doi.org/10.1016/S1359-6454(01)00062-3},
url = {https://www.sciencedirect.com/science/article/pii/S1359645401000623},
author = {H.-H. Fu and D.J. Benson and M.A. Meyers},
keywords = {Nanocrystalline materials, Grain size, Hall–Petch},
abstract = {Four principal factors contribute to grain-boundary strengthening: (a) the grain boundaries act as barriers to plastic flow; (b) the grain boundaries act as dislocation sources; (c) elastic anisotropy causes additional stresses in grain-boundary surroundings; (d) multislip is activated in the grain-boundary regions, whereas grain interiors are initially dominated by single slip, if properly oriented. As a result, the regions adjoining grain boundaries harden at a rate much higher than grain interiors. A phenomenological constitutive equation predicting the effect of grain size on the yield stress of metals is discussed and extended to the nanocrystalline regime. At large grain sizes, it has the Hall–Petch form, and in the nanocrystalline domain the slope gradually decreases until it asymptotically approaches the flow stress of the grain boundaries. The material is envisaged as a composite, comprised of the grain interior, with flow stress σfG, and grain boundary work-hardened layer, with flow stress σfGB. The predictions of this model are compared with experimental measurements over the mono, micro, and nanocrystalline domains. Computational predictions are made of plastic flow as a function of grain size incorporating differences of dislocation accumulation rate in grain-boundary regions and grain interiors. The material is modeled as a monocrystalline core surrounded by a mantle (grain-boundary region) with a high work hardening rate response. This is the first computational plasticity calculation that accounts for grain size effects in a physically-based manner. A discussion of statistically stored and geometrically necessary dislocations in the framework of strain-gradient plasticity is introduced to describe these effects. Grain-boundary sliding in the nanocrystalline regime is predicted from calculations using the Raj–Ashby model and incorporated into the computations; it is shown to predispose the material to shear localization.}
}
@article{YANG2001167,
title = {Computational verb systems: computing with perceptions of dynamics},
journal = {Information Sciences},
volume = {134},
number = {1},
pages = {167-248},
year = {2001},
note = {Computing with Words},
issn = {0020-0255},
doi = {https://doi.org/10.1016/S0020-0255(01)00096-2},
url = {https://www.sciencedirect.com/science/article/pii/S0020025501000962},
author = {Tao Yang},
abstract = {The concepts and principles of (computational) verb logic, (computational) verb set, (computational) verb number, (computational) verb prediction and (computational) verb control are presented.}
}
@article{ELVEVAG2023115098,
title = {Reflections on measuring disordered thoughts as expressed via language},
journal = {Psychiatry Research},
volume = {322},
pages = {115098},
year = {2023},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2023.115098},
url = {https://www.sciencedirect.com/science/article/pii/S0165178123000513},
author = {Brita Elvevåg},
keywords = {Assessment, Language, Memory},
abstract = {Thought disorder, as inferred from disorganized and incoherent speech, is an important part of the clinical presentation in schizophrenia. Traditional measurement approaches essentially count occurrences of certain speech events which may have restricted their usefulness. Applying speech technologies in assessment can help automate traditional clinical rating tasks and thereby complement the process. Adopting these computational approaches affords clinical translational opportunities to enhance the traditional assessment by applying such methods remotely and scoring various parts of the assessment automatically. Further, digital measures of language may help detect subtle clinically significant signs and thus potentially disrupt the usual manner by which things are conducted. If proven beneficial to patient care, methods where patients’ voice are the primary data source could become core components of future clinical decision support systems that improve risk assessment. However, even if it is possible to measure thought disorder in a sensitive, reliable and efficient manner, there remain many challenges to then translate into a clinically implementable tool that can contribute towards providing better care. Indeed, embracing technology - notably artificial intelligence - requires vigorous standards for reporting underlying assumptions so as to ensure a trustworthy and ethical clinical science.}
}
@article{KLATT2009536,
title = {Perspectives for process systems engineering—Personal views from academia and industry},
journal = {Computers & Chemical Engineering},
volume = {33},
number = {3},
pages = {536-550},
year = {2009},
note = {Selected Papers from the 17th European Symposium on Computer Aided Process Engineering held in Bucharest, Romania, May 2007},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2008.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0098135408001737},
author = {Karsten-Ulrich Klatt and Wolfgang Marquardt},
keywords = {Review, Modeling, Design, Optimization, Control, Operations, Numerical algorithms, Software, Computer-aided process engineering (CAPE)},
abstract = {Process systems engineering (PSE) has been an active research field for almost 50 years. Its major achievements include methodologies and tools to support process modeling, simulation and optimization (MSO). Mature, commercially available technologies have been penetrating all fields of chemical engineering in academia as well as in industrial practice. MSO technologies have become a commodity, they are not a distinguishing feature of the PSE field any more. Consequently, PSE has to reassess and to reposition its future research agenda. Emphasis should be put on model-based applications in all PSE domains including product and process design, control and operations. Furthermore, systems thinking and systems problem solving have to be prioritized rather than the mere application of computational problem solving methods. This essay reflects on the past, present and future of PSE from an academic and industrial point of view. It redefines PSE as an active and future-proof research field which can play an active role in providing enabling technologies for product and process innovations in the chemical industries and beyond.}
}
@article{LEE2005261,
title = {Insights into nucleic acid reactivity through gas-phase experimental and computational studies},
journal = {International Journal of Mass Spectrometry},
volume = {240},
number = {3},
pages = {261-272},
year = {2005},
note = {Mass Spectrometry of Biopolymers: From Model Systems to Ribosomes},
issn = {1387-3806},
doi = {https://doi.org/10.1016/j.ijms.2004.09.020},
url = {https://www.sciencedirect.com/science/article/pii/S1387380604003975},
author = {Jeehiun K. Lee},
keywords = {Nucleic acids, RNA, DNA, Enzyme nucleobase, Acidity, Proton affinity},
abstract = {Accurate measurements of the acidities and basicities of nucleic bases and nucleic base derivatives is essential for understanding issues of fundamental importance in biological systems. Hydrogen bonding modulates recognition of DNA and RNA bases, and the interaction energy between two bonded complementary nucleobases is dependent on the intrinsic basicity and acidity of the acceptor and donor groups. In addition, understanding the intrinsic reactivity of nucleic bases can shed light on key biosynthetic mechanisms for which nucleobases are substrates. In this review, we highlight advances in our lab toward understanding the fundamental reactivity of DNA and RNA. In particular, we focus on our investigation of the gas phase acidities and basicities of natural and unnatural nucleobases, and the implications of our results for the mechanisms of nucleotide biosynthetic and repair enzymes.}
}
@article{KIMSEY1994113,
title = {Parallel computation of impact dynamics},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {119},
number = {1},
pages = {113-121},
year = {1994},
issn = {0045-7825},
doi = {https://doi.org/10.1016/0045-7825(94)00079-4},
url = {https://www.sciencedirect.com/science/article/pii/0045782594000794},
author = {K.D. Kimsey and M.A. Olson},
abstract = {This paper discusses a parallel algorithm and data structures for implementing multimaterial, two-step Eulerian finite difference solution schemes on hypercube architectures. Selected problems in impact dynamics have been modeled on the Connection Machine model CM5, and the results are compared with computational results reported in the literature, as well as direct comparison with experimental data.}
}
@incollection{ESFELD2015131,
title = {Atomism and Holism: Philosophical Aspects},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {131-135},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.63003-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868630039},
author = {Michael Esfeld},
keywords = {Atomism, Collectivism, Confirmation, Externalism, Holism, Human nature, Individualism, Internalism, Meaning, Ontological dependence, Rationality, Rule-following, Thought},
abstract = {In the philosophy of the social sciences, atomism is the view that human beings can be thinking, rational beings independently of social relations. Holism, by contrast, is the view that social relations are essential to human beings insofar as they are thinking, rational beings. This article first provides an overview of different sorts of atomism and holism (see Section Types of Atomism and Holism). It then briefly sketches the historical background of these notions in modern philosophy (Section The Historical Background of Atomism and Holism). The main part is a systematic characterization of atomism and holism (see Section A Characterization of Atomism and Holism) and a summary of the most important arguments for both these positions (see Section Arguments for Atomism and Holism).}
}
@article{TSUTAKAWA2020102972,
title = {Envisioning how the prototypic molecular machine TFIIH functions in transcription initiation and DNA repair},
journal = {DNA Repair},
volume = {96},
pages = {102972},
year = {2020},
issn = {1568-7864},
doi = {https://doi.org/10.1016/j.dnarep.2020.102972},
url = {https://www.sciencedirect.com/science/article/pii/S1568786420302214},
author = {Susan E. Tsutakawa and Chi-Lin Tsai and Chunli Yan and Amer Bralić and Walter J. Chazin and Samir M. Hamdan and Orlando D. Schärer and Ivaylo Ivanov and John A. Tainer},
keywords = {TFIIH, Helicase, Transcription initiation, Transcription-coupled repair, Nucleotide excision repair, XPB, XPD, Translocase, DNA damage, DNA repair},
abstract = {Critical for transcription initiation and bulky lesion DNA repair, TFIIH provides an exemplary system to connect molecular mechanisms to biological outcomes due to its strong genetic links to different specific human diseases. Recent advances in structural and computational biology provide a unique opportunity to re-examine biologically relevant molecular structures and develop possible mechanistic insights for the large dynamic TFIIH complex. TFIIH presents many puzzles involving how its two SF2 helicase family enzymes, XPB and XPD, function in transcription initiation and repair: how do they initiate transcription, detect and verify DNA damage, select the damaged strand for incision, coordinate repair with transcription and cell cycle through Cdk-activating-kinase (CAK) signaling, and result in very different specific human diseases associated with cancer, aging, and development from single missense mutations? By joining analyses of breakthrough cryo-electron microscopy (cryo-EM) structures and advanced computation with data from biochemistry and human genetics, we develop unified concepts and molecular level understanding for TFIIH functions with a focus on structural mechanisms. We provocatively consider that TFIIH may have first evolved from evolutionary pressure for TCR to resolve arrested transcription blocks to DNA replication and later added its key roles in transcription initiation and global DNA repair. We anticipate that this level of mechanistic information will have significant impact on thinking about TFIIH, laying a robust foundation suitable to develop new paradigms for DNA transcription initiation and repair along with insights into disease prevention, susceptibility, diagnosis and interventions.}
}
@article{MATHESON2020116697,
title = {The role of the motor system in generating creative thoughts},
journal = {NeuroImage},
volume = {213},
pages = {116697},
year = {2020},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2020.116697},
url = {https://www.sciencedirect.com/science/article/pii/S1053811920301841},
author = {Heath E. Matheson and Yoed N. Kenett},
keywords = {Motor system, Creativity, Simulations, Embodied cognition, Grounded cognition, Divergent thinking, Improvisation},
abstract = {Neurocognitive research is pertinent to developing mechanistic models of how humans generate creative thoughts. Such models usually overlook the role of the motor cortex in creative thinking. The framework of embodied or grounded cognition suggests that creative thoughts (e.g. using a shoe as a hammer, improvising a piano solo) are partially served by simulations of motor activity associated with tools and their use. The major hypothesis stemming from the embodied or grounded account is that, while the motor system is used to execute actions, simulations within this system also support higher-order cognition, creativity included. That is, the cognitive process of generating creative output, not just executing it, is deeply embedded in motor processes. Here, we highlight a collection of neuroimaging research that implicates the motor system in generating creative thoughts, including some evidence for its functionally necessary role in generating creative output. Specifically, the grounded or embodied framework suggests that generating creative output may, in part, rely on motor simulations of possible actions, and that these simulations may by partially implemented in the motor regions themselves. In such cases, action simulations (i.e. reactivating or re-using the motor system), do not result in overt action but instead are used to support higher-order cognitive goals like generating creative uses or improvising.}
}
@article{OH2023100602,
title = {Making computing visible & tangible: A paper-based computing toolkit for codesigning inclusive computing education activities},
journal = {International Journal of Child-Computer Interaction},
volume = {38},
pages = {100602},
year = {2023},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2023.100602},
url = {https://www.sciencedirect.com/science/article/pii/S2212868923000399},
author = {HyunJoo Oh and Sherry Hsi and Noah Posner and Colin Dixon and Tymirra Smith and Tingyu Cheng},
keywords = {Paper-based computing, Codesign, Inclusive CS education, Learning through making},
abstract = {MCVT (Making Computing Visible and Tangible) Cards are a toolkit of paper-based computing cards intended for use in the codesign of inclusive computing education. Working with groups of teachers and students over multiple design sessions, we share our toolkit, design drivers and material considerations; and use cases drawn from a week-long codesign workshop where seven teachers made and adapted cards for their future classroom facilitation. Our findings suggest that teachers valued the MCVT toolkit as a resource for their own learning and perceived the cards to be useful for supporting new computational practices, specifically for learning through making and connecting to examples of everyday computing. Critically reviewed by teachers during codesign workshops, the toolkit however posed some implementation challenges and constraints for learning through making and troubleshooting circuitry. From teacher surveys, interviews, workshop video recordings, and teacher-constructed projects, we show how teachers codesigned new design prototypes and pedagogical activities while also adapting and extending paper-based computing materials so their students could take advantage of the unique technical and expressive affordances of MCVT Cards. Our design research contributes a new perspective on using interactive paper computing cards as a medium for instructional materials development to support more inclusive computing education.}
}
@article{HUBERMAN19981169,
title = {Computation as economics},
journal = {Journal of Economic Dynamics and Control},
volume = {22},
number = {8},
pages = {1169-1186},
year = {1998},
issn = {0165-1889},
doi = {https://doi.org/10.1016/S0165-1889(98)00008-6},
url = {https://www.sciencedirect.com/science/article/pii/S0165188998000086},
author = {Bernardo A. Huberman},
abstract = {We use computers to study economics, but few people realize that we can use economics to study and design computational systems. The reason is that computer networks can be regarded as a community of processes that in their interactions, strategies and lack of perfect knowledge face the same issues as people in markets. This paper describes how computers have evolved to a point where economics approaches are useful for designing them and understanding their dynamics. Examples are given of existing computer systems that use market mechanisms and of novel phenomena, such as clustered volatility, that we uncovered when studying their evolution.}
}
@article{MORGAN20052564,
title = {The visual computation of 2-D area by human observers},
journal = {Vision Research},
volume = {45},
number = {19},
pages = {2564-2570},
year = {2005},
issn = {0042-6989},
doi = {https://doi.org/10.1016/j.visres.2005.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0042698905002075},
author = {M.J. Morgan},
keywords = {Psychophysics, Shape, Weber fraction},
abstract = {Normal human observers compared either the width, height or area of two simultaneously-presented shapes (the standard and the test), with a cue to indicate which decision had to be made. On ‘area’ trials, test width was a random variable, ensuring that neither shape (aspect ratio), width nor height by themselves was a reliable signal. Weber fractions for width and height of both ellipses and rectangles were in the range 5–10%, but for area they were higher (10–20%) than predicted from the combination of noisy width and height decisions. With ellipses, observers were more likely to overestimate width or height when the other dimension differed from the standard in the same direction (e.g. both greater). We conclude that observers have no access to high-precision codes for 2-D area, and that they base their decisions on a variety of heuristics derived from 1-D codes. A second experiment measured acuity for changes in aspect ratio. For ellipses, accuracy for aspect ratio was higher than predicted by the combination of noisy width and height signals; for rectangles it was worse, suggesting that 2-D curvature is a potent cue to shape.}
}
@article{OLIVEIRA2022102347,
title = {Beyond energy services: A multidimensional and cross-disciplinary agenda for home energy management research},
journal = {Energy Research & Social Science},
volume = {85},
pages = {102347},
year = {2022},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2021.102347},
url = {https://www.sciencedirect.com/science/article/pii/S2214629621004382},
author = {Sonja Oliveira and Lidia Badarnah and Merate Barakat and Anna Chatzimichali and Ed Atkins},
keywords = {Architecture, Biomimetics, Computational design, Cross-disciplinary methods, Home energy management},
abstract = {Home Energy Management (HEM) has a significantly growing impact on strategic energy policy, digital equity, as well as housing development and transport issues. With the proliferation of home working, reliance on electricity for heating and cooling and the increasing needs for electric charging for transportation, there is an urgent need to develop novel ways for efficient management of home energy use. Current efforts focus on HEM technologies at individual household levels, without considering the social or spatial context or their collective community-wide interrelated dependencies. We propose a multifaceted agenda at the intersection of disciplinary domains to tackle this problem by using a multidimensional lens that draws on energy behaviour, architectural research, biomimetics, and computational design, simultaneously. Optimal and effective behavioural patterns can be extracted and abstracted from nature, informing a more collective and interrelated behavioural dependencies approach that considers the complex multidimensional energy use patterns of different housing typologies. This paper discusses the analytical benefits of this new research approach through a study of home energy management behaviour. The approach though could be expanded to consider other similar empirical contexts whereby sustainable multidimensional resource management is sought such as water use, food distribution as well as transport and mobility.}
}
@article{CHEN2014740,
title = {On the Systematic Method to Enhance the Epiphany Ability of Individuals},
journal = {Procedia Computer Science},
volume = {31},
pages = {740-746},
year = {2014},
note = {2nd International Conference on Information Technology and Quantitative Management, ITQM 2014},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.05.322},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914004992},
author = {Ailing Chen and Wei Liu and Zhihui Wu and Jun Zhang},
keywords = {Prototype heuristics, epiphany, extenics, Theory of Creativity, sytematic scheme ;},
abstract = {Epiphany is a crucial stage in the process of creative thinking. The prototype heuristic theory has proved that the individual epiphany ability depends on the individual's ability to get out of the fetter of mental fixation, activate the prototype and acquire the key heuristic information from the activated prototype. Based on this theory, this present research combines the findings of extenics, TRIZ and theory of creativity to have developed a systematic method on enhancing individual epiphany ability. Supported by information technology, the method takes theory of creativity as its methodology, extension strategy generation as its framework, element theory its database and knowledge management its feedback chain. The research aims to cultivate creative thinking and eventually enhance the creativity of individuals.}
}
@article{NOWACK2024459,
title = {Science and reflections: With some thoughts to young applied scientists and engineers},
journal = {Earthquake Science},
volume = {37},
number = {5},
pages = {459-493},
year = {2024},
issn = {1674-4519},
doi = {https://doi.org/10.1016/j.eqs.2024.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1674451924000648},
author = {Robert L. Nowack},
keywords = {geophysics, computational and data science, applied science and engineering},
abstract = {I provide some science and reflections from my experiences working in geophysics, along with connections to computational and data sciences, including recent developments in machine learning. I highlight several individuals and groups who have influenced me, both through direct collaborations as well as from ideas and insights that I have learned from. While my reflections are rooted in geophysics, they should also be relevant to other computational scientific and engineering fields. I also provide some thoughts for young, applied scientists and engineers.}
}
@article{KLOOSTER2024110771,
title = {A systematic review on eHealth technology personalization approaches},
journal = {iScience},
volume = {27},
number = {9},
pages = {110771},
year = {2024},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2024.110771},
url = {https://www.sciencedirect.com/science/article/pii/S2589004224019965},
author = {Iris ten Klooster and Hanneke Kip and Lisette {van Gemert-Pijnen} and Rik Crutzen and Saskia Kelders},
keywords = {Health sciences, Health technology},
abstract = {Summary
Despite the widespread use of personalization of eHealth technologies, there is a lack of comprehensive understanding regarding its application. This systematic review aims to bridge this gap by identifying and clustering different personalization approaches based on the type of variables used for user segmentation and the adaptations to the eHealth technology and examining the role of computational methods in the literature. From the 412 included reports, we identified 13 clusters of personalization approaches, such as behavior + channeling and environment + recommendations. Within these clusters, 10 computational methods were utilized to match segments with technology adaptations, such as classification-based methods and reinforcement learning. Several gaps were identified in the literature, such as the limited exploration of technology-related variables, the limited focus on user interaction reminders, and a frequent reliance on a single type of variable for personalization. Future research should explore leveraging technology-specific features to attain individualistic segmentation approaches.}
}
@article{OXMAN2006229,
title = {Theory and design in the first digital age},
journal = {Design Studies},
volume = {27},
number = {3},
pages = {229-265},
year = {2006},
note = {Digital Design},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2005.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X05000840},
author = {Rivka Oxman},
keywords = {digital design, design theory, design methodology, design thinking},
abstract = {Digital design and its growing impact on design and production practices have resulted in the need for a re-examination of current design theories and methodologies in order to explain and guide future research and development. The present research postulates the requirements for a conceptual framework and theoretical basis of digital design; reviews the recent theoretical and historical background; and defines a generic schema of design characteristics through which the paradigmatic classes of digital design are formulated. The implication of this research for the formulation of ‘digital design thinking’ is presented and discussed.}
}
@article{JOEL2002535,
title = {Actor–critic models of the basal ganglia: new anatomical and computational perspectives},
journal = {Neural Networks},
volume = {15},
number = {4},
pages = {535-547},
year = {2002},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(02)00047-3},
url = {https://www.sciencedirect.com/science/article/pii/S0893608002000473},
author = {Daphna Joel and Yael Niv and Eytan Ruppin},
keywords = {Basal ganglia, Dopamine, Reinforcement learning, Actor–critic, Dimensionality reduction, Evolutionary computation, Behavioral switching, Striosomes/patches},
abstract = {A large number of computational models of information processing in the basal ganglia have been developed in recent years. Prominent in these are actor–critic models of basal ganglia functioning, which build on the strong resemblance between dopamine neuron activity and the temporal difference prediction error signal in the critic, and between dopamine-dependent long-term synaptic plasticity in the striatum and learning guided by a prediction error signal in the actor. We selectively review several actor–critic models of the basal ganglia with an emphasis on two important aspects: the way in which models of the critic reproduce the temporal dynamics of dopamine firing, and the extent to which models of the actor take into account known basal ganglia anatomy and physiology. To complement the efforts to relate basal ganglia mechanisms to reinforcement learning (RL), we introduce an alternative approach to modeling a critic network, which uses Evolutionary Computation techniques to ‘evolve’ an optimal RL mechanism, and relate the evolved mechanism to the basic model of the critic. We conclude our discussion of models of the critic by a critical discussion of the anatomical plausibility of implementations of a critic in basal ganglia circuitry, and conclude that such implementations build on assumptions that are inconsistent with the known anatomy of the basal ganglia. We return to the actor component of the actor–critic model, which is usually modeled at the striatal level with very little detail. We describe an alternative model of the basal ganglia which takes into account several important, and previously neglected, anatomical and physiological characteristics of basal ganglia–thalamocortical connectivity and suggests that the basal ganglia performs reinforcement-biased dimensionality reduction of cortical inputs. We further suggest that since such selective encoding may bias the representation at the level of the frontal cortex towards the selection of rewarded plans and actions, the reinforcement-driven dimensionality reduction framework may serve as a basis for basal ganglia actor models. We conclude with a short discussion of the dual role of the dopamine signal in RL and in behavioral switching.}
}
@article{CHEVRETTE20212024,
title = {The confluence of big data and evolutionary genome mining for the discovery of natural products},
journal = {Natural Product Reports},
volume = {38},
number = {11},
pages = {2024-2040},
year = {2021},
issn = {0265-0568},
doi = {https://doi.org/10.1039/d1np00013f},
url = {https://www.sciencedirect.com/science/article/pii/S0265056822008789},
author = {Marc G. Chevrette and Athina Gavrilidou and Shrikant Mantri and Nelly Selem-Mojica and Nadine Ziemert and Francisco Barona-Gómez},
abstract = {ABSTRACT
This review covers literature between 2003–2021 The development and application of genome mining tools has given rise to ever-growing genetic and chemical databases and propelled natural products research into the modern age of Big Data. Likewise, an explosion of evolutionary studies has unveiled genetic patterns of natural products biosynthesis and function that support Darwin's theory of natural selection and other theories of adaptation and diversification. In this review, we aim to highlight how Big Data and evolutionary thinking converge in the study of natural products, and how this has led to an emerging sub-discipline of evolutionary genome mining of natural products. First, we outline general principles to best utilize Big Data in natural products research, addressing key considerations needed to provide evolutionary context. We then highlight successful examples where Big Data and evolutionary analyses have been combined to provide bioinformatic resources and tools for the discovery of novel natural products and their biosynthetic enzymes. Rather than an exhaustive list of evolution-driven discoveries, we highlight examples where Big Data and evolutionary thinking have been embraced for the evolutionary genome mining of natural products. After reviewing the nascent history of this sub-discipline, we discuss the challenges and opportunities of genomic and metabolomic tools with evolutionary foundations and/or implications and provide a future outlook for this emerging and exciting field of natural product research.}
}
@article{LAKAMSANI1995993,
title = {Mapping molecular dynamics computations on to hypercubes},
journal = {Parallel Computing},
volume = {21},
number = {6},
pages = {993-1013},
year = {1995},
issn = {0167-8191},
doi = {https://doi.org/10.1016/0167-8191(95)00006-A},
url = {https://www.sciencedirect.com/science/article/pii/016781919500006A},
author = {Vamsee Lakamsani and Laxmi N. Bhuyan and D.Scott Linthicum},
keywords = {Mapping problem, Recursive mincut, Molecular dynamics, Compact MD graph, Hypercube},
abstract = {We propose an approach for partitioning an irregular application problem in computational biology called Molecular Dynamics (MD) of Macromolecules. We model the application as a task graph which we call a compact MD graph. Such a modeling allows existing mapping heuristics to be applied to this problem. We then provide a parallel algorithm for this application, by using an efficient mapping heuristic called Allocation By Recursive Mincut (ARM) to map the compact MD graph to a hypercube connected parallel computer, the nCUBE 2S. A canonical model for executing parallel computations modeled as graphs is described. Thus, we attempt to provide the missing link between the mapping research and application implementation research, and demonstrate that the execution time can be sufficiently reduced by considering formal mapping techniques, while designing parallel programs for important applications.}
}
@article{MERRITT2024103670,
title = {Igniting kid power: The impact of environmental service-learning on elementary students' awareness of energy problems and solutions},
journal = {Energy Research & Social Science},
volume = {116},
pages = {103670},
year = {2024},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2024.103670},
url = {https://www.sciencedirect.com/science/article/pii/S2214629624002615},
author = {Eileen G. Merritt and Andrea E. Weinberg and Candace Lapan and Sara E. Rimm-Kaufman},
keywords = {Environmental service-learning, Energy literacy, Elementary students, Science education, Randomized controlled trial},
abstract = {Energy concepts are taught in many schools, but children rarely have an opportunity to grapple with energy problems and work on their own solutions. This study explores the impacts of Connect Science, a service-learning (SL) program developed to enhance elementary students' energy literacy in the United States. Program impacts were explored within the context of a randomized controlled trial. Teachers in the SL intervention group were provided with professional development, coaching and curricular materials. Each fourth grade class chose an energy problem to address, and designed projects to test out a solution. Teachers in a waitlist control group taught their typical energy unit. Upon completion of the unit, students were asked to write about a problem related to energy production or use and propose a potential solution. Inductive content analysis was used to code 703 student responses (377 from control group and 326 from SL group). The majority of students expressed concerns about wasting or using too much electricity or the use of nonrenewable energy sources. Solutions focused on energy conservation and the use of renewable or clean resources were mentioned most frequently overall. Students in the SL group were significantly more likely to mention environmental impacts of various energy sources and to suggest energy conservation solutions or educating others. Conversely, the control group student responses more often focused on electric circuits or electrical safety. Results from this study suggest the promise of environmental SL programs to advance energy literacy and promote critical thinking about how to address energy problems.}
}
@article{ALOUPIS2015135,
title = {Classic Nintendo games are (computationally) hard},
journal = {Theoretical Computer Science},
volume = {586},
pages = {135-160},
year = {2015},
note = {Fun with Algorithms},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2015.02.037},
url = {https://www.sciencedirect.com/science/article/pii/S0304397515001735},
author = {Greg Aloupis and Erik D. Demaine and Alan Guo and Giovanni Viglietta},
keywords = {Nintendo games, Video games, Computational complexity, NP-hardness, PSPACE-hardness},
abstract = {We prove NP-hardness results for five of Nintendo's largest video game franchises: Mario, Donkey Kong, Legend of Zelda, Metroid, and Pokémon. Our results apply to generalized versions of Super Mario Bros. 1–3, The Lost Levels, and Super Mario World; Donkey Kong Country 1–3; all Legend of Zelda games; all Metroid games; and all Pokémon role-playing games. In addition, we prove PSPACE-completeness of the Donkey Kong Country games and several Legend of Zelda games.}
}
@incollection{WU2012223,
title = {10 - Computational modeling and ab initio calculations in MAX phases – II},
editor = {I.M. Low},
booktitle = {Advances in Science and Technology of Mn+1AXn Phases},
publisher = {Woodhead Publishing},
pages = {223-270},
year = {2012},
isbn = {978-1-84569-991-8},
doi = {https://doi.org/10.1533/9780857096012.223},
url = {https://www.sciencedirect.com/science/article/pii/B9781845699918500102},
author = {E. Wu},
keywords = {computational modeling,  calculations, density function theory, energy band, electronic properties, density of states},
abstract = {Abstract:
This chapter reviews the latest researches and advances in the uses of the computational modeling and ab initio calculations on the study of the MAX phases and their properties. The fundamentals and approaches of the density functional theory in the ab initio quantum mechanical calculations and the importance of the theory in the study of the MAX phases are introduced. The studies of the electronic structures and properties, in particular, the energy band structures and total and/or partial density of states of the MAX phases, by using the means of the density function theory are illustrated and discussed. The stability and occurrence of the MAX phases predicted and confirmed by the density functional theory based energetic calculations are addressed. The ab initio calculated elastic and other physical properties of the MAX phases, and the effects of pressure, defects and impurities on the various structural and physical properties are also discussed.}
}
@incollection{MOHAN2025541,
title = {Chapter 51 - Exploring the exciting potential and challenges of brain computer interfaces},
editor = {M.A. Ansari and R.S. Anand and Pragati Tripathi and Rajat Mehrotra and Md Belal Bin Heyat},
booktitle = {Artificial Intelligence in Biomedical and Modern Healthcare Informatics},
publisher = {Academic Press},
pages = {541-550},
year = {2025},
isbn = {978-0-443-21870-5},
doi = {https://doi.org/10.1016/B978-0-443-21870-5.00051-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443218705000510},
author = {Anand Mohan and R.S. Anand},
keywords = {Brain–computer interface (BCI), EEG, Machine learning, Motor imagery, PSD},
abstract = {Electroencephalogram (EEG) signals contain various information about the cognitive thinking, emotion, and thoughts of a person. Verbal communication is the normal form of interaction method used, but various kinds of physically disabled people who are not in the condition to express themselves can be assisted using the EEG signal rehabilitation technique. EEG signals can be used effectively in rehabilitation by using brain–computer interfaces (BCIs). BCI is a technology that allows interaction between the brain and a computer. This kind of technique can be used to treat patients with paralyzed muscles and locked in syndromes by helping them interact with others using their EEG signals. The application of BCI can be in medical field, education, and security. In this chapter, all aspects of BCIs are discussed in great detail and also have worked on motor imaginary-based dataset and have used linear discriminant analysis (LDA) algorithm as the classifier, which showed 91% accuracy.}
}
@article{BENTON20001135,
title = {Computational modelling of interleaved first- and second-order motion sequences and translating 3f+4f beat patterns},
journal = {Vision Research},
volume = {40},
number = {9},
pages = {1135-1142},
year = {2000},
issn = {0042-6989},
doi = {https://doi.org/10.1016/S0042-6989(00)00026-2},
url = {https://www.sciencedirect.com/science/article/pii/S0042698900000262},
author = {Christopher P. Benton and Alan Johnston and Peter W. McOwan},
keywords = {Motion perception, Computational modelling, Second-order, Feature tracking},
abstract = {Despite detailed psychophysical, neurophysiological and electrophysiological investigation, the number and nature of independent and parallel motion processing mechanisms in the visual cortex remains controversial. Here we use computational modelling to evaluate evidence from two psychophysical studies collectively thought to demonstrate the existence of three separate and independent motion processing channels. We show that the pattern of psychophysical results can largely be accounted for by a single mechanism. The results demonstrate that a low-level luminance based approach can potentially provide a wider account of human motion processing than generally thought possible.}
}
@article{ACISECHE2023109386,
title = {A perspective on the sharing of docking data},
journal = {Data in Brief},
volume = {49},
pages = {109386},
year = {2023},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2023.109386},
url = {https://www.sciencedirect.com/science/article/pii/S2352340923004985},
author = {Samia Aci-Sèche and Stéphane Bourg and Pascal Bonnet and Joseph Rebehmed and Alexandre G. {de Brevern} and Julien Diharce},
keywords = {3D coordinates, Docking, Files, SDF, Sharing, FAIR principles},
abstract = {Computational approaches are nowadays largely applied in drug discovery projects. Among these, molecular docking is the most used for hit identification against a drug target protein. However, many scientists in the field shed light on the lack of availability and reproducibility of the data obtained from such studies to the whole community. Consequently, sustaining and developing the efforts toward a large and fully transparent sharing of those data could be beneficial for all researchers in drug discovery. The purpose of this article is first to propose guidelines and recommendations on the appropriate way to conduct virtual screening experiments and second to depict the current state of sharing molecular docking data. In conclusion, we have explored and proposed several prospects to enhance data sharing from docking experiment that could be developed in the foreseeable future.}
}