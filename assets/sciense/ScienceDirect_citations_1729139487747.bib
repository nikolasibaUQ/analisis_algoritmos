@incollection{ASCHEID200711,
title = {Chapter 2 - Opportunities for Application-Specific Processors: The Case of Wireless Communications},
editor = {Paolo Lenne and Rainer Leupers},
booktitle = {Customizable Embedded Processors},
publisher = {Morgan Kaufmann},
address = {Burlington},
pages = {11-37},
year = {2007},
series = {Systems on Silicon},
issn = {18759661},
doi = {https://doi.org/10.1016/B978-012369526-0/50003-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780123695260500036},
author = {Gerd Ascheid and Heinrich Meyr},
abstract = {Publisher Summary
A paradigm change in designing complex systems-on-chip (SoCs) occurs roughly every 12 years because of the exponentially increasing number of transistors on a chip. This paradigm change is characterized by a move to a higher level of abstraction. Instead of thinking in register-transfer level (RTL) blocks and wires, computing elements and interconnect are needed to be thought. The next design discontinuity will lead to different solutions, depending on the application. The following core propositions for wireless communications are made: future SoC for wireless communications will be heterogeneous, reconfigurable Multi-Processor System-on-Chip (MPSoC). They will contain computational elements that cover the entire spectrum, from fixed functionality blocks to domain-specific DSPs and general-purpose processors. A key role will be played by ASIPs. ASIPs exploit the full architectural space (memory, interconnect, instruction set, parallelism), so they are optimally matched to a specific task. The heterogeneous computational elements will communicate via a network-on-chip (NoC), as the conventional bus structures do not scale. These MPSoC platforms will be designed by a cross-disciplinary team. This chapter substantiates this proposition. It begins by analyzing the properties of future wireless communication systems and observes that the systems are computationally demanding. Furthermore, they need innovative architectural concepts to be energy efficient. The chapter discusses the canonical structure of a digital receiver for wireless communication and addresses the design of ASIPs.}
}
@article{JONES2013122,
title = {Understanding the integral: Students’ symbolic forms},
journal = {The Journal of Mathematical Behavior},
volume = {32},
number = {2},
pages = {122-141},
year = {2013},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2012.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732312312000612},
author = {Steven R. Jones},
keywords = {Calculus, Integral, Student understanding, Undergraduate mathematics education, Symbolic form, Accumulation},
abstract = {Researchers are currently investigating how calculus students understand the basic concepts of first-year calculus, including the integral. However, much is still unknown regarding the cognitive resources (i.e., stable cognitive units that can be accessed by an individual) that students hold and draw on when thinking about the integral. This paper presents cognitive resources of the integral that a sample of experienced calculus students drew on while working on pure mathematics and applied physics problems. This research provides evidence that students hold a variety of productive cognitive resources that can be employed in problem solving, though some of the resources prove more productive than others, depending on the context. In particular, conceptualizations of the integral as an addition over many pieces seem especially useful in multivariate and physics contexts.}
}
@article{PANESCU2013375,
title = {At the Crossroads between Western and Eastern Views on Psychotherapy: An Integrative Approach},
journal = {Procedia - Social and Behavioral Sciences},
volume = {78},
pages = {375-379},
year = {2013},
note = {PSIWORLD 2012},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2013.04.314},
url = {https://www.sciencedirect.com/science/article/pii/S1877042813008835},
author = {Oana Pănescu and Alexandra Timofte and Melania Macovei and Carmen Popescu},
keywords = {Psychoterapy, Body, Mind, Meditation, Transactional analysis},
abstract = {This paper aims at indicating the convergence points between what is habitually understood as a pair of opposing terms: mind (as in thinking) and body (as in sensation). Structural models in psychotherapy conceptualize human mind in terms of levels of information processing (both internal and external information). We suggest that a mental split between mind and body leads to a feeling of estrangement from self, as well as an estrangement from external world. Drawing on relational approaches on psychotherapy, we suggest that focusing on perceiving own sensations does not necessarily imply a state of personal isolation from outside world; rather, this simultaneously means the perceiving and acceptance of “otherness”.}
}
@article{BUHLER1990577,
title = {The COIN model for concurrent computation and its implementation},
journal = {Microprocessing and Microprogramming},
volume = {30},
number = {1},
pages = {577-584},
year = {1990},
note = {Proceedings Euromicro 90: Hardware and Software in System Engineering},
issn = {0165-6074},
doi = {https://doi.org/10.1016/0165-6074(90)90302-P},
url = {https://www.sciencedirect.com/science/article/pii/016560749090302P},
author = {Peter Buhler},
abstract = {COIN is a model for object-oriented programming with special emphasis on concurrent and distributed systems. It was developed to integrate design, implementation, and visualization of distributed applications. The distinguishing characteristics of COIN are: a) hierarchial object structures; b) multiple explicit object interfaces; c) explicit and dynamic binding of interface operations to operation implementations; d) generation of structures consisting of several objects and their interconnections as an atomic action. The paper gives an overview of the COIN model and its implementation in the COIN/L programming language.}
}
@article{HUANG2024369,
title = {A linear-attention-combined convolutional neural network for EEG-based visual stimulus recognition},
journal = {Biocybernetics and Biomedical Engineering},
volume = {44},
number = {2},
pages = {369-379},
year = {2024},
issn = {0208-5216},
doi = {https://doi.org/10.1016/j.bbe.2024.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0208521624000299},
author = {Junjie Huang and Wanzhong Chen and Tao Zhang},
keywords = {Brain–computer interface (BCI), Convolutional neural network (CNN), Electroencephalogram (EEG), Linear attention mechanism, Visual stimulus recognition},
abstract = {The recognition task of visual stimuli based on EEG (Electroencephalogram) has become a major and important topic in the field of Brain–Computer Interfaces (BCI) research. Although the underlying spatial features of EEG can effectively represent visual stimulus information, it still remains a highly challenging task to explore the local–global information of the underlying EEG to achieve better decoding performance. Therefore, in this paper we propose a deep learning architecture called Linear-Attention-combined Convolutional Neural Network (LACNN) for visual stimuli EEG-based classification task. The proposed architecture combines the modules of Convolutional Neural Networks (CNN) and Linear Attention, effectively extracting local and global features of EEG for decoding while maintaining low computational complexity and model parameters. We conducted extensive experiments on a public EEG dataset from the Stanford Digital Repository. The experimental results demonstrate that LACNN achieves an average decoding accuracy of 54.13% and 29.83% in 6-category and 72-exemplar classification tasks respectively, outperforming the state-of-the-art methods, which indicates that our method can effectively decode visual stimuli from EEG. Further analysis of LACNN shows that the Linear Attention module improves the separability between different category features and localizes key brain region information that aligns with the paradigm principles.}
}
@article{HUNT201645,
title = {Levels of participatory conception of fractional quantity along a purposefully sequenced series of equal sharing tasks: Stu's trajectory},
journal = {The Journal of Mathematical Behavior},
volume = {41},
pages = {45-67},
year = {2016},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2015.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732312315300122},
author = {Jessica H. Hunt and Arla Westenskow and Juanita Silva and Jasmine Welch-Ptak},
keywords = {Conceptions, Cognition, Learning disabilities, Rational number, Pedagogy, Constructivism},
abstract = {Current intervention research in special education focuses on children's responsiveness to teacher modeled strategies and not conceptual development within children's thinking. As a result, there is a need for research that provides a characterization of key understandings (KUs) of fractional quantity evidenced by children with learning disabilities (LD) and how growth of conceptual knowledge may occur within these children's mathematical activity. This case study extends current literature by presenting KUs of fractional quantity, evidenced through problem solving strategies, observable operations, and naming/quantification of one fifth grader with LD before, during, and after seven instructional sessions situated in equal sharing. The researchers utilized a characterization of evolving fraction conceptions developed from research of children without disabilities that was ultimately productive in facilitating conceptual advances of the child with LD. We hypothesize that the trajectory of the child's conceptions is a case of something more general. Pending future research, the trajectory may be a useful tool to practitioners wishing to plan thoughtful, conceptually-based fraction instruction that is responsive to all children's evolving conceptions of fractions as quantities built through their own mathematical activity.}
}
@article{YANG2022101239,
title = {Identifying keyword sleeping beauties: A perspective on the knowledge diffusion process},
journal = {Journal of Informetrics},
volume = {16},
number = {1},
pages = {101239},
year = {2022},
issn = {1751-1577},
doi = {https://doi.org/10.1016/j.joi.2021.101239},
url = {https://www.sciencedirect.com/science/article/pii/S1751157721001103},
author = {Jinqing Yang and Yi Bu and Wei Lu and Yong Huang and Jiming Hu and Shengzhi Huang and Li Zhang},
keywords = {Sleeping beauty, Delayed recognition, Knowledge diffusion trajectory, Survival analysis},
abstract = {Knowledge diffusion is a significant driving force behind discipline development and technological innovation. Keyword is a unique knowledge diffusion trajectory, in which the sleeping beauty phenomenon sometimes appears. In this paper, we first put forward the concept of Keyword Sleeping Beauties (KSBs) on the basis of the scientific literature phenomenon of sleeping beauties. Then, we construct a parameter-free identification method to distinguish KSBs based on beauty coefficient criteria. Furthermore, we analyze the intrinsic and extrinsic influencing factors to explore the awakening mechanism of KSBs. The experiment results show that sleeping beauty phenomena also exist in the keyword diffusion trajectory and 284 KSBs are identified. The depth of sleep has a positive correlation with awakening intensity, while the length of sleep has a negative correlation with awakening intensity. In the two years of pre-awakening, KSBs tend to appear in the journals with a higher impact factor. In addition, the adoption frequency and the number of KSBs both increase obviously in the one year of pre-awakening. The findings of this paper enrich the patterns of knowledge diffusion and extend academic thinking on the sleeping beauty in science.}
}
@article{FIRMIN2024128483,
title = {Parallel hyperparameter optimization of spiking neural networks},
journal = {Neurocomputing},
volume = {609},
pages = {128483},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128483},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224012542},
author = {Thomas Firmin and Pierre Boulet and El-Ghazali Talbi},
keywords = {Spiking neural networks, Hyperparameter optimization, Parallel asynchronous optimization, Bayesian optimization, STDP, SLAYER},
abstract = {Hyperparameter optimization of spiking neural networks (SNNs) is a difficult task which has not yet been deeply investigated in the literature. In this work, we designed a scalable constrained Bayesian based optimization algorithm that prevents sampling in non-spiking areas of an efficient high dimensional search space. These search spaces contain infeasible solutions that output no or only a few spikes during the training or testing phases, we call such a mode a “silent network”. Finding them is difficult, as many hyperparameters are highly correlated to the architecture and to the dataset. We leverage silent networks by designing a spike-based early stopping criterion to accelerate the optimization process of SNNs trained by spike timing dependent plasticity and surrogate gradient. We parallelized the optimization algorithm asynchronously, and ran large-scale experiments on heterogeneous multi-GPU Petascale architecture. Results show that by considering silent networks, we can design more flexible high-dimensional search spaces while maintaining a good efficacy. The optimization algorithm was able to focus on networks with high performances by preventing costly and worthless computation of silent networks.}
}
@incollection{LU2024173,
title = {Chapter 13 - Collection and transmission planning for large offshore wind power base},
editor = {Zongxiang Lu and Haibo Li and Ying Qiao and Le Xie and Chanan Singh},
booktitle = {Power System Flexibility},
publisher = {Academic Press},
pages = {173-191},
year = {2024},
isbn = {978-0-323-99517-7},
doi = {https://doi.org/10.1016/B978-0-323-99517-7.00016-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323995177000166},
author = {Zongxiang Lu and Haibo Li and Ying Qiao and Le Xie and Chanan Singh},
keywords = {Mid- to offshore wind power, hierarchical planning, steepest descent method, minimum spanning tree algorithm, transmission equipment selection, improved ant colony algorithm},
abstract = {Offshore wind power planning in China thus far has generally inherited the principles and thinking of onshore wind power planning. Mid- to offshore wind power access could incur a cost far higher than that of onshore wind generation, accounting for 15%–30% of the total investment. Sea state resources, corridor resources, landing conditions, and submarine cable wiring all face materially contrasting constraints to onshore wind power, necessitating dedicated planning efforts for offshore wind power. In this chapter, a planning framework for the sequential and cascaded development of mid- to offshore wind power bases is proposed, and a tiered master planning approach featuring on-site, cluster, and AC/DC transmission is established, filling the gap in transmission planning of large-scale mid- to offshore wind farm clusters. Firstly, location optimization of offshore hub substations based on a steepest descent method is established and explaining its specific implementation methods. Secondly, an improved minimum spanning tree algorithm is used to find a topology connection method for the collector system with the optimal length of submarine cable, which makes the cost optimal. Finally, the optimization model of transmission equipment selection considering the risk of high wind speed truncation is established, and the topology optimization modeling method of offshore wind power cluster transmission systems based on the improved ant colony optimization algorithm is proposed.}
}
@article{KRUIJVER2022102748,
title = {The number of alleles in DNA mixtures with related contributors},
journal = {Forensic Science International: Genetics},
volume = {61},
pages = {102748},
year = {2022},
issn = {1872-4973},
doi = {https://doi.org/10.1016/j.fsigen.2022.102748},
url = {https://www.sciencedirect.com/science/article/pii/S1872497322000898},
author = {Maarten Kruijver and James M. Curran},
keywords = {DNA mixtures, Probabilistic genotyping},
abstract = {The maximum allele count (MAC) across loci and the total allele count (TAC) are often used to gauge the number of contributors to a DNA mixture. Computational strategies that predict the total number of alleles in a mixture arising from a certain number of contributors of a given population have been developed. Previous work considered the restricted case where all of the contributors to a mixture are unrelated. We relax this assumption and allow mixture contributors to be related according to a pedigree. We introduce an efficient computational strategy. This strategy based on first determining a probability distribution on the number of independent alleles per locus, and then conditioning on this distribution to compute a distribution of the number of distinct alleles per locus. The distribution of the number of independent alleles per locus is obtained by leveraging the Identical by Descent (IBD) pattern distribution which can be computed from the pedigree. We explain how allelic dropout and a subpopulation correction can be accounted for in the calculations.}
}
@article{ADHIKARI20121374,
title = {Multi-commodity network flow models for dynamic energy management – Smart Grid applications},
journal = {Energy Procedia},
volume = {14},
pages = {1374-1379},
year = {2012},
note = {2011 2nd International Conference on Advances in Energy Engineering (ICAEE)},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2011.12.1104},
url = {https://www.sciencedirect.com/science/article/pii/S1876610211045243},
author = {R.S. Adhikari and N. Aste and M. Manfren},
keywords = {Dynamic energy management, Smart Grid, Multi-commodity network flow models},
abstract = {The strong interconnection between human activities, energy use and pollution reduction strategies in contemporary society has determined the necessity of collecting scientific knowledge from different fields to provide useful methods and models to foster the transition towards more sustainable energy systems. This is a challenging task in particular for contemporary communities where an increasing demand for services is combined with rapidly changing lifestyles and habits. The Smart Grid concept is the result of a confluence of issues and a convergence of objectives, which include national energy security, climate change, pollution reduction, grid reliability, etc. While thinking about a paradigm shift in energy systems, drivers, characteristics, market segments, applications and other interconnected aspects must be taken into account simultaneously. In this context, the use of multi-commodity network flow models for dynamic energy management aims at finding a compromise between model usefulness, accuracy, flexibility, solvability and scalability in Smart Grid applications.}
}
@incollection{RENNE2022147,
title = {Chapter 8 - Measuring and assessing resilience},
editor = {John L. Renne and Brian Wolshon and Anurag Pande and Pamela Murray-Tuite and Karl Kim},
booktitle = {Creating Resilient Transportation Systems},
publisher = {Elsevier},
pages = {147-192},
year = {2022},
isbn = {978-0-12-816820-2},
doi = {https://doi.org/10.1016/B978-0-12-816820-2.00005-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128168202000050},
author = {John L. Renne and Brian Wolshon and Anurag Pande and Pamela Murray-Tuite and Karl Kim},
keywords = {Resilience, Measurement, Assessment, Goals, Metrics, Data collection},
abstract = {The ability to effectively apply resilience-oriented thinking into practice starts with understanding, measuring, and evaluating the benefits and costs of resilience. With this knowledge it also becomes possible to comparatively assess potential planning, design, and maintenance options to plan and allocate financial and personnel resources most effectively to address needs. Other key components of practical and meaningful measurements and assessments of resilience are establishing metrics that quantify its performance and knowing what and how much data to collect. Then, understanding what these data mean so that goals, objectives, and expectations of resilience can be set, both within transportation organizations and for the consumers of the services they provide. Unfortunately, there is no universal agreement on what resilience even is, let alone how to systematically measure and assess it. However, recent reviews of practice and research show that ideas and methods to evaluate and assess resilience are evolving at a rapid pace, both within and outside of transportation. This chapter presents a summary of these ideas and compares and contrasts the effort they require to implement and the benefits they are expected to bring.}
}
@incollection{MARR1988534,
title = {A computational theory of human stereo vision††M.I.T. Psychology Department, 79 Amherst Street, Cambridge Ma 02139, U.S.A.},
editor = {Allan Collins and Edward E. Smith},
booktitle = {Readings in Cognitive Science},
publisher = {Morgan Kaufmann},
pages = {534-547},
year = {1988},
isbn = {978-1-4832-1446-7},
doi = {https://doi.org/10.1016/B978-1-4832-1446-7.50046-7},
url = {https://www.sciencedirect.com/science/article/pii/B9781483214467500467},
author = {D. MARR and T. POGGIO}
}
@article{FALZON2006629,
title = {Using Bayesian network analysis to support centre of gravity analysis in military planning},
journal = {European Journal of Operational Research},
volume = {170},
number = {2},
pages = {629-643},
year = {2006},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2004.06.028},
url = {https://www.sciencedirect.com/science/article/pii/S0377221704005156},
author = {Lucia Falzon},
keywords = {Military, Decision analysis, Probabilistic models, Bayesian networks},
abstract = {Centre of gravity (COG) analysis is an integral and cognitively demanding aspect of military operational planning. It involves identifying the enemy and friendly COG and subsequently determining the critical vulnerabilities that have to be degraded or negated to influence the COG of each side. This paper describes a modelling framework based on the causal relationships among the critical capabilities and requirements for an operation. The framework is subsequently used as a basis for the construction, population and analysis of Bayesian networks to support a rigorous and systematic approach to COG analysis. The importance of this work is that it uses existing planning process concepts to facilitate the construction of comprehensive models in which uncertainties and subjective judgements are clearly represented, thus enabling future re-use and traceability. The visual representation of the COG causal structure helps to clarify thinking and provides a way to record and impart this thinking. Moreover, it gives planners the capability to perform impact analysis, that is, to determine which actions are most likely to achieve a desirable end-state. The paper discusses the methodology, development and implementation of the COG Network Effects Tool (COGNET) suite for model population and model checking as well as impact analysis.}
}
@article{SHARMA201524,
title = {Urban greenways: Operationalizing design syntax and integrating mathematics and science in design},
journal = {Frontiers of Architectural Research},
volume = {4},
number = {1},
pages = {24-34},
year = {2015},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2014.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S2095263514000727},
author = {Archana Sharma},
keywords = {Design thinking, Syntax, Greenway, Urban, Planning, Landscape, STEM integrated design, Inter-disciplinary},
abstract = {The ubiquitous sameness of urban greenways prompts questions on generative design grammar and syntax, whether creative, critical rethinking at that level might be lacking. However the design syntax of urban greenways is not explicitly discussed thus leaving a critical gap in knowledge. This paper begins tackling the larger question by acting on the fundamental subset of it, by operationalizing the design syntax of urban greenways. This is done through mathematics-based graph studies to analyze patterns and shapes, photography based thermal, material and morphology studies, and section analyses to make imagery-derived deductions on the design syntax. Recommendation on approaches to diversify and enrich the design syntax includes a more direct reference from ecosystem science theories such for siting and planning the urban greenways at macro- to meso-scale, a mixed-method approach, combining mathematics, photography and drawings based frames for analyses at meso-, to micro-scale, and a turtle view scale for designing at meso- to micro-scale, with an emphasis on latter.}
}
@article{AGNOLI2020116385,
title = {Predicting response originality through brain activity: An analysis of changes in EEG alpha power during the generation of alternative ideas},
journal = {NeuroImage},
volume = {207},
pages = {116385},
year = {2020},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2019.116385},
url = {https://www.sciencedirect.com/science/article/pii/S1053811919309760},
author = {Sergio Agnoli and Marco Zanon and Serena Mastria and Alessio Avenanti and Giovanni Emanuele Corazza},
keywords = {EEG, Alpha power, Originality, Idea generation, Divergent-thinking, Temporal dynamics, Creativity},
abstract = {Growing neurophysiological evidence points to a role of alpha oscillations in divergent thinking (DT). In particular, studies have shown a consistent EEG alpha synchronization during performance on the Alternative Uses Task (AUT), a well-established DT task. However, there is a need for investigating the brain dynamics underlying the production of a sequence of multiple, alternative ideas at the AUT and their relationship with idea originality. In twenty young adults, we investigated changes in alpha power during performance on a structured version of the AUT, requiring to ideate four alternative uses for conventional objects in distinct and sequentially balanced time periods. Data analysis followed a three-step approach, including behaviour aspects, physiology aspects, and their mutual relationship. At the behavioural level, we observed a typical serial order effect during DT production, with an increase of originality associated with an increase in ideational time and a decrease in response percentage over the four responses. This pattern was paralleled by a shift from alpha desynchronization to alpha synchronization across production of the four alternative ideas. Remarkably, alpha power changes were able to explain response originality, with a differential role of alpha power over different sensor sites. In particular, alpha synchronization over frontal, central, and temporal sites was able to predict the generation of original ideas in the first phases of the DT process, whereas alpha synchronization over centro-parietal sites persistently predicted response originality during the entire DT production. Moreover, a bilateral hemispheric effect in frontal sites and a left-lateralized effect in central, temporal, and parietal sensor sites emerged as predictors of the increase in response originality. These findings highlight the temporal dynamics of DT production across the generation of alternative ideas and support a partially distinct functional role of specific cortical areas during DT.}
}
@article{MITTAL2023105092,
title = {The method of harmonic balance for the Giesekus model under oscillatory shear},
journal = {Journal of Non-Newtonian Fluid Mechanics},
volume = {321},
pages = {105092},
year = {2023},
issn = {0377-0257},
doi = {https://doi.org/10.1016/j.jnnfm.2023.105092},
url = {https://www.sciencedirect.com/science/article/pii/S0377025723001040},
author = {Shivangi Mittal and Yogesh M. Joshi and Sachin Shanbhag},
keywords = {LAOS, Spectral method, Fourier series, Numerical method},
abstract = {The method of harmonic balance (HB) is a spectrally accurate method used to obtain periodic steady state solutions to dynamical systems subjected to periodic perturbations. We adapt HB to solve for the stress response of the Giesekus model under large amplitude oscillatory shear (LAOS) deformation. HB transforms the system of differential equations to a set of nonlinear algebraic equations in the Fourier coefficients. Convergence studies find that the difference between the HB and true solutions decays exponentially with the number of harmonics (H) included in the ansatz as e−mH. The decay coefficient m decreases with increasing strain amplitude, and exhibits a “U” shaped dependence on applied frequency. The computational cost of HB increases slightly faster than linearly with H. The net result of rapid convergence and modest increase in computational cost with increasing H implies that HB outperforms the conventional method of using numerical integration to solve differential constitutive equations under oscillatory shear. Numerical experiments find that HB is simultaneously about three orders of magnitude cheaper, and several orders of magnitude more accurate than numerical integration. Thus, it offers a compelling value proposition for parameter estimation or model selection.}
}
@article{KORDAKI2017122,
title = {Digital card games in education: A ten year systematic review},
journal = {Computers & Education},
volume = {109},
pages = {122-161},
year = {2017},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2017.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S036013151730043X},
author = {Maria Kordaki and Anthi Gousiou},
keywords = {Applications in subject areas, Interactive learning environments, Pedagogical issues, Review, Digital card games},
abstract = {This paper presents a 10-year review study that focuses on the investigation of the use of Digital Card Games (DCGs) as learning tools in education. Specific search terms keyed into 10 large scientific electronic databases identified 50 papers referring to the use of non-commercial DCGs in education during the last decade (2003–2013). The findings revealed that the DCGs reported in the reviewed papers: (a) were used for the learning of diverse subject disciplines across all educational levels and leaning towards the school curriculum, in two ways: game-construction and game-play, (b) were mainly proposed by their designers as meaningful, familiar and appealing learning contexts, in order to motivate and engage players/students and also to promote social, rich and constructivist educational experiences while at the same time integrating modern technologies and innovative gamed-based approaches, (c) were implemented using a plethora of digital tools, (d) mainly adopted social and constructivist views of learning during their design and use, although the views were explicitly reported in only a few of these, (e) were evaluated – in more than half of the studies – with positive results in terms of: student learning, attitudes towards DCGs and enrichment of social interaction and collaboration, (f) appeared to support students to acquire essential thinking skills through DCG-play. However, despite the rich DCG-game experiences reported in the reviewed papers, some essential but under-researched topics were also specified.}
}
@article{LEITE20231,
title = {Interval incremental learning of interval data streams and application to vehicle tracking},
journal = {Information Sciences},
volume = {630},
pages = {1-22},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.02.027},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523002165},
author = {Daniel Leite and Igor Škrjanc and Sašo Blažič and Andrej Zdešar and Fernando Gomide},
keywords = {Granular machine learning, Online learning, Granular computing, Interval analysis, Data stream},
abstract = {This paper presents a method called Interval Incremental Learning (IIL) to capture spatial and temporal patterns in uncertain data streams. The patterns are represented by information granules and a granular rule base with the purpose of developing explainable human-centered computational models of virtual and physical systems. Fundamentally, interval data are either included into wider and more meaningful information granules recursively, or used for structural adaptation of the rule base. An Uncertainty-Weighted Recursive-Least-Squares (UW-RLS) method is proposed to update affine local functions associated with the rules. Online recursive procedures that build interval-based models from scratch and guarantee balanced information granularity are described. The procedures assure stable and understandable rule-based modeling. In general, the model can play the role of a predictor, a controller, or a classifier, with online sample-per-sample structural adaptation and parameter estimation done concurrently. The IIL method is aligned with issues and needs of the Internet of Things, Big Data processing, and eXplainable Artificial Intelligence. An application example concerning real-time land-vehicle localization and tracking in an uncertain environment illustrates the usefulness of the method. We also provide the Driving Through Manhattan interval dataset to foster future investigation.}
}
@article{LIU2023100050,
title = {Literature review of digital twin technologies for civil infrastructure},
journal = {Journal of Infrastructure Intelligence and Resilience},
volume = {2},
number = {3},
pages = {100050},
year = {2023},
issn = {2772-9915},
doi = {https://doi.org/10.1016/j.iintel.2023.100050},
url = {https://www.sciencedirect.com/science/article/pii/S2772991523000257},
author = {Cheng Liu and Peining Zhang and Xuebing Xu},
keywords = {Digital Twin, Civil Infrastructure, Bridges, High-speed Railway},
abstract = {Currently, there are numerous drawbacks associated with infrastructure health monitoring and management, such as inefficiency, subpar real-time functionality, demanding data requirements, and high cost. Digital twin (DT), a hybrid of a computational simulation and an actual physical system, has been proposed to overcome these challenges and become increasingly popular for modeling civil infrastructure systems. This literature review summarized different methods to build digital twins in civil infrastructure. In addition, this review also introduced the current progress of digital twins in different infrastructure sectors, including smart cities and urban spaces, transport systems, and energy systems, along with detailed examples of their various applications. Finally, the current challenges in digital twin technologies for civil infrastructure are also highlighted.}
}
@article{QAMMAR2023e16230,
title = {Statistical analysis of the university sustainability in the higher education institution a case study from the Khyber Pakhtunkhwa province in Pakistan},
journal = {Heliyon},
volume = {9},
number = {5},
pages = {e16230},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e16230},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023034370},
author = {Naseha Wafa Qammar and Zohaib Ur {Rehman Afridi} and Shamaima Wafa Qammar},
keywords = {Sustainability, Higher education institute, Students and faculty, Pakistan},
abstract = {Educational institutions can incorporate the idea of sustainability at the grass root level for any society. This study is part of an effort to get insight into the campus sustainability in one of the Higher Education Institution (HEI) in the Khyber Pakhtunkhwa region of Pakistan. Aim is to investigation university students' and faculty members insight regarding sustainability. Thus, questionnaire-based survey followed by statistical inference was conducted for the potential outcomes. The questionnaire is comprised of 24 questions, 05 of which are on demographics and the remaining 19 are about sustainability. The sustainability related questions focused mostly on the respondents' knowledge, understanding, and interest in sustainability. A handful of the other questions in the questionnaire were tailored to the university input to achieve sustainability. The dataset is manipulated with basic statistical and computational approaches, and the results are analyzed using mean values. The mean values are further classified into flag values of 0 and 1. Flag value 1 indicates a good marker of the received response, while flag value 0 indicates the least amount of information included in responses. The results show that respondents' knowledge, awareness, interest, and engagement in sustainability are significantly sufficient, as we obtained a flag value of 1 for all questions about sustainability. The study's findings, on the other hand, indicated that the institution is lagging in terms of supporting, disseminating, and implementing campus-wide sustainability-related activities. This study is one of the first initiatives to provide a baseline dataset and substantial information to go a step further in achieving the bottom-line target of being and acting sustainable in the HEI.}
}
@article{EBBY200573,
title = {The powers and pitfalls of algorithmic knowledge: a case study},
journal = {The Journal of Mathematical Behavior},
volume = {24},
number = {1},
pages = {73-87},
year = {2005},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2004.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0732312304000768},
author = {Caroline Brayer Ebby},
keywords = {Algorithms, Computation, Learning, Student understanding, Sociocultural perspective, Reform curriculum},
abstract = {This study examines one child's use of computational procedures over a period of 3 years in an urban elementary school where teachers were using a standards-based curriculum. From a sociocultural perspective, the use of standard algorithms to solve mathematical problems is viewed as a cultural tool that both enables and constrains particular practices. As this student appropriated and mastered procedures for addition, subtraction, multiplication and division, she could solve problems that involved fairly straightforward computations or where she could easily model the action to determine an appropriate computation. At the same time, her use of these algorithms, along with other readily available tools, such as her fingers or multiplication tables, constrained her ability to reflect on the tens-structure of the number system, an effect that had serious consequences for her overall mathematical achievement. The results of this study suggest that even when not directly introduced, algorithms have such strong currency that they can mediate more reform-oriented instruction.}
}
@article{DUAL2024596,
title = {The Future of Durable Mechanical Circulatory Support: Emerging Technological Innovations and Considerations to Enable Evolution of the Field},
journal = {Journal of Cardiac Failure},
volume = {30},
number = {4},
pages = {596-609},
year = {2024},
issn = {1071-9164},
doi = {https://doi.org/10.1016/j.cardfail.2024.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S1071916424000411},
author = {Seraina A. Dual and Jennifer Cowger and Ellen Roche and Aditi Nayak},
keywords = {Mechanical circulatory support, left ventricular assist device, translation, innovation},
abstract = {The field of durable mechanical circulatory support (MCS) has undergone an incredible evolution over the past few decades, resulting in significant improvements in longevity and quality of life for patients with advanced heart failure. Despite these successes, substantial opportunities for further improvements remain, including in pump design and ancillary technology, perioperative and postoperative management, and the overall patient experience. Ideally, durable MCS devices would be fully implantable, automatically controlled, and minimize the need for anticoagulation. Reliable and long-term total artificial hearts for biventricular support would be available; and surgical, perioperative, and postoperative management would be informed by the individual patient phenotype along with computational simulations. In this review, we summarize emerging technological innovations in these areas, focusing primarily on innovations in late preclinical or early clinical phases of study. We highlight important considerations that the MCS community of clinicians, engineers, industry partners, and venture capital investors should consider to sustain the evolution of the field.}
}
@article{GRANJO202021,
title = {Enhancing the autonomy of students in chemical engineering education with LABVIRTUAL platform},
journal = {Education for Chemical Engineers},
volume = {31},
pages = {21-28},
year = {2020},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2020.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S174977282030018X},
author = {José F.O. Granjo and Maria G. Rasteiro},
keywords = {Web platform, Virtual labs, Chemical processes, Autonomous learning},
abstract = {Engineering educators have been developing different approaches to supplement scientific background and further develop the ability for autonomous and critical thinking in students. In 2009, the University of Coimbra has made available on-line a virtual platform with a wide scope, directed towards Chemical Engineering education. The platform is divided into four different educational topics: Unit Operations and Separations, Chemical Reaction, Process Systems Engineering and Biological Processes. These sections include simulators, applications, and case studies to help understanding chemical/biochemical processes and improve their autonomy. This paper presents an assessment of the use of that platform by two different groups of students in the school years of 2015/2016 and 2018/2019: a group from the 3rd-year of Chemical Engineering, and another one from a Project Design course (2nd cycle, MSc of Chemical Engineering). A case study addressing the synthesis of phthalic anhydride by o-xylene oxidation on a fixed-bed catalytic reactor is also given to show the use of existing simulators in LABVIRTUAL.}
}
@article{SCHNEIDER2012475,
title = {Eye gaze reveals a fast, parallel extraction of the syntax of arithmetic formulas},
journal = {Cognition},
volume = {125},
number = {3},
pages = {475-490},
year = {2012},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2012.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S0010027712001357},
author = {Elisa Schneider and Masaki Maruyama and Stanislas Dehaene and Mariano Sigman},
keywords = {Arithmetic, Gestalt, Cognitive architecture, Language, Mathematical education},
abstract = {Mathematics shares with language an essential reliance on the human capacity for recursion, permitting the generation of an infinite range of embedded expressions from a finite set of symbols. We studied the role of syntax in arithmetic thinking, a neglected component of numerical cognition, by examining eye movement sequences during the calculation of arithmetic expressions. Specifically, we investigated whether, similar to language, an expression has to be scanned sequentially while the nested syntactic structure is being computed or, alternatively, whether this structure can be extracted quickly and in parallel. Our data provide evidence for the latter: fixations sequences were stereotypically organized in clusters that reflected a fast identification of syntactic embeddings. A syntactically relevant pattern of eye movement was observed even when syntax was defined by implicit procedural rules (precedence of multiplication over addition) rather than explicit parentheses. While the total number of fixations was determined by syntax, the duration of each fixation varied with the complexity of the arithmetic operation at each step. These findings provide strong evidence for a syntactic organization for arithmetic thinking, paving the way for further comparative analysis of differences and coincidences in the instantiation of recursion in language and mathematics.}
}
@article{GARDECKI2018138,
title = {Innovative Internet of Things-reinforced Human Recognition for Human-Machine Interaction Purposes},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {6},
pages = {138-143},
year = {2018},
note = {15th IFAC Conference on Programmable Devices and Embedded Systems PDeS 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.07.143},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318308875},
author = {Arkadiusz Gardecki and Michal Podpora and Aleksandra Kawala-Janik},
keywords = {Human-Machine Interaction, Internet of Things, Human Recognition, Humanoid Robots, Human Identification},
abstract = {Accurate and reliable human recognition and parametrisation have always been an important challenge in efficient Man-Machine Interaction. A humanoid robot is able to offer a much richer and more natural behaviour and human-like communication, but only if the robot possesses sufficient knowledge about the interlocutor, such as inter alia: gender, age, mood, behaviour data, interaction history. In this paper authors introduced an innovative conception in Human-Machine Interaction, where instead of thinking about an interaction as an event (which uses and produces information) an innovative point of view was proposed, where the interaction is just an event in a continuous flow of information. The difference, once perceived, results in an astounding change of conception, as well as a whole new set of ideas. The human detection, information acquisition, human recognition – can be performed earlier, before a human reaches the humanoid robot, also the history of interactions and possible interests of the interlocutor can be predicted before they would even start the conversation. This paper contains a detailed analysis of the proposed environment-based approach to interaction, as well as the Internet of Things-reinforced information acquisition.}
}
@article{KHEDMATIMORASAE2024219,
title = {Advancing the discourse: A next-generation value chain-based taxonomy for circular economy key performance indicators},
journal = {Sustainable Production and Consumption},
volume = {48},
pages = {219-234},
year = {2024},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2024.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S2352550924001428},
author = {Esmaeil Khedmati-Morasae and Markus Zils and Peter Hopkinson and Ryan Nolan and Fiona Charnley and Okechukwu Okorie and Halid Abu-Bakar},
keywords = {Circular economy, Key performance indicators, Taxonomy, Value chain, Systemic},
abstract = {The growth of interest in circular economy (CE) has been accompanied by different approaches to measurement of CE outcomes and impacts, leading to a wide portfolio of indicators with varying degrees of overlap, inconsistency, and convergence. The aim of this paper is to propose a unifying framework for CE indicators, as the next generation of CE taxonomies. We first undertake a scoping review of 59 review papers on CE indicators using manual and computational methods (i.e., topic modelling) to inform the taxonomy structure and content. As a result, we report on 11 clusters of approaches that have been attentive to different dimensions of CE (e.g. horizontal value chain, vertical scale of operation (macro, meso, micro), impact category (economic, biophysical, social), material vs product focus, etc.). Highlighting the strengths and weakness of these approaches, we identify gaps in dimensions related to horizontal and vertical scales of measurement, and propose an agnostic, integrative framework that builds on the scientific foundations of previous research, within a more systemic and comprehensive taxonomy. This taxonomy could be used as a guiding framework or heuristic for regulators, both nationally and internationally, and for practitioners to undertake a comprehensive measurement and assessment of CE related interventions and initiatives at scale.}
}
@article{SEDJELMACI2019101970,
title = {An efficient cyber defense framework for UAV-Edge computing network},
journal = {Ad Hoc Networks},
volume = {94},
pages = {101970},
year = {2019},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2019.101970},
url = {https://www.sciencedirect.com/science/article/pii/S1570870519302136},
author = {Hichem Sedjelmaci and Aymen Boudguiga and Inès Ben Jemaa and Sidi Mohammed Senouci},
keywords = {UAV-Edge computing, Detection, Stackelberg game, Energy consumption, Computation overhead},
abstract = {Mobile Edge Computing (MEC) is usually deployed in energy and delay constrained networks, such as internet of things networks and transportation systems to address the issues of energy consumption, computation capacity and network delay. In this work, we focus on a special case, which is Unmanned Aerial Vehicle Edge Computing (UEC) network. Addressing the security issues in UAV-Edge Computing network is mandatory due to the criticality of UEC services, such as network traffic monitoring, or search and rescue operations. However, cyber defense and protection of UEC network have not yet received sufficient research attention. Thereby, we propose and develop a cyber-defense solution based on a non-cooperative game to protect the UEC from network and offloading attacks, while taking into account nodes’ energy constraints and computation overhead. Simulation results show that, the deployment of our cyber defense system in UEC network requires low energy consumption and low computation overhead to obtain a high protection rate.}
}
@article{REVACH2021103229,
title = {Expanding the discussion: Revision of the fundamental assumptions framing the study of the neural correlates of consciousness},
journal = {Consciousness and Cognition},
volume = {96},
pages = {103229},
year = {2021},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2021.103229},
url = {https://www.sciencedirect.com/science/article/pii/S1053810021001550},
author = {Daniel Revach and Moti Salti},
keywords = {Consciousness, Awareness, Conscious perception, Unconscious perception, Cognition, Neuroscience, Assumptions, Premises, Neurobiological, Mechanism, Phenomenology},
abstract = {The way one asks a question is shaped by a-priori assumptions and constrains the range of possible answers. We identify and test the assumptions underlying contemporary debates, models, and methodology in the study of the neural correlates of consciousness, which was framed by Crick and Koch’s seminal paper (1990). These premises create a sequential and passive conception of conscious perception: it is considered the product of resolved information processing by unconscious mechanisms, produced by a singular event in time and place representing the moment of entry. The conscious percept produced is then automatically retained to be utilized by post-conscious mechanisms. Major debates in the field, such as concern the moment of entry, the all-or-none vs graded nature, and report vs no-report paradigms, are driven by the consensus on these assumptions. We show how removing these assumptions can resolve some of the debates and challenges and prompt additional questions. The potential non-sequential nature of perception suggests new ways of thinking about consciousness as a dynamic and dispersed process, and in turn about the relationship between conscious and unconscious perception. Moreover, it allows us to present a parsimonious account for conscious perception while addressing more aspects of the phenomenon.}
}
@article{JUST20121292,
title = {Autism as a neural systems disorder: A theory of frontal-posterior underconnectivity},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {36},
number = {4},
pages = {1292-1313},
year = {2012},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2012.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0149763412000334},
author = {Marcel Adam Just and Timothy A. Keller and Vicente L. Malave and Rajesh K. Kana and Sashank Varma},
keywords = {Autism, Connectivity, Underconnectivity, 4CAPS, Computational model, fMRI},
abstract = {The underconnectivity theory of autism attributes the disorder to lower anatomical and functional systems connectivity between frontal and more posterior cortical processing. Here we review evidence for the theory and present a computational model of an executive functioning task (Tower of London) implementing the assumptions of underconnectivity. We make two modifications to a previous computational account of performance and brain activity in typical individuals in the Tower of London task (Newman et al., 2003): (1) the communication bandwidth between frontal and parietal areas was decreased and (2) the posterior centers were endowed with more executive capability (i.e., more autonomy, an adaptation is proposed to arise in response to the lowered frontal-posterior bandwidth). The autism model succeeds in matching the lower frontal-posterior functional connectivity (lower synchronization of activation) seen in fMRI data, as well as providing insight into behavioral response time results. The theory provides a unified account of how a neural dysfunction can produce a neural systems disorder and a psychological disorder with the widespread and diverse symptoms of autism.}
}
@article{SHARMA2024100944,
title = {Towards intelligent industrial systems: A comprehensive survey of sensor fusion techniques in IIoT},
journal = {Measurement: Sensors},
volume = {32},
pages = {100944},
year = {2024},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2023.100944},
url = {https://www.sciencedirect.com/science/article/pii/S2665917423002805},
author = {Deepak sharma and Anuj kumar and Nitin Tyagi and Sunil S. Chavan and Syam Machinathu Parambil Gangadharan},
keywords = {Sensor fusion, Machine learning, Fault tolerance, Fault prediction, Neural network},
abstract = {Industrial Internet of Things (IIoT) is systems aim to facilitate human monitoring and the direction of efficient production of goods in industrial settings by linking a wide variety of intelligent devices such as sensors, actuators, and controllers. This is achieved by utilizing Internet of Things (IoT) to diagnose a problem with a specific IIoT part is to employ a basic diagnostic technique that's based on models and data. Physical models, signal patterns, and machine-learning strategies must be adequately built to account for system challenges. Another factor that could lead to an exponential rise in complexity is the ever-increasing interconnections between different electronic hardware. The knowledge-based defect diagnosis methods boost interoperability in the operation. Users don't need to be experts in the field to benefit from the system's high-level thinking and response to their queries. So, in advanced IIoT systems, a knowledge-based fault diagnostic approach is favored over traditional model-based and data-driven diagnosis methods. The goal of this study is to evaluate recent improvements in the design of knowledge-based defect detection in the context of IIoT systems, deductive and inductive reasoning, and many other forms of logical reasoning. IIoT-based systems have revolutionized industrial settings by connecting intelligent devices such as sensors, actuators, and controllers to enable efficient production and human monitoring. In this survey paper, we explore machine learning-based sensor fusion techniques within the realm of Industrial Internet of Things (IIoT), addressing critical challenges in fault detection and diagnosis.}
}
@article{YANG2023125877,
title = {Multi-parameter controlled mechatronics-electro-hydraulic power coupling electric vehicle based on active energy regulation},
journal = {Energy},
volume = {263},
pages = {125877},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.125877},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222027633},
author = {Jian Yang and Bo Liu and Tiezhu Zhang and Jichao Hong and Hongxin Zhang},
keywords = {Mechatronics-electro-hydraulic power coupling, Energy efficiency, K-means clustering analysis, Torque characteristic, Fuzzy control},
abstract = {To enhance the hydraulic energy utilization and torque output stability, a novel mechatronics-electro-hydraulic power coupling electric vehicle (MEH-PCEV) is proposed, integrating a hydraulic pump/motor and a motor into a single device for mutual energy conversion. For MEH-PCEVs equipped with multiple energy sources, a cluster analysis method is used to classify the actual road test dataset and provide guiding ideas for designing rule-based energy management strategies (RB-EMS). Simultaneously, for the output torque anomaly phenomenon in RB-EMS, an inverse thinking fuzzy logic optimization energy management strategy (FLO-EMS) conside ring multi-parameter objectives as input is used to adjust the electromagnetic torque in real-time and reasonably allocate the energy flow. The simulation results demonstrate that the electric and total torque output are more stable. The electric peak torque is relieved, with a corresponding increase in the percentage of electrical energy recovery. With the equal power demand, the overall efficiency of the motor working point is substantially improved, and the energy consumption rate is decreased by 24.42%. Under the active regulation of FLO-EMS, hydraulic energy is more reasonably utilized to meet the vehicle demand power while avoiding energy dissipation and waste. Moreover, this work is expected to reference the development and engineering applications of electro-hydraulic coupling systems.}
}
@article{ZONG2024120192,
title = {Parameter estimation of multivariable Wiener nonlinear systems by the improved particle swarm optimization and coupling identification},
journal = {Information Sciences},
volume = {661},
pages = {120192},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.120192},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524001051},
author = {Tiancheng Zong and Junhong Li and Guoping Lu},
keywords = {System identification, Multivariable Wiener system, Particle swarm optimization, Coupling identification, Auxiliary model},
abstract = {This paper investigates the parameter estimation of multivariable Wiener nonlinear systems. To solve the inconsistency problem of the parameter vector and the parameter matrix, the coupling identification concept is applied. Combined with particle swarm optimization (PSO) and an auxiliary model, the partially coupled improved particle swarm optimization (PC-IPSO) method is proposed. In this algorithm, the adaptive feedback inertia weight is improved to accelerate the convergence speed, and the retirement update mechanism is introduced to improve the optimization ability of the basic PSO algorithm. To verify the performance of PC-IPSO, we also derive a multivariable improved PSO (M-IPSO) method for comparison. The computational complexity analysis shows that the PC-IPSO algorithm requires less computational resources than the M-IPSO algorithm. Then, the convergence of the improved PSO method is analyzed. The simulation results indicate that the PC-IPSO method has a faster convergence speed and higher identification accuracy than the M-IPSO and several existing state-of-the-art methods for multivariable Wiener system identification.}
}
@article{CHEN2022101380,
title = {An automated quality evaluation framework of psychotherapy conversations with local quality estimates},
journal = {Computer Speech & Language},
volume = {75},
pages = {101380},
year = {2022},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2022.101380},
url = {https://www.sciencedirect.com/science/article/pii/S0885230822000213},
author = {Zhuohao Chen and Nikolaos Flemotomos and Karan Singla and Torrey A. Creed and David C. Atkins and Shrikanth Narayanan},
keywords = {Cognitive behavioral therapy, Computational linguistics, Hierarchical framework, Local quality estimates},
abstract = {Text-based computational approaches for assessing the quality of psychotherapy are being developed to support quality assurance and clinical training. However, due to the long durations of typical conversation based therapy sessions, and due to limited annotated modeling resources, computational methods largely rely on frequency-based lexical features or dialogue acts to assess the overall session level characteristics. In this work, we propose a hierarchical framework to automatically evaluate the quality of transcribed Cognitive Behavioral Therapy (CBT) interactions. Given the richly dynamic nature of the spoken dialog within a talk therapy session, to evaluate the overall session level quality, we propose to consider modeling it as a function of local variations across the interaction. To implement that empirically, we divide each psychotherapy session into conversation segments and initialize the segment-level qualities with the session-level scores. First, we produce segment embeddings by fine-tuning a BERT-based model, and predict segment-level (local) quality scores. These embeddings are used as the lower-level input to a Bidirectional LSTM-based neural network to predict the session-level (global) quality estimates. In particular, we model the global quality as a linear function of the local quality scores, which allows us to update the segment-level quality estimates based on the session-level quality prediction. These newly estimated segment-level scores benefit the BERT fine-tuning process, which in turn results in better segment embeddings. We evaluate the proposed framework on automatically derived transcriptions from real-world CBT clinical recordings to predict session-level behavior codes. The results indicate that our approach leads to improved evaluation accuracy for most codes when used for both regression and classification tasks.}
}
@incollection{VALLERO2014953,
title = {Chapter 33 - Grand Challenges},
editor = {Daniel Vallero},
booktitle = {Fundamentals of Air Pollution (Fifth Edition)},
publisher = {Academic Press},
edition = {Fifth Edition},
address = {Boston},
pages = {953-961},
year = {2014},
isbn = {978-0-12-401733-7},
doi = {https://doi.org/10.1016/B978-0-12-401733-7.00033-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780124017337000335},
author = {Daniel Vallero},
keywords = {Bayesian, Biogeochemical cycles, Categorical imperative, Circle of poisons, CO, Command and control, Computational tools, Fossil fuels, Fundamentals of air pollution, Geographic information system (GIS), Geostatistics, Global greenhouse gas emissions (GHG), Grand Challenges, Immanuel Kant, Indoor air pollution, Informatics, Kriging, National Academy of Engineering, Precautionary principle, Pre-Kindergarten, Real-world exposures, Reductionist, Risk, Sustainability, Systems thinking, Transdisciplinary, Translational science},
abstract = {This chapter looks to the future of air quality and how the lessons learned in recent decades can be applied to new problems. The challenges include finding ways to prevent emerging economies from repeating the air pollution mistakes and harm that developed nations have experienced in arriving at solutions to air pollution problems. Other challenges include: global problems, such as long-range transport of pollutants, climate change; real-world-exposures (including indoor air pollution); improvements in technologies to remove difficult-to-treat pollutants; and addressing the growing number of mobile sources. This will require more systems thinking and sustainable, transdisciplinary solutions. The legacy of the current cadre of air pollution experts must be one of translational science and the enhancement of early air pollution education for the next generation.}
}
@article{HUGHES2021338,
title = {High-content phenotypic and pathway profiling to advance drug discovery in diseases of unmet need},
journal = {Cell Chemical Biology},
volume = {28},
number = {3},
pages = {338-355},
year = {2021},
issn = {2451-9456},
doi = {https://doi.org/10.1016/j.chembiol.2021.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S2451945621001008},
author = {Rebecca E. Hughes and Richard J.R. Elliott and John C. Dawson and Neil O. Carragher},
keywords = {high-content imaging, machine learning, structural similarity, network pharmacology, esophageal cancer, glioblastoma},
abstract = {Summary
Conventional thinking in modern drug discovery postulates that the design of highly selective molecules which act on a single disease-associated target will yield safer and more effective drugs. However, high clinical attrition rates and the lack of progress in developing new effective treatments for many important diseases of unmet therapeutic need challenge this hypothesis. This assumption also impinges upon the efficiency of target agnostic phenotypic drug discovery strategies, where early target deconvolution is seen as a critical step to progress phenotypic hits. In this review we provide an overview of how emerging phenotypic and pathway-profiling technologies integrate to deconvolute the mechanism-of-action of phenotypic hits. We propose that such in-depth mechanistic profiling may support more efficient phenotypic drug discovery strategies that are designed to more appropriately address complex heterogeneous diseases of unmet need.}
}
@incollection{KOSSLYN1988615,
title = {Seeing and Imagining in the Cerebral Hemispheres: A Computational Approach},
editor = {Allan Collins and Edward E. Smith},
booktitle = {Readings in Cognitive Science},
publisher = {Morgan Kaufmann},
pages = {615-642},
year = {1988},
isbn = {978-1-4832-1446-7},
doi = {https://doi.org/10.1016/B978-1-4832-1446-7.50052-2},
url = {https://www.sciencedirect.com/science/article/pii/B9781483214467500522},
author = {Stephen M. Kosslyn}
}
@article{MELGAREJO2025121371,
title = {Optimization test function synthesis with generative adversarial networks and adaptive neuro-fuzzy systems},
journal = {Information Sciences},
volume = {686},
pages = {121371},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121371},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524012854},
author = {Miguel Melgarejo and Mariana Medina and Juan Lopez and Angie Rodriguez},
keywords = {Optimization test functions, Generative adversarial networks, Adaptive neuro-fuzzy systems, Fuzzy basis function expansion, Symbolic regression},
abstract = {This paper presents an approach to synthesizing optimization test functions that couples generative adversarial networks and adaptive neuro-fuzzy systems. A generative adversarial network produces optimization landscapes from a database of known optimization test functions, and an adaptive neuro-fuzzy system performs regression on the generated landscapes to provide closed-form expressions. These expressions can be implemented as fuzzy basis function expansions. Eight databases of two-dimensional optimization landscapes reported in the literature are used to train the generative network. Exploratory landscape analysis over the generated samples reveals that the network can lead to new optimization landscapes with features of interest. In addition, fuzzy basis function expansions provide the best approximation results when compared against two symbolic regression frameworks over several selected landscapes. Examples are used to illustrate the ability of these functions to model complex surface artifacts such as plateaus. The proposed approach can be used as a mathematical collaboration tool that couples generative artificial and computational intelligence techniques to formulate high-dimensional optimization test problems from two-dimensional synthesized functions.}
}
@article{FURNARI2023103763,
title = {Streaming egocentric action anticipation: An evaluation scheme and approach},
journal = {Computer Vision and Image Understanding},
volume = {234},
pages = {103763},
year = {2023},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2023.103763},
url = {https://www.sciencedirect.com/science/article/pii/S1077314223001431},
author = {Antonino Furnari and Giovanni Maria Farinella},
keywords = {Action anticipation, Egocentric vision, Streaming perception},
abstract = {Egocentric action anticipation aims to predict the future actions the camera wearer will perform from the observation of the past. While predictions about the future should be available before the predicted events take place, most approaches do not pay attention to the computational time required to make such predictions. As a result, current evaluation schemes assume that predictions are available right after the input video is observed, i.e., presuming a negligible runtime, which may lead to overly optimistic evaluations. We propose a streaming egocentric action evaluation scheme which assumes that predictions are performed online and made available only after the model has processed the current input segment, which depends on its runtime. To evaluate all models considering the same prediction horizon, we hence propose that slower models should base their predictions on temporal segments sampled ahead of time. Based on the observation that model runtime can affect performance in the considered streaming evaluation scenario, we further propose a lightweight action anticipation model based on feed-forward 3D CNNs which is optimized using knowledge distillation techniques with a novel past-to-future distillation loss. Experiments on the three popular datasets EPIC-KITCHENS-55, EPIC-KITCHENS-100 and EGTEA Gaze+ show that (i) the proposed evaluation scheme induces a different ranking on state-of-the-art methods as compared to classic evaluations, (ii) lightweight approaches tend to outmatch more computationally expensive ones, and (iii) the proposed model based on feed-forward 3D CNNs and knowledge distillation outperforms current art in the streaming egocentric action anticipation scenario.}
}
@article{SOPER2022126712,
title = {Quantifying the effect of solvent on the morphology of organic crystals using a statistical thermodynamics approach},
journal = {Journal of Crystal Growth},
volume = {591},
pages = {126712},
year = {2022},
issn = {0022-0248},
doi = {https://doi.org/10.1016/j.jcrysgro.2022.126712},
url = {https://www.sciencedirect.com/science/article/pii/S0022024822002007},
author = {Eleanor M. Soper and Radoslav Y. Penchev and Stephen M. Todd and Frank Eckert and Marc Meunier},
keywords = {A2. Solvent screening, A2. Particle engineering, A1. Surface chemistry, A1. Cosmo-RS, A1. Morphology},
abstract = {A method for predicting the effect of solvent on the morphology of organic crystals is presented, providing an efficient screening tool for identifying ideal crystallization solvents. The solvent effect is estimated by the computation of chemical potentials and activity coefficients of crystal surfaces using a first principles-based statistical thermodynamics approach. Density functional theory and COSMO-RS are utilized to determine the activity coefficients of the crystal growth faces of a selection of active pharmaceutical ingredients (APIs) in solvents across a broad range of polarities. The ability of COSMO-RS to predict and quantify the effects of solvent on crystal growth and morphology is assessed using hierarchical clustering to classify the solvents according to their overall interaction strength with the crystal faces. The COSMO-RS approach allows for a physical interpretation of the predictions in terms of surface polarity and is confirmed by comparison to published experimental data. Herein a methodology is reported for automated computation of the activity coefficients of all solvent-surface pairs directly from the drug crystal structure. The procedure goes beyond the traditional trial-and-error solvent selection process and has the potential to be used as a rapid computational screening tool in pharmaceutical drug development.}
}
@article{SALCEANU2014837,
title = {The Influence of Computer Games on Children's Development. Exploratory Study on the Attitudes of Parents},
journal = {Procedia - Social and Behavioral Sciences},
volume = {149},
pages = {837-841},
year = {2014},
note = {LUMEN 2014 - From Theory to Inquiry in Social Sciences, Iasi, Romania, 10-12 April 2014},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2014.08.323},
url = {https://www.sciencedirect.com/science/article/pii/S1877042814050368},
author = {Claudia Sălceanu},
keywords = {Computer games, influence on children, positive and negative effects of computer games, parents’ attitudes;},
abstract = {The current study aims to investigate the attitudes of parents (N=1087) regarding the influence of computer games on their children's development in the following aspects: time they spend at the computer to play, types of favourite games, ways of child supervision, benefits and disadvantages of computer games. The results of the research show: 30.47% of children may access the computer anytime they want; the computer is mostly used for games (36.28%); 42.87% of parents supervise their children's activities at the computer only when they have spare time; 50% of parents allow their children to spend 1-2hours at computer games every day, while 28.54% allow 3-4hours (and more) of computer games every day. The biggest benefits of computer games, according to parents, are thinking development (9.60%), observation capacity (8.27%), and creativity (8.01%). The biggest disadvantages of computer games are the lack of physical movement (13.37%), sight disorders (13.15%) and agitation (8.58%). Parents recognize that games can have powerful effects on children, and should therefore set limits on the amount and content of games their children play. In this way, we can realize the potential benefits while minimizing the potential harms.}
}
@article{XIE2023119,
title = {2D magnetotelluric inversion based on ResNet},
journal = {Artificial Intelligence in Geosciences},
volume = {4},
pages = {119-127},
year = {2023},
issn = {2666-5441},
doi = {https://doi.org/10.1016/j.aiig.2023.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S2666544123000266},
author = {LiAn Xie and Bo Han and Xiangyun Hu and Ningbo Bai},
keywords = {Magnetotellurics, 2D inversion, Residual network, Deep learning},
abstract = {In this study, a deep learning algorithm was applied to two-dimensional magnetotelluric (MT) data inversion. Compared with the traditional linear iterative inversion methods, the MT inversion method based on convolutional neural networks (CNN) does not rely on the selection of the initial model parameters and does not fall into the local optima. Although the CNN inversion models can provide a clear electrical interface division, their inversion results may remain prone to abrupt electrical interfaces as opposed to the actual underground electrical situation. To solve this issue, a neural network with a residual network architecture (ResNet-50) was constructed in this study. With the apparent resistivity and phase pseudo-section data as the inputs and with the resistivity parameters of the geoelectric model as the training labels, the modified ResNet-50 model was trained end-to-end for producing samples according to the corresponding production strategy of the study area. Through experiments, the training of the ResNet-50 with the dice loss function effectively solved the issue of over-segmentation of the electrical interface by the cross-entropy function, avoided its abrupt inversion, and overcame the computational inefficiency of the traditional iterative methods. The proposed algorithm was validated against MT data measured from a geothermal field prospect in Huanggang, Hubei Province, which showed that the deep learning method has opened up broad prospects in the field of MT data inversion.}
}
@article{BUCKNER200749,
title = {Self-projection and the brain},
journal = {Trends in Cognitive Sciences},
volume = {11},
number = {2},
pages = {49-57},
year = {2007},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2006.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S1364661306003275},
author = {Randy L. Buckner and Daniel C. Carroll},
abstract = {When thinking about the future or the upcoming actions of another person, we mentally project ourselves into that alternative situation. Accumulating data suggest that envisioning the future (prospection), remembering the past, conceiving the viewpoint of others (theory of mind) and possibly some forms of navigation reflect the workings of the same core brain network. These abilities emerge at a similar age and share a common functional anatomy that includes frontal and medial temporal systems that are traditionally associated with planning, episodic memory and default (passive) cognitive states. We speculate that these abilities, most often studied as distinct, rely on a common set of processes by which past experiences are used adaptively to imagine perspectives and events beyond those that emerge from the immediate environment.}
}
@article{AUGIER2001307,
title = {Sublime Simon: The consistent vision of economic psychology's Nobel laureate},
journal = {Journal of Economic Psychology},
volume = {22},
number = {3},
pages = {307-334},
year = {2001},
issn = {0167-4870},
doi = {https://doi.org/10.1016/S0167-4870(01)00036-8},
url = {https://www.sciencedirect.com/science/article/pii/S0167487001000368},
author = {Mie Augier},
keywords = {Herbert Simon, Bounded rationality, Carnegie Mellon University, Economics and psychology},
abstract = {This essay contains a study of some of Herbert Simon's ideas, with particular emphasis on the role of bounded rationality in Simon's thinking and his contributions to economics and psychology. I describe Simon's visions for challenging rational choice theory, through limited rationality, and for bringing psychology into economics, putting this in perspective by describing the evolution of some of this thoughts, focusing on the continuity in his work.}
}
@incollection{VELINGKAR2025239,
title = {Chapter 16 - Smart computing in brain-computer interface and neuroscientific research: opportunities, methods, and challenges},
editor = {Bikesh Kumar Singh and G.R. Sinha},
booktitle = {Intelligent Computing Techniques in Biomedical Imaging},
publisher = {Academic Press},
pages = {239-249},
year = {2025},
isbn = {978-0-443-15999-2},
doi = {https://doi.org/10.1016/B978-0-443-15999-2.00005-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443159992000050},
author = {Harish Velingkar and Roopa R. Kulkarni and Prashant P. Patavardhan},
keywords = {BCI, BMI, AI algorithms, machine learning, neuroscientific, brain waves, EEG},
abstract = {Brain-machine interfacing (BMI), also known as brain-computer interfacing (BCI), is an innovative field of technology with a primary objective to establish a connection and foster seamless interaction between the human brain and machines. The underlying concept aims to facilitate direct communication between the brain and external devices, enabling a direct interface between the two, allowing individuals to control various applications, such as prosthetics, robotics, or computer software, with their thoughts alone. BCI is a multidisciplinary field that involves expertise from different areas, such as neuroscience, computer science, engineering, and psychology. BMI is also being extensively used in early prediction of neurophysiological abnormalities and brain disorders. This article will lay out various benefits of using computational intelligence, specifically machine learning, in mental health disciplines and will also explain some of the popular AI algorithms from a neuroscientific research point of view.}
}
@article{ASOULIN201998,
title = {Phrase structure grammars as indicative of uniquely human thoughts},
journal = {Language Sciences},
volume = {74},
pages = {98-109},
year = {2019},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2019.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0388000119300117},
author = {Eran Asoulin},
keywords = {Thought, Cognition, Phrase structure grammars, Chomsky hierarchy, Animal cognition},
abstract = {I argue that the ability to compute phrase structure grammars is indicative of a particular kind of thought. This type of thought that is only available to cognitive systems that have access to the computations that allow the generation and interpretation of the structural descriptions of phrase structure grammars. The study of phrase structure grammars, and formal language theory in general, is thus indispensable to studies of human cognition, for it makes explicit both the unique type of human thought and the underlying mechanisms in virtue of which this thought is made possible.}
}
@article{SALAHSHOORI2024123888,
title = {Simulation-based approaches for drug delivery systems: Navigating advancements, opportunities, and challenges},
journal = {Journal of Molecular Liquids},
volume = {395},
pages = {123888},
year = {2024},
issn = {0167-7322},
doi = {https://doi.org/10.1016/j.molliq.2023.123888},
url = {https://www.sciencedirect.com/science/article/pii/S0167732223026958},
author = {Iman Salahshoori and Mahdi Golriz and Marcos A.L. Nobre and Shahla Mahdavi and Rahime {Eshaghi Malekshah} and Afsaneh Javdani-Mallak and Majid {Namayandeh Jorabchi} and Hossein {Ali Khonakdar} and Qilin Wang and Amir H. Mohammadi and Seyedeh {Masoomeh Sadat Mirnezami} and Farshad Kargaran},
keywords = {Computational fluid dynamics, Drug delivery systems, Molecular simulations, Optimization, Molecular dynamics simulation, Monte Carlo simulation},
abstract = {Efficient drug delivery systems (DDSs) play a pivotal role in ensuring pharmaceuticals’ targeted and effective administration. However, the intricate interplay between drug formulations and delivery systems poses challenges in their design and optimization. Simulations have emerged as indispensable tools for comprehending these interactions and enhancing DDSs performance to address this complexity. This comprehensive review explores the latest advancements in simulation techniques for DDSs and provides a detailed analysis. The review encompasses various simulation methodologies, including molecular dynamics (MD), Monte Carlo (MC), finite element analysis (FEA), computational fluid dynamics (CFD), density functional theory (DFT), machine learning (ML), and dissipative particle dynamics (DPD). These techniques are critically examined in the context of drug delivery research. The article presents illustrative case studies involving liposomal, polymer-based, nano-particulate, and implantable DDSs, demonstrating the influential role of simulations in optimizing these systems. Furthermore, the review addresses the advantages and limitations of simulations in drug delivery research. It also identifies future directions for research and development, such as integrating multiple simulation techniques, refining and validating models for greater accuracy, overcoming computational limitations, and exploring applications of simulations in personalized medicine and innovative DDSs. Simulations employing various techniques like MD, MC, FEA, CFD, DFT, ML, and DPD offer crucial insights into drug behaviour, aiding in DDS design and optimization. Despite their advantages, including rapid and cost-effective screening, simulations require validation and addressing computational limitations. Future research should focus on integrating techniques, refining models, and exploring personalized medicine applications to enhance drug delivery outcomes. This paper underscores the indispensable contribution of simulations to drug research and development, emphasizing their role in providing valuable insights into drug behaviour, facilitating the development and optimization of DDSs, and ultimately enhancing patient outcomes. As we continue to explore and enhance simulation techniques, their impact on advancing drug discovery and improving DDSs is expected to be profound.}
}
@article{SHOTTER201734,
title = {Persons as dialogical-hermeneutical-relational beings – New circumstances ‘call out’ new responses from us},
journal = {New Ideas in Psychology},
volume = {44},
pages = {34-40},
year = {2017},
note = {SI: The Person},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2016.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X1630157X},
author = {John Shotter},
keywords = {Persons, Ideals, Generalities, Particularities, Hermeneutics, Indeterminacy, Dialogicality},
abstract = {Shifting from a world of already-made-things to a world of things-continually-in-the-making changes everything. Psychology, like all other sciences, tries to proceed by analysis, by breaking down a living, unique, always developing organic whole into a set of general, already-existing, nameable elements. But as Bakhtin makes clear, in discussing how Dostoevsky portrays the inner dynamics of people worrying over how to act for the best in living their lives, such an itemization of merely observed behavioural characteristics leads to a degrading reification of a person's unfinalizability, of their still-developing nature. Below, I first examine the Cartesianism that still seems present in much of our thinking in social inquiry today. I then turn attention to the primacy of our living movements out in the world and their responsiveness to events occurring around us. While finally turning to the fact that, as living beings, what ‘goes on inside us’, is not so important as ‘what we go on inside of’. Although Dostoevsky portrays this indivisible, flowing reality, in terms of a set of discontinuous fragments —because that is the nature of our experience in everyday life — as hermeneutical-dialogical-relational beings, we have a basic capability of organizing them into unitary wholes which sit in the background to everything we think and do.}
}
@article{1985174,
title = {Learning to use a word processor: by doing, by thinking, and by knowing: John M. Carroll and Robert L. Mack: Rep. RC 9481, IBM T.J. Watson Research Center, Yorktown Heights, New York 10598, U.S.A., (July 1982)},
journal = {Decision Support Systems},
volume = {1},
number = {2},
pages = {174},
year = {1985},
issn = {0167-9236},
doi = {https://doi.org/10.1016/0167-9236(85)90071-5},
url = {https://www.sciencedirect.com/science/article/pii/0167923685900715}
}
@article{YU2022102230,
title = {Spatial processing rather than logical reasoning was found to be critical for mathematical problem-solving},
journal = {Learning and Individual Differences},
volume = {100},
pages = {102230},
year = {2022},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2022.102230},
url = {https://www.sciencedirect.com/science/article/pii/S1041608022001170},
author = {Mingxin Yu and Jiaxin Cui and Li Wang and Xing Gao and Zhanling Cui and Xinlin Zhou},
keywords = {Logical reasoning, Spatial processing, Mathematical problem-solving},
abstract = {Students' ability to solve mathematical problems is a standard mathematical skill; however, its cognitive correlates are unclear. Thus, this study aimed to examine whether spatial processing (mental rotation, paper folding, and the Corsi blocks test) and logical reasoning (abstract and concrete syllogisms) were correlated with mathematical problem-solving (word problems and geometric proofing) for college students. The regression results showed that after controlling for gender, age, general IQ, language processing, cognitive processing (visual perception, attention, and memory skills), and number sense and arithmetic computation skills, spatial processing skills still predicted mathematical problem-solving and geometry skills in Chinese college students. Contrastingly, logical reasoning measures related to syllogisms did not predict after controlling for these variables. Further, notably, it did not correlate significantly with geometry performance when no control variables were included. Our results suggest that spatial processing is a significant component of math skills involving word and geometry problems (even after controlling for multiple key cognitive factors).}
}
@article{NIEMYSKA2024168455,
title = {Discovery of a trefoil knot in the RydC RNA: Challenging previous notions of RNA topology},
journal = {Journal of Molecular Biology},
volume = {436},
number = {6},
pages = {168455},
year = {2024},
issn = {0022-2836},
doi = {https://doi.org/10.1016/j.jmb.2024.168455},
url = {https://www.sciencedirect.com/science/article/pii/S0022283624000214},
author = {Wanda Niemyska and Sunandan Mukherjee and Bartosz A. Gren and Szymon Niewieczerzal and Janusz M. Bujnicki and Joanna I. Sulkowska},
keywords = {Entanglement of biomolecules, Topology in soft matter, RNA 3D structure, RNA folding, Molecular dynamics},
abstract = {Knots are very common in polymers, including DNA and protein molecules. Yet, no genuine knot has been identified in natural RNA molecules to date. Upon re-examining experimentally determined RNA 3D structures, we discovered a trefoil knot 31, the most basic non-trivial knot, in the RydC RNA. This knotted RNA is a member of a small family of short bacterial RNAs, whose secondary structure is characterized by an H-type pseudoknot. Molecular dynamics simulations suggest a folding pathway of the RydC RNA that starts with a native twisted loop. Based on sequence analyses and computational RNA 3D structure predictions, we postulate that this trefoil knot is a conserved feature of all RydC-related RNAs. The first discovery of a knot in a natural RNA molecule introduces a novel perspective on RNA 3D structure formation and on fundamental research on the relationship between function and spatial structure of biopolymers.}
}
@incollection{BALANAY201949,
title = {3 - Tools for circular economy: Review and some potential applications for the Philippine textile industry},
editor = {Subramanian Senthilkannan Muthu},
booktitle = {Circular Economy in Textiles and Apparel},
publisher = {Woodhead Publishing},
pages = {49-75},
year = {2019},
series = {The Textile Institute Book Series},
isbn = {978-0-08-102630-4},
doi = {https://doi.org/10.1016/B978-0-08-102630-4.00003-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780081026304000030},
author = {Raquel Balanay and Anthony Halog},
keywords = {Circular economy, Industrial sustainability, Life cycle thinking, Sustainable development, Systems modelling, Textile industry},
abstract = {Instituting circular economy (CE) for sustainability is the aim of taking stock of various analytical/assessment tools. A review of these tools reveals a continuing endeavor to incorporate in the procedures the systems and life cycle thinking and the triple bottom-line framework of sustainable development (economic, social, and environmental). Over time, the CE tools have been modified with the incorporation of some unique attributes in the cases being studied. Life cycle assessment (LCA) remains the popular and the only standardized procedure to analyze CE issues in industries, specifically in the environmental aspect. However, consistency, measurement, and aggregation issues are the major setbacks of having an integrated LCA for economic, social, and environmental impacts. The alternative tools used across the world to study the economic, social, and environmental aspects of CE have increased in both number and sophistication. Optimization and systems models have been increasingly used on a case-based format. Although the downside is the less standardized approach with less chances of comparability in terms of results, these models have been designed appropriately to tackle challenges associated with intricate, multifaceted, and encompassing sustainability and CE issues to improve policy development. In the textile industry, LCA as a popular tool is only used for environmental sustainability assessment but not much in social and economic aspects. The Philippine textile industry still has to catch up in the application of those tools for sustainability assessment. A framework has been suggested for the country's roadmap/guide to attain circularity in textile industry operations.}
}
@article{KEELEY2020194,
title = {Modeling statistical dependencies in multi-region spike train data},
journal = {Current Opinion in Neurobiology},
volume = {65},
pages = {194-202},
year = {2020},
note = {Whole-brain interactions between neural circuits},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2020.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0959438820301720},
author = {Stephen L Keeley and David M Zoltowski and Mikio C Aoi and Jonathan W Pillow},
abstract = {Neural computations underlying cognition and behavior rely on the coordination of neural activity across multiple brain areas. Understanding how brain areas interact to process information or generate behavior is thus a central question in neuroscience. Here we provide an overview of statistical approaches for characterizing statistical dependencies in multi-region spike train recordings. We focus on two classes of models in particular: regression-based models and shared latent variable models. Regression-based models describe interactions in terms of a directed transformation of information from one region to another. Shared latent variable models, on the other hand, seek to describe interactions in terms of sources that capture common fluctuations in spiking activity across regions. We discuss the advantages and limitations of each of these approaches and future directions for the field. We intend this review to be an introduction to the statistical methods in multi-region models for computational neuroscientists and experimentalists alike.}
}
@article{TANG2014245,
title = {On the causes of early life experience effects: Evaluating the role of mom},
journal = {Frontiers in Neuroendocrinology},
volume = {35},
number = {2},
pages = {245-251},
year = {2014},
note = {CRH/Stress in Honor of Wylie Vale},
issn = {0091-3022},
doi = {https://doi.org/10.1016/j.yfrne.2013.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S009130221300068X},
author = {Akaysha C. Tang and Bethany C. Reeb-Sutherland and Russell D. Romeo and Bruce S. McEwen},
keywords = {Maternal care, Stress, CORT, HPA, Self-regulation, Novelty, Maternal mediation, Maternal modulation, Early experience, Cognitive development},
abstract = {Early life experiences are thought to have long-lasting effects on cognitive, emotional, and social function during adulthood. Changes in neuroendocrine function, particularly the hypothalamic–pituitary–adrenal (HPA) axis, contribute to these systems-level behavioral effects. In searching for causal mechanisms underlying these early experience effects, pioneering research has demonstrated an important role for maternal care in offspring development, and this has led to two persistent ideas that permeate current research and thinking: first, environmental impact on the developing infant is mediated through maternal care behavior; second, the more care that a mother provides, the better off her offspring. While a good beginning, the reality is likely more complex. In this review, we critically examine these ideas and propose a computationally-motivated theoretical framework, and within this framework, we consider evidence supporting a hypothesis of maternal modulation. These findings may inform policy decisions in the context of child health and development.}
}
@article{MAJUMDAR2023100126,
title = {A novel approach for communicating with patients suffering from completely locked-in-syndrome (CLIS) via thoughts: Brain computer interface system using EEG signals and artificial intelligence},
journal = {Neuroscience Informatics},
volume = {3},
number = {2},
pages = {100126},
year = {2023},
issn = {2772-5286},
doi = {https://doi.org/10.1016/j.neuri.2023.100126},
url = {https://www.sciencedirect.com/science/article/pii/S2772528623000110},
author = {Sharmila Majumdar and Amin Al-Habaibeh and Ahmet Omurtag and Bubaker Shakmak and Maryam Asrar},
keywords = {Brain, CLIS, EEG, Feature extraction, ASPS, ANN, ALS, MND, AI},
abstract = {This paper investigates the development of an intelligent system method to address completely locked-in-syndrome (CLIS) that is caused by some illnesses such as Amyotrophic Lateral Sclerosis (ALS) as the most predominant type of Motor Neuron Disease (MND). In the last stages of ALS and despite the limitations in body movements, patients however will have a fully functional brain and cognitive capabilities and able to feel pain but fail to communicate. This paper aims to address the CLIS problem by utilizing EEG signals that human brain generates when thinking about a specific feeling or imagination as a way to communicate. The aim is to develop a low-cost and affordable system for patients to use to communicate with carers and family members. In this paper, the novel implementation of the ASPS (Automated Sensor and Signal Processing Selection) approach for feature extraction of EEG is presented to select the most suitable Sensory Characteristic Features (SCFs) to detect human thoughts and imaginations. Artificial Neural Networks (ANN) are used to verify the results. The findings show that EEG signals are able to capture imagination information that can be used as a means of communication; and the ASPS approach allows the selection of the most important features for reliable communication. This paper explains the implementation and validation of ASPS approach in brain signal classification for bespoke arrangement. Hence, future work will present the results of relatively high number of volunteers, sensors and signal processing methods.}
}
@article{KAFUKU2019192,
title = {Application of Fuzzy Logic in Selection of Remanufacturing Technology},
journal = {Procedia Manufacturing},
volume = {33},
pages = {192-199},
year = {2019},
note = {Sustainable Manufacturing for Global Circular Economy: Proceedings of the 16th Global Conference on Sustainable Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2019.04.023},
url = {https://www.sciencedirect.com/science/article/pii/S2351978919305001},
author = {John Mbogo Kafuku and Muhamad Zameri {Mat Saman} and Sha’ri Mohd Yusof},
keywords = {Remanufacturing Operations, Technology Selection, Fuzzy Logic, Technology Selection Criteria, Fuzzy Decision Tool},
abstract = {Fuzzy approach is frequently used for selection of manufacturing technology. However, the application of the fuzzy tool for choosing the appropriate remanufacturing technology is seldom applied. This study applies the fuzzy logic approach for the selection of technology in order to minimize vagueness in decision making, thereby making results more similar to experts’ thinking. Through elicitation of experts’ inputs, six cleaning technologies were evaluated and ranked appropriately, using criteria of technology cost, operating cost, and disposal effect. Moreover, the technology selection was computed through experts’ opinion using the fuzzy logic inference system. The results show that when technical function of the technology is at the low level of 20%, the technology quality is as low as 15%, and the technology flexibility is rated as low at 25%; then the technical adequacy of the assessed technology will be as low as 10%. The fuzzy approach shows that technology performance is largely impacted by criteria far beyond the technology itself, including purchasing cost, disposal cost, operating cost, and other support functions to compliment experience of experts. Despite the fact that decision makers are appropriately selecting technology, the application of the fuzzy logic tool helps to accommodate vagueness, ambiguity, and subjective views of experts. Notwithstanding the robustness of the approach, application of software to help selection of technology is more reliable and accurate, reduce time of decision, and can be accessed worldwide.}
}
@incollection{VALLERO20211,
title = {Chapter 1 - Systems science},
editor = {Daniel A. Vallero},
booktitle = {Environmental Systems Science},
publisher = {Elsevier},
pages = {1-24},
year = {2021},
isbn = {978-0-12-821953-9},
doi = {https://doi.org/10.1016/B978-0-12-821953-9.00014-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128219539000143},
author = {Daniel A. Vallero},
keywords = {Systems science, Scientific method, Data-intensive discovery, Computational methods, Emergence, Fuzziness, Ecotone, Ecocline, Environmental risk, Biosolids},
abstract = {This chapter manifests how some of the connotations of systems apply to environmental science. The discussion begins with the history and evolution of scientific methods and paradigms, especially the agreement on the scientific method, spatial and temporal complexity, and the first principles of thermodynamics and motion. These are compared to modern environmental applications, including computational methods, governance, and emergence. Scientific and technical communication approaches needed for environmental systems science are described.}
}
@article{HUMPHREYS2020101346,
title = {Explaining short-term memory phenomena with an integrated episodic/semantic framework of long-term memory},
journal = {Cognitive Psychology},
volume = {123},
pages = {101346},
year = {2020},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2020.101346},
url = {https://www.sciencedirect.com/science/article/pii/S001002852030075X},
author = {Michael S. Humphreys and Gerald Tehan and Oliver Baumann and Shayne Loft},
keywords = {Short-term memory, Long-term memory, Episodic memory, Semantic memory, Systems consolidation, Associative interference, Context},
abstract = {Current thinking about human memory is dominated by distinctions between episodic and semantic memory and between short-term memory (STM) and long-term memory (LTM). However, many memory phenomena seem to cut across these distinctions. This article attempts to set the groundwork for the issues that need to be resolved in generating an integrated model of long-term memory that incorporates semantic, episodic, and short-term memory. We contrast Nairne’s (2002, Annual Review of Psychology) consensus account of short-term memory with a relatively generic theory of an integrated episodic-semantic memory. The later consists primarily of a list of principles which we and others argue are necessary to include in any theory of long-term memory. We then add some more specific assumptions to outline a modern theory of forgetting. We then turn to the issue of much of the phenomena thought to necessitate a dedicated short-term memory can be explained by an integrated theory of episodic and semantic memory. Our conclusion is that an integrated theory of long-term memory must be augmented to explain a small number of outstanding memory phenomena. Finally, we ask whether the augmentation needs to involve a dedicated mnemonic system, or sensory or language-based systems, which also have mnemonic capabilities.}
}
@article{JIANG2023104680,
title = {Using sequence mining to study students’ calculator use, problem solving, and mathematics achievement in the National Assessment of Educational Progress (NAEP)},
journal = {Computers & Education},
volume = {193},
pages = {104680},
year = {2023},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2022.104680},
url = {https://www.sciencedirect.com/science/article/pii/S0360131522002512},
author = {Yang Jiang and Gabrielle A. Cayton-Hodges and Leslie {Nabors Oláh} and Ilona Minchuk},
keywords = {Calculator, Assessment, Problem solving, Sequence mining, Process data, Mathematics},
abstract = {Using appropriate tools strategically to aid in problem solving is a crucial skill identified in K-12 mathematics curriculum standards. As more assessments transition from paper-and-pencil to digital formats, a variety of interactive tools have been made available to test takers in digital testing platforms. Using onscreen calculators as an example, this study illustrates how process data obtained from student interactions with a digitally-based large-scale assessment can be leveraged to explore how and how well test takers use interactive tools and unveil their mathematical problem-solving processes and strategies. Specifically, sequence mining techniques using the longest common subsequence were applied on process data collected from a nationally representative sample who took the National Assessment of Educational Progress (NAEP) mathematics assessment to examine patterns of eighth-grade students’ calculator-use behaviors and the content of calculator input across a series of items. Sequences of keystrokes executed on the onscreen calculator by test takers were compared to reference sequences identified by content experts as proficient and efficient use to infer how well and how consistently the calculator was used. Results indicated that calculator-use behaviors and content differed by item characteristics. Students were more likely to use calculators on calculation-demanding items that involve intensive and complex computations than on items that involve simple or no computation. Using the calculator on more calculation-demanding items and using it in a manner that is more efficient and more similar to reference sequences on these items were related to higher mathematical proficiency. Findings have implications for assessment design and can be used in educational practices to provide educators with actionable process-related information on tool use and problem solving.}
}
@article{BARTELS2008381,
title = {Principled moral sentiment and the flexibility of moral judgment and decision making},
journal = {Cognition},
volume = {108},
number = {2},
pages = {381-417},
year = {2008},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2008.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0010027708000607},
author = {Daniel M. Bartels},
keywords = {Morality, Judgment, Decision making, Values, Ethics, Intuition, Emotions, Reasoning, Moral rules, Moral dilemmas},
abstract = {Three studies test eight hypotheses about (1) how judgment differs between people who ascribe greater vs. less moral relevance to choices, (2) how moral judgment is subject to task constraints that shift evaluative focus (to moral rules vs. to consequences), and (3) how differences in the propensity to rely on intuitive reactions affect judgment. In Study 1, judgments were affected by rated agreement with moral rules proscribing harm, whether the dilemma under consideration made moral rules versus consequences of choice salient, and by thinking styles (intuitive vs. deliberative). In Studies 2 and 3, participants evaluated policy decisions to knowingly do harm to a resource to mitigate greater harm or to merely allow the greater harm to happen. When evaluated in isolation, approval for decisions to harm was affected by endorsement of moral rules and by thinking style. When both choices were evaluated simultaneously, total harm – but not the do/allow distinction – influenced rated approval. These studies suggest that moral rules play an important, but context-sensitive role in moral cognition, and offer an account of when emotional reactions to perceived moral violations receive less weight than consideration of costs and benefits in moral judgment and decision making.}
}
@article{WADHWA2022101177,
title = {Most significant hotspot detection using improved particle swarm optimizers},
journal = {Swarm and Evolutionary Computation},
volume = {75},
pages = {101177},
year = {2022},
issn = {2210-6502},
doi = {https://doi.org/10.1016/j.swevo.2022.101177},
url = {https://www.sciencedirect.com/science/article/pii/S2210650222001444},
author = {Ankita Wadhwa and Manish Kumar Thakur},
keywords = {Hotspot detection, Emergency response planning, Scan statistics, Improved particle swarm optimization, Geospatial data},
abstract = {Significant circular hotspot detection (SCHD) aims at identifying those circular regions in a spatial space where the occurrence of a particular activity is uncommonly higher than the surrounding areas. Plentiful societal applications make SCHD a problem of utmost interest. In many domains, detection of the most significant circular hotspot (MSCHD) is of further usefulness. Well-timed detection of the most significant hotspot helps crucially for short term response planning (STRP) in situations like a disease outbreak, police vigilance, etc. State of the art methods like SaTScan, identify circular hotspots by listing all possible circles in the search area, followed by a statistical significance test, making it a very high computational cost problem. Considering their high costs, these methods are inefficient in applications related to STRP. To reduce the computational time of SaTScan, two randomized versions of the SaTScan algorithm are presented in this paper. Further, the MSCHD problem is modeled as an optimization problem and three improved variants of Particle Swarm Optimizer (PSO) namely I-PSO, HCL-PSO and Ensemble PSO are applied to detect the most significant hotspots. The comparative and sensitivity analysis are performed using synthetic datasets. The comparative analysis of SaTScan and the presented PSO based schemes is made in terms of the quality of identified hotspots and the computational time. Results reveal that the PSO based schemes (I-PSO, HCL-PSO, and Ensemble PSO) are promising and far efficient than randomized and traditional SaTScan algorithms. Further, for the datasets containing only a single hotspot, the performance of all PSO based schemes is at par with each other. However, for datasets with more than one hotspot in the study area, HCL-PSO has lower average rank than I-PSO and Ensemble PSO schemes and hence seems more promising for MSCHD. The superiority of HCL-PSO based hotspot detection is also validated using Friedman Test. Finally, the presented schemes are applied to the case study of Chicago city for identification of different types of crime hotspots.}
}
@article{SELESNICK2012115,
title = {Quantum-like logics and schizophrenia},
journal = {Journal of Applied Logic},
volume = {10},
number = {1},
pages = {115-126},
year = {2012},
note = {Special issue on Automated Specification and Verification of Web Systems},
issn = {1570-8683},
doi = {https://doi.org/10.1016/j.jal.2011.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1570868311000656},
author = {S.A. Selesnick and G.S. Owen},
keywords = {Logic, Quantum logic, Linear logic, Schizophrenia},
abstract = {Many researchers in different disciplines have independently concluded that brains are, possibly among other things, vector processing devices. In this paper we offer support for this hypothesis coming from a new perspective. Namely, we test it against some known anomalies in the processing by schizophrenic patients of certain logical tasks: they perform better at them than normal controls, despite the observation that they do not generally employ “normal” or “commonsense” logic. On the assumption that they are compelled to use the intrinsic logic of the brain instead of commonsense logic, and that this logic is linear or quantum-like, we are able to resolve these and other anomalies. Our conclusions support the idea that human brains (at least) perform intrinsic logical operations according to the dictates of a linear (or Grassmannian, or quantum-like) logic rather than “classical” or Aristotelian logic (which seems not to be intrinsic to brains, these having evolved under the pressure of different constraints). If this is the case, then commonsense logic must be acquired through experience and the construction of contexts, an ability schizophrenic patients seem to lack, and who are consequently compelled to rely on the intrinsic logic, which is quantum-like and more efficient at certain tasks. Moreover, the proclivity toward errors of von Domarus type (namely the inference that shared attributes imply identity), which seems to be endemic to human thinking and has been discussed in connection with schizophrenia, is also explained on this basis.}
}
@article{WANG2023120829,
title = {DBCT-Net:A dual branch hybrid CNN-transformer network for remote sensing image fusion},
journal = {Expert Systems with Applications},
volume = {233},
pages = {120829},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120829},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423013313},
author = {Quanli Wang and Xin Jin and Qian Jiang and Liwen Wu and Yunchun Zhang and Wei Zhou},
keywords = {Image fusion, Convolutional neural network, Pansharpening, Transformer},
abstract = {Remote sensing image fusion aims at fusing high spatial resolution single-band panchromatic (PAN) image with spectrally informative multispectral (MS) image to generate panchromatic sharpened image with high resolution and color information, it is also called pansharpening. Most of the proposed single convolutional neural network (CNN) or transformer-based pansharpening methods own several problems, such as inability to acquire long-range features or difficult to train, resulting the loss of spatial details and colors. In addition, the computational complexity of transformer cannot be ignored. In this work, we propose a dual-branch hybrid CNN-Transformer network (DBCT-Net) that utilizes the local specificity of CNN and models the global dependencies by transformer. First, a multi-branch dense connected block (MDCB-4) network is designed to obtain spectral and textural information in MS and PAN images, respectively. Next, an encoder–decoder transformer based on the self-attention and co-attention modules is able to inject the missing local and global information, which can further enhance the results. It is worth noting that an inverted multi-head transposed attention (IMTA) is applied here to build attention maps from feature dimensions, which greatly reduces the computation time. Finally, an image reconstruction module is employed to effectively fuse the acquired texture and spectral features. Furthermore, to generate visually better pansharpened images, we propose a combined loss function that includes a focal frequency loss. Extensive experiments on WorldView II (WV2), GF-2,and QuickBird (QB) datasets show that DBCT-Net can perform better in spatial preservation and spectral feature recovery.}
}
@article{BUI1997575,
title = {Computational modelling of thermophysical processes in the light metals industry},
journal = {Revue Générale de Thermique},
volume = {36},
number = {8},
pages = {575-591},
year = {1997},
issn = {0035-3159},
doi = {https://doi.org/10.1016/S0035-3159(97)89985-0},
url = {https://www.sciencedirect.com/science/article/pii/S0035315997899850},
author = {Rung T Bui},
keywords = {computer modelling, light metals, thermophysical processes, electrolysis, casting, furnaces, modèles numériques, métaux légers, procédés thermophysiques, électrolyse, coulée, fours},
abstract = {This survey focuses on the aluminium industry, mostly on process aspects as opposed to metallurgical aspects. It covers recent work on process models involving fluid flow and heat transfer, and extends to all important categories of processes encountered in the primary aluminium industry, from raw materials and reduction to cast shop and recycling. This includes a wide variety of processes from precipitators, calciners, rotary kilns, baking furnaces, reduction cells, casting and mixing furnaces to recycling furnaces and metal filtration. A review is carried out on the modelling work, the applications, and the needs expressed not only in analysis and design but also in process control, optimization and supervision, as well as operator training. A summary is given of the problems perceived, mainly in the field of model parameters and model validation. Indications on future trends are also given. Conclusions are drawn from the survey of this fast-expanding body of knowledge that suggests tough challenges as well as unprecedented opportunities. Suggestions are made as to how some of those challenges could be met.
Résumé
Cette synthèse concerne l'industrie de l'aluminium et s'intéresse surtout aux procédés de fabrication, par opposition aux aspects métallurgiques. Elle couvre les travaux récents sur les modèles de procédés impliquant les écoulements et le transfert de chaleur. Elle inclut toutes les catégories de procédés rencontrées dans l'industrie de l'aluminium de première fusion, allant des matières premières à la réduction, la coulée et le recyclage. On y retrouve les précipitateurs, les calcinateurs, les fours rotatifs, les fours de cuisson, les cuves d'électrolyse, les fours de coulée ou de mélange, les fours de recyclage, ainsi que la filtration du métal. Les travaux de modélisation et leurs applications sont brièvement énumérés, les besoins exprimés, concernant non seulement l'analyse et la conception des procédés, mais aussi le contrôle, sont examinés, ainsi que des aspects touchant à l'optimisation, à la supervision et à la formation du personnel. Un résumé est fait des problèmes, notamment en ce qui a trait à la détermination des paramètres de modélisation et à la validation des modèles. On tente de dégager les tendances d'avenir. Des conclusions sont tirées de cette étude, qui semble mettre en lumière des défis de taille aussi bien que des opportunités sans précédent ; quelques suggestions visant à relever certains de ces défis sont proposées.}
}
@article{CUI2019305,
title = {Developing reflection analytics for health professions education: A multi-dimensional framework to align critical concepts with data features},
journal = {Computers in Human Behavior},
volume = {100},
pages = {305-324},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2019.02.019},
url = {https://www.sciencedirect.com/science/article/pii/S0747563219300718},
author = {Yi Cui and Alyssa Friend Wise and Kenneth L. Allen},
keywords = {Reflection, Learning analytics, Natural language processing, Professional education, Dental education, Health professions education},
abstract = {Reflection is a key activity in self-regulated learning (SRL) and a critical part of health professions education that supports the development of effective lifelong-learning health professionals. Despite widespread use and plentiful theoretical models, empirical understanding of and support for reflection in health professions education remains limited due to simple manual assessment and rare feedback to students. Recent moves to digital reflection practices offer opportunities to computationally study and support reflection as a part of SRL. The critical task in such an endeavor, and the goal of this paper, is to align high-level reflection qualities that are valued conceptually with low-level features in the data that are possible to extract computationally. This paper approaches this goal by (a) developing a unified framework for conceptualizing reflection analytics in health professions education and (b) empirically examining potential data features through which these elements can be assessed. Synthesizing the prior literature yields a conceptual framework for health professions reflection comprised of six elements: Description, Analysis, Feelings, Perspective, Evaluation, and Outcome. These elements then serve as the conceptual grounding for the computational analysis in which 27 dental students’ reflections (in six reflective statement types) over the course of 4 years were examined using selected LIWC (Linguistic Inquiry and Word Count) indices. Variation in elements of reflection across students, years, and reflection-types supports use of the multi-dimensional analysis framework to (a) increase precision of research claims; (b) evaluate whether reflection activities are engaged in as intended; and (c) diagnose aspects of reflection in which specific students need support. Implications for the development of health professions reflection analytics that can contribute to SRL and promising areas for future research are discussed.}
}
@article{WYLIE2025107472,
title = {15+ years of joint parallel application performance analysis/tools training with Scalasca/Score-P and Paraver/Extrae toolsets},
journal = {Future Generation Computer Systems},
volume = {162},
pages = {107472},
year = {2025},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.07.050},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24004187},
author = {Brian J.N. Wylie and Judit Giménez and Christian Feld and Markus Geimer and Germán Llort and Sandra Mendez and Estanislao Mercadal and Anke Visser and Marta García-Gasulla},
keywords = {Hybrid parallel programming, MPI message-passing, OpenMP multithreading, OpenACC device offload acceleration, HPC application execution performance measurement & analysis, Performance assessment & optimisation methodology & tools, Hands-on training & coaching},
abstract = {The diverse landscape of distributed heterogeneous computer systems currently available and being created to address computational challenges with the highest performance requirements presents daunting complexity for application developers. They must effectively decompose and distribute their application functionality and data, efficiently orchestrating the associated communication and synchronisation, on multi/manycore CPU processors with multiple attached acceleration devices structured within compute nodes with interconnection networks of various topologies. Sophisticated compilers, runtime systems and libraries are (loosely) matched with debugging, performance measurement and analysis tools, with proprietary versions by integrators/vendors provided exclusively for their systems complemented by portable (primarily) open-source equivalents developed and supported by the international research community over many years. The Scalasca and Paraver toolsets are two widely employed examples of the latter, installed on personal notebook computers through to the largest leadership HPC systems. Over more than fifteen years their developers have worked closely together in numerous collaborative projects culminating in the creation of a universal parallel performance assessment and optimisation methodology focused on application execution efficiency and scalability, and the associated training and coaching of application developers (often in teams) in its productive use, reviewed in this article with lessons learnt therefrom.}
}
@article{GUZMANURBINA2022109295,
title = {FIEMA, a system of fuzzy inference and emission analytics for sustainability-oriented chemical process design},
journal = {Applied Soft Computing},
volume = {126},
pages = {109295},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.109295},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622004859},
author = {Alexander Guzman-Urbina and Kakeru Ouchi and Hajime Ohno and Yasuhiro Fukushima},
keywords = {Sustainability engineering, Emission analytics, Fuzzy systems, Data clustering},
abstract = {In the quest to achieve sustainable development goals, developments in sustainability-oriented chemical process design are key to innovation in the chemical industry, especially important for processes aiming for sustainable fuels. One of the greatest challenges is the difficulty of modeling the highly complex interactions among the design variables, such as catalyst technology attributes, and greenhouse gas emissions. Most of the computational aids crucial to deal with the complexity of chemical processes require data that is either unavailable or uncertain at an early stage of design. The multistage integrated system for sustainable design proposed in this paper boosts these computational aids by applying data science techniques to allow uncertainty to be handled more efficiently, thereby facilitating the modeling of the interactions between the properties of new materials or processes and sustainability indicators. In this system, current data connectivity methods are used to find paths of correlation among catalysts properties and greenhouse gas emissions. The key feature of the proposed system relies on the integration through multiple stages of Fuzzy Inference systems and a data-driven technique for Emissions Analytics, FIEMA.11FIEMA: Fuzzy Inference and Emission Analytics. The algorithm in FIEMA provides a semi-supervised learning approach to emission analytics: it determines data clusters by a C-means algorithm and subsequently builds fuzzy sets for multiple stages of input–output inference. The proposed FIEMA system was demonstrated in an effort to determine the optimal configurations of the properties of catalysts to minimize the probability of associated greenhouse gas emissions for a methanol production process. The results showed the potential of this approach to reduce the search space of catalyst material designs by suggesting promising configurations for oxygen storage capacity, mechanical strength, lifetime, size, and poisoning level. The research impacts of this study contribute to the development of clean fuels by a computationally-efficient system for early design, and by the determination of catalysts development paths that assure an actual reduction of the life-cycle emissions.}
}
@article{SUN2020104036,
title = {R4 Det: Refined single-stage detector with feature recursion and refinement for rotating object detection in aerial images},
journal = {Image and Vision Computing},
volume = {103},
pages = {104036},
year = {2020},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2020.104036},
url = {https://www.sciencedirect.com/science/article/pii/S0262885620301682},
author = {Peng Sun and Yongbin Zheng and Zongtan Zhou and Wanying Xu and Qiang Ren},
keywords = {Rotating object detection in aerial images and videos, Single-stage detector, A novel encoder-decoder architecture, Recursive feature pyramid, Recursive feature contact, RetinaNet-based rotation detection, Feature refinement},
abstract = {The detection of objects with multi-orientations and multi-scales in aerial images is receiving increasing attention because of numerous useful applications in computer vision, image understanding, satellite remote sensing and surveillance. However, such detection can be exceedingly challenging because of a birds eye view, multi-scale rotating objects with large aspect ratios, dense distributions and extremely imbalanced categories. Despite the considerable progress that has been made, detection performance falls considerably below that required for real-world applications. In this paper, we propose an accurate and fast end-to-end detector to address the aforementioned challenges. Our contributions are threefold. First, inspired by the looking and thinking twice mechanism, recursive neural networks and the DetectoRS detector, we propose a novel encoder-decoder based architecture by introducing the recursive feature pyramid into a single-stage object detection framework. The improved backbone network can generate increasingly powerful multi-scale representations for classification and regression. Second, we propose a refined single-stage detector with feature recursion and refinement for rotating objects. Third, we use instance balance to improve focal loss, thereby optimizing the loss in the correct direction. Extensive experiments on two challenging aerial image object detection public datasets, DOTA and HRSC2016, show that the proposed R4Det detector achieves the state-of-the-art accuracy while running very fast. Moreover, further experiments show that our detector is more robust to adversarial image patch attacks than the previous state-of-art detector.}
}
@article{MCNAUGHTON2024102653,
title = {The homogenous hippocampus: How hippocampal cells process available and potential goals},
journal = {Progress in Neurobiology},
volume = {240},
pages = {102653},
year = {2024},
issn = {0301-0082},
doi = {https://doi.org/10.1016/j.pneurobio.2024.102653},
url = {https://www.sciencedirect.com/science/article/pii/S0301008224000893},
author = {Neil McNaughton and David Bannerman},
keywords = {Hippocampus, Cell fields, Iteration, Goals, Space, Anxiety, Memory, Eye blink conditioning, Vinogradova},
abstract = {We present here a view of the firing patterns of hippocampal cells that is contrary, both functionally and anatomically, to conventional wisdom. We argue that the hippocampus responds to efference copies of goals encoded elsewhere; and that it uses these to detect and resolve conflict or interference between goals in general. While goals can involve space, hippocampal cells do not encode spatial (or other special types of) memory, as such. We also argue that the transverse circuits of the hippocampus operate in an essentially homogeneous way along its length. The apparently different functions of different parts (e.g. memory retrieval versus anxiety) result from the different (situational/motivational) inputs on which those parts perform the same fundamental computational operations. On this view, the key role of the hippocampus is the iterative adjustment, via Papez-like circuits, of synaptic weights in cell assemblies elsewhere.}
}
@article{ROTHMCDUFFIE2018173,
title = {Middle school mathematics teachers’ orientations and noticing of features of mathematics curriculum materials},
journal = {International Journal of Educational Research},
volume = {92},
pages = {173-187},
year = {2018},
issn = {0883-0355},
doi = {https://doi.org/10.1016/j.ijer.2018.09.019},
url = {https://www.sciencedirect.com/science/article/pii/S0883035518305512},
author = {Amy {Roth McDuffie} and Jeffrey Choppin and Corey Drake and Jon Davis},
keywords = {Curriculum, Curriculum analysis, Teacher orientation, Middle school mathematics, Teacher noticing},
abstract = {We report findings on teachers’ noticing of features in the teacher resources of mathematics curriculum programs. Based on prior analysis, we selected teachers using one of two curriculum types: delivery mechanism or thinking device. The participating teachers and the curriculum programs aimed to align with the Common Core Standards for Mathematics, and thus, they ostensibly held a common aim for instruction. We analyzed 147 lesson planning interviews with 20 middle school mathematics teachers. We found that teachers attended to similar features of teacher resources; however, patterns for interpreting and planning decisions varied based on teachers’ orientations and curriculum type.}
}
@incollection{DALE201343,
title = {Chapter Two - The Self-Organization of Human Interaction},
editor = {Brian H. Ross},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {59},
pages = {43-95},
year = {2013},
issn = {0079-7421},
doi = {https://doi.org/10.1016/B978-0-12-407187-2.00002-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780124071872000022},
author = {Rick Dale and Riccardo Fusaroli and Nicholas D. Duran and Daniel C. Richardson},
keywords = {Alignment, Conversation, Coordination, Dynamics, Interaction, Language, Self-organization, Synergy},
abstract = {We describe a “centipede’s dilemma” that faces the sciences of human interaction. Research on human interaction has been involved in extensive theoretical debate, although the vast majority of research tends to focus on a small set of human behaviors, cognitive processes, and interactive contexts. The problem is that naturalistic human interaction must integrate all of these factors simultaneously, and grander theoretical mitigation cannot come only from focused experimental or computational agendas. We look to dynamical systems theory as a framework for thinking about how these multiple behaviors, processes, and contexts can be integrated into a broader account of human interaction. By introducing and utilizing basic concepts of self-organization and synergy, we review empirical work that shows how human interaction is flexible and adaptive and structures itself incrementally during unfolding interactive tasks, such as conversation, or more focused goal-based contexts. We end on acknowledging that dynamical systems accounts are very short on concrete models, and we briefly describe ways that theoretical frameworks could be integrated, rather than endlessly disputed, to achieve some success on the centipede’s dilemma of human interaction.}
}
@article{POLHILL2023103121,
title = {Cognition and hypocognition: Discursive and simulation-supported decision-making within complex systems},
journal = {Futures},
volume = {148},
pages = {103121},
year = {2023},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2023.103121},
url = {https://www.sciencedirect.com/science/article/pii/S0016328723000253},
author = {J. Gareth Polhill and Bruce Edmonds},
keywords = {Simulation, Cognition, Hypocognition, Divination, Ecocyborgs, Blasphemy},
abstract = {Homo sapiens is currently believed to have evolved in the African savannah several hundreds of thousands of years ago. Since then, human societies have become, through technological innovation and application, powerful influencers of the planet’s ecological, hydrological and meteorological systems – for good and ill. They have experimented with many different systems of governance, in order to manage their societies and the environments they inhabit – using computer simulations as a tool to help make decisions concerning highly complex systems, is only the most recent of these. In questioning whether, when and how computer simulations should play a role in determining decision-making in these systems of governance, it is also worth reflecting on whether, when and how humans, or groups of humans, have the capability to make such decisions without the aid of such technology. This paper looks at and compares the characteristics of natural language-based and simulation-based decision-making. We argue that computational tools for decision-making can and should be complementary to natural language discourse approaches, but that this requires that both systems are used with their limitations in mind. All tools and approaches – physical, social and mental – have dangers when used inappropriately, but it seems unlikely humankind can survive without them. The challenge is how to do so.}
}
@article{KUZNETSOV20206378,
title = {Harmonic balance analysis of pull-in range and oscillatory behavior of third-order type 2 analog PLLs},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {6378-6383},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.1773},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320323818},
author = {N.V. Kuznetsov and M.Y. Lobachev and M.V. Yuldashev and R.V. Yuldashev and G. Kolumbán},
keywords = {Phase-locked loop, third-order PLL, type 2 PLL, nonlinear analysis, harmonic balance method, describing function, global stability, birth of oscillations, hold-in range, pull-in range, lock-in range, Egan conjecture},
abstract = {The most important design parameters of each phase-locked loop (PLL) are the local and global stability properties, and the pull-in range. To extend the pull-in range, engineers often use type 2 PLLs. However, the engineering design relies on approximations which prevent a full exploitation of the benefits of type 2 PLLs. Using an exact mathematical model and relying on a rigorous mathematical thinking this problem is revisited here and the stability and pull-in properties of the third-order type 2 analog PLLs are determined. Both the local and global stability conditions are derived. As a new idea, the harmonic balance method is used to derive the global stability conditions. That approach offers an extra advantage, the birth of unwanted oscillations can be also predicted. As a verification it is shown that the sufficient conditions of global stability derived by the harmonic balance method proposed here and the well-known direct Lyapunov approach coincide with each other, moreover, the harmonic balance predicts the birth of oscillations in the gap between the local and global stability conditions. Finally, an example when the conditions for local and global stability coincide, is considered.}
}
@article{SHER1992505,
title = {A computational normative theory of scientific evidence},
journal = {International Journal of Approximate Reasoning},
volume = {6},
number = {4},
pages = {505-524},
year = {1992},
issn = {0888-613X},
doi = {https://doi.org/10.1016/0888-613X(92)90002-H},
url = {https://www.sciencedirect.com/science/article/pii/0888613X9290002H},
author = {David B. Sher},
keywords = {interval probability, evidence combination, experimental evidence, probabilistic logic, statistical inference},
abstract = {A scientific reasoning system makes decisions using objective evidence in the form of independent experimental trials, propositional axioms, and constraints on the probabilities of events. I propose a collection of algorithms that derive probability intervals and estimate conditional probabilities from objective evidence in those forms. This reasoning system can manage uncertainty about data and rules in a rule-based expert system. I expect that the system will be particularly applicable to diagnosis and analysis in domains with a wealth of experimental evidence such as medicine. The algorithms currently apply to systems with arbitrary amounts of experimental evidence but with less than 20 variables. I discuss limitations of this solution and propose future directions for this research. This work can be considered a generalization of Nilsson's “probabilistic logic” to intervals and experimental observations.}
}
@incollection{ERNST2021265,
title = {Chapter 22 - Pharmaceutical toxicology},
editor = {Martin Wehling},
booktitle = {Principles of Translational Science in Medicine (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Boston},
pages = {265-279},
year = {2021},
isbn = {978-0-12-820493-1},
doi = {https://doi.org/10.1016/B978-0-12-820493-1.00008-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204931000088},
author = {Steffen W. Ernst and Richard Knight and Jenny Royle and Laura Stephenson},
keywords = {Regulatory toxicology, discovery toxicology, dose-resonse relationship, pharmaceutial safety, drug develoment, risk assessment},
abstract = {This chapter aims to highlight the core principles of pharmaceutical toxicology. It is an interrelated discipline that needs to be applied at all stages of the drug development process to appropriately characterise the safety profile of a drug compound and acknowledge the uncertainties associated with models available. With a strategic mindset, the preclinical safety activities aim to build a comprehensive profile of the drug so that potential hazards can be identified and the risks for healthy trial subjects or patients quantified, and, if necessary, suitable means for eliminating or reducing unacceptable risks can be put in place. We focus on the 2 distinct phases of pharmaceutical toxicology:  Discovery toxicology and regulatory toxicology, to explain how the thinking is built upon at each stage and how the mindset shifts from enabling the selection of an optimally derisked clinical candidate through to thorough risk characterisation and management of those risks for clinical development. Considering attrition due to safety reasons, whether clinical or preclinical, is one of the main reasons for drug project failure, safety assessments should be viewed with equal importance as drug efficacy assessments.}
}
@article{YE2024116982,
title = {A fast cosine transformation accelerated method for predicting effective thermal conductivity},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {426},
pages = {116982},
year = {2024},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2024.116982},
url = {https://www.sciencedirect.com/science/article/pii/S004578252400238X},
author = {Changqing Ye and Shubin Fu and Eric T. Chung},
keywords = {Effective thermal conductivity, Preconditioner, Fast cosine transformation, CUDA, GPU},
abstract = {Predicting effective thermal conductivity by solving a Partial Differential Equation (PDE) defined on a high-resolution Representative Volume Element (RVE) is a computationally intensive task. In this paper, we tackle the task by proposing an efficient and implementation-friendly computational method that can fully leverage the computing power offered by hardware accelerators, namely, graphical processing units (GPUs). We first employ the Two-Point Flux-Approximation scheme to discretize the PDE and then utilize the preconditioned conjugate gradient method to solve the resulting algebraic linear system. The construction of the preconditioner originates from FFT-based homogenization methods, and an engineered linear programming technique is utilized to determine the homogeneous reference parameters. The fundamental observation presented in this paper is that the preconditioner system can be effectively solved using multiple Fast Cosine Transformations (FCT) and parallel tridiagonal matrix solvers. Regarding the fact that default multiple FCTs are unavailable on the CUDA platform, we detail how to derive FCTs from FFTs with nearly optimal memory usage. Numerical experiments including the stability comparison with standard preconditioners are conducted for 3D RVEs. Our performance reports indicate that the proposed method can achieve a 5-fold acceleration on the GPU platform over the pure CPU platform and solve the problems with 5123 degrees of freedom and reasonable contrast ratios in less than 30 s.}
}
@article{TEMPL20249,
title = {Advancing forensic research: An examination of compositional data analysis with an application on petrol fraud detection},
journal = {Science & Justice},
volume = {64},
number = {1},
pages = {9-18},
year = {2024},
issn = {1355-0306},
doi = {https://doi.org/10.1016/j.scijus.2023.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S1355030623001223},
author = {M. Templ and J. Gonzalez-Rodriguez},
keywords = {Forensic science, Petrol data, Chemical compounds, Compositional data analysis, Classification},
abstract = {In recent years, numerous studies have examined the chemical compounds of petrol and petrol data for forensic research. Standard quantitative methods often assume that the variables or compounds do not have compositional constraints or are not part of a constrained whole, operating within an Euclidean vector space. However, chemical compounds are typically part of a whole, and the appropriate vector space for their analysis is the simplex. Biased and arbitrary results result when statistical analysis are applied on such data without proper pre-processing of such data. Compositional analysis of data has not yet been considered in forensic science. Therefore, we compare classical statistical analysis as applied in forensic research and the new proposed paradigm of compositional data analysis (CoDa). It is demonstrated how such analysis improves the analysis in petrol and forensic science. Our study shows how principal component analysis (PCA) and classification results are affected by the preprocessing steps performed on the raw data. Our results indicate that results from a log ratio analysis provides a better separation between subgroups of the data and leads to an easier interpretation of the results. In addition, with a compositional analysis a higher classification accuracy is obtained. Even a non-linear classification method - in our case a random forest - was shown to perform poorly when applied without using compositional methods. Moreover, normalization of samples due to laboratory/unit-of-measurement effects is no longer necessary, since the composition of an observation is in compositional thinking equivalent to a multiple of it, because the used (log) ratios on raw and log ratio transformed data are equal. Petrol data from different petrol stations in Brazil are used for the demonstration. This data is highly susceptible to counterfeit petrol. Forensic analysis of its chemical elements requires non-biased statistical analysis designed for compositional data to detect fraud. Based on these results, we recommend the use of compositional data methods for gasoline and petrol chemical element analysis and gasoline product characterization, authentication and fraud detection in forensic sciences.}
}
@article{DOSHI2020103202,
title = {Recursively modeling other agents for decision making: A research perspective},
journal = {Artificial Intelligence},
volume = {279},
pages = {103202},
year = {2020},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2019.103202},
url = {https://www.sciencedirect.com/science/article/pii/S000437021930027X},
author = {Prashant Doshi and Piotr Gmytrasiewicz and Edmund Durfee},
keywords = {Decision theory, Game theory, Hierarchical beliefs, Multiagent systems, Recursive modeling, Theory of mind},
abstract = {Individuals exhibit theory of mind, attributing beliefs, intent, and mental states to others as explanations of observed actions. Dennett's intentional stance offers an analogous abstraction for computational agents seeking to understand, explain, or predict others' behaviors. These recognized theories provide a formal basis to ongoing investigations of recursive modeling. We review and situate various frameworks for recursive modeling that have been studied in game- and decision- theories, and have yielded methods useful to AI researchers. Sustained attention given to these frameworks has produced new analyses and methods with an aim toward making recursive modeling practicable. Indeed, we also review some emerging uses and the insights these yielded, which are indicative of pragmatic progress in this area. The significance of these frameworks is that higher-order reasoning is critical to correctly recognizing others' intent or outthinking opponents. Such reasoning has been utilized in academic, business, military, security, and other contexts both to train and inform decision-making agents in organizational and strategic contexts, and also to more realistically predict and best respond to other agents' intent.}
}
@article{DEMSAR2007551,
title = {Investigating visual exploration of geospatial data: An exploratory usability experiment for visual data mining},
journal = {Computers, Environment and Urban Systems},
volume = {31},
number = {5},
pages = {551-571},
year = {2007},
note = {Geospatial Analysis and Modeling},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2007.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0198971507000579},
author = {Urška Demšar},
keywords = {Exploratory geovisualisation, Visual data mining, Exploratory usability},
abstract = {This study presents a small exploratory usability experiment with the goal to observe how people visually explore geospatial data. The well-known iris dataset from pattern recognition was put into geographical context for this experiment, in order to provide the participants with a dataset with easily observable spatial and other relationships. The participants were given free hand to explore this dataset with a visual data mining system in any way they liked. The protocols collected during the experiment with the thinking-aloud method were analysed with the aim to understand what types of hypotheses the participants formed, which visualisations they used to either derive, confirm or reject their hypotheses and what exploration strategies they adopted.}
}
@article{MORETTI1980145,
title = {Computational aerodynamics using mini computers},
journal = {Computers & Fluids},
volume = {8},
number = {1},
pages = {145-153},
year = {1980},
note = {Special Issue: Computers in Aerodynamics},
issn = {0045-7930},
doi = {https://doi.org/10.1016/0045-7930(80)90037-7},
url = {https://www.sciencedirect.com/science/article/pii/0045793080900377},
author = {Gino Moretti},
abstract = {The importance of minicomputers as a research tool in gasdynamics is explained, and a few examples are given to show their efficiency.}
}
@article{ESSAKALI2024123910,
title = {Advanced predictive maintenance and fault diagnosis strategy for enhanced HVAC efficiency in buildings},
journal = {Applied Thermal Engineering},
volume = {254},
pages = {123910},
year = {2024},
issn = {1359-4311},
doi = {https://doi.org/10.1016/j.applthermaleng.2024.123910},
url = {https://www.sciencedirect.com/science/article/pii/S1359431124015783},
author = {Niima Es-sakali and Zineb Zoubir and Samir {Idrissi Kaitouni} and Mohamed Oualid Mghazli and Moha Cherkaoui and Jens Pfafferott},
keywords = {Variable refrigerant flow, Predictive maintenance, Fault detection and diagnosis, Refrigerant leakage, Noise reduction models, Building Environment},
abstract = {Variable refrigerant flow (VRF) systems are constantly prone to failures during their lifespan, causing breakdowns, high energy bills, and indoor discomfort. In addition to correctly identifying these defects, fault detection, and diagnostic studies should be able to anticipate and predict the anomalies before they occur for efficient maintenance. Therefore, this study introduces an efficient self-learning predictive maintenance system, CACMMS (Cloud Air Conditioning Monitoring & Management System), designed to anticipate refrigerant leaks in VRF systems. Unlike previous efforts, this system leverages advanced fault detection and diagnosis strategies in a real existing building to enhance prediction accuracy. The study employed three noise filtering models (Kalman filter, moving average, S-G smoothing) in the preprocessing phase. Ten features were selected for assessment, and four machine learning models (decision tree, random forest, K-nearest neighbor, support vector machine) were compared. The accuracy, precision, sensitivity, computation time as well and confusion matrix were used as performance indicators and metrics to evaluate and choose the best performant model. Results indicated that decision tree and random forest models achieved over 95 % accuracy with execution times between 0.70 s and 3.32 s, outperforming K-nearest neighbor and support vector machine models. These findings highlight the system’s potential to reduce downtime and energy costs through effective predictive maintenance.}
}
@article{YEUNG2024104999,
title = {A systematic review of Drone integrated STEM education at secondary schools (2005–2023): Trends, pedagogies, and learning outcomes},
journal = {Computers & Education},
volume = {212},
pages = {104999},
year = {2024},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2024.104999},
url = {https://www.sciencedirect.com/science/article/pii/S0360131524000137},
author = {Richard Chung Yiu Yeung and Chi Ho Yeung and Daner Sun and Chee-Kit Looi},
keywords = {Systematic review, Drone-integrated learning, STEM education, Secondary schools},
abstract = {As the prominence of drone technology continues to captivate interest for its myriad applications in education, an understanding of the current status of drone-integrated education becomes imperative. This systematic review endeavors to furnish an updated and comprehensive analysis of the drone education studies across academic levels, with a specific emphasis on secondary education settings. To accomplish this objective, a review study with 181 publications was conducted, with a particular focus on 41 publications explicitly addressing the integration of drones in secondary STEM education. Employing a systematic approach, this review identifies, analyzes, and synthesizes pertinent literature, ensuring a thorough comprehension of the current state of the field. The key findings of this review can be summarized as follows: 1) Among the diverse array of subjects incorporating drones, STEM disciplines emerge as the most prominently featured. 2) Experiential and project-based learning stand out as the most commonly adopted pedagogical methods in drone-integrated STEM education. The incorporation of teamwork and hands-on activities is frequently cited as instructional strategies aimed at enhancing drone-integrated STEM learning experiences. 3) Beyond the acquisition of drone-related technical skills, the reported learning outcomes encompass a spectrum of aspects, including heightened STEM career awareness, increased engagement and learning interest, and collaborative problem-solving abilities. The findings underscore the potential of drones to ignite passion for STEM subjects among secondary students, achieved through interdisciplinary, hands-on applications that foster problem-solving and design competencies.}
}
@article{HENNE2019157,
title = {A counterfactual explanation for the action effect in causal judgment},
journal = {Cognition},
volume = {190},
pages = {157-164},
year = {2019},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2019.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0010027719301301},
author = {Paul Henne and Laura Niemi and Ángel Pinillos and Felipe {De Brigard} and Joshua Knobe},
keywords = {Action effect, Omissions, Omission effect, Causal reasoning, Counterfactual thinking, Causation by omission},
abstract = {People’s causal judgments are susceptible to the action effect, whereby they judge actions to be more causal than inactions. We offer a new explanation for this effect, the counterfactual explanation: people judge actions to be more causal than inactions because they are more inclined to consider the counterfactual alternatives to actions than to consider counterfactual alternatives to inactions. Experiment 1a conceptually replicates the original action effect for causal judgments. Experiment 1b confirms a novel prediction of the new explanation, the reverse action effect, in which people judge inactions to be more causal than actions in overdetermination cases. Experiment 2 directly compares the two effects in joint-causation and overdetermination scenarios and conceptually replicates them with new scenarios. Taken together, these studies provide support for the new counterfactual explanation for the action effect in causal judgment.}
}
@article{KHISTY200577,
title = {Possibilities of steering the transportation planning process in the face of bounded rationality and unbounded uncertainty},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {13},
number = {2},
pages = {77-92},
year = {2005},
note = {Handling Uncertainty in the Analysis of Traffic and Transportation Systems (Bari, Italy, June 10–13 2002)},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2005.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X05000161},
author = {C. Jotin Khisty and Turan Arslan},
keywords = {Paradigm shift, Planning, Rationality, Systemicity, Transportation, Uncertainty},
abstract = {This paper describes and discusses the possibilities of steering the transportation planning process in the face of bounded rationality and unbounded uncertainty: (a) through the introduction of the concept of ‘systemicity’; (b) by expanding the spectrum of the existing planning paradigm currently in use; (c) by reducing complexity through the application of tests of adequacy, dependency, suitability, and adaptability; (d) through the introduction of soft systems thinking; and (e) by using ‘abductive’ in addition to deductive and inductive inferencing. It is concluded that the application of these strategies, adjustments, and tests to the existing planning procedure will hopefully enrich and strengthen our planning effort and make it more robust.}
}
@article{TAGHIKHANI2024114574,
title = {A hybrid modified PSO algorithm for the inverse p-median location problem in fuzzy random environment},
journal = {Theoretical Computer Science},
volume = {1000},
pages = {114574},
year = {2024},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2024.114574},
url = {https://www.sciencedirect.com/science/article/pii/S0304397524001890},
author = {Sepideh Taghikhani and Fahimeh Baroughi and Behrooz Alizadeh},
keywords = {Inverse p-median location problem, Fuzzy random variable, Conditional value at risk, Particle swarm optimization},
abstract = {This paper considers the inverse p-median location problem with variable edge lengths and variable vertex weights on general graphs in which the modification costs are the fuzzy random variables. We present a model for the problem in fuzzy random environment in which the objective value is computed by conditional value at risk criterion. Then, we show that the problem is NP-hard under this criterion. Therefore, a new hybrid modified particle swarm optimization algorithm is proposed to obtain the approximate optimal solution of the proposed model. Finally, computational experiments are given to illustrate high efficiency of the proposed algorithm.}
}
@article{LEANZA20236716,
title = {Into the dynamics of rotaxanes at atomistic resolution††Electronic supplementary information (ESI) available: ESI Movies 1 and 2. See DOI: https://doi.org/10.1039/d3sc01593a},
journal = {Chemical Science},
volume = {14},
number = {24},
pages = {6716-6729},
year = {2023},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d3sc01593a},
url = {https://www.sciencedirect.com/science/article/pii/S2041652023062065},
author = {Luigi Leanza and Claudio Perego and Luca Pesce and Matteo Salvalaglio and Max {von Delius} and Giovanni M. Pavan},
abstract = {Mechanically-interlocked molecules (MIMs) are at the basis of artificial molecular machines and are attracting increasing interest for various applications, from catalysis to drug delivery and nanoelectronics. MIMs are composed of mechanically-interconnected molecular sub-parts that can move with respect to each other, imparting these systems innately dynamical behaviors and interesting stimuli-responsive properties. The rational design of MIMs with desired functionalities requires studying their dynamics at sub-molecular resolution and on relevant timescales, which is challenging experimentally and computationally. Here, we combine molecular dynamics and metadynamics simulations to reconstruct the thermodynamics and kinetics of different types of MIMs at atomistic resolution under different conditions. As representative case studies, we use rotaxanes and molecular shuttles substantially differing in structure, architecture, and dynamical behavior. Our computational approach provides results in agreement with the available experimental evidence and a direct demonstration of the critical effect of the solvent on the dynamics of the MIMs. At the same time, our simulations unveil key factors controlling the dynamics of these systems, providing submolecular-level insights into the mechanisms and kinetics of shuttling. Reconstruction of the free-energy profiles from the simulations reveals details of the conformations of macrocycles on the binding site that are difficult to access via routine experiments and precious for understanding the MIMs' behavior, while their decomposition in enthalpic and entropic contributions unveils the mechanisms and key transitions ruling the intermolecular movements between metastable states within them. The computational framework presented herein is flexible and can be used, in principle, to study a variety of mechanically-interlocked systems.}
}
@article{MARTIN1993141,
title = {Neural connections, mental computation: Lynn Nadel, Lynn A. Cooper, Peter Culicover and R. Michael Harnish, eds.},
journal = {Artificial Intelligence},
volume = {62},
number = {1},
pages = {141-151},
year = {1993},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(93)90052-D},
url = {https://www.sciencedirect.com/science/article/pii/000437029390052D},
author = {Benjamin Martin}
}
@article{DUAN2023103365,
title = {Mining multigranularity decision rules of concept cognition for knowledge graphs based on three-way decision},
journal = {Information Processing & Management},
volume = {60},
number = {4},
pages = {103365},
year = {2023},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2023.103365},
url = {https://www.sciencedirect.com/science/article/pii/S0306457323001024},
author = {Jiangli Duan and Guoyin Wang and Xin Hu and Deyou Xia and Di Wu},
keywords = {Granular computing, Cognitive intelligence, Concept cognition, Knowledge graph, Three-way decision},
abstract = {Machine understanding and thinking require prior knowledge consisting of explicit and implicit knowledge. The current knowledge base contains various explicit knowledge but not implicit knowledge. As part of implicit knowledge, the typical characteristics of the things referred to by the concept are available by concept cognition for knowledge graphs. Therefore, this paper attempts to realize concept cognition for knowledge graphs from the perspective of mining multigranularity decision rules. Specifically, (1) we propose a novel multigranularity three-way decision model that merges the ideas of multigranularity (i.e., from coarse granularity to fine granularity) and three-way decision (i.e., acceptance, rejection, and deferred decision). (2) Based on the multigranularity three-way decision model, an algorithm for mining multigranularity decision rules is proposed. (3) The monotonicity of positive or negative granule space ensured that the positive (or negative) granule space from coarser granularity does not need to participate in the three-classification process at a finer granularity, which accelerates the process of mining multigranularity decision rules. Moreover, the experimental results show that the multigranularity decision rule is better than the two-way decision rule, frequent decision rule and single granularity decision rule, and the monotonicity of positive or negative granule space can accelerate the process of mining multigranularity decision rules.}
}
@incollection{MONTEIRO202253,
title = {4 - An artificial intelligent cognitive approach for classification and recognition of white blood cells employing deep learning for medical applications},
editor = {Deepak Gupta and Utku Kose and Ashish Khanna and Valentina Emilia Balas},
booktitle = {Deep Learning for Medical Applications with Unique Data},
publisher = {Academic Press},
pages = {53-69},
year = {2022},
isbn = {978-0-12-824145-5},
doi = {https://doi.org/10.1016/B978-0-12-824145-5.00012-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128241455000125},
author = {Ana Carolina Borges Monteiro and Reinaldo Padilha França and Rangel Arthur and Yuzo Iano},
keywords = {Artificial intelligence, Biomedical signals, CNN, Cognitive computing, Cognitive health care, Cognitive models, Deep learning, Digital image, Erythrocytes, Health care, Health care data, Health care informatics, Image processing, Leukocytes, Python},
abstract = {Cognitive computing aims to implement a unified computational theory similar to human thought, consisting of systems whose objective is to mimic human mental tasks based on the concepts of artificial intelligence and machine learning generating and understanding knowledge. Deep learning is an abstraction of the biological neural network, and can understood as a complex structure interconnected by simple processing elements (neurons), can perform operations as calculations in parallel, for data processing and representation of knowledge. Convolutional neural networks for image recognition through deep learning has been modeled to determine good performance with respect to digital image recognition, especially in medical areas, which is a classic problem of computational classification. In this context, an artificial intelligent cognitive approach was developed achieving an accuracy of 84.19%, which used Python employing Jupyter Notebook, with a dataset of 12,500 medical digital images of human blood smear fields of nonpathologic leukocytes.}
}
@article{SAHU2023105206,
title = {SCZ-SCAN: An automated Schizophrenia detection system from electroencephalogram signals},
journal = {Biomedical Signal Processing and Control},
volume = {86},
pages = {105206},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2023.105206},
url = {https://www.sciencedirect.com/science/article/pii/S1746809423006390},
author = {Geet Sahu and Mohan Karnati and Abhishek Gupta and Ayan Seal},
keywords = {Schizophrenia, Electroencephalography, Continuous wavelet transform, Scalogram, Convolutional neural network},
abstract = {Schizophrenia (SCZ) is a severe neurological and physiological syndrome that perverts a patient’s perception of reality. SCZ exhibits several symptoms, including hallucinations, delusions, aberrant behavior, and thinking. It affects their professional, academic, personal, and social lives. Neurologists use a variety of verbal and visual tests to determine SCZ. However, these methods are laborious, time-consuming, superficial, and vulnerable to mistakes. Therefore, it is necessary to create an automated model for SCZ detection. Convolutional neural networks have swiftly established themselves in the field of mental health care due to the growth of deep learning in recent decades. Electroencephalogram (EEG) data records the variations in the neural dynamics of human memory. Using EEG data, this study proposes an automatic SCZ detection method using separable convolution attention network (SCZ-SCAN). The proposed network employs depth-wise separable convolution and attention networks on high-level and low-level to aggregate characteristics of 2-D scalogram images acquired from the continuous wavelet transform. The depth-wise separable convolutions help to create a lightweight framework, while attention techniques concentrate on significant features and reduce futile computations by removing the transmission of irrelevant features. The proposed approach has an average classification accuracy of 99% and 95% on the IBIB-PAN and EEG data from the basic sensory task in SZ dataset. Moreover, statistical hypothesis testing is performed using Wilcoxon’s Rank-Sum test to signify the model performance and it proves that SCZ-SCAN is statistically efficient to nine cutting-edge methods. Experimental results show that the PSFAN statistically defeats 11 contemporary methods, proving its effectiveness for medical industrial applications.}
}
@article{HORVATH2024593,
title = {AI for conceptual architecture: Reflections on designing with text-to-text, text-to-image, and image-to-image generators},
journal = {Frontiers of Architectural Research},
volume = {13},
number = {3},
pages = {593-612},
year = {2024},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2024.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S2095263524000256},
author = {Anca-Simona Horvath and Panagiota Pouliou},
keywords = {Machine learning, StyleGAN2-ADA, RNN TensorFlow, VQGAN + clip, AD journal, eVolo, Conceptual design, Architectural design},
abstract = {In this paper we present a research-through-design study where we employed text-to-text, text-to-image, and image-to-image generative tools for a conceptual architecture project for the eVolo skyscraper competition. We trained these algorithms on a dataset that we collected and curated, consisting of texts about and images of architecture. We describe our design process, present the final proposal, reflect on the usefulness of such tools for early-stage design, and discuss implications for future research and practice. By analysing the results from training the text-to-text generators we could establish a specific design brief that informed the final concept. The results from the image-to-image generator gave an overview of the shape grammars of previous submissions. All results were intriguing and can assist creativity and in this way, the tools were useful for gaining insight into historical architectural data, helped shape a specific design brief, and provoked new ideas. By reflecting on our design process, we argue that the use of language when employing such tools takes a new role and that three layers of language intertwined in our work: architectural discourse, programming languages, and annotations. We present a map that unfolds how these layers came together as a contribution to making machine learning more explainable for creatives.}
}
@article{GENNARI2023103006,
title = {Design for social digital well-being with young generations: Engage them and make them reflect},
journal = {International Journal of Human-Computer Studies},
volume = {173},
pages = {103006},
year = {2023},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2023.103006},
url = {https://www.sciencedirect.com/science/article/pii/S1071581923000125},
author = {Rosella Gennari and Maristella Matera and Diego Morra and Alessandra Melonio and Mehdi Rizvi},
keywords = {Digital well-being, Social digital well-being, Responsible design, Smart-thing design, Toolkit},
abstract = {Digital well-being traditionally means limiting the effects on individuals of technology abuses. However, in a broader perspective, it can be crucial to consider the pervasiveness of technology, and the effect it can have not only on individuals but also on their peers in the context of diverse everyday-life situations. Within this view, which emphasises the social side of digital well-being, the paper argues the need of educating young generations to participate in the making of technology for a social goal and have a reflective attitude towards technology and its impact on society. It, therefore, presents a design toolkit as a means to (i) engage young generations to become active in design for social digital well-being and, thanks to the exposure to how technology works, (ii) reflect deeply on the pros and cons of technology in use in their everyday life. By presenting the results of a study with 24 high-school pupils and their teachers, the paper discusses how a phygital toolkit, which structures the design process, engages them in the rapid prototyping of their own smart things, and how it acts as a proxy for soliciting their own reflections around technology and social digital well-being.}
}
@incollection{MILLER2017141,
title = {8 - Doctoral and professional programs},
editor = {Susan M. Miller and Walter H. Moos and Barbara H. Munk and Stephen A. Munk},
booktitle = {Managing the Drug Discovery Process},
publisher = {Woodhead Publishing},
address = {Boston},
pages = {141-169},
year = {2017},
isbn = {978-0-08-100625-2},
doi = {https://doi.org/10.1016/B978-0-08-100625-2.00008-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780081006252000088},
author = {Susan M. Miller and Walter H. Moos and Barbara H. Munk and Stephen A. Munk},
keywords = {Critical thinking, Basic/applied/clinical, Problem identification, Research design, Teams, PhD/PharmD, Postdoc/postdoctoral, Writing/publishing.},
abstract = {In this chapter on graduate and professional education, we explore Doctoral and professional programs, posing a number of key questions you should ask yourself. Where to apply to graduate school or a postdoc, and why? With whom should you work? A PhD or a PharmD, and to postdoc or not? What must you do to be successful? Moreover, we touch on traits important to becoming an independent researcher and ask whether success in graduate school or a postdoctoral fellowship requires different skills than undergraduate degrees. Critical thinking habits and skills underpin this discussion. We outline possible career choices, touching on the knowledge and expertise used by drug hunters, and also ask what might be of most value to potential employers. Each of us is different, and what's best for you is something you will have to decipher, but hopefully only after you consult with family, friends, and advisors or mentors. Regardless, “the big leap” is coming, so get ready.}
}
@article{CARPENTER1992457,
title = {Chapter 4 Cognitively guided instruction: Building on the knowledge of students and teachers},
journal = {International Journal of Educational Research},
volume = {17},
number = {5},
pages = {457-470},
year = {1992},
issn = {0883-0355},
doi = {https://doi.org/10.1016/S0883-0355(05)80005-9},
url = {https://www.sciencedirect.com/science/article/pii/S0883035505800059},
author = {Thomas P. Carpenter and Elizabeth Fennema},
abstract = {This chapter summarizes the results of a series of correlational, experimental, and case studies on Cognitively Guided Instruction (CGI), a program designed to help teachers understand children's thinking and use this knowledge to make instructional decisions. Results of the studies show that teachers' knowledge and beliefs about students' thinking are related to students' achievement. There were significant differences between CGI classes and control classes on the emphasis on problem solving and low level skills, the freedom given to students to construct their own strategies for solving problems, the teachers' knowledge of their students thinking, and the students' achievement in both problem solving and skills.}
}
@article{EVANS201123659,
title = {Advancing Science through Mining Libraries, Ontologies, and Communities*},
journal = {Journal of Biological Chemistry},
volume = {286},
number = {27},
pages = {23659-23666},
year = {2011},
issn = {0021-9258},
doi = {https://doi.org/10.1074/jbc.R110.176370},
url = {https://www.sciencedirect.com/science/article/pii/S0021925819487164},
author = {James A. Evans and Andrey Rzhetsky},
keywords = {Biophysics, Computation, Computer Modeling, Drug Design, Epigenetics, Computational Biology, Information Cascade, Sociology of Science, Text Mining},
abstract = {Life scientists today cannot hope to read everything relevant to their research. Emerging text-mining tools can help by identifying topics and distilling statements from books and articles with increased accuracy. Researchers often organize these statements into ontologies, consistent systems of reality claims. Like scientific thinking and interchange, however, text-mined information (even when accurately captured) is complex, redundant, sometimes incoherent, and often contradictory: it is rooted in a mixture of only partially consistent ontologies. We review work that models scientific reason and suggest how computational reasoning across ontologies and the broader distribution of textual statements can assess the certainty of statements and the process by which statements become certain. With the emergence of digitized data regarding networks of scientific authorship, institutions, and resources, we explore the possibility of accounting for social dependences and cultural biases in reasoning models. Computational reasoning is starting to fill out ontologies and flag internal inconsistencies in several areas of bioscience. In the not too distant future, scientists may be able to use statements and rich models of the processes that produced them to identify underexplored areas, resurrect forgotten findings and ideas, deconvolute the spaghetti of underlying ontologies, and synthesize novel knowledge and hypotheses.}
}
@article{KHAN2024e31470,
title = {Catch-up growth with alpha and beta decoupling and their relationships between CO2 emissions by GDP, population, energy production, and consumption},
journal = {Heliyon},
volume = {10},
number = {11},
pages = {e31470},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e31470},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024075017},
author = {Rabnawaz Khan},
keywords = {Economic growth, Alpha and beta decoupling, CO emissions, Energy production and consumption, Populace},
abstract = {This study explores the relationship between CO2 emissions by GDP, population, energy production, and consumption in the United States, China, Romania, and Thailand economies from 1990 to 2019. It evaluates the phenomenon of catch-up growth, which transpires when an lagging economy goes through an expansionary phase after a period of below-average performance. We used the stochastic model to illustrate in terms of alpha and beta decoupling techniques. The outcomes validated by positive and negative decoupling attitudes play a crucial role in predicting a rise in CO2 emissions owing to oil, gas, and coal use in comparison to Romania. Thailand and Romania have a more viable road to sustainability than the United States and China. The United States and China appear to have an antagonistic relationship, as suggested by decoupling attitudes. Thailand and Romania are considered to be highly environmentally sustainable countries on account of their minimal carbon emissions, efficient energy usage, and forward-thinking environmental policies. Accordingly, policy recommendations are offered based on CO2 emissions and effective mitigation policies, since this allows for determining which countries with high emissions need technological advances, best practices, and intersectoral policies.}
}
@incollection{SCHNEEGANS2008241,
title = {13 - Dynamic Field Theory as a Framework for Understanding Embodied Cognition},
editor = {Paco Calvo and Antoni Gomila},
booktitle = {Handbook of Cognitive Science},
publisher = {Elsevier},
address = {San Diego},
pages = {241-271},
year = {2008},
series = {Perspectives on Cognitive Science},
issn = {15564495},
doi = {https://doi.org/10.1016/B978-0-08-046616-3.00013-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008046616300013X},
author = {Sebastian Schneegans and Gregor Schöner},
abstract = {Publisher Summary
Embodied cognition is an approach to cognition that has roots in motor behavior. This approach emphasizes that cognition typically involves acting with a physical body on an environment in which that body is immersed. The approach of embodied cognition postulates that understanding cognitive processes entails understanding their close link to the motor surfaces that may generate action and to the sensory surfaces that provide sensory signals about the environment. To a certain extent, the embodiment stance implies a mistrust of the abstraction inherent in much information processing thinking, in which the interface between cognitive processes and their sensorimotor support is drawn at a level that is quite removed from both the sensory and the motor systems. New theoretical tools are needed to address cognition within the embodiment perspective. This chapter reviews one set of theoretical concepts which is believed to be particularly suited to address the constraints of embodiment and situatedness. It refers to this set of concepts as Dynamical Systems Thinking.}
}
@article{VANOPHEUSDEN2019127,
title = {Tasks for aligning human and machine planning},
journal = {Current Opinion in Behavioral Sciences},
volume = {29},
pages = {127-133},
year = {2019},
note = {Artificial Intelligence},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2019.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352154619300622},
author = {Bas {van Opheusden} and Wei Ji Ma},
abstract = {Research on artificial intelligence and research on human intelligence rely on similar conceptual foundations and have long inspired each other [1,2•]. However, achieving concrete synergy has been difficult, with one obstacle being a lack of alignment of the tasks used in both fields. Artificial intelligence research has traditionally focused on tasks that are challenging to solve, often using human performance as a benchmark to surpass [3, 4, 5, 6, 7]. By contrast, cognitive science and psychology have moved towards tasks that are simple enough to allow for detailed computational modeling of people’s choices. These divergent objectives have led to a divide in the complexity of tasks studied, both in perception and cognition. The purpose of this paper is to explore the middle ground: are there tasks that are reasonably attractive to both fields and could provide fertile ground for synergy?}
}
@article{CLANCY2008248,
title = {Applications of complex systems theory in nursing education, research, and practice},
journal = {Nursing Outlook},
volume = {56},
number = {5},
pages = {248-256.e3},
year = {2008},
note = {Special Issue Informatics: Science and Practice},
issn = {0029-6554},
doi = {https://doi.org/10.1016/j.outlook.2008.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S0029655408001619},
author = {Thomas R. Clancy and Judith A. Effken and Daniel Pesut},
abstract = {The clinical and administrative processes in today's healthcare environment are becoming increasingly complex. Multiple providers, new technology, competition, and the growing ubiquity of information all contribute to the notion of health care as a complex system. A complex system (CS) is characterized by a highly connected network of entities (e.g., physical objects, people or groups of people) from which higher order behavior emerges. Research in the transdisciplinary field of CS has focused on the use of computational modeling and simulation as a methodology for analyzing CS behavior. The creation of virtual worlds through computer simulation allows researchers to analyze multiple variables simultaneously and begin to understand behaviors that are common regardless of the discipline. The application of CS principles, mediated through computer simulation, informs nursing practice of the benefits and drawbacks of new procedures, protocols and practices before having to actually implement them. The inclusion of new computational tools and their applications in nursing education is also gaining attention. For example, education in CSs and applied computational applications has been endorsed by The Institute of Medicine, the American Organization of Nurse Executives and the American Association of Colleges of Nursing as essential training of nurse leaders. The purpose of this article is to review current research literature regarding CS science within the context of expert practice and implications for the education of nurse leadership roles. The article focuses on 3 broad areas: CS defined, literature review and exemplars from CS research and applications of CS theory in nursing leadership education. The article also highlights the key role nursing informaticists play in integrating emerging computational tools in the analysis of complex nursing systems.}
}
@incollection{AGARWAL2024625,
title = {4.10 - Digital twin},
editor = {Kenneth S. Ramos},
booktitle = {Comprehensive Precision Medicine (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {625-638},
year = {2024},
isbn = {978-0-12-824256-8},
doi = {https://doi.org/10.1016/B978-0-12-824010-6.00051-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128240106000514},
author = {Sarvesh Agarwal and Vijay Pratap Singh and Paulamy Ganguly and Pujita Munnangi and Claire Collins and Sadmaan Sarker and Jason Shenoi and Scott Heston and Shruti Pandita and Tej K. Pandita and Michael Moreno and Douglas A. Baxter and Roderick I. Pettigrew and Shameer Khader and Kamlesh K. Yadav},
keywords = {Artificial intelligence, Biomarker, Clinical trial, Digital twin, Healthcare, Industrial digital twin, Manufacturing, NASA, Oncology, Personalized medicine, Supply chain},
abstract = {Since the very beginning of space exploration, NASA has been building actual size replicas of spaceships and rovers to help them troubleshoot issues when the vehicles are out in outer space. Furthermore, real-time data captured and processed during spaceship launches has helped with timely maneuvering decisions. Similar concepts are currently utilized in various industrial processes such as manufacturing, oil and gas industry, and supply chain to name a few. A good example of the use of Digital twin technology is the GPS-based navigation system where maps are overlaid with location coordinates and real-time traffic data to make decisions on the best available routes. In this chapter we describe the use of digital twin technology in various industries such as manufacturing (including drug and vaccine development), pharmaceutical, healthcare and the practice of medicine. We further describe the rapid development of computation technologies and better structuring of electronic health records which in turn has ushered the emergence of digital health in medicine. Lastly, we provide examples of the use of digital twin technology in the areas of personalized medicine (immunology, dementia, and oncology), biomarker development (Multiple sclerosis, Chron's and Cardiovascular diseases), and clinical trials (Alzheimer's and breast cancer).}
}