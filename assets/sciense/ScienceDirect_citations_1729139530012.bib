@article{ZHUAN2012248,
title = {The Assimilation Rule on the Parameters of Feedback System},
journal = {AASRI Procedia},
volume = {1},
pages = {248-260},
year = {2012},
note = {AASRI Conference on Computational Intelligence and Bioinformatics},
issn = {2212-6716},
doi = {https://doi.org/10.1016/j.aasri.2012.06.039},
url = {https://www.sciencedirect.com/science/article/pii/S2212671612000406},
author = {Ping Zhuan and Su Yun Gan and Shi Ming Zhou},
keywords = {Feedback, Assimilation factor, Supporting structure, Assimilation of parameters, Evolutionary mechanism},
abstract = {In this paper, the reaction mechanism of system feedback on the changes of external environmental parameters has been discussed. And the conclusions have been attributed to the assimilation rule. According to the research results of the circuit system, assimilated factors should be defined at first--- the parts which have been isolated from the system equivalent parameters and the external parameters have been also contained. Afterwards, the analog inductive method has been adopted to conduct the overall feasibility study for the establishment of the rules. Then, several new ideas have been also provided in accordance with the applications of assimilation rules in the fields of biology, cognitive science and social organizations, etc. Finally, the block diagram has been provided so as to give a comprehensive overview for the thinking ideas of system.}
}
@article{YAN2024120835,
title = {CPS-3WS: A critical pattern supported three-way sampling method for classifying class-overlapped imbalanced data},
journal = {Information Sciences},
volume = {676},
pages = {120835},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.120835},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524007497},
author = {Yuanting Yan and Zhong Zheng and Yiwen Zhang and Yanping Zhang and Yiyu Yao},
keywords = {Three-way sampling, Class-imbalance problem, Critical pattern, Class overlap},
abstract = {Class-imbalance problem widely exists in real applications ranging from medial diagnosis to economic fraud detection, etc. As one of the mainstream techniques in dealing with imbalanced data, SMOTE (Synthetic Minority Over-sampling TEchnique) and its extensions mainly rebalance the datasets via generation of observations in specific regions with various adapted strategies. Many of them do not consider the cost of role assignment of samples, and the intractable data complexity (overlap, small disjuncts, etc.) poses additional challenges to them. This paper proposes a critical pattern supported three-way sampling method (CPS-3WS) for classifying class-overlapped imbalanced data, introducing the philosophy of thinking in threes to effective classification in imbalanced learning. Specifically, CPS-3WS uses a three-way sample partition strategy with the Bayes posterior probability by dividing majority and minority classes into three disjoint subsets: risky, critical and safe patterns. CPS-3WS conducts a three-way hybrid sampling through (i) evaluating the risky majority pattern to be eliminated and (ii) selecting critical minority pattern to synthesize new samples under local information constraint. Extensive experiments on 42 UCI benchmark datasets demonstrate the superiority of the proposed CPS-3WS compared with 11 data-level methods. The source code of CPS-3WS is available at https://github.com/ytyancp/CPS-3WS.}
}
@article{ZHOU20211491,
title = {Transcriptome based functional identification and application of regulator AbrB on alkaline protease synthesis in Bacillus licheniformis 2709},
journal = {International Journal of Biological Macromolecules},
volume = {166},
pages = {1491-1498},
year = {2021},
issn = {0141-8130},
doi = {https://doi.org/10.1016/j.ijbiomac.2020.11.028},
url = {https://www.sciencedirect.com/science/article/pii/S0141813020349539},
author = {Cuixia Zhou and Huitu Zhang and Honglei Fang and Yanqing Sun and Huiying Zhou and Guangcheng Yang and Fuping Lu},
keywords = {Transcriptome analysis, Gene regulation, AbrB, Alkaline protease, },
abstract = {Bacillus licheniformis 2709 is the major alkaline protease producer, which has great potential value of industrial application, but how the high-producer can be regulated rationally is still not completely understood. It's meaningful to understand the metabolic processes during alkaline protease production in industrial fermentation medium. Here, we collected the transcription database at various enzyme-producing stages (preliminary stage, stable phase and decline phase) to specifically research the synthesized and regulatory mechanism of alkaline protease in B. licheniformis. The RNA-sequencing analysis showed differential expression of numerous genes related to several processes, among which genes correlated with regulators were concerned, especially the major differential gene abrB on enzyme (AprE) synthesis was investigated. It was further verified that AbrB is a repressor of AprE by plasmid-mediated over-expression due to the severely descending enzyme activity (11,300 U/mL to 2695 U/mL), but interestingly it is indispensable for alkaline protease production because the enzyme activity of the null abrB mutant was just about 2279 U/mL. Thus, we investigated the aprE transcription by eliminating the theoretical binding site (TGGAA) of AbrB protein predicated by computational strategy, which significantly improved the enzyme activity by 1.21-fold and gene transcription level by 1.77-fold in the mid-log phase at a cultivation time of 18 h. Taken together, it is of great significance to improve the production strategy, control the metabolic process and oriented engineering by rational molecular modification of regulatory network based on the high throughput sequencing and computational prediction.}
}
@article{SERNAM2015647,
title = {Maturity model of transdisciplinary knowledge management},
journal = {International Journal of Information Management},
volume = {35},
number = {6},
pages = {647-654},
year = {2015},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2015.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S026840121500064X},
author = {Edgar {Serna M.}},
keywords = {Management, Complexity, Interdisciplinary, Multidisciplinary, Complex thinking},
abstract = {In this article a maturity model for the management of transdisciplinary knowledge is presented, although research nowadays is transdisciplinary the different maturity models proposed in the literature are oriented towards interdisciplinary knowledge management, and, at most, they are oriented toward multidisciplinary knowledge management. The objective is proposing an evolutionary model which accepts knowledge as intensely active and dynamic and evolving in maturity from the early stages of research. But this is possible only if the research team adopt a clear, clean and joint process of disciplinary integration and transdisciplinary integration of the produced and discovered knowledge. In this way, the results of research will have a greater influence on society and they also will be adopted by society.}
}
@article{CHEN2019398,
title = {Form-finding with robotics: Rapid and flexible fabrication of glass fiber reinforced concrete panels using thermoformed molds},
journal = {Journal of Computational Design and Engineering},
volume = {6},
number = {3},
pages = {398-403},
year = {2019},
issn = {2288-4300},
doi = {https://doi.org/10.1016/j.jcde.2018.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S2288430018300642},
author = {Yutong Chen and Jing Lin Koh and Xia Tian and Yi Qian Goh and Stylianos Dritsas},
keywords = {Form-finding, Digital fabrication, Parametric design},
abstract = {We present a process that revisits form-finding within digital media, namely parametric design and robotic fabrication. It is inspired by classical architectural and engineering experiments producing minimal surfaces and tensile structures by physical simulation of materials and natural forces. Fabrication is based on thermoforming, where thin sheets of amorphous PET are heat-treated and while in malleable state, where the material behaves like stretchable membrane, an industrial robot imprints a shape and sheets are rapidly cooled down assuming their final form. Key aspects of the approach include: (a) Speed: as each sheet is formed within seconds; (b) Flexibility, as a wide-range of shapes are produced without fabrication of unique dies; and (c) Resilience, as unlike traditional form-finding processes where the derived forms are ephemeral, the objects produced here are robust, they may be used directly or employed in subsequent fabrication processes. The produced sheets are used here as molds for glass-reinforced concrete casting offering excellent surface quality and the ability to create geometry unlike any conventional fabrication techniques. We present the design and development of the process and a proof-of-concept artwork produced.}
}
@article{PYKA2024107668,
title = {Unlocking the potential of higher-molecular-weight 5-HT7R ligands: Synthesis, affinity, and ADMET examination},
journal = {Bioorganic Chemistry},
volume = {151},
pages = {107668},
year = {2024},
issn = {0045-2068},
doi = {https://doi.org/10.1016/j.bioorg.2024.107668},
url = {https://www.sciencedirect.com/science/article/pii/S004520682400573X},
author = {Patryk Pyka and Sabrina Garbo and Aleksandra Murzyn and Grzegorz Satała and Artur Janusz and Michał Górka and Wojciech Pietruś and Filip Mituła and Delfina Popiel and Maciej Wieczorek and Biagio Palmisano and Alessia Raucci and Andrzej J. Bojarski and Clemens Zwergel and Ewa Szymańska and Katarzyna Kucwaj-Brysz and Cecilia Battistelli and Jadwiga Handzlik and Sabina Podlewska},
keywords = {Serotonin receptor 5-HT, G protein-coupled receptors, ADMET properties, Docking, Molecular modelling,  experiments, MTS assay, Gene expression assay},
abstract = {An increasing number of drugs introduced to the market and numerous repositories of compounds with confirmed activity have posed the need to revalidate the state-of-the-art rules that determine the ranges of properties the compounds should possess to become future drugs. In this study, we designed a series of two chemotypes of aryl-piperazine hydantoin ligands of 5-HT7R, an attractive target in search for innovative CNS drugs, with higher molecular weight (close to or over 500). Consequently, 14 new compounds were synthesised and screened for their receptor activity accompanied by extensive docking studies to evaluate the observed structure–activity/properties relationships. The ADMET characterisation in terms of the biological membrane permeability, metabolic stability, hepatotoxicity, cardiotoxicity, and protein plasma binding of the obtained compounds was carried out in vitro. The outcome of these studies constituted the basis for the comprehensive challenge of computational tools for ADMET properties prediction. All the compounds possessed high affinity to the 5-HT7R (Ki below 250 nM for all analysed structures) with good selectivity over 5-HT6R and varying affinity towards 5-HT2AR, 5-HT1AR and D2R. For the best compounds of this study, the expression profile of genes associated with neurodegeneration, anti-oxidant response and anti-inflammatory function was determined, and the survival of the cells (SH-SY5Y as an in vitro model of Alzheimer’s disease) was evaluated. One 5-HT7R agent (32) was characterised by a very promising ADMET profile, i.e. good membrane permeability, low hepatotoxicity and cardiotoxicity, and high metabolic stability with the simultaneous high rate of plasma protein binding and high selectivity over other GPCRs considered, together with satisfying gene expression profile modulations and neural cell survival. Such encouraging properties make it a good candidate for further testing and optimisation as a potential agent in the treatment of CNS-related disorders.}
}
@article{HEIRDSFIELD200257,
title = {Flexibility and inflexibility in accurate mental addition and subtraction: two case studies},
journal = {The Journal of Mathematical Behavior},
volume = {21},
number = {1},
pages = {57-74},
year = {2002},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(02)00103-7},
url = {https://www.sciencedirect.com/science/article/pii/S0732312302001037},
author = {Ann M Heirdsfield and Tom J Cooper},
keywords = {Mental computation, Addition, Subtraction, Accuracy},
abstract = {This paper reports on a study of two children’s mental computation in addition and subtraction, and compares their mental architecture. Both students were identified as being accurate, however, one student used a variety of mental strategies (was flexible) while the other student used only one strategy that reflected the written procedure for each of the addition and subtraction algorithms taught in the classroom. Interviews were used to identify both children’s knowledge and ability with respect to number sense (including numeration, number and operations, basic facts, estimation), metacognition and affects. Frameworks were developed to show how these factors interacted to explain the two types of accuracy in mental addition and subtraction. Flexible accuracy was related to the presence of strong number sense knowledge integrated with metacognitive strategies and beliefs and beliefs about self and teaching; while inflexible accuracy was a result of compensation of inadequate knowledge supported by beliefs about self and teaching.}
}
@article{BACK202423,
title = {Accelerated chemical science with AI},
journal = {Digital Discovery},
volume = {3},
number = {1},
pages = {23-33},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d3dd00213f},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X24000858},
author = {Seoin Back and Alán Aspuru-Guzik and Michele Ceriotti and Ganna Gryn'ova and Bartosz Grzybowski and Geun Ho Gu and Jason Hein and Kedar Hippalgaonkar and Rodrigo Hormázabal and Yousung Jung and Seonah Kim and Woo Youn Kim and Seyed Mohamad Moosavi and Juhwan Noh and Changyoung Park and Joshua Schrier and Philippe Schwaller and Koji Tsuda and Tejs Vegge and O. Anatole {von Lilienfeld} and Aron Walsh},
abstract = {In light of the pressing need for practical materials and molecular solutions to renewable energy and health problems, to name just two examples, one wonders how to accelerate research and development in the chemical sciences, so as to address the time it takes to bring materials from initial discovery to commercialization. Artificial intelligence (AI)-based techniques, in particular, are having a transformative and accelerating impact on many if not most, technological domains. To shed light on these questions, the authors and participants gathered in person for the ASLLA Symposium on the theme of ‘Accelerated Chemical Science with AI’ at Gangneung, Republic of Korea. We present the findings, ideas, comments, and often contentious opinions expressed during four panel discussions related to the respective general topics: ‘Data’, ‘New applications’, ‘Machine learning algorithms’, and ‘Education’. All discussions were recorded, transcribed into text using Open AI's Whisper, and summarized using LG AI Research's EXAONE LLM, followed by revision by all authors. For the broader benefit of current researchers, educators in higher education, and academic bodies such as associations, publishers, librarians, and companies, we provide chemistry-specific recommendations and summarize the resulting conclusions.}
}
@article{ABRAHAM200738,
title = {Creative cognition: The diverse operations and the prospect of applying a cognitive neuroscience perspective},
journal = {Methods},
volume = {42},
number = {1},
pages = {38-48},
year = {2007},
note = {Neurocognitive Mechanisms of Creativity: A Toolkit},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2006.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S1046202306002994},
author = {Anna Abraham and Sabine Windmann},
keywords = {Creative cognition, Conceptual expansion, Creative imagery, Constraints of examples, Insight, Alternate uses task, Cognitive neuroscience, Neuropsychology, Top-down and bottom-up processes},
abstract = {Creativity is defined quite simply as “the ability to create” in most lexicons, but, in reality, this is a complex and heterogeneous construct about which there is much to be discovered. The cognitive approach to investigating creativity recognizes and seeks to understand this complexity by investigating the component processes involved in creative thinking. The cognitive neuroscience approach, which has only limitedly been applied in the study of creativity, should ideally build on these ideas in uncovering the neural substrates of these processes. Following an introduction into the early experimental ideas and the cognitive approach to creativity, we discuss the theoretical background and behavioral methods for testing various processes of creative cognition, including conceptual expansion, the constraining influence of examples, creative imagery and insight. The complex relations between the underlying component processes of originality and relevance across these tasks are presented thereafter. We then outline how some of these conceptual distinctions can be evaluated by neuroscientific evidence and elaborate on the neuropsychological approach in the study of creativity. Given the current state of affairs, our recommendation is that despite methodological difficulties that are associated with investigating creativity, adopting the cognitive neuroscience perspective is a highly promising framework for validating and expanding on the critical issues that have been raised in this paper.}
}
@article{POOBALAN2025101667,
title = {A novel and secured email classification using deep neural network with bidirectional long short-term memory},
journal = {Computer Speech & Language},
volume = {89},
pages = {101667},
year = {2025},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2024.101667},
url = {https://www.sciencedirect.com/science/article/pii/S0885230824000500},
author = {A. Poobalan and K. Ganapriya and K. Kalaivani and K. Parthiban},
keywords = {Email classification, DNN-BiLSTM, AES algorithm, Rabit algorithm, Random forests (RF)},
abstract = {Email data has some characteristics that are different from other social media data, such as a large range of answers, formal language, notable length variations, high degrees of anomalies, and indirect relationships. The main goal in this research is to develop a robust and computationally efficient classifier that can distinguish between spam and regular email content. The benchmark Enron dataset, which is accessible to the public, was used for the tests. The six distinct Enron data sets we acquired were combined to generate the final seven Enron data sets. The dataset undergoes early preprocessing to remove superfluous sentences. The proposed model Bidirectional Long Short-Term Memory (BiLSTM) apply spam labels and to examine email documents for spam. On seven Enron datasets, DNN-BiLSTM performs better than other classifiers in the performance comparison in terms of accuracy. DNN-BiLSTM and convolutional neural networks demonstrated that they can classify spam with 96.39 % and 98.69 % accuracy, respectively, in comparison to other machine learning classifiers. The risks associated with cloud data management and potential security flaws are also covered in the paper. This research presents hybrid encryption as a means of protecting cloud data while preserving privacy by using the hybrid AES-Rabit encryption algorithm which is based on symmetric session key exchange.}
}
@article{MERIVAARA2021480,
title = {Preservation of biomaterials and cells by freeze-drying: Change of paradigm},
journal = {Journal of Controlled Release},
volume = {336},
pages = {480-498},
year = {2021},
issn = {0168-3659},
doi = {https://doi.org/10.1016/j.jconrel.2021.06.042},
url = {https://www.sciencedirect.com/science/article/pii/S0168365921003400},
author = {Arto Merivaara and Jacopo Zini and Elle Koivunotko and Sami Valkonen and Ossi Korhonen and Francisco M. Fernandes and Marjo Yliperttula},
keywords = {Freeze-drying, Cells, Extracellular vesicles, Process analytical technology, Quality-by-design, Biophotonics},
abstract = {Freeze-drying is the most widespread method to preserve protein drugs and vaccines in a dry form facilitating their storage and transportation without the laborious and expensive cold chain. Extending this method for the preservation of natural biomaterials and cells in a dry form would provide similar benefits, but most results in the domain are still below expectations. In this review, rather than consider freeze-drying as a traditional black box we “break it” through a detailed process thinking approach. We discuss freeze-drying from process thinking aspects, introduce the chemical, physical, and mechanical environments important in this process, and present advanced biophotonic process analytical technology. In the end, we review the state of the art in the freeze-drying of the biomaterials, extracellular vesicles, and cells. We suggest that the rational design of the experiment and implementation of advanced biophotonic tools are required to successfully preserve the natural biomaterials and cells by freeze-drying. We discuss this change of paradigm with existing literature and elaborate on our perspective based on our new unpublished results.}
}
@article{CRONIN2022100213,
title = {A review of in silico toxicology approaches to support the safety assessment of cosmetics-related materials},
journal = {Computational Toxicology},
volume = {21},
pages = {100213},
year = {2022},
issn = {2468-1113},
doi = {https://doi.org/10.1016/j.comtox.2022.100213},
url = {https://www.sciencedirect.com/science/article/pii/S2468111322000019},
author = {Mark T.D. Cronin and Steven J. Enoch and Judith C. Madden and James F. Rathman and Andrea-Nicole Richarz and Chihae Yang},
keywords = {Cosmetics, Risk assessment, , Computational, Read-across, Quantitative structure-activity relationship},
abstract = {In silico tools and resources are now used commonly in toxicology and to support the “Next Generation Risk Assessment” (NGRA) of cosmetics ingredients or materials. This review provides an overview of the approaches that are applied to assess the exposure and hazard of a cosmetic ingredient. For both hazard and exposure, databases of existing information are used routinely. In addition, for exposure, in silico approaches include the use of rules of thumb for systemic bioavailability as well as physiologically-based kinetics (PBK) and multi-scale models for estimating internal exposure at the organ or tissue level. (Internal) Thresholds of Toxicological Concern are applicable for the safety assessment of ingredients at low concentrations. The use of structural rules, (Quantitative) Structure-Activity Relationships ((Q)SARs) and read-across are the most typically applied modelling approaches to predict hazard. Data from exposure and hazard assessment are increasingly being brought together in NGRA to provide an overall assessment of the safety of a cosmetic ingredient. All in silico approaches are reviewed in terms of their maturity and robustness for use.}
}
@article{RUIZ201575,
title = {A transformational creativity tool to support chocolate designers},
journal = {Pattern Recognition Letters},
volume = {67},
pages = {75-80},
year = {2015},
note = {Cognitive Systems for Knowledge Discovery},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2015.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167865515001579},
author = {Francisco J. Ruiz and Cristóbal Raya and Albert Samà and Núria Agell},
keywords = {Cognigtive system, Creativity, Creativity support system},
abstract = {A new formulation of the central ideas of Boden's well-established theory on combinational, exploratory and transformational creativity is presented. This new formulation, based on the idea of conceptual space, redefines some terms and includes several types of concept properties (appropriateness and relevance), whose relationship facilitates the computational implementation of the transformational creativity mechanism. The presented formulation is applied to a real case of chocolate designing in which a novel and flavorful combination of chocolate and fruit is generated. The experimentation was conducted jointly with a Spanish chocolate chef. Experimental results prove the relationship between appropriateness and relevance in different frameworks and show that the formulation presented is not only useful for understanding how the creative mechanisms of design works but also facilitates its implementation in real cases to support creativity processes.}
}
@article{ZHENG2018266,
title = {When algorithms meet journalism: The user perception to automated news in a cross-cultural context},
journal = {Computers in Human Behavior},
volume = {86},
pages = {266-275},
year = {2018},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2018.04.046},
url = {https://www.sciencedirect.com/science/article/pii/S0747563218302103},
author = {Yue Zheng and Bu Zhong and Fan Yang},
keywords = {Automated news, Algorithm, Computational journalism, Robot journalism, News user, Cultural difference},
abstract = {Automated journalism – the use of algorithms in writing news reports – underscores the new direction of media transformation in the 21st century as it may reshape how the news is produced and consumed. Such writing algorithms have been increasingly adopted in U.S. and Chinese newsroom, but how well they are accepted by news users deserves more research. A comparative study was thus conducted to examine how U.S. and Chinese news users perceive the quality of algorithm-generated news reports, how much they like and trust such reports. Results show that U.S. and Chinese users demonstrated more shared, rather than different, perceptions to automated news. The users did not perceive automated content in a linear way, but viewed them by considering the interaction of the authors (i.e., journalists or algorithms), the media outlets (i.e., traditional or online media) and cultural background (i.e., U.S. or Chinese users).}
}
@article{DAVEY2012e139,
title = {Results from a study with Threshold Concepts in two chemical engineering undergraduate courses},
journal = {Education for Chemical Engineers},
volume = {7},
number = {3},
pages = {e139-e152},
year = {2012},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2012.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S1749772812000127},
author = {K.R. Davey},
keywords = {Threshold Concepts, Undergraduate peer presentations, Enhancement of teaching and learning in core chemical engineering, Learning and teaching in higher education},
abstract = {A new study in peer presentation of Threshold Concepts as the focus of learning in two core chemical engineering undergraduate courses has shown that students benefit from an explanatory and illustrative presentation they give to their class peers in place of the traditional lecturer. The methodology was that the lecturer identified a (progressively linked) inventory of Threshold Concepts and had students critically prepare and then explain these in brief (3–5min) presentation-and-question sessions to their cohort. The inventory was informed by Rowbottom's (2007) notion of looking for abilities for which a concept is necessary. The two courses were a level III core course on separations processing with 74 students and a level IV elective in specialist heat transfer with 15 students. Students welcomed and highly valued this type of learning with more than 90% agreeing that it improved understanding of the course material both because it revealed things better than their experiences in lectures and because it promoted a mental organisation of necessary course ideas. It is concluded that peer presentations of Threshold Concepts is a useful and economic instrument to overcoming traditional barriers to student learning. The findings could be readily applied to other courses in distinctive chemical engineering thinking and practise.}
}
@incollection{HOWARD2024,
title = {Types of AI},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95689-5.00274-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956895002741},
author = {Katherine Howard},
keywords = {AI, Artificial intelligence, Deep learning, Machine learning, Types of AI},
abstract = {The world of artificial intelligence (AI) is complex. As a relatively young discipline that is continually evolving, our understanding of how we categorize the various facets of AI also must evolve and adapt. AI falls into just two categories: AI based on capability, and AI based on functionality. AI based on capability has three sub-categories, two of which remain as purely theoretical concepts. AI that is based on functionality is distinguished by four sub-categories, with each one targeted at different problems, and solving them.}
}
@article{RAVI2023151,
title = {Evolving trends in student assessment in chemical engineering education},
journal = {Education for Chemical Engineers},
volume = {45},
pages = {151-160},
year = {2023},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2023.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S1749772823000453},
author = {Manoj Ravi},
keywords = {Assessment, Authentic assessment, Digitalisation},
abstract = {Alongside innovation in teaching practice, student assessment in chemical engineering has seen significant changes in the recent past. This article undertakes a systematic review of the recent advances that have been reported in assessment practice in chemical engineering education. The main trends that emerge are: a shift towards authentic assessment methods, an increase in emphasis on peer-assessment and other approaches for group-based assignments, and a greater use of digital tools for the delivery of authentic assessments and improvement of marking and feedback practice. The analysis also examines the diversity of assessment methods used across the different chemical engineering subjects and how these map against assessment frameworks reported in the wider pedagogical literature. The emerging strand of research on synoptic and interdisciplinary assessment is used to develop an assessment framework for producing chemical engineering graduates who are also socially responsible and competent global citizens.}
}
@article{LACHANCE2001503,
title = {Helping students build a path of understanding from ratio and proportion to decimal notation},
journal = {The Journal of Mathematical Behavior},
volume = {20},
number = {4},
pages = {503-526},
year = {2001},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(02)00087-1},
url = {https://www.sciencedirect.com/science/article/pii/S0732312302000871},
author = {Andrea Lachance and Jere Confrey},
keywords = {Decimal notation, Ratio and proportion, Multiplicative thinking},
abstract = {Various studies have shown that students of all levels struggle to understand decimal numbers. This paper discusses a novel approach to increasing students’ conceptual understanding of decimal numbers. Rather than approach decimal notation as a discrete and separate mathematical topic, this approach enables students to work with contextual problems to gain a solid understanding of ratio and proportion. Using their understanding of ratio and proportion as a foundation, students can then build connected and related understandings of fractions, decimals and percents. The study discussed in this paper illustrates that grounding decimal instruction in the broader context of ratio can help students gain deeper conceptual understandings of decimal notation as well as fractions and percents.}
}
@article{ALOIMONOS201542,
title = {The Cognitive Dialogue: A new model for vision implementing common sense reasoning},
journal = {Image and Vision Computing},
volume = {34},
pages = {42-44},
year = {2015},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2014.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S0262885614001656},
author = {Yiannis Aloimonos and Cornelia Fermüller},
keywords = {Vision and language, Integration of perception, action, and cognition, Cognition modulating image processing},
abstract = {We propose a new model for vision, where vision is part of an intelligent system that reasons. To achieve this we need to integrate perceptual processing with computational reasoning and linguistics. In this paper we present the basics of this formalism.}
}
@article{COHEN2013620,
title = {Autoantibody repertoires, natural biomarkers, and system controllers},
journal = {Trends in Immunology},
volume = {34},
number = {12},
pages = {620-625},
year = {2013},
issn = {1471-4906},
doi = {https://doi.org/10.1016/j.it.2013.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S147149061300080X},
author = {Irun R. Cohen},
abstract = {The immune system is composed of networks of interacting cells and molecules; therefore, to understand and control immune behavior we need to adopt the thinking and tools of systems immunology. This review describes the use of an antigen microarray device and informatics to profile the repertoires of autoantibodies in health and disease. Autoantibody profiling provides an insight into the biomarkers used by the immune system in its dialog with the body. Heat shock protein 60 (HSP60) and HSP70 are cited as examples of key hubs in physiological regulatory networks; HSP molecules and peptides can be viewed as natural regulators because the immune system itself deploys them to modulate inflammatory reactions. The discovery of such natural biomarkers paves the way towards natural control.}
}
@incollection{RAHI2025597,
title = {Chapter 56 - Crowdsourcing and artificial intelligence based modeling framework for effective Public Healthcare Informatics and Smart eHealth System},
editor = {M.A. Ansari and R.S. Anand and Pragati Tripathi and Rajat Mehrotra and Md Belal Bin Heyat},
booktitle = {Artificial Intelligence in Biomedical and Modern Healthcare Informatics},
publisher = {Academic Press},
pages = {597-608},
year = {2025},
isbn = {978-0-443-21870-5},
doi = {https://doi.org/10.1016/B978-0-443-21870-5.00056-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044321870500056X},
author = {Pankaj Rahi and Monika Dandotiya and Santi Basa and Souvik Sen and Mayur D. Jakhete and P. Vijayakumar},
keywords = {Artificial intelligence, Crowdsourcing, IoT, Machine learning, Public health informatics, Smart eHealth system},
abstract = {With the help of a multiagent interactive healthcare plan, healthy and independent aging is possible. Testing one's fitness and keeping tabs on one's health can both benefit from an awareness of one's regular routine. The Healthcare Strategies Partnership is introduced here. Geographic and economic factors, including the prevalence of infectious tropical diseases and an increasing number of chronic illnesses, have contributed to a shift in the region's medical requirements. This takes place in a world that is not only difficult but also intricate. This system employs a smartphone's sensor, a machine learning algorithm, multiple agents (including a doctor, fitness instructor, guardian, and intelligent ranker agent), and a smartphone itself to increase a user's sense of independence. A group of health professionals collaborate to assess the patient's day-to-day activities and offer suggestions for improvement. The algorithm figures out a typical day in the life of an adult. A smart autonomous agent using crowdsourced data recommends the best possible treatment plan. In contrast to crowdsourcing, which places value on the abilities of people to generate, aggregate, or filter original data, automatic tools make use of information retrieval techniques to analyze publicly available information. Crowdsourcing, which facilitates collaboration among numerous individuals, is increasingly being used in a variety of industries. Methodical crowdsourcing of useful data and human computation of interchangeable knowledge will aid future advancements in public health informatics. The disease burden on any country can and will be decreased by these efforts the share of healthcare costs borne by individuals and their families. The work also aids in achieving the most crucial objectives along the path to the Sustainable Development Goals. To improve public health surveillance for the prevention and control of communicable and noncommunicable diseases, this chapter presents the fundamental modeling for its design and development. So that a smart autonomous agent can recommend the best course of treatment, lowering the disease burden in any country. Crowdsourcing is defined, and how it can be used to better public health, in this chapter of a book. They are better able to achieve a healthy lifestyle exit, and the statistics and indicators that are tracked primarily under the Sustainable Development Goals Framework are improved as a result. The goal of this chapter is to provide a high-level overview of recent developments in applying artificial intelligence techniques to public health surveillance and response, which is the single most important step toward preventing the spread of disease and saving the lives of humans and other sentient beings.}
}
@article{DAVIES2023100692,
title = {Idea generation and knowledge creation through maker practices in an artifact-mediated collaborative invention project},
journal = {Learning, Culture and Social Interaction},
volume = {39},
pages = {100692},
year = {2023},
issn = {2210-6561},
doi = {https://doi.org/10.1016/j.lcsi.2023.100692},
url = {https://www.sciencedirect.com/science/article/pii/S2210656123000089},
author = {Sini Davies and Pirita Seitamaa-Hakkarainen and Kai Hakkarainen},
keywords = {Constructionism, Design practices, Engineering practices, Epistemic object, Epistemic practices, Knowledge creation, Learning by making, Material mediation},
abstract = {This investigation involved carrying out interventions that engaged teams of lower-secondary (13–14-year-old) Finnish students in using traditional and digital fabrication technologies to make materially embodied collaborative inventions. By relying on video data and ethnographic observations of the student teams' collaborative invention processes, the investigation focused on investigating 1) how the teams generated and developed their design ideas in their materially anchored making process and 2) what kinds of maker practices they relied on during open-ended invention projects. The study focused on a microanalytic study of three teams of students, and we utilized and developed visual data analysis methods. Our findings reveal the complex nature of the student teams' materially contextualized ideation and the knowledge creation activities that took place within their projects. The findings suggest that open-ended, materially mediated co-invention projects offer ample opportunities for creative cultural participation and practice-based knowledge creation in schools.}
}
@article{TYFLOPOULOS2019979,
title = {Messing with boundaries - quantifying the potential loss by pre-set parameters in topology optimization},
journal = {Procedia CIRP},
volume = {84},
pages = {979-985},
year = {2019},
note = {29th CIRP Design Conference 2019, 08-10 May 2019, Póvoa de Varzim, Portgal},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.04.307},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119309552},
author = {Evangelos Tyflopoulos and Martin Steinert},
keywords = {topology optimization, SIMP, additive manufacturing, product development, design, finite element analysis},
abstract = {Additive manufacturing can increase the flexibility in the design phase of product development and that, in its turn, has changed the designer’s way of thinking. The design problem has reformulated; from designs that were not possible to be constructed, due to lack of equipment and technology, to constructions that the designer could not think to design. Topology optimization and generative design are useful tools in the hands of designer that can help him/her in the pursuit of the global optimum of a construction and in the choice of an alternative design solution respectively. However, topology optimization results are always depended on the given boundary conditions and restrictions. In other words, the designer’s decisions can affect the results of topology optimization and can easily lead to a local and not a global solution. In this paper, an identification and categorization of the most important parameters, that can affect the topology optimization results, were conducted. The main focus of the implemented research was on the pre-processing of topology optimization and especially on the designer’s decisions. The applied topology optimization approach here was a simple compliance optimization based on the SIMP interpolation methodology (Solid Isotropic Material with Penalization) and it was executed with the use of the commercial software Tosca (Abaqus). Different alternative designs of a wall bracket were used as a case study to test the sensitivity of the optimization algorithm and quantify the potential loss.}
}
@article{MARIA2007695,
title = {Emotional agents: A modeling and an application},
journal = {Information and Software Technology},
volume = {49},
number = {7},
pages = {695-716},
year = {2007},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2006.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584906001030},
author = {Khulood Abu Maria and Raed Abu Zitar},
keywords = {Agent, Emotions, Behavior, Personality},
abstract = {This paper proposes modeling of artificial emotions through agents based on symbolic approach. The symbolic approach utilizes symbolic emotional rule-based systems (rule base that generated emotions) with continuous interactions with environment and an internal “thinking” machinery that comes as a result of series of inferences, evaluation, evolution processes, adaptation, learning, and emotions. We build two models for agent based systems; one is supported with artificial emotions and the other one without emotions. We use both in solving a bench mark problem; “The Orphanage Care Problem”. The two systems are simulated and results are compared. Our study shows that systems with proper model of emotions can perform in many cases better than systems without emotions. We try to shed the light here on how artificial emotions can be modeled in a simple rule-based agent systems and if emotions as they exist in “real intelligence” can be helpful for “artificial intelligence”. Agent architectures are presented as a generic blueprint on which the design of agents can be based. Our focus is on the functional design, including flow of information and control. With this information provided, the generic blueprints of architectures should not be difficult to implement agents, thus putting these theoretical models into practice. We build the agents using this architecture, and many experiments and analysis are shown.}
}
@article{GTOTH202138,
title = {Revascularization decisions in patients with chronic coronary syndromes: Results of the second International Survey on Interventional Strategy (ISIS-2)},
journal = {International Journal of Cardiology},
volume = {336},
pages = {38-44},
year = {2021},
issn = {0167-5273},
doi = {https://doi.org/10.1016/j.ijcard.2021.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167527321008172},
author = {Gabor {G. Toth} and Nils P. Johnson and William Wijns and Balint Toth and Alexandru Achim and Stephane Fournier and Emanuele Barbato},
keywords = {Chronic coronary syndrome, Coronary ischemia, Coronary revascularization},
abstract = {Background
In chronic coronary syndromes, guidelines mandate invasive functional guidance of revascularization whenever non-invasive proof of ischemia is missing. ISIS-2 survey aimed to evaluate how the adoption of guideline recommendation on ischemia-guided revascularization has evolved over the last 5–7 years.
Methods
In ISIS-2 participants assessed five complete angiograms, presenting only intermediate stenoses without information on non-invasive pre-testing. Fractional flow reserve was known for each stenosis, but remained undisclosed. Participants could determine stenosis significance either by angiography or by requesting an adjunctive invasive diagnostic method (intravascular imaging or functional tests). Primary endpoint was the rate of requesting adjunctive functional assessment. Secondary endpoints were the rate of concordance between angiography-based decisions and know functional severity. ISIS-2 utilized the same web-based platform as ISIS-1 in 2013. (NCT04001452).
Results
334 participants performed 2059 lesion evaluations: 1202 (59%) decisions were based solely on angiography without expressed need for further evaluation. These decisions were discordant with known functional significance in 39%, mainly with potential of overtreatment. Participants requested invasive functional assessment in 643 (31%) and intravascular imaging in 214 (10%) cases. Compared to ISIS-1 the rate of purely angiography-based decisions has decreased (59% vs 66%; p < 0.001), while invasive functional tests were more frequently requested (31% vs 25%; p < 0.001).
Conclusions
ISIS-2 suggests an evolving pattern in the intention to integrate invasive coronary physiology into the revascularization decisions. However, the disconnect between recommendations and current thinking is still dominant.}
}
@article{RANTANEN20153612,
title = {The Future of Pharmaceutical Manufacturing Sciences},
journal = {Journal of Pharmaceutical Sciences},
volume = {104},
number = {11},
pages = {3612-3638},
year = {2015},
issn = {0022-3549},
doi = {https://doi.org/10.1002/jps.24594},
url = {https://www.sciencedirect.com/science/article/pii/S0022354916301514},
author = {Jukka Rantanen and Johannes Khinast},
keywords = {quality by design (QBD), process analytical technology (PAT), mathematical model, materials science,  modeling},
abstract = {ABSTRACT
The entire pharmaceutical sector is in an urgent need of both innovative technological solutions and fundamental scientific work, enabling the production of highly engineered drug products. Commercial-scale manufacturing of complex drug delivery systems (DDSs) using the existing technologies is challenging. This review covers important elements of manufacturing sciences, beginning with risk management strategies and design of experiments (DoE) techniques. Experimental techniques should, where possible, be supported by computational approaches. With that regard, state-of-art mechanistic process modeling techniques are described in detail. Implementation of materials science tools paves the way to molecular-based processing of future DDSs. A snapshot of some of the existing tools is presented. Additionally, general engineering principles are discussed covering process measurement and process control solutions. Last part of the review addresses future manufacturing solutions, covering continuous processing and, specifically, hot-melt processing and printing-based technologies. Finally, challenges related to implementing these technologies as a part of future health care systems are discussed.}
}
@article{ASTLE2024105539,
title = {Understanding divergence: Placing developmental neuroscience in its dynamic context},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {157},
pages = {105539},
year = {2024},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2024.105539},
url = {https://www.sciencedirect.com/science/article/pii/S0149763424000071},
author = {Duncan E. Astle and Dani S. Bassett and Essi Viding},
keywords = {Development, Systems neuroscience, Neurodevelopmental condition, Mental health, Computational neuroscience},
abstract = {Neurodevelopment is not merely a process of brain maturation, but an adaptation to constraints unique to each individual and to the environments we co-create. However, our theoretical and methodological toolkits often ignore this reality. There is growing awareness that a shift is needed that allows us to study divergence of brain and behaviour across conventional categorical boundaries. However, we argue that in future our study of divergence must also incorporate the developmental dynamics that capture the emergence of those neurodevelopmental differences. This crucial step will require adjustments in study design and methodology. If our ultimate aim is to incorporate the developmental dynamics that capture how, and ultimately when, divergence takes place then we will need an analytic toolkit equal to these ambitions. We argue that the over reliance on group averages has been a conceptual dead-end with regard to the neurodevelopmental differences. This is in part because any individual differences and developmental dynamics are inevitably lost within the group average. Instead, analytic approaches which are themselves new, or simply newly applied within this context, may allow us to shift our theoretical and methodological frameworks from groups to individuals. Likewise, methods capable of modelling complex dynamic systems may allow us to understand the emergent dynamics only possible at the level of an interacting neural system.}
}
@article{SURYADI2023730,
title = {”Read on”: comprehending challenging texts at university through gamification App},
journal = {Procedia Computer Science},
volume = {216},
pages = {730-738},
year = {2023},
note = {7th International Conference on Computer Science and Computational Intelligence 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.190},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922022682},
author = {Phillip Suryadi and Irfan Rifai and Hady Pranoto},
keywords = {gamification, reading, application, students, texts},
abstract = {Despite the common misperception of playing games as wasteful activity, studies found that some of its components may contribute to users’ knowledge generation, soft skill improvement, and foreign language learning. This article reports the development of an application for reading and the initial impacts of the gamification-based application on students’ reading comprehension in English. The application was aimed to support generation Z's university students who are well exposed to gadgets with the ability in comprehending challenging texts. In addition to the sociocultural theory of learning and second language acquisition theories, we considered factors like university students as users, texts’ complexity offered at the university level, and gamification features in designing the application. The study resulted in a prototype of a gaming activity called” ReadOn”. Surveys, interviews, and experiments were carried out on a small group of participants during, and after designing processes. The Survey data was used as a foundation to design the app while the interview and the experiments provided data to explore the usability of the newly built prototype. The data of students’ experience in using the prototype was used as feedback for future development of the platform.}
}
@article{HUDLICKA201498,
title = {Affective BICA: Challenges and open questions},
journal = {Biologically Inspired Cognitive Architectures},
volume = {7},
pages = {98-125},
year = {2014},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2013.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X13000947},
author = {Eva Hudlicka},
keywords = {Emotion modeling, Emotion theories, Cognitive–affective architectures},
abstract = {In spite of the progress in emotion research over the past 20years, emotions remain an elusive phenomenon. While some underlying circuitry has been identified for some aspects of affective processing (e.g., amygdala-mediated processing of threatening stimuli, the role of orbitofrontal cortex in emotion regulation), much remains unknown about the mechanisms of emotions. Computational models of cognitive and affective processes provide a unique and powerful means of refining psychological theories, and can help elucidate the mechanisms that mediate affective phenomena. This paper outlines a number of open questions and challenges associated with developing computational models of emotion, and with their integration within biologically-inspired cognitive architectures. These include the following: the extent to which mechanisms in biological affective agents should be simulated or emulated in affective BICAs; importance of more precise, design-based terminology; identification of fundamental affective processes, and the computational tasks necessary for their implementation; improved understanding of affective dynamics and development of more accurate models of these phenomena; and understanding the alternative means of integrating emotions within agent architectures. The challenges associated with data availability and model validation are also discussed.}
}
@article{MENEGHETTI2021101614,
title = {Learning from navigation, and tasks assessing its accuracy: The role of visuospatial abilities and wayfinding inclinations},
journal = {Journal of Environmental Psychology},
volume = {75},
pages = {101614},
year = {2021},
issn = {0272-4944},
doi = {https://doi.org/10.1016/j.jenvp.2021.101614},
url = {https://www.sciencedirect.com/science/article/pii/S0272494421000670},
author = {Chiara Meneghetti and Laura Miola and Enrico Toffalini and Massimiliano Pastore and Francesca Pazzaglia},
keywords = {Navigation, Route retracing, Shortcut, Landmark locating, Visuospatial abilities, Wayfinding inclinations, Informed priors},
abstract = {How individual differences in visuospatial thinking relate to environment learning from navigation is of growing interest and needs to be approached systematically. Here, a sample of 292 undergraduates learnt a virtual path (desktop-based), and their learning accuracy was assessed with recall tasks, i.e. route retracing, shortcut finding and landmark locating tasks. Several individual visuospatial measures, tasks and questionnaires, were administered. Relations between individual measures and recall tasks were estimated with regression models taking quantitative evidence available in the literature into account, and treated as Bayesian informed priors established by a meta-analysis. The results provide robust evidence of visuospatial abilities and wayfinding inclinations (composing two distinct factors) both affecting recall task performance, particularly the former. A different contribution of individual measures as a function of recall task is envisaged. This study offers new insight on the role of individual visuospatial measures in environment learning (navigation-like) and how they are related.}
}
@article{ZHU2024100138,
title = {Exploring the impact of ChatGPT on art creation and collaboration: Benefits, challenges and ethical implications},
journal = {Telematics and Informatics Reports},
volume = {14},
pages = {100138},
year = {2024},
issn = {2772-5030},
doi = {https://doi.org/10.1016/j.teler.2024.100138},
url = {https://www.sciencedirect.com/science/article/pii/S2772503024000240},
author = {Sijin Zhu and Zheng Wang and Yuan Zhuang and Yuyang Jiang and Mengyao Guo and Xiaolin Zhang and Ze Gao},
keywords = {Creative AI, HumanAI collaboration, Language models, Interactive AI literacy},
abstract = {This paper examines the chaos caused by introducing advanced language models, specifically ChatGPT, to art. Our focus is on the potential impact of ChatGPT on art creation and collaboration. We explore how it has been utilized to generate art and assist in creative writing and how it facilitates collaboration between artists. This exploration includes an investigation into the use of AI in creating art, music, and literature, emphasizing ChatGPT’s role in generating poetry and prose and its ability to provide valuable suggestions for sentence structure and word choice in creative writing. We conduct case studies and interviews with diverse artists and AI experts to understand the benefits and challenges of using ChatGPT in the creative process. Our findings reveal that artists find ChatGPT helpful in generating new ideas, overcoming creative blocks, and improving the quality of their work. It enables remote collaboration between artists by providing a real-time communication and idea-sharing platform. However, ethical concerns relating to authorship ownership and authenticity have emerged. Artists fear using ChatGPT may lead to losing their artistic identity and ownership of their work. While our data suggests that ChatGPT holds the potential to transform the art world, careful consideration must be given to the ethical implications of AI in art. We recommend future research to focus on developing guidelines for the responsible use of AI in art, safeguarding artists’ rights, and preserving artistic authenticity.}
}
@article{CHEN2020103670,
title = {A visual learning analytics (VLA) approach to video-based teacher professional development: Impact on teachers’ beliefs, self-efficacy, and classroom talk practice},
journal = {Computers & Education},
volume = {144},
pages = {103670},
year = {2020},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2019.103670},
url = {https://www.sciencedirect.com/science/article/pii/S0360131519302234},
author = {Gaowei Chen},
keywords = {Teacher professional development, Data science applications in education, Improving classroom teaching, Pedagogical issues},
abstract = {To address the challenge of overwhelming data inherent in classroom lesson videos, this study proposed a visual learning analytics (VLA) approach to video-based teacher professional development (TPD). Using a two-year experimental design, 46 secondary mathematics teachers were divided randomly into a treatment group (N = 24) and a control group (N = 22) to learn about and integrate academically productive talk into their teaching. The treatment teachers participated in a VLA-supported TPD program, while the control teachers participated in conventional knowledge-based workshops. Results show that teachers in the treatment group had more positive beliefs and higher self-efficacy in the post-test and delayed-post-test, while the control group improved, but not significantly, in their beliefs about the usefulness of classroom talk. In addition, although the control group made a significant improvement in their self-efficacy in guiding classroom talk in the post-test, this improvement was not sustained to the delayed post-test. Moreover, the coding of classroom teaching behaviour revealed that teachers in the treatment group relative to the control group significantly increased their use of academically productive talk in the post-test lessons to encourage the students' elaboration, reasoning, and thinking with others in the classroom. The results suggest that, while attending knowledge-based workshops had, to some degree, positive effects on the control teachers' beliefs and self-efficacy, these effects were not sustainable over time. In contrast, the use of visual learning analytics to support the treatment group's reflection on the classroom data not only had significant and sustained effects on the teachers' beliefs and self-efficacy but also significantly influenced their actual classroom teaching behaviour. Implications for designing VLA to support teacher learning and professional development are discussed.}
}
@incollection{RAPAPORT1994225,
title = {CHAPTER 10 - Syntactic Semantics: Foundations of Computational Natural-Language Understanding},
editor = {Eric Dietrich},
booktitle = {Thinking Computers and Virtual Persons},
publisher = {Academic Press},
pages = {225-273},
year = {1994},
isbn = {978-0-12-215495-9},
doi = {https://doi.org/10.1016/B978-0-12-215495-9.50015-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780122154959500156},
author = {William J. Rapaport},
abstract = {Publisher Summary
This chapter discusses the way by which it is possible to understand natural language and whether a computer could do so. It presents the argument that although a certain kind of semantic interpretation is needed for understanding natural language, it is a kind that only involves syntactic symbol manipulation of precisely the sort of which computers are capable, so that it is possible, in principle, for computers to understand natural language. The chapter highlights the recent arguments by John R. Searle and by Fred Dretske to the effect that computers cannot understand natural language. A program is like the script of a play. The computer is like the actors, sets, and a process is like an actual production of the play—the play in the process of being performed. The computer must be able to do things like: to convince someone, to imitate a human, that is, it must not merely be a cognitive agent, but also an acting one. In particular, to imitate a human, the computer needs to be able to reason about what another cognitive agent, such as a human, believes.}
}
@article{VANBAVEL2015167,
title = {The neuroscience of moral cognition: from dual processes to dynamic systems},
journal = {Current Opinion in Psychology},
volume = {6},
pages = {167-172},
year = {2015},
note = {Morality and ethics},
issn = {2352-250X},
doi = {https://doi.org/10.1016/j.copsyc.2015.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S2352250X15002031},
author = {Jay J {Van Bavel} and Oriel FeldmanHall and Peter Mende-Siedlecki},
abstract = {Prominent theories of morality have integrated philosophy with psychology and biology. Although this approach has been highly generative, we argue that it does not fully capture the rich and dynamic nature of moral cognition. We review research from the dual-process tradition, in which moral intuitions are automatically elicited and reasoning is subsequently deployed to correct these initial intuitions. We then describe how the computations underlying moral cognition are diverse and widely distributed throughout the brain. Finally, we illustrate how social context modulates these computations, recruiting different systems for real (vs. hypothetical) moral judgments, examining the dynamic process by which moral judgments are updated. In sum, we advocate for a shift from dual-process to dynamic system models of moral cognition.}
}
@article{LI2000233,
title = {Can younger students succeed where older students fail? An examination of third graders' solutions of a division-with-remainder (DWR) problem},
journal = {The Journal of Mathematical Behavior},
volume = {19},
number = {2},
pages = {233-246},
year = {2000},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(00)00046-8},
url = {https://www.sciencedirect.com/science/article/pii/S0732312300000468},
author = {Yeping Li and Edward A. Silver},
keywords = {Problem solving, Mathematics education, Sense making, Division, Elementary school students, Mathematics cognition},
abstract = {In this study, 14 third graders, who had not yet been taught the division algorithm (DA), solved a division-with-remainder (DWR) problem, and their solution processes were examined. Students also solved a set of eight numerical computation tasks involving addition, subtraction, multiplication, and division. In contrast to their poor performance on the division computation tasks and in contrast to the findings in previous studies of DWR problem solving by middle-school students and to the third graders in this study were quite successful in solving the DWR problem. Although the students lacked knowledge of formal procedures for division computation, they successfully used non-division solution strategies that were closely tied to problem context in order to solve the problem. In general, the results indicate that student performance in solving this complex problem was enhanced by their engagement in sense making as they solved the DWR problem. The highly context-embedded approaches used by these students also allowed them to avoid the difficulties in treating the “remainder,” as have been reported in research involving older students.}
}
@article{VAZIRIZADE2017230,
title = {Seismic reliability assessment of structures using artificial neural network},
journal = {Journal of Building Engineering},
volume = {11},
pages = {230-235},
year = {2017},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2017.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352710216303163},
author = {Sayyed Mohsen Vazirizade and Saeed Nozhati and Mostafa Allameh Zadeh},
keywords = {Seismic reliability, Artificial neural network, Monte Carlo Simulation, Failure probability},
abstract = {Localization and quantification of structural damage and estimating the failure probability are key outputs in the reliability assessment of structures. In this study, an Artificial Neural Network (ANN) is used to reduce the computational effort required for reliability analysis and damage detection. Toward this end, one demonstrative structure is modeled and then several damage scenarios are defined. These scenarios are considered as training data sets for establishing an ANN model. In this regard, the relationship between structural response (input) and structural stiffness (output) is established using ANN models. The established ANN is more economical and achieves reasonable accuracy in detection of structural damage under a set of ground motions. Furthermore, in order to assess the reliability of a structure, five random variables are considered. These are columns’ area of the first, second, and third floor, elasticity modulus, and gravity loads. The ANN is trained by suing the Monte Carlo Simulation (MCS) technique. Finally, the trained neural network specifies the failure probability of the proposed structure. Although MCS can predict the failure probability for a given structure, the ANN model helps simulation techniques to receive an acceptable accuracy and reduce computational effort.}
}
@article{KAR2023102661,
title = {Guest Editorial: Big data-driven theory building: Philosophies, guiding principles, and common traps},
journal = {International Journal of Information Management},
volume = {71},
pages = {102661},
year = {2023},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2023.102661},
url = {https://www.sciencedirect.com/science/article/pii/S0268401223000427},
author = {Arpan Kumar Kar and Spyros Angelopoulos and H. Raghav Rao},
keywords = {Big data, Information systems, Artificial intelligence, Machine learning, Theory building, Computational social science},
abstract = {While data availability and access used to be a major challenge for information systems research, the growth and ease of access to large datasets and data analysis tools has increased interest to use such resources for publishing. Such publications, however, seem to offer weak theoretical contributions. While big data-driven studies increasingly gain popularity, they rarely introspect why a phenomenon is better explained by a theory and limit the analysis to data descriptive by mining and visualizing large volumes of big data. We address this pressing need and provide directions to move towards theory building with Big Data. We differentiate based on inductive and deductive approaches and provide guidelines how may undertake steps for theory building. In doing so, we further provide directions surrounding common pitfalls that should be avoided in this journey of Big-Data driven theory building.}
}
@incollection{ZANNI202057,
title = {Chapter 4 - Life cycle sustainability assessment: An ongoing journey},
editor = {Jingzheng Ren and Sara Toniolo},
booktitle = {Life Cycle Sustainability Assessment for Decision-Making},
publisher = {Elsevier},
pages = {57-93},
year = {2020},
isbn = {978-0-12-818355-7},
doi = {https://doi.org/10.1016/B978-0-12-818355-7.00004-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012818355700004X},
author = {Sara Zanni and Eric Awere and Alessandra Bonoli},
keywords = {Life cycle sustainability assessment, Integrated sustainability assessment, Sustainable development, Review, Standardization, Case studies},
abstract = {As the call for sustainable solutions at operational, industrial, and policy level increases, the need for a comprehensive assessment tool has been addressed by literature and practitioners. In particular, for the definition of a complete framework, the application of a life cycle thinking lens is required to explore the longitudinal dimension of the impacts and possible indirect effects triggered on environmental, social, and economic levels. The definition of an integrated life cycle sustainability assessment framework is currently an ongoing journey, which is summarized in the present chapters. The narrative follows a set of milestones, namely the definition of the concept and the preliminary scheme in the early years, the pathway towards the implementation of a standardized set of tools, an anthology of significant case studies in different sectors, and an overview of the challenges identified by literature and yet remaining open for future researches.}
}
@incollection{CAMERER2014479,
title = {Chapter 25 - The Neural Basis of Strategic Choice},
editor = {Paul W. Glimcher and Ernst Fehr},
booktitle = {Neuroeconomics (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {479-492},
year = {2014},
isbn = {978-0-12-416008-8},
doi = {https://doi.org/10.1016/B978-0-12-416008-8.00025-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780124160088000255},
author = {Colin F. Camerer and Todd A. Hare},
keywords = {Game theory, Learning, Strategic choice},
abstract = {In this chapter, we present a set of concepts and tools for defining and examining strategic choice that are drawn from behavioral economics and discuss how they can be applied to and tested with neuroscience techniques. The standard language for studying strategic choice in economics is game theory. Game theory provides concrete mathematical formulas for linking strategic actions to rewarding payoffs. After outlining the four components necessary to make predictions about strategic social behavior, we present recent evidence that the computations predicted by game theory in specific strategic choice contexts are reflected in the brain. In addition, we discuss links between strategic decision making and the psychological concept of theory of mind. We conclude by suggesting that developing mathematical models of social and strategic actions may aid in the understanding of how the brain implements typical choice behavior as well as categorizing dysfunctions that lead to aberrant behavior in psychiatric disorders.}
}
@incollection{SANCHEZ1995865,
title = {Interfaces for learning},
editor = {Yuichiro Anzai and Katsuhiko Ogawa and Hirohiko Mori},
series = {Advances in Human Factors/Ergonomics},
publisher = {Elsevier},
volume = {20},
pages = {865-870},
year = {1995},
booktitle = {Symbiosis of Human and Artifact},
issn = {0921-2647},
doi = {https://doi.org/10.1016/S0921-2647(06)80136-X},
url = {https://www.sciencedirect.com/science/article/pii/S092126470680136X},
author = {J. Sanchez and M. Lumbreras},
abstract = {The current literature emphasizes critical aspects of learning and cognition involved in human-computer interaction. We present a conceptualization for designing interfaces for learning and thinking through the use of modern ideas for building educational software. We address the construction of Hyperstories as a metaphor for enhancing thought and reasoning skills. The advantages of using multimedia for building this type of software, as well as the complexities involved are analyzed and discussed.}
}
@article{SANZ2021103070,
title = {The entropic tongue: Disorganization of natural language under LSD},
journal = {Consciousness and Cognition},
volume = {87},
pages = {103070},
year = {2021},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2020.103070},
url = {https://www.sciencedirect.com/science/article/pii/S1053810020305377},
author = {Camila Sanz and Carla Pallavicini and Facundo Carrillo and Federico Zamberlan and Mariano Sigman and Natalia Mota and Mauro Copelli and Sidarta Ribeiro and David Nutt and Robin Carhart-Harris and Enzo Tagliazucchi},
keywords = {LSD, Psychedelics, Natural language, Entropy, Psychosis},
abstract = {Serotonergic psychedelics have been suggested to mirror certain aspects of psychosis, and, more generally, elicit a state of consciousness underpinned by increased entropy of on-going neural activity. We investigated the hypothesis that language produced under the effects of lysergic acid diethylamide (LSD) should exhibit increased entropy and reduced semantic coherence. Computational analysis of interviews conducted at two different time points after 75 μg of intravenous LSD verified this prediction. Non-semantic analysis of speech organization revealed increased verbosity and a reduced lexicon, changes that are more similar to those observed during manic psychoses than in schizophrenia, which was confirmed by direct comparison with reference samples. Importantly, features related to language organization allowed machine learning classifiers to identify speech under LSD with accuracy comparable to that obtained by examining semantic content. These results constitute a quantitative and objective characterization of disorganized natural speech as a landmark feature of the psychedelic state.}
}
@article{DIX201013,
title = {Human–computer interaction: A stable discipline, a nascent science, and the growth of the long tail},
journal = {Interacting with Computers},
volume = {22},
number = {1},
pages = {13-27},
year = {2010},
note = {Special Issue: Festschrift for John Long},
issn = {0953-5438},
doi = {https://doi.org/10.1016/j.intcom.2009.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0953543809000952},
author = {Alan Dix},
keywords = {HCI discipline, Methodology, Theory, Peak experience, Single person study},
abstract = {This paper represents a personal view of the state of HCI as a design discipline and as a scientific discipline, and how this is changing in the face of new technological and social situations. Going back 20years a frequent topic of discussion was whether HCI was a ‘discipline’. It is unclear whether this was ever a fruitful topic, but academic disciplines are effectively about academic communities and there is ample evidence of the long-term stability of the international HCI/CHI community. However, as in computer ‘science’, the central scientific core of HCI is perhaps still unclear; for example, a strength of HCI is the closeness between theory and practice, but the corresponding danger is that the two are often confused. The paper focuses particularly on the challenge of methodological thinking in HCI, especially as the technological and social context of HCI rapidly changes. This is set alongside two other challenges: the development of reliable knowledge in HCI and the clear understanding of interlinked human roles within the discipline. As a case study of the need for methodological thinking, the paper considers the use of single person studies in research and design. These are likely to be particularly valuable as we move from a small number of applications used by many people to a ‘long tail’ where large numbers of applications are used by small numbers of people. This change calls for different practical design strategies; focusing on the peak experience of a few rather than acceptable performance for many. Moving back to the broader picture, as we see more diversity both in terms of types of systems and kinds of concerns, this may also be an opportunity to reflect on what is core across these; potential fragmentation becoming a locus to understand more clearly what defines HCI, not just for the things we see now, but for the future that we cannot see.}
}
@article{RIDDERINKHOF20143,
title = {Neurocognitive mechanisms of perception–action coordination: A review and theoretical integration},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {46},
pages = {3-29},
year = {2014},
note = {Micro- and Macro-Perspectives on Cognitive Conflict Control},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2014.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0149763414001250},
author = {K. Richard Ridderinkhof},
keywords = {Perception–action coordination, Impetus, Motivation, Prediction, Appraisal, Valuation, Impulsive action, Intuitive action, Dual systems, Conation},
abstract = {The present analysis aims at a theoretical integration of, and a systems-neuroscience perspective on, a variety of historical and contemporary views on perception–action coordination (PAC). We set out to determine the common principles or lawful linkages between sensory and motor systems that explain how perception is action-oriented and how action is perceptually guided. To this end, we analyze the key ingredients to such an integrated framework, examine the architecture of dual-system conjectures of PAC, and endeavor in an historical analysis of the key characteristics, mechanisms, and phenomena of PACs. This analysis will reveal that dual-systems views are in need of fundamental re-thinking, and its elements will be amalgamated with current views on action-oriented predictive processing into a novel integrative theoretical framework (IMPPACT: Impetus, Motivation, and Prediction in Perception–Action Coordination theory). From this framework and its neurocognitive architecture we derive a number of non-trivial predictions regarding conative, motive-driven PAC. We end by presenting a brief outlook on how IMPPACT might present novel insights into certain pathologies and into action expertise.}
}
@article{FRERICHS2018135,
title = {Mind maps and network analysis to evaluate conceptualization of complex issues: A case example evaluating systems science workshops for childhood obesity prevention},
journal = {Evaluation and Program Planning},
volume = {68},
pages = {135-147},
year = {2018},
issn = {0149-7189},
doi = {https://doi.org/10.1016/j.evalprogplan.2018.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0149718917300113},
author = {Leah Frerichs and Tiffany L. Young and Gaurav Dave and Doris Stith and Giselle Corbie-Smith and Kristen {Hassmiller Lich}},
keywords = {Concept mapping, Mental models, Network analysis, Systems science},
abstract = {Across disciplines, it is common practice to bring together groups to solve complex problems. Facilitators are often asked to help groups organize information about and better understand the problem in order to develop and prioritize solutions. However, despite existence of several methods to elicit and characterize how individuals and groups think about and conceptualize an issue, many are difficult to implement in practice-based settings where resources such as technology and participant time are limited and research questions shift over time. This paper describes an easy-to-implement diagramming technique for eliciting conceptualization and a flexible network analysis method for characterizing changes in both individual and group conceptualization. We use a case example to illustrate how we used the methods to evaluate African American adolescent’s conceptual understanding of obesity before and after participating in a series of four systems thinking workshops. The methods produced results that were sensitive to changes in conceptualization that were likely driven by the specific activities employed during the workshop sessions. The methods appear strong for capturing salient levels of conceptualization at both individual and collective levels. The paper concludes with a critical examination of strengths and weaknesses of the methods and implications for future practice and research.}
}
@article{GUPTA2022103,
title = {The interaction between technology, business environment, society, and regulation in ICT industries},
journal = {IIMB Management Review},
volume = {34},
number = {2},
pages = {103-115},
year = {2022},
issn = {0970-3896},
doi = {https://doi.org/10.1016/j.iimb.2022.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0970389622000465},
author = {Subhashish Gupta},
keywords = {ICT, Technology, Disruption, Convergence, Interaction, Big data, Network effects, Innovation, Competition law, Strategy},
abstract = {This paper focusses on the interaction between technology, the business environment, regulation, and society in ICT industries. The role of technological advances in communication (e.g., cellular mobile, 5 G, spectrum allocation) and in computational advances (e.g., cloud, Internet of Things, artificial intelligence) along with developments in the business environment (e.g., disruption, convergence, Industry 4.0) and the regulatory environment (e.g., competition law and market regulation) in the model is explained. The economics of network industries and competition law and strategies such as vertical integration, bundling, and tying are described. The role of regulation and innovation is discussed along with some cases.}
}
@article{BAYER202380,
title = {The SPEAK study rationale and design: A linguistic corpus-based approach to understanding thought disorder},
journal = {Schizophrenia Research},
volume = {259},
pages = {80-87},
year = {2023},
note = {Language and Speech Analysis in Schizophrenia and Related Psychoses},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2022.12.048},
url = {https://www.sciencedirect.com/science/article/pii/S0920996422004959},
author = {J.M.M. Bayer and J. Spark and M. Krcmar and M. Formica and K. Gwyther and A. Srivastava and A. Selloni and M. Cotter and J. Hartmann and A. Polari and Z.R. Bilgrami and C. Sarac and A. Lu and Alison R. Yung and A. McGowan and P. McGorry and J.L. Shah and G.A. Cecchi and R. Mizrahi and B. Nelson and C.M. Corcoran},
keywords = {Thought disorder, Ultra/clinical high risk, Natural language processing, Psychosis, Latent semantic analysis, Part-of-speech-tagging},
abstract = {Aim
Psychotic symptoms are typically measured using clinical ratings, but more objective and sensitive metrics are needed. Hence, we will assess thought disorder using the Research Domain Criteria (RDoC) heuristic for language production, and its recommended paradigm of “linguistic corpus-based analyses of language output”. Positive thought disorder (e.g., tangentiality and derailment) can be assessed using word-embedding approaches that assess semantic coherence, whereas negative thought disorder (e.g., concreteness, poverty of speech) can be assessed using part-of-speech (POS) tagging to assess syntactic complexity. We aim to establish convergent validity of automated linguistic metrics with clinical ratings, assess normative demographic variance, determine cognitive and functional correlates, and replicate their predictive power for psychosis transition among at-risk youths.
Methods
This study will assess language production in 450 English-speaking individuals in Australia and Canada, who have recent onset psychosis, are at clinical high risk (CHR) for psychosis, or who are healthy volunteers, all well-characterized for cognition, function and symptoms. Speech will be elicited using open-ended interviews. Audio files will be transcribed and preprocessed for automated natural language processing (NLP) analyses of coherence and complexity. Data analyses include canonical correlation, multivariate linear regression with regularization, and machine-learning classification of group status and psychosis outcome.
Conclusions
This prospective study aims to characterize language disturbance across stages of psychosis using computational approaches, including psychometric properties, normative variance and clinical correlates, important for biomarker development. SPEAK will create a large archive of language data available to other investigators, a rich resource for the field.}
}
@article{KRAVCHENKO2020310,
title = {Multi-faceted approach to solving issue of ensuring “zero mortality” on Russian roads},
journal = {Transportation Research Procedia},
volume = {50},
pages = {310-320},
year = {2020},
note = {XIV International Conference on Organization and Traffic Safety Management in Large Cities (OTS-2020)},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2020.10.037},
url = {https://www.sciencedirect.com/science/article/pii/S2352146520307833},
author = {Pavel Kravchenko and Sultan Zhankaziev and Elena Oleshchenko},
keywords = {system analysis, efficiency, automated monitoring system, vehicle, stochastic approach, traffic safety},
abstract = {“The fundamental principle of systems thinking is the ability to look at events, objects and phenomena from various perspectives considered as an aggregate” (O’Connor and McDermott 2006). The article continues studying the issue of preventing the occurrence of causes of death on Russian roads, i.e. “zero” mortality. The study results are presented in a series of articles published in the “Transport of the Russian Federation” journal. It provides a rationale for the feasibility and relevance of applying a methodological approach, which is new for Russian science and practice, to solving the issue of ensuring road traffic safety (RTS) in the interpretation of its meaning, comparable with the meaning of the term “zero mortality”, namely, a multi-faceted approach that considers a multilateral assessment of the quality of management decisions adopted using the knowledge of a full set of different opinions on the objects of any complexity being studied. They include RTS assurance systems, for which a set of different opinions (aspects) regarding all their possible facets, sides, properties, features, etc. — on behalf of the state, authorities, operating structures, and society — can be transformed into a set of factors affecting the level of provided RTS, having a subset of “dangerous” factors that can become causes of death in road traffic accidents (RTAs) in the road traffic environment. The article contains: a digest of the above subset of factors, which are essential for the issue being studied and can serve as the basis for expanding the possibilities of forming a set of death causes, adjusting the functions and corresponding types of required activities functionally bound by a common goal; substantiation of a functional and structural mathematical model for the state RTS system, an algorithmic model for the mechanisms forming its main functional properties and subsets of non-accidental causes of death, which can be understood and addressed preventing the moment of a possible serious RTA, and accidental ones, which cannot be understood and addressed; a “starting” sample of literary sources, capable of expanding (when referring to the publications) and ensuring an increase in the progress of solving the issue of “zero mortality” on Russian roads by 2030 as established by the state strategy for RTS.}
}
@article{TRESADERN20171478,
title = {Industrial medicinal chemistry insights: neuroscience hit generation at Janssen},
journal = {Drug Discovery Today},
volume = {22},
number = {10},
pages = {1478-1488},
year = {2017},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2017.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S1359644617301216},
author = {Gary Tresadern and Frederik J.R. Rombouts and Daniel Oehlrich and Gregor Macdonald and Andres A. Trabanco},
abstract = {The role of medicinal chemistry has changed over the past 10 years. Chemistry had become one step in a process; funneling the output of high-throughput screening (HTS) on to the next stage. The goal to identify the ideal clinical compound remains, but the means to achieve this have changed. Modern medicinal chemistry is responsible for integrating innovation throughout early drug discovery, including new screening paradigms, computational approaches, novel synthetic chemistry, gene-family screening, investigating routes of delivery, and so on. In this Foundation Review, we show how a successful medicinal chemistry team has a broad impact and requires multidisciplinary expertise in these areas.}
}
@article{BELLAOUAR2021106654,
title = {Weighted automata sequence kernel: Unification and generalization},
journal = {Knowledge-Based Systems},
volume = {212},
pages = {106654},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2020.106654},
url = {https://www.sciencedirect.com/science/article/pii/S0950705120307838},
author = {Slimane Bellaouar and Hadda Cherroun and Attia Nehar and Djelloul Ziadi},
keywords = {Sequence kernel, Weighted automata, Formal power series, Sequence multiset kernel, Unification, Generalization},
abstract = {Sequence kernels have been widely used for learning from sequential data. In recent years, a significant effort has been devoted to sequence kernels focusing on individual problems, and so devising several approaches. As a contribution in developing a unified theory of machine learning, in this paper, we complement our previous general framework that deals with sequence kernels, termed weighted automata sequence kernel. We provide a more formal presentation of the framework fully supported with proofs. In fact, the mapping of a string s to a high dimensional feature space can be modeled by a formal power series that can be realized by a weighted automaton (WA) As representing all subsequences of the string s. The computation of the kernel K(s,t) between two strings is the behavior of the intersection weighted automaton As,t=As∩At. Regarding kernel computation efficiency, we propose a forward lookup automaton intersection technique to prevent unsuccessful ε-paths while evaluating the WA computations. The experiments use the Reuters-21578 collection. The results reveal that the kernel evaluation using our proposed technique is faster than the one that uses standard intersection. In order To highlight the unification aspect of our model, we study the relationship between our general framework and a variety of common sequence kernels. Finally, we prove that the proposed formalization can be generalized to sequence multiset kernels showing the robustness of our approach. In our case, the new defined sequence multiset kernel can also be seen as a tree kernel.}
}
@article{LUND2017556,
title = {Smart energy and smart energy systems},
journal = {Energy},
volume = {137},
pages = {556-565},
year = {2017},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2017.05.123},
url = {https://www.sciencedirect.com/science/article/pii/S0360544217308812},
author = {Henrik Lund and Poul Alberg Østergaard and David Connolly and Brian Vad Mathiesen},
keywords = {Renewable energy systems, Smart grid, Energy system modelling, Electro fuels, Power-to-Gas, Power-to-heat},
abstract = {In recent years, the terms “Smart Energy” and “Smart Energy Systems” have been used to express an approach that reaches broader than the term “Smart grid”. Where Smart Grids focus primarily on the electricity sector, Smart Energy Systems take an integrated holistic focus on the inclusion of more sectors (electricity, heating, cooling, industry, buildings and transportation) and allows for the identification of more achievable and affordable solutions to the transformation into future renewable and sustainable energy solutions. This paper first makes a review of the scientific literature within the field. Thereafter it discusses the term Smart Energy Systems with regard to the issues of definition, identification of solutions, modelling, and integration of storage. The conclusion is that the Smart Energy System concept represents a scientific shift in paradigms away from single-sector thinking to a coherent energy systems understanding on how to benefit from the integration of all sectors and infrastructures.}
}
@article{SUE20219,
title = {Generative design in factory layout planning},
journal = {Procedia CIRP},
volume = {99},
pages = {9-14},
year = {2021},
note = {14th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 15-17 July 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121002584},
author = {Marian Süße and Matthias Putz},
keywords = {Generative design, Computational design, Evolutionary design, Factory planning, Layout planning, Literature search},
abstract = {Planning and optimization of facility layouts have been investigated for decades and manifold approaches are applied for structuring and design of production layouts. However, results heavily depend on the experience and creativity of involved planning experts. Currently, complexity of planning processes constantly increases, e.g. due to further requirements of energy and media supply. Generative Design, hitherto mainly applied in component development, provides opportunities to cope with a much larger solution space and develop creative layout concepts. Thus, based on a structured overview on established planning methods, a concept and first results of factory layout planning with Generative Design are described.}
}
@incollection{TURKLE20017035,
title = {Human–Computer Interface},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {7035-7038},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/04333-3},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767043333},
author = {S. Turkle},
abstract = {The computer/human interface refers to the modalities through which people interact with computational technologies. Looked at over the last half century, the trend has been from a style of interaction in which the computer is approached as a mechanism to a style of interaction in which the computer is approached as a behaving and aware organism. This first trend has been related to another, from people acting directly on the machine to acting indirectly on its increasingly elaborate self presentation in which many layers of programming stand between underlying processes and what is presented to the user. This movement is more than technical. Computational technologies have served as objects to think with, that is, as carrier objects for ideas. Through their changing interfaces, the computer has moved from being the carrier object of a culture of calculation to that of a culture of simulation. The culture of simulation promotes a way of understanding in which users are encouraged to take computers ‘at interface value.’}
}
@article{RAMFUL2014119,
title = {Reversible reasoning in fractional situations: Theorems-in-action and constraints},
journal = {The Journal of Mathematical Behavior},
volume = {33},
pages = {119-130},
year = {2014},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2013.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0732312313000990},
author = {Ajay Ramful},
keywords = {Division, Fraction, Multiplicative reasoning, Reversibility, Units, Vergnaud's theory},
abstract = {The aim of this study was to investigate, at a fine-grained level of detail, the theorems-in-action deployed and the constraints encountered by middle-school students in reasoning reversibly in the multiplicative domain of fraction. A theorem-in-action (Vergnaud, 1988) is a conceptual construct to trace students’ reasoning in a problem solving situation. Two seventh grade students were interviewed in a rural middle-school in the southern part of the United States. The students’ strategies were examined with respect to the numerical features of the problem situations and the ways they viewed and operated on fractional units. The results show that reversible reasoning is sensitive to the numeric feature of problem parameters. Relatively prime numbers and fractional quantities acted as inhibitors preventing the cueing of the multiplication–division invariant, thereby constraining students from reasoning reversibly. Among others, two key resources were identified as being essential for reasoning reversibly in fractional contexts: firstly, interpreting fractions in terms of units, which enabled the students to access their whole number knowledge and secondly, the unit-rate theorem-in-action. Failure to conceptualize multiplicative relations in reverse constrained the students to use more primitive strategies, leading them to solve problems non-deterministically and at higher computational costs.}
}
@article{2024100670,
title = {Erratum regarding missing declaration of competing interest statements in previously published articles},
journal = {International Journal of Child-Computer Interaction},
volume = {41},
pages = {100670},
year = {2024},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100670},
url = {https://www.sciencedirect.com/science/article/pii/S2212868924000382}
}
@article{CERONGARCIA20221,
title = {Jigsaw cooperative learning of multistage counter-current liquid-liquid extraction using Mathcad®},
journal = {Education for Chemical Engineers},
volume = {38},
pages = {1-13},
year = {2022},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2021.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S1749772821000488},
author = {Mª Carmen Cerón-García and Lorenzo López-Rosales and Juan José Gallardo-Rodríguez and Elvira Navarro-López and Asterio Sánchez-Mirón and Francisco García-Camacho},
keywords = {Liquid-liquid extraction, Mass transfer, Jigsaw cooperative learning, Computational tools, Mathcad®},
abstract = {This work shows the improvement in comprehending counter-current liquid-liquid extraction by applying jigsaw-type cooperative learning and the engineering math software Mathcad® in a chemical engineering course, part of the Chemistry degree. This study was performed on two different groups at the University of Almería (Spain) over three academic years. The students were divided into two groups: one half of the class followed a non-cooperative learning methodology (the control) while the other half were spread among the jigsaw cooperative groups following the methodology known as “Jigsaw Experts Groups”. A main template made with Mathcad® of a multistage counter-current liquid-liquid extraction was supplied to both the jigsaw and non-jigsaw groups. The assessment of this educational experience in the course revealed that the jigsaw group outperformed the control group. The use of Mathcad® proved to be very intuitive and effective in explaining these relatively complex problems and utilising it is highly recommended; we suggest it is used rather than classical and less intuitive graphical methods.}
}
@article{TIAN2015338,
title = {Safety assessment method of performance-based navigation airspace planning},
journal = {Journal of Traffic and Transportation Engineering (English Edition)},
volume = {2},
number = {5},
pages = {338-345},
year = {2015},
issn = {2095-7564},
doi = {https://doi.org/10.1016/j.jtte.2015.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S2095756415000690},
author = {Yong Tian and Lili Wan and Chun-hung Chen and Ye Yang},
keywords = {Air traffic management, Safety assessment, Operational planning, Conflict risk, Adverse weather},
abstract = {The paper introduces a computational model of airspace conflict risk in the hierarchy of performance-based navigation (PBN) airspace operation and combines it with air traffic controller (ATC) workload to propose a method for safety assessment of PBN airspace operational planning. Capacity probability distribution is employed to depict airspace capacity in uncertain weather, errors of deviating from nominal PBN track are taken into consideration, and the stochastic process based on Gaussian distribution is used to depict random aircraft motion according to airspace PBN specification, so as to build an airspace conflict risk computational model in corresponding capacity scenario. Guangzhou No. 15 sector is chosen for simulation validation. The analysis results suggest that 60% of ATC workload is corresponding to sector traffic flow of 31 aircraft/h and airspace risk of 0.018 conflict/h, while 70% of ATC workload is corresponding to sector traffic flow of 35 aircraft/h and airspace risk of 0.03 conflict/h. As air traffic flow increases, both airspace conflict risk value and ATC workload will increase, resulting in reduction of airspace safety, though their increasing magnitudes differ with different capacity scenarios. The safety assessment method enables effective quantization of safety with regard to airspace operational planning strategy, and benefits the development of optimal operational scheme that balances risk with capacity demand.}
}
@article{BRODIN2009345,
title = {Univariate and bivariate GPD methods for predicting extreme wind storm losses},
journal = {Insurance: Mathematics and Economics},
volume = {44},
number = {3},
pages = {345-356},
year = {2009},
issn = {0167-6687},
doi = {https://doi.org/10.1016/j.insmatheco.2008.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167668708001455},
author = {Erik Brodin and Holger Rootzén},
keywords = {Extreme value statistics, Generalized Pareto distribution, Likelihood prediction intervals, Peaks over threshold, Trend analysis, Wind storm losses},
abstract = {Wind storm and hurricane risks are attracting increased attention as a result of recent catastrophic events. The aim of this paper is to select, tailor, and develop extreme value methods for use in wind storm insurance. The methods are applied to the 1982–2005 losses for the largest Swedish insurance company, the Länsförsäkringar group. Both a univariate and a new bivariate Generalized Pareto Distribution (GPD) gave models which fitted the data well. The bivariate model led to lower estimates of risk, except for extreme cases, but taking statistical uncertainty into account the two models lead to qualitatively similar results. We believe that the bivariate model provided the most realistic picture of the real uncertainties. It additionally made it possible to explore the effects of changes in the insurance portfolio, and showed that loss distributions are rather insensitive to portfolio changes. We found a small trend in the sizes of small individual claims, but no other trends. Finally, we believe that companies should develop systematic ways of thinking about “not yet seen” disasters.}
}
@article{AMARAL20211,
title = {Overlapping but distinct: Distal connectivity dissociates hand and tool processing networks},
journal = {Cortex},
volume = {140},
pages = {1-13},
year = {2021},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2021.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S0010945221001088},
author = {Lénia Amaral and Fredrik Bergström and Jorge Almeida},
keywords = {Tools, Hands, Distal connectivity, Representation, Functional organization, fMRI},
abstract = {The processes and organizational principles of information involved in object recognition have been a subject of intense debate. These research efforts led to the understanding that local computations and feedforward/feedback connections are essential to our representations and their organization. Recent data, however, has demonstrated that distal computations also play a role in how information is locally processed. Here we focus on how long-range connectivity and local functional organization of information are related, by exploring regions that show overlapping category-preferences for two categories and testing whether their connections are related with distal representations in a category-specific way. We used an approach that relates functional connectivity with distal areas to local voxel-wise category-preferences. Specifically, we focused on two areas that show an overlap in category-preferences for tools and hands–the inferior parietal lobule/anterior intraparietal sulcus (IPL/aIPS) and the posterior middle temporal gyrus/lateral occipital temporal cortex (pMTG/LOTC) – and how connectivity from these two areas relate to voxel-wise category-preferences in two ventral temporal regions dedicated to the processing of tools and hands separately–the left medial fusiform gyrus and the fusiform body area respectively–as well as across the brain. We show that the functional connections of the two overlap areas correlate with categorical preferences for each category independently. These results show that regions that process both tools and hands maintain object topography in a category-specific way. This potentially allows for a category-specific flow of information that is pertinent to computing object representations.}
}
@article{KAPITANYFOVENY202466,
title = {EEG based depression detection by machine learning: Does inner or overt speech condition provide better biomarkers when using emotion words as experimental cues?},
journal = {Journal of Psychiatric Research},
volume = {178},
pages = {66-76},
year = {2024},
issn = {0022-3956},
doi = {https://doi.org/10.1016/j.jpsychires.2024.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0022395624004515},
author = {Máté Kapitány-Fövény and Mihály Vetró and Gábor Révy and Dániel Fabó and Danuta Szirmai and Gábor Hullám},
keywords = {EEG, Depression, Machine learning, Diagnostic approach},
abstract = {Background
Objective diagnostic approaches need to be tested to enhance the efficacy of depression detection. Non-invasive EEG-based identification represents a promising area.
Aims
The present EEG study addresses two central questions: 1) whether inner or overt speech condition result in higher diagnositc accuracy of depression detection; and 2) does the affective nature of the presented emotion words count in such diagnostic approach.
Methods
A matched case-control sample consisting of 10 depressed subjects and 10 healthy controls was assessed. An EEG headcap containing 64 electrodes measured neural responses to experimental cues presented in the form of 15 different words that belonged to three emotional categories: neutral, positive, and negative. 120 experimental cues was presented for every participant, each containing an “inner speech” and an “overt speech” segment. An EEGNet neural network was utilized.
Results
The highest diagnostic accuracy of the EEGNet model was observed in the case of the overt speech condition (i.e. 69.5%), while a an overall subject-wise accuracy of 80% was achieved by the model. Only a negligible difference in diagnostic accuracy could be found between aggregated emotion word categories, with the highest accuracy (i.e. 70.2%) associated with the presentation of positive emotion words. Model decision was primarily influenced by electrodes representing the regions of the left parietal, the left temporal lobe and the middle frontal areas.
Conclusions
While the generalizability of our results is limited by the small sample size and potentially uncontrolled confounders, depression was associated with sensitive and presumably network-like aspects of these brain areas, potentially implying a higher level of emotion regulation that increases primarily in open communication.}
}
@article{XING2024103704,
title = {Financial risk tolerance profiling from text},
journal = {Information Processing & Management},
volume = {61},
number = {4},
pages = {103704},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103704},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324000645},
author = {Frank Xing},
keywords = {Artificial intelligence in finance, Risk tolerance, Risk profiling, Text mining, Convolutional neural network},
abstract = {Traditionally, individual financial risk tolerance information is gathered via questionnaires or similar structured psychometric tools. Our abundant digital footprint, as an unstructured alternative, is less investigated. Leveraging such information can potentially support large-scale and cost-efficient financial services. Therefore, I explore the possibility of building a computational model that distills risk tolerance information from user texts in this study, and discuss the design principles discovered from empirical results and their implications. Specifically, a new quaternary classification task is defined for text mining-based risk profiling. Experiments show that pre-trained large language models set a baseline micro-F1 of circa 0.34. Using a convolutional neural network (CNN), the reported system achieves a micro-F1 of circa 0.51, which significantly outperforms the baselines, and is a circa 4% further improvement over the standard CNN configurations (micro-F1 of circa 0.47). Textual feature richness and supervised learning are found to be the key contributors to model performances, while other machine learning strategies suggested by previous research (data augmentation and multi-tasking) are less effective. The findings confirm user texts to be a useful risk profiling resource and provide several insights on this task.}
}
@article{HAMALAINEN2013623,
title = {On the importance of behavioral operational research: The case of understanding and communicating about dynamic systems},
journal = {European Journal of Operational Research},
volume = {228},
number = {3},
pages = {623-634},
year = {2013},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2013.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0377221713001197},
author = {Raimo P. Hämäläinen and Jukka Luoma and Esa Saarinen},
keywords = {Process of OR, Practice of OR, Systems dynamics, Behavioral OR, Systems thinking},
abstract = {We point out the need for Behavioral Operational Research (BOR) in advancing the practice of OR. So far, in OR behavioral phenomena have been acknowledged only in behavioral decision theory but behavioral issues are always present when supporting human problem solving by modeling. Behavioral effects can relate to the group interaction and communication when facilitating with OR models as well as to the possibility of procedural mistakes and cognitive biases. As an illustrative example we use well known system dynamics studies related to the understanding of accumulation. We show that one gets completely opposite results depending on the way the phenomenon is described and how the questions are phrased and graphs used. The results suggest that OR processes are highly sensitive to various behavioral effects. As a result, we need to pay attention to the way we communicate about models as they are being increasingly used in addressing important problems like climate change.}
}
@article{KYRIAZOS2024105070,
title = {Quantum concepts in Psychology: Exploring the interplay of physics and the human psyche},
journal = {BioSystems},
volume = {235},
pages = {105070},
year = {2024},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2023.105070},
url = {https://www.sciencedirect.com/science/article/pii/S0303264723002459},
author = {Theodoros Kyriazos and Mary Poga},
keywords = {Quantum mechanics, Quantum psychology, Interdisciplinary, Human psyche},
abstract = {This paper delves into the innovative intersection of quantum mechanics and psychology, examining the potential of quantum principles to provide fresh insights into human emotions, cognition, and consciousness. Drawing parallels between quantum phenomena such as superposition, entanglement, tunneling, decoherence and their psychological counterparts, we present a quantum-psychological model that reimagines emotional states, cognitive breakthroughs, interpersonal relationships, and the nature of consciousness. The study uses computational models and simulations to explore this interdisciplinary fusion's implications and applications, highlighting its potential benefits and inherent challenges. While quantum concepts offer a rich metaphorical lens to view the intricacies of human experience, it is essential to approach this nascent framework with enthusiasm and skepticism. Rigorous empirical validation is paramount to realize its full potential in research and therapeutic contexts. This exploration stands as a promising thread in the tapestry of intellectual history, suggesting a deeper understanding of the human psyche through the lens of quantum mechanics.}
}
@article{ANDRAS2021110734,
title = {Where do successful populations originate from?},
journal = {Journal of Theoretical Biology},
volume = {524},
pages = {110734},
year = {2021},
issn = {0022-5193},
doi = {https://doi.org/10.1016/j.jtbi.2021.110734},
url = {https://www.sciencedirect.com/science/article/pii/S0022519321001569},
author = {Peter Andras and Adam Stanton},
keywords = {Computational modelling, Socio-technical evolution, Socio-biological simulation, Human geography, Geography of speciation},
abstract = {In order to understand the dynamics of emergence and spreading of socio-technical innovations and population moves it is important to determine the place of origin of these populations. Here we focus on the role of geographical factors, such as land fertility and mountains in the context of human population evolution and distribution dynamics. We use a constrained diffusion-based computational model, computer simulations and the analysis of geographical and land-quality data. Our analysis shows that successful human populations, i.e. those which become dominant in their socio – geographical environment, originate from lands of many valleys with relatively low land fertility, which are close to areas of high land fertility. Many of the homelands predicted by our analysis match the assumed homelands of known successful populations (e.g. Bantus, Turkic, Maya). We also predict other likely homelands as well, where further archaeological, linguistic or genetic exploration may confirm the place of origin for populations with no currently identified urheimat. Our work is significant because it advances the understanding of human population dynamics by guiding the identification of the origin locations of successful populations.}
}
@article{BOHLENDER2018428,
title = {Compositional Verification of PLC Software using Horn Clauses and Mode Abstraction},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {7},
pages = {428-433},
year = {2018},
note = {14th IFAC Workshop on Discrete Event Systems WODES 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.06.336},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318306669},
author = {Dimitri Bohlender and Stefan Kowalewski},
keywords = {Formal verification, Programmable logic controllers, Industry automation, Software tools, Software safety},
abstract = {Real-world PLC software is modular and composed of many different function blocks. Due to its cyclic execution, an engineer’s mental model of a single function block often exhibits state machine semantics – partitioning a block’s behaviours into different modes of operation. We propose a technique called mode abstraction for automatic computation of such high-level models from program semantics and investigate its impact on the overall model checking performance. To this end, we use constrained horn clauses to characterise the program semantics, mode transitions and safety goals in a compositional way. The resulting verification conditions are discharged using an SMT solver. Evaluation of our prototypical implementation on examples from the PLCopen Safety library shows the effectiveness of both the chosen formalism and mode abstraction.}
}
@article{HERSHCOVICH2021107809,
title = {Thermal performance of sculptured tiles for building envelopes},
journal = {Building and Environment},
volume = {197},
pages = {107809},
year = {2021},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2021.107809},
url = {https://www.sciencedirect.com/science/article/pii/S036013232100216X},
author = {Cheli Hershcovich and René {van Hout} and Vladislav Rinsky and Michael Laufer and Yasha j. Grobman},
keywords = {Microclimate, Computational fluid dynamics simulations, Particle Image Velocimetry (PIV) experiments, Building envelope, Thermal insulation},
abstract = {Since effective thermal insulation of buildings could significantly decrease energy consumption worldwide, this study examined the potential of concrete tiles with complex geometries at improving thermal performance in building envelopes. The study focused on air flow characteristics that occur near the external surface of the tile, and their influence on the rate of heat transfer within the material. Six groups of sculptured tiles were developed, using a systematic approach (i.e., repetition of basic geometries), biomimicry, and inspiration from natural envelopes. Air flow behavior and heat transfer rates were examined at three different wind speeds, through both experiments and computational fluid dynamics simulations using a configuration of flow impingement that can be regarded as the worst case scenario. After successfully validating the data, additional numerical simulations were conducted for all developed tiles using the Star-CCM + commercial software. The results showed an improvement in the insulation performance of about half of all the tested cases. Moreover, significant improvements were seen in the geometries that mimicked animal fur, achieving heat transfer rates that were up to 24% lower than those achieved by smooth tiles. Our results indicate that the application of such tiles with increased thermal resistance could save on thermal insulation materials and improve the thermal performance of building façades.}
}
@article{WIECHA2024101129,
title = {Deep learning for nano-photonic materials – The solution to everything!?},
journal = {Current Opinion in Solid State and Materials Science},
volume = {28},
pages = {101129},
year = {2024},
issn = {1359-0286},
doi = {https://doi.org/10.1016/j.cossms.2023.101129},
url = {https://www.sciencedirect.com/science/article/pii/S1359028623000748},
author = {Peter R. Wiecha},
abstract = {Deep learning is currently being hyped as an almost magical tool for solving all kinds of difficult problems that computers have not been able to solve in the past. Particularly in the fields of computer vision and natural language processing, spectacular results have been achieved. The hype has now infiltrated several scientific communities. In (nano-) photonics, researchers are trying to apply deep learning to all kinds of forward and inverse problems. A particularly challenging problem is for instance the rational design of nanophotonic materials and devices. In this opinion article, I will first discuss the public expectations of deep learning and give an overview of the quite different scales at which actors from industry and research are operating their deep learning models. I then examine the weaknesses and dangers associated with deep learning. Finally, I’ll discuss the key strengths that make this new set of statistical methods so attractive, and review a personal selection of opportunities that shouldn’t be missed in the current developments.}
}
@article{KOSTERHALE2013836,
title = {Theory of Mind: A Neural Prediction Problem},
journal = {Neuron},
volume = {79},
number = {5},
pages = {836-848},
year = {2013},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2013.08.020},
url = {https://www.sciencedirect.com/science/article/pii/S089662731300754X},
author = {Jorie Koster-Hale and Rebecca Saxe},
abstract = {Predictive coding posits that neural systems make forward-looking predictions about incoming information. Neural signals contain information not about the currently perceived stimulus, but about the difference between the observed and the predicted stimulus. We propose to extend the predictive coding framework from high-level sensory processing to the more abstract domain of theory of mind; that is, to inferences about others’ goals, thoughts, and personalities. We review evidence that, across brain regions, neural responses to depictions of human behavior, from biological motion to trait descriptions, exhibit a key signature of predictive coding: reduced activity to predictable stimuli. We discuss how future experiments could distinguish predictive coding from alternative explanations of this response profile. This framework may provide an important new window on the neural computations underlying theory of mind.}
}
@article{LIMASILVA2024109436,
title = {Dynamical homotopy transient-based technique to improve the convergence of ill-posed power flow problem},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {155},
pages = {109436},
year = {2024},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2023.109436},
url = {https://www.sciencedirect.com/science/article/pii/S0142061523004933},
author = {Alisson Lima-Silva and Francisco Damasceno Freitas},
keywords = {Euler method, Homotopy, Newton–Raphson method, Numerical integration, Power flow problem, Ill-posed problem},
abstract = {This paper proposes a hybrid technique to solve the ill-posed Power Flow Problem (PFP), considering a homotopy approach. The primary proposal is to solve large-scale problems where the traditional Newton–Raphson (NR) fails to converge, as in the case of ill-posed systems. The method explores a dynamical homotopy transient-based technique to improve the convergence of the ill-conditioned problem instead of using the classical static method. Depending on the integration selected scheme and the integration step, the result furnished by the dynamical homotopy method has low accuracy. Then, the NR method is employed to refine the low-accuracy result and accurately determine the ill-posed PFP solution. The proposed approach can be implemented efficiently using only one Jacobian matrix computation and LU factorization per point of the homotopy path. In the static homotopy problem, a PFP using previous results must be solved per path point. In this case, some LU factorizations are necessary for each path point. The technique’s performance was evaluated through experiments, including a 70,000-bus large-scale system. The approximate dynamical homotopy result used as an initial estimate provided appropriate convergence quality for the NR method to determine a high-precision solution to the PFP.}
}
@article{VIJAYALAKSHMI20222382,
title = {Topological indices relating some nanostructures},
journal = {Materials Today: Proceedings},
volume = {68},
pages = {2382-2386},
year = {2022},
note = {4th International Conference on Advances in Mechanical Engineering},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2022.09.106},
url = {https://www.sciencedirect.com/science/article/pii/S2214785322059144},
author = {K. {Vijaya Lakshmi} and N. Parvathi},
keywords = {Topological indices, Chemical graph theory, Molecular structure, Nanotechnology, Nanotubes, Nanosheet},
abstract = {The application of mathematics mainly graph theory has a significant role in the development of chemistry. Topological indices act as a bridge between mathematics and chemistry. The topological index is nothing but a numerical quantity that has yielded valuable insights in terms of molecular structure. In nanotechnology, the structural characterization of the molecular species and the nanoparticles are indicated by the indices. These indices identify the symmetry of molecular structures and provide them a scientific terminology to predict qualities like boiling temperatures, viscosity, and gyrating radius. The main objective of the current paper is to compute topological indices with the corresponding formulae, moreover, these indices help in predicting physicochemical properties without the involvement of the laboratory. The current research paper investigated the computation of degree-based topological indices for the nanotubes HAC5C6C7[a,b] and TU(C4C6C8[a,b].}
}
@article{GRAY2000401,
title = {Objects, Actions, and Images: A Perspective on Early Number Development},
journal = {The Journal of Mathematical Behavior},
volume = {18},
number = {4},
pages = {401-413},
year = {2000},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(00)00025-0},
url = {https://www.sciencedirect.com/science/article/pii/S0732312300000250},
author = {Eddie Gray and Demetra Pitta and David Tall},
abstract = {It is the purpose of this article to present a review of research evidence that indicates the existence of qualitatively different thinking in elementary number development. In doing so, the article summarizes empirical evidence obtained over a period of 10 years. This evidence first signaled qualitative differences in numerical processing, and was seminal in the development of the notion of procept. More recently, it examines the role of imagery in elementary number processing. Its conclusions indicate that in the abstraction of numerical concepts from numerical processes qualitatively different outcomes may arise because children concentrate on different objects or different aspects of the objects, which are components of numerical processing.}
}
@article{ROY2022105849,
title = {EEG based stress analysis using rhythm specific spectral feature for video game play},
journal = {Computers in Biology and Medicine},
volume = {148},
pages = {105849},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105849},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522006023},
author = {Shidhartho Roy and Monira Islam and Md. Salah Uddin Yusuf and Nushrat Jahan},
keywords = {Beta–Alpha ratio, EEG, Stress-relaxation modeling, Topography, Video gameplay},
abstract = {Background and objective:
For the emerging significance of mental stress, various research directives have been established over time to understand better the causes of stress and how to deal with it. In recent years, the rise of video gameplay has been unprecedented, further triggered by the lockdown imposed due to the COVID-19 pandemic. Several researchers and organizations have contributed to the practical analysis of the impacts of such extended periods of gameplay, which lacks coordinated studies to underline the outcomes and reflect those in future game designing and public awareness about video gameplay. Investigations have mainly focused on the “gameplay stress” based on physical syndromes. Some studies have analyzed the effects of video gameplay with Electroencephalogram (EEG), Magnetic resonance imaging (MRI), etc., without concentrating on the relaxation procedure after video gameplay.
Methods:
This paper presents an end-to-end stress analysis for video gaming stimuli using EEG. The power spectral density (PSD) of the Alpha and Beta bands is computed to calculate the Beta-to-Alpha ratio (BAR). The Alpha and Beta band power is computed, and the Beta-to-Alpha band power ratio (BAR) has been determined. In this article, BAR is used to denote mental stress. Subjects are chosen based on various factors such as gender, gameplay experience, age, and Body mass index (BMI). EEG is recorded using Scan SynAmps2 Express equipment. There are three types of video gameplay: strategic, puzzle, and combinational. Relaxation is accomplished in this study by using music of various pitches. Two types of regression analysis are done to mathematically model stress and relaxation curve. Brain topography is rendered to indicate the stressed and relaxed region of the brain.
Results:
In the relaxed state, the subjects have BAR 0.701, which is considered the baseline value. Non-gamer subjects have an average BAR of 2.403 for 1 h of strategic video gameplay, whereas gamers have 2.218 BAR concurrently. After 12 minutes of listening to low-pitch music, gamers achieved 0.709 BAR, which is nearly the baseline value. In comparison to Quartic regression, the 4PL symmetrical sigmoid function performs regression analysis with fewer parameters and computational power.
Conclusion:
Non-gamers experience more stress than gamers, whereas strategic games stress the human brain more. During gameplay, the beta band in the frontal region is mostly activated. For relaxation, low pitch music is the most useful medium. Residual stress is evident in the frontal lobe when the subjects have listened to high pitch music. Quartic regression and 4PL symmetrical sigmoid function have been employed to find the model parameters of the relaxation curve. Among them, quartic regression performs better in terms of Akaike information criterion (AIC) and R2 measure.}
}
@article{RAYAMORENO2024125385,
title = {Degradation of the ZT thermoelectric figure of merit in silicon when nanostructuring: From bulk to nanowires},
journal = {International Journal of Heat and Mass Transfer},
volume = {225},
pages = {125385},
year = {2024},
issn = {0017-9310},
doi = {https://doi.org/10.1016/j.ijheatmasstransfer.2024.125385},
url = {https://www.sciencedirect.com/science/article/pii/S0017931024002163},
author = {Martí Raya-Moreno and Riccardo Rurali and Xavier Cartoixà},
keywords = {Thermoelectrics, Nanowires, Phonon drag, , Coupled e-ph Boltzmann transport equation},
abstract = {Since the landmark paper by Hicks and Dresselhaus [Phys. Rev. B 47, 16631(R) (1993)], there has been a general consensus that one-dimensional nanoscale conductors, i.e. nanowires, provide the long sought paradigm to implement the so-called phonon-glass electron-crystal material, which results in large improvements in the thermoelectric figure of merit ZT. Despite some encouraging—though isolated—experimental results, this idea has never been subjected to a rigorous scrutiny and the effect of the coupled dynamics of electrons and phonons has usually been oversimplified. To bypass these limitations, we have calculated the effective thermoelectric parameters for silicon nanowires (SiNWs) by iteratively solving the coupled electron-phonon Boltzmann transport equation (EPBTE) supplied with first-principles data. This allows for an unprecedented precision in determining the correct dependence of the thermoelectric parameters with system size; including, but not limited to, the figure of merit and its enhancement or degradation due to nanostructuring. Indeed, we demonstrate that the commonly used relaxation time approximation (RTA), or the uncoupled beyond the RTA (iterative) solution fail to describe the correct effect of nanostructuring on the thermoelectric properties and efficiency in SiNWs due to the strong contribution of phonon drag to the Seebeck coefficient, so that the use of fully coupled solution of the EPBTE is essential to obtain the correct effect of nanostructuring. Most importantly, we show that, contrarily to what commonly argued, resorting to NWs is not necessarily beneficial for ZT. Indeed, in a wide range of diameters nanostructuring diminishes the Seebeck coefficient faster than the decrease in thermal conductivity, due to the suppression of very long wavelength phonons responsible for the largest contribution to the phonon drag component of the Seebeck coefficient. This penalty to ZT can be mitigated if the NWs have a very rough surface, providing additional reduction to the thermal conductivity. Additionally, we demonstrate that our methodology provides improved data sets for an accurate determination of doping concentration in NWs through electrical-based inference and excellent agreement with the available experimental data.}
}
@article{ATKINSON20137060,
title = {Evolutionary optimization for ranking how-to questions based on user-generated contents},
journal = {Expert Systems with Applications},
volume = {40},
number = {17},
pages = {7060-7068},
year = {2013},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2013.06.017},
url = {https://www.sciencedirect.com/science/article/pii/S0957417413003977},
author = {John Atkinson and Alejandro Figueroa and Christian Andrade},
keywords = {Community question-answering, Question-answering systems, Concept clustering, Evolutionary computation, HPSG parsing},
abstract = {In this work, a new evolutionary model is proposed for ranking answers to non-factoid (how-to) questions in community question-answering platforms. The approach combines evolutionary computation techniques and clustering methods to effectively rate best answers from web-based user-generated contents, so as to generate new rankings of answers. Discovered clusters contain semantically related triplets representing question–answers pairs in terms of subject-verb-object, which is hypothesized to improve the ranking of candidate answers. Experiments were conducted using our evolutionary model and concept clustering operating on large-scale data extracted from Yahoo! Answers. Results show the promise of the approach to effectively discovering semantically similar questions and improving the ranking as compared to state-of-the-art methods.}
}
@article{MEJIA201899,
title = {The Power of Writing, a Pebble Hierarchy and a Narrative for the Teaching of Automata Theory},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {339},
pages = {99-110},
year = {2018},
note = {The XLII Latin American Computing Conference},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2018.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S1571066118300513},
author = {Carolina Mejía and J. {Andres Montoya} and Christian Nolasco},
keywords = {Pebble automata, finite state automata},
abstract = {In this work we study pebble automata. Those automata constitute an infinite hierarchy of discrete models of computation. The hierarchy begins at the level of finite state automata (0-pebble automata) and approaches the model of one-tape Turing machines. Thus, it can be argued that it is a complete hierarchy that covers, in a continuous way, all the models of automata that are important in the theory of computation. We investigate the use of this hierarchy as a narrative for the teaching of automata theory. We also investigate some fundamental questions concerning the power of pebble automata.}
}
@article{DERBYSHIRE201777,
title = {Potential surprise theory as a theoretical foundation for scenario planning},
journal = {Technological Forecasting and Social Change},
volume = {124},
pages = {77-87},
year = {2017},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2016.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0040162516300671},
author = {James Derbyshire},
keywords = {G. L. S. Shackle, Scenario planning, Plausibility, Intuitive Logics, Crucial decisions, Uncertainty},
abstract = {Despite some recent progress, scenario planning's development as an academic discipline remains constrained by the perception it is solely a practical tool for thinking about the future, with limited theoretical foundations. The paper addresses this issue by showing that G. L. S. Shackle's ‘Potential Surprise Theory’ (PST) contains much that can lend theoretical support to scenario planning - especially its use of plausibility rather than probability, and its focus on potential extreme outcomes. Moreover, PST and scenario planning share the same ontology, viewing the future as constructed by the imagination of individuals. Yet, under PST, while the future is imagined and, therefore, subjective, individuals nevertheless seek to identify the ‘best’ option through a deductive process of elimination. PST therefore assists in overcoming the divide between the constructivist and deductivist perspectives in scenario planning as it employs both. Finally, the paper shows that theoretically underpinning scenario planning with PST would place it at the heart of contemporary debates on decision making under uncertainty taking place in economics and other fields, enhancing its status and profile as a discipline.}
}
@article{RUI2024100711,
title = {Simulation of e-learning in vocal network teaching experience system based on intelligent Internet of things technology},
journal = {Entertainment Computing},
volume = {50},
pages = {100711},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100711},
url = {https://www.sciencedirect.com/science/article/pii/S187595212400079X},
author = {Yu Rui},
keywords = {Intelligent technology, Internet of Things, E-learning, Vocal music network, Teaching experience, System simulation},
abstract = {With the rapid development of smart Internet of Things technology, education is also starting to use this technology to improve the learning experience. As an art subject, vocal music teaching has many limitations in the traditional face-to-face teaching methods, and e-learning provides new possibilities for vocal music network teaching. We analyze the problems and challenges existing in the traditional face-to-face mode of vocal music teaching, and then based on the intelligent Internet of Things technology, we design a vocal music network teaching experience system. The system combines sound acquisition equipment, intelligent audio processing algorithm, virtual classroom and other technologies to realize the simulation experience of online vocal music teaching. We have developed an intelligent audio processing algorithm for analyzing and processing students’ singing sounds. This algorithm can detect problems in tone, timbre, rhythm, and more, and provide real-time feedback and advice. In this way, students can understand the shortcomings of their own singing skills, and make timely adjustments and improvements. This study shows that e-learning based on intelligent Internet of Things technology has important application value in vocal music network teaching. By simulating the classroom environment and providing real-time feedback, students can obtain a better learning experience and improve their vocal skills.}
}
@article{LI20231485,
title = {A Data Driven Security Correction Method for Power Systems with UPFC},
journal = {Energy Engineering},
volume = {120},
number = {6},
pages = {1485-1502},
year = {2023},
issn = {0199-8595},
doi = {https://doi.org/10.32604/ee.2023.022856},
url = {https://www.sciencedirect.com/science/article/pii/S0199859523000519},
author = {Qun Li and Ningyu Zhang and Jianhua Zhou and Xinyao Zhu and Peng Li},
keywords = {Manuscript, security correction, data-driven, deep neural network (DNN), unified power flow controller (UPFC), overload of transmission lines},
abstract = {The access of unified power flow controllers (UPFC) has changed the structure and operation mode of power grids all across the world, and it has brought severe challenges to the traditional real-time calculation of security correction based on traditional models. Considering the limitation of computational efficiency regarding complex, physical models, a data-driven power system security correction method with UPFC is, in this paper, proposed. Based on the complex mapping relationship between the operation state data and the security correction strategy, a two-stage deep neural network (DNN) learning framework is proposed, which divides the offline training task of security correction into two stages: in the first stage, the stacked auto-encoder (SAE) classification model is established, and the node correction state (0/1) output based on the fault information; in the second stage, the DNN learning model is established, and the correction amount of each action node is obtained based on the action nodes output in the previous stage. In this paper, the UPFC demonstration project of Nanjing West Ring Network is taken as a case study to validate the proposed method. The results show that the proposed method can fully meet the real-time security correction time requirements of power grids, and avoid the inherent defects of the traditional model method without an iterative solution and can also provide reasonable security correction strategies for N-1 and N-2 faults.}
}
@article{MORA2020140,
title = {A collaborative working model for enhancing the learning process of science & engineering students},
journal = {Computers in Human Behavior},
volume = {103},
pages = {140-150},
year = {2020},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2019.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S074756321930336X},
author = {Higinio Mora and María Teresa Signes-Pont and Andrés Fuster-Guilló and María L. Pertegal-Felices},
keywords = {Higher education, ICT learning technologies, Quality assessments, Computers in human behavior, Collaborative learning, Student experiences, Collective intelligence},
abstract = {Science and engineering education are mostly based on content assimilation and development of skills. However, to adequately prepare students for today's world, it is also necessary to stimulate critical thinking and make them reflect on how to improve current practices using new tools and technologies. In this line, the main motivation of this research consists in exploring ways supported by technology to enhance the learning process of students and to better prepare them to face the challenges of today's world. To this end, the purpose of this work is to design an innovative learning project based on collaborative work among students, and research its impact in achieving better learning outcomes, generating of collective intelligence and further motivation. The proposed collaborative working model is based on peer review assessment methodology implemented through a learning web-platform. Thus, students were encouraged to peer review their classmates' works. They had to make comments, suggest improvements, and assess final assignments. Teaching staff managed and supervised the whole process. Students were selected from computer science engineering at the University of Alicante (Spain). Results suggested greater content assimilation and enhanced learning in several scientific skills. The students' final grade exceeded what any student could produce individually, but we cannot conclude that real collective intelligence was generated. Learning methodologies based on the possibilities of Information and Communication Technologies (ICT) provide new ways to transmit and manage knowledge in higher education. Collaborating in peer assessment enhances the students' motivation and promotes the active learning. In addition, this method can be very helpful and time saving for instructors in the management of large groups.}
}
@article{CHEN1998541,
title = {Toward a better understanding of idea processors},
journal = {Information and Software Technology},
volume = {40},
number = {10},
pages = {541-553},
year = {1998},
issn = {0950-5849},
doi = {https://doi.org/10.1016/S0950-5849(98)00080-9},
url = {https://www.sciencedirect.com/science/article/pii/S0950584998000809},
author = {Z. Chen},
keywords = {Artificial intelligence, Brainstorming, Computational creativity, Idea processors, Creativity support systems},
abstract = {Idea processors, as a kind of software widely used in the business world, have not received much attention from academia. In this paper we provide an overview of the current status of idea processors. We start from the foundations of idea processors, pointing out their roots in brainstorming techniques. By examining several experimental systems and commercial products, we further discuss how idea processors work, their nature, and their typical architecture. We also summarize some research work related to idea processors, as well as relationships between idea processors and studies of computational creativity in artificial intelligence. Other related issues, such as group decision support systems and evaluation methods, are also briefly examined.}
}
@incollection{CAULLER1992199,
title = {Chapter 8 - Functions of Very Distal Dendrites: Experimental and Computational Studies of Layer I Synapses on Neocortical Pyramidal Cells},
editor = {THOMAS MCKENNA and JOEL DAVIS and STEVEN F. ZORNETZER},
booktitle = {Single Neuron Computation},
publisher = {Academic Press},
address = {San Diego},
pages = {199-229},
year = {1992},
series = {Neural Networks: Foundations to Applications},
isbn = {978-0-12-484815-3},
doi = {https://doi.org/10.1016/B978-0-12-484815-3.50014-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780124848153500141},
author = {LARRY J. CAULLER and BARRY W. CONNORS},
abstract = {Publisher Summary
This chapter reviews an approach that combines quantitative morphology, physiology, and computational analysis to understandthe functions of a complex synaptic–neuronal interaction in the cortex. A variation of the in vitro slice of rat somatosensory neocortex examines the effectiveness of layer 1 inputs to pyramidal cells whose bodies lie 0.5–1 mm deeper, in layers 3 or 5. The horizontal fibers in layer 1 (HL1) were isolated by disconnecting all deeper horizontal fibers with a cut perpendicular to the surface, extending from just below layer 1 downward through subcortical white matter and Layer 1 was stimulated on one side of the cut and the response mediated by HL1 fibers passing to the other side was recorded extracellularly and intracellularly. Backward cortico–cortical projections, which end largely on distal apical dendrites in layer 1, are important for higher cortical functions. By isolating horizontal afferents to layer 1 in an in vitro neocortical slice, layer 1 synapses can strongly excite pyramidal cells as deep as layer 5b.}
}
@article{BAJAJ2021117750,
title = {Association between emotional intelligence and effective brain connectome: A large-scale spectral DCM study},
journal = {NeuroImage},
volume = {229},
pages = {117750},
year = {2021},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2021.117750},
url = {https://www.sciencedirect.com/science/article/pii/S1053811921000276},
author = {Sahil Bajaj and William D.S. Killgore},
abstract = {Introduction
Emotional Intelligence (EI) is a well-documented aspect of social and interpersonal functioning, but the underlying neural mechanisms for this capacity remain poorly understood. Here we used advanced brain connectivity techniques to explore the associations between EI and effective connectivity (EC) within four functional brain networks.
Methods
The Mayer-Salovey-Caruso Emotional Intelligence Test (MSCEIT) was used to collect EI data from 55 healthy individuals (mean age = 30.56±8.3 years, 26 males). The MSCEIT comprises two area cores – experiential EI (T1) and strategic EI (T2). The T1 core included two sub-scales – perception of emotions (S1) and using emotions to facilitate thinking (S2), and the T2 core included two sub-scales – understanding of emotions (S3) and management of emotions (S4). All participants underwent structural and resting-state functional magnetic resonance imaging (rsfMRI) scans. The spectral dynamic causal modeling approach was implemented to estimate EC within four networks of interest – the default-mode network (DMN), dorsal attention network (DAN), control-execution network (CEN) and salience network (SN). The strength of EC within each network was correlated with the measures of EI, with correlations at pFDR < 0.05 considered as significant.
Results
There was no significant association between any of the measures of EI and EC strength within the DMN and DAN. For CEN, however, we found that there were significant negative associations between EC strength from the right anterior prefrontal cortex (RAPFC) to the left anterior prefrontal cortex (LAPFC) and both S2 and T1, and significant positive associations between EC strength from LAPFC to RAPFC and S2. EC strength from the right superior parietal cortex (SPC) to RAPFC also showed significant negative association with S4 and T2. For the SN, S3 showed significant negative association with EC strength from the right insula to RAPFC and significant positive association with EC strength from the left insula to dorsal anterior cingulate cortex (DACC).
Conclusions
We provide evidence that the negative ECs within the right hemisphere, and from the right to left hemisphere, and positive ECs within the left hemisphere and from the left to right hemisphere of CEN (involving bilateral frontal and right parietal region) and SN (involving right frontal, anterior cingulate and bilateral insula) play a significant role in regulating and processing emotions. These findings also suggest that measures of EC can be utilized as important biomarkers to better understand the underlying neural mechanisms of EI.}
}
@article{IGAMBERDIEV201715,
title = {The quantum basis of spatiotemporality in perception and consciousness},
journal = {Progress in Biophysics and Molecular Biology},
volume = {130},
pages = {15-25},
year = {2017},
note = {Quantum information models in biology: from molecular biology to cognition},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2017.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0079610716301687},
author = {Abir U. Igamberdiev and Nikita E. Shklovskiy-Kordi},
keywords = {Cytoskeleton, Molecular computation, Neuron, Quantum measurement, Spatiotemporality},
abstract = {Living systems inhabit the area of the world which is shaped by the predictable space-time of physical objects and forces that can be incorporated into their perception pattern. The process of selecting a “habitable” space-time is the internal quantum measurement in which living systems become embedded into the environment that supports their living state. This means that living organisms choose a coordinate system in which the influence of measurement is minimal. We discuss specific roles of biological macromolecules, in particular of the cytoskeleton, in shaping perception patterns formed in the internal measurement process. Operation of neuron is based on the transmission of signals via cytoskeleton where the digital output is generated that can be decoded through a reflective action of the perceiving agent. It is concluded that the principle of optimality in biology as formulated by Liberman et al. (BioSystems 22, 135–154, 1989) is related to the establishment of spatiotemporal patterns that are maximally predictable and can hold the living state for a prolonged time. This is achieved by the selection of a habitable space approximated to the conditions described by classical physics.}
}
@article{LI20221,
title = {Computing for Chinese Cultural Heritage},
journal = {Visual Informatics},
volume = {6},
number = {1},
pages = {1-13},
year = {2022},
issn = {2468-502X},
doi = {https://doi.org/10.1016/j.visinf.2021.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S2468502X21000644},
author = {Meng Li and Yun Wang and Ying-Qing Xu},
keywords = {Cultural computing, Chinese Cultural Heritage, Computable cultural ecosystem, Mogao caves, Guqin},
abstract = {Implementing computational methods for preservation, inheritance, and promotion of Cultural Heritage (CH) has become a research trend across the world since the 1990s. In China, generations of scholars have dedicated themselves to studying the country’s rich CH resources; there are great potential and opportunities in the field of computational research on specific cultural artefacts or artforms. Based on previous works, this paper proposes a systematic framework for Chinese Cultural Heritage Computing that consists of three conceptual levels which are Chinese CH protection and development strategy, computing process, and computable cultural ecosystem. The computing process includes three modules: (1) data acquisition and processing, (2) digital modeling and database construction, and (3) data application and promotion. The modules demonstrate the computing approaches corresponding to different phases of Chinese CH protection and development, from digital preservation and inheritance to presentation and promotion. The computing results can become the basis for the generation of cultural genes and eventually the formation of computable cultural ecosystem Case studies on the Mogao caves in Dunhuang and the art of Guqin, recognized as world’s important tangible and intangible cultural heritage, are carried out to elaborate the computing process and methods within the framework. With continuous advances in data collection, processing, and display technologies, the framework can provide constructive reference for building up future research roadmaps in Chinese CH computing and related fields, for sustainable protection and development of Chinese CH in the digital age.}
}
@article{BOWER2024455,
title = {Model-Based Analysis of Pathway Recruitment During Subthalamic Deep Brain Stimulation},
journal = {Neuromodulation: Technology at the Neural Interface},
volume = {27},
number = {3},
pages = {455-463},
year = {2024},
issn = {1094-7159},
doi = {https://doi.org/10.1016/j.neurom.2023.02.084},
url = {https://www.sciencedirect.com/science/article/pii/S109471592300140X},
author = {Kelsey L. Bower and Angela M. Noecker and Anneke M. Frankemolle-Gilbert and Cameron C. McIntyre},
keywords = {Axons, electrode, Parkinson’s disease, subthalamic nucleus},
abstract = {Background
Subthalamic deep brain stimulation (DBS) is an established clinical therapy, but an anatomically clear definition of the underlying neural target(s) of the stimulation remains elusive. Patient-specific models of DBS are commonly used tools in the search for stimulation targets, and recent iterations of those models are focused on characterizing the brain connections that are activated by DBS.
Objective
The goal of this study was to quantify axonal pathway activation in the subthalamic region from DBS at different electrode locations and stimulation settings.
Materials and Methods
We used an anatomically and electrically detailed computational model of subthalamic DBS to generate recruitment curves for eight different axonal pathways of interest, at three generalized DBS electrode locations in the subthalamic nucleus (STN) (ie, central STN, dorsal STN, posterior STN). These simulations were performed with three levels of DBS electrode localization uncertainty (ie, 0.5 mm, 1.0 mm, 1.5 mm).
Results
The recruitment curves highlight the diversity of pathways that are theoretically activated with subthalamic DBS, in addition to the dependence of the stimulation location and parameter settings on the pathway activation estimates. The three generalized DBS locations exhibited distinct pathway recruitment curve profiles, suggesting that each stimulation location would have a different effect on network activity patterns. We also found that the use of anodic stimuli could help limit activation of the internal capsule relative to other pathways. However, incorporating realistic levels of DBS electrode localization uncertainty in the models substantially limits their predictive capabilities.
Conclusions
Subtle differences in stimulation location and/or parameter settings can impact the collection of pathways that are activated during subthalamic DBS.}
}
@article{ZHANG2019215,
title = {Food-energy-water (FEW) nexus for urban sustainability: A comprehensive review},
journal = {Resources, Conservation and Recycling},
volume = {142},
pages = {215-224},
year = {2019},
issn = {0921-3449},
doi = {https://doi.org/10.1016/j.resconrec.2018.11.018},
url = {https://www.sciencedirect.com/science/article/pii/S0921344918304361},
author = {Pengpeng Zhang and Lixiao Zhang and Yuan Chang and Ming Xu and Yan Hao and Sai Liang and Gengyuan Liu and Zhifeng Yang and Can Wang},
keywords = {Urban system, Food-energy-water nexus, Conceptual framework, Resilience},
abstract = {The emerging popularity of the nexus discussion reflects the ongoing transition from a sectoral or silo approach to an integrative approach to address the global challenges pertinent to the three essential resources: food, energy, and water (FEW). Cities are critically important for advancing regional sustainable development and are thus placed at the center of the FEW nexus. This paper provides a comprehensive literature review to debate the current concepts and methods of the FEW nexus at different scales, with the aim of developing a conceptual knowledgebase framework for scientific analysis and policy making associated with the urban FEW nexus. Although the concept of nexus thinking has been widely accepted, a consistent and explicit cognition of the FEW nexus is still lacking, and a sophisticated methodological modeling framework is urgently required at various scales. As such, we proposed a three-dimensional conceptual framework of the urban FEW nexus from the perspective of resource interdependency, resource provision and system integration. This framework is useful in steering the systematic modeling and integrative management of the complex nexus issues of urban systems with different perspectives. Finally, the future directions of urban nexus research are identified from four aspects, including systematic characterization, cross-region tele-connection mechanisms, co-decision model development, and governance transition.}
}
@article{BAKHTAVAR2020122886,
title = {Assessment of renewable energy-based strategies for net-zero energy communities: A planning model using multi-objective goal programming},
journal = {Journal of Cleaner Production},
volume = {272},
pages = {122886},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.122886},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620329310},
author = {Ezzeddin Bakhtavar and Tharindu Prabatha and Hirushie Karunathilake and Rehan Sadiq and Kasun Hewage},
keywords = {Community energy planning, Renewable energy, Life cycle assessment, Multi-objective optimi, z, ation, Grey numbers, Goal programming},
abstract = {Planning decentralised community-level hybrid energy systems has emerged as a solution to the various environmental and economic issues associated with conventional centralised energy supply systems. However, the optimal planning of community energy systems is a challenging issue due to the complexities, uncertainties, conflicting objectives, and high computational times in analysis. This study introduces a new multi-objective model based on weighted goal programming and grey pairwise comparison to assess renewable energy-based strategies in the case of net-zero energy communities. The problem was formulated to determine the optimal energy mix based on minimization of life cycle impacts and costs and maximization of renewable contributions and operational energy savings. To this end, binary integer and continues variables were applied on the code developed in a CPLEX environment. A pairwise comparison based on grey numbers was used to find the impacts of the goals in the objective function of the model under uncertainty. In addition to the grey-based weighting scenario, different weighting scenarios were employed to consider the importance of all goals on the system. These weighting scenarios were used to investigate the effects of changing decision priorities on the outcomes and on stakeholder interests at different levels. The developed goal-programming model was applied to the data of a case example and solved based on the weighting scenarios. Results indicated that the model is capable of finding the best possible strategies with the lowest total undesirable deviations from the desired levels of the goals compared to the literature of the decision-making techniques. The integration of maximum renewable energy (RE) supply in the energy mix with the locally available energy resources can deliver considerable benefits in terms of energy supply cost reduction as well as in mitigating life cycle environmental impacts. When environmental goals are prioritized, integrating low emissions RE as much as possible and excluding waste-to-energy technologies makes best sense, while under a pro-economic perspective, solar integration is comparatively discouraged. The findings of the study are expected to assist community developers and other decision makers involved in regional energy planning. The developed method will also be of use for those who are interested in the use of goal programming to solve complex planning issues involving numerous uncertain parameters.}
}
@article{PITTNAUER2023382,
title = {Observing the creation of new knowledge in the economics laboratory—Do participants discover how to learn from outcome feedback in a dynamic decision problem?},
journal = {Journal of Economic Behavior & Organization},
volume = {215},
pages = {382-405},
year = {2023},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2023.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167268123003311},
author = {Sabine Pittnauer and Martin Hohnisch},
keywords = {Learning, Outcome feedback, Discovery, Conjecture, Heuristic simplification, Dynamic decision making},
abstract = {Domain-general learning rules often enable decision makers to learn from outcome feedback which actions tend to achieve a desired goal. However, in novel and complex environments decision makers must explore how to learn, i.e., acquire procedural knowledge of how to elicit and evaluate outcome feedback that will enable them to navigate toward a desired goal despite the vastness of the set of possible policies. Using a dynamic business simulation, this study investigated: (1) whether and how frequently participants discovered an effective procedure to learn from outcome feedback that allowed them to navigate toward a policy that maximizes long-term business profit (and hence their monetary payoff from the experiment), and (2) whether high monetary incentives affected learning procedures and performance. We found that a number of participants discovered an effective learning procedure and succeeded in approximating the optimal policy. In line with the heuristic method, this learning procedure involved a simplification of the search space and the application of domain-general learning rules to this simplified space. Although the decision histories of about half of the participants feature the key aspect of the effective learning procedure—search among the different steady states of the dynamical system—implementation errors prevented many of the participants from realizing the full potential of the learning procedure. We found no evidence to suggest that high monetary incentives affect the effectiveness of learning. Overall, the study illustrates that a “prepared mind” can discover new, effective learning procedures, although their initial implementation may require substantial refinement.}
}
@article{ABRAMSKY200637,
title = {What are the Fundamental Structures of Concurrency?: We still don't know!},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {162},
pages = {37-41},
year = {2006},
note = {Proceedings of the Workshop "Essays on Algebraic Process Calculi" (APC 25)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2005.12.075},
url = {https://www.sciencedirect.com/science/article/pii/S1571066106004105},
author = {Samson Abramsky},
keywords = {Concurrency, process algebra, Petri nets, geometry, quantum information and computation},
abstract = {Process algebra has been successful in many ways; but we don't yet see the lineaments of a fundamental theory. Some fleeting glimpses are sought from Petri Nets, physics and geometry.}
}
@article{LIU2024100744,
title = {Application of entertainment E-learning mode based on Apriori algorithm in intelligent English reading assistance mode},
journal = {Entertainment Computing},
volume = {51},
pages = {100744},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100744},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124001125},
author = {Shanshan Liu},
keywords = {Apriori algorithm, Entertainment E-learning model, Intelligent English teaching, Auxiliary reading mode},
abstract = {With the assistance of digital media, entertainment oriented E-learning models can effectively enhance students’ learning enthusiasm. This article analyzes the application of entertainment E-learning mode based on Apriori algorithm in intelligent English reading assistance mode. At present, English reading teaching faces some problems, such as outdated teaching methods, passive learning among students, excessive emphasis on imparting grammar knowledge while neglecting the improvement of students’ reading skills and strategies, and so on. Therefore, this article conducts research on intelligent English reading comprehension tools based on semantic analysis and Apriori algorithm. This paper proposes a recreational E-learning model based on Apriori algorithm. Based on Apriori algorithm, students’ interests and preferences on different learning resources and entertainment elements are mined and incorporated into the learning model design. Then, a set of entertaining English reading assistant model is designed, which uses a variety of entertainment elements, such as gamified learning, interactive activities and reward mechanism, to increase students’ learning participation and enthusiasm. This article adopts the idea of LSA algorithm to construct a BERT semantic analysis model. We treat nodes in the network as word items and then use singular value decomposition algorithm to decompose the word document matrix. Secondly, the original association rule Apriori algorithm was optimized, and the optimized association rule Apriori algorithm effectively solved the problem of excessive computation in traditional algorithms. Finally, based on semantic analysis and Apriori algorithm, this article designs an intelligent English reading comprehension tool, mainly analyzing the practical application of the system and greatly improving the efficiency of English reading teaching.}
}
@incollection{KADAR201425,
title = {2 - Developing a personalized and adapted curriculum for engineering education through an ambient intelligence environment},
editor = {J. {Paulo Davim}},
booktitle = {Engineering Education},
publisher = {Chandos Publishing},
address = {Oxford},
pages = {25-65},
year = {2014},
isbn = {978-1-84334-687-6},
doi = {https://doi.org/10.1533/9781780633589.25},
url = {https://www.sciencedirect.com/science/article/pii/B978184334687650002X},
author = {M. Kadar and M. Muntean and L. Marina},
keywords = {engineering education, ambient intelligence environment, brain lateralization system, adapted and personalized curriculum},
abstract = {Abstract:
This chapter describes a research model that enables students to become all that they are capable of becoming, and educators and decision makers to maximize their efforts in the field of engineering education through an ambient intelligence environment. This research proposes to translate conceptual ideas for the functionality of the environment and appliances into concrete designs. The core of the intelligent educational environment is an information system called the brain lateralization information system (BLIS). The BLIS can provide valuable information on users’ brain lateralization and students’ thinking style. Such information can be used by educators in designing new teaching methodologies that will finally lead to adapted, personalized study programs within the university curricula. The chapter shows how this approach, which has hitherto been applied to students, teaching staff and management staff from the departments of Computer Science, Applied Electronics, and Environmental Engineering of the University of Alba Iulia, was validated to allow future development of methodologies, strategies, and operational programs in the field of engineering education. In order to achieve this vision the chapter introduces a number of novel concepts and a model, in particular a new brain lateralization information system embedded into an ambient intelligent environment. Finally, the chapter reports on conclusions, recommendations and examples of adapted and personalized courseware designed for blended learning, and a user evaluation of this model, which demonstrates that users find the ambient intelligence environment useful for their career choice and easy and enjoyable to use for teachers and decision makers.}
}
@article{ALEMANY2018429,
title = {Effects of binary variables in mixed integer linear programming based unit commitment in large-scale electricity markets},
journal = {Electric Power Systems Research},
volume = {160},
pages = {429-438},
year = {2018},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2018.03.019},
url = {https://www.sciencedirect.com/science/article/pii/S0378779618300919},
author = {Juan Alemany and Leszek Kasprzyk and Fernando Magnago},
keywords = {Binary variables relaxation, Branch and cut algorithm, Day-ahead electricity market clearing, Mixed integer linear programming, Unit commitment},
abstract = {Mixed integer linear programming is one of the main approaches used to solve unit commitment problems. Due to the computational complexity of unit commitment problems, several researches remark the benefits of using less binary variables or relaxing them for the branch-and-cut algorithm. However, integrality constraints relaxation seems to be case dependent because there are many instances where applying it may not improve the computational burden. In addition, there is a lack of extensive numerical experiments evaluating the effects of the relaxation of binary variables in mixed integer linear programming based unit commitment. Therefore, the primary purpose of this work is to analyze the effects of binary variables and compare different relaxations, supported by extensive computational experiments. To accomplish this objective, two power systems are used for the numerical tests: the IEEE118 test system and a very large scale real system. The results suggest that a direct link between the relaxation of binary variables and computational burden cannot be easily assured in the general case. Therefore, relaxing binary variables should not be used as a general rule-of-practice to improve computational burden, at least, until each particular model is tested under different load scenarios and formulations to quantify the final effects of binary variables on the specific UC implementation. The secondary aim of this work is to give some preliminary insight into the reasons that could be supporting the binary relaxation in some UC instances.}
}
@article{GRABOWSKI201921,
title = {A Primer on Data Analytics in Functional Genomics: How to Move from Data to Insight?},
journal = {Trends in Biochemical Sciences},
volume = {44},
number = {1},
pages = {21-32},
year = {2019},
issn = {0968-0004},
doi = {https://doi.org/10.1016/j.tibs.2018.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S0968000418302263},
author = {Piotr Grabowski and Juri Rappsilber},
keywords = {data integration, data science, functional genomics, machine learning, systems biology},
abstract = {High-throughput methodologies and machine learning have been central in developing systems-level perspectives in molecular biology. Unfortunately, performing such integrative analyses has traditionally been reserved for bioinformaticians. This is now changing with the appearance of resources to help bench-side biologists become skilled at computational data analysis and handling large omics data sets. Here, we show an entry route into the field of omics data analytics. We provide information about easily accessible data sources and suggest some first steps for aspiring computational data analysts. Moreover, we highlight how machine learning is transforming the field and how it can help make sense of biological data. Finally, we suggest good starting points for self-learning and hope to convince readers that computational data analysis and programming are not intimidating.}
}
@article{VELICHKOVSKY2018227,
title = {Consciousness in a multilevel architecture: Evidence from the right side of the brain},
journal = {Consciousness and Cognition},
volume = {64},
pages = {227-239},
year = {2018},
note = {Visual Experience and Guidance of Action: A Tribute to Bruce Bridgeman},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2018.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1053810018300412},
author = {Boris M. Velichkovsky and Olga A. Krotkova and Artemy A. Kotov and Vyacheslav A. Orlov and Vitaly M. Verkhlyutov and Vadim L. Ushakov and Maxim G. Sharaev},
keywords = {Consciousness, Dynamic Causal Modeling (DCM), Resting state, Lateralization, Frontopolar cortex, Hippocampal formation, Ventrolateral prefrontal-amygdala emotional pathway, Egocentric spatial representation, Self-referential cognition, Levels of cognitive organization},
abstract = {By taking into account Bruce Bridgeman's interest in an evolutionary framing of human cognition, we examine effective (cause-and-effect) connectivity among cortical structures related to different parts of the triune phylogenetic stratification: archicortex, paleocortex and neocortex. Using resting-state functional magnetic resonance imaging data from 25 healthy subjects and spectral Dynamic Causal Modeling, we report interactions among 10 symmetrical left and right brain areas. Our results testify to general rightward and top-down biases in excitatory interactions of these structures during resting state, when self-related contemplation prevails over more objectified conceptual thinking. The right hippocampus is the only structure that shows bottom-up excitatory influences extending to the frontopolar cortex. The right ventrolateral cortex also plays a prominent role as it interacts with the majority of nodes within and between evolutionary distinct brain subdivisions. These results suggest the existence of several levels of cognitive-affective organization in the human brain and their profound lateralization.}
}
@article{URBINA2022100031,
title = {The commoditization of AI for molecule design},
journal = {Artificial Intelligence in the Life Sciences},
volume = {2},
pages = {100031},
year = {2022},
issn = {2667-3185},
doi = {https://doi.org/10.1016/j.ailsci.2022.100031},
url = {https://www.sciencedirect.com/science/article/pii/S2667318522000022},
author = {Fabio Urbina and Sean Ekins},
keywords = {Artificial intelligence, Design-make-test, Machine learning, Molecule design, Recurrent neural networks},
abstract = {Anyone involved in designing or finding molecules in the life sciences over the past few years has witnessed a dramatic change in how we now work due to the COVID-19 pandemic. Computational technologies like artificial intelligence (AI) seemed to become ubiquitous in 2020 and have been increasingly applied as scientists worked from home and were separated from the laboratory and their colleagues. This shift may be more permanent as the future of molecule design across different industries will increasingly require machine learning models for design and optimization of molecules as they become “designed by AI”. AI and machine learning has essentially become a commodity within the pharmaceutical industry. This perspective will briefly describe our personal opinions of how machine learning has evolved and is being applied to model different molecule properties that crosses industries in their utility and ultimately suggests the potential for tight integration of AI into equipment and automated experimental pipelines. It will also describe how many groups have implemented generative models covering different architectures, for de novo design of molecules. We also highlight some of the companies at the forefront of using AI to demonstrate how machine learning has impacted and influenced our work. Finally, we will peer into the future and suggest some of the areas that represent the most interesting technologies that may shape the future of molecule design, highlighting how we can help increase the efficiency of the design-make-test cycle which is currently a major focus across industries.}
}
@incollection{MEY200651,
title = {Pragmatics: Overview},
editor = {Keith Brown},
booktitle = {Encyclopedia of Language & Linguistics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {51-62},
year = {2006},
isbn = {978-0-08-044854-1},
doi = {https://doi.org/10.1016/B0-08-044854-2/00306-0},
url = {https://www.sciencedirect.com/science/article/pii/B0080448542003060},
author = {J.L. Mey},
keywords = {affordance, assimilation, bestrangement, common scene, communication, community of, context, conversational maxim, cooperative principle, cotext, deixis, felicity condition, flouting, ghettoization, illocutionary force, implicature, indexical, indexing, indirect speech act, interact, institutional, placement, mandate, misunderstanding, pragmatic acts, presuppositions, relevance, semantics, situated speech, situation, placement, situational condition, social, social environment, social practice, societal stance, speech act, syntax},
abstract = {Pragmatics is the study of human language use as it is exercised in a community of social practice. The exercise is, however, not limited to verbal signs: other communicative means are also included under the definition (e.g., as becomes clear when one studies pragmatic acts in addition to the traditional speech acts). In addition, emphasis is placed on the way communication is organized, first of all in the classical models put forward by Austin, Searle, and Grice, but extending also this approach to comprise more recent advances in pragmatic thinking, especially in relation to what used to be called the hyphenated disciplines: sociolinguistics, psycholinguistics, the study of humans in interaction with computers, the teaching of first and second languages, and a host of other practically and theoretically oriented fields of study.}
}
@article{LIU2024122986,
title = {Tackling fuel poverty and decarbonisation in a distributed heating system through a three-layer whole system approach},
journal = {Applied Energy},
volume = {362},
pages = {122986},
year = {2024},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2024.122986},
url = {https://www.sciencedirect.com/science/article/pii/S0306261924003696},
author = {Xinyao Liu and Floris Bierkens and Ishanki {De Mel} and Matthew Leach and Michael Short and Mona Chitnis and Boyue Zheng and Lirong Liu},
keywords = {Residential heating decarbonisation, Fuel poverty, Cambridge housing model, Mixed-integer linear programming, Input-output-simulation},
abstract = {Residential heating displays huge decarbonisation potential towards Net-Zero. The complexity of heating system and socio-economic system appeals for a systematic design to avoid exacerbating fuel poverty. This study develops a three-layer heat-for-all model which integrates building stocks analysis, distributed heating system optimisation, economic and environmental impacts simulation to tackle heating decarbonisation and fuel poverty simultaneously. This whole system model is a powerful decision support tool that can help conceive heating decarbonisation strategies for wider regions and countries. More than 400,000 scenarios are created, considering the effects of future policy schemes (No Grant, Business as Usual, Proposed), minimum emission reduction target, carbon intensity of grid, future natural gas, and electricity prices. Results show that optimised heating system decarbonisation plan heavily relies on future energy prices. In the case study, only air source heat pumps are chosen when electricity price is lower than 3 times gas price. Secondly, investment in heating system could stimulate the greenhouse gas emission of whole supply chain, hedging the emission reduction achieved in heating system. This further reveals that life cycle thinking is imperative in GHG emission mitigation. Thirdly, electricity decarbonisation plays a vital role in achieving whole system emission reduction. The grid carbon intensity reduction makes substantial contribution to the emission reduction of heating system and industry system. In tackling fuel poverty, it's worth noticing that the fuel poverty is aggravated with more grant support under certain scenarios, since current policy schemes focus on capital investment in heating system but overlook the increased energy bills. It appeals for a more comprehensive policy design considering all stakeholders.}
}
@article{ALMOJEL2000297,
title = {The implementation and performance evaluation of N-body gravitational simulation algorithm on high-performance computers},
journal = {Computers & Electrical Engineering},
volume = {26},
number = {3},
pages = {297-316},
year = {2000},
issn = {0045-7906},
doi = {https://doi.org/10.1016/S0045-7906(99)00048-8},
url = {https://www.sciencedirect.com/science/article/pii/S0045790699000488},
author = {Abdullah I. Almojel},
keywords = {Astrophysics simulations, Dynamic load balancing, Performance analysis, MIMD machines, Barnes–Hut algorithm},
abstract = {Gravity determines the dynamical evaluation of many astrophysical systems. Each of these systems can be described as N gravitationally interacting particles. Solutions of the resulting N-body problem by direct simulation entails the calculation of O(N2) forces at each time step. To increase the number of particles that can be simulated, approximate algorithms have been developed and implemented in so-called tree codes. In these algorithms the particles are stored into a spatial hierarchy that forms a tree data structure. For a fixed level of accuracy the complexity of this algorithm is O(Nlog(N)). In this paper, a “manager–worker” model for a parallel implementation of hierarchical N-body algorithm is introduced. We describe a load-balanced, efficient algorithm for solving the Astrophysics simulation of N-body problem using tree-based data structures and massively parallel computing architectures. This algorithm, based on the Barnes–Hut method, first assembles a tree data structure that represents the distribution of bodies, or particles, at all length-scales. An adaptive load balancing technique is used to assign bodies to processors as well as to insure that processors are assigned equal amounts of work. A number of performance measurements were carried out in order to reveal the behavior of the N-body application with respect to a partitioning technique and load imbalance overhead. We also show that, with using the introduced manager–worker model and the cost zones domain decomposition technique, the algorithm is load balanced and that the majority of the time of the algorithm is spent in performing on-processor functions and not in inter-processor communications. We have conducted our study on several high-performance MIMD supercomputer machines such as the 256-processor Cray T3D and the 64-processor Intel Paragon at NASA/JPL and on the 32-processor Thinking Machines CM-5 at UMC.}
}
@article{NAKHAEI2022116422,
title = {A novel framework for technical performance evaluation of water distribution networks based on the water-energy nexus concept},
journal = {Energy Conversion and Management},
volume = {273},
pages = {116422},
year = {2022},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2022.116422},
url = {https://www.sciencedirect.com/science/article/pii/S0196890422012006},
author = {Mahdi Nakhaei and Mehran Akrami and Mohammad Gheibi and Pedro {Daniel Urbina Coronado} and Mostafa Hajiaghaei-Keshteli and Jürgen Mahlknecht},
keywords = {Water distribution network, Water energy nexus, EPANET, Design of experiments, Machine learning},
abstract = {Today energy recovery using Micro-Hydropowers (MHPs) in Water Distribution Networks (WDN) is a well-known approach for recycling the wasted energy in infrastructures as a sample of circular economy. Likewise, in this study for the first time a framework for evaluation of WDN for energy harvesting have been designed with the application of statistical optimization, simulation, and artificial intelligence concepts. In this study, after modelling a WDN in Mashhad, Iran, with Environmental Protection Agency Network Evaluation Tool (EPANET) software, the potential of energy recovery using MHP technology was optimized with the application of Design of Experiment (DOE) methods, including Taguchi and Response Surface Methodology (RSM) and then the model prediction ability was improved by Artificial Neural Network (ANN) technique. Results of this investigation revealed that the combination of Taguchi and RSM methods could successfully optimize the energy recovery potential with consideration of improving the hydraulic parameters of WDN. With the application of RSM and Taguchi, high potential positions for MHP placement are detected and analyzed based on a high-performance operational decision-making methodology. According to Artificial Intelligence (AI) computations, energy harvesting and hydraulic responses can be estimated with more than a 99 % correlation coefficient. Also, it shows that the soft-operator can be executed to control the features of MHPs in WDNs. The outputs of this research demonstrated that MHP harvested energy is more than 400KW for the run time of this study with consideration of hydraulic parameters.}
}
@article{HART2023182,
title = {Riders on a storm – The evaluation and control of creative processes A comment on: “A systematic framework of creative metacognition” by Izabela Lebuda and Mathias Benedek},
journal = {Physics of Life Reviews},
volume = {47},
pages = {182-183},
year = {2023},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2023.10.029},
url = {https://www.sciencedirect.com/science/article/pii/S1571064523001689},
author = {Yuval Hart}
}
@article{DEMPSTER1990261,
title = {Causality and statistics},
journal = {Journal of Statistical Planning and Inference},
volume = {25},
number = {3},
pages = {261-278},
year = {1990},
issn = {0378-3758},
doi = {https://doi.org/10.1016/0378-3758(90)90076-7},
url = {https://www.sciencedirect.com/science/article/pii/0378375890900767},
author = {A.P. Dempster},
keywords = {Causal inference, causal effects, subjectivity, objectivity, randomization, experimentation.},
abstract = {Many aspects of statistical design, modelling, and inference have close and important connections with causal thinking. These are analyzed in the paper against a philosophical background that regards formal mathematical models as having dual interpretations, reflecting both objectivist reality and subjectivist rationality. The latter aspect weakens the need for an objective theory of probabilistic causation, and suggests that a traditional image of causes as deterministic mechanisms should remain primary. It is argued that such causes should guide much preformal thinking about what to include in formal statistical models, especially of dynamic phenomena. The statistical measurement of causal effects is facilitated by good statistical design, including randomization where feasible, and requires other methodologies for controlling and assessing uncertainties, for example in model construction and inference. Illustrative examples include case studies where the problem is to assess retrospectively the causes of observed events and where the task is to assess future risks from controllable factors.}
}