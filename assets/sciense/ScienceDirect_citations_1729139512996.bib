@incollection{WANG20249,
title = {1.02 - Artificial Intelligence and Bioinformatics Applications in Precision Medicine and Future Implications},
editor = {Kenneth S. Ramos},
booktitle = {Comprehensive Precision Medicine (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {9-24},
year = {2024},
isbn = {978-0-12-824256-8},
doi = {https://doi.org/10.1016/B978-0-12-824010-6.00058-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128240106000587},
author = {Ni Wang and Qiang He},
keywords = {Artificial intelligence, Bioinformatics, Clinical trials, Disease risk assessment, Drug discovery, Early disease detection, Genome sequencing, Oncology, Personalized medicine, Pharmacogenomics, Precision medicine},
abstract = {Artificial intelligence (AI) and bioinformatics have emerged as key technologies for advancing precision medicine. Many tools of AI and bioinformatics have been applied in healthcare that would be of great value to advance the goals of precision medicine. AI is a branch of computer science that deals with the automation of intelligent behavior. Bioinformatics is a field of study that combines biology, computer science, and statistics to analyze and interpret biologic data. The latter involves the development and application of computational methods and tools for storing, organizing, analyzing, and interpreting biologic information, including genomic, proteomic, and metabolomic data. AI and bioinformatics are well-positioned to help tailor medical decisions and treatments at the individual and population levels. Some of those applications and the multidisciplinary implications are presented in this chapter.}
}
@article{GOUTAUDIER2021113755,
title = {Proper Generalized Decomposition with time adaptive space separation for transient wave propagation problems in separable domains},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {380},
pages = {113755},
year = {2021},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2021.113755},
url = {https://www.sciencedirect.com/science/article/pii/S0045782521000918},
author = {Dimitri Goutaudier and Laurent Berthe and Francisco Chinesta},
keywords = {Proper Generalized Decomposition (PGD), Transient wave propagation, Time adaptive space separation, Separable domain, Scalar wave equation, Elastodynamics},
abstract = {Transient wave propagation problems may involve rich discretizations, both in space and in time, leading to computationally expensive simulations, even for simple spatial domains. The Proper Generalized Decomposition (PGD) is an attractive model order reduction technique to address this issue, especially when the spatial domain is separable. In this work, we propose a space separation with a time adaptive number of modes to efficiently capture transient wave propagation in separable domains. We combine standard time integration schemes with this original space separated representation for empowering standard procedures. The numerical behavior of the proposed method is explored through several 2D wave propagation problems involving radial waves, propagation on long time analyses, and wave conversions. We show that the PGD solution approximates its standard finite element solution counterpart with acceptable accuracy, while reducing the storage needs and the computation time (CPU time). Numerical results show that the CPU time per time step linearly increases when refining the mesh, even with implicit time integration schemes, which is not the case with standard procedures.}
}
@article{JOLY2017133,
title = {Corruption: The shortcut to disaster},
journal = {Sustainable Production and Consumption},
volume = {10},
pages = {133-156},
year = {2017},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2016.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S2352550916300288},
author = {Marcel Joly},
keywords = {Climate, Complex systems, Ethics, Petroleum, Politics, Zika virus},
abstract = {I invite readers to briefly explore the utility of mathematical modeling and systems thinking to properly address the impact of causal relationships associated with the higher levels of scattered corruption in the public administration on the constrained-based analysis of high-profile concerns for a sustainable economy. A recent and disastrous, but very educational, Brazilian experience is taken as the motivating example: the unequaled governmental corruption scandal hitherto known, which its epicenter has publicly being associated with the Brazilian state-owned energy company, Petrobras. Few, but remarkable, socio-environmental consequences thought to have been triggered by (or related to) the resulting political crisis, currently devastating the socioeconomic scenario in Brazil, are didactically selected for the in silico analysis proposed. Major findings show that: (a) a reductionist perspective may be illusive when comparing the distinct real-life production and consumption scenarios under corruption, and (b) nonlinear dynamics can efficiently provide theoretical plausibility for the emergent behaviors of the system, which are typically scenario-dependent and can drastically be altered when corruption evolves from a circumscribed into a scattered condition. In conclusion, these results corroborate the need for far greater attention to the issue of corruption–or more generally ethics–to aptly cope with a new array of complex global sustainability challenges that would have been unthinkable just a few decades, or years, ago.}
}
@article{YANG2024106650,
title = {Integrating parcel delivery schedules with public transport networks in urban co-modality systems},
journal = {Computers & Operations Research},
volume = {167},
pages = {106650},
year = {2024},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2024.106650},
url = {https://www.sciencedirect.com/science/article/pii/S0305054824001229},
author = {Xuan Yang and Xinyao Nie and Hao Luo and George Q. Huang},
keywords = {Logistics, Public transport, Co-modality, Parcel assignment},
abstract = {Co-modality transportation advocates using urban public transport to support urban freight operations. This study considers the implementation of co-modality in a fixed-route transit network comprising multiple lines following predetermined routes and schedules. We first develop a schedule-based parcel assignment model to formulate the synchronized co-modality transportation problem (SCTP). The effectiveness of the proposed arc-based meta-heuristic algorithm is substantiated through a comprehensive computational analysis, comparing its performance with that of an exact approach and genetic algorithm. Our findings reveal a nuanced trade-off between transportation efficiency and co-modal stop utilization, identifying a threshold beyond which additional stops do not improve efficiency but increase costs. We also discover a 'buckets effect' in co-modal capacities, suggesting that balanced vehicle and stop capacities are crucial for optimizing system performance. A case study with real urban transit data validates our model's potential for significant efficiency gains in co-modality transportation systems, offering actionable insights for urban logistics.}
}
@article{ARS2021102805,
title = {Underground ancient mine work ventilation modeling},
journal = {Journal of Archaeological Science: Reports},
volume = {37},
pages = {102805},
year = {2021},
issn = {2352-409X},
doi = {https://doi.org/10.1016/j.jasrep.2021.102805},
url = {https://www.sciencedirect.com/science/article/pii/S2352409X21000171},
author = {Christophe Ars and Joseph Gauthier and Nicolas Florsch},
keywords = {Mining archaeology, Ventilation system, Numerical modeling, Computational fluid dynamics, Air quality, Sainte-Marie-aux-Mines},
abstract = {Excavations at the Giro mine located in the commune of Sainte-Marie-aux-Mines (France) have revealed a metallic and cylindrical artifact that resembles a connecting element for wooden duct sections. Early modern literature, and especially De Re Metallica, mention such technologies intended in particular to force the ventilation of underground mines in which air quality was harmful for miners. The connecting element was found in a gallery leading to a stope of which ventilation seems problematic. The numerical simulation of the air flow in the tunnel makes it possible to test the ventilation hypotheses formulated from archaeological data. These simulations are performed with OpenFOAM, a free and open source software for computational fluid dynamics (CFD). Simulating not only the air flow, but also the heat and CO2 production of five miners at work highlights the need to force ventilation in the underground with a ventilation system. It also appears that the construction of scaffolding in the stope can fulfill the double function of facilitating circulation and improving ventilation. This first quantitative approach to one of the main obstacles to mining offers a new method for testing the solutions implemented by miners of the past.}
}
@article{BRIANTHOROMAN2020104859,
title = {An integrated approach to near miss analysis combining AcciMap and Network Analysis},
journal = {Safety Science},
volume = {130},
pages = {104859},
year = {2020},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2020.104859},
url = {https://www.sciencedirect.com/science/article/pii/S0925753520302563},
author = {M.S. {Brian Thoroman} and Paul Salmon},
keywords = {Near miss, Systems thinking, AcciMap, Network analysis, Incident analysis},
abstract = {Contemporary safety philosophies, such as Safety II, promote the importance of understanding effective work practices as well as those leading to adverse events. Despite this, little safety research has focussed on identifying and representing these protective practices in incident analysis. This study combined AcciMap and network analysis methods to identify and evaluate the system-wide protective practices occurring within a set of led outdoor activity domain near miss incidents. The analysis was based on subject matter expert surveys and interviews regarding near miss incidents. The findings revealed a large set of interrelated protective factors. These factors were about communication, policy and procedure, individual and organisational behavioural influences, and environmental conditions across the sociotechnical system. It is argued that network analysis should be combined with AcciMap in future incident analyses.}
}
@article{HONG2023116066,
title = {Portfolio allocation strategy for active learning Kriging-based structural reliability analysis},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {412},
pages = {116066},
year = {2023},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2023.116066},
url = {https://www.sciencedirect.com/science/article/pii/S0045782523001901},
author = {Linxiong Hong and Bin Shang and Shizheng Li and Huacong Li and Jiaming Cheng},
keywords = {Structural reliability analysis, Portfolio allocation, Kriging, Active learning, Failure probability},
abstract = {Recently, numerous studies have focused on structural reliability analysis, with the Kriging-based active learning method being particularly popular. A variety of Kriging-based learning functions have been proposed, and shown to perform well in various tasks. However, no single learning function has been demonstrated to consistently outperformed the others in all tasks, and selecting the most appropriate learning function for a given task remains a challenge in engineering applications. In this paper, inspired by the multi-armed bandit strategy, a portfolio allocation of different learning functions is proposed to resolve the issue of selecting a single one, where the better learning functions are selected online according to their past performance. Finally, three classical numerical examples and two engineering applications are adopted to validate the effectiveness of the proposed method.}
}
@article{DELLACQUA2021199,
title = {Increased functional connectivity within alpha and theta frequency bands in dysphoria: A resting-state EEG study},
journal = {Journal of Affective Disorders},
volume = {281},
pages = {199-207},
year = {2021},
issn = {0165-0327},
doi = {https://doi.org/10.1016/j.jad.2020.12.015},
url = {https://www.sciencedirect.com/science/article/pii/S0165032720331049},
author = {Carola Dell'Acqua and Shadi Ghiasi and Simone {Messerotti Benvenuti} and Alberto Greco and Claudio Gentili and Gaetano Valenza},
keywords = {depression, depressive symptoms, dysphoria, functional connectivity, EEG, vulnerability},
abstract = {Background: The understanding of neurophysiological correlates underlying the risk of developing depression may have a significant impact on its early and objective identification. Research has identified abnormal resting-state electroencephalography (EEG) power and functional connectivity patterns in major depression. However, the entity of dysfunctional EEG dynamics in dysphoria is yet unknown. Methods: 32-channel EEG was recorded in 26 female individuals with dysphoria and in 38 age-matched, female healthy controls. EEG power spectra and alpha asymmetry in frontal and posterior channels were calculated in a 4-minute resting condition. An EEG functional connectivity analysis was conducted through phase locking values, particularly mean phase coherence. Results: While individuals with dysphoria did not differ from controls in EEG spectra and asymmetry, they exhibited dysfunctional brain connectivity. Particularly, in the theta band (4-8 Hz), participants with dysphoria showed increased connectivity between right frontal and central areas and right temporal and left occipital areas. Moreover, in the alpha band (8-12 Hz), dysphoria was associated with increased connectivity between right and left prefrontal cortex and between frontal and central-occipital areas bilaterally. Limitations: All participants belonged to the female gender and were relatively young. Mean phase coherence did not allow to compute the causal and directional relation between brain areas. Conclusions: An increased EEG functional connectivity in the theta and alpha bands characterizes dysphoria. These patterns may be associated with the excessive self-focus and ruminative thinking that typifies depressive symptoms. EEG connectivity patterns may represent a promising measure to identify individuals with a higher risk of developing depression.}
}
@incollection{GOUGH2024547,
title = {25 - Mycelium-based materials for the built environment: a case study on simulation, fabrication and repurposing myco-materials},
editor = {Emina Kristina Petrović and Morten Gjerde and Fabricio Chicca and Guy Marriage},
booktitle = {Sustainability and Toxicity of Building Materials},
publisher = {Woodhead Publishing},
pages = {547-571},
year = {2024},
series = {Woodhead Publishing Series in Civil and Structural Engineering},
isbn = {978-0-323-98336-5},
doi = {https://doi.org/10.1016/B978-0-323-98336-5.00025-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032398336500025X},
author = {Phillip Gough and Anastasia Globa and Dagmar Ingrid Elfriede Reinhardt},
keywords = {Biodegradable building materials, mycelium, circular economy, materials science, case study research, digital printing},
abstract = {Remanufacturing organic waste for composite materials is an opportunity to create circular economic approaches in construction. Research shows how the mycelium (root network) of some fungi, such as Reishi mushrooms, can be guided and formed to create sustainable, biodegradable composite myco-materials for a range of applications. As they degrade wood, paper or coffee waste, Reishi mushrooms create a new material with potential applications in architecture. Research into biocomposites and eco-materials such as mycelium integrates advanced digital fabrication processes, complex structures and algorithmically generated forms for packaging, thermal and sound insulation or as cladding and structural materials. Significantly, the capacity of mycelium composites to seamlessly return as resource material after use in a sustainable ecological cycle indicates the potential for a new approach to sustainability in building and construction processes across the lifespan of buildings. The adaptation of a living organism as an interactive architecture building module could allow strategies for signalling and negotiating climatic variations and local site conditions around the building. Moreover, the capacity of mycelia to not only thrive in contaminated industrial wastelands but to actively detoxify and colonise could further be employed for bioremediation. The research discussed here presents an empirical study into mycelium composites, using computational design and desktop 3D printing to identify strengths and limitations of material, moulds, growth and form. The case study demonstrates how substrate inclusions impact the formation of the material, opportunities to re-awaken inert myco-material for new growth and the quality of fine-detailed elements, using domestic technology and readily available materials. We contribute a taxonomy of existing uses and applications of myco-materials and a range of implications for the process of designing with myco-materials in architecture.}
}
@article{BERKE2017222,
title = {Optimizing trauma-informed intervention for intimate partner violence in veterans: The role of alexithymia},
journal = {Behaviour Research and Therapy},
volume = {97},
pages = {222-229},
year = {2017},
issn = {0005-7967},
doi = {https://doi.org/10.1016/j.brat.2017.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0005796717301651},
author = {Danielle S. Berke and Alexandra Macdonald and Gina M. Poole and Galina A. Portnoy and Savannah McSheffrey and Suzannah K. Creech and Casey T. Taft},
keywords = {Veteran, Trauma, Alexithymia, Randomized control trial, Intimate partner violence},
abstract = {Recent research supports the efficacy of Strength at Home-Men's Program (SAH-M), a trauma-informed group intervention designed to reduce use of intimate partner violence (IPV) in veterans (Taft, Macdonald, Creech, Monson, & Murphy, 2016). However, change-processes facilitating the effectiveness of SAH-M have yet to be specified. Alexithymia, a deficit in the cognitive processing of emotional experience characterized by difficulty identifying and distinguishing between feelings, difficulty describing feelings, and use of an externally oriented thinking style, has been shown to predict PTSD severity and impulsive aggression; however, no studies have investigated the relationship between alexithymia and IPV. As such, the current study examined the role of improvements in alexithymia as a potential facilitator of treatment efficacy among 135 male veterans/service members, in a randomized control trial SAH-M. After an initial assessment including measures of IPV and alexithymia, participants were randomized to an Enhanced Treatment as Usual (ETAU) condition or SAH-M. Participants were assessed three and six months after baseline. Results demonstrated a statistically significant association between alexithymia and use of psychological IPV at baseline. Moreover, participants in the SAH-M condition self-reported significantly greater reductions in alexithymia over time relative to ETAU participants. Findings suggest that a trauma-informed intervention may optimize outcomes, helping men who use IPV both limit their use of violence and improve deficits in emotion processing.}
}
@article{MOALLEMI2018205,
title = {A participatory exploratory modelling approach for long-term planning in energy transitions},
journal = {Energy Research & Social Science},
volume = {35},
pages = {205-216},
year = {2018},
note = {Energy and the Future},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2017.10.022},
url = {https://www.sciencedirect.com/science/article/pii/S221462961730350X},
author = {Enayat A. Moallemi and Shirin Malekpour},
keywords = {Exploratory modelling, Policy analysis, Sustainability transitions, Energy policy, Uncertainty},
abstract = {Energy transitions are complex transformation processes, which involve different actors and unfold in a deeply uncertain future. These features make the long-term planning of energy transitions a wicked problem. Traditional strategic planning approaches fail to address this wickedness as they have a predictive, deterministic, and reactive standpoint to future issues. Modelling approaches that are used within conventional contexts are perceived to be inadequate too. They often simplify the qualitative characteristics of transitions and cannot cope with deeply uncertain futures. More recently, new ways of qualitative participatory planning, as well as new approaches to quantitative modelling have emerged to enable policy analysis under deep uncertainty. We argue that qualitative participatory and quantitative modelling approaches can be complementary to each other in different ways. We operationalise their coupling in the form of a practical approach to be used for long-term planning of energy transitions. The suggested approach enables energy decision makers to test various policy interventions under numerous possibilities with a computational model and in a participatory process. We explain our approach with illustrative examples mostly from transitions in electricity sectors. However, our approach is applicable to different forms of energy transitions, and to the broader context of transition in any societal system, such as water and transportation.}
}
@article{CHAVAS2024476,
title = {Bridging the microscopic divide: a comprehensive overview of micro-crystallization and in vivo crystallography},
journal = {IUCrJ},
volume = {11},
number = {4},
pages = {476-485},
year = {2024},
issn = {2052-2525},
doi = {https://doi.org/10.1107/S205225252400513X},
url = {https://www.sciencedirect.com/science/article/pii/S205225252400054X},
author = {Leonard Michel Gabriel Chavas and Fasséli Coulibaly and Damià Garriga and E. N. Baker},
keywords = {micro-crystallization,  crystallography, structural biology, macromolecular research, X-ray diffraction, XFELs, MicroED},
abstract = {The 26th IUCr congress held in Melbourne brought discussions on micro-crystallization and in vivo crystallography within structural biology to the forefront, highlighting innovative approaches and collaborative efforts to advance macromolecular research.
A series of events underscoring the significant advancements in micro-crystallization and in vivo crystallography were held during the 26th IUCr Congress in Melbourne, positioning microcrystallography as a pivotal field within structural biology. Through collaborative discussions and the sharing of innovative methodologies, these sessions outlined frontier approaches in macromolecular crystallography. This review provides an overview of this rapidly moving field in light of the rich dialogues and forward-thinking proposals explored during the congress workshop and microsymposium. These advances in microcrystallography shed light on the potential to reshape current research paradigms and enhance our comprehension of biological mechanisms at the molecular scale.}
}
@article{SHAWKY2023103476,
title = {Blockchain-based secret key extraction for efficient and secure authentication in VANETs},
journal = {Journal of Information Security and Applications},
volume = {74},
pages = {103476},
year = {2023},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2023.103476},
url = {https://www.sciencedirect.com/science/article/pii/S2214212623000601},
author = {Mahmoud A. Shawky and Muhammad Usman and David Flynn and Muhammad Ali Imran and Qammer H. Abbasi and Shuja Ansari and Ahmad Taha},
keywords = {AVISPA simulation, BAN-logic, Key reconciliation, Public key infrastructure, Secret key extraction, Smart contracts-based blockchain},
abstract = {Intelligent transportation systems are an emerging technology that facilitates real-time vehicle-to-everything communication. Hence, securing and authenticating data packets for intra- and inter-vehicle communication are fundamental security services in vehicular ad-hoc networks (VANETs). However, public-key cryptography (PKC) is commonly used in signature-based authentication, which consumes significant computation resources and communication bandwidth for signatures generation and verification, and key distribution. Therefore, physical layer-based secret key extraction has emerged as an effective candidate for key agreement, exploiting the randomness and reciprocity features of wireless channels. However, the imperfect channel reciprocity generates discrepancies in the extracted key, and existing reconciliation algorithms suffer from significant communication costs and security issues. In this paper, PKC-based authentication is used for initial legitimacy detection and exchanging authenticated probing packets. Accordingly, we propose a blockchain-based reconciliation technique that allows the trusted third party (TTP) to publish the correction sequence of the mismatched bits through a transaction using a smart contract. The smart contract functions enable the TTP to map the transaction address to vehicle-related information and allow vehicles to obtain the transaction contents securely. The obtained shared key is then used for symmetric key cryptography (SKC)-based authentication for subsequent transmissions, saving significant computation and communication costs. The correctness and security robustness of the scheme are proved using Burrows–Abadi–Needham (BAN)-logic and Automated Validation of Internet Security Protocols and Applications (AVISPA) simulator. We also discussed the scheme’s resistance to typical attacks. The scheme’s performance in terms of packet delay and loss ratio is evaluated using the network simulator (OMNeT++). Finally, the computation analysis shows that the scheme saves ∼99% of the time required to verify 1000 messages compared to existing PKC-based schemes.}
}
@article{NAKAMURA2021198,
title = {Explanation of emotion regulation mechanism of mindfulness using a brain function model},
journal = {Neural Networks},
volume = {138},
pages = {198-214},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.01.029},
url = {https://www.sciencedirect.com/science/article/pii/S089360802100037X},
author = {Haruka Nakamura and Yoshimasa Tawatsuji and Siyuan Fang and Tatsunori Matsui},
keywords = {Emotion regulation in mindfulness, Mechanism, Brain function model, Top-down, Bottom-up},
abstract = {The emotion regulation mechanism of mindfulness plays an important role in the stress reduction effect. Many researchers in the fields of cognitive psychology and cognitive neuroscience have attempted to elucidate this mechanism by documenting the cognitive processes that occur and the neural activities that characterize each process. However, previous findings have not revealed the mechanism of information propagation in the brain that achieves emotion regulation during mindfulness. In this study, we constructed a functional brain model based on its anatomical network structure and a computational model representing the propagation of information between brain regions. We then examined the effects of mindfulness meditation on information propagation in the brain using simulations of changes in the activity of each region. These simulations of changes represent the degree of processing resource allocation to the neural activity via changes in the weights of each region’s output. As a result of the simulations, we reveal how the neural activity characteristic of emotion regulation in mindfulness, which has been reported in previous studies, is realized in the brain. Mindfulness meditation increases the weight of the output from each region of the thalamus and sensory cortex, which processes sensory stimuli from the external world. This sensory information activates the insula and anterior cingulate cortex (ACC). The orbitofrontal cortex and dorsolateral prefrontal cortex inhibit amygdala activity (i.e., top-down emotion regulation). However, when mindfulness meditation dominates bottom-up processing via sensory stimuli from the external world, amygdala activity increases through the insula and ACC activation.}
}
@article{WHITACRE2019148,
title = {Exploring unfamiliar paths through familiar mathematical territory: Constraints and affordances in a preservice teacher’s reasoning about fraction comparisons},
journal = {The Journal of Mathematical Behavior},
volume = {53},
pages = {148-163},
year = {2019},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2018.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0732312317301608},
author = {Ian Whitacre and Şebnem Atabaş and Kelly Findley},
keywords = {Preservice teachers, Fractions, Environment metaphor, Beliefs, Productive disposition},
abstract = {Preservice elementary teachers (PSTs) have been described as having difficulties with fractions, relying on standard procedures, and experiencing math anxiety. We are interested in productive ways in which PSTs can and do use their prior knowledge when exploring unfamiliar paths through familiar mathematical territory. We conducted interviews with PSTs in which we challenged them with various fraction comparison tasks and encouraged them to develop new strategies. In this paper, we present a case study focused on one PST who made considerable progress in her reasoning about fraction comparisons during such an interview. We use Greeno’s (1991) environment metaphor to conceptualize number sense as situated knowing in a conceptual domain. This perspective helps us account for both cognitive and affective factors. We highlight 4 themes concerning features of the mathematical environment that Jennifer (pseudonym) appeared to inhabit during the interview: (a) the interview context created a safe space that emphasized the interviewer’s interest in Jennifer’s ideas, as opposed to correct answers; (b) Jennifer used her prior knowledge of parts and wholes to ground her arguments meaningfully and as building blocks to invent new strategies; (c) she used her prior knowledge of cross multiplication as an established path that provided reassurance and facilitated her exploration of unfamiliar paths; (d) it was beliefs and affective factors, not deficiencies in knowledge, that constrained Jennifer’s exploration of unfamiliar paths through familiar mathematical territory. We discuss the implications of these findings for research concerning PSTs’ mathematical thinking and learning and for mathematics teacher education.}
}
@incollection{MCKAY2017,
title = {Behavior Therapy: Theoretical Bases☆},
booktitle = {Reference Module in Neuroscience and Biobehavioral Psychology},
publisher = {Elsevier},
year = {2017},
isbn = {978-0-12-809324-5},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.05242-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093245052421},
author = {D. McKay and W.W. Tryon},
keywords = {Acceptance and Commitment Therapy, Applied behavior analysis, Behavior therapy, Behavioral contextualism, Cognitive therapy, Computational neuropsychology, Conditioning, Connectionist neural networks, Dialectical behavior therapy, Mindfulness based cognitive therapy, Rational emotive behavior therapy},
abstract = {Behavior therapy has become a dominant approach to treating psychological conditions. The theories underlying this approach have been broadly defined by three conceptual frameworks, or ‘waves’. The first wave is based on classical and operant conditioning. The second wave is based on targeting cognitions, or dysfunctional beliefs, that are assumed to contribute to distressing emotional experiences and problematic behaviors. The third wave is based, broadly, on acceptance and mindfulness. Collectively, all three waves may be understood from a computational neuropsychology perspective that is based on connectionist neural network models.}
}
@article{SUKHOBOKOV2024101279,
title = {A universal knowledge model and cognitive architectures for prototyping AGI},
journal = {Cognitive Systems Research},
volume = {88},
pages = {101279},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101279},
url = {https://www.sciencedirect.com/science/article/pii/S1389041724000731},
author = {Artem Sukhobokov and Evgeny Belousov and Danila Gromozdov and Anna Zenger and Ilya Popov},
keywords = {Cognitive architecture, AGI, Metagraph, Archigraph, Universal knowledge model, Machine consciousness, Machine subconsciousness, Machine reflection, Machine worldview},
abstract = {The article identified 56 cognitive architectures for creating general artificial intelligence (AGI) and proposed a set of interrelated functional blocks that an agent approaching AGI in its capabilities should possess. Since the required set of blocks is not found in any of the existing architectures, the article proposes a reference cognitive architecture for intelligent systems approaching AGI in their capabilities. As one of the key solutions within the framework of the architecture, a universal method of knowledge representation is proposed, which allows combining various non-formalized, partially and fully formalized methods of knowledge representation in a single knowledge base, such as texts in natural languages, images, audio and video recordings, graphs, algorithms, databases, neural networks, knowledge graphs, ontologies, frames, essence-property-relation models, production systems, predicate calculus models, conceptual models, and others. To combine and structure various fragments of knowledge, archigraph model are used, constructed as a development of annotated metagraphs. As other components, the reference cognitive architecture being developed includes following modules: machine consciousness, machine subconsciousness, interaction with the external environment, a goal management, an emotional control, social interaction, reflection, ethics, worldview, learning, monitoring, statement problems, solving problems, self-organization and meta learning. Based on the composition of the proposed reference architecture modules, existing cognitive architectures containing the following modules were analyzed: machine consciousness, machine subconsciousness, reflection, worldview.}
}
@incollection{SCHEINER2005831,
title = {Chapter 29 - The CH···O hydrogen bond: A historical account},
editor = {Clifford E. Dykstra and Gernot Frenking and Kwang S. Kim and Gustavo E. Scuseria},
booktitle = {Theory and Applications of Computational Chemistry},
publisher = {Elsevier},
address = {Amsterdam},
pages = {831-857},
year = {2005},
isbn = {978-0-444-51719-7},
doi = {https://doi.org/10.1016/B978-044451719-7/50072-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044451719750072X},
author = {Steve Scheiner},
abstract = {Publisher Summary
This chapter presents a history of the problem, how the CH· · ·O H-bond went from a nonissue to one of recognized importance. The history also includes a discussion on the contributions made by computational chemistry along the way, at each stage. The chapter discusses a unique and surprising property of the CH· · ·O bond that led some to initially deny its characterization as a H-bond at all, and others to go so far as to dub it an “anti-H-bond”. This property has been analyzed and placed into proper perspective through the power of modern quantum chemistry. The early definition of a H-bond paired a proton donor group, typically OH or NH, with an acceptor that contained a nonbonded electron pair. It was the OH· · ·O, OH· · ·N, and NH· · ·O sorts of H-bonds that dominated most thinking about H-bonds. Nonetheless, the weaker sorts of H-bonds were not completely ignored: a smaller number of studies considered the H-bonding abilities of F, Cl, S, and so on.}
}
@article{GALLAGHER2023100127,
title = {Navigating the uncertainty of precision cancer screening: The role of shared decision-making},
journal = {PEC Innovation},
volume = {2},
pages = {100127},
year = {2023},
issn = {2772-6282},
doi = {https://doi.org/10.1016/j.pecinn.2023.100127},
url = {https://www.sciencedirect.com/science/article/pii/S2772628223000079},
author = {Joseph H. Gallagher and Jason L. Vassy and Marla L. Clayman},
keywords = {Shared decision making, Polygenic risk scores, Cancer screening, Genetic counseling, Genomic testing, Patient-provider communication},
abstract = {Objective
Describe how applying a shared decision making (SDM) lens to the implementation of new technologies can improve patient-centeredness.
Methods
This paper argues that the emergence of polygenic risk scores (PRS) for cancer screening presents an illustrative opportunity to include SDM when novel technologies enter clinical care.
Results
PRS are novel tools that indicate an individual’s genetic risk of a given disease relative to the population. PRS are anticipated to help identify individuals most and least likely to benefit from screening. However, PRS have several types of uncertainty, including validity across populations, disparate computational methods, and inclusion of different genomic data across laboratories.
Conclusion
Implementing SDM alongside new technologies could prove useful for their ethical and patient-centered utilization. SDM’s importance as an approach to decision-making will not diminish, as evidence, uncertainty, and patient values will remain intrinsic to the art and science of clinical care.
Innovation
SDM can help providers and patients navigate the considerable uncertainty inherent in implementing new technologies, enabling decision-making based on existing evidence and patient values.}
}
@article{FLORES201825,
title = {Problem-based science, a constructionist approach to science literacy in middle school},
journal = {International Journal of Child-Computer Interaction},
volume = {16},
pages = {25-30},
year = {2018},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2017.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S2212868917300417},
author = {Christa Flores},
keywords = {Curriculum design, Design science, Learning spaces design, Science literacy, Maker education, Mindsets, Problem-based science, Constructionism, Constructivism, Inclusivity},
abstract = {This paper describes a four-year observation using a model designed and tested in a middle school maker space, called problem-based science (PbS). PbS was used as the primary model for a middle school science curriculum adapted by the tools and mindsets of the maker movement. PbS is learning through inventing and problem solving — while using the latest in fabrication technology, like 3D printers and laser cutters, as well as more traditional making skills, like electronics, robotics, sewing and carpentry. PbS is based on Seymour Papert’s constructionism, set to a science curriculum taught full time in a makerspace or fablab. Bridging ideas in design thinking, maker education, and applied math and science, the term problem-based science was used to describe how learning would look, sound, and feel different in a makerspace, when a focus was on learner-centered curriculum. The design and testing of this curriculum took place as part of the 5th and 6th grade science courses offered at a private (non-public) school in California (USA) the fall of 2012, through the spring of 2016. Through daily formative assessment, as well as exit surveys, the patterns and benefits of learning in a self-directed learning space, designed for constructionism, were observed. This paper shares the highlights of those years. Video taped exit surveys conducted by the author, show that self-direction is both challenging and rewarding, students often felt trusted and respected, even if they did not always feel supported in a manner common in a more teacher directed classroom setting. Daily informal classroom observations revealed that using student driven, open-ended problem solving, rather than a 100% teacher led, step by step lab, lends to a more diverse pool of leadership practice in students and higher engagement in hard problems. Students typically seen as struggling in traditional classrooms, identified as experts and successful learners in this setting. Lastly, using PbS as a model for science literacy allows the youngest of learners to practice mindsets and habits typical of real scientists and inventors, fostering early identify formation in STEM fields.}
}
@article{HUSSAIN2023135948,
title = {Efficient synthesis of nicotinaldehyde-based crystalline organic derivatives: Comparative analysis between experimental and DFT study},
journal = {Journal of Molecular Structure},
volume = {1290},
pages = {135948},
year = {2023},
issn = {0022-2860},
doi = {https://doi.org/10.1016/j.molstruc.2023.135948},
url = {https://www.sciencedirect.com/science/article/pii/S0022286023010426},
author = {Shahid Hussain and Muhammad Adeel and Muhammad Khalid and Ume Aiman and Alexander Villinger and Ataualpa A.C. Braga and Saad M. Alshehri and Muhammad {Adnan Asghar}},
keywords = {Pyridine, Nicotinaldehyde, NBO analysis, Density functional theory, MEP},
abstract = {The analogues of pyridine ring structures demonstrate various physiological as well as biological activities. The current research is based on experimental along with computational investigations of two new phenyl substituted nicotinaldehyde derivatives; 2-(2,4-difluorophenyl)pyridine-3-carbaldehyde (DFPPC) and 2-(2,5-dichlorophenyl)pyridine-3-carbaldehyde (DCPPC) . For structural optimization of DFPPC as well as DCPPC and to explore nonlinear optical properties, computational quantum chemical analysis was executed via density functional theory (DFT) calculations by employing M06 level with 6–311G(d,p) basis set. A consensus among theoretical (DFT) and experimental (SC-XRD) results was observed by the calculation of geometric parameters. Molecular electrostatic potential (MEP), natural bond orbital (NBO) analysis, natural population analysis (NPA), nonlinear optical (NLO), global reactivity parameters (GRPs), and frontier molecular orbital (FMO) exploration were carried out at M06/6–311G(d,p), to comprehend hyper-conjugative interactions, electron density, electronic communications and oscillation strength. The HOMO/LUMO energy gap of DCPPC (5.108 eV) was observed to be lower than DFPPC i.e., 5.170 eV, which resulted in its higher value of global softness (0.196 Eh) along with lower global hardness (2.554 Eh) value than DFPPC. The NLO attributes of DFPPC as well as DCPPC was calculated by evaluating the total dipole moment (μtot), average linear polarizability ⟨α⟩ and second hyperpolarizability (γtot) at aforementioned level. From the NLO results, it was observed that DCPPC exhibits a higher average linear polarizability value such as 3.0772 × 10−23 esu than DFPPC i.e., 2.6116 × 10−23 esu . Whereas, higher results of γtot were observed for DFPPC i.e., 3.2455 × 10−35 than DCPPC (3.0708 × 10−35 esu). The distinguished NLO characteristics revealed that, both the chromophores (DFPPC and DCPPC) can be recognized as highly efficient NLO materials for future applications.}
}
@article{CHEN2006103,
title = {Toward development of activity coefficient models for process and product design of complex chemical systems},
journal = {Fluid Phase Equilibria},
volume = {241},
number = {1},
pages = {103-112},
year = {2006},
note = {A Festschrift in Honor of John M. Prausnitz},
issn = {0378-3812},
doi = {https://doi.org/10.1016/j.fluid.2006.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0378381206000331},
author = {Chau-Chyun Chen},
keywords = {Complex chemical systems, Electrolytes, Nonelectrolytes, Polymers, Pharmaceuticals, Activity coefficient models, Excess Gibbs energy},
abstract = {Molecular thermodynamics, an engineering science for quantitative representation of thermophysical properties and phase behavior for mixtures, has served as a core scientific foundation for process modeling and process and product design in the industries. This paper presents a personal adventure through molecular thermodynamics that follows the footprints of John Prausnitz and leads toward the development of activity coefficient models for process modeling and process and product design of complex chemical systems. In this scientific expedition, passion and endurance, industrial applications, molecular insights, and out-of-the-box thinking all play key roles. We venerate past accomplishments that serve industrial needs and cherish new opportunities that await future exploration by adventurous souls.}
}
@article{FIELDS2023104927,
title = {Regulative development as a model for origin of life and artificial life studies},
journal = {Biosystems},
volume = {229},
pages = {104927},
year = {2023},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2023.104927},
url = {https://www.sciencedirect.com/science/article/pii/S0303264723001028},
author = {Chris Fields and Michael Levin},
keywords = {Free energy principle, Kinematic replication, Learning, Multicellularity, Multiscale competency architecture, Target morphology},
abstract = {Using the formal framework of the Free Energy Principle, we show how generic thermodynamic requirements on bidirectional information exchange between a system and its environment can generate complexity. This leads to the emergence of hierarchical computational architectures in systems that operate sufficiently far from thermal equilibrium. In this setting, the environment of any system increases its ability to predict system behavior by “engineering” the system towards increased morphological complexity and hence larger-scale, more macroscopic behaviors. When seen in this light, regulative development becomes an environmentally-driven process in which “parts” are assembled to produce a system with predictable behavior. We suggest on this basis that life is thermodynamically favorable and that, when designing artificial living systems, human engineers are acting like a generic “environment”.}
}
@article{BOETTKE202344,
title = {On the feasibility of technosocialism},
journal = {Journal of Economic Behavior & Organization},
volume = {205},
pages = {44-54},
year = {2023},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2022.10.046},
url = {https://www.sciencedirect.com/science/article/pii/S0167268122004048},
author = {Peter J. Boettke and Rosolino A. Candela},
keywords = {Economic calculation, F.A. Hayek, Ludwig von Mises, Technosocialism},
abstract = {Technological advances associated with computing power and the prospect of artificial intelligence have renewed interest on the economic feasibility of socialism. The question of such feasibility turns on whether the problem of economic calculation has fundamentally changed. In spite of the prospect of what King and Petty (2021) refer to as “technosocialism,” we argue that technological advances in computation cannot replace the competitive discovery process that takes place within the context of the market. We do so by situating the case for technosocialism in the context of the socialist calculation debate. Understood in these terms, technosocialism represents a restatement of the case for market socialism, which incorrectly framed the “solution” to economic calculation under socialism as one of computing data, rather than the discovery of context-specific knowledge that only emerges through the exchange of property rights. Therefore, the arguments put forth by Ludwig von Mises and F.A. Hayek, and later Israel Kirzner and Don Lavoie, regarding the impossibility of economic calculation under socialism remains just as relevant today.}
}
@article{KIRSCHNER2017135,
title = {The myths of the digital native and the multitasker},
journal = {Teaching and Teacher Education},
volume = {67},
pages = {135-142},
year = {2017},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2017.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X16306692},
author = {Paul A. Kirschner and Pedro {De Bruyckere}},
keywords = {Digital native, Multitasking, Homo zappiëns, Educational reform},
abstract = {Current discussions about educational policy and practice are often embedded in a mind-set that considers students who were born in an age of omnipresent digital media to be fundamentally different from previous generations of students. These students have been labelled digital natives and have been ascribed the ability to cognitively process multiple sources of information simultaneously (i.e., they can multitask). As a result of this thinking, they are seen by teachers, educational administrators, politicians/policy makers, and the media to require an educational approach radically different from that of previous generations. This article presents scientific evidence showing that there is no such thing as a digital native who is information-skilled simply because (s)he has never known a world that was not digital. It then proceeds to present evidence that one of the alleged abilities of students in this generation, the ability to multitask, does not exist and that designing education that assumes the presence of this ability hinders rather than helps learning. The article concludes by elaborating on possible implications of this for education/educational policy.}
}
@article{FENG2017150,
title = {Parallel programming with pictures is a Snap!},
journal = {Journal of Parallel and Distributed Computing},
volume = {105},
pages = {150-162},
year = {2017},
note = {Keeping up with Technology: Teaching Parallel, Distributed and High-Performance Computing},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2017.01.018},
url = {https://www.sciencedirect.com/science/article/pii/S0743731517300242},
author = {Annette Feng and Mark Gardner and Wu-chun Feng},
keywords = {Explicit parallel computing, Computer science education, Block-based programming, Visual programming, Parallel computational patterns, Pedagogical tools, Programming environments, Languages for PDC and HPC},
abstract = {For decades, computing speeds seemingly doubled every 24 months by increasing the processor clock speed, thus giving software a “free ride” to better performance. This free ride, however, effectively ended by the mid-2000s. With clock speeds having plateaued and computational horsepower instead increasing due to increasing the number of cores per processor, the vision for parallel computing, which started more than 40 years ago, is a revolution that has now (ubiquitously) arrived. In addition to traditional supercomputing clusters, parallel computing with multiple cores can be found in desktops, laptops, and even mobile smartphones. This ubiquitous parallelism in hardware presents a major challenge: the difficulty in easily extracting parallel performance via current software abstractions. Consequently, this paper presents an approach that reduces the learning curve to parallel programming by introducing such concepts into a visual (but currently sequential) programming language called Snap!, which was inspired by MIT’s Scratch project. Furthermore, our proposed visual abstractions can automatically generate parallel code for the end user to run in parallel on a variety of platforms from personal computing devices to supercomputers. Ultimately, this work seeks to increase parallel programming literacy so that users, whether novice or experienced, may leverage a world of ubiquitous parallelism to enhance productivity in all walks of life, including the sciences, engineering, commerce, and liberal arts.}
}
@article{DEGRAAF2006181,
title = {Fall and rise of behavioural pharmacology},
journal = {Drug Discovery Today: Technologies},
volume = {3},
number = {2},
pages = {181-185},
year = {2006},
issn = {1740-6749},
doi = {https://doi.org/10.1016/j.ddtec.2006.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S174067490600031X},
author = {Joop S. {de Graaf}},
abstract = {Since the 1970s, a fortunate ensemble of technological and scientific developments has radically changed pharmacology, both in practice and imaginative thinking, towards a predominantly molecular science. Economic and political forces contributed to the undervaluation of in vivo experiments. The present generation of bioscientists, undertrained in whole animal, particularly behavioural pharmacology, now faces the challenge to interpret and translate an interminable hoard of molecular data into understandable and applicable medicine. The article provides a retrospection in four decades of progress.}
}
@article{SOYEL20131312,
title = {Towards an affect sensitive interactive companion},
journal = {Computers & Electrical Engineering},
volume = {39},
number = {4},
pages = {1312-1319},
year = {2013},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2013.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0045790613000712},
author = {Hamit Soyel and Peter W. McOwan},
abstract = {As robots are increasingly being viewed as social entities to be integrated in our daily lives, social perceptive abilities seem a necessary requirement for enabling more natural interaction with human users. In this paper, we present an interaction scenario where user play chess with an iCat robot and propose an affect recognition system that uses computational models to automatically extract visual features allowing the detection of the level of engagement with a social robot that acts as a game companion. Experimental results show that the multimodal integration of head direction information with facial expressions displayed by the user improves the recognition of the user’s affective states.}
}
@incollection{HALLGRIMSSON20051,
title = {CHAPTER 1 - Variation and Variability: Central Concepts in Biology},
editor = {Benedikt Hallgrímsson and Brian K. Hall},
booktitle = {Variation},
publisher = {Academic Press},
address = {Burlington},
pages = {1-7},
year = {2005},
isbn = {978-0-12-088777-4},
doi = {https://doi.org/10.1016/B978-012088777-4/50003-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012088777450003X},
author = {Benedikt Hallgrímsson and Brian K. Hall},
abstract = {Publisher Summary
Variation is a central topic, both conceptually and historically in evolutionary biology. Phenotypic variation was Darwin's fundamental observation. Indeed, the first two chapters of On the Origin of Species deal explicitly with variation. Variation within and among species has certainly been as central to the thinking of Ernst Mayr (1963) as it was to the thinking of Sewall Wright (1968), two of the fathers of the modern synthesis. However, the study of variability or the propensity to vary, with few exceptions, has remained peripheral to study of the mechanisms of evolutionary change at any level of the biological hierarchy. Although implicit in virtually all research in the biological sciences, whether one is seeking understanding at the genetic, developmental, organismal, species, population, or ecologic/community levels, variation is seldom treated as a subject in and of itself. Variation is an extremely broad topic, and a modern treatment of this subject is not possible without a thematic focus. This chapter introduces this theme through both a hierarchical treatment and integrative approaches that point toward new directions of research.}
}
@article{IVANOV201952,
title = {Infinite lattice models by an expansion with a non-Gaussian initial approximation},
journal = {Physics Letters B},
volume = {796},
pages = {52-58},
year = {2019},
issn = {0370-2693},
doi = {https://doi.org/10.1016/j.physletb.2019.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0370269319304460},
author = {Aleksandr Ivanov and Vasily Sazonov},
abstract = {Recently, a convergent series employing a non-Gaussian initial approximation was constructed and shown to be an effective computational tool for the finite size lattice models with a polynomial interaction. Here we show that the Borel summability is a sufficient condition for the correctness of the convergent series applied to infinite lattice models. We test the numerical workability of the convergent series method by examining one- and two-dimensional ϕ4-infinite lattice models. The comparison of the convergent series computations and the infinite lattice extrapolations of the Monte Carlo simulations reveals an agreement between two approaches.}
}
@article{ANDERSON2011R123,
title = {Neuroscience: What We Cannot Model, We Do Not Understand},
journal = {Current Biology},
volume = {21},
number = {3},
pages = {R123-R125},
year = {2011},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2010.12.049},
url = {https://www.sciencedirect.com/science/article/pii/S0960982210017173},
author = {William S. Anderson and Gabriel Kreiman},
abstract = {Summary
To understand computations in neuronal circuits, a model of a small patch of cortex has been developed that can describe the firing regime in the visual system remarkably well.}
}
@article{BASOV2024101869,
title = {Professional patios, emotional studios: Locating social ties in European art residences},
journal = {Poetics},
volume = {102},
pages = {101869},
year = {2024},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2024.101869},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X24000081},
author = {Nikita Basov and Dafne Muntanyola-Saura and Sergi Méndez and Oleksandra Nenko},
keywords = {Material space, Social network, Socio-material network analysis, Mixed method, Statistical modeling of ethnographic data, Artistic residence},
abstract = {To foster creativity through sociality, residences put artists together. At the same time, in their quest for originality, artists often opt for individualism. Little is known on how physical collocation in residences affects artistic sociality. Addressing this gap, we draw on a combination of interviews, observations, and surveys, analysed with an innovative mixture of abductive coding, computational space analysis, and statistical network modeling. This allows us to unveil how room sharing and object usage relate to friendships and collaborations between residents. Along with explicit individualism of artists, we spot plenty of social ties between them. And these ties are positively related to joint material embeddedness. Simultaneously, the two main types of residential zones – working studios and leisure areas – appear to encourage the types of social ties inverse to our expectations. Our findings inform the practice of artistic residence organising and the proposed approach enables explanatory analysis of the relation between material space and sociality in various settings.}
}
@article{FARISCO2024106714,
title = {Is artificial consciousness achievable? Lessons from the human brain},
journal = {Neural Networks},
volume = {180},
pages = {106714},
year = {2024},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106714},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024006385},
author = {Michele Farisco and Kathinka Evers and Jean-Pierre Changeux},
keywords = {Brain, Consciousness, Artificial intelligence, Neuromorphic computing, Robotics, Cognition, Neuroscience},
abstract = {We here analyse the question of developing artificial consciousness from an evolutionary perspective, taking the evolution of the human brain and its relation with consciousness as a reference model or as a benchmark. This kind of analysis reveals several structural and functional features of the human brain that appear to be key for reaching human-like complex conscious experience and that current research on Artificial Intelligence (AI) should take into account in its attempt to develop systems capable of human-like conscious processing. We argue that, even if AI is limited in its ability to emulate human consciousness for both intrinsic (i.e., structural and architectural) and extrinsic (i.e., related to the current stage of scientific and technological knowledge) reasons, taking inspiration from those characteristics of the brain that make human-like conscious processing possible and/or modulate it, is a potentially promising strategy towards developing conscious AI. Also, it cannot be theoretically excluded that AI research can develop partial or potentially alternative forms of consciousness that are qualitatively different from the human form, and that may be either more or less sophisticated depending on the perspectives. Therefore, we recommend neuroscience-inspired caution in talking about artificial consciousness: since the use of the same word “consciousness” for humans and AI becomes ambiguous and potentially misleading, we propose to clearly specify which level and/or type of consciousness AI research aims to develop, as well as what would be common versus differ in AI conscious processing compared to human conscious experience.}
}
@article{CARAYON2010657,
title = {Human factors in patient safety as an innovation},
journal = {Applied Ergonomics},
volume = {41},
number = {5},
pages = {657-665},
year = {2010},
note = {Human Factors and Ergonomics in Patient Safety},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2009.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S0003687009001719},
author = {Pascale Carayon},
keywords = {Human factors and ergonomics, Patient safety, Healthcare, Innovation, Adoption, Dissemination, Diffusion, Implementation},
abstract = {The use of Human Factors and Ergonomics (HFE) tools, methods, concepts and theories has been advocated by many experts and organizations to improve patient safety. To facilitate and support the spread of HFE knowledge and skills in healthcare and patient safety, we propose to conceptualize HFE as innovations whose diffusion, dissemination, implementation and sustainability need to be understood and specified. Using Greenhalgh et al. (2004) model of innovation, we identified various factors that can either hinder or facilitate the spread of HFE innovations in healthcare organizations. Barriers include lack of systems thinking, complexity of HFE innovations and lack of understanding about the benefits of HFE innovations. Positive impact of HFE interventions on task performance and the presence of local champions can facilitate the adoption, implementation and sustainability of HFE innovations. This analysis concludes with a series of recommendations for HFE professionals, researchers and educators.}
}
@article{HEMASPAANDRA202266,
title = {The complexity of online bribery in sequential elections},
journal = {Journal of Computer and System Sciences},
volume = {127},
pages = {66-90},
year = {2022},
issn = {0022-0000},
doi = {https://doi.org/10.1016/j.jcss.2022.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0022000022000071},
author = {Edith Hemaspaandra and Lane A. Hemaspaandra and Jörg Rothe},
keywords = {Bribery, Computational complexity, Computational social choice, Logic, Quantifier assignment, Sequential elections},
abstract = {Prior work on the complexity of bribery assumes that the bribery happens simultaneously, and that the briber has full knowledge of all votes. However, in many real-world settings votes come in sequentially, and the briber may have a use-it-or-lose-it moment to decide whether to alter a given vote, and when making that decision the briber may not know what votes remaining voters will cast. We introduce a model for, and initiate the study of, bribery in such an online, sequential setting. We show that even for election systems whose winner-determination problem is polynomial-time computable, an online, sequential setting may vastly increase the complexity of bribery, jumping the problem up to completeness for high levels of the polynomial hierarchy or even PSPACE. But we also show that for some natural, important election systems, such a dramatic complexity increase does not occur, and we pinpoint the complexity of their bribery problems.}
}
@article{KRAUZLIS2014457,
title = {Attention as an effect not a cause},
journal = {Trends in Cognitive Sciences},
volume = {18},
number = {9},
pages = {457-464},
year = {2014},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2014.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S1364661314001296},
author = {Richard J. Krauzlis and Anil Bollimunta and Fabrice Arcizet and Lupeng Wang},
keywords = {attention, basal ganglia, decision making, learning, perception, superior colliculus},
abstract = {Attention is commonly thought to be important for managing the limited resources available in sensory areas of the neocortex. Here we present an alternative view that attention arises as a byproduct of circuits centered on the basal ganglia involved in value-based decision making. The central idea is that decision making depends on properly estimating the current state of the animal and its environment and that the weighted inputs to the currently prevailing estimate give rise to the filter-like properties of attention. After outlining this new framework, we describe findings from physiological, anatomical, computational, and clinical work that support this point of view. We conclude that the brain mechanisms responsible for attention employ a conserved circuit motif that predates the emergence of the neocortex.}
}
@article{BUCHANAN2019332,
title = {Metal 3D printing in construction: A review of methods, research, applications, opportunities and challenges},
journal = {Engineering Structures},
volume = {180},
pages = {332-348},
year = {2019},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2018.11.045},
url = {https://www.sciencedirect.com/science/article/pii/S0141029618307958},
author = {C. Buchanan and L. Gardner},
keywords = {3D printing, Additive manufacturing, Applications, Concrete, Metal, Polymers, Research, Review, Stainless steel, Structural engineering},
abstract = {3D printing, more formally known as additive manufacturing (AM), has the potential to revolutionise the construction industry, with foreseeable benefits including greater structural efficiency, reduction in material consumption and wastage, streamlining and expedition of the design-build process, enhanced customisation, greater architectural freedom and improved accuracy and safety on-site. Unlike traditional manufacturing methods for construction products, metal 3D printing offers ready opportunities to create non-prismatic sections, internal stiffening, openings, functionally graded elements, variable microstructures and mechanical properties through controlled heating and cooling and thermally-induced prestressing. Additive manufacturing offers many opportunities for the construction sector, but there will also be fresh challenges and demands, such as the need for more digitally savvy engineers, greater use of advanced computational analysis and a new way of thinking for the design and verification of structures, with greater emphasis on inspection and load testing. It is envisaged that AM will complement, rather than replace, conventional production processes, with clear potential for hybrid solutions and structural strengthening and repairs. These opportunities and challenges are explored in this paper as part of a wider review of different methods of metal 3D printing, research and early applications of additive manufacturing in the construction industry. Lessons learnt for metal 3D printing in construction from additive manufacturing using other materials and in other industries are also presented.}
}
@article{ZHANG20247,
title = {Large language model in electrocatalysis},
journal = {Chinese Journal of Catalysis},
volume = {59},
pages = {7-14},
year = {2024},
issn = {1872-2067},
doi = {https://doi.org/10.1016/S1872-2067(23)64612-1},
url = {https://www.sciencedirect.com/science/article/pii/S1872206723646121},
author = {Chengyi Zhang and Xingyu Wang and Ziyun Wang},
keywords = {Large language model, Electrocatalysis, Artificial intelligence, Multimodal large language model},
abstract = {ABSTRACT
Large language models have recently brought a massive storm on modern society in all fields. While many view them as mere search engines for specific answers or text refinement tools like a chatbot, their broader applications remain largely unexplored. These large language models, consisting of billions of interconnected neurons, derived from all knowledge of the human, possess the remarkable ability to engage in smooth and precise conversations with individuals across the globe. Human-like intelligence enables them to address modern challenges and display immense potential in various scientific domains. In this perspective, we delve into the potential applications of modern large language model and its future iterations within the field of catalysis, aiming to shed light on how these AI-driven models can contribute to a deeper understanding of catalysis science and the intelligent design of catalysts.}
}
@article{MIYATA2018370,
title = {Emergence of symbolic inference based on value-driven intuitive inference via associative memory},
journal = {Procedia Computer Science},
volume = {145},
pages = {370-375},
year = {2018},
note = {Postproceedings of the 9th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2018 (Ninth Annual Meeting of the BICA Society), held August 22-24, 2018 in Prague, Czech Republic},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.11.087},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918323755},
author = {Masahiro Miyata and Takashi Omori},
keywords = {Human inference, Associative memory, Model, Symbolic inference, Intuitive inference},
abstract = {Humans use two types of inferences: intuitive and logical. However, they are studied separately. A site for logical inference has not been found in the brain, but modeling it as a distributed neural network form is desirable. In this study, we propose an inference model of an intuitive search process in continuous and distributed associative memory (AM), and it switches to a symbolic mode, in which each step of association converges to a stable state of self-recollection, realizing step-by-step logic. Switching is evoked by biasing the associative gain upon finding a valued state during the intuitive inference. In this study, we show the computational model of symbolic inference via AM, and we verify its practicality by solving a maze task. We show the emergence of a tree search-like behavior with pruning.}
}
@article{DIGLIO2023233,
title = {Approximation schemes for districting problems with probabilistic constraints},
journal = {European Journal of Operational Research},
volume = {307},
number = {1},
pages = {233-248},
year = {2023},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2022.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0377221722007172},
author = {Antonio Diglio and Juanjo Peiró and Carmela Piccolo and Francisco Saldanha-da-Gama},
keywords = {Location, Districting, Stochastic demand, Chance-constraint balancing, Heuristics},
abstract = {In this work a districting problem with stochastic demand is investigated. Chance-constraints are used to model the balancing requirements. Explicit contiguity constraints are also considered. After motivating the problem and discussing several modeling aspects, an approximate deterministic counterpart is proposed which is the core of new solution algorithms devised. The latter are based upon a location-allocation scheme, whose first step consists of considering either a problem with a sample of scenarios or a sample of single-scenario problems. This leads to two variants of a new heuristic. The second version calls for the use of a so-called attractiveness function as a means to find a good trade-off between the (approximate) solutions obtained for the single-scenario problems. Different definitions of such functions are discussed. Extensive computational tests were performed whose results are reported.}
}
@article{KOLERS1984289,
title = {Symbol manipulation: Alternatives to the computational view of mind},
journal = {Journal of Verbal Learning and Verbal Behavior},
volume = {23},
number = {3},
pages = {289-314},
year = {1984},
issn = {0022-5371},
doi = {https://doi.org/10.1016/S0022-5371(84)90182-8},
url = {https://www.sciencedirect.com/science/article/pii/S0022537184901828},
author = {Paul A. Kolers and William E. Smythe},
abstract = {Acquisition and manipulation of symbols are the fundamental constituents of cognitive activity, but modern information processing theory has not explored their basis sufficiently. Computationalism is the one modern approach that takes an explicit position in regard to symbol manipulation. We here explore some of the virtues of that approach and show its principal deficiency, which is to construe symbolization too narrowly, thereby blocking more adequate treatments of learning and acquisition of skills. Symbols come in many kinds; the different kinds allow for different representational capabilities. A proper sorting of symbols and an understanding of their different capabilities is prerequisite, and should be of the greatest benefit, to an account of cognitive processes.}
}
@article{CHEN2023103837,
title = {Recursive reasoning-based training-time adversarial machine learning},
journal = {Artificial Intelligence},
volume = {315},
pages = {103837},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103837},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222001771},
author = {Yizhou Chen and Zhongxiang Dai and Haibin Yu and Bryan Kian Hsiang Low and Teck-Hua Ho},
keywords = {Recursive reasoning, Adversarial machine learning, Game theory},
abstract = {The training process of a machine learning (ML) model may be subject to adversarial attacks from an attacker who attempts to undermine the test performance of the ML model by perturbing the training minibatches, and thus needs to be protected by a defender. Such a problem setting is referred to as training-time adversarial ML. We formulate it as a two-player game and propose a principled Recursive Reasoning-based Training-Time adversarial ML (R2T2) framework to model this game. R2T2 models the reasoning process between the attacker and the defender and captures their bounded reasoning capabilities (due to bounded computational resources) through the recursive reasoning formalism. In particular, we associate a deeper level of recursive reasoning with the use of a higher-order gradient to derive the attack (defense) strategy, which naturally improves its performance while requiring greater computational resources. Interestingly, our R2T2 framework encompasses a variety of existing adversarial ML methods which correspond to attackers (defenders) with different recursive reasoning capabilities. We show how an R2T2 attacker (defender) can utilize our proposed nested projected gradient descent-based method to approximate the optimal attack (defense) strategy at an arbitrary level of reasoning. R2T2 can empirically achieve state-of-the-art attack and defense performances on benchmark image datasets.}
}
@article{YANG2024234071,
title = {Application and development of the Lattice Boltzmann modeling in pore-scale electrodes of solid oxide fuel cells},
journal = {Journal of Power Sources},
volume = {599},
pages = {234071},
year = {2024},
issn = {0378-7753},
doi = {https://doi.org/10.1016/j.jpowsour.2024.234071},
url = {https://www.sciencedirect.com/science/article/pii/S0378775324000223},
author = {Xiaoxing Yang and Guogang Yang and Shian Li and Qiuwan Shen and He Miao and Jinliang Yuan},
keywords = {Lattice Boltzmann method modeling, Pore-scale simulation, Reactive transport, Solid oxide fuel cells simulation},
abstract = {The lattice Boltzmann method (LBM) plays an important role in the study of the internal flow behavior at the pore-scale inside the electrodes of solid oxide fuel cells (SOFCs). Porosity, tortuosity, and particle size have a remarkable effect on gas transport and electrocatalytic processes, determining the performance of cells when SOFCs are applied in electric power generation, energy storage systems, and industrial production in recent years. However, these pore-scale transport progresses are not well characterized in the numerical studies of conventional computational fluid dynamics (CFD), thus modeling with LBM at the pore-scale is an effective tool for simulating gas transport and electrochemical reactions in electrodes. It overcomes the drawbacks of experimental techniques that do not characterize these processes accurately enough and detail the distribution of important variables. In this review, the methodology and process of electrode pore-scale modeling are presented, along with the application and current studies of LBM for diffusion, electrochemical reactions, and ion migration in SOFC porous electrodes. Important results are discussed. Finally, future perspectives on pore-scale studies of porous electrodes are given. This in-depth review intends to provide ideas for the development and further application of LBM in porous SOFC electrodes.}
}
@article{YANG2023103712,
title = {Study of the water entry and exit problems by coupling the APR and PST within SPH},
journal = {Applied Ocean Research},
volume = {139},
pages = {103712},
year = {2023},
issn = {0141-1187},
doi = {https://doi.org/10.1016/j.apor.2023.103712},
url = {https://www.sciencedirect.com/science/article/pii/S0141118723002535},
author = {Xi Yang and Song Feng and Jinxin Wu and Guiyong Zhang and Guangqi Liang and Zhifan Zhang},
keywords = {Water entry, Water exit, Particle shifting technique, Adaptive particle refinement, Coupled scheme},
abstract = {In this manuscript, the adaptive particle refinement (APR) and particle shifting technique (PST) are coupled and applied to investigate the entire process of water entry and exit. The PST implementation for various types of particles is quantitatively discussed in detail in the coupled APR-PST approach. A comprehensive analysis of different APR-PST coupled schemes is conducted with crucial variables compared and analyzed for the first time. The stability, accuracy and robustness of the developed numerical model are verified by three benchmark tests: 2D wedge water entry, 2D cylinder water exit and 2D cylinder water entry and exit. The obtained results show that the current model can ensure the overall computational precision in the situation of local refinement, which indicates it is a viable way to solve the problems of water entry and exit.}
}
@article{DAVIS2024307,
title = {AI rising in higher education: opportunities, risks and limitations},
journal = {Asian Education and Development Studies},
volume = {13},
number = {4},
pages = {307-319},
year = {2024},
issn = {2046-3162},
doi = {https://doi.org/10.1108/AEDS-01-2024-0017},
url = {https://www.sciencedirect.com/science/article/pii/S2046316224000154},
author = {Adrian John Davis},
keywords = {Human mind, Human intelligence, Human consciousness, Artificial intelligence (AI), Artificial consciousness, Quality teaching},
abstract = {Purpose
The aim of this paper is twofold: to explore the significance and implications of the rise of AI technology for the field of tertiary education in general and, in particular, to answer the question of whether teachers can be replaced by intelligent AI systems such as androids, what that requires in terms of human capabilities and what that might mean for teaching and learning in higher education.
Design/methodology/approach
Given the interdisciplinary nature of this conceptual paper, a literature review serves as a methodological tool to access data pertaining to the research question posed in the paper.
Findings
This exploratory paper gathers a range of evidence from the philosophy of mind (the mind-body problem), Kahneman’s (2011) System 1 and System 2 models of the mind, Gödel’s (1951) Two Incompleteness Theorems, Polanyi’s (1958, 1966) theory of tacit knowing and Searle’s (1980) Chinese Room thought experiment to the effect that no AI system can ever fully replace a human being because no machine can replicate the human mind and its capacity for intelligence, consciousness and highly developed social skills such as empathy and cooperation.
Practical implications
AI is rising, but there are inherent limits to what machines can achieve when compared to human capabilities. An android can at most attain “weak AI”, that is, it can be smart but lack awareness or empathy. Therefore, an analysis of good teaching at the tertiary level shows that learning, knowledge and understanding go far beyond any quantitative processing that an AI machine does so well, helping us to appreciate the qualitative dimension of education and knowledge acquisition. ChatGPT is robotic, being AI-generated, but human beings thrive on the human-to-human interface – that is, human relationships and meaningful connections – and that is where the true qualitative value of educational attainment will be gauged.
Social implications
This paper has provided evidence that human beings are irreplaceable due to our unique strengths as meaning-makers and relationship-builders, our capacity for morality and empathy, our creativity, our expertise and adaptability and our capacity to build unity and cooperate in building social structures and civilization for the benefit of all. Furthermore, as society is radically automated, the purpose of human life and its reevaluation will also come into question. For instance, as more and more occupations are replaced by ChatGPT services, more and more people will be freed up to do other things with their time, such as caring for relatives, undertaking creative projects, studying further and having children.
Originality/value
The investigation of the scope and limitations of AI is significant for two reasons. First, the question of the nature and functions of a mind becomes critical to the possibility of replication because if the human mind is like a super-sophisticated computer, then the relationship between a brain and mind is similar (if not identical) to the relationship between a computer as machine hardware and its programme or software (Dreyfus, 1979). [ ] If so, it should be theoretically possible to understand its mechanism and reproduce it, and then it is just a matter of time before AI research and development can replicate the human mind and eventually replace a human teacher, especially if an AI machine can teach just as intelligently yet more efficiently and economically. But if AI has inherent limitations that preclude the possibility of ever having a human-like mind and thought processes, then our investigation can at least clarify in what ways AI/AGI – such as ChatGPT – could support teaching and learning at universities.}
}
@article{MALOMO2024106108,
title = {Discontinuum models for the structural and seismic assessment of unreinforced masonry structures: a critical appraisal},
journal = {Structures},
volume = {62},
pages = {106108},
year = {2024},
issn = {2352-0124},
doi = {https://doi.org/10.1016/j.istruc.2024.106108},
url = {https://www.sciencedirect.com/science/article/pii/S2352012424002601},
author = {D. Malomo and B. Pulatsu},
keywords = {Unreinforced masonry, Structural analysis, Seismic analysis, Discontinuum analysis, Discrete Element Methods, DEM, AEM, NSCD, Computational modelling, Numerical modelling},
abstract = {In the last few decades, discontinuum (or discrete, discontinuous) numerical modelling strategies – i.e. those capable of representing the motion of multiple, intersecting discontinuities explicitly – have become increasingly popular for the structural and seismic assessment of unreinforced masonry (URM) structures. The automatic recognition of new contact points and prediction of large deformations up to complete separation are unique features of discontinuum-based models, making them particularly suitable for unit-by-unit simulations. The adaptation of discrete computational models, primarily used for analyzing rock mechanics and geomechanics problems, to the conservation, structural and earthquake engineering evaluation of URM assemblies is still ongoing, and recent advances in computer-aided technologies are accelerating significantly their adoption. Researchers have now developed fracture energy-based contact models tailored to unreinforced masonry mechanics, explored discontinuum analysis from the mortar joint- to the 3D building-level, combined discrete modelling strategies with analytical or continuum approaches, integrated the latest structural health monitoring and image-based developments into discontinuum-based analysis framework. Concurrently, new and still unsolved issues have also arisen, including the selection of appropriate damping schemes, degree of idealization and discretization strategies, identification of appropriate lab or onsite tests to infer meaningful equivalent mechanical input parameters. This paper offers to the research and industry communities an updated critical appraisal and practical guidelines on the use of discontinuum-based structural and seismic assessment strategies for URM structures, providing opportunities to uncover future key research paths. First, masonry mechanics and discontinuum-based idealization options are discussed by considering micro-, meso- and macro-scale modelling strategies. Pragmatic suggestions are provided to select appropriate input parameters essential to model masonry composite and its constituents at different scales. Then, discontinuum approaches are classified based on their formulation, focusing on the Distinct Element Method (DEM), Applied Element Method (AEM) and Non-Smooth Contact Dynamics (NSCD), and an overview of primary differences, capabilities, pros and cons are thoroughly discussed. Finally, previous discontinuum-based analyses of URM small-scale specimens, isolated planar or curved components, assemblies or complex structures are critically reviewed and compared in terms of adopted strategies and relevant outcomes. This paper presents to new and experienced analysts an in-depth summary of what modern discontinuum-based tools can provide to the structural and earthquake engineering fields, practical guidelines on implementing robust and meaningful modelling strategies at various scales, and potential future research directions.}
}
@article{MACK2000307,
title = {Long-term effects of building on informal knowledge in a complex content domain: the case of multiplication of fractions},
journal = {The Journal of Mathematical Behavior},
volume = {19},
number = {3},
pages = {307-332},
year = {2000},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(00)00050-X},
url = {https://www.sciencedirect.com/science/article/pii/S073231230000050X},
author = {Nancy K. Mack},
keywords = {Fractions, Informal knowledge, Long-term effects},
abstract = {Four students participated in a 2-year study (fifth and sixth grades) that examined the development of their understanding of multiplication of fractions. During both years, students received individualized instruction that encouraged them to build on their informal knowledge of partitioning to understand and solve problems involving multiplication of fractions. Students also received classroom instruction on algorithmic procedures for multiplication of fractions during the second year. In the long term, students consistently drew on their informal knowledge of partitioning to reconceptualize and partition units to solve problems involving multiplication of fractions in meaningful ways. At times, students' thinking was also dominated by their knowledge of algorithmic procedures for multiplication of fractions.}
}
@article{TERAMOTO201893,
title = {Betti number ratios as quantitative indices for bone morphometry in three dimensions},
journal = {Computer Methods and Programs in Biomedicine},
volume = {162},
pages = {93-98},
year = {2018},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2018.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0169260718300695},
author = {Takashi Teramoto and Takeshi Kamiya and Taira Sakurai and Fuminori Kanaya},
keywords = {Computational homology, Bone morphometry, Image processing},
abstract = {Background and objective: Computational homology is an emerging mathematical tool for characterizing shapes of data. In this work, we present a methodology using computational homology for obtaining quantitative measurements of the connectivity in bone morphometry. We introduce the Betti number ratios as novel morphological descriptor for the classification of bone fine structures in three dimensions.
Methods
A total of 51 Japanese white rabbits were used to investigate the connectivity of bone trabeculae after the administration of alendronate in a tendon graft model in rabbits. They were divided into a control group C and an experimental group A. Knee joints specimens were harvested for examination of their bone trabecular structure by micro-CT. Applying the computational homology software to the reconstructed 3D image data, we extract the morphological feature of a steric bone structure as the Betti numbers set (β0, β1, β2). The zeroth Betti number β0 indicates the number of the connected components corresponding to isolated bone fragments. The first and second Betti numbers, β1 and β2, indicate the numbers of open pores and closed pores of bone trabeculae, corresponding to a 2D empty space enclosed by a 1D curve and a 3D empty space enclosed by a 2D surface, respectively.
Results
We define the Betti number ratios β1/β0 and β2/β0 to better distinguish the two groups A and C in the scatter plots. Testing the discriminant function line for 29 data points of A (22 data points of C), the 17 points (resp. 18 points) are correctly classified into group A (resp. C). The accuracy rate is 35/51. The classification results in terms of the Betti number ratios are consistent with the histomorphometric measurements observed by medical doctors. Conclusions: This study is the first application of computational homology to bone morphometry in three dimensions. We show the mathematical basis of the Betti numbers index which are useful in a statistical description of the topological features of sponge-like structures. The potential benefits associated with our method include both improved quantification and reproducibility for the stereology.}
}
@article{MAIR201646,
title = {Briefing: Advanced sensing technologies for structural health monitoring},
journal = {Proceedings of the Institution of Civil Engineers - Forensic Engineering},
volume = {169},
number = {2},
pages = {46-49},
year = {2016},
issn = {2043-9903},
doi = {https://doi.org/10.1680/jfoen.16.00013},
url = {https://www.sciencedirect.com/science/article/pii/S2043990316000119},
author = {Robert J. Mair},
keywords = {buildings, structures & design, diaphragm & in situ walls, field testing & monitoring},
abstract = {The engineering, management, maintenance and upgrading of infrastructure requires fresh thinking to minimise the use of materials, energy and labour while still ensuring resilience. This can only be achieved by a full understanding of the performance of the infrastructure, both during its construction and throughout its design life, through the application of innovative sensor technologies and other emerging technologies. This approach covers the design and commissioning, construction process, exploitation and use and eventual decommissioning. Crucial elements of these emerging technologies are the innovative application of the latest sensor technologies, data management tools, manufacturing processes and supply chain management processes to the construction industry, both during infrastructure construction and throughout its design life. In particular, advances in sensing technologies such as fibre-optic sensors and wireless sensor networks for structural health monitoring have a huge potential to transform the approaches to the design, the construction and the operation of an infrastructure. This briefing article gives some examples of the recent application of these advanced sensing technologies to structural health monitoring for a large number of construction sites and existing infrastructure.}
}
@article{SALOMATIN2021582,
title = {Web user identification based on browser fingerprints using machine learning methods},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {13},
pages = {582-587},
year = {2021},
note = {20th IFAC Conference on Technology, Culture, and International Stability TECIS 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.10.512},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321019492},
author = {Alexander A. Salomatin and Andrey Y. Iskhakov and Anastasia O. Iskhakova},
keywords = {browser fingerprint, cybersecurity, identification, digital footprint, machine learning, web server},
abstract = {The article developed a method for identifying users on the network based on browser fingerprints using machine learning methods. The resulting method is a modification of the user identification method based on a digital footprint, which can be more efficient due to two components. First, the selection of attributes for a digital footprint is made from a limited set of attributes to form a user browser fingerprint. Secondly, the identification accuracy can be increased through the combined use of classification methods and the probabilistic-statistical approach. To check the successful operation of the method, a computational experiment is carried out on real data, which consists in solving the problem of classifying a user based on his browser fingerprint using the K nearest neighbors method.}
}
@article{DRESHER1990137,
title = {A computational learning model for metrical phonology},
journal = {Cognition},
volume = {34},
number = {2},
pages = {137-195},
year = {1990},
issn = {0010-0277},
doi = {https://doi.org/10.1016/0010-0277(90)90042-I},
url = {https://www.sciencedirect.com/science/article/pii/001002779090042I},
author = {B.Elan Dresher and Jonathan D. Kaye},
abstract = {One of the major challenges to linguistic theory is the solution of what has been termed the “projection problem”. Simply put, linguistics must account for the fact that starting from a data base that is both unsystematic and relatively small, a human child is capable of constructing a grammar that mirrors, for all intents and purposes, the adult system. In this article we shall address ourselves to the question of the learnability of a postulated subsystem of phonological structure: the stress system. We shall describe a computer program which is designed to acquire this subpart of linguistic structure. Our approach follows the “principles and parameters” model of Chomsky (1981a, b). This model is particularly interesting from both a computational point of view and with respect to the development of learning theories. We encode the relevant aspects of universal grammar (UG) - those aspects of linguistic structure that are presumed innate}
}
@article{WANG2024119836,
title = {Novel score function and standard coefficient-based single-valued neutrosophic MCDM for live streaming sales},
journal = {Information Sciences},
volume = {654},
pages = {119836},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.119836},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523014214},
author = {Fei Wang},
keywords = {Single-valued neutrosophic set, Score function, Standard coefficient, Multi-criteria decision making, Live streaming sales},
abstract = {Single-valued neutrosophic sets (SVNS) provide a comprehensive approach to express uncertainty in decision scenarios, surpassing the utility of fuzzy sets (FS) and intuitionistic fuzzy sets (IFS). Yet, current SVNS score function concepts stem from FS and IFS construction methods, showing inconsistencies. Thus, we introduce a novel SVNS score function based on inherent uncertainty essence. Additionally, we devise a standard coefficient to gauge SVNS standardization akin to fuzzy sets. Addressing SVNS researchability and limitations in fundamental concepts, especially the score function, we propose an SVNS-based multi-criteria decision-making (MCDM) model. This leverages the new score function and standard coefficient. We demonstrate its effectiveness on two decisions: “software engineer recruitment” with known weights and “investment selection” with unknown weights. Ultimately, we successfully applied the model to the field of live streaming sales to solve the actual MCDM problem. By comparing with existing methods, we affirm the model's validity and practicality. Compared to prior approaches, the new method exhibits: (1) Enhanced stability and credibility in result values and rankings, promoting robust optimal solutions. (2) Reduced computational steps and workload, enhancing usability and practicality.}
}
@article{BLAU2020100722,
title = {How does the pedagogical design of a technology-enhanced collaborative academic course promote digital literacies, self-regulation, and perceived learning of students?},
journal = {The Internet and Higher Education},
volume = {45},
pages = {100722},
year = {2020},
issn = {1096-7516},
doi = {https://doi.org/10.1016/j.iheduc.2019.100722},
url = {https://www.sciencedirect.com/science/article/pii/S1096751619304403},
author = {Ina Blau and Tamar Shamir-Inbal and Orit Avdiel},
keywords = {Digital literacy skills, Pedagogical design, Online communication and collaboration, Psychological ownership, Self-regulated learning strategies, Cognitive, emotional and social perceived learning},
abstract = {The wide expansion of digital technologies in higher education has introduced the need for an examination of the added value of various technological tools for quality teaching and active individual and collaborative learning. The current study explored whether and how the pedagogical design of an academic course, which developed a variety of digital literacy competencies, supported students in regulating collaborative technology-enhanced learning and helped them cope with the sense of psychological ownership over collaborative learning outcomes. In addition, we examined how these issues were expressed in cognitive, emotional and social aspects of students' perceived learning (Caspi & Blau, 2011). During four semesters, we conducted a qualitative analysis on reflective learning diaries, written by 78 graduate students studying education (N = 1870 codes). The bottom-up analysis focused on learning processes that enabled the development of various digital literacies conceptualized by the Digital Literacy Framework (DLF; Eshet-Alkalai, 2012): photo-visual, information, reproduction, branching, social-emotional, and real-time thinking skills. Furthermore, findings highlighted the importance of self-regulation and learning new technologies as an integral part of digital literacies. In addition, social-emotional statements expressed the development of effective communication and collaboration that enable students to cope with a sense of ownership over learning outcomes, and present different levels of teamwork: sharing, cooperation, and collaboration. Qualitative coding provided a more granulated perspective on perceived learning by differentiating between positive and negative aspects of emotional and social retrospection during the learning process. The findings contribute to educational theory by extending DLF and by providing new insights to the literature on students' perceived learning. We discuss the implications for instructional design and adoption of innovative pedagogy in higher education.}
}
@article{BUSCEMA2022112439,
title = {A nonlinear, data-driven, ANNs-based approach to culture-led development policies in rural areas: The case of Gjakove and Peć districts, Western Kosovo},
journal = {Chaos, Solitons & Fractals},
volume = {162},
pages = {112439},
year = {2022},
issn = {0960-0779},
doi = {https://doi.org/10.1016/j.chaos.2022.112439},
url = {https://www.sciencedirect.com/science/article/pii/S096007792200649X},
author = {Massimo Buscema and Guido Ferilli and Christer Gustafsson and Giulia Massini and Pier Luigi Sacco},
keywords = {Theory of impossible worlds, Culture, Cultural policy, Topologically weighted centroid, AutoCM, Kosovo},
abstract = {We develop a computational approach to the analysis of cultural vibrancy and to the role of the cultural and creative sectors in the socio-economic organization of two districts of Western Kosovo, Gjakove and Peć. Our analysis is built on a geolocalized mapping of the cultural activities and facilities, and on the main socio-economic variables for the two districts, and makes use of innovative data analysis techniques: Theory of Impossible Words (TIW), the Topological Weighted Centroid (TWC), and the AutoCM ANN. We find that the dynamics of cultural vibrancy of the territory is mainly driven by the competing attraction pulls of the nearby countries of Serbia and Albania, that also form the region's main and often conflicting ethnicities, and that such dynamics are likely to further polarize in the future. We also find that the cultural system plays a marginal role in the territory's socio-economic organization. This situation makes a case for a more active role of cultural policy in shaping future local developmental models in rural areas and in acting as an agent of social cohesion.}
}
@article{GILBOA202196,
title = {The complexity of the consumer problem},
journal = {Research in Economics},
volume = {75},
number = {1},
pages = {96-103},
year = {2021},
issn = {1090-9443},
doi = {https://doi.org/10.1016/j.rie.2021.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S1090944321000016},
author = {Itzhak Gilboa and Andrew Postlewaite and David Schmeidler},
keywords = {Consumer theory, Computational complexity, Mental accounting},
abstract = {A literal interpretation of neo-classical consumer theory suggests that the consumer solves a very complex problem. In the presence of indivisible goods, the consumer problem is NP-Hard, and it appears unlikely that it can be optimally solved by a human. Two implications of this observation are that (i) households may imitate each other’s choices; (ii) households may adopt heuristics that give rise to the phenomenon of mental accounting.}
}
@article{BRIGHT2020187,
title = {Applying computer algebra systems with SAT solvers to the Williamson conjecture},
journal = {Journal of Symbolic Computation},
volume = {100},
pages = {187-209},
year = {2020},
note = {Symbolic Computation and Satisfiability Checking},
issn = {0747-7171},
doi = {https://doi.org/10.1016/j.jsc.2019.07.024},
url = {https://www.sciencedirect.com/science/article/pii/S0747717119300902},
author = {Curtis Bright and Ilias Kotsireas and Vijay Ganesh},
keywords = {Williamson matrices, Boolean satisfiability, SAT solvers, Exhaustive search, Autocorrelation},
abstract = {We employ tools from the fields of symbolic computation and satisfiability checking—namely, computer algebra systems and SAT solvers—to study the Williamson conjecture from combinatorial design theory and increase the bounds to which Williamson matrices have been enumerated. In particular, we completely enumerate all Williamson matrices of even order up to and including 70 which gives us deeper insight into the behaviour and distribution of Williamson matrices. We find that, in contrast to the case when the order is odd, Williamson matrices of even order are quite plentiful and exist in every even order up to and including 70. As a consequence of this and a new construction for 8-Williamson matrices we construct 8-Williamson matrices in all odd orders up to and including 35. We additionally enumerate all Williamson matrices whose orders are divisible by 3 and less than 70, finding one previously unknown set of Williamson matrices of order 63.}
}
@article{MCCARTHY2019152,
title = {Shaking Gordon Wilson Flats: early seismic engineering research in New Zealand},
journal = {Proceedings of the Institution of Civil Engineers - Engineering History and Heritage},
volume = {172},
number = {4},
pages = {152-163},
year = {2019},
issn = {1757-9430},
doi = {https://doi.org/10.1680/jenhh.18.00030},
url = {https://www.sciencedirect.com/science/article/pii/S1757943019000066},
author = {Christine McCarthy},
keywords = {buildings, structures & design, history, seismic engineering},
abstract = {In the late 1950s and early 1960s, Gordon Wilson Memorial Flats (Wellington, New Zealand, 19551959) was instrumented for seismic engineering research and subjected to vibration testing. The research was prompted by new thinking about architectural design in the mid-twentieth century (i.e. modernism) that had caused a mismatch between structural assumptions in building codes (which relied on significant amounts of uncalculated stiffness inherent in 1920s building design) and the structural characteristics of new buildings that had, for example, greater areas of glazing. This type of research led to the revision of New Zealand building codes in the 1960s and informed Japanese processes for permitting buildings higher than 100 ft (305m). This paper outlines the research conducted and provides the context for understanding its significance. It is particularly topical given current proposals to instrument 400 Wellington buildings, creating the highest density of seismic instrumentation in any city.}
}
@article{ZHU20207240,
title = {A New Distribution-Free Concept for Representing, Comparing, and Propagating Uncertainty in Dynamical Systems with Kernel Probabilistic Programming⁎⁎This project has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 798321, the German Federal Ministry for Economic Affairs and Energy (BMWi) via eco4wind (0324125B) and DyConPV (0324166B), and by DFG via Research Unit FOR 2401.},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {7240-7247},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.557},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320308569},
author = {Jia-Jie Zhu and Krikamol Muandet and Moritz Diehl and Bernhard Schölkopf},
keywords = {Uncertainty Quantification, Machine Learning, Kernel Methods, Nonparametric Methods, Stochastic System Identification, Robust Control, Randomized Algorithms},
abstract = {This work presents the concept of kernel mean embedding and kernel probabilistic programming in the context of stochastic systems. We propose formulations to represent, compare, and propagate uncertainties for fairly general stochastic dynamics in a distribution-free manner. The new tools enjoy sound theory rooted in functional analysis and wide applicability as demonstrated in distinct numerical examples. The implication of this new concept is a new mode of thinking about the statistical nature of uncertainty in dynamical systems.}
}
@article{ZHANG202044,
title = {From mathematical equivalence such as Ma equivalence to generalized Zhang equivalency including gradient equivalency},
journal = {Theoretical Computer Science},
volume = {817},
pages = {44-54},
year = {2020},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2019.07.027},
url = {https://www.sciencedirect.com/science/article/pii/S0304397519304621},
author = {Yunong Zhang and Min Yang and Binbin Qiu and Jian Li and Mingjie Zhu},
keywords = {Mathematical equivalence, Physical equivalency, Ma equivalence, Generalized Zhang equivalency, Gradient equivalency},
abstract = {The authors carried out time-varying problems solving (TVPS) including robot problems solving in 2001, and began to figure out the reasons for the problems solving via diverse layers. After eight years' thinking, i.e., in 2009, the authors began to manifest, put forward and carry out the thought of “physical equivalency”. By another eight years' practicing and experimenting, i.e., in 2017, the authors basically finished establishing the framework of Zhang equivalency. Now, it is the time to establish the complete theory in a brief manner. Therefore, concepts about mathematical equivalence simply termed equivalence are presented firstly including Ma equivalence (especially for robotics), and then concepts about physical equivalency simply termed equivalency are proposed. Meanwhile, concepts about Zhang equivalency as a kind of equivalency are further proposed, and concepts about gradient-dynamics equivalency simply termed gradient equivalency as a kind of equivalency are proposed as well. Furthermore, two specific applications are considered and investigated, which substantiate the efficacy of Zhang equivalency.}
}
@article{SIMS202226,
title = {Externalized memory in slime mould and the extended (non-neuronal) mind},
journal = {Cognitive Systems Research},
volume = {73},
pages = {26-35},
year = {2022},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2021.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041721000954},
author = {Matthew Sims and Julian Kiverstein},
keywords = {Extended mind, Slime mould, Navigational memory, Basal cognition, Stigmergy, Sensorimotor coordination},
abstract = {The hypothesis of extended cognition (HEC) claims that the cognitive processes that materially realise thinking are sometimes partially constituted by entities that are located external to an agent’s body in its local environment. We show how proponents of HEC need not claim that an agent must have a central nervous system, or physically instantiate processes organised in such a way as to play a causal role equivalent to that of the brain if that agent is to be capable of cognition. Focusing on the case of spatial memory, we make our argument by taking a close look at the striking example of Physarum Polycephalum plasmodium (i.e., slime mould) which uses self-produced non-living extracellular slime trails to navigate its environment. We will argue that the use of externalized spatial memory by basal organisms like Physarum is an example of extended cognition. Moreover, it is a possible evolutionary precursor to the use of internal spatial memory and recall in animals thus demonstrating how extended cognition may have emerged early in evolutionary history.}
}
@article{LOMBAERS1987387,
title = {Computational techniques in operations research: A.M. Andrew Computer Language & Programming Series, Abacus, Tunbridge Wells, 1985, viii + 201 pages, £14.95},
journal = {European Journal of Operational Research},
volume = {31},
number = {3},
pages = {387-388},
year = {1987},
note = {Methodology for Public Decision-Making Interactive Decision Support Systems Queue and Game Theory},
issn = {0377-2217},
doi = {https://doi.org/10.1016/0377-2217(87)90050-6},
url = {https://www.sciencedirect.com/science/article/pii/0377221787900506},
author = {H.J.M. Lombaers}
}
@article{DELGIUDICE201644,
title = {The evolutionary future of psychopathology},
journal = {Current Opinion in Psychology},
volume = {7},
pages = {44-50},
year = {2016},
note = {Evolutionary psychology},
issn = {2352-250X},
doi = {https://doi.org/10.1016/j.copsyc.2015.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S2352250X15001888},
author = {Marco {Del Giudice}},
abstract = {Evolutionary approaches to psychopathology have made considerable progress over the last years. In this paper, I review recent advances in the field focusing on three core themes: the role of trade-offs and conflicts in the origins mental disorders, the evolution of developmental mechanisms, and the emergence of alternative classification systems based on life history theory. I situate these advances in the context of current research in psychopathology, and highlight their connections with other innovative approaches such as developmental psychopathology and computational psychiatry. In total, I argue that evolutionary psychopathology offers an integrative framework for the study of mental disorders, and allows complementary approaches to connect and cross-fertilize.}
}
@incollection{QUEK20021831,
title = {49 - Tap: An Inquiry Teaching Shell Using Both Rule-Based and State-Space Approaches},
editor = {Cornelius T. Leondes},
booktitle = {Expert Systems},
publisher = {Academic Press},
address = {Burlington},
pages = {1831-1895},
year = {2002},
isbn = {978-0-12-443880-4},
doi = {https://doi.org/10.1016/B978-012443880-4/50093-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780124438804500934},
author = {C. Quek and L.H. Wong and C.K. Looi},
abstract = {Publisher Summary
Inquiry teaching is similar to a scientific discovery process. The essence of inquiry teaching lies in the “higher-level Thinking” or “scientific-systematic reasoning” that helps in coping with and making proper use of any causal, rule-based, and procedural knowledge. The core of the process of inquiry teaching is the loop consisting of “forming-debugging hypothesis” and “testing hypothesis.” Once a hypothesis is successfully verified, one exits the loop and proceeds to study the generalizations or the applications of the rule. Inquiry teaching can be divided into three inquiry levels characterized by some degree of expectation about the students' performance. At level 1, students are guided to go through the form-debug-test loop of inquiry teaching for learning each concept. The hypothesizing and testing procedures-experiments-methods are based on processes predefined by the teacher or the intelligent tutoring system (ITS). The students learn the “core loop” of systematic method. At level 2, students experience the core loop while learning a rule. On verifying the rule, they proceed to learn about how to “make novel predictions” based on the rule and/or “apply the rule in the real world.” The aim at this level is to familiarize the student with the complete systematic reasoning process. Inquiry level 3 is introduced as a hypothetical level, where students encounter all the steps involved at level 2 but there are no predefined procedures. Students are required to propose their own methods for hypothesizing and testing. The main challenge at this level is how to validate the proposed methods. A human teacher is able to carry out such a task, whereas, an ITS is not. Building a computer system that has the intelligence to understand, analyze, and critique a user-defined method is an important computational problem.}
}
@article{JABEEN2023108475,
title = {Deep learning-based prediction of inhibitors interaction with Butyrylcholinesterase for the treatment of Alzheimer's disease},
journal = {Computers and Electrical Engineering},
volume = {105},
pages = {108475},
year = {2023},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.108475},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622006905},
author = {Farah Jabeen and Zia Ur Rehman and Sajid Shah and Rima D. Alharthy and Saquib Jalil and Imtiaz Ali Khan and Jamshed Iqbal and Ahmed A. Abd El-Latif},
keywords = {Deep learning, Machine learning, Personalized and precision medicine, Alzheimer's disease, Computer-aided diagnosis and detection},
abstract = {Butyrylcholinesterase (BChE) is a significant pharmaceutical drug for treating Alzheimer's disease (AD) . Thanks to the computational methods as which decreases significantly the overhead for screening BChE inhibitors. However, some of them have used one-hot encoding which ignores the sequential information. In this study, Term Frequency-Inverse Document Frequency (TF-IDF) is used for encoding SMILES expressions and Long Short-Term Memory (LSTM) for classification to preserve sequential information. Apart from LSTM, different models were used to evaluate the discriminative power of TF-IDF and to show the significance of sequential information. The dataset used in this study con-sists of 4,515 records of BChE inhibitors and non-inhibitors in the form of SMILES. The results obtained by the machine learning models were tested through invitro activity assays as well. The molecular docking study further confirmed the binding modes inside the BChE. The LSTM model showed 98.20% testing ac-curacy for the prediction of BChE inhibitors.}
}
@article{CHA2011990,
title = {Measuring achievement of ICT competency for students in Korea},
journal = {Computers & Education},
volume = {56},
number = {4},
pages = {990-1002},
year = {2011},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2010.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0360131510003246},
author = {Seung Eun Cha and Soo Jin Jun and Dai Yong Kwon and Han Sung Kim and Seung Bum Kim and Ja Mee Kim and Young Ae Kim and Sun Gwan Han and Soon Sik Seo and Woo Cheon Jun and Hyun Cheol Kim and Won Gyu Lee},
keywords = {Elementary education, Pedagogical issues, Teaching/learning strategies},
abstract = {In the current information society, the need for securing human resources acquired with ICT competency is becoming a very important issue. In USA, England, Japan, India and Israel improving students’ ICT competency has become a pedagogical issue. Accordingly, education on ICT competency is changing in many countries emphasizing the basis of computer science. The Korean government revised the ICT curriculum of 2001 focused on the basic concepts and principles of computer science as educational policy in 2005. However, it is still difficult to determine a student’s ICT competency level and the outcome of ICT curriculum based on changed direction. Thereupon, this study has developed test tool for measuring the level of Korean elementary school students’ ICT competency based on computer science. In this study, ‘Content’ and ‘Information processing’ are established as the two axes of the test frame standard through literature research, consideration and discussion. The validity and reliability of questions are verified though the preliminary test and the main test tool has completed through question revisions considering the distribution of answers. About 40,000 students, roughly 1% of the total elementary school students, are selected for the main test. There were several findings made in this study. Korea’s elementary school students have a weakness in ‘algorithm and modeling’. Information processing stage has been found to vary by grade. A modified ‘Angoff method’ is used to confirm the spread of the ICT competency levels of the target students. From the results, the cutoff score employed to divide the subjects into three levels, excellent, average and below average, the ratio of excellent levels decreases and the ratio of below average increases in higher grades. To solve these problems, we need to emphasize algorithmic thinking oriented more principal of computer science in ICT curriculum. For more effective ICT elementary education, teaching and learning strategies appropriate for young children to teach computer science should be introduced.}
}
@article{KANGASHARJU2022100048,
title = {Lower secondary students’ poetry writing with the AI-based Poetry Machine},
journal = {Computers and Education: Artificial Intelligence},
volume = {3},
pages = {100048},
year = {2022},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2022.100048},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X22000030},
author = {Arja Kangasharju and Liisa Ilomäki and Minna Lakkala and Auli Toom},
keywords = {AI-based learning, Lower secondary education, Technology in education, Poetry writing, AI-supported writing},
abstract = {Despite poetry’s important role in improving linguistic skills and creative thinking, students often find poetry writing to be difficult and boring. This study is an investigation of how the digital Poetry Machine influences students’ poetry writing by applying AI techniques. It uses qualitative and quantitative analysis of the log data of poems that the seventh graders wrote with the Poetry Machine. The results show that the draft poems functioned as affordances, which the students followed as models. The drafts encouraged students to experiment with several different poetic features. The data suggest an association between the number of edited versions and the quality of the final poem. The results suggest that a co-creative AI-based tool inspires and supports those students who engage in the writing process, and the poems are developed from the first versions. More studies regarding the role of AI based digital tools in developing students’ writing competencies would be worthwhile.}
}
@article{MAMOLO2022101014,
title = {Coding and climate change: Investigating prospective teachers’ pathways of attention},
journal = {The Journal of Mathematical Behavior},
volume = {68},
pages = {101014},
year = {2022},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2022.101014},
url = {https://www.sciencedirect.com/science/article/pii/S0732312322000827},
author = {Ami Mamolo and Sheree Rodney and Diane Tepylo},
keywords = {Awareness, Curiosity, Climate change, Coding, Pathways of attention, Mathematics for social justice, Teacher education},
abstract = {This research is part of a broader research program that explores teacher educators’ mathematical knowledge. We examine the experiences, perceptions, and needs of prospective teachers as they navigate a complex set of new and interweaving ideas for how to teach mathematics with socially relevant and responsible connections. In doing so, we draw on Mason’s (1998) perspectives about the structure of attention and awareness for mathematics teaching, to investigate the pathways of attention of middle school prospective teachers in a technology-intensive undergraduate coding course. The research findings show that teachers face challenges when they try to navigate the interdisciplinary space of mathematics, technology and societal issues (climate change) and that curiosity acts as a potential stimulus for determining how each pathway is developed and sustained.}
}
@incollection{UTEVSKY2015231,
title = {Social Decision Making},
editor = {Arthur W. Toga},
booktitle = {Brain Mapping},
publisher = {Academic Press},
address = {Waltham},
pages = {231-234},
year = {2015},
isbn = {978-0-12-397316-0},
doi = {https://doi.org/10.1016/B978-0-12-397025-1.00185-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780123970251001858},
author = {A.V. Utevsky and S.A. Huettel},
keywords = {Altruism, Decision making, Frontal cortex, Mentalizing, Reward, Social, Temporoparietal junction, Theory of mind},
abstract = {Many decisions are made in a social context. These decisions may be interactive and involve cooperation or conflict, or they may be made individually but have consequences for others. The brain processes underlying social decisions are complex, requiring not only computations associated with valuation and comparison but also social-cognitive processes like inferring others' mental states and predicting their behaviors. Accordingly, progress in understanding social decision making comes from combinations of techniques, primarily from the fields of economics and neuroscience. A standard model has arisen that contends that interpreting social information relies on a network of brain regions – including the medial prefrontal cortex, the posterior cingulate cortex, and the temporoparietal junction – and that information in turn feeds into the brain's system for valuation. Here, we review different types of social decisions, as well as the neural regions underlying their components.}
}
@article{MENG2023116227,
title = {Efficient path planning for AUVs in unmapped marine environments using a hybrid local–global strategy},
journal = {Ocean Engineering},
volume = {288},
pages = {116227},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.116227},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823026112},
author = {Wenlong Meng and Ya Gong and Fan Xu and Pingping Tao and Pengbo Bo and Shiqing Xin},
keywords = {Path planning, Autonomous undersea vehicle, Unmapped obstacle environment, Rapidly exploring random tree, Dynamic step},
abstract = {The ability of autonomous undersea vehicles (AUVs) to plan paths in unknown marine environments is the precondition for executing complicated missions. However, existing path planning algorithms based on underwater sensing equipment often struggle to achieve efficient exploration and generate high-quality trajectories. In this paper, we introduce a novel approach to efficiently handle the challenge of AUV navigation under limited information. Our solution combines global and local planning techniques to generate optimized paths that guarantee collision-free and efficient operations. In global path planning, we incrementally use the rolling windows to make decisions on high-level path branching while utilizing waypoints from selected branches to refine the calculation of local paths for enhanced accuracy. We employ an efficient small-scale path search strategy at the local path computation level by leveraging sensor-detected environments. In this stage, we propose an advanced rapidly exploring random tree (RRT) algorithm called circle-RRT. By combining adaptive circle sampling with dynamic step sizes, this algorithm can significantly reduce the generation of redundant sampling points and improve the efficiency of local path planning. We evaluated the efficiency of our algorithm in unknown environments through simulations and compared it with previous leading methods.}
}
@incollection{HERNANDEZLEMUS2017251,
title = {Chapter 14 - Handling Big Data in Precision Medicine},
editor = {Mukesh Verma and Debmalya Barh},
booktitle = {Progress and Challenges in Precision Medicine},
publisher = {Academic Press},
pages = {251-268},
year = {2017},
isbn = {978-0-12-809411-2},
doi = {https://doi.org/10.1016/B978-0-12-809411-2.00014-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128094112000143},
author = {E. Hernández-Lemus and J. Espinal-Enríquez and R. García-Herrera},
keywords = {Big data, Cloud computing, Data confidentiality, High-throughput data, Individualized therapy, Meta-data, Precision medicine},
abstract = {Precision medicine looks for the integration of vast information data sets on the molecular and environmental origins of disease for the development of individualized, context-dependent diagnostics and therapies. To build predictive models of complex disease compliant with individual variability on genetic and socially determined conditions, it is necessary to have computationally efficient methods to handle, visualize, and integrate large data sets of different origins, in a multitude of formats and subject to different levels of confidentiality into a single framework. This involves the ability to comply with the big data paradigm under demanding conditions of performance and subject to time, computational power, and bioethical constraints. This is still an open problem; however, we can devise some ways in which big data analytics may join forces with bioinformatics, medical informatics, and computational systems biology in a fast and effective way, motivated with the success of approaches such as the one given by translational bioinformatics.}
}
@incollection{VODOVOTZ201557,
title = {Chapter 3.1 - Towards Translational Systems Biology of Inflammation},
editor = {Yoram Vodovotz and Gary An},
booktitle = {Translational Systems Biology},
publisher = {Academic Press},
address = {Boston},
pages = {57-61},
year = {2015},
isbn = {978-0-12-397884-4},
doi = {https://doi.org/10.1016/B978-0-12-397884-4.00008-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780123978844000082},
author = {Yoram Vodovotz and Gary An},
keywords = {Translational medicine, translational research, systems biology, computational biology, bioinformatics, molecular biology, genomics, proteomics, metabolomics, clinical trials},
abstract = {Having introduced the historical, methodological, procedural, and societal antecedents that have contributed to the Translational Dilemma, in this chapter we propose our strategy to overcoming the challenges associated with the Dilemma, a research program we call Translational Systems Biology. This investigative strategy is predicated on the use of dynamic computational modeling and associated computational methods of data analysis and aggregation to accelerate the Scientific Cycle with an explicit target of generating clinically actionable knowledge. Translational Systems Biology is firmly grounded in the fundamental scientific principles discussed in earlier chapters, with its computational component specifically designed to overcome the numerous factors previously identified as contributing to the Translational Dilemma. These factors include the challenge of integrating and synthesizing mechanistic knowledge in systems with known nonlinear dynamics, utilizing and analyzing high-throughput data in a manner that facilitates the construction and use of dynamic computational models, the use of computational models as means of dynamic knowledge representation to test and falsify mechanistic hypotheses, and the use of computational modeling to bridge experimental biology to clinical application through the execution of in silico clinical trials.}
}
@article{MA2024109594,
title = {Robust adaptive learning framework for semi-supervised pattern classification},
journal = {Signal Processing},
volume = {224},
pages = {109594},
year = {2024},
issn = {0165-1684},
doi = {https://doi.org/10.1016/j.sigpro.2024.109594},
url = {https://www.sciencedirect.com/science/article/pii/S0165168424002135},
author = {Jun Ma and Guolin Yu},
keywords = {Non-convex distance metric, Semi-supervised robust classification, Generalized adaptive robust loss function, Outliers, Kernel method},
abstract = {Hessian scatter regularized twin support vector machine (HSR-TSVM) employs hinge loss function and L2-distance metric, which makes it ineffective in dealing with outliers and noise data problems. Aiming to this problem, this paper a novel robust adaptive learning framework CL2,pHSR-TSVM is developed for semi-supervised classification tasks. In CL2,pHSR-TSVM, the generalized adaptive robust loss function Lδ(u) is first innovatively introduced to overcome the problem that hinge loss function is not sensitive to noise and outliers. Intuitively, Lδ(u) can improve the robustness of the model by selecting different robust loss functions for different learning tasks during the learning process via the adaptive parameter δ. Secondly, the robust distance metric capped L2,p-norm is introduced in CL2,pHSR-TSVM to reduce and eliminate the exaggerated influence of L2-distance metric on the learning process of outliers, especially when the outliers are far from the normal data distribution, by setting the appropriate parameters. Furthermore, to improve the computational efficiency of CL2,pHSR-TSVM, the fast CL2,pHSR-TSVM is presented for semi-supervised classification tasks. Finally, two effective algorithms are designed to solve our methods respectively, and the convergence and computational complexity are analyzed theoretically. Experimental results demonstrate the effectiveness and robustness of our methods.}
}
@article{REGIER201440,
title = {Task complexity and response certainty in discrete choice experiments: An application to drug treatments for juvenile idiopathic arthritis},
journal = {Journal of Behavioral and Experimental Economics},
volume = {50},
pages = {40-49},
year = {2014},
issn = {2214-8043},
doi = {https://doi.org/10.1016/j.socec.2014.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S221480431400038X},
author = {Dean A. Regier and Verity Watson and Heather Burnett and Wendy J. Ungar},
keywords = {Discrete choice experiment, Stated response certainty, Random heterogeneity, Sampling uncertainty, Dual-process thinking},
abstract = {Responding to a stated preference discrete choice experiment (DCE) is a complex task for respondents to undertake. Task complexity can induce response error, thereby decreasing the statistical precision of the econometric model. This study explores the link between task complexity and statistical precision as moderated by the level of thoughtful deliberation respondents exert when completing choice tasks. To do this, we make novel use of subjects’ certainty of response to DCE tasks as a measure of their deliberation. The distinction between intuitive and deliberate thought (System 1 and System 2, respectively) motivates how task complexity will differentially affect System 1 and System 2 respondents. The principle of utility balance in experimental design theory is used to understand how greater deliberation will increase task complexity, but will also improve statistical precision if respondents are engaging in System 2 processing. Our analyses find that increases in choice task utility balance decreases response certainty, and re-weighting the regression to favor respondents who are more uncertain of their choices increases the statistical precision of the econometric model.}
}
@article{GULOTTA2022106698,
title = {Life Cycle Assessment and Life Cycle Costing of unitized regenerative fuel cell: A systematic review},
journal = {Environmental Impact Assessment Review},
volume = {92},
pages = {106698},
year = {2022},
issn = {0195-9255},
doi = {https://doi.org/10.1016/j.eiar.2021.106698},
url = {https://www.sciencedirect.com/science/article/pii/S0195925521001487},
author = {Teresa Maria Gulotta and Roberta Salomone and Francesco Lanuzza and Giuseppe Saija and Giovanni Mondello and Giuseppe Ioppolo},
keywords = {Unitized regenerative proton exchange membrane fuel cells, Environmental impacts, Economic impacts, Hydrogen technologies, PEM devices},
abstract = {Unitized Regenerative Fuel Cell (URFC) is considered a promising green hydrogen technology for producing clean energy, but further research is needed to make it attractive for a wide range of sectors and applications. In particular, the environmental and economic implications related to the life cycle of this electrochemical device play a fundamental role in determining its attractiveness and potential for improvement, and Life Cycle Thinking (LCT) assessment methods are considered to be the most effective means to improve knowledge about these implications. In this context, the present article provides a systematic and bibliometric literature review analysis of Life Cycle Assessment (LCA) and Life Cycle Costing (LCC) studies applied to URFCs using proton exchange membrane (PEM) devices. The aim is to evaluate the state-of-the-art of implementations of LCT methods to this electrochemical device in order to highlight good practices and critical issues, referred to both technical and methodological data. A reference sample of 44 scientific articles is extracted from the Web of Science (WoS), Scopus, and ScienceDirect databases and analysed using two computational tools: VOS viewer and Microsoft Excel. This group of publications helped establish the development over the last few decades of some key themes: LCC and LCA studies applied on PEM and URFC, also extending the search to its main components (such as fuel cell and electrolyser) and its original shape (i.e., regenerative fuel cell). The results of the analysis are presented quantitatively and qualitatively. Regarding the technical issues, there is significant variability in environmental and economic impacts, given by the selected system boundaries, the final users, and the fuel used by the systems. Regarding the methodological issues, no consensus emerges on how to model the LCT studies according to functional units, system boundaries, type of data selected, or model environmental externalities. The analysis also highlights the strong need for a higher level of transparency and harmonization of LCAs and LCCs applied on PEM technologies in order to improve the comparability of the results of these assessments.}
}
@article{VALENAS2017102,
title = {Prediction of pre-exam state anxiety from ruminative disposition: The mediating role of impaired attentional disengagement from negative information},
journal = {Behaviour Research and Therapy},
volume = {91},
pages = {102-110},
year = {2017},
issn = {0005-7967},
doi = {https://doi.org/10.1016/j.brat.2017.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S0005796717300232},
author = {Sergiu P. Vălenaş and Aurora Szentágotai-Tătar and Ben Grafton and Lies Notebaert and Andrei C. Miu and Colin MacLeod},
keywords = {Anxiety, Rumination, Attentional biases},
abstract = {Rumination is a maladaptive form of repetitive thinking that enhances stress responses, and heightened disposition to engage in rumination may contribute to the onset and persistence of stress-related symptoms. However, the cognitive mechanisms through which ruminative disposition influences stress reactivity are not yet fully understood. This study investigated the hypothesis that the impact of ruminative disposition on stress reactivity is carried by an attentional bias reflecting impaired attentional disengagement from negative information. We examined the capacity of a measure of ruminative disposition to predict both attentional biases to negative exam-related information, and state anxiety, in students approaching a mid-term exam. As expected, ruminative disposition predicted state anxiety, over and above the level predicted by trait anxiety. Ruminative disposition also predicted biased attentional disengagement from, but not biased attentional engagement with, negative information. Importantly, biased attentional disengagement from negative information mediated the relation between ruminative disposition and state anxiety. These findings confirm that dispositional rumination is associated with difficulty disengaging attention from negative information, and suggest that this attentional bias may be one of the mechanisms through which ruminative disposition influences stress reactivity.}
}
@article{LIEBERMAN2020355,
title = {Comparison of intelligent transportation systems based on biocybernetic vehicle control systems},
journal = {Transportation Research Procedia},
volume = {50},
pages = {355-362},
year = {2020},
note = {XIV International Conference on Organization and Traffic Safety Management in Large Cities (OTS-2020)},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2020.10.042},
url = {https://www.sciencedirect.com/science/article/pii/S2352146520307900},
author = {Irina Lieberman and Pavel Klachek and Sergei Korjagin},
keywords = {intelligent transportation system, artificial intelligence vehicle, biocybernetic control system, sensor network, traffic safety, control module},
abstract = {Prompt thinking and quick reaction in complex dynamically changing traffic situations are determinant factors of road accidents. A key part of modern and promising intelligent transportation systems (ITSs) can be represented by VANET vehicular ad hoc network and its equivalents, whose nodes are represented by vehicles with installed special communication modules and new-generation intelligent on-board control systems. A concept of VANET network development is presented, which is based on biocybernetic systems of vehicle control and can serve as a starting point for building conceptually new ITSs and allow solving the primary ITS task at a totally new level, which lies in obtaining optimal prompt decisions (when driving a vehicle) in a short period of time, during which neither human no automated system can make a safe decision. We consider the architecture and basics of creating an intelligent on-board information-and-control module of a vehicle, based on the integration of a biocybernetic human-machine interface and elements of VANET vehicular ad hoc network. We also consider the basics of creating a bank of mathematical models as a central element of the intelligent on-board information-and-control module. Based on the analysis of completed experiments, we can conclude that the use of biocybernetic approaches based on sensor networks and corresponding applied vehicle control systems helps solving crucial traffic safety issues, namely: obtaining optimal prompt decisions (when driving a vehicle) in a short period of time, which significantly improves traffic safety and increases traffic intensity.}
}
@article{SCHORR2003465,
title = {Motion, speed, and other ideas that “should be put in books”},
journal = {The Journal of Mathematical Behavior},
volume = {22},
number = {4},
pages = {465-477},
year = {2003},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2003.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0732312303000531},
author = {Roberta Y. Schorr},
keywords = {Motion, Speed, Velocity, Graphs, Graphical representations},
abstract = {This paper focuses on a portion of a research project involving a group of inner-city middle school students who used SimCalc simulation software over the course of an entire school year to investigate ideas relating to graphical representations of motion and speed. The classroom environment was one in which students openly defended and justified their thinking as they actively explored and solved rich mathematical problems. The activities, generally speaking, involved functions that were intended to tap students’ real world intuitions as well as prior mathematical skills and understandings about speed, motion, and other graphical representations that underlie the mathematics of motion. Results indicate that these students did build ideas related to those concepts. This paper will provide documentation of the ways in which these students interpreted graphical representations involving linear and quadratic functions that are associated with constant and linearly changing velocities, respectively.}
}
@article{FACQUE2022281,
title = {Present bias in economic choice demonstrates increased cognitive fatigability of glioma patients},
journal = {Cortex},
volume = {151},
pages = {281-293},
year = {2022},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2022.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S0010945222000703},
author = {Valentine Facque and Antonius Wiehler and Emmanuelle Volle and Emmanuel Mandonnet and Mathias Pessiglione},
keywords = {Low-grade glioma, Fatigue, Impulsivity, Decision-making, Delay discounting, Cognitive control, Computational modelling},
abstract = {Fatigue is a frequent symptom in many clinical conditions that is still poorly understood despite having a major impact on quality of life. Here, we propose a novel approach using model-based analysis of choice behaviour to extract fatigue markers. We applied this approach to the case of low-grade glioma, with the aim of testing the hypothesis that fatigability in this condition may manifest as limited control over choice impulsivity. Patients with intact or resected glioma (n = 29) and matched healthy controls (n = 27) performed a series of behavioural tasks included in a 4 h-long neuropsychological assessment. Intertemporal choices, opposing smaller-sooner to larger-later monetary rewards, were intermixed with tasks designed to test cognitive and motor performance and to assess perceived fatigue with subjective ratings. All dependent variables were analysed with generalised linear models testing the main effects of group and time-on-task, as well as their interaction. While absent in standard measures of fatigue (subjective rating and objective performance), a significant group-by-time interaction was observed in the rate of impulsive choices: contrary to controls, patients developed a preference for the smaller-sooner option in the course of neuropsychological assessment. This preference shift was captured by computational modelling as an increase in the present bias, a parameter that assigns an additive bonus to immediate rewards. Thus, choice impulsivity was the only reliable marker that reflected the enhanced fatigability of patients relative to controls. These results suggest that the impact of glioma (or its resection) on brain functioning limits the exertion of cognitive control during decision-making. More generally, they pave the way to using model-based analysis of choice behaviour for future investigations of the many clinical conditions plagued with cognitive fatigue.}
}
@article{ALGERAFI202461,
title = {Designing of an effective e-learning website using inter-valued fuzzy hybrid MCDM concept: A pedagogical approach},
journal = {Alexandria Engineering Journal},
volume = {97},
pages = {61-87},
year = {2024},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2024.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S1110016824003867},
author = {Mohammed Abdulwahab Al-Gerafi and Shankha Shubhra Goswami and Mohammad Amir Khan and Quadri Noorulhasan Naveed and Ayodele Lasisi and Abdulaziz AlMohimeed and Ahmed Elaraby},
keywords = {E-Learning, Pedagogy, IVF, COPRAS, EDAS, PIV, MCDM},
abstract = {The demand for effective e-learning platforms requires prioritizing pedagogical excellence in online educational websites. Current approaches struggle with uncertainties, hindering optimal e-learning environments due to a lack of comprehensive evaluation in traditional methods. An integrated approach is crucial to avoid inefficiencies and incomplete understanding of learner needs. This research introduces a pioneering methodology integrating Inter-Valued Fuzzy (IVF) COPRAS-EDAS-PIV hybrid Multiple Criteria Decision-Making (MCDM) techniques, addressing existing limitations. Leveraging the IVF concept allows a holistic assessment of pedagogical parameters, ensuring a thorough understanding of the decision-making landscape. The study involves an extensive literature review, parameter identification, and data acquisition through group decision-making. The selection of a suitable e-learning website is based on seven conflicting parameters, and preference ranking orders are prescribed using EDAS, COPRAS, and PIV MCDM model. Rigorous analysis using these techniques facilitates precise ranking and informed decision-making. The findings underscore the efficacy of the proposed IVF-MCDM approach for the design of a pedagogical e-learning website. Final results reveal that alternative 5 as the most preferable, followed by alternative 2, while alternative 3 is the least favored option among the group. Comparative and sensitivity analyses validate the approach’s superiority, enabling stakeholders to make well-informed decisions for optimal e-learning websites that cater to diverse learner needs, thus enhancing the overall online learning experience.}
}
@article{LI2012276,
title = {Predicting sRNAs and Their Targets in Bacteria},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {10},
number = {5},
pages = {276-284},
year = {2012},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2012.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1672022912000745},
author = {Wuju Li and Xiaomin Ying and Qixuan Lu and Linxi Chen},
keywords = {Bacterial, sRNA, Target, Bioinformatics, Prediction},
abstract = {Bacterial small RNAs (sRNAs) are an emerging class of regulatory RNAs of about 40–500 nucleotides in length and, by binding to their target mRNAs or proteins, get involved in many biological processes such as sensing environmental changes and regulating gene expression. Thus, identification of bacterial sRNAs and their targets has become an important part of sRNA biology. Current strategies for discovery of sRNAs and their targets usually involve bioinformatics prediction followed by experimental validation, emphasizing a key role for bioinformatics prediction. Here, therefore, we provided an overview on prediction methods, focusing on the merits and limitations of each class of models. Finally, we will present our thinking on developing related bioinformatics models in future.}
}
@article{KOU2023109788,
title = {Infrared small target segmentation networks: A survey},
journal = {Pattern Recognition},
volume = {143},
pages = {109788},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109788},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004867},
author = {Renke Kou and Chunping Wang and Zhenming Peng and Zhihe Zhao and Yaohong Chen and Jinhui Han and Fuyu Huang and Ying Yu and Qiang Fu},
keywords = {Infrared small target, Characteristic analysis, Segmentation network, Deep learning, Collaborative technology, Data-driven, False alarm, Missed detection},
abstract = {Fast and robust small target detection is one of the key technologies in the infrared (IR) search and tracking systems. With the development of deep learning, there are many data-driven IR small target segmentation algorithms, but they have not been extensively surveyed; we believe our proposed survey is the first to systematically survey them. Focusing on IR small target segmentation tasks, we summarized 7 characteristics of IR small targets, 3 feature extraction methods, 8 design strategies, 30 segmentation networks, 8 loss functions, and 13 evaluation indexes. Then, the accuracy, robustness, and computational complexities of 18 segmentation networks on 5 public datasets were compared and analyzed. Finally, we have discussed the existing problems and future trends in the field of IR small target detection. The proposed survey is a valuable reference for both beginners adapting to current trends in IR small target detection and researchers already experienced in this field.}
}
@article{KENETT201879,
title = {Driving the brain towards creativity and intelligence: A network control theory analysis},
journal = {Neuropsychologia},
volume = {118},
pages = {79-90},
year = {2018},
note = {The neural bases of creativity and intelligence: common grounds and differences},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2018.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0028393218300010},
author = {Yoed N. Kenett and John D. Medaglia and Roger E. Beaty and Qunlin Chen and Richard F. Betzel and Sharon L. Thompson-Schill and Jiang Qiu},
keywords = {Creativity, Intelligence, Network control theory, Cognitive control},
abstract = {High-level cognitive constructs, such as creativity and intelligence, entail complex and multiple processes, including cognitive control processes. Recent neurocognitive research on these constructs highlight the importance of dynamic interaction across neural network systems and the role of cognitive control processes in guiding such a dynamic interaction. How can we quantitatively examine the extent and ways in which cognitive control contributes to creativity and intelligence? To address this question, we apply a computational network control theory (NCT) approach to structural brain imaging data acquired via diffusion tensor imaging in a large sample of participants, to examine how NCT relates to individual differences in distinct measures of creative ability and intelligence. Recent application of this theory at the neural level is built on a model of brain dynamics, which mathematically models patterns of inter-region activity propagated along the structure of an underlying network. The strength of this approach is its ability to characterize the potential role of each brain region in regulating whole-brain network function based on its anatomical fingerprint and a simplified model of node dynamics. We find that intelligence is related to the ability to “drive” the brain system into easy to reach neural states by the right inferior parietal lobe and lower integration abilities in the left retrosplenial cortex. We also find that creativity is related to the ability to “drive” the brain system into difficult to reach states by the right dorsolateral prefrontal cortex (inferior frontal junction) and higher integration abilities in sensorimotor areas. Furthermore, we found that different facets of creativity—fluency, flexibility, and originality—relate to generally similar but not identical network controllability processes. We relate our findings to general theories on intelligence and creativity.}
}
@article{KELTYSTEPHEN2022104810,
title = {Turing’s cascade instability supports the coordination of the mind, brain, and behavior},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {141},
pages = {104810},
year = {2022},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2022.104810},
url = {https://www.sciencedirect.com/science/article/pii/S0149763422002998},
author = {Damian G. Kelty-Stephen and Madhur Mangalam},
keywords = {Criticality, Dynamic touch, Effortful touch, Executive function, Fractality, Multifractality, Multiscale, Multiplicativity, Multimodal perception, Neural avalanche, Perception and action, Posture, Power-law},
abstract = {Turing inspired a computer metaphor of the mind and brain that has been handy and has spawned decades of empirical investigation, but he did much more and offered behavioral and cognitive sciences another metaphor—that of the cascade. The time has come to confront Turing’s cascading instability, which suggests a geometrical framework driven by power laws and can be studied using multifractal formalism and multiscale probability density function analysis. Here, we review a rapidly growing body of scientific investigations revealing signatures of cascade instability and their consequences for a perceiving, acting, and thinking organism. We review work related to executive functioning (planning to act), postural control (bodily poise for turning plans into action), and effortful perception (action to gather information in a single modality and action to blend multimodal information). We also review findings on neuronal avalanches in the brain, specifically about neural participation in body-wide cascades. Turing’s cascade instability blends the mind, brain, and behavior across space and time scales and provides an alternative to the dominant computer metaphor.}
}
@article{SCHULZ20197,
title = {The algorithmic architecture of exploration in the human brain},
journal = {Current Opinion in Neurobiology},
volume = {55},
pages = {7-14},
year = {2019},
note = {Machine Learning, Big Data, and Neuroscience},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2018.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0959438818300904},
author = {Eric Schulz and Samuel J. Gershman},
abstract = {Balancing exploration and exploitation is one of the central problems in reinforcement learning. We review recent studies that have identified multiple algorithmic strategies underlying exploration. In particular, humans use a combination of random and uncertainty-directed exploration strategies, which rely on different brain systems, have different developmental trajectories, and are sensitive to different task manipulations. Humans are also able to exploit sophisticated structural knowledge to aid their exploration, such as information about correlations between options. New computational models, drawing inspiration from machine learning, have begun to formalize these ideas and offer new ways to understand the neural basis of reinforcement learning.}
}
@article{MITTERAUER199899,
title = {An interdisciplinary approach towards a theory of consciousness},
journal = {Biosystems},
volume = {45},
number = {2},
pages = {99-121},
year = {1998},
issn = {0303-2647},
doi = {https://doi.org/10.1016/S0303-2647(97)00070-1},
url = {https://www.sciencedirect.com/science/article/pii/S0303264797000701},
author = {Bernhard Mitterauer},
keywords = {Reflection processes, Glial–neuronal interaction, Glial boundary-setting function, Tree of reflection, Self-systems},
abstract = {Instead of attacking the difficult problem of consciousness or self-consciousness directly, the theory is based on the more basic concept of reflection. A concept of reflection is suggested on four levels (recursion, reflective thinking, self-reflection, intersubjective reflection). We propose the glial–neuronal interaction as a neurobiological substrate for reflection processes. It is assumed that glia have a boundary-setting function (scaffolding, compartmentalization) in the spatio–temporal interaction with the neurons. This function could be a possible mechanism of `dividing' the brain into different self-systems each with their own capacity of self-organization. Although the brain's different self-systems are normally integrated, they may disintegrate and show themselves in special states of the brain (e.g. multiple personality disorder). A tree of reflection consisting of a number of places (ontological loci) on which reflection processes of varying complexity take place, is suggested as the formal model. Finally, the problem of self-conscious qualitative experience (Qualia) is discussed in terms of the reflection model.}
}
@article{MAGNANI2004439,
title = {Reasoning through doing. Epistemic mediators in scientific discovery},
journal = {Journal of Applied Logic},
volume = {2},
number = {4},
pages = {439-450},
year = {2004},
note = {CMSRA},
issn = {1570-8683},
doi = {https://doi.org/10.1016/j.jal.2004.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S1570868304000448},
author = {Lorenzo Magnani},
keywords = {Epistemic mediators, Abduction, Manipulative reasoning, Optical diagrams, Morphodynamics of discovery},
abstract = {The recent epistemological and cognitive studies concentrate on the concept of abduction, as a means to originate and refine new ideas. Traditional cognitive science and computational accounts concerning abduction aim at illustration discovery and creativity processes in terms of theoretical and “internal” aspects, by means of computational simulations and/or abstract cognitive models. I will illustrate in this paper that some typical internal abductive processes are involved in scientific reasoning and discovery (for example through radical innovations). Nevertheless, especially concrete manipulations of the external world constitute a fundamental passage in science: by a process of manipulative abduction it is possible to build prostheses (epistemic mediators) for human minds, by interacting with external objects and representations in a constructive way. In this manner it is possible to create implicit knowledge through doing and to produce various opportunity to find, for example, anomalies and fruitful new risky perspectives. This kind of embodied and unexpressed knowledge holds a key role in the subsequent processes of scientific comprehension and discovery.}
}
@article{NABONI2017110,
title = {Thermal Comfort-CFD maps for Architectural Interior Design},
journal = {Procedia Engineering},
volume = {180},
pages = {110-117},
year = {2017},
note = {International High-Performance Built Environment Conference – A Sustainable Built Environment Conference 2016 Series (SBE16), iHBE 2016},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.04.170},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817316776},
author = {Emanuele Naboni and Daniel Sang-Hoon Lee and Kristian Fabbri},
keywords = {Building Retrofit, Thermal Comfort Map, Computational Fluid Dynamic, Mean Radiant Temperature, Architectural Design Process},
abstract = {Within the context of nearly Zero-Energy Buildings, it is debated that the energy-centred notion of design, proposed by regulatory frames, needs to be combined with a further focus toward users’ comfort and delight. Accordingly, the underlying theory of the research is that designers should take responsibility for understanding the heat flows through the building parts and its spaces. A design, which is sensible to the micro-thermal conditions coexisting in space, allows the inhabitants to control the building to their needs and desires: for instance, maximising the benefits of heat gain from the sun moving a series of internal partitions so as to avoid the danger of over-heating. It is thus necessary that existing simulation software tools are tested to the purpose of modelling and visualising the indoor thermal environment complexity. The research discusses how thermal comfort maps, which are prepared with the use of Computational Fluid Dynamic simulation method, could integrate energy simulation outputs to uphold qualitative architectural design decisions. Mean radiant temperature maps were thus used to design the retrofit of a small educational building in Copenhagen. The thermal opportunities of movable interior partitions (operated by the users) could be estimated, providing a new layer of information to the designer. The applicability of the thermal maps within an architectural design process is discussed adopting standard energy simulation comfort outputs as a reference. The capabilities and the limitations of the method are appraised.}
}
@article{STROM2024111887,
title = {Diborane anharmonic vibrational frequencies and Intensities: Experiment and theory},
journal = {Journal of Molecular Spectroscopy},
volume = {400},
pages = {111887},
year = {2024},
issn = {0022-2852},
doi = {https://doi.org/10.1016/j.jms.2024.111887},
url = {https://www.sciencedirect.com/science/article/pii/S0022285224000146},
author = {Aaron I. Strom and Ibrahim Muddasser and Guntram Rauhut and David T. Anderson},
keywords = {Matrix Isolation Spectroscopy, Anharmonic Vibrational Dynamics, Infrared Spectroscopy, Computational Spectroscopy, Diborane, Fermi Resonance, Darling-Dennison Resonance},
abstract = {The vibrational dynamics of diborane have been extensively studied both theoretically and experimentally ever since the bridge structure of diborane was established in the 1950s. Numerous infrared and several Raman spectroscopic studies have followed in the ensuing years at ever increasing levels of spectral resolution. In parallel, ab initio computations of the underlying potential energy surface have progressed as well as the methods to calculate the anharmonic vibration dynamics beyond the double harmonic approximation. Nevertheless, even 70 years after the bridge structure of diborane was established, there are still significant discrepancies between experiment and theory for the fundamental vibrational frequencies of diborane. In this work we use parahydrogen (pH2) matrix isolation infrared spectroscopy to characterize six fundamental vibrations of B2H6 and B2D6 and compare them with results from configuration-selective vibrational configuration interaction theory. The calculated frequencies and intensities are in very good agreement with the pH2 matrix isolation spectra, even several combination bands are well reproduced. We believe that the reason discrepancies have existed for so long is related to the large amount of anharmonicity that is associated with the bridge BH stretching modes. However, the calculated frequencies and intensities reported here for the vibrational modes of all three boron isotopologues of B2H6 and B2D6 are within ± 2.00 cm−1 and ± 1.44 cm−1, respectively, of the experimental frequencies and therefore a refined vibrational assignment of diborane has been achieved.}
}
@article{LI2023106688,
title = {Machine learning assisted advanced battery thermal management system: A state-of-the-art review},
journal = {Journal of Energy Storage},
volume = {60},
pages = {106688},
year = {2023},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2023.106688},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X23000853},
author = {Ao Li and Jingwen Weng and Anthony Chun Yin Yuen and Wei Wang and Hengrui Liu and Eric Wai Ming Lee and Jian Wang and Sanghoon Kook and Guan Heng Yeoh},
keywords = {Battery thermal management, Thermal runaway, Mitigation, Artificial neural networks, Machine learning},
abstract = {With an increasingly wider application of the lithium-ion battery (LIB), specifically the drastic increase of electric vehicles in cosmopolitan cities, improving the thermal and fire resilience of LIB systems is inevitable. Thus, in-depth analysis and performance-based study on battery thermal management system (BTMs) design have arisen as a popular research topic in energy storage systems. Among the LIB system parameters, such as battery temperature distribution, battery heat generation rate, cooling medium properties, electrical properties, physical dimension design, etc., multi-factor design optimisation is one of the most difficult experimental tasks. Computational simulations deliver a holistic solution to the BTMs design, yet it demands an immense amount of computational power and time, which is often not practical for the design optimisation process. Therefore, machine learning (ML) models play a non-substitute role in the safety management of battery systems. ML models aid in temperature prediction and safety diagnosis, thereby assisting in the early warning of battery fire and its mitigation. In this review article, we summarise extensive lists of literature on BTMs employing ML models and identify the current state-of-the-art research, which is expected to serve as a much-needed guideline and reference for future design optimisation. Following that, the application of various ML models in battery fire diagnosis and early warning is illustrated. Finally, the authors propose improved approaches to advanced battery safety management with ML. This review paper aims to bring new insights into the application of ML in the LIB thermal safety issue and BTMs design and anticipate boosting further advanced battery system design not limited to the thermal management system, as well as proposing potential digital twin modelling for BTMs.}
}
@article{ZHANG2022104630,
title = {Accurate band gap prediction based on an interpretable Δ-machine learning},
journal = {Materials Today Communications},
volume = {33},
pages = {104630},
year = {2022},
issn = {2352-4928},
doi = {https://doi.org/10.1016/j.mtcomm.2022.104630},
url = {https://www.sciencedirect.com/science/article/pii/S2352492822014714},
author = {Lingyao Zhang and Tianhao Su and Musen Li and Fanhao Jia and Shuobo Hu and Peihong Zhang and Wei Ren},
keywords = {2D materials, Bandgap, Machine learning, DFT calculation, Interpretable},
abstract = {Most materials science datasets are not so large that the accuracy of machine learning (ML) models is relatively limited if only simple features are used. Here, we constructed an interpretable ∆-machine learning (∆-ML) model to connect the hybrid functional HSE bandgap (EgHSE) with the PBE functional bandgap (EgPBE). The former can reproduce the band gap comparable with experiments, but the computational cost is much more challenging. The training is based on our high-throughput calculations on a set of two-dimensional semiconductors. Four complex descriptors, all based on the EgPBE are constructed using the sure independence screening and sparsifying operator (SISSO) algorithm. Using these descriptors, the ∆-ML can accurately predict the EgHSE of test set with a determination coefficient (R2) of 0.96. The error satisfies a normal distribution with a mean of zero. We provide a direct functional relationship between input descriptors and target properties. We find that EgHSE and the 5/6th power of EgPBE show a significant linear correlation, which may guide rapid prediction of EgHSE from EgPBE for materials with a EgHSE greater than 0.22 eV. We also discussed the correlation between the atomic radius and the EgHSE. Our work will provide an effective and interpretable model to construct the optimal physical descriptors for ML prediction on bandgaps in screening massive new 2D materials research.}
}
@incollection{KAMEDA2015441,
title = {Evolutionary Group Dynamics},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {441-447},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.81040-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868810405},
author = {Tatsuya Kameda and Mark {Van Vugt} and Robert Scott Tindale},
keywords = {Adaptation, Coordination, Evolution, Evolutionary psychology, Group cohesion, Group decision making, Group living, Human sociality, Intergroup relations, Psychological processes, Social brain, Social exchange, Social psychology, Status},
abstract = {Evolutionary psychology adds many insights to the literature on group dynamics and small-group processes. First, groups are a fundamental aspect of human evolution, suggesting that humans have evolved a range of adaptations to deal with specific threats and opportunities afforded by living in groups. Second, an evolutionary perspective integrates knowledge from numerous behavioral science disciplines such as psychology, evolutionary biology, primatology, biological anthropology, social neuroscience, and behavioral economics that are all concerned with group dynamics. Third, an evolutionary analysis produces many novel hypotheses about different aspects of our group psychology. We show the generativity of an evolutionary psychological approach through discussing examples of research applying evolutionary thinking to understanding key adaptive group tasks such as coordination, social exchange, status, group cohesion, collective decision making, and intergroup relations.}
}
@article{ARBIB201883,
title = {From cybernetics to brain theory, and more: A memoir},
journal = {Cognitive Systems Research},
volume = {50},
pages = {83-145},
year = {2018},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2018.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041718301360},
author = {Michael A. Arbib},
keywords = {Action-oriented perception, Ape, Architecture, Artificial intelligence, Automata theory, Basal ganglia, Brain theory, Cerebellum, Cerebral cortex, Cognitive science, Computational neuroscience, Cybernetics, Frog, Hippocampus, Human, Language evolution, Linguistics, Monkey, Rat, Robotics, Schema theory, Social implications, Systems theory, Theological implications},
abstract = {While structured as an autobiography, this memoir exemplifies ways in which classic contributions to cybernetics (e.g., by Wiener, McCulloch & Pitts, and von Neumann) have fed into a diversity of current research areas, including the mathematical theory of systems and computation, artificial intelligence and robotics, computational neuroscience, linguistics, and cognitive science. The challenges of brain theory receive special emphasis. Action-oriented perception and schema theory complement neural network modeling in analyzing cerebral cortex, cerebellum, hippocampus, and basal ganglia. Comparative studies of frog, rat, monkey, ape and human not only deepen insights into the human brain but also ground an EvoDevoSocio view of “how the brain got language.” The rapprochement between neuroscience and architecture provides a recent challenge. The essay also assesses some of the social and theological implications of this broad perspective.}
}
@article{DUCH201928,
title = {Mind as a shadow of neurodynamics},
journal = {Physics of Life Reviews},
volume = {31},
pages = {28-31},
year = {2019},
note = {Physics of Mind},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2019.01.023},
url = {https://www.sciencedirect.com/science/article/pii/S1571064519300971},
author = {Włodzisław Duch},
keywords = {Mind models, Neurodynamics, Physics of the mind, Mental spaces, Mental trajectories}
}
@incollection{MARINESCU2017113,
title = {Chapter 4 - Computer Clouds},
editor = {Dan C. Marinescu},
booktitle = {Complex Systems and Clouds},
publisher = {Elsevier},
address = {Boston},
pages = {113-145},
year = {2017},
series = {Computer Science Reviews and Trends},
isbn = {978-0-12-804041-6},
doi = {https://doi.org/10.1016/B978-0-12-804041-6.00004-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128040416000049},
author = {Dan C. Marinescu},
keywords = {Cloud delivery models, Hierarchical organization, Warehouse-scale computers, Over-provisioning, Cloud energy consumption, Cloud resource management, Cloud federations, Combinatorial auctions},
abstract = {Computer clouds have altered our thinking about computing and in this chapter, we first provide a down-to-earth view of the new paradigm and present the cloud delivery models. Next we discuss the hierarchical organization of the cloud infrastructure, consisting of multiple warehouse-scale computers. Cloud elasticity, the effects of over-provisioning on costs and energy consumption, and existing cloud resource management (CRM) policies and mechanisms for implementing these policies are analyzed. Alternative CRMs based on market mechanisms, such as auctions and server coalitions are then introduced. Combinatorial auctions allow access to packages of resources for applications with a complex workflow.}
}
@article{CAKIROGLU2023101279,
title = {A model to develop activities for teaching programming through metacognitive strategies},
journal = {Thinking Skills and Creativity},
volume = {48},
pages = {101279},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101279},
url = {https://www.sciencedirect.com/science/article/pii/S1871187123000494},
author = {Ünal ÇAKIROĞLU and Betül ER},
keywords = {Programming, Teaching programming, Metacognition, Problem solving},
abstract = {Among many endeavors to explore ways to effective teaching programming in classrooms, focusing on metacognition is somewhat neglected. This study suggests a framework for integrating metacognitive strategies to the programming teaching process. In order to achieve this, the knowledge and skills required for writing quality programs and the nature of the metacognition is interrelated with each other encompassing problem solving strategies with programming. The framework has two dimensions; one is metacognitive knowledge including conditional, procedure and declarative knowledge and the other is metacognitive regulation including planning, monitoring and evaluating activities embodied in a specialized worksheet. The activities were designed to be implemented in three phases of programming: at the beginning of the programming, during the programming and after finishing the programming. The suggested framework with the worksheet is pilot tested and validated to be used in the undergraduate introductory programming classrooms. We hope that the suggested roadmap may contribute to the instructional designers and researchers in the field of programming teaching and evaluation.}
}
@article{GUIMARAES2019242,
title = {Extension of Reward-Attention Circuit Model: Alcohol’s Influence on Attentional Focus and Consequences on Autism Spectrum Disorder},
journal = {Neurocomputing},
volume = {325},
pages = {242-253},
year = {2019},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2018.10.034},
url = {https://www.sciencedirect.com/science/article/pii/S0925231218312220},
author = {Karine Guimarães},
keywords = {Alcohol, Dopamine, Attention, Autism spectrum disorder, Computational neuroscience},
abstract = {Attention is a key element that allows us to enhance or decrease the cognitive processing of distinct stimuli, depending on their relevance. In this work we investigate the influence that alcohol exerts on attention focusing, modeling the coupling of reward and thalamocortical circuits. Computer simulations of the reward-attention circuit reflect the spiking behavior of each neuron in the network, under the presence or absence of alcohol. Each neuron in the neural networks that replicate such circuits is described by a carefully designed coupled system of nonlinear differential equations that details essential neurophysiological properties. The computational simulations highlight aspects of clinical inattention symptoms in the autism spectrum disorder. Our results indicate that alcohol may lead to distraction or lack of attentional focus. Also, the simulations suggest why people with ASD might relaxes enhanced attentional focus when exposed to alcohol.}
}
@article{MANNUCCI2023103265,
title = {Exploring potential futures: Evaluating the influence of deep uncertainties in urban planning through scenario planning: A case study in Rome, Italy},
journal = {Futures},
volume = {154},
pages = {103265},
year = {2023},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2023.103265},
url = {https://www.sciencedirect.com/science/article/pii/S0016328723001696},
author = {Simona Mannucci and Jan H. Kwakkel and Michele Morganti and Marco Ferrero},
keywords = {Scenario planning, Deep uncertainties, Urban planning, Decision-making, Climate adaptation},
abstract = {Cities play a critical role in developing adaptable strategies to address the challenges posed by climate change. However, the inherent complexity of urban environments and their uncertain future conditions necessitate exploring innovative approaches and tools to assist the current planning practices. This paper presents a workflow rooted in model-based scenario planning for long-term adaptation planning given uncertain futures. To demonstrate the workflow’s effectiveness, a pertinent case study was conducted in a flood-prone area of Rome. The study employed a land-use change model to examine potential urban growth patterns, considering the uncertain implementation of new poles of attraction. This interdisciplinary study constitutes an initial stride toward implementing uncertainty within urban planning frameworks. Future prospects encompass the integration of multiple models for cross-scale analysis, embracing further critical environmental and social aspects. This research contributes to advancing urban resilience strategies. It enhances the understanding of adapting to an uncertain future in the face of climate change, as urban areas must embrace comprehensive planning to ensure flexible adaptation when faced with climate-driven uncertainties in long-term planning. In conclusion, the study underscores that embracing uncertainty is a challenge and a pivotal opportunity to shape resilient and adaptable urban futures.}
}
@article{LI202261,
title = {Stochastic configuration networks for self-blast state recognition of glass insulators with adaptive depth and multi-scale representation},
journal = {Information Sciences},
volume = {604},
pages = {61-79},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.04.061},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522004133},
author = {Weitao Li and Qian Zhang and Dianhui Wang and Wei Sun and Qiyue Li},
keywords = {Self-blast state, Adaptive depth and multi-scale representation, Ensemble learning, Stochastic configuration networks, Feedback mechanism},
abstract = {The operating state of insulators is directly related to the stability of power transmission line. The existing methods for insulator state recognition cannot achieve satisfactory performance. In this paper, the self-blast state recognition of glass insulators is investigated by using an adaptive learning representation. To increase the adaptability of the network to different scales, we propose a solution based on multi-scale information throughout the entire process, beginning from a low-scale to high-scale subnetworks. The multi-scale information is aggregated in parallel way to take advantage of rich information representation. Then, an imitation of the human thinking pattern is employed. Utilizing entropy-based cost function, we update the parameters of the learner model in real-time. Based on the constraint of the evaluation index, adaptive depth representation for training glass insulators that are unsatisfied with the reliability evaluation is constructed to realize the self-optimizing regulation of feature space. Correspondingly, a stochastic configuration networks (SCNs) classifier is re-constructed to fit for the update multi-hierarchies knowledge space to carry out the re-recognition process. Finally, fuzzy integration is employed to ensemble multi-hierarchies network to improve the model’s generalization. The recognition results on aerial dataset of insulators images demonstrate the effectiveness of our proposed approach.}
}
@article{LAU2017241,
title = {The many worlds hypothesis of dopamine prediction error: implications of a parallel circuit architecture in the basal ganglia},
journal = {Current Opinion in Neurobiology},
volume = {46},
pages = {241-247},
year = {2017},
note = {Computational Neuroscience},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2017.08.015},
url = {https://www.sciencedirect.com/science/article/pii/S0959438817301587},
author = {Brian Lau and Tiago Monteiro and Joseph J Paton},
abstract = {Computational models of reinforcement learning (RL) strive to produce behavior that maximises reward, and thus allow software or robots to behave adaptively [1]. At the core of RL models is a learned mapping between ‘states’—situations or contexts that an agent might encounter in the world—and actions. A wealth of physiological and anatomical data suggests that the basal ganglia (BG) is important for learning these mappings [2, 3]. However, the computations performed by specific circuits are unclear. In this brief review, we highlight recent work concerning the anatomy and physiology of BG circuits that suggest refinements in our understanding of computations performed by the basal ganglia. We focus on one important component of basal ganglia circuitry, midbrain dopamine neurons, drawing attention to data that has been cast as supporting or departing from the RL framework that has inspired experiments in basal ganglia research over the past two decades. We suggest that the parallel circuit architecture of the BG might be expected to produce variability in the response properties of different dopamine neurons, and that variability in response profile may not reflect variable functions, but rather different arguments that serve as inputs to a common function: the computation of prediction error.}
}
@article{NGUYEN2015257,
title = {Identifiability Challenges in Mathematical Models of Viral Infectious Diseases**This work was supported by iMed - the Helmholtz Initiative on Pesonalized Medicine.},
journal = {IFAC-PapersOnLine},
volume = {48},
number = {28},
pages = {257-262},
year = {2015},
note = {17th IFAC Symposium on System Identification SYSID 2015},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2015.12.135},
url = {https://www.sciencedirect.com/science/article/pii/S2405896315027597},
author = {Van Kinh Nguyen and Esteban A. Hernandez-Vargas},
keywords = {parameter estimation, identifiability, viral infections},
abstract = {Nowadays, infections by viral pathogens are one of the biggest health threats to mankind. The development of new avenues of thinking to integrate the complexity of infectious diseases and the immune system is urgently needed. Recently mathematical modelling has emerged as a tool to interpret experimental results on quantitative grounds providing relevant insights to understand several infectious diseases. Nevertheless, modelling the complex mechanisms between viruses and the immune system can result in models with a large number of parameters to be estimated. Furthermore, experimental measurements have the problem to be sparse (in time) and highly noisy. Therefore, structural and practical identifiability are key obstacles to overcome towards mathematical models with predictive value. This paper addresses the identifiability limitations in the most common mathematical model to represent viral infections. Additionally, numerical simulations reveal how initial conditions of differential equations and fixing parameter values can alter the profile likelihood.}
}