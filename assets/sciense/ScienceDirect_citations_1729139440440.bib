@article{FEDORENKO2014120,
title = {Reworking the language network},
journal = {Trends in Cognitive Sciences},
volume = {18},
number = {3},
pages = {120-126},
year = {2014},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2013.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S136466131300288X},
author = {Evelina Fedorenko and Sharon L. Thompson-Schill},
keywords = {domain specificity, domain generality, language network, cognitive control, fMRI},
abstract = {Prior investigations of functional specialization have focused on the response profiles of particular brain regions. Given the growing emphasis on regional covariation, we propose to reframe these questions in terms of brain ‘networks’ (collections of regions jointly engaged by some mental process). Despite the challenges that investigations of the language network face, a network approach may prove useful in understanding the cognitive architecture of language. We propose that a language network plausibly includes a functionally specialized ‘core’ (brain regions that coactivate with each other during language processing) and a domain-general ‘periphery’ (a set of brain regions that may coactivate with the language core regions at some times but with other specialized systems at other times, depending on task demands). Framing the debate around network properties such as this may prove to be a more fruitful way to advance our understanding of the neurobiology of language.}
}
@incollection{SAWLEY1995181,
title = { - A serial data-parallel multi-block method for compressible flow computations},
editor = {A. Ecer and J. Hauser and P. Leca and J. Periaux},
booktitle = {Parallel Computational Fluid Dynamics 1993},
publisher = {North-Holland},
address = {Amsterdam},
pages = {181-188},
year = {1995},
isbn = {978-0-444-81999-4},
doi = {https://doi.org/10.1016/B978-044481999-4/50148-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780444819994501481},
author = {M.L. Sawley and J.K. Tegnér and C.M. Bergman},
abstract = {Publisher Summary
Block structured meshes not only provide the possibility to compute flows in complex geometries but also lend themselves in a natural way to coarse-grain parallel processing via the distribution of different blocks to different processors. Nevertheless, for some flow computations, a fine-grain data parallel implementation may be more appropriate. This chapter presents a study of such an implementation, which utilizes the simplicity of the data parallel approach. Particular attention is placed on a dynamic block management strategy that allows computations to be undertaken only in blocks where useful work is to be performed. The question of code portability among four different parallel computer systems is addressed in the chapter. This chapter concludes that the serial data-parallel multi-block method provides a number of advantages: (1) it retains the simplicity of the above-mentioned data parallel methods, because each block is treated individually in the same manner as for a single block computation; (2) it does not impose any parallelization constraints on the mesh generation procedure, in principle, any number of blocks of unequal size can be employed; the transfer of data between two blocks (block connectivity) is performed in a transparent manner via globally addressable memory contrasting with the explicit data transfer required by message passing implementations; (3) because individual blocks are treated sequentially, a simple dynamic block management algorithm can be applied to avoid performing unnecessary operations; and (4) the use of standard Fortran 90 facilitates code portability among different platforms supporting the data parallel programming method.}
}
@incollection{MILLER2023125,
title = {Chapter 7 - Graduate and postgraduate education at a crossroads},
editor = {Susan Miller and Walter Moos and Barbara Munk and Stephen Munk and Charles Hart and David Spellmeyer},
booktitle = {Managing the Drug Discovery Process (Second Edition)},
publisher = {Woodhead Publishing},
edition = {Second Edition},
pages = {125-155},
year = {2023},
series = {Woodhead Publishing Series in Biomedicine},
isbn = {978-0-12-824304-6},
doi = {https://doi.org/10.1016/B978-0-12-824304-6.00009-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128243046000092},
author = {Susan Miller and Walter Moos and Barbara Munk and Stephen Munk and Charles Hart and David Spellmeyer},
keywords = {Academia, Career, Critical thinking, Diversity, Education, Graduate school, Immigration, Industry, Jobs, Learn by doing, Medicinal chemistry, Online education, Organic chemistry, Pharmaceutical, Pharmacology, Postdoctoral, Postgraduate, Master’s degree, Doctorate},
abstract = {In this chapter, we introduce a proverbial crossroads in graduate and postgraduate education and jobs. We use medicinal chemistry as a core example for many topics, representative of what could also be said about pharmacology and other critical disciplines involved in drug discovery. Many factors are at play today for drug hunters, including an explosion of information, available now, at your fingertips, a move away from memorization toward critical thinking, the importance of learning by doing, and what has in the past been called “the gathering storm.” Core drug discovery disciplines are discussed, along with the importance of diversity and interdisciplinary skills and the value of academia-industry symbiosis. Challenges in making sure we continue to educate and engage the best and the brightest to tackle important biomedical problems are considered, especially in the context of personalized medicine and its interfaces with big data, bioinformatics, pharmacogenomics, and more. Finally, we scratch the surface on how to navigate graduate school, postdocs, employers, and careers.}
}
@article{KONG20241462,
title = {On locality of quantum information in the Heisenberg picture for arbitrary states},
journal = {Chinese Journal of Physics},
volume = {89},
pages = {1462-1473},
year = {2024},
issn = {0577-9073},
doi = {https://doi.org/10.1016/j.cjph.2024.04.028},
url = {https://www.sciencedirect.com/science/article/pii/S0577907324001655},
author = {Otto C.W. Kong},
keywords = {Quantum information, Quantum locality, Deutsch–Hayden descriptors, Noncommutative values of observables},
abstract = {The locality issue of quantum mechanics is a key issue to a proper understanding of quantum physics and beyond. What has been commonly emphasized as quantum nonlocality has received an inspiring examination through the notion of the Heisenberg picture of quantum information. Deutsch and Hayden established a local description of quantum information in a setting of quantum information flow in a system of qubits. With the introduction of a slightly modified version of what we call the Deutsch–Hayden matrix values of observables, together with our recently introduced parallel notion of the noncommutative values from a more fundamental perspective, we clarify all the locality issues based on such values as quantum information carried by local observables in any given arbitrary state of a generic composite system. Quantum information as the ‘quantum’ values of observables gives a transparent conceptual picture of all that. Spatial locality for a projective measurement is also discussed. The pressing question is if and how such information for an entangled system can be retrieved through local processes which can only be addressed with new experimental thinking.}
}
@incollection{OBRIEN2014141,
title = {7 - Reasoning with graphs},
editor = {Jamie O’Brien},
booktitle = {Shaping Knowledge},
publisher = {Chandos Publishing},
pages = {141-174},
year = {2014},
series = {Chandos Information Professional Series},
isbn = {978-1-84334-751-4},
doi = {https://doi.org/10.1533/9781780634326.141},
url = {https://www.sciencedirect.com/science/article/pii/B9781843347514500078},
author = {Jamie O’Brien},
keywords = {graph databases, logic and computing, reasoning, spatial data structures, visualization},
abstract = {Abstract:
Knowledge complexity poses a problem to the modeller of representation and, in turn, of reasoning. We seek to overcome this problem by using our ‘privileged’ sense of vision. This means that we render dynamic, multi-modal phenomena as graphic depictions, be they technical visualizations, thought experiments, logic constructions or network graphs. The ‘graphic act’ has been described a being a fundamental activity in human perception, and both scientists and artists have undertaken advanced analyses of the human perception of nature based on visual experiments. Analysis based on reasoning is similarly a graphic act, in the sense that it seeks out patterns of connectivity among socio-spatial agents and entities. Reasoning also depends upon symbolism, which serves to overcome the problem of infinity in nature (a matter that lies at the heart of machine computation). Hence, this chapter introduces some elements in logical reasoning, set theory and computation, and outlines the particular importance of working with symbols. It also provides an introduction to data modelling with graphs, including current advances in graph databases. It provides some ‘tools for thinking’ about knowledge complexity and suggests the potential power in adapting these technologies to organize knowledge of dynamic, complex domains. It also introduces some standard methods for spatial data modelling, including powerful surface network models, which borrow from physical landscape analysis, to support reasoning about knowledge-driven domains.}
}
@article{PETERSON20223586,
title = {Physical computing for materials acceleration platforms},
journal = {Matter},
volume = {5},
number = {11},
pages = {3586-3596},
year = {2022},
issn = {2590-2385},
doi = {https://doi.org/10.1016/j.matt.2022.09.022},
url = {https://www.sciencedirect.com/science/article/pii/S2590238522005409},
author = {Erik Peterson and Alexander Lavin},
keywords = {materials acceleration platforms, AI-driven science, simulation intelligence, physical computing, self-driving labs, inverse design, computational metamaterials},
abstract = {Summary
A “technology lottery” describes a research idea or technology succeeding over others because it is suited to available software/hardware and not necessarily because it is superior. The nascent field of self-driving laboratories, particularly materials acceleration platforms (MAPs), is at risk: while it is logical and opportunistic to inject existing lab equipment and workflows with artificial intelligence (AI) and automation, such MAPs can constrain research by proliferating existing biases in science, mechatronics, and general-purpose computing. Rather than conformity, MAPs present opportunity to pursue new vectors of engineering physics with advances in cyber-physical learning and closed-loop, self-optimizing systems. We outline a simulation-based MAP program to design computers that use physics to solve optimization problems: the physical computing (PC)-MAP can mitigate hardware-software-substrate-user information losses present in all other MAP classes and eliminate lotteries by perfecting alignment between computing problems and media. We describe early PC advances and research pursuits toward optimal design of new materials and computing media.}
}
@article{LIU2024105391,
title = {Exploring three pillars of construction robotics via dual-track quantitative analysis},
journal = {Automation in Construction},
volume = {162},
pages = {105391},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105391},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524001274},
author = {Yuming Liu and Aidi Hizami Bin Alias and Nuzul Azam Haron and Nabilah Abu Bakar and Hao Wang},
keywords = {Construction robotics, BERTopic model, BIM, Human–robot collaboration, Deep reinforcement learning, Dual-track quantitative analysis},
abstract = {Construction robotics has emerged as a leading technology in the construction industry. This paper conducts an innovative dual-track quantitative comprehensive method to analyze the current literature and assess future trends. First, a bibliometric review of 955 journal articles published between 1974 and 2023 was performed, exploring keywords, journals, countries, and clusters. Furthermore, a neural topic model based on BERTopic addresses topic modeling repetition issues. The study identifies building information modeling (BIM), human–robot collaboration (HRC), and deep reinforcement learning (DRL) as “three pillars” in the field. Additionally, we systematically reviewed the relevant literature and nested symbiotic relationships. The outcome of this study is twofold: first, the findings provide quantitative and qualitative scientific guidance for future research on trends; second, the innovative dual-track quantitative analysis research methodology simultaneously stimulates critical thinking about the modeling of other similarly trending topics characterized to avoid high degree of homogeneity and corpus overlap.}
}
@article{CARBONARO20101098,
title = {Computer-game construction: A gender-neutral attractor to Computing Science},
journal = {Computers & Education},
volume = {55},
number = {3},
pages = {1098-1111},
year = {2010},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2010.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0360131510001399},
author = {Mike Carbonaro and Duane Szafron and Maria Cutumisu and Jonathan Schaeffer},
keywords = {Computing Science, Females in Science, Computer game construction},
abstract = {Enrollment in Computing Science university programs is at a dangerously low level. A major reason for this is the general lack of interest in Computing Science by females. In this paper, we discuss our experience with using a computer game construction environment as a vehicle to encourage female participation in Computing Science. Experiments with game construction in grade 10 English classes showed that females enjoyed this activity as much as males and were just as successful. In this paper, we argue that: a) computer game construction is a viable activity for teaching higher-order thinking skills that are essential for Science; b) computer game construction that involves scripting teaches valuable Computing Science abstraction skills; c) this activity is an enjoyable introduction to Computing Science; and d) outcome measures for this activity are not male-dominated in any of the three aspects (higher-order thinking, Computing Science abstraction skills, activity enjoyment). Therefore, we claim that this approach is a viable gender-neutral approach to teaching Computing Science in particular and Science in general that may increase female participation in the discipline.}
}
@incollection{MAYER2010273,
title = {Problem Solving and Reasoning},
editor = {Penelope Peterson and Eva Baker and Barry McGaw},
booktitle = {International Encyclopedia of Education (Third Edition)},
publisher = {Elsevier},
edition = {Third Edition},
address = {Oxford},
pages = {273-278},
year = {2010},
isbn = {978-0-08-044894-7},
doi = {https://doi.org/10.1016/B978-0-08-044894-7.00487-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780080448947004875},
author = {R.E. Mayer},
keywords = {Convergent thinking, Creativity, Deductive reasoning, Directed thinking, Divergent thinking, Einstellung, Everyday thinking, Expert problem solving, Functional fixedness, Ill-defined problem, Inductive reasoning, Insight, Means-ends analysis, Nonroutine problem, Problem solving, Problem space, Productive thinking, Reasoning, Reproductive thinking, Routine problem, Thinking, Transfer, Well-defined problem},
abstract = {A major goal of education is to help students become effective problem solvers, that is, people who can generate useful and original solutions when they are confronted with problems they have never seen before. This article covers definitions of problem solving and reasoning, types of problems, cognitive processes and types of knowledge in problem solving, rigidity in thinking, problem-solving transfer, the distinction between productive and reproductive thinking, the nature of insight, problem space and search processes, and problem solving in realistic situations.}
}
@article{KITTAS2010401,
title = {Evolution of the rate of biological aging using a phenotype based computational model},
journal = {Journal of Theoretical Biology},
volume = {266},
number = {3},
pages = {401-407},
year = {2010},
issn = {0022-5193},
doi = {https://doi.org/10.1016/j.jtbi.2010.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0022519310003619},
author = {Aristotelis Kittas},
keywords = {Evolution, Aging, Computer simulations, Age-structured populations, Modelling},
abstract = {In this work I introduce a simple model to study how natural selection acts upon aging, which focuses on the viability of each individual. It is able to reproduce the Gompertz law of mortality and can make predictions about the relation between the level of mutation rates (beneficial/deleterious/neutral), age at reproductive maturity and the degree of biological aging. With no mutations, a population with low age at reproductive maturity R stabilizes at higher density values, while with mutations it reaches its maximum density, because even for large pre-reproductive periods each individual evolves to survive to maturity. Species with very short pre-reproductive periods can only tolerate a small number of detrimental mutations. The probabilities of detrimental (Pd) or beneficial (Pb) mutations are demonstrated to greatly affect the process. High absolute values produce peaks in the viability of the population over time. Mutations combined with low selection pressure move the system towards weaker phenotypes. For low values in the ratio Pd/Pb, the speed at which aging occurs is almost independent of R, while higher values favor significantly species with high R. The value of R is critical to whether the population survives or dies out. The aging rate is controlled by Pd and Pb and the amount of the viability of each individual is modified, with neutral mutations allowing the system more “room” to evolve. The process of aging in this simple model is revealed to be fairly complex, yielding a rich variety of results.}
}
@incollection{DUNBAR200113746,
title = {Scientific Reasoning and Discovery, Cognitive Psychology of},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {13746-13749},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/01602-8},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767016028},
author = {K. Dunbar},
abstract = {The cognitive mechanisms underlying scientific thinking and discovery have been investigated using approaches from cognitive psychology, cognitive science, and artificial intelligence. In this article, six overlapping approaches are discussed. First, historical analyses and interviews have provided important information on the types of thinking involved in particular discoveries or used by individual scientists. Second, scientific reasoning has been thought of as a form of inductive thinking, and as a form of problem solving. Researchers using this approach have delineated some of the problem solving and inductive reasoning strategies used in science. Third, much research on errors in scientific reasoning, particularly on the topic of ‘confirmation bias’ has revealed some of the circumstances under which science can go awry. Fourth, many researchers have investigated how children's thinking is similar to, or different from, that of scientists. A fifth approach has been to investigate scientists reasoning live or ‘in vivo’ in their own labs. This work has shown how processes such as analogy, distributed cognition, and specific types of inductive and deductive reasoning strategies are used together by scientists. Finally, the incorporation of cognitive mechanisms into computer programs that make discoveries is seen as an important development in the cognitive psychology of scientific thinking.}
}
@article{ROSSITER20083713,
title = {Compromises between feasibility and performance within linear MPC},
journal = {IFAC Proceedings Volumes},
volume = {41},
number = {2},
pages = {3713-3718},
year = {2008},
note = {17th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20080706-5-KR-1001.00627},
url = {https://www.sciencedirect.com/science/article/pii/S147466701639526X},
author = {J.A. Rossiter and Yihang Ding},
keywords = {Constraints, Feasibility, Performance, Computational Efficiency, Contours},
abstract = {This paper explores the issues of feasibility and performance within predictive control. Conventional thinking is that there is typically a trade off between performance and the volume of the feasible region. However, this paper seeks to show that the trade off is often not as stark as might be expected and in fact one can sometimes gain huge amounts in feasibility with an almost negligible loss in performance while using a simple and conventional MPC algorithm.}
}
@article{BELVEDERE201218,
title = {A computational index derived from whole-genome copy number analysis is a novel tool for prognosis in early stage lung squamous cell carcinoma},
journal = {Genomics},
volume = {99},
number = {1},
pages = {18-24},
year = {2012},
issn = {0888-7543},
doi = {https://doi.org/10.1016/j.ygeno.2011.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0888754311002424},
author = {Ornella Belvedere and Stefano Berri and Rebecca Chalkley and Caroline Conway and Fabio Barbone and Federica Pisa and Kenneth MacLennan and Catherine Daly and Melissa Alsop and Joanne Morgan and Jessica Menis and Peter Tcherveniakov and Kostas Papagiannopoulos and Pamela Rabbitts and Henry M. Wood},
keywords = {Lung cancer, Copy number, Survival, Next-generation sequencing},
abstract = {Squamous cell carcinoma of the lung is remarkable for the extent to which the same chromosomal abnormalities are detected in individual tumours. We have used next generation sequencing at low coverage to produce high resolution copy number karyograms of a series of 89 non-small cell lung tumours specifically of the squamous cell subtype. Because this methodology is able to create karyograms from formalin-fixed paraffin-embedded material, we were able to use archival stored samples for which survival data were available and correlate frequently occurring copy number changes with disease outcome. No single region of genomic change showed significant correlation with survival. However, adopting a whole-genome approach, we devised an algorithm that relates to total genomic damage, specifically the relative ratios of copy number states across the genome. This algorithm generated a novel index, which is an independent prognostic indicator in early stage squamous cell carcinoma of the lung.}
}
@article{MASHALEH20242245,
title = {IoT Smart Devices Risk Assessment Model Using Fuzzy Logic and PSO},
journal = {Computers, Materials and Continua},
volume = {78},
number = {2},
pages = {2245-2267},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.047323},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824001267},
author = {Ashraf S. Mashaleh and Noor Farizah Binti Ibrahim and Mohammad Alauthman and Mohammad Almseidin and Amjad Gawanmeh},
keywords = {IoT botnet detection, risk assessment, fuzzy logic, particle swarm optimization (PSO), cybersecurity, interconnected devices},
abstract = {Increasing Internet of Things (IoT) device connectivity makes botnet attacks more dangerous, carrying catastrophic hazards. As IoT botnets evolve, their dynamic and multifaceted nature hampers conventional detection methods. This paper proposes a risk assessment framework based on fuzzy logic and Particle Swarm Optimization (PSO) to address the risks associated with IoT botnets. Fuzzy logic addresses IoT threat uncertainties and ambiguities methodically. Fuzzy component settings are optimized using PSO to improve accuracy. The methodology allows for more complex thinking by transitioning from binary to continuous assessment. Instead of expert inputs, PSO data-driven tunes rules and membership functions. This study presents a complete IoT botnet risk assessment system. The methodology helps security teams allocate resources by categorizing threats as high, medium, or low severity. This study shows how CICIoT2023 can assess cyber risks. Our research has implications beyond detection, as it provides a proactive approach to risk management and promotes the development of more secure IoT environments.}
}
@article{DIAS2022140,
title = {Utilization of the Arena simulation software and Lean improvements in the management of metal surface treatment processes},
journal = {Procedia Computer Science},
volume = {204},
pages = {140-147},
year = {2022},
note = {International Conference on Industry Sciences and Computer Science Innovation},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.08.017},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922007554},
author = {A.S.M.E. Dias and R.M.G. Antunes and A. Abreu and V. Anes and H.V.G. Navas and T. Morgado and J.M.F. Calado},
keywords = {Process management, Arena software, Lean tools, Case study, Metal surface treatments},
abstract = {For companies to stand out in increasingly competitive, dynamic and global markets, they must have customer satisfaction goals, create value through their processes, products and services and also aim for innovation. In this context, computer sciences combined with engineering processes constitutes a powerful way for companies to be able to improve process management, to interact with such markets in an efficient and effective way. The main objective of this article is to use Arena simulation software, to quantitatively predict the impact of improvements applied in metal surface treatment processes, based on tools to support Lean thinking. A case study in a Portuguese company in the metalworking sector is presented, in which it is verified that the proposed improvements in terms of the factory layout and resource management, suggested by the comparison between simulations of the current state of the company and the improved one, streamline the processes of finishing in metals, namely zinc coating and lacquering which prevent the occurrence of oxidation and the consequent corrosion of the base metals, by adding other metals and materials to their surface, which adhere and protect it. Through the results obtained, it is concluded that the reduction of waiting times and transport of stocks without production and of work-in-progress, as well as the increase of the productive capacity, make the company more able to guarantee the satisfaction of the requirements of its customers and improve its positioning in the market compared to its competitors.}
}
@incollection{MILLER2023169,
title = {Chapter 9 - Doctoral and professional programs},
editor = {Susan Miller and Walter Moos and Barbara Munk and Stephen Munk and Charles Hart and David Spellmeyer},
booktitle = {Managing the Drug Discovery Process (Second Edition)},
publisher = {Woodhead Publishing},
edition = {Second Edition},
pages = {169-196},
year = {2023},
series = {Woodhead Publishing Series in Biomedicine},
isbn = {978-0-12-824304-6},
doi = {https://doi.org/10.1016/B978-0-12-824304-6.00013-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128243046000134},
author = {Susan Miller and Walter Moos and Barbara Munk and Stephen Munk and Charles Hart and David Spellmeyer},
keywords = {Basic/applied/clinical, Career/job, Collaboration/teams, Critical thinking, -index, PhD/PharmD, Postdoc/postdoctoral, Problem identification, Research design, Writing/publishing},
abstract = {In this chapter on graduate and professional education, we explore doctoral and professional programs, posing a number of key questions you should ask yourself. Where to apply to graduate school or a postdoc, and why? With whom should you work? A PhD or a PharmD? What must you do to be successful? Moreover, we touch on traits important to becoming an independent researcher and ask whether success in graduate school or a postdoctoral fellowship requires different skills than undergraduate degrees. Critical thinking habits underpin this discussion. We outline possible career choices—jobs!—touching on the knowledge and expertise used by drug hunters, and also ask what might be of most value to potential employers. Each of us is different, and what’s best for you is something you will have to decipher, but hopefully you will consult with family, friends, and advisors or mentors before making a final decision. Regardless, “the big leap” is coming, so get ready.}
}
@article{ZHANG2024100479,
title = {Open source implementations of numerical algorithms for computing the complete elliptic integral of the first kind},
journal = {Results in Applied Mathematics},
volume = {23},
pages = {100479},
year = {2024},
issn = {2590-0374},
doi = {https://doi.org/10.1016/j.rinam.2024.100479},
url = {https://www.sciencedirect.com/science/article/pii/S2590037424000499},
author = {Hong-Yan Zhang and Wen-Juan Jiang},
keywords = {Complete elliptic integral of the first kind (CEI-1), Algorithm design, Orthogonal polynomials, Verification-validation-testing (VVT), STEM education, Computer programming},
abstract = {The complete elliptic integral of the first kind (CEI-1) plays a significant role in mathematics, physics and engineering. There is no simple formula for its computation, thus numerical algorithms are essential for coping with the practical problems involved. The commercial implementations for the numerical solutions, such as the functions ellipticK and EllipticK provided by MATLAB and Mathematica respectively, are based on Kcs(m) instead of the usual form K(k) such that Kcs(k2)=K(k) and m=k2. It is necessary to develop open source implementations for the computation of the CEI-1 in order to avoid potential risks of using commercial software and possible limitations due to the unknown factors. In this paper, the infinite series method, arithmetic-geometric mean (AGM) method, Gauss–Chebyshev method and Gauss–Legendre methods are discussed in details with a top-down strategy. The four key algorithms for computing the CEI-1 are designed, verified, validated and tested, which can be utilized in R& D and be reused properly. Numerical results show that our open source implementations based on K(k) are equivalent to the commercial implementation based on Kcs(m). The general algorithms for computing orthogonal polynomials developed are valuable for the STEM education and scientific computation.}
}
@article{KALBANDE2023138474,
title = {Machine learning based quantification of VOC contribution in surface ozone prediction},
journal = {Chemosphere},
volume = {326},
pages = {138474},
year = {2023},
issn = {0045-6535},
doi = {https://doi.org/10.1016/j.chemosphere.2023.138474},
url = {https://www.sciencedirect.com/science/article/pii/S0045653523007415},
author = {Ritesh Kalbande and Bipin Kumar and Sujit Maji and Ravi Yadav and Kaustubh Atey and Devendra Singh Rathore and Gufran Beig},
keywords = {Ozone, VOCs, Machine learning, Meteorology, Isoprene},
abstract = {The prediction of surface ozone is essential attributing to its impact on human and environmental health. Volatile organic compounds (VOCs) are crucial in driving ozone concentration; particularly in urban areas where VOC limited regimes are prominent. The limited measurements of VOCs, however, hinder assessing the VOC-ozone relationship. This work applies machine learning (ML) algorithms for temporal forecasting of surface ozone over a metropolitan city in India. The availability of continuous VOCs measurement data along with meteorology and other pollutants during 2014–2016 makes it possible to deduce the influence of various input parameters on surface ozone prediction. After evaluating the best ML model for ozone prediction, simulations were carried out using varied input combinations. The combination with isoprene, meteorology, NOx, and CO (Isop + MNC) was the best with RMSE 4.41 ppbv and MAPE 6.77%. A season-wise comparison of simulations having all data, only meteorological data and Isop + MNC as input showed that Isop + MNC simulation gives the best results during the summer season (RMSE: 5.86 ppbv, MAPE: 7.05%). This shows the increased ability of the model to capture ozone peaks (high ozone during summer) relatively better when isoprene data is used. The overall results highlight that using all available data doesn't necessarily give best prediction results; also critical thinking is essential when evaluating the model results.}
}
@article{GOERTZEL199595,
title = {Self-reference, computation, and mind},
journal = {Journal of Social and Evolutionary Systems},
volume = {18},
number = {1},
pages = {95-101},
year = {1995},
issn = {1061-7361},
doi = {https://doi.org/10.1016/1061-7361(95)90018-7},
url = {https://www.sciencedirect.com/science/article/pii/1061736195900187},
author = {Ben Goertzel and Harold Bowman}
}
@article{GAO2024,
title = {Hetero-Bäcklund transformation, bilinear forms and multi-solitons for a (2＋1)-dimensional generalized modified dispersive water-wave system for the shallow water},
journal = {Chinese Journal of Physics},
year = {2024},
issn = {0577-9073},
doi = {https://doi.org/10.1016/j.cjph.2024.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0577907324003940},
author = {Xin-Yi Gao},
keywords = {Shallow water, Nonlinear and dispersive long gravity waves, (2＋1)-dimensional generalized modified dispersive water-wave system, Hetero-Bäcklund transformation, Bilinear form, Soliton, Symbolic computation},
abstract = {Curiosity for the shallow water has been excited by four recent Chin. J. Phys. papers, so that this shallow-water-directed paper plans to consider a (2＋1)-dimensional generalized modified dispersive water-wave (2DGMDWW) system, which describes the nonlinear and dispersive long gravity waves travelling along two horizontal directions in the shallow water of uniform depth. With symbolic computation, (1) a hetero-Bäcklund transformation is constructed, coupling the solutions as for the 2DGMDWW system with the solutions as for a known (2＋1)-dimensional Boiti–Leon–Pempinelli system describing the water waves in an infinitely narrow channel of constant depth, with that hetero-Bäcklund transformation dependent on the shallow-water coefficients in the 2DGMDWW system, the former solutions indicating certain shallow-water-wave patterns for the height of the water surface and the horizontal velocity of the water wave, while the latter solutions related to the horizontal velocity and elevation of the water wave; (2) two sets of the bilinear forms are obtained, each set of which is shown to depend on the shallow-water coefficients in the 2DGMDWW system and to be linked to certain shallow-water-wave patterns for the height of the water surface and the horizontal velocity of the water wave; and (3) two sets of the N-soliton solutions are also worked out, each set of which is seen to rely on the shallow-water coefficients in the 2DGMDWW system and to represent the existence of N-solitonic shallow-water-wave patterns with respect to the height of the water surface and the horizontal velocity of the water wave, with N as a positive integer. All our results are new.}
}
@article{JOHNSTON2003325,
title = {Biological computation of image motion from flows over boundaries},
journal = {Journal of Physiology-Paris},
volume = {97},
number = {2},
pages = {325-334},
year = {2003},
note = {Neurogeometry and visual perception},
issn = {0928-4257},
doi = {https://doi.org/10.1016/j.jphysparis.2003.09.016},
url = {https://www.sciencedirect.com/science/article/pii/S0928425703000664},
author = {A. Johnston and P.W. McOwan and C.P. Benton},
keywords = {Optic flow, Cortex, Differential forms, Vision, Motion},
abstract = {A theory of early motion processing in the human and primate visual system is presented which is based on the idea that spatio-temporal retinal image data is represented in primary visual cortex by a truncated 3D Taylor expansion that we refer to as a jet vector. This representation allows all the concepts of differential geometry to be applied to the analysis of visual information processing. We show in particular how the generalised Stokes theorem can be used to move from the calculation of derivatives of image brightness at a point to the calculation of image brightness differences on the boundary of a volume in space–time and how this can be generalised to apply to integrals of products of derivatives. We also provide novel interpretations of the roles of direction selective, bi-directional and pan-directional cells and of type I and type II cells in V5/MT.}
}
@article{MUSSO2015267,
title = {A single dual-stream framework for syntactic computations in music and language},
journal = {NeuroImage},
volume = {117},
pages = {267-283},
year = {2015},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2015.05.020},
url = {https://www.sciencedirect.com/science/article/pii/S1053811915004000},
author = {Mariacristina Musso and Cornelius Weiller and Andreas Horn and Volkmer Glauche and Roza Umarova and Jürgen Hennig and Albrecht Schneider and Michel Rijntjes},
abstract = {This study is the first to compare in the same subjects the specific spatial distribution and the functional and anatomical connectivity of the neuronal resources that activate and integrate syntactic representations during music and language processing. Combining functional magnetic resonance imaging with functional connectivity and diffusion tensor imaging-based probabilistic tractography, we examined the brain network involved in the recognition and integration of words and chords that were not hierarchically related to the preceding syntax; that is, those deviating from the universal principles of grammar and tonal relatedness. This kind of syntactic processing in both domains was found to rely on a shared network in the left hemisphere centered on the inferior part of the inferior frontal gyrus (IFG), including pars opercularis and pars triangularis, and on dorsal and ventral long association tracts connecting this brain area with temporo-parietal regions. Language processing utilized some adjacent left hemispheric IFG and middle temporal regions more than music processing, and music processing also involved right hemisphere regions not activated in language processing. Our data indicate that a dual-stream system with dorsal and ventral long association tracts centered on a functionally and structurally highly differentiated left IFG is pivotal for domain–general syntactic competence over a broad range of elements including words and chords.}
}
@article{ARUN2009S1116,
title = {P03-117 A bedside schizophrenia thought disorder scale},
journal = {European Psychiatry},
volume = {24},
pages = {S1116},
year = {2009},
note = {17th EPA Congress - Lisbon, Portugal, January 2009, Abstract book},
issn = {0924-9338},
doi = {https://doi.org/10.1016/S0924-9338(09)71349-5},
url = {https://www.sciencedirect.com/science/article/pii/S0924933809713495},
author = {C.P. Arun},
abstract = {Present classification systems for thought disorder lack consistency and require one to remember long-winded definitions limiting their use to research settings. As an extension of recent work in this area (World Congress, 2008), we classify the characteristic thought disorder patterns seen in schizophrenia according to the location of the lesion in notional "threads" of mental computational processes that string speech together. These threads must take both semantics and syntax into consideration in performing their function. When we speak - just as when we write - there is a natural hierarchy topic thread (the topic of the ‘essay’) and multiples of paragraph threads, sentence threads, clause threads, word threads and phoneme threads. Intuitively, we grade the severity of thought disorder depending upon whether a particular thread gets stuck (S), reconnects abnormally (R) or is absent altogether: I.paragraph thread R: Disjointed sentences S: Circumstantiality;II.topic threadR: Tangentiality S: Preoccupatory thinking;III.sentence threads R: Knight's move thinking S: Clause perseveration;IV.clause threads R: Word salad S: Word perseveration, fusion;V.word threads R: Incoherent sounds/ neologisms/ paraphasias S: Phoneme/syllable perseveration;VI.phoneme threads - Failure of production: Mutism.Of course, one must record all the lesions that are present at any given time. This scale incorporates a intuitive progression from mild to severe thought disorder in Schizophrenia. Using the STDS would allow the straightforward ‘bedside’ quantification of the severity of thought disorder and enforce discipline into the thought assessment section of the Mental State Examination.}
}
@article{BLACK202010653,
title = {A revolution in biochemistry and molecular biology education informed by basic research to meet the demands of 21st century career paths},
journal = {Journal of Biological Chemistry},
volume = {295},
number = {31},
pages = {10653-10661},
year = {2020},
issn = {0021-9258},
doi = {https://doi.org/10.1074/jbc.AW120.011104},
url = {https://www.sciencedirect.com/science/article/pii/S0021925817501040},
author = {Paul N. Black},
keywords = {biochemistry, molecular biology, teaching, learning, primary research, leadership, environment, inclusive excellence, STEM education, biochemistry and molecular biology teaching and learning},
abstract = {The National Science Foundation estimates that 80% of the jobs available during the next decade will require math and science skills, dictating that programs in biochemistry and molecular biology must be transformative and use new pedagogical approaches and experiential learning for careers in industry, research, education, engineering, health-care professions, and other interdisciplinary fields. These efforts require an environment that values the individual student and integrates recent advances from the primary literature in the discipline, experimentally directed research, data collection and analysis, and scientific writing. Current trends shaping these efforts must include critical thinking, experimental testing, computational modeling, and inferential logic. In essence, modern biochemistry and molecular biology education must be informed by, and integrated with, cutting-edge research. This environment relies on sustained research support, commitment to providing the requisite mentoring, access to instrumentation, and state-of-the-art facilities. The academic environment must establish a culture of excellence and faculty engagement, leading to innovation in the classroom and laboratory. These efforts must not lose sight of the importance of multidimensional programs that enrich science literacy in all facets of the population, students and teachers in K-12 schools, nonbiochemistry and molecular biology students, and other stakeholders. As biochemistry and molecular biology educators, we have an obligation to provide students with the skills that allow them to be innovative and self-reliant. The next generation of biochemistry and molecular biology students must be taught proficiencies in scientific and technological literacy, the importance of the scientific discourse, and skills required for problem solvers of the 21st century.}
}
@incollection{GISZTER2007323,
title = {Primitives, premotor drives, and pattern generation: a combined computational and neuroethological perspective},
editor = {Paul Cisek and Trevor Drew and John F. Kalaska},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {165},
pages = {323-346},
year = {2007},
booktitle = {Computational Neuroscience: Theoretical Insights into Brain Function},
issn = {0079-6123},
doi = {https://doi.org/10.1016/S0079-6123(06)65020-6},
url = {https://www.sciencedirect.com/science/article/pii/S0079612306650206},
author = {Simon Giszter and Vidyangi Patil and Corey Hart},
keywords = {primitives, motor synergies, force-fields, modularity, feedback, motor pattern analysis, decomposition, rhythm generation, pattern shaping},
abstract = {A modular motor organization may be needed to solve the degrees of freedom problem in biological motor control. Reflex elements, kinematic primitives, muscle synergies, force-field primitives and/or pattern generators all have experimental support as modular elements. We discuss the possible relations of force-field primitives, spinal feedback systems, and pattern generation and shaping systems in detail, and review methods for examining underlying motor pattern structure in intact or semi-intact behaving animals. The divisions of systems into primitives, synergies, and rhythmic elements or oscillators suggest specific functions and methods of construction of movement. We briefly discuss the limitations and caveats needed in these interpretations given current knowledge, together with some of the hypotheses arising from these frameworks.}
}
@article{ZAHEDI2024103730,
title = {How hypnotic suggestions work – A systematic review of prominent theories of hypnosis},
journal = {Consciousness and Cognition},
volume = {123},
pages = {103730},
year = {2024},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2024.103730},
url = {https://www.sciencedirect.com/science/article/pii/S1053810024000977},
author = {Anoushiravan Zahedi and Steven {Jay Lynn} and Werner Sommer},
keywords = {Hypnosis, Theory, Suggestibility, Hypnotizability, Hypnotic Suggestions (HS), Posthypnotic Suggestions (PHS), Direct Verbal Suggestions},
abstract = {In recent decades, hypnosis has increasingly moved into the mainstream of scientific inquiry. Hypnotic suggestions are frequently implemented in behavioral, neurocognitive, and clinical investigations and interventions. Despite abundant reports about the effectiveness of suggestions in altering behavior, perception, cognition, and agency, no consensus exists regarding the mechanisms driving these changes. This article reviews competing theoretical accounts that address the genesis of subjective, behavioral, and neurophysiological responses to hypnotic suggestions. We systematically analyze the broad landscape of hypnosis theories that best represent our estimation of the current status and future avenues of scientific thinking. We start with procedural descriptions of hypnosis, suggestions, and hypnotizability, followed by a comparative analysis of systematically selected theories. Considering that prominent theoretical perspectives emphasize different aspects of hypnosis, our review reveals that each perspective possesses salient strengths, limitations, and heuristic values. We highlight the necessity of revisiting extant theories and formulating novel evidence-based accounts of hypnosis.}
}
@article{CASTELLO202354,
title = {Towards competency-based education in the chemical engineering undergraduate program in Uruguay: Three examples of integrating essential skills},
journal = {Education for Chemical Engineers},
volume = {44},
pages = {54-62},
year = {2023},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2023.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S1749772823000210},
author = {E. Castelló and C. Santiviago and J. Ferreira and R. Coniglio and E. Budelli and V. Larnaudie and M. Passeggi and I. López},
keywords = {Engineering education, Professional skills, Unconventional laboratory practice, Industrial Internships, Autonomous learning},
abstract = {In 2021, Universidad de la República in Uruguay approved a new Chemical Engineering undergraduate program that incorporates novel conceptual definitions such as competency-based education. This paper describes the process of defining the new curriculum plan and presents the program's structure, as well as specific and cross-disciplinary competencies. These competencies are then compared to the learning outcomes established in the guide for programs accreditation of the Institution of Chemical Engineers. To provide practical examples of how the competency-based approach was incorporated into the program, three specific cases are presented. The first case focuses on the implementation of the internship and industry project. The second case illustrates the incorporation of computational tools as an essential part of different courses throughout the degree program. Finally, the third case describes a new design for the fluid mechanics laboratory that emphasizes hands-on learning and helps students develop several competencies.}
}
@article{LIU2006207,
title = {Evolutionary design in a multi-agent design environment},
journal = {Applied Soft Computing},
volume = {6},
number = {2},
pages = {207-220},
year = {2006},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2005.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S156849460500013X},
author = {Hong Liu and Mingxi Tang},
keywords = {Multi-agent system, Evolutionary computing, Generic algorithm, Computer-aided design, Creative design},
abstract = {This paper presents a novel evolutionary design approach in a multi-agent design environment. Multi-agent system architecture offers a promising framework for dynamically managing cooperative agents in a distributed environment while the tree structure based generic algorithm provides a foundation for supporting evolutionary and innovative design abilities. Design is a complex knowledge discovery process. Creative design is a human trait that is not easily converted into a computational tool. Rather than to implement the innovative design by computers, this environment is used to stimulate the imagination of designers and extend their thinking space. It wants to explore a feasible and useful evolutionary approach in a distributed environment that will give the designers concrete help for the creative designs. This approach is illustrated by a mobile phone design example, which used binary algebraic expression tree to form sketch shapes and a feature based product tree to produce component combination choices. Because evolution is guided by human selectors, the evolutionary algorithm is not complex. It shows that approach is able to generate some creative solutions, demonstrating the power of explorative evolution.}
}
@article{LONGIN2022103280,
title = {Augmenting perception: How artificial intelligence transforms sensory substitution},
journal = {Consciousness and Cognition},
volume = {99},
pages = {103280},
year = {2022},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2022.103280},
url = {https://www.sciencedirect.com/science/article/pii/S1053810022000125},
author = {Louis Longin and Ophelia Deroy},
keywords = {Sensory substitution, Sensory extension, Intelligent sensory augmentation, Information quality, Senses, Artificial intelligence},
abstract = {What happens when artificial sensors are coupled with the human senses? Using technology to extend the senses is an old human dream, on which sensory substitution and other augmentation technologies have already delivered. Laser tactile canes, corneal implants and magnetic belts can correct or extend what individuals could otherwise perceive. Here we show why accommodating intelligent sensory augmentation devices not just improves but also changes the way of thinking and classifying former sensory augmentation devices. We review the benefits in terms of signal processing and show why non-linear transformation is more than a mere improvement compared to classical linear transformation.}
}
@article{RUNNELS201563,
title = {Capturing plasticity effects in overdriven shocks on the finite scale},
journal = {Mathematics and Computers in Simulation},
volume = {111},
pages = {63-79},
year = {2015},
issn = {0378-4754},
doi = {https://doi.org/10.1016/j.matcom.2014.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0378475414003334},
author = {Scott R. Runnels},
keywords = {Shocks, Plasticity, Hardening, Hydrodynamics, Radial return},
abstract = {An ordinary differential equation (ODE) form of the radial return algorithm, which is essentially a Prandtl-Reuss material model, is combined with a strain-rate hardening model to produce an ODE that describes deviatoric stress through a prescribed density rise. An analytical solution is found to the resulting ODE for a specific choice of one of the hardening model’s parameters. That solution is used to prove that if the prescribed density rise is allowed to be infinitely thin, i.e., like a shock in the mathematical sense, the resulting deviatoric stress is still bounded. In other words, the singularity is integrable; integration of the radial return ODE regularizes the infinite strain rate and resulting yield stress in the presence of an ideal shock singularity. The analytical tools developed for this line of thinking are applied to study the variation of deviatoric stress through a nearly shock-like density rise using different density rise profiles, revealing the impact of the shape choice. The tools are also used to compute what rise times are needed to converge upon the correct value of deviatoric stress through a shock; the results indicate that most contemporary hydrocodes cannot be expected to achieve those rise times. A demonstration of connecting the analytical tools to a hydrocode, using surrogate numerical shock shapes, is provided thereby opening the door for using such surrogates to perform sub-grid computations of converged shock behavior for strain-rate hardening materials.}
}
@article{JIANG2021106740,
title = {Accelerator for crosswise computing reduct},
journal = {Applied Soft Computing},
volume = {98},
pages = {106740},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106740},
url = {https://www.sciencedirect.com/science/article/pii/S1568494620306785},
author = {Zehua Jiang and Keyu Liu and Jingjing Song and Xibei Yang and Jinhai Li and Yuhua Qian},
keywords = {Accelerator, Attribute reduction, Cross computation, Rough set},
abstract = {Attribute reduction, as a technique for selecting qualified attributes which can satisfy the intended constraint related to considered measure, has been widely explored. Notably, one and only one reduct is derived through using one searching strategy in most cases. Nevertheless, only one reduct may be not enough for us to evaluate its effectiveness. To fill such gap, an approach of crosswise computing reduct is proposed for obtaining multiple reducts. The computation of reduct is realized through partitioning the whole data into several groups, and crosswise selecting some groups to form different subsets of data, then computing reducts over these different subsets of data. Moreover, to speed up the process of crosswise computing reduct, an acceleration strategy is designed. The main thinking of our acceleration strategy is to compute the reduct over different subsets of data on the basis of reduct over the whole data. The experimental results over 16 data sets show the following superiorities of our strategy: (1) our approach can decrease the elapsed time of crosswise computing reducts significantly; (2) our approach can not only provide reduct with higher stability, but also maintain the classification performance; (3) the attributes in reduct can provide more stable classification results.}
}
@article{GOLDBERG2012261,
title = {An efficient tree-based computation of a metric comparable to a natural diffusion distance},
journal = {Applied and Computational Harmonic Analysis},
volume = {33},
number = {2},
pages = {261-281},
year = {2012},
issn = {1063-5203},
doi = {https://doi.org/10.1016/j.acha.2011.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1063520311001266},
author = {Maxim J. Goldberg and Seonja Kim},
keywords = {Tree, Diffusion, Distance, Metric},
abstract = {Using diffusion to define distances between points on a manifold (or a sampled data set) has been successfully employed in various applications such as data organization and approximately isometric embedding of high dimensional data in low dimensional Euclidean space. Recently, P. Jones has proposed a diffusion distance which is both intuitively appealing and scales appropriately with increasing time. In the first part of our paper, we present an efficient tree-based approach to computing an approximation to Jonesʼs diffusion distance. We also show our approximation is comparable to Jonesʼs distance. Neither Jonesʼs distance, nor our approximation, satisfies the triangle inequality; in particular, in the case of heat flow on Rn, Jonesʼs separation distance gives a scaled square of the Euclidean distance. In the second part of our paper, we present a general construction to obtain an “almost” metric from a general distance. We also discuss a numerical procedure to implement our construction. Additionally, we show that in the case of heat flow on Rn, we recover (scaled) Euclidean distance from Jonesʼs distance.}
}
@article{OMORI19991157,
title = {Emergence of symbolic behavior from brain like memory with dynamic attention},
journal = {Neural Networks},
volume = {12},
number = {7},
pages = {1157-1172},
year = {1999},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(99)00054-4},
url = {https://www.sciencedirect.com/science/article/pii/S0893608099000544},
author = {T. Omori and A. Mochizuki and K. Mizutani and M. Nishizaki},
keywords = {Symbolic behavior, Associative memory, Attention, PATON, Inference, Hippocampus, Model, Computational theory},
abstract = {An important feature of human intelligence is the use of symbols. This is seen in our daily use of language and logical thinking. However, the use of symbols is not limited to humans. We observe planned action sequences in primate behavior and prediction-based action in higher mammals. For the representation and operation of symbols by the brain neural circuit, no specific construction principle or computational theory is known so far. In this paper, we regard the brain as a complex of associative memory and dynamic attentional system, and starting from two hypotheses on information representation and operation in the brain, we propose a model of primitive symbolic behavior emergence that is consistent with the conventional symbolic processing model. We also describe a computational theory of the symbolic processing model in associative memory. Through computer simulation studies on a language-like memory search and map learning by a moving robot, we discuss the validity of the model.}
}
@incollection{RUBIN2023125,
title = {Chapter 9 - Unlocking creative tensions with a paradox approach},
editor = {Roni Reiter-Palmon and Sam Hunter},
booktitle = {Handbook of Organizational Creativity (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {125-145},
year = {2023},
isbn = {978-0-323-91840-4},
doi = {https://doi.org/10.1016/B978-0-323-91840-4.00006-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323918404000062},
author = {Matthew Rubin and Ella Miron-Spektor and Joshua Keller},
keywords = {Creativity, Innovation, Paradox, Mindset, Paradoxical leadership, Culture},
abstract = {Leaders and their employees must navigate competing yet interrelated demands and processes when developing and implementing creative ideas. They have to engage in divergent and convergent thinking, challenge existing assumptions and accept them, plan and persist while remaining spontaneous and adaptive. We explore how, why, and when adopting a paradox approach to navigating such tensions enhances creativity and innovation. Rather than seeking to eliminate the discomfort associated with tensions by prioritizing one demand or process over the other, the paradox approach sees tensions as an opportunity for growth and learning. When adopting a paradox approach, people feel comfortable with the discomfort as tensions arise and recognize that by engaging in one process they enable the seemingly opposing process. We review research on paradoxical frames, mindset, and leadership, and offer a comprehensive theoretical model that delineates the related cognitive, affective, motivational, and social pathways, as well as contextual and cultural boundary conditions. We conclude by identifying promising future directions for research.}
}
@incollection{MARON1965118,
title = {On Cybernetics, Information Processing, and Thinking},
editor = {Norbert Wiener and J.P. Schadé},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {17},
pages = {118-138},
year = {1965},
issn = {0079-6123},
doi = {https://doi.org/10.1016/S0079-6123(08)60158-2},
url = {https://www.sciencedirect.com/science/article/pii/S0079612308601582},
author = {M.E. Maron},
abstract = {Publisher Summary
It is the purpose of this chapter to examine the origins, development, and present status of those key cybernetic notions that provide an information-flow framework within which to attack one aspect of the question of how a person thinks— that is,.the question of the information mechanisms and processes that underlie and are correlated with thinking. After an introductory survey of the scope and ramifications of the information sciences, the cybernetic way of looking at the information processing in the nervous system is examined, so as to see in what sense it provides new and sharp tools of analysis for the neurophysiologist. With this as background, the problem of artificial intelligence is considered and with that the logical and linguistic difficulties in talking about the relationship between thinking and brain activity. An information-flow model of an artificial brain mechanism is described whose activity; it is argued is the correlate to activity, such as perceiving, learning, thinking, knowing, etc. This leads finally to a consideration of the impact of these notions on theoretical neurophysiology and its attempt to frame suitable hypotheses and on epistemology that is concerned with the logical analysis of measures, methods, and techniques, which can justify the activity of knowing.}
}
@article{LOMBARDI2024e00322,
title = {Semantic modelling and HBIM: A new multidisciplinary workflow for archaeological heritage},
journal = {Digital Applications in Archaeology and Cultural Heritage},
volume = {32},
pages = {e00322},
year = {2024},
issn = {2212-0548},
doi = {https://doi.org/10.1016/j.daach.2024.e00322},
url = {https://www.sciencedirect.com/science/article/pii/S2212054824000079},
author = {Matteo Lombardi and Dario Rizzi},
keywords = {Digital archaeology, Semantic modelling, HBIM, Blender, BlenderBIM, Extended matrix},
abstract = {The aim of the study is to describe a methodological approach to represent, interpret, model and manage pluristratified archaeological contexts. The proposed methodology envisages a digital workflow, a BIM-thinking strategy, which integrates geometric and texture data obtained from laser and photogrammetric scans with information about construction techniques and materials, archaeological reports and documentation. The integration is based on a balanced combination of open-source and proprietary solutions, allowing professionals to work with their “comfort software” and assuring interoperability through the adoption of Open Standards. Experimentations are being conducted exploring the potential of connecting semantic 3D modelling and virtual reconstructions based on archaeological data made with Blender and the Extended Matrix Tool, with BIM software and capabilities thanks to the BlenderBIM addon. The proposed workflow, in combination with the described data-sharing-oriented process, adopts a new approach towards 3D models in order to promote a more sustainable mindset towards 3D dataset life-cycle by optimizing their usage and reducing waste on different levels, such as re-documenting the same structure twice. The expected overall result is the ability to generate semantic models that can enhance our understanding of the context as much as foster multidisciplinary BIM (Building Information Modelling) collaboration thus improving archaeological research, documentation and conservation practices.}
}
@article{AMEL2023104187,
title = {Toward an automatic detection of cardiac structures in short and long axis views},
journal = {Biomedical Signal Processing and Control},
volume = {79},
pages = {104187},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.104187},
url = {https://www.sciencedirect.com/science/article/pii/S1746809422006413},
author = {Laidi Amel and Mohammed Ammar and Mostafa {El Habib Daho} and Said Mahmoudi},
keywords = {Cardiac MRI Segmentation, Shape Descriptors, Particle Swarm Optimization, Residual Network, Interpretability},
abstract = {Objective
This work aims to create an automatic detection process of cardiac structures in both short-axis and long-axis views. A workflow inspired by human thinking process, for better explainability.
Methods
we began by separating the images into two classes: long axis and short axis, using a Residual Network model. Then, we used Particle Swarm Optimization for general segmentation. After segmentation, a characterization step based on shape descriptors calculated from bounding box and ANOVA for features selection were applied on the binary images to detect the location of each region of interest: lung, left and right ventricle in the short-axis view, the aorta, the left heart (left atrium and ventricle), and the right heart (right atrium and ventricle) in the long axis view.
Results
we achieved a 90% accuracy on view separation. We have selected: Elongation, Compactness, Circularity, Type Factor, for short axis identification; and:Area, Centre of Mass Y, Moment of Inertia XY, Moment of Inertia YY, for long axis identification.
Conclusion
a successful separation of long axis and short axis views allows for a better characterization and detection of segmented cardiac structures. After that, any method can be applied for segmentation, attribute selection, and classification.
Significance
an attempt to introduce explainability into cardiac image segmentation, we tried to mimic the human workflow while computerizing each step. The process seems to be valid and added clarity and interpretability to the detection.}
}
@article{CHEN202321,
title = {Varieties of specification: Redefining over- and under-specification},
journal = {Journal of Pragmatics},
volume = {216},
pages = {21-42},
year = {2023},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2023.07.019},
url = {https://www.sciencedirect.com/science/article/pii/S037821662300200X},
author = {Guanyi Chen and Kees {van Deemter}},
keywords = {Referring expressions, Over-specification, Under-specification},
abstract = {A long tradition of research in theoretical, experimental and computational pragmatics has investigated over-specification and under-specification in referring expressions. Along broadly Gricean lines, these studies compare the amount of information expressed by a referring expression against the amount of information that is required. Often, however, these studies offer no formal definition of what “required” means, and how the comparison should be performed. In this paper, we use a simple set-theoretic perspective to define some communicatively important types of over-/under-specification. We argue that our perspective enables an enhanced understanding of reference phenomena that can pay important dividends for the analysis of reference in corpora and for the evaluation of computational models of referring. To illustrate and substantiate our claims, we analyse two corpora, containing Chinese and English referring expressions respectively, using the new perspective. The results show that interesting new monolingual and cross-linguistic insights can be obtained from our perspective.}
}
@article{BEAR2020104057,
title = {What comes to mind?},
journal = {Cognition},
volume = {194},
pages = {104057},
year = {2020},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2019.104057},
url = {https://www.sciencedirect.com/science/article/pii/S0010027719302306},
author = {Adam Bear and Samantha Bensinger and Julian Jara-Ettinger and Joshua Knobe and Fiery Cushman},
keywords = {Sampling, Decision-making, Consciousness, Computation},
abstract = {When solving problems, like making predictions or choices, people often “sample” possibilities into mind. Here, we consider whether there is structure to the kinds of thoughts people sample by default—that is, without an explicit goal. Across three experiments we found that what comes to mind by default are samples from a probability distribution that combines what people think is likely and what they think is good. Experiment 1 found that the first quantities that come to mind for everyday behaviors and events are quantities that combine what is average and ideal. Experiment 2 found, in a manipulated context, that the distribution of numbers that come to mind resemble the mathematical product of the presented statistical distribution and a (softmax-transformed) prescriptive distribution. Experiment 3 replicated these findings in a visual domain. These results provide insight into the process generating people’s conscious thoughts and invite new questions about the value of thinking about things that are both likely and good.}
}
@article{FILIPPOU2016892,
title = {Modelling the impact of study behaviours on academic performance to inform the design of a persuasive system},
journal = {Information & Management},
volume = {53},
number = {7},
pages = {892-903},
year = {2016},
note = {Special Issue on Papers Presented at Pacis 2015},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2016.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0378720616300507},
author = {Justin Filippou and Christopher Cheong and France Cheong},
keywords = {Study behaviour, Persuasive systems, Linear modelling, Higher education},
abstract = {Information technology is deeply ingrained in most aspects of everyday life and can be designed to influence users to behave in a certain way. Influencing students to improve their study behaviour would be a useful application of this technology. As a preamble to the design of a persuasive system for learning, we collected data to identify the study behaviours of students and recent alumni. We then developed two models to measure which behaviours have the most significant impact on learning performance. Current students reported more foundational behaviours whereas alumni demonstrated more higher-order thinking traits.}
}
@article{VERNON2019122,
title = {Internal simulation in embodied cognitive systems: Comment on “Muscleless motor synergies and actions without movements: From motor neuroscience to cognitive robotics” by Vishwanathan Mohan et al.},
journal = {Physics of Life Reviews},
volume = {30},
pages = {122-125},
year = {2019},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2019.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S1571064519300429},
author = {David Vernon},
keywords = {Internal simulation, Embodied cognition, Cognitive robotics, Episodic future thinking}
}
@article{VIERTEL2019109,
title = {A Computational model of the mammalian external tufted cell},
journal = {Journal of Theoretical Biology},
volume = {462},
pages = {109-121},
year = {2019},
issn = {0022-5193},
doi = {https://doi.org/10.1016/j.jtbi.2018.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0022519318304752},
author = {Ryan Viertel and Alla Borisyuk},
keywords = {External tufted cell, Bursting, Glomerulus, Olfactory bulb, Hodgkin Huxley model},
abstract = {We introduce a novel detailed conductance-based model of the bursting activity in external tufted (ET) cells of the olfactory bulb. We investigate the mechanisms underlying their bursting, and make experimentally-testable predictions. The ionic currents included in the model are specific to ET cells, and their kinetic and other parameters are based on experimental recordings. We validate the model by showing that its bursting characteristics under various conditions (e.g. blocking various currents) are consistent with experimental observations. Further, we identify the bifurcation structure and dynamics that explain bursting behavior. This analysis allows us to make predictions of the response of the cell to current pulses at different burst phases. We find that depolarizing (but not hyperpolarizing) inputs received during the interburst interval can advance burst timing, creating the substrate for synchronization by excitatory connections. It has been hypothesized that such synchronization among the ET cells within one glomerulus might help coordinate the glomerular output. Next we investigate model parameter sensitivity and identify parameters that play the most prominent role in controlling each burst characteristic, such as the burst frequency and duration. Finally, the response of the cell to periodic inputs is examined, reflecting the sniffing-modulated input that these cell receive in vivo. We find that individual cells can be better entrained by inputs with higher, rather than lower, frequencies than the intrinsic bursting frequency of the cell. Nevertheless, a heterogeneous population of ET cells (as may be found in a glomerulus) is able to produce reliable periodic population responses even at lower input frequencies.}
}
@article{DIACOPOULOS2020103911,
title = {A systematic review of mobile learning in social studies},
journal = {Computers & Education},
volume = {154},
pages = {103911},
year = {2020},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2020.103911},
url = {https://www.sciencedirect.com/science/article/pii/S036013152030110X},
author = {Mark Michael Diacopoulos and Helen Crompton}
}
@article{RICHARDSON1991305,
title = {Computational physics on the CM-2 supercomputer},
journal = {Physics Reports},
volume = {207},
number = {3},
pages = {305-320},
year = {1991},
issn = {0370-1573},
doi = {https://doi.org/10.1016/0370-1573(91)90149-G},
url = {https://www.sciencedirect.com/science/article/pii/037015739190149G},
author = {John L. Richardson},
abstract = {The Connection Machine Supercomputer system is described with emphasis on the solution to large scale physics problems. Numerous parallel algorithms as well as their implementation are given that demonstrate the use of the Connection Machine for physical simulations. Applications discussed include classical mechanics, quantum mechanics, electromagnetism, fluid flow, statistical physics and quantum field theories. The visualization of physical phenomena is also discussed and in the lectures video tapes demonstrating this capability are shown. Connection Machine performance and I/O characteristics are also described as well as the CM-2 software.}
}
@article{CUI2024124662,
title = {Cooperative interference to achieve interval many-objective evolutionary algorithm for association privacy secure computing migration},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124662},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124662},
url = {https://www.sciencedirect.com/science/article/pii/S095741742401529X},
author = {Zhihua Cui and Zhenyu Shi and Qi Li and Tianhao Zhao and Wensheng Zhang and Jinjun Chen},
keywords = {Mobile edge computing, Computing migration, Physical layer security(PLS), Interval many-objective optimization, Evolutionary algorithm},
abstract = {In this paper, we study secure computing migration scenarios in uncertain environments with the presence of multiple malicious eavesdroppers (MEs). Specifically, when edge servers (ESs) execute tasks delivered by smart devices (SDs), SDs may move beyond the coverage of ESs, and computing migration (CM) of unfinished tasks is required to ensure service continuity. There is a risk of privacy leakage during task migration, and MEs use colluding eavesdropping to eavesdrop on the migrated tasks, and we consider eavesdropping on the associated tasks through data sharing among MEs to improve the eavesdropping efficiency. For eavesdropping in MEs, we achieve eavesdropping strikes using cooperative interference by jammers, which benefit by providing jamming services. In addition, uncertain computational scenarios directly affect the efficiency of task execution, and we consider the uncertainty factor in the malicious eavesdropping environment. To this end, this paper proposes the secure computational migration of associative privacy in uncertain environments (SCMAPUE) model, which transforms uncertainties into interval parameters, and optimizes the five objectives of migration delay, maximum completion time, energy consumption, load balancing and migration reliability to achieve efficient task execution and reliable migration. Aiming at the model characteristics, this paper designs an interval many-objective evolutionary algorithm for reliable migration (IMaOEA-RM), which employs a condition-based interval confidence strategy and a multi-access secure migration selection strategy to improve the convergence of the algorithm, and utilizes a dual-migration crossover strategy in order to adjust the jammer partners and improve the population diversity. Simulation results show that our proposed IMaOEA-RM algorithm can provide a more reliable and efficient migration scheme than existing algorithms.}
}
@article{KOTIR2024140042,
title = {Field experiences and lessons learned from applying participatory system dynamics modelling to sustainable water and agri-food systems},
journal = {Journal of Cleaner Production},
volume = {434},
pages = {140042},
year = {2024},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2023.140042},
url = {https://www.sciencedirect.com/science/article/pii/S0959652623042002},
author = {Julius H. Kotir and Renata Jagustovic and George Papachristos and Robert B. Zougmore and Aad Kessler and Martin Reynolds and Mathieu Ouedraogo and Coen J. Ritsema and Ammar Abdul Aziz and Ron Johnstone},
keywords = {Africa, Group model building, Systems thinking, Stakeholder engagement, System modelling, Sustainable development},
abstract = {Achieving the objectives of sustainable development in water and agri-food systems requires the utilisation of decision-support tools in stakeholder-driven processes to construct and simulate various scenarios and evaluate the outcomes of associated policy interventions. While it is common practice to involve stakeholders in participatory modelling processes, their comprehensive documentation and the lessons learned remain scarce. In this paper, we share our experience of engaging stakeholders throughout the entire system dynamics modelling process. We draw on two projects implemented in the Volta River Basin, West Africa, to understand the dynamics of water and agri-food systems under changing environmental and socioeconomic conditions. We outline eight key insights and lessons as practical guides derived from each stage of the participatory modelling process, including the pre-workshop stage, problem definition, model conceptualization, simulation model formulation, model testing and verification, and policy design and evaluation. Our findings demonstrate that stakeholders can actively contribute to all phases of the system dynamics modelling process, including parameter estimation, sensitivity analysis, and numerical simulation experiments. However, we encountered notable challenges, including the time-intensive nature of the process, the struggle to reach a consensus on the modelled problem, and the difficulty of translating the conceptual model into a simulation model using stock and flow diagrams – all of which were addressed through a structured facilitation process. While the projects were anchored in the specific context of West Africa, the key lessons and insights highlighted have broader significance, particularly for researchers employing PSDM in regions characterised by multifaceted human-environmental systems and where stakeholder involvement is crucial for holistic understanding and effective policy interventions. This paper contributes practical guidance for future efforts with participatory modelling, particularly in regions worldwide grappling with sustainable development challenges in water and agri-food systems, and where stakeholder involvement is crucial for holistic understanding of the multiple challenges and for designing effective policy interventions.}
}
@article{MORABIA2020164,
title = {Pandemics and methodological developments in epidemiology history},
journal = {Journal of Clinical Epidemiology},
volume = {125},
pages = {164-169},
year = {2020},
issn = {0895-4356},
doi = {https://doi.org/10.1016/j.jclinepi.2020.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0895435620306454},
author = {Alfredo Morabia},
keywords = {Epidemiology, History, Pandemics, Covid-19, Plague, Cholera, Tuberculosis, Influenza, HIV/AIDS},
abstract = {The crisis spurred by the pandemic of COVID-19 has revealed weaknesses in our epidemiologic methodologic corpus, which scientists are struggling to compensate. This article explores whether this phenomenon is characteristic of pandemics or not. Since the emergence of population-based sciences in the 17th century, we can observe close temporal correlations between the plague and the discovery of population thinking, cholera and population-based group comparisons, tuberculosis and the formalization of cohort studies, the 1918 Great Influenza and the creation of an academic epidemiologic counterpart to the public health service, the HIV/AIDS epidemic, and the formalization of causal inference concepts. The COVID-19 pandemic seems to have promoted the widespread understanding of population thinking both with respect to ways of flattening an epidemic curve and the societal bases of health inequities. If the latter proves true, it will support my hypothesis that pandemics did accelerate profound changes in epidemiologic methods and concepts.}
}
@article{RYLE1953189,
title = {Thinking},
journal = {Acta Psychologica},
volume = {9},
pages = {189-196},
year = {1953},
issn = {0001-6918},
doi = {https://doi.org/10.1016/0001-6918(53)90012-2},
url = {https://www.sciencedirect.com/science/article/pii/0001691853900122},
author = {Gilbert Ryle}
}
@incollection{ADRIAANS2008133,
title = {LEARNING AND THE COOPERATIVE COMPUTATIONAL UNIVERSE},
editor = {Pieter Adriaans and Johan {van Benthem}},
booktitle = {Philosophy of Information},
publisher = {North-Holland},
address = {Amsterdam},
pages = {133-167},
year = {2008},
series = {Handbook of the Philosophy of Science},
issn = {18789846},
doi = {https://doi.org/10.1016/B978-0-444-51726-5.50010-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780444517265500108},
author = {Pieter Adriaans}
}
@article{GROSBERG2009359,
title = {Computational models of heart pumping efficiencies based on contraction waves in spiral elastic bands},
journal = {Journal of Theoretical Biology},
volume = {257},
number = {3},
pages = {359-370},
year = {2009},
issn = {0022-5193},
doi = {https://doi.org/10.1016/j.jtbi.2008.11.022},
url = {https://www.sciencedirect.com/science/article/pii/S0022519308006103},
author = {Anna Grosberg and Morteza Gharib},
keywords = {Cardiac modeling, Left ventricular twist, Myocardium macro-structure, Finite element simulations},
abstract = {We present a framework for modeling biological pumping organs based on coupled spiral elastic band geometries and active wave-propagating excitation mechanisms. Two pumping mechanisms are considered in detail by way of example: one of a simple tube, which represents a embryonic fish heart and another more complicated structure with the potential to model the adult human heart. Through finite element modeling different elastic contractions are induced in the band. For each version the pumping efficiency is measured and the dynamics are evaluated. We show that by combining helical shapes of muscle bands with a contraction wave it is possible not only to achieve efficient pumping, but also to create desired dynamics of the structure. As a result we match the function of the model pumps and their dynamics to physiological observations.}
}
@article{DUAN2021107596,
title = {Equidistant k-layer multi-granularity knowledge space},
journal = {Knowledge-Based Systems},
volume = {234},
pages = {107596},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107596},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121008583},
author = {Jiangli Duan and Guoyin Wang and Xin Hu},
keywords = {Granular computing, Multi-granularity knowledge space, Knowledge space distance, Hierarchical quotient space},
abstract = {A multi-granularity knowledge space is a computational model that simulates human thinking and solves complex problems. However, as the amount of data increases, the multi-granularity knowledge space will have a larger number of layers, which will reduce its problem-solving ability. Therefore, we define a knowledge space distance measurement and propose two algorithms to select k representative layers from the multi-granularity knowledge space, where k is specified by the user according to the needs in problem solving, and k representative layers are approximately equidistant. First, we propose a knowledge space distance to measure the distance between any two layers in a multi-granularity knowledge space with superset-subset relationships, and the rationality of the knowledge space distance is verified by theory and experiment. Second, relying on the knowledge space distance and knowledge space distance variance, we propose two algorithms (i.e., a deterministic algorithm and a heuristic algorithm) to select an approximate equidistant k-layer multi-granularity knowledge space. Third, in addition to verifying the effectiveness of the knowledge space distance, the knowledge space distance variance, the deterministic algorithm and the heuristic algorithm, we verify that the equidistant k-layer multi-granularity knowledge space is more efficient than the original multi-granularity knowledge space.}
}
@incollection{ASHBY2024255,
title = {Chapter 10 - Circular Materials Economics},
editor = {Michael F. Ashby},
booktitle = {Materials and Sustainable Development (Second Edition)},
publisher = {Butterworth-Heinemann},
edition = {Second Edition},
pages = {255-295},
year = {2024},
isbn = {978-0-323-98361-7},
doi = {https://doi.org/10.1016/B978-0-323-98361-7.00010-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323983617000105},
author = {Michael F. Ashby},
keywords = {Circularity, Material efficiency, Linear materials economy, Circular materials economy, Reuse, Repair, Recycling, Take-back legislation, Recycling targets, Increased product life, Urban mining, Business models, Measuring circularity, Modelling circularity, Limits to circularity},
abstract = {When products come to the end of their lives, the materials they contain are still there. Repurposing, repair or recycling can return them to active use, creating a technological cycle that, in some ways, parallels the carbon, nitrogen and hydrological cycles of the biosphere. In developed nations they lost urgency as the cost of materials fell and that of labour rose, making it cheaper to make new products than to fix old ones, leading to a materials economy that is largely linear, characterized by the sequence “take – make – use – dispose”. Increasing population and affluence, and the limited capacity for the planet to provide resources and absorb waste direct thinking towards a more circular way of using materials. Governments have sought to reduce waste by imposing take-back regulations, setting mandatory recycling targets and requiring minimum service lives. These, and the efficiency movement – eco-efficiency, material-efficiency, energy efficiency – seek to allow business as usual with reduced drain on natural resources without any real change of behaviour. The ‘circularity’ concept is a way of thinking that looks not just for efficiencies but also for new ways to provide functions. The idea of deploying rather than consuming materials, of using them not once but many times has economic as well as environmental appeal. This chapter examines the background, the successes, the difficulties, and the ultimate limits of implementing a circular materials economy.}
}
@incollection{KOZA2002275,
title = {Chapter 10 - Genetic Programming: Biologically Inspired Computation That Exhibits Creativity in Producing Human-Competitive Results},
editor = {Peter J Bentley and David W. Corne},
booktitle = {Creative Evolutionary Systems},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {275-298},
year = {2002},
series = {The Morgan Kaufmann Series in Artificial Intelligence},
isbn = {978-1-55860-673-9},
doi = {https://doi.org/10.1016/B978-155860673-9/50048-3},
url = {https://www.sciencedirect.com/science/article/pii/B9781558606739500483},
author = {John R. Koza and Forrest H. Bennett and David Andre and Martin A. Keane},
abstract = {Publisher Summary
One of the central challenges of computer science is to get a computer to solve a problem without programming it explicitly. The challenge is to create an automatic system whose input is a high-level statement of a problem's requirements and whose output is a satisfactory solution to the given problem. This challenge is the common goal of such fields of research as artificial intelligence and machine learning. Paraphrasing Arthur Samuel, this challenge addresses the question: How can computers are made to do what needs to be done, without being told exactly how to do it? As Samuel further explained: “The aim is to get machines to exhibit behavior, which if done by humans, would be assumed to involve the use of intelligence.” This chapter provides an affirmative answer to the following two questions: Starting only with a high-level statement of the problem's requirements, can computers automatically discover the solution to nontrivial problems? And, can automatically created solutions be competitive with the products of human creativity and inventiveness? In answering these questions, this chapter focuses on a biologically inspired domain-independent problem-solving technique of evolutionary computation, called genetic programming. For each problem, genetic programming automatically creates entities that improve on previously patented inventions, or duplicate the functionality of previously patented inventions or duplicate the functionality of previously patented inventions. The chapter also discusses the importance of illogic in achieving creativity and inventiveness.}
}
@article{BIBRI2018758,
title = {A foundational framework for smart sustainable city development: Theoretical, disciplinary, and discursive dimensions and their synergies},
journal = {Sustainable Cities and Society},
volume = {38},
pages = {758-794},
year = {2018},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2017.12.032},
url = {https://www.sciencedirect.com/science/article/pii/S2210670717313069},
author = {Simon Elias Bibri},
keywords = {Smart sustainable cities, Theories, Academic disciplines, Academic discourses, Multidimensional framework, Interdisciplinarity and transdisciplinarity, Systems thinking, Complexity science, Sustainability, Computing and ICT},
abstract = {In the subject of smart sustainable cities, the underlying theories are a foundation for practice. Moreover, scholarly research in the field of smart sustainable cities operates out of the understanding that advances in the underlying knowledge necessitate pursuing multifaceted questions that can only be resolved from the vantage point of interdisciplinarity or transdisciplinarity. Indeed, research problems in this field are inherently too complex to be addressed by single disciplines. The PhD study addressing the topic of smart sustainable city development falls within the broad research field of sustainability transition and sustainability science where ICT is seen as a salient factor given its transformational, disruptive, and synergetic effects as an enabling, integrative, and constitutive technology. In light of this, the approach to the PhD study is of an applied theoretical kind, and its aim is to investigate and analyze how to advance and sustain the contribution of sustainable urban forms to the goals of sustainable development with support of ICT of pervasive computing. This is to primarily create a framework for strategic smart sustainable city development based on scientific principles, theories, and academic disciplines and discourses used to guide urban actors in their practice towards sustainability and analyze its impact. This involves the application of a set of integrative foundational elements drawn from urban planning, urban design, sustainability, sustainable development, sustainability science, data science, computer science, complexity science, systems theory, systems thinking, and ICT. Accordingly, it is deemed of high significance to devise a multidimensional framework consisting of relevant theories and academic disciplines and discourses that underpin the development of smart sustainable cities as a set of future practices. This framework in turn emphasizes the interdisciplinary and transdisciplinary nature and orientation of the topic of smart sustainable cities and thus the relevance of pursuing an interdisciplinary and transdisciplinary approach into studying this topic. Therefore, this paper endeavors to systematize the very complex and dense scientific area of smart sustainable cities in terms of identifying, distilling, and structuring the core dimensions of a foundational framework for smart sustainable city development as a set of future practices. In doing so, it focuses on a number of fundamental theories along with academic disciplines and discourses, with the aim of setting a framework that analytically relates city development, sustainability, and ICT, while emphasizing how and to what extent sustainability and ICT have particularly become influential in city development in modern society. In addition, this paper offers an in–depth interdisciplinary and transdisciplinary discussion covering topics of high relevance to the PhD study and at the heart of the very synergic relationship between the theoretical, disciplinary, and discursive dimensions of the foundational framework underpinning smart sustainable city development. These dimensions thus form the basis for the framework for strategic smart sustainable city development that is under investigation and will be developed based on a backcasting approach to strategic planning. This study provides an important lens through which to understand a set of influential theories and established academic disciplines and discourses with high potential for integration, fusion, and practicality in relation to the practice of smart sustainable city development.}
}
@article{STATHOPOULOS20031565,
title = {Wind loads on low buildings: in the wake of Alan Davenport's contributions},
journal = {Journal of Wind Engineering and Industrial Aerodynamics},
volume = {91},
number = {12},
pages = {1565-1585},
year = {2003},
note = {ENGINEERING SYMPOSIUM To Honour ALAN G. DAVENPORT for his 40 Years of Contributions},
issn = {0167-6105},
doi = {https://doi.org/10.1016/j.jweia.2003.09.019},
url = {https://www.sciencedirect.com/science/article/pii/S0167610503001302},
author = {Ted Stathopoulos},
keywords = {Building, Code, Computational wind engineering, Design, Load, Pressure, Standard, Time series, Wind},
abstract = {The paper reviews the evolution of knowledge and its current state regarding the evaluation of wind loads on low buildings by placing particular emphasis on Alan Davenport's contributions. These contributions have paved the way to the current state-of-the-art and have influenced the thinking of not only Alan's closest collaborators but also of other researchers in this area around the world. The paper will provide a brief historical perspective, followed by some detailed description of the University of Western Ontario's research on wind loads on low buildings carried out in the 1970s. Visualizing the wake of Davenport's contributions in this area, the paper will refer to the influence of this knowledge in the formulation of design load provisions in contemporary wind standards and codes of practice. The paper will also discuss the status of computational wind engineering as well as the so-called computer-aided wind engineering in the evaluation of wind pressures on low buildings.}
}
@article{ALVARADO20043,
title = {Autonomous agents and computational intelligence: the future of AI application for petroleum industry},
journal = {Expert Systems with Applications},
volume = {26},
number = {1},
pages = {3-8},
year = {2004},
note = {Intelligent Computing in the Petroleum Industry, ICPI-02},
issn = {0957-4174},
doi = {https://doi.org/10.1016/S0957-4174(03)00103-9},
url = {https://www.sciencedirect.com/science/article/pii/S0957417403001039},
author = {Matı&#x0301;as Alvarado and Leonid Cheremetov and Francisco Cantú}
}
@article{SARTON195551,
title = {The astral religion of antiquity and the “thinking machines” of to-day},
journal = {Vistas in Astronomy},
volume = {1},
pages = {51-60},
year = {1955},
issn = {0083-6656},
doi = {https://doi.org/10.1016/0083-6656(55)90012-X},
url = {https://www.sciencedirect.com/science/article/pii/008366565590012X},
author = {George Sarton}
}
@article{DELLACQUA20241727,
title = {Empathy-Aware Behavior Trees for Social Care Decision Systems},
journal = {Procedia Computer Science},
volume = {239},
pages = {1727-1735},
year = {2024},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2023},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.06.351},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924015953},
author = {Pierangelo Dell’Acqua and Stefania Costantini and Abeer Dyoub and Giovanni De Gasperis and Andrea Monaldini and Andrea Rafanelli},
keywords = {Behavior Trees, Affective Computing, Decision Making},
abstract = {There is growing attention on the importance of building intelligent systems where humans and Artificial Intelligence-based systems (AIs) form teams exploiting the potentially synergistic relationships between humans and automation. In the last decade, the computational modeling of empathy has gained increasing attention. Empowering interactive agents with empathic capabilities leads, on the human’s side, to more trust, increases engagement, and thus interaction length, helps cope with stress. These findings suggest that agents endowed with empathy may enhance social interaction in educational applications, artificial companions, medical assistants, and gaming applications. This article focuses on modeling the empathic behavior of virtual agents interacting with humans. We propose a formal model that enables virtual agents to exhibit empathic, emotional behavior. Specifically, we extend the modeling of empathy via behavior trees with a new type of node allowing the specification of various kinds of empathy. Using the proposed extension, we show how different agents’ reactive behavior can be modeled.}
}
@article{GIRARD2005215,
title = {From brainstem to cortex: Computational models of saccade generation circuitry},
journal = {Progress in Neurobiology},
volume = {77},
number = {4},
pages = {215-251},
year = {2005},
issn = {0301-0082},
doi = {https://doi.org/10.1016/j.pneurobio.2005.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S030100820500153X},
author = {B. Girard and A. Berthoz},
keywords = {Saccade generation circuitry, Computational models, Brainstem, Superior colliculus, Cerebellum, Basal ganglia, Cortex},
abstract = {The brain circuitry of saccadic eye movements, from brainstem to cortex, has been extensively studied during the last 30 years. The wealth of data gathered allowed the conception of numerous computational models. These models proposed descriptions of the putative mechanisms generating this data, and, in turn, made predictions and helped to plan new experiments. In this article, we review the computational models of the five main brain regions involved in saccade generation: reticular formation saccadic burst generators, superior colliculus, cerebellum, basal ganglia and premotor cortical areas. We present the various topics these models are concerned with: location of the feedback loop, multimodal saccades, long-term adaptation, on the fly trajectory correction, strategy and metrics selection, short-term spatial memory, transformations between retinocentric and craniocentric reference frames, sequence learning, to name the principle ones. Our objective is to provide a global view of the whole system. Indeed, narrowing too much the modelled areas while trying to explain too much data is a recurrent problem that should be avoided. Moreover, beyond the multiple research topics remaining to be solved locally, questions regarding the operation of the whole structure can now be addressed by building on the existing models.}
}
@article{PAPADOPOULOS2019210,
title = {Using mobile puzzles to exhibit certain algebraic habits of mind and demonstrate symbol-sense in primary school students},
journal = {The Journal of Mathematical Behavior},
volume = {53},
pages = {210-227},
year = {2019},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2018.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S073231231730189X},
author = {Ioannis Papadopoulos},
keywords = {Algebraic habits of mind, mobile puzzles, Symbol sense},
abstract = {Given the growing concern for developing students’ algebraic ideas and thinking in earlier grades (NCTM, 2000) it is important for students to have experiences that better prepare them for their formal introduction to algebra. Mobile puzzles seem to be an opportunity for exhibiting certain algebraic habits of mind as well as for demonstrating symbol-sense which might support students in their transition from arithmetic to algebra. These puzzles include multiple balanced collections of objects whose weights must be determined by the solver. The arms/beams must be perfectly balanced for it to hang properly. Therefore, they represent, in a pictorial way, systems of equations. Each arm/beam that balances two sets of objects (representing variables as unknown “weights”) represents an equation. The data derived from Grade-6 students who were asked to solve a collection of tasks reflect the presence of the “Puzzling and Persevering” and “Seeking and Using Structure” habits of mind. At the same time these data incorporate instances of some main components of symbol-sense such as “friendliness with symbols”, “manipulating and ‘reading through’ symbolic expressions”, and “choice of symbols”. Also discussed is the way this experience contributes to an intuitive application of the conventional rules for solving equations that will be later introduced to the students as the standard algebraic “moves”.}
}
@article{LEBARON2000679,
title = {Agent-based computational finance: Suggested readings and early research},
journal = {Journal of Economic Dynamics and Control},
volume = {24},
number = {5},
pages = {679-702},
year = {2000},
issn = {0165-1889},
doi = {https://doi.org/10.1016/S0165-1889(99)00022-6},
url = {https://www.sciencedirect.com/science/article/pii/S0165188999000226},
author = {Blake LeBaron},
keywords = {Agents, Heterogeneous information, Simulated markets, Learning, Evolution},
abstract = {The use of computer simulated markets with individual adaptive agents in finance is a new, but growing field. This paper explores some of the early works in the area concentrating on a set of some of the earliest papers. Six papers are summarized in detail, along with references to many other pieces of this wide ranging research area. It also covers many of the questions that new researchers will face when getting into the field, and hopefully can serve as a kind of minitutorial for those interested in getting started.}
}
@article{WEICHBROTH20223798,
title = {A note on the affective computing systems and machines: a classification and appraisal},
journal = {Procedia Computer Science},
volume = {207},
pages = {3798-3807},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.441},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922013345},
author = {Paweł Weichbroth and Wiktor Sroka},
keywords = {Affective Computing, Artificial Emotional Intelligence, Classification, System, Machine},
abstract = {Affective computing (AfC) is a continuously growing multidisciplinary field, spanning areas from artificial intelligence, throughout engineering, psychology, education, cognitive science, to sociology. Therefore, many studies have been devoted to the aim of addressing numerous issues, regarding different facets of AfC solutions. However, there is a lack of classification of the AfC systems. This study aims to fill this gap by reviewing and evaluating the state-of-the-art studies in a qualitative manner. In this line of thinking, we put forward a threefold classification that breaks down to desktop and mobile AfC systems, and AfC machines. Moreover, we identified four types of AfC systems, based on the features extracted. In our opinion, the results of this study can serve as a guide for future affect-related research and design, on the one hand, and provide a better understanding on the role of emotions and affect in human-computer interaction, on the other hand.}
}
@article{OLSON1995183,
title = {Emergent computation and the modeling and management of ecological systems},
journal = {Computers and Electronics in Agriculture},
volume = {12},
number = {3},
pages = {183-209},
year = {1995},
issn = {0168-1699},
doi = {https://doi.org/10.1016/0168-1699(94)00022-I},
url = {https://www.sciencedirect.com/science/article/pii/016816999400022I},
author = {Richard L. Olson and Ronaldo A. Sequeira},
keywords = {Emergent computation, Ecosystem dynamics, Ecosystem management},
abstract = {This paper introduces the emergent computational paradigm, discusses its applicability and potential in ecosystem management, and reviews the literature. Emergent computation is significantly different from the “classic” computational paradigm, where control is top-down and centralized. In emergent systems, overall system dynamics emerge from the local interactions of independent agents. In such systems, overall global control is minimized or eliminated altogether. Applications in ecosystem management include use of “artificial ecosystems” as surrogate experimental systems, and genetics-based machine learning systems to evolve management rule-sets for complex domains. Cellular automata, neural networks, genetic algorithms and classifier systems are discussed as examples of the emergent approach. Finally, an in-depth literature review of artificial ecosystems is provided.}
}
@article{YAP19973,
title = {Towards exact geometric computation},
journal = {Computational Geometry},
volume = {7},
number = {1},
pages = {3-23},
year = {1997},
issn = {0925-7721},
doi = {https://doi.org/10.1016/0925-7721(95)00040-2},
url = {https://www.sciencedirect.com/science/article/pii/0925772195000402},
author = {Chee-Keng Yap},
abstract = {Exact computation is assumed in most algorithms in computational geometry. In practice, implementors perform computation in some fixed-precision model, usually the machine floating-point arithmetic. Such implementations have many well-known problems, here informally called “robustness issues”. To reconcile theory and practice, authors have suggested that theoretical algorithms ought to be redesigned to become robust under fixed-precision arithmetic. We suggest that in many cases, implementors should make robustness a non-issue by computing exactly. The advantages of exact computation are too many to ignore. Many of the presumed difficulties of exact computation are partly surmountable and partly inherent with the robustness goal. This paper formulates the theoretical framework for exact computation based on algebraic numbers. We then examine the practical support needed to make the exact approach a viable alternative. It turns out that the exact computation paradigm encompasses a rich set of computational tactics. Our fundamental premise is that the traditional “BigNumber” package that forms the work-horse for exact computation must be reinvented to take advantage of many features found in geometric algorithms. Beyond this, we postulate several other packages to be built on top of the BigNumber package.}
}
@article{METTLER2024101932,
title = {Same same but different: How policies frame societal-level digital transformation},
journal = {Government Information Quarterly},
volume = {41},
number = {2},
pages = {101932},
year = {2024},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2024.101932},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X24000248},
author = {Tobias Mettler and Gianluca Miscione and Claus D. Jacobs and Ali A. Guenduez},
keywords = {Digital transformation, IS policy research, Computational content analysis, Narratives},
abstract = {The digital transformation (DT) is not only forcing companies to rethink their business models but is also challenging governments to address the question of how information technology will change society today and in the future. By setting the legal boundaries and acting as an investor and promoter of the domestic digital economy, governments actively influence in which ways this transformational process takes place. The vision and objectives how DT should be realized on state level is portrayed in well-crafted DT policies. Yet, little is known how governments, as strategic actors, see their role in the DT and how they frame these documents. In this paper, we argue that policymaking about DT is isomorphic in the global context, rather than a differentiator for countries to gain a competitive edge. Using machine learning to analyze a vast text corpus of policy documents, we identify the common repertoire of narratives used by governments from all around the globe to picture their vision of the DT and show that DT policies appear to be almost context-free due to their high similarity.}
}
@incollection{DAVIES2024,
title = {Software Tools for Green and Sustainable Chemistry},
booktitle = {Reference Module in Chemistry, Molecular Sciences and Chemical Engineering},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-12-409547-2},
doi = {https://doi.org/10.1016/B978-0-443-15742-4.00049-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780443157424000491},
author = {Joseph C. Davies and Jonathan D. Hirst},
keywords = {Computer aided synthesis planning., Digitalization, Electronic laboratory notebook, Green metrics, Solvent selection},
abstract = {We present the concepts of green and sustainable chemistry and related software tools. Making chemistry greener and more sustainable is a growing priority for researchers and software tools have been developed to aid in this pursuit. Software tools for green and sustainable chemistry have been developed to assess existing methods, propose new ones, and replace some experimental methods altogether with in silico approaches. We discuss the digitalization of chemistry and the computational advances that enable software tools to play a growing role in all aspects of chemical research. Barriers and limitations of current tools are discussed along with future trajectories.}
}
@incollection{SCHOMMERS201991,
title = {Chapter 2 - Theoretical and Computational Methods},
editor = {Wolfram Schommers},
booktitle = {Basic Physics of Nanoscience},
publisher = {Elsevier},
pages = {91-202},
year = {2019},
isbn = {978-0-12-813718-5},
doi = {https://doi.org/10.1016/B978-0-12-813718-5.00002-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128137185000028},
author = {Wolfram Schommers},
keywords = {Simulation methods, interaction potentials, anharmonicities, temperature effects, molecular dynamics, nanosystems, structures, dynamics},
abstract = {It is underlined that typical nanosystems are adequately described only by the fundamental laws of theoretical physics. It is in particular argued that phenomenological models are in most cases not sophisticated enough. For the description of such nanosystems the theoretical and computational tools have to be selected carefully and have in particular to be improved in many cases. In this connection the interaction laws (potentials) between the atoms, forming a nanosystem, are critical functions because the structure and dynamics of such systems are very sensitive to small variations in the potentials. This point has been studied in detail. Various potential laws have been introduced and discussed in connection with applications. The most relevant simulations methods are quoted and their relevance for nanotechnology is discussed. In particular, the molecular dynamics method is described in detail. We give typical examples, which demonstrate the fact the molecular dynamics method is a powerful and reliable tool for the investigation of typical nanosystems with their large variety of structures and complex dynamical states. The examples deal with wear at the nanotechnological level and with metallic nanoclusters as building blocks.}
}
@article{WANG201854,
title = {Studying cognitive development in cultural context: A multi-level analysis approach},
journal = {Developmental Review},
volume = {50},
pages = {54-64},
year = {2018},
note = {Towards a Cultural Developmental Science},
issn = {0273-2297},
doi = {https://doi.org/10.1016/j.dr.2018.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0273229717301041},
author = {Qi Wang},
keywords = {Culture, Cognition, Episodic thinking, Memory, Multiple levels of analysis},
abstract = {I discuss a multi-level analysis approach in the study of cognitive development in cultural context. In this approach, culture is conceived of as a system and a process of symbolic mediation, where values, norms, and beliefs manifest in and through customs, rituals, and practices in directing and regulating both intrapersonal and interpersonal psychological functions. To capture the dynamic process in which cognitive development unfolds in cultural context, this approach examines the influence of culture on the developing cognitive skills between groups – group level analysis, between the child and socialization agents – dyadic level analysis, at the level of the child – individual level analysis, and within the child – situation level analysis. The temporal level analysis further situates cognitive development in historical time. By utilizing different analytical and methodological strategies, the multi-level analysis approach provides converging evidence for the development of cognitive skills in cultural context. Important challenges in this approach include the development of a “big picture” to guide the investigation, methodological training for research assistants, and difficulties in collecting, managing, and analyzing diverse datasets. I discuss the conceptual and methodological issues by using our work on the development of episodic thinking as an example.}
}
@article{NOH2006554,
title = {Computational tools for isotopically instationary 13C labeling experiments under metabolic steady state conditions},
journal = {Metabolic Engineering},
volume = {8},
number = {6},
pages = {554-577},
year = {2006},
issn = {1096-7176},
doi = {https://doi.org/10.1016/j.ymben.2006.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S1096717606000449},
author = {Katharina Nöh and Aljoscha Wahl and Wolfgang Wiechert},
keywords = {Instationary C metabolic flux analysis, C labeling experiment, C labeling dynamics, Parameter identifiability, Optimal experimental design, },
abstract = {13C metabolic flux analysis (MFA) has become an important and powerful tool for the quantitative analysis of metabolic networks in the framework of metabolic engineering. Isotopically instationary 13C MFA under metabolic stationary conditions is a promising refinement of classical stationary MFA. It accounts for the experimental requirements of non-steady-state cultures as well as for the shortening of the experimental duration. This contribution extends all computational methods developed for classical stationary 13C MFA to the instationary situation by using high-performance computing methods. The developed tools allow for the simulation of instationary carbon labeling experiments (CLEs), sensitivity calculation with respect to unknown parameters, fitting of the model to the measured data, statistical identifiability analysis and an optimal experimental design facility. To explore the potential of the new approach all these tools are applied to the central metabolism of Escherichia coli. The achieved results are compared to the outcome of the stationary counterpart, especially focusing on statistical properties. This demonstrates the specific strengths of the instationary method. A new ranking method is proposed making both an a priori and an a posteriori design of the sampling times available. It will be shown that although still not all fluxes are identifiable, the quality of flux estimates can be strongly improved in the instationary case. Moreover, statements about the size of some immeasurable pool sizes can be made.}
}
@incollection{WU202257,
title = {Chapter 3 - CTDA methodology},
editor = {Jiaping Wu and Junyu He and George Christakos},
booktitle = {Quantitative Analysis and Modeling of Earth and Environmental Data},
publisher = {Elsevier},
pages = {57-100},
year = {2022},
isbn = {978-0-12-816341-2},
doi = {https://doi.org/10.1016/B978-0-12-816341-2.00010-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128163412000101},
author = {Jiaping Wu and Junyu He and George Christakos},
keywords = {Methodological chain, Knowledge bases, Big data, Scales, Visualization, Chronotopologic statistics},
abstract = {Abstract The methodological characteristics of the chronotopologic data analysis chain are discussed. Various kinds of knowledge are considered and properly classified, and several illustrative examples in applied sciences are presented. Big data and data-driven analyses are critically reviewed, and their implementation carefully assessed. Data scale types are classifications considered in property- and attribute-oriented settings. Classical statistics inadequacies are pointed out and the need of a chronotopology-dependent statistics is outlined. The chronotopologic visualization thinking mode and techniques are briefly reviewed.}
}
@article{OLIVER2014289,
title = {Crack-path field and strain-injection techniques in computational modeling of propagating material failure},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {274},
pages = {289-348},
year = {2014},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2014.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0045782514000139},
author = {J. Oliver and I.F. Dias and A.E. Huespe},
keywords = {Fracture, Computational material failure, Strong discontinuities, Crack-path field, Strain injection, Finite elements with embedded discontinuities},
abstract = {The work presents two new numerical techniques devised for modeling propagating material failure, i.e. cracks in fracture mechanics or slip-lines in soil mechanics. The first one is termed crack-path-field technique and is conceived for the identification of the path of those cracks, or slip-lines, represented by strain-localization based solutions of the material failure problem. The second one is termed strain-injection, and consists of a procedure to insert, during specific stages of the simulation and in selected areas of the domain of analysis, goal oriented specific strain fields via mixed finite element formulations. In the approach, a first injection, of elemental constant strain modes (CSM) in quadrilaterals, is used, in combination of the crack-path-field technique, for obtaining reliable information that anticipates the position of the crack-path. Based on this information, in a subsequent stage, a discontinuous displacement mode (DDM) is efficiently injected, ensuring the required continuity of the crack-path across sides of contiguous elements. Combination of both techniques results in an efficient and robust procedure based on the staggered resolution of the crack-path-field and the mechanical failure problems. It provides the classical advantages of the “intra-elemental” methods for capturing complex propagating displacement discontinuities in coarse meshes, as E-FEM or X-FEM methods, with the non-code-invasive character of the crack-path-field technique. Numerical representative simulations of a wide range of benchmarks, in terms of the type of material and the failure problem, show the broad applicability, accuracy and robustness of the proposed methodology. The finite element code used for the simulations is open-source and available at http://www.cimne.com/compdesmat/.}
}
@incollection{RUNCO201469,
title = {Chapter 3 - Biological Perspectives on Creativity},
editor = {Mark A. Runco},
booktitle = {Creativity (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {69-108},
year = {2014},
isbn = {978-0-12-410512-6},
doi = {https://doi.org/10.1016/B978-0-12-410512-6.00003-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780124105126000035},
author = {Mark A. Runco},
keywords = {Split brain, Corpus callosum, Pre-frontal cortex, Cerebellum, Altered states of consciousness, Exercise, Stress, Dreams, Drugs, Genetics, Dopamine, Adoption studies, Genealogies},
abstract = {This chapter discusses various aspects of biological perspectives on creativity. Some of the research on creativity as of late involves the brain and biological correlates of originality, novelty, and insight. Handedness is sometimes used as an indication of hemispheric dominance or hemisphericity, with right-handed people being compared with left-handed people. There are several reports of left-handed persons outnumbering the right-handed in creative and eminent samples. Hemisphericity and other important brain structures and processes contributing to creative thinking and behavior have been studied with electroencephalogram (EEG), positron emission topography (PET), cerebral blood flow, and magnetic resonance imaging (MRI) techniques. Numerous EEG studies suggest that there are particular brain wave patterns and brain structures that are associated with creative problem solving, or at least specific phases within the problem solving process. EEGs suggest a complex kind of activity while individuals work on divergent thinking tasks. The complexity disappears when those same individuals work on convergent thinking tasks. It is found that the role of the prefrontal cortex in creative thinking and behavior comes from several sources and uses different methodologies.}
}
@incollection{SHI2021117,
title = {Chapter 4 - Mind model},
editor = {Zhongzhi Shi},
booktitle = {Intelligence Science},
publisher = {Elsevier},
pages = {117-149},
year = {2021},
isbn = {978-0-323-85380-4},
doi = {https://doi.org/10.1016/B978-0-323-85380-4.00004-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032385380400004X},
author = {Zhongzhi Shi},
keywords = {Mind model, Turing machine, physical symbol system, SOAR model, ACT-R model, CAM model, cognitive cycle, PMJ model},
abstract = {The technology of building mind model is often called mind modeling, which aims to explore and study the human thinking mechanism.}
}
@article{KEARNS2024543,
title = {The Application of Knowledge Engineering via the Use of a Biomimetic Digital Twin Ecosystem, Phenotype-Driven Variant Analysis, and Exome Sequencing to Understand the Molecular Mechanisms of Disease},
journal = {The Journal of Molecular Diagnostics},
volume = {26},
number = {7},
pages = {543-551},
year = {2024},
issn = {1525-1578},
doi = {https://doi.org/10.1016/j.jmoldx.2024.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S152515782400062X},
author = {William G. Kearns and Georgios Stamoulis and Joseph Glick and Lawrence Baisch and Andrew Benner and Dalton Brough and Luke Du and Bradford Wilson and Laura Kearns and Nicholas Ng and Maya Seshan and Raymond Anchan},
abstract = {Applied artificial intelligence, particularly large language models, in biomedical research is accelerating, but effective discovery and validation requires a toolset without limitations or bias. On January 30, 2023, the National Academies of Sciences, Engineering, and Medicine (NAS) appointed an ad hoc committee to identify the needs and opportunities to advance the mathematical, statistical, and computational foundations of digital twins in applications across science, medicine, engineering, and society. On December 15, 2023, the NAS released a 164-page report, “Foundational Research Gaps and Future Directions for Digital Twins.” This report described the importance of using digital twins in biomedical research. The current study was designed to develop an innovative method that incorporated phenotype-ranking algorithms with knowledge engineering via a biomimetic digital twin ecosystem. This ecosystem applied real-world reasoning principles to nonnormalized, raw data to identify hidden or "dark" data. Clinical exome sequencing study on patients with endometriosis indicated four variants of unknown clinical significance potentially associated with endometriosis-related disorders in nearly all patients analyzed. One variant of unknown clinical significance was identified in all patient samples and could be a biomarker for diagnostics. To the best of our knowledge, this is the first study to incorporate the recommendations of the NAS to biomedical research. This method can be used to understand the mechanisms of any disease, for virtual clinical trials, and to identify effective new therapies.}
}
@article{COOPER19821,
title = {Energy conservation in buildings: Part 2-A commentary on British government thinking},
journal = {Applied Energy},
volume = {10},
number = {1},
pages = {1-45},
year = {1982},
issn = {0306-2619},
doi = {https://doi.org/10.1016/0306-2619(82)90058-7},
url = {https://www.sciencedirect.com/science/article/pii/0306261982900587},
author = {Ian Cooper},
abstract = {Like my previous paper in this journal this commentary is focused on government statements published during the period 1974 to 1979. It is intended as an introductory guide aimed at two overlapping audiences. First, it is addressed to those interested in the reasoning which lies behind the Government's technical arguments on energy conservation in buildings. Secondly, it is directed towards those who seek to understand the social implications and consequences of this area of government endeavour. Not all the statements examined in this commentary represent official expressions of government policy. Some, indeed, are prefaced in their originals by specific disclaimers to this effect. Rather, they should be read as examples of arguments voiced by a variety of individuals and groups who are capable of informing, influencing or making decisions that affect this field of government activity. It should not be supposed that the government statements brought together in this commentary are necessarily consistent or coherent. Instead, in some cases at least, they seem incompatible and may even be irreconcilable. But, given that the source material is drawn from a wide range of documents with a broad range of authors and was published over a number of years, the extent of their unanimity is remarkable. As an introductory guide, this commentary is not offered as exhaustive, as representative of all aspects or shades of government thinking on this subject. On the contrary, only statements published in documents emanating from, or associated with, the Department of Energy have, for the most part, been cited. For the sake of brevity, statements published by other government departments with responsibility for the conservation of energy in buildings—such as the Department of the Environment—have not been drawn upon.}
}
@article{RUTHERFORD2023102255,
title = {“Don't [ruminate], be happy”: A cognitive perspective linking depression and anhedonia},
journal = {Clinical Psychology Review},
volume = {101},
pages = {102255},
year = {2023},
issn = {0272-7358},
doi = {https://doi.org/10.1016/j.cpr.2023.102255},
url = {https://www.sciencedirect.com/science/article/pii/S0272735823000132},
author = {Ashleigh V. Rutherford and Samuel D. McDougle and Jutta Joormann},
keywords = {Rumination, Emotion regulation, Working memory, Reinforcement learning, Depression},
abstract = {Anhedonia, a lack of pleasure in things an individual once enjoyed, and rumination, the process of perseverative and repetitive attention to specific thoughts, are hallmark features of depression. Though these both contribute to the same debilitating disorder, they have often been studied independently and through different theoretical lenses (e.g., biological vs. cognitive). Cognitive theories and research on rumination have largely focused on understanding negative affect in depression with much less focus on the etiology and maintenance of anhedonia. In this paper, we argue that by examining the relation between cognitive constructs and deficits in positive affect, we may better understand anhedonia in depression thereby improving prevention and intervention efforts. We review the extant literature on cognitive deficits in depression and discuss how these dysfunctions may not only lead to sustained negative affect but, importantly, interfere with an ability to attend to social and environmental cues that could restore positive affect. Specifically, we discuss how rumination is associated to deficits in working memory and propose that these deficits in working memory may contribute to anhedonia in depression. We further argue that analytical approaches such as computational modeling are needed to study these questions and, finally, discuss implications for treatment.}
}
@article{INTRONE201479,
title = {Improving decision-making performance through argumentation: An argument-based decision support system to compute with evidence},
journal = {Decision Support Systems},
volume = {64},
pages = {79-89},
year = {2014},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2014.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167923614001262},
author = {Joshua Introne and Luca Iandoli},
keywords = {Computer-supported argumentation, Evidence-based reasoning, Dempster–Shafer belief aggregation, Housing market prediction},
abstract = {While research has shown that argument based systems (ABSs) can be used to improve aspects of individual thinking and learning, relatively few studies have shown that ABSs improve decision performance in real world tasks. In this article, we strive to improve the value-proposition of ABSs for decision makers by showing that individuals can, with minimal training, use a novel ABS called Pendo to improve their ability to predict housing market trends. Pendo helps to weight and aggregate evidence through a computational engine to support evidence-based reasoning, a well-documented deficiency in human decision-making. It also supports individuals in the creation of knowledge artifacts that can be used to solve similar problems in the same domain. An unexpected finding and one of the major contributions of this work is that individual unaided decision-making performance was not predictive of an individual's performance with Pendo, even though the average performance of assisted individuals was higher. We infer that the skills activated when using the tool are substantially different than those enacted to solve the same problem without that tool. We discuss the implications this result has for the design and application of ABSs to decision-making, and possibly other decision support technologies.}
}
@article{POLZER2022100181,
title = {The rise of people analytics and the future of organizational research},
journal = {Research in Organizational Behavior},
volume = {42},
pages = {100181},
year = {2022},
issn = {0191-3085},
doi = {https://doi.org/10.1016/j.riob.2023.100181},
url = {https://www.sciencedirect.com/science/article/pii/S0191308523000011},
author = {Jeffrey T. Polzer},
keywords = {People analytics, Algorithms, Decision-making, Networks, Teams, Meetings, Culture, Monitoring, Computational social science, Organizational behavior},
abstract = {Organizations are transforming as they adopt new technologies and use new sources of data, changing the experiences of employees and pushing organizational researchers to respond. As employees perform their daily activities, they generate vast digital data. These data, when combined with established methods and new analytic techniques, create unprecedented opportunities for studying human behavior at work and have fueled the rise of people analytics as a new institutional field of practice. In this chapter, I describe the emerging field of people analytics and new organizational phenomena that accompany the use of data and algorithms. These practices are affecting how individuals, groups, and organizations function, ranging from decision-making processes and work procedures, to communication and collaboration, to attempts to monitor and control employees. In each of these domains, I describe recent research and propose new research directions. Many of these domains intersect with the emerging field of Computational Social Science, in which disciplinary scholars are applying computational methods to an expanding array of digitized data, pursuing interests that extend far into the organizational domain. Organizational scholars are well-positioned to bridge organizational and disciplinary advances to stay at the forefront of research on the future of work.}
}
@article{VASSILIADIS2024380,
title = {Reloading Process Systems Engineering within Chemical Engineering},
journal = {Chemical Engineering Research and Design},
volume = {209},
pages = {380-398},
year = {2024},
issn = {0263-8762},
doi = {https://doi.org/10.1016/j.cherd.2024.07.066},
url = {https://www.sciencedirect.com/science/article/pii/S0263876224004568},
author = {Vassilios S. Vassiliadis and Vasileios Mappas and Thomas A. Espaas and Bogdan Dorneanu and Adeniyi Isafiade and Klaus Möller and Harvey Arellano-Garcia},
keywords = {Chemical Engineering, Process Systems Engineering, Process model construction and deployment, Digital Twinning, Machine Learning},
abstract = {Established as a sub-discipline of Chemical Engineering in the 1960s by the late Professor R.W.H. Sargent at Imperial College London, Process Systems Engineering (PSE) has played a significant role in advancing the field, positioning it as a leading engineering discipline in the contemporary technological landscape. Rooted in Applied Mathematics and Computing, PSE aligns with the key components driving advancements in our modern, information-centric era. Sargent’s visionary foresight anticipated the evolution of early computational tools into fundamental elements for future technological and scientific breakthroughs, all while maintaining a central focus on Chemical Engineering. This paper aims to present concise and concrete ideas for propelling PSE into a new era of progress. The objective is twofold: to preserve PSE’s extensive and diverse knowledge base and to reposition it more prominently within modern Chemical Engineering, while also establishing robust connections with other data-driven engineering and applied science domains that play important roles in industrial and technological advancements. Rather than merely reacting to contemporary challenges, this article seeks to proactively create opportunities to lead the future of Chemical Engineering across its vital contributions in education, research, technology transfer, and business creation, fully leveraging its inherent multidisciplinarity and versatile character.}
}
@article{BARELI2013472,
title = {Sketching profiles: Awareness to individual differences in sketching as a means of enhancing design solution development},
journal = {Design Studies},
volume = {34},
number = {4},
pages = {472-493},
year = {2013},
note = {Special Issue: Articulating Design Thinking},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2013.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X13000197},
author = {Shoshi Bar-Eli},
keywords = {design processes, design research, design behavior, problem solving, design tools},
abstract = {This paper focuses on the differences between interior design students' design processes as derived from an analysis of their sketching and design behavior. By implementing qualitative methodologies in the analysis of the sketches produced in the conceptual phase of the design process, the experiment allows identifying sketching characteristics and profiles. The motivation is to show that sketches can serve as a tool to differentiate between designers and recognize their personal approach and design strategies. The results point to three distinct sketching profiles that characterize designers' use of sketches as a tool for thinking and communicating ideas during their solution generation process. Awareness to differences between students' sketches and design behavior may support the development of pedagogical concepts, strategies and tools.}
}
@article{GONG2023105530,
title = {Continuous time causal structure induction with prevention and generation},
journal = {Cognition},
volume = {240},
pages = {105530},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2023.105530},
url = {https://www.sciencedirect.com/science/article/pii/S0010027723001646},
author = {Tianwei Gong and Neil R. Bramley},
keywords = {Causal learning, Time, Prevention, Structure induction, Summary statistics},
abstract = {Most research into causal learning has focused on atemporal contingency data settings while fewer studies have examined learning and reasoning about systems exhibiting events that unfold in continuous time. Of these, none have yet explored learning about preventative causal influences. How do people use temporal information to infer which components of a causal system are generating or preventing activity of other components? In what ways do generative and preventative causes interact in shaping the behavior of causal mechanisms and their learnability? We explore human causal structure learning within a space of hypotheses that combine generative and preventative causal relationships. Participants observe the behavior of causal devices as they are perturbed by fixed interventions and subject to either regular or irregular spontaneous activations. We find that participants are capable learners in this setting, successfully identifying the large majority of generative, preventative and non-causal relationships but making certain attribution errors. We lay out a computational-level framework for normative inference in this setting and propose a family of more cognitively plausible algorithmic approximations. We find that participants’ judgment patterns can be both qualitatively and quantitatively captured by a model that approximates normative inference via a simulation and summary statistics scheme based on structurally local computation using temporally local evidence.}
}
@article{CAMPAGNOLO2007387,
title = {The Methods of Approximation and Lifting in Real Computation},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {167},
pages = {387-423},
year = {2007},
note = {Proceedings of the Third International Conference on Computability and Complexity in Analysis (CCA 2006)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2006.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S1571066107000229},
author = {Manuel L. Campagnolo and Kerry Ojakian},
keywords = {Computable Analysis, Real Recursive Functions, Elementary Computable},
abstract = {The basic motivation behind this work is to tie together various computational complexity classes, whether over different domains such as the naturals or the reals, or whether defined in different manners, via function algebras (Real Recursive Functions) or via Turing Machines (Computable Analysis). We provide general tools for investigating these issues, using a technique we call the method of approximation. We give the general development of this method, and apply it to obtain 2 theorems. First we connect the discrete operation of linear recursion (basically equivalent to the combination of bounded sums and bounded products) to linear differential equations, thus providing an alternative proof of the result from Campagnolo, Moore and Costa [M.L. Campagnolo, C. Moore and J. F. Costa, An analog characterization of the Grzegorczyk hierarchy, Journal of Complexity 18 (2002) 977–100]. Secondly, we extend this to prove a result similar to that of Bournez and Hainry [O. Bournez and E. Hainry, Elementarily computable functions over the real numbers and R-sub-recursive functions, Theoretical Computer Science 348 (2005) 130–147], providing a function algebra for the real functions computable in elementary time. Their proof involves simulating the operation of a Turing Machine using a function algebra. We avoid this simulation, using a technique we call “lifting,” which allows us to lift the classic result regarding the Kalmar elementary computable functions to a result on the reals. While we do not claim that our result is necessarily an improvement (perhaps just different), we do want to make the point that our two techniques appear readily applicable to other problems of this sort.}
}
@article{DASILVA2022402,
title = {A Predictive, Context-Dependent Stochastic Model for Engineering Applications},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {2},
pages = {402-407},
year = {2022},
note = {14th IFAC Workshop on Intelligent Manufacturing Systems IMS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.04.227},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322002282},
author = {Márcio J. {da Silva} and Gustavo Künzel and Carlos E. Pereira},
keywords = {Data Mining, Predictive Situation, Context Testing, Industrial Alarm System, Recommendation Systems},
abstract = {This work explores the architecture of a context-dependent probabilistic model. We identify opportunities for providing reminders to operators in their environment as a means to address information overload. Hence, there is a need to represent a state of knowledge and help them stay vigilant during their jobs. Along with the architectural improvements, which further specialize information flows and develop a data-driven approach, continual learning techniques covered events in a probabilistic graphical model called Context-Dependent Recommendation Systems (CD-RS). We demonstrated, as a result, the use of statistical thinking and Design of Experiments (DoE), which are most clear in conducting a suitable experiment. Moreover, the validation of the model and experiments of the novel architecture based on the collected data from a real case study demonstrates the value of the proposed methods.}
}
@article{STORLIE20091735,
title = {Implementation and evaluation of nonparametric regression procedures for sensitivity analysis of computationally demanding models},
journal = {Reliability Engineering & System Safety},
volume = {94},
number = {11},
pages = {1735-1763},
year = {2009},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2009.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0951832009001112},
author = {Curtis B. Storlie and Laura P. Swiler and Jon C. Helton and Cedric J. Sallaberry},
keywords = {Bootstrap, Confidence intervals, Meta-model, Nonparametric regression, Sensitivity analysis, Surrogate model, Uncertainty analysis, Variance decomposition},
abstract = {The analysis of many physical and engineering problems involves running complex computational models (simulation models, computer codes). With problems of this type, it is important to understand the relationships between the input variables (whose values are often imprecisely known) and the output. The goal of sensitivity analysis (SA) is to study this relationship and identify the most significant factors or variables affecting the results of the model. In this presentation, an improvement on existing methods for SA of complex computer models is described for use when the model is too computationally expensive for a standard Monte-Carlo analysis. In these situations, a meta-model or surrogate model can be used to estimate the necessary sensitivity index for each input. A sensitivity index is a measure of the variance in the response that is due to the uncertainty in an input. Most existing approaches to this problem either do not work well with a large number of input variables and/or they ignore the error involved in estimating a sensitivity index. Here, a new approach to sensitivity index estimation using meta-models and bootstrap confidence intervals is described that provides solutions to these drawbacks. Further, an efficient yet effective approach to incorporate this methodology into an actual SA is presented. Several simulated and real examples illustrate the utility of this approach. This framework can be extended to uncertainty analysis as well.}
}
@article{ALDAYA2024116708,
title = {Tachyons in “momentum-space” representation},
journal = {Nuclear Physics B},
volume = {1008},
pages = {116708},
year = {2024},
issn = {0550-3213},
doi = {https://doi.org/10.1016/j.nuclphysb.2024.116708},
url = {https://www.sciencedirect.com/science/article/pii/S0550321324002748},
author = {V. Aldaya and J. Guerrero and F.F. López-Ruiz},
abstract = {Obtaining the momentum space associated with tachyonic “particles” from the Poincaré group manifold proves to be rather intricate, departing very much from the ordinary dual to Minkowski space directly parametrized by space-time translations of the Poincaré group. In fact, although described by the constants of motion (Noether invariants) associated with space-time translations, they depend non-trivially on the parameters of the rotation subgroup. However, once the momentum space is parametrized by the Noether invariants, it behaves as that of ordinary particles. On the other hand, the evolution parameter is no longer the one associated with time translation, whose Noether invariant, Po, is now a basic one. Evolution takes place in a spatial direction. These facts not only make difficult the computation of the corresponding representation, but also force us to a sound revision of several traditional ingredients related to Cauchy hypersurface, scalar product and, of course, causality. After that, the theory becomes consistent and could shed new light on some special physical situations like inflation or traveling inside a black hole.}
}
@article{CIPRIANI2024102277,
title = {Personality traits and climate change denial, concern, and proactivity: A systematic review and meta-analysis},
journal = {Journal of Environmental Psychology},
volume = {95},
pages = {102277},
year = {2024},
issn = {0272-4944},
doi = {https://doi.org/10.1016/j.jenvp.2024.102277},
url = {https://www.sciencedirect.com/science/article/pii/S0272494424000501},
author = {Enrico Cipriani and Sergio Frumento and Angelo Gemignani and Danilo Menicucci},
keywords = {Climate change, Personality, Communication, Big five, Climate change denial, Climate change concern},
abstract = {Climate Change is a global issue which touches the lives of all human beings, each of whom have their own unique outlooks and motivations. Hence, the high degree of complexity which emerges from the involvement of such a large number of people might be better understood through the lenses of their individual differences. We performed a systematic review and meta-analysis following PRISMA guidelines. We searched keywords on Web of Science™ and Scopus®, and included peer-reviewed articles which quantitatively examined correlations between personality and climate attitudes. After screening, 74 papers were included in our review. From these articles, k = 100 samples were extracted and included in meta-analysis models. Our results show that Climate Change Denial is positively correlated with Social Dominance Orientation (r = 0.39) and Right-Wing Authoritarianism (r = 0.42), and negatively with Openness (r = −0.14), Conscientiousness (r = −0.05), Agreeableness (r = −0.11), Consideration of Future Consequences (r = −0.38), and Actively Open-Minded Thinking (r = −0.38). Concern for Climate Change correlates with Openness (r = 0.10), Neuroticism (r = 0.12), Consideration of Future Consequences (r = 0.34), and negatively with Social Dominance Orientation (r = -0.36) and Right-Wing Authoritarianism (r = −0.22). Finally, Proactivity towards Climate Change correlates positively with Openness (r = 0.17), Extraversion (r = 0.09), Agreeableness (r = 0.05), Neuroticism (r = 0.10), Consideration of Future Consequences (r = 0.39), and negatively with Social Dominance Orientation (r = -0.25) and Right-Wing Authoritarianism (r = -0.31). Moderation analysis shows geographical variations in the Social Dominance Orientation and Climate Denial relationship. We conclude that some personality traits – such as Openness – transversally affect climate change attitudes. Moreover, meta-analytic data suggest that the personality involvement in Climate Change may be dependent on the socio-political context of different countries. Future research, policies, and communication campaigns should take these peculiarities into account.}
}
@article{BARKER2023100569,
title = {An Inflection Point in Cancer Protein Biomarkers: What was and What's Next},
journal = {Molecular & Cellular Proteomics},
volume = {22},
number = {7},
pages = {100569},
year = {2023},
issn = {1535-9476},
doi = {https://doi.org/10.1016/j.mcpro.2023.100569},
url = {https://www.sciencedirect.com/science/article/pii/S1535947623000804},
author = {Anna D. Barker and Mario M. Alba and Parag Mallick and David B. Agus and Jerry S.H. Lee},
keywords = {proteomics, cancer biomarkers, protein biomarkers, complex adaptive systems, clinical proteomics},
abstract = {Biomarkers remain the highest value proposition in cancer medicine today—especially protein biomarkers. Despite decades of evolving regulatory frameworks to facilitate the review of emerging technologies, biomarkers have been mostly about promise with very little to show for improvements in human health. Cancer is an emergent property of a complex system, and deconvoluting the integrative and dynamic nature of the overall system through biomarkers is a daunting proposition. The last 2 decades have seen an explosion of multiomics profiling and a range of advanced technologies for precision medicine, including the emergence of liquid biopsy, exciting advances in single-cell analysis, artificial intelligence (machine and deep learning) for data analysis, and many other advanced technologies that promise to transform biomarker discovery. Combining multiple omics modalities to acquire a more comprehensive landscape of the disease state, we are increasingly developing biomarkers to support therapy selection and patient monitoring. Furthering precision medicine, especially in oncology, necessitates moving away from the lens of reductionist thinking toward viewing and understanding that complex diseases are, in fact, complex adaptive systems. As such, we believe it is necessary to redefine biomarkers as representations of biological system states at different hierarchical levels of biological order. This definition could include traditional molecular, histologic, radiographic, or physiological characteristics, as well as emerging classes of digital markers and complex algorithms. To succeed in the future, we must move past purely observational individual studies and instead start building a mechanistic framework to enable integrative analysis of new studies within the context of prior studies. Identifying information in complex systems and applying theoretical constructs, such as information theory, to study cancer as a disease of dysregulated communication could prove to be “game changing” for the clinical outcome of cancer patients.}
}
@article{LI2023106299,
title = {Constructing a link between multivariate titanium-based semiconductor band gaps and chemical formulae based on machine learning},
journal = {Materials Today Communications},
volume = {35},
pages = {106299},
year = {2023},
issn = {2352-4928},
doi = {https://doi.org/10.1016/j.mtcomm.2023.106299},
url = {https://www.sciencedirect.com/science/article/pii/S235249282300990X},
author = {Jiawei Li and Zhengxin Chen and Jiang Wu and Jia Lin and Ping He and Rui Zhu and Cheng Peng and Hai Zhang and Wenhao Li and Xu Fang and Hongtao Shen},
keywords = {Random forest model, Chemical formula, Components, Bandgap, Machine learning},
abstract = {Titanium-based semiconductors are wildly recognized as one of the most commonly used photocatalysts for photocatalysis. Energy band modulation is a key aspect of the catalytic activity of photocatalytic semiconductors, but the acquisition of semiconductor energy bands is still a complex and important task. In recent years, machine learning has played an important role in materials prediction, where the crystal structure of a material is usually used as input in energy band prediction. However, existing machine learning algorithms cannot accurately predict the energy bands of materials from structural components. Here, we convert the chemical formula into component descriptors after comparing first principles and ultraviolet-visible spectrophotometry (UV–vis) errors on bandgap values, and the component and bandgap values form a set of labeled data pairs. The chemical formula components are used as input to accurately predict the energy bands of the material. In our evaluation, the model outperforms existing machine learning methods in predicting energy bands, yielding mean absolute value errors of about 0.277 eV, and possesses a significant advantage in component prediction. In particular, this method of predicting the photocatalytic semiconductor energy band gap from chemical formula components provides a new way of thinking about photocatalyst selectivity.}
}
@article{LI2023119775,
title = {Neural representations of self-generated thought during think-aloud fMRI},
journal = {NeuroImage},
volume = {265},
pages = {119775},
year = {2023},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2022.119775},
url = {https://www.sciencedirect.com/science/article/pii/S1053811922008965},
author = {Hui-Xian Li and Bin Lu and Yu-Wei Wang and Xue-Ying Li and Xiao Chen and Chao-Gan Yan},
keywords = {Self-generated thoughts, Think-aloud fMRI, Natural language processing, Representational similarity analysis},
abstract = {Is the brain at rest during the so-called resting state? Ongoing experiences in the resting state vary in unobserved and uncontrolled ways across time, individuals, and populations. However, the role of self-generated thoughts in resting-state fMRI remains largely unexplored. In this study, we collected real-time self-generated thoughts during “resting-state” fMRI scans via the think-aloud method (i.e., think-aloud fMRI), which required participants to report whatever they were currently thinking. We first investigated brain activation patterns during a think-aloud condition and found that significantly activated brain areas included all brain regions required for speech. We then calculated the relationship between divergence in thought content and brain activation during think-aloud and found that divergence in thought content was associated with many brain regions. Finally, we explored the neural representation of self-generated thoughts by performing representational similarity analysis (RSA) at three neural scales: a voxel-wise whole-brain searchlight level, a region-level whole-brain analysis using the Schaefer 400-parcels, and at the systems level using the Yeo seven-networks. We found that “resting-state” self-generated thoughts were distributed across a wide range of brain regions involving all seven Yeo networks. This study highlights the value of considering ongoing experiences during resting-state fMRI and providing preliminary methodological support for think-aloud fMRI.}
}
@incollection{VALLERO2021601,
title = {Chapter 14 - The Future},
editor = {Daniel A. Vallero},
booktitle = {Environmental Systems Science},
publisher = {Elsevier},
pages = {601-613},
year = {2021},
isbn = {978-0-12-821953-9},
doi = {https://doi.org/10.1016/B978-0-12-821953-9.00004-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128219539000040},
author = {Daniel A. Vallero},
keywords = {Precautionary principle, Evidence-based risk assessment, Exposome, Translational science, Scientific workflow, Ontologies, Resilience, Data-driven decision-making, Precision science, Life cycle risk assessment (LCRA)},
abstract = {Solving and preventing environmental problems will continue to rely on systems thinking that translates and combines data, information, knowledge, and wisdom from numerous scientific and other perspectives. The chapter provides insights into possible directions for environmental systems science, especially ways to address complexities at every scale from cellular to planetary. Environmental scientists and engineers will engage in precision science and customized approaches to reduce risks, improve reliability and resilience, and ensure sustainability.}
}
@article{GUSTAFSON199557,
title = {Theory and computation of periodic solutions of autonomous partial differential equation boundary value problems, with application to the driven cavity problem},
journal = {Mathematical and Computer Modelling},
volume = {22},
number = {9},
pages = {57-75},
year = {1995},
issn = {0895-7177},
doi = {https://doi.org/10.1016/0895-7177(95)00168-2},
url = {https://www.sciencedirect.com/science/article/pii/0895717795001682},
author = {K. Gustafson},
keywords = {Navier-Stokes equations, Driven cavity problem, Pressure boundary condition, Vortex shedding, Hopf bifurcation},
abstract = {In ordinary differential equations, one distinguishes two cases: autonomous and nonautonomous. Roughly speaking, the theory of the latter is built upon the theory of the former. The same distinction should be applied to partial differential equations, where much less is known. Here I will focus on the question of the generation of periodic solutions for autonomous partial differential equation boundary value problems. Specifically, I consider the incompressible Navier-Stokes equations, and the important driven cavity problem. For simplicity, attention is restricted to two bifurcation parameters, the Reynolds number and the Aspect ratio. Only Dirichlet velocity boundary conditions are considered. Both the known theory and known computational results for the driven cavity are surveyed. The importance of computationally adhering to the div u = 0 condition to accurately simulate unsteady flows which will be qualitatively correct for the incompressible Navier-Stokes equations is stressed. The dependence of sustained periodicity upon the existence of highly localized vortex shedding sequences somewhere along the boundary is pointed out. A new analysis of the pressure boundary condition, based upon a general regularity principle, is given. A conjectured Hopf bifurcation criticality curve is explained.}
}
@article{ZOHDI20073927,
title = {Computation of strongly coupled multifield interaction in particle–fluid systems},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {196},
number = {37},
pages = {3927-3950},
year = {2007},
note = {Special Issue Honoring the 80th Birthday of Professor Ivo Babuška},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2006.10.040},
url = {https://www.sciencedirect.com/science/article/pii/S004578250700117X},
author = {T.I. Zohdi},
keywords = {Particle–fluid interaction, Multiple fields, Iterative methods},
abstract = {The present work develops a flexible and robust solution strategy to resolve coupled systems comprised of large numbers of flowing particles embedded within a fluid. A model problem, consisting of particles which may undergo inelastic collisions in the presence of near-field forces, is considered. The particles are surrounded by a continuous interstitial fluid which is assumed to obey the compressible Navier–Stokes equations. Thermal effects are also considered. Such particle/fluid systems are strongly coupled, due to the mechanical forces and heat transfer induced by the fluid onto the particles and vice-versa. Because the coupling of the various particle and fluid fields can dramatically change over the course of a flow process, a primary focus of this work is the development of a recursive “staggering” solution scheme, whereby the time-steps are adaptively adjusted to control the error associated with the incomplete resolution of the coupled interaction between the various solid particulate and continuum fluid fields. A central feature of the approach is the ability to account for the presence of particles within the fluid in a straightforward manner that can be easily incorporated within any standard computational fluid mechanics code based on finite difference, finite element or finite volume type discretization. A three dimensional example is provided to illustrate the overall approach.}
}
@article{CAGLAYAN2015131,
title = {Making sense of eigenvalue–eigenvector relationships: Math majors’ linear algebra – Geometry connections in a dynamic environment},
journal = {The Journal of Mathematical Behavior},
volume = {40},
pages = {131-153},
year = {2015},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2015.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0732312315000504},
author = {Günhan Caglayan},
keywords = {Undergraduate mathematics education, Dynamic geometry software, Visualization, Representation, Connection, Linear algebra, Eigenvectors, Eigenvalues, Matrices},
abstract = {The present qualitative case study on mathematics majors’ visualization of eigenvector–eigenvalue concepts in a dynamic environment highlights the significance of student-generated representations in a theoretical framework drawn from Sierpinska's (2000) modes of thinking in linear algebra. Such an approach seemed to provide the research participants with mathematical freedom, which resulted in an awareness of the multiple ways that eigenvalue–eigenvector relationships could be visualized in a manner that widened students’ repertoire of meta-representational competences (diSessa, 2004) in coordination with their preferred modes of visualization. Students’ expression of visual fluency in the course of making sense of the eigenvalue problem Au=λu associated with a variety of matrices occurred in different, yet not necessarily hierarchical modes of visualizations that differed from matrix to matrix: (i) synthetic/analytic mode manifested in the process of detecting eigenvectors when the sought eigenvector and the matrix-applied product vector were aligned in the same/opposite directions; (ii) analytic arithmetic mode manifested in the case of singular matrices (in the determination of the zero eigenvalue) and invertible matrices with nonreal eigenvalues; (iii) analytic structural mode, though rarely occurred, manifested in making sense of the trajectory (circle, ellipse, line segment) of the matrix-applied product vector and relating trajectory behavior to matrix type. While the connection between the thinking modes (Sierpinska, 2000) and the concreteness–necessity–generalizability triad (Harel, 2000) was not sharp, math majors still frequently implemented the CNG principles, which proved facilitatory tools in the evolution of students’ thinking about the eigenvalue–eigenvector relationships.}
}
@article{PRONK202443,
title = {Qualitative systems mapping in promoting physical activity and cardiorespiratory fitness: Perspectives and recommendations},
journal = {Progress in Cardiovascular Diseases},
volume = {83},
pages = {43-48},
year = {2024},
note = {Cardiorespiratory Fitness and Physical Activity: An Update of Evidence, Global Status and Recommendations},
issn = {0033-0620},
doi = {https://doi.org/10.1016/j.pcad.2024.02.013},
url = {https://www.sciencedirect.com/science/article/pii/S0033062024000355},
author = {Nicolaas P. Pronk and Bruce Y. Lee},
keywords = {Systems mapping, Causal loop diagram, Physical activity, Cardiorespiratory fitness, Complexity},
abstract = {The purpose of this report is to provide a perspective on the use of qualitative systems mapping, provide examples of physical activity (PA) systems maps, discuss the role of PA systems mapping in the context of iterative learning to derive breakthrough interventions, and provide actionable recommendations for future work. Systems mapping methods and applications for PA are emerging in the scientific literature in the study of complex health issues and can be used as a prelude to mathematical/computational modeling where important factors and relationships can be elucidated, data needs can be prioritized and guided, interventions can be tested and (co)designed, and metrics and evaluations can be developed. Examples are discussed that describe systems mapping based on Group Model Building or literature reviews. Systems maps are highly informative, illustrate multiple components to address PA and physical inactivity issues, and make compelling arguments against single intervention action. No studies were identified in the literature scan that considered cardiorespiratory fitness the focal point of a systems maps. Recommendations for future research and education are presented and it is concluded that systems mapping represents a valuable yet underutilized tool for visualizing the complexity of PA promotion.}
}
@incollection{GRIFFIN2020303,
title = {Information Graphics},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {303-314},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10563-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955105633},
author = {Amy L. Griffin},
keywords = {Big data, Cognition, Exploratory data analysis, Geovisualization, Graphics, Information dashboard, Information visualization, Interactivity, Perception, Semiotics, Storytelling, Visual analytics},
abstract = {Information graphics include a wide variety of static and dynamic visual representations of information such as diagrams, statistical graphics, and maps. These displays take different forms depending on the purpose for which the graphic is being constructed (e.g., for thinking or communication), characteristics of the data, potential visual display forms, and whether or not the information graphic is interactive. Choosing an appropriate method for representing data requires a basic understanding of the perceptual and cognitive processing that occurs in the human visual system. Information graphics can be constructed using a wide array of tools, but are increasingly constructed using computer code and distributed through the internet.}
}
@incollection{2004201,
title = {Chapter 6 Alternatives to purely Lagrangian computations},
editor = {Jonas A. Zukas},
series = {Studies in Applied Mechanics},
publisher = {Elsevier},
volume = {49},
pages = {201-250},
year = {2004},
booktitle = {Introduction to Hydrocodes},
issn = {0922-5382},
doi = {https://doi.org/10.1016/S0922-5382(04)80007-2},
url = {https://www.sciencedirect.com/science/article/pii/S0922538204800072},
abstract = {Publisher Summary
Lagrangian techniques deal with problems involving fast and transient loading. Lagrangian methods offer several advantages over the competition. Because Lagrangian codes cannot solve all the problems involving fast short-duration loading, other techniques shoulb be mentioned. The chapter describes the popular alternative methods, Euler codes, coupled Euler-Lagrange codes, arbitrary Lagrange-Euler (ALE) techniques, and meshless methods The advantages of Lagrange codes are offset by grid distortion. With large distortions, the time increment for advancing the computations is forced to approach zero, thus rendering the calculations uneconomical. The use of sliding interfaces and rezoning can extend the range of applicability of Lagrange codes to larger distortions. Similarly, the ability to handle large distortions in Euler codes is offset by the need to account for material transport. Pure Euler techniques are ideal for handling large distortions.}
}
@article{HEIDELBERGER1996627,
title = {Accelerating mean time to failure computations},
journal = {Performance Evaluation},
volume = {27-28},
pages = {627-645},
year = {1996},
issn = {0166-5316},
doi = {https://doi.org/10.1016/S0166-5316(96)90049-8},
url = {https://www.sciencedirect.com/science/article/pii/S0166531696900498},
author = {Philip Heidelberger and Jogesh K. Muppala and Kishor S. Trivedi},
keywords = {Markov chains, Mean time to failure, Numerical methods},
abstract = {In this paper we consider the problem of numerical computation of the mean time to failure (MTTF) in Markovian dependability and/or performance models. The problem can be cast as a system of linear equations which is solved using an iterative method preserving sparsity of the Markov chain matrix. For highly dependable systems, system failure is a rare event and the above system solution can take an extremely large number of iterations. We propose to solve the problem by dividing the computation in two parts. First, by making some of the high probability states absorbing, we compute the MTTF of the modified Markov chain. In a subsequent step, by solving another system of linear equations, we are able to compute the MTTF of the original model. We prove that for a class of highly dependable systems, the resulting method can speed up computation of the MTTF by orders of magnitude. Experimental results supporting this claim are presented. We also obtain bounds on the convergence rate for computing the mean entrance time of a rare set of states in a class of queueing models.}
}
@article{BLOCK20191003,
title = {What Is Wrong with the No-Report Paradigm and How to Fix It},
journal = {Trends in Cognitive Sciences},
volume = {23},
number = {12},
pages = {1003-1013},
year = {2019},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2019.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S1364661319302360},
author = {Ned Block},
keywords = {consciousness, perception, rivalry, frontal, global workspace, higher order},
abstract = {Is consciousness based in prefrontal circuits involved in cognitive processes like thought, reasoning, and memory or is it based in sensory areas in the back of the neocortex? The no-report paradigm has been crucial to this debate because it aims to separate the neural basis of the cognitive processes underlying post-perceptual decision and report from the neural basis of conscious perception itself. However, the no-report paradigm is problematic because, even in the absence of report, subjects might engage in post-perceptual cognitive processing. Therefore, to isolate the neural basis of consciousness, a no-cognition paradigm is needed. Here, I describe a no-cognition approach to binocular rivalry and outline how this approach can help to resolve debates about the neural basis of consciousness.}
}
@article{RIEBEL2024105084,
title = {Transient modeling of stratified thermal storage tanks: Comparison of 1D models and the Advanced Flowrate Distribution method},
journal = {Case Studies in Thermal Engineering},
volume = {61},
pages = {105084},
year = {2024},
issn = {2214-157X},
doi = {https://doi.org/10.1016/j.csite.2024.105084},
url = {https://www.sciencedirect.com/science/article/pii/S2214157X24011158},
author = {Adrian Riebel and Ian Wolde and Rodrigo Escobar and Rodrigo Barraza and José M. Cardemil},
keywords = {Sensible heat storage, TES, Thermal modeling, Transient simulation, Experimental validation},
abstract = {Thermal energy storage (TES) is one of the key technologies for enabling a higher deployment of renewable energy. In this context, the present study analyzes the modeling strategies of one of the most common TES systems: stratified thermal storage tanks. These systems are essential to many solar thermal installations and heat pumps, among other clean energy technologies. Three different one-dimensional tank models are compared by their computing speed and resilience to long time steps. Two of the models analyzed are numerical, one being explicit and the other one implicit, and the other is analytical. The models are validated against data from experiments carried out considering small-scale stratified tanks, showing that their performance can be improved by using the Advanced Flowrate Distribution (AFD) method. The results show that the analytical model maintains its accuracy with longer time steps and is robust against divergence. Conversely, the numerical models show equivalent performance for short time steps, while the computation time is reduced. Although the AFD method shows promising results by achieving an improvement of 43% in terms of Dynamic Time Warping, its parameter optimization must be generalized for different tank designs, flow rates, and temperatures.}
}
@article{ZHAN2024121679,
title = {Conceptualizing future groundwater models through a ternary framework of multisource data, human expertise, and machine intelligence},
journal = {Water Research},
volume = {257},
pages = {121679},
year = {2024},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2024.121679},
url = {https://www.sciencedirect.com/science/article/pii/S0043135424005803},
author = {Chuanjun Zhan and Zhenxue Dai and Shangxian Yin and Kenneth C. Carroll and Mohamad Reza Soltanian},
keywords = {Groundwater model, Deep learning, Machine intelligence, Multisource data, Human expertise},
abstract = {Groundwater models are essential for understanding aquifer systems behavior and effective water resources spatio-temporal distributions, yet they are often hindered by challenges related to model assumptions, parametrization, uncertainty, and computational efficiency. Machine intelligence, especially deep learning, promises a paradigm shift in overcoming these challenges. A critical examination of existing machine-driven methods reveals the inherent limitations, particularly in terms of the interpretability and the ability to generalize findings. To overcome these challenges, we develop a ternary framework that synergizes the valuable insights from multisource data, human expertise, and machine intelligence. This framework capitalizes on the distinct strengths of each element: the value and relevance of multisource data, the innovative capacity of human expertise, and the analytical efficiency of machine intelligence. Our goal is to conceptualize sustainable water management practices and enhance our understanding and predictive capabilities of groundwater systems. Unlike approaches that rely solely on abundant data, our framework emphasizes the quality and strategic use of available data, combined with human intellect and advanced computing, to overcome current limitations and pave the way for more realistic groundwater simulations.}
}