@incollection{CHANG2023235,
title = {Chapter 11 - Prefix sum (scan): An introduction to work efficiency in parallel algorithms},
editor = {Wen-mei W. Hwu and David B. Kirk and Izzat {El Hajj}},
booktitle = {Programming Massively Parallel Processors (Fourth Edition)},
publisher = {Morgan Kaufmann},
edition = {Fourth Edition},
pages = {235-261},
year = {2023},
isbn = {978-0-323-91231-0},
doi = {https://doi.org/10.1016/B978-0-323-91231-0.00022-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323912310000227},
author = {Li-Wen Chang and Juan Gómez-Luna and John Owens},
keywords = {Scan, prefix sum, reduction, linear recursion, resource allocation, work assignment, polynomial evaluation, Kogge-Stone, race condition, data dependence, double-buffering, work efficiency, Brent-Kung, segmented scan, adjacent (block) synchronization, stream-based scan},
abstract = {This chapter introduces parallel scan (prefix sum), an important parallel computation pattern and the concept of work efficiency for parallel algorithms. It introduces three styles of kernels: Kogge-Stone, Brent-Kung, and two-phase hybrid. These kernels each present a different tradeoff in terms of work efficiency, speed, and complexity. The chapter then introduces two hierarchical parallel scan algorithms that are designed to process arbitrarily long input lists while maintaining work efficiency.}
}
@article{JAMES20142124,
title = {Chaos forgets and remembers: Measuring information creation, destruction, and storage},
journal = {Physics Letters A},
volume = {378},
number = {30},
pages = {2124-2127},
year = {2014},
issn = {0375-9601},
doi = {https://doi.org/10.1016/j.physleta.2014.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S0375960114004873},
author = {Ryan G. James and Korana Burke and James P. Crutchfield},
keywords = {Chaos, Entropy rate, Bound information, Shannon information measures, Information diagram, Discrete-time maps},
abstract = {The hallmark of deterministic chaos is that it creates information—the rate being given by the Kolmogorov–Sinai metric entropy. Since its introduction half a century ago, the metric entropy has been used as a unitary quantity to measure a system's intrinsic unpredictability. Here, we show that it naturally decomposes into two structurally meaningful components: A portion of the created information—the ephemeral information—is forgotten and a portion—the bound information—is remembered. The bound information is a new kind of intrinsic computation that differs fundamentally from information creation: it measures the rate of active information storage. We show that it can be directly and accurately calculated via symbolic dynamics, revealing a hitherto unknown richness in how dynamical systems compute.}
}
@article{BARDT2022609,
title = {Recapturing meaning: Toward a new material-based design theory for architecture},
journal = {Frontiers of Architectural Research},
volume = {11},
number = {4},
pages = {609-617},
year = {2022},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2022.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S2095263522000280},
author = {Christopher Bardt},
keywords = {Architecture, Design, Material engagement, Material-based design, Meaning, Digital tools},
abstract = {Architects and students of architecture today are less physically engaged with modeling and drawing representations of proposed things and increasingly rely on digital means and methods that are transforming their embodied interactions with actual materials. The result is that meaning, which makes architecture so central to our cultures, is being diminished. It is being replaced by a misplaced belief that “better” buildings and architecture result from increased use of digital tools. To recapture meaning will require a new design theory for architecture that builds on material engagement theory and the critical role of resistance and sensuous reasoning in the design process, which material has historically provided.}
}
@article{MOFFETT2018905,
title = {Using molecular simulation to explore the nanoscale dynamics of the plant kinome},
journal = {Biochemical Journal},
volume = {475},
number = {5},
pages = {905-921},
year = {2018},
issn = {1470-8728},
doi = {https://doi.org/10.1042/BCJ20170299},
url = {https://www.sciencedirect.com/science/article/pii/S1470872818002680},
author = {Alexander S. Moffett and Diwakar Shukla},
keywords = {computational biophysics, kinases, molecular dynamics, plant kinome, structure and function},
abstract = {Eukaryotic protein kinases (PKs) are a large family of proteins critical for cellular response to external signals, acting as molecular switches. PKs propagate biochemical signals by catalyzing phosphorylation of other proteins, including other PKs, which can undergo conformational changes upon phosphorylation and catalyze further phosphorylations. Although PKs have been studied thoroughly across the domains of life, the structures of these proteins are sparsely understood in numerous groups of organisms, including plants. In addition to efforts towards determining crystal structures of PKs, research on human PKs has incorporated molecular dynamics (MD) simulations to study the conformational dynamics underlying the switching of PK function. This approach of experimental structural biology coupled with computational biophysics has led to improved understanding of how PKs become catalytically active and why mutations cause pathological PK behavior, at spatial and temporal resolutions inaccessible to current experimental methods alone. In this review, we argue for the value of applying MD simulation to plant PKs. We review the basics of MD simulation methodology, the successes achieved through MD simulation in animal PKs, and current work on plant PKs using MD simulation. We conclude with a discussion of the future of MD simulations and plant PKs, arguing for the importance of molecular simulation in the future of plant PK research.}
}
@article{MA2024104714,
title = {MAFT-SO: A novel multi-atlas fusion template based on spatial overlap for ASD diagnosis},
journal = {Journal of Biomedical Informatics},
volume = {157},
pages = {104714},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104714},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424001321},
author = {Yuefeng Ma and Xiaochen Mu and Tengfei Zhang and Yu Zhao},
keywords = {Autism spectrum disorder, Multi-atlas fusion, Spatial overlap degree, Brain networks},
abstract = {Autism spectrum disorder (ASD) is a common neurological condition. Early diagnosis and treatment are essential for enhancing the life quality of individuals with ASD. However, most existing studies either focus solely on the brain networks of subjects within a single atlas or merely employ simple matrix concatenation to represent the fusion of multi-atlas. These approaches neglected the natural spatial overlap that exists between brain regions across multi-atlas and did not fully capture the comprehensive information of brain regions under different atlases. To tackle this weakness, in this paper, we propose a novel multi-atlas fusion template based on spatial overlap degree of brain regions, which aims to obtain a comprehensive representation of brain networks. Specifically, we formally define a measurement of the spatial overlap among brain regions across different atlases, named spatial overlap degree. Then, we fuse the multi-atlas to obtain brain networks of each subject based on spatial overlap. Finally, the GCN is used to perform the final classification. The experimental results on Autism Brain Imaging Data Exchange (ABIDE) demonstrate that our proposed method achieved an accuracy of 0.757. Overall, our method outperforms SOTA methods in ASD/TC classification.}
}
@article{CARVALHAES2015174,
title = {The surface Laplacian technique in EEG: Theory and methods},
journal = {International Journal of Psychophysiology},
volume = {97},
number = {3},
pages = {174-188},
year = {2015},
note = {On the benefits of using surface Laplacian (current source density) methodology in electrophysiology},
issn = {0167-8760},
doi = {https://doi.org/10.1016/j.ijpsycho.2015.04.023},
url = {https://www.sciencedirect.com/science/article/pii/S0167876015001749},
author = {Claudio Carvalhaes and J. Acacio {de Barros}},
keywords = {Surface Laplacian, Surface Laplacian matrix, High-resolution EEG, EEG regularization, Spline Laplacian, Discrete Laplacian},
abstract = {This paper reviews the method of surface Laplacian differentiation to study EEG. We focus on topics that are helpful for a clear understanding of the underlying concepts and its efficient implementation, which is especially important for EEG researchers unfamiliar with the technique. The popular methods of finite difference and splines are reviewed in detail. The former has the advantage of simplicity and low computational cost, but its estimates are prone to a variety of errors due to discretization. The latter eliminates all issues related to discretization and incorporates a regularization mechanism to reduce spatial noise, but at the cost of increasing mathematical and computational complexity. These and several other issues deserving further development are highlighted, some of which we address to the extent possible. Here we develop a set of discrete approximations for Laplacian estimates at peripheral electrodes. We also provide the mathematical details of finite difference approximations that are missing in the literature, and discuss the problem of computational performance, which is particularly important in the context of EEG splines where data sets can be very large. Along this line, the matrix representation of the surface Laplacian operator is carefully discussed and some figures are given illustrating the advantages of this approach. In the final remarks, we briefly sketch a possible way to incorporate finite-size electrodes into Laplacian estimates that could guide further developments.}
}
@article{NESHYBA1965369,
title = {Graphic aid for geostrophic computations from vertical sections},
journal = {Deep Sea Research and Oceanographic Abstracts},
volume = {12},
number = {3},
pages = {369-371},
year = {1965},
issn = {0011-7471},
doi = {https://doi.org/10.1016/0011-7471(65)90008-2},
url = {https://www.sciencedirect.com/science/article/pii/0011747165900082},
author = {Steve Neshyba and David E. Amstutz}
}
@article{DING2014142,
title = {Two tales of how expectation of reward modulates behavior},
journal = {Current Opinion in Neurobiology},
volume = {29},
pages = {142-147},
year = {2014},
note = {SI: Neuromodulation},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2014.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0959438814001391},
author = {Long Ding and David J Perkel},
abstract = {Expectation of reward modulates many types of behaviors. Here we highlight two lines of research on reward-modulated perceptual decision making in primates and social context-modulated singing in songbirds, respectively. These two seemingly distinct behaviors are both known to involve cortico-basal ganglia-thalamic circuits. The underlying computations may be conceptualized using a simple, common framework. We summarize and compare our current knowledge of the two fields to motivate new experiments for each field, with the goal of finding general principles for how the brain implements reward-modulated behavior.}
}
@article{WESTHOLM20111897,
title = {Mirtrons: microRNA biogenesis via splicing},
journal = {Biochimie},
volume = {93},
number = {11},
pages = {1897-1904},
year = {2011},
note = {"Coding or Non-coding: need they be exclusive?"},
issn = {0300-9084},
doi = {https://doi.org/10.1016/j.biochi.2011.06.017},
url = {https://www.sciencedirect.com/science/article/pii/S0300908411002136},
author = {Jakub O. Westholm and Eric C. Lai},
keywords = {Mirtron, microRNA, Small RNA biogenesis, Splicing},
abstract = {A well-defined mechanism governs the maturation of most microRNAs (miRNAs) in animals, via stepwise cleavage of precursor hairpin transcripts by the Drosha and Dicer RNase III enzymes. Recently, several alternative miRNA biogenesis pathways were elucidated, the most prominent of which substitutes Drosha cleavage with splicing. Such short hairpin introns are known as mirtrons, and their study has uncovered related pathways that combine splicing with other ribonucleolytic machinery to yield Dicer substrates for miRNA biogenesis. In this review, we consider the mechanisms of splicing-mediated miRNA biogenesis, computational strategies for mirtron discovery, and the evolutionary implications of the existence of multiple miRNA biogenesis pathways. Altogether, the features of mirtron pathways illustrate unexpected flexibility in combining RNA processing pathways, and highlight how multiple functions can be encoded by individual transcripts.}
}
@article{KITSIOS2018,
title = {Translating Lung Microbiome Profiles into the Next-Generation Diagnostic Gold Standard for Pneumonia: a Clinical Investigator’s Perspective},
journal = {mSystems},
volume = {3},
number = {2},
year = {2018},
issn = {2379-5077},
doi = {https://doi.org/10.1128/msystems.00153-17},
url = {https://www.sciencedirect.com/science/article/pii/S2379507718000910},
author = {Georgios D. Kitsios},
keywords = {intensive care unit, lung microbiome, metagenomics, next-generation sequencing, pneumonia},
abstract = {Severe bacterial pneumonia is a major global cause of morbidity and mortality, yet current diagnostic approaches rely on identification of causative pathogens by cultures, which require extended incubation periods and often fail to detect relevant pathogens. Consequently, patients are prescribed broad-spectrum antibiotics in a “one-size-fits-all” manner, which may be inappropriate for their individual needs and promote antibiotic resistance.
ABSTRACT
Severe bacterial pneumonia is a major global cause of morbidity and mortality, yet current diagnostic approaches rely on identification of causative pathogens by cultures, which require extended incubation periods and often fail to detect relevant pathogens. Consequently, patients are prescribed broad-spectrum antibiotics in a “one-size-fits-all” manner, which may be inappropriate for their individual needs and promote antibiotic resistance. My research focuses on leveraging next-generation sequencing of microbial DNA directly from patient samples for the development of new, culture-independent definitions of pneumonia. In this perspective article, I discuss the current state of the field and focus on the conceptual and research design challenges for clinical translation. With ongoing technological advancements and application of computational biology methods for assessing clinical validity and utility, I anticipate that sequencing-based diagnostics will soon be able to positively disrupt the way we think about, diagnose, and treat pulmonary infections.}
}
@article{LARREAGALLEGOS2022946,
title = {Sustainability, resilience and complexity in supply networks: A literature review and a proposal for an integrated agent-based approach},
journal = {Sustainable Production and Consumption},
volume = {30},
pages = {946-961},
year = {2022},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2022.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S2352550922000100},
author = {Gustavo Larrea-Gallegos and Enrico Benetto and Antonino Marvuglia and Tomás Navarrete Gutiérrez},
keywords = {Sustainable supply network, Simulation, Agent based modelling, Disruption mitigation, Supply chain resilience},
abstract = {Supply Networks (SN) can be seriously affected by unplanned disruptions producing important consequences on system’s functioning. These alterations may have implications over dimensions of sustainability due to the re-adaptation of the network to cope with the disruptive event. In this sense, it is relevant to understand how sustainability can be measured while considering aspects like resilience and network’s dynamism. This article presents a critical review to enhance the understanding of sustainability assessment of supply networks affected by disruptions under a CAS perspective. A non-systematic literature search was conducted where relevant studies were identified. The dissociation between sustainability and resilience observed in literature was discussed from motivational, temporal and methodological perspectives. The review led to the proposition of four principles that underpin the conceptual foundations that should guide the development of any complexity-driven sustainability assessment methodology (SAM). Moreover, using agent-based modelling as the core computational paradigm, a SAM framework was outlined as a first step to implement a functioning tool that embeds the new assessment approach. Finally, the article concludes that sustainability should adopt a complexity-oriented approach when analysing disruptions. Challenges for future research such as delimitation of sustainability boundaries and validation of models are also discussed.}
}
@incollection{20241,
title = {Preface},
editor = {Manuel Yáñez and Russell J. Boyd},
booktitle = {Comprehensive Computational Chemistry (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {1-4},
year = {2024},
isbn = {978-0-12-823256-9},
doi = {https://doi.org/10.1016/B978-0-12-821978-2.09005-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012821978209005X}
}
@article{MOGLES201859,
title = {Systemic approaches to incident analysis in aviation: Comparison of STAMP, agent-based modelling and institutions},
journal = {Safety Science},
volume = {108},
pages = {59-71},
year = {2018},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2018.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0925753517309943},
author = {Nataliya Mogles and Julian Padget and Tibor Bosse},
keywords = {Aviation safety, Norms, Cognitive functioning, Socio-technical systems, Incident analysis},
abstract = {The rapid development and increasing complexity of modern socio-technical systems suggest an urgent need for systemic safety analysis approaches because traditional linear models cannot cope with this complexity. In the aviation safety literature, among systemic accident and incident analysis methods, Systems Theoretic Accident Modelling and Processes (STAMP) and Agent-based modelling (ABM) are the most cited ones. STAMP is a qualitative analysis approach known for its thoroughness and comprehensiveness. Computational ABM approach is a formal quantitative method which proved to be suitable for modelling complex flexible systems. In addition, from a legal point of view, formal systemic institutional modelling potentially provides an interesting contribution to accident and incident analysis. The current work compares three systemic modelling approaches: STAMP, ABM and institutional modelling applied to a case study in an aviation domain.}
}
@article{MAHON2024108841,
title = {Reciprocal interactions among parietal and occipito-temporal representations support everyday object-directed actions},
journal = {Neuropsychologia},
volume = {198},
pages = {108841},
year = {2024},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2024.108841},
url = {https://www.sciencedirect.com/science/article/pii/S0028393224000563},
author = {Bradford Z. Mahon and Jorge Almeida},
abstract = {Everyday interactions with common manipulable objects require the integration of conceptual knowledge about objects and actions with real-time sensory information about the position, orientation and volumetric structure of the grasp target. The ability to successfully interact with everyday objects involves analysis of visual form and shape, surface texture, material properties, conceptual attributes such as identity, function and typical context, and visuomotor processing supporting hand transport, grasp form, and object manipulation. Functionally separable brain regions across the dorsal and ventral visual pathways support the processing of these different object properties and, in cohort, are necessary for functional object use. Object-directed grasps display end-state-comfort: they anticipate in form and force the shape and material properties of the grasp target, and how the object will be manipulated after it is grasped. End-state-comfort is the default for everyday interactions with manipulable objects and implies integration of information across the ventral and dorsal visual pathways. We propose a model of how visuomotor and action representations in parietal cortex interact with object representations in ventral and lateral occipito-temporal cortex. One pathway, from the supramarginal gyrus to the middle and inferior temporal gyrus, supports the integration of action-related information, including hand and limb position (supramarginal gyrus) with conceptual attributes and an appreciation of the action goal (middle temporal gyrus). A second pathway, from posterior IPS to the fusiform gyrus and collateral sulcus supports the integration of grasp parameters (IPS) with the surface texture and material properties (e.g., weight distribution) of the grasp target. Reciprocal interactions among these regions are part of a broader network of regions that support everyday functional object interactions.}
}
@article{HOOSHYAR2022116670,
title = {GameDKT: Deep knowledge tracing in educational games},
journal = {Expert Systems with Applications},
volume = {196},
pages = {116670},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.116670},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422001555},
author = {Danial Hooshyar and Yueh-Min Huang and Yeongwook Yang},
keywords = {Learner model, Deep knowledge tracing, Educational game, Prediction of player performance, Deep learning},
abstract = {Despite the multiple deep knowledge tracing (DKT) methods developed for intelligent tutoring systems and online learning environments, there exists only a few applications of such methods in educational computer games. One key challenge is that a player may deploy several interweaved and overlapped skills during gameplay, making the assessment task nontrivial. In this research, we present a generalizable DKT approach called GameDKT that integrates state-of-the-art machine learning with domain knowledge to model the learners’ knowledge state during gameplay, in an attempt to monitor and trace their proficiency level for the different skills required for educational games. Our findings reveal that GameDKT approach could successfully predict the performance of players in the coming game task using the cross-validated CNN model with accuracy and AUC of roughly 85% and 0.913, respectively, thus outperforming the MLP baseline model by up to 14%. When the performance of players is forecasted for up to four game tasks in advance, results show that the CNN model can achieve more than 70% accuracy. Interestingly, this model seems to be better and faster at identifying local patterns and it could achieve a higher performance compared to RNN and LSTM in both one-step and multi-step prediction of learners’ performance in game tasks.}
}
@article{VAZQUEZCANO2023101380,
title = {ChatGPT: The brightest student in the class},
journal = {Thinking Skills and Creativity},
volume = {49},
pages = {101380},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101380},
url = {https://www.sciencedirect.com/science/article/pii/S1871187123001487},
author = {Esteban Vázquez-Cano and José M. Ramírez-Hurtado and José M. Sáez-López and Eloy López-Meneses},
keywords = {ChatGPT, Summarizing, Assessment, Content, Style},
abstract = {This paper presents a research study that evaluated the score ChatGPT would get when summarizing a reading comprehension text from the PISA international tests with a prompt that made it simulate doing this as if it were a 15-year-old student. For this purpose, the text was camouflaged among 30 other summaries made by real 15-year-old students and was evaluated by 30 Spanish language teachers with different profiles in terms of age, professional experience, and gender who were unaware that one of the texts was made by artificial intelligence (AI). The evaluation of the summary, for which a homogeneous rubric is used, is based on two fundamental criteria: content and style. For the data analysis descriptive and inferential statistical techniques were used. The results show that the ChatGPT summary obtained the best marks in terms of content and style, with its respective marks being 3 and 2.5 points higher than those of the students. Therefore, we can deduce that the style and content of the ChatGPT summary greatly exceeded those presented by the students. These results are independent of the ages, levels of professional experience, and genders of the teachers who corrected the summary. The integration of AI tools such as ChatGPT must be based on solid methodological proposals that integrate their use from a creative and critical perspective that allows learning with the support of these tools and not using them as substitutes for the development of basic student competencies.}
}
@incollection{YADAV2024221,
title = {Chapter 11 - Smart meter data management challenges},
editor = {Vijay K. Sood and Monalisa Biswal and Saumendra Sarangi and Hassan Haes Alhelou},
booktitle = {Smart Metering},
publisher = {Elsevier},
pages = {221-256},
year = {2024},
isbn = {978-0-443-15317-4},
doi = {https://doi.org/10.1016/B978-0-443-15317-4.00002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443153174000026},
author = {Pankaj Kumar Yadav and Monalisa Biswal and Haripriya Vemuganti},
keywords = {Smart grid, Smart meters, Meter data management system, Data storage, Data analytics, Big data, Challenges, Cyber security},
abstract = {Modernizing the energy industry and using the full potential of smart meter technologies need effective handling of smart meter data. A new era of data-driven energy management has begun as a result of the widespread use of smart meters, providing unrivaled prospects for increased sustainability and efficiency. However, these benefits are accompanied by serious problems with how data from smart meters is handled. As utility, companies and energy providers deploy smart meters on a large scale; they face numerous challenges in effectively collecting, storing, and utilizing the vast amounts of data generated by these devices. The technical challenges stem from the sheer volume and velocity of data produced by smart meters, which demands robust computational resources and scalable data storage and processing solutions. Additionally, sophisticated analytical techniques and domain knowledge in the energy industry are needed to interpret and analyze the data in a way that yields actionable insights. This chapter explores the main obstacles that energy suppliers and utility companies must overcome in order to efficiently manage and make use of the enormous volumes of data produced by smart meters. The importance of tackling the problems with smart meter data processing is highlighted in this chapter. Utility companies may unleash the entire potential of smart meter data for effective energy management and make a contribution to a sustainable energy future by investing in cutting-edge technology solutions, data security measures, and customer engagement methods.}
}
@article{DAI2024101899,
title = {Dual-contrast pedagogy for AI literacy in upper elementary schools},
journal = {Learning and Instruction},
volume = {91},
pages = {101899},
year = {2024},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2024.101899},
url = {https://www.sciencedirect.com/science/article/pii/S0959475224000264},
author = {Yun Dai},
keywords = {Artificial intelligence, AI literacy, Pedagogy, Instructional design, Human-AI comparison},
abstract = {Background
Advances in artificial intelligence (AI) have highlighted the need to equip young students with basic AI-related knowledge, skills, values, and attitudes. However, pedagogical design for AI literacy remains a critical challenge, especially for upper elementary students aged 10–12.
Aims
This design-based study had two goals: to develop a pedagogical approach for AI literacy in upper elementary education and to empirically assess this approach through an experiment.
Sample
One hundred forty-seven sixth graders in an upper elementary school were randomly assigned to a control group (n = 75) and an experimental group (n = 72).
Methods
Following a theory-informed design convention, we proposed a dual-contrast pedagogical (DCP) approach. This approach centers on human-AI comparisons by integrating analogies and cognitive conflicts. Two teaching examples on machine learning and large language models were provided. The experimental group was taught with the DCP approach, while the control group received conventional direct instruction. Data drawn from assessment tasks and questionnaires were subjected to two-way analyses of variance and covariance.
Results
The experimental group demonstrated significantly higher performance in AI knowledge, skills, and ethical awareness. They also exhibited a significant increase in AI learning confidence and intrinsic motivation and a significant decrease in learning anxiety.
Conclusions
The DCP approach significantly improved students’ learning performance and attitudes, demonstrating its effectiveness in promoting AI literacy. This study highlights the pedagogical value of human-AI comparisons in teaching AI, while contributing to a research agenda on the cognitive and conceptual aspects of AI education.}
}
@article{LUALDI201918,
title = {Statistical analysis of proteomics data: A review on feature selection},
journal = {Journal of Proteomics},
volume = {198},
pages = {18-26},
year = {2019},
note = {10 Year Anniversary of Proteomics},
issn = {1874-3919},
doi = {https://doi.org/10.1016/j.jprot.2018.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1874391918304263},
author = {Marta Lualdi and Mauro Fasano},
keywords = {Inductive reasoning, Dimensionality and Sparsity, Feature selection, Proteomics signature},
abstract = {The spread of “-omics” strategies has strongly changed the way of thinking about the scientific method. Indeed, managing huge amounts of data imposes the replacement of the classical deductive approach with a data-driven inductive approach, so to generate mechanistical hypotheses from data. Data reduction is a crucial step in the process of proteomics data analysis, because of the sparsity of significant features in big datasets. Thus, feature selection methods are applied to obtain a set of features based on which a proteomics signature can be drawn, with a functional significance (e.g., classification, diagnosis, prognosis). In this frame, the aim of the present review article is to give an overview of the methods available for proteomics data analysis, with a focus on biomedical translational research. Suggestions for the choice of the most appropriate standard statistical procedures are presented to perform data reduction by feature selection, cross-validation and functional analysis of proteomics profiles.
Significance
The proteome, including all so-called “proteoforms”, represents the highest level of complexity of biomolecules when compared to the other “-omes” (i.e., genome, transcriptome). For this reason, the use of proper data reduction strategies is mandatory for proteomics data analysis. However, the strategies to be employed for feature selection must be carefully chosen, since many different approaches exist based on both input data and desired output. So far, a well-established decision-making workflow for proteomics data analysis is lacking, opening up to misleading and incorrect data analysis and interpretation. In this review article many statistical approaches are described and compared for their application in the field of biomedical research, in order to suggest the reader the most suitable analysis pathway and to avoid mistakes.}
}
@article{TIPPAREDDY2023,
title = {Radiology Reading Room for the Future: Harnessing the Power of Large Language Models Like ChatGPT},
journal = {Current Problems in Diagnostic Radiology},
year = {2023},
issn = {0363-0188},
doi = {https://doi.org/10.1067/j.cpradiol.2023.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S0363018823001330},
author = {Charit Tippareddy and Sirui Jiang and Kaustav Bera and Nikhil Ramaiya},
abstract = {Radiology has usually been the field of medicine that has been at the forefront of technological advances, often being the first to wholeheartedly embrace them. Whether it's from digitization to cloud side architecture, radiology has led the way for adopting the latest advances. With the advent of large language models (LLMs), especially with the unprecedented explosion of freely available ChatGPT, time is ripe for radiology and radiologists to find novel ways to use the technology to improve their workflow. Towards this, we believe these LLMs have a key role in the radiology reading room not only to expedite processes, simplify mundane and archaic tasks, but also to increase the radiologist's and radiologist trainee's knowledge base at a far faster pace. In this article, we discuss some of the ways we believe ChatGPT, and the likes can be harnessed in the reading room.}
}
@article{ZULNAIDI2024104151,
title = {Ethical mediation: The influence of mathematics teachers cooperation on readiness for the industrial revolution era in Indonesia and Malaysia},
journal = {Acta Psychologica},
volume = {243},
pages = {104151},
year = {2024},
issn = {0001-6918},
doi = {https://doi.org/10.1016/j.actpsy.2024.104151},
url = {https://www.sciencedirect.com/science/article/pii/S0001691824000283},
author = {Hutkemri Zulnaidi and Nofouz Mafarja and Suzieleez Syrene Abdul Rahim and Umi Kalsum Mohd Salleh},
keywords = {Ethics, Era of industrial revolution, Mathematics teacher cooperation, Readiness},
abstract = {This study contributes to the existing body of research by examining the mediating effect of ethics in the relationship between mathematics teacher cooperation and readiness. It fills a gap in the literature by investigating the ethical dimensions of collaboration and their impact on readiness for the industrial revolution. This study aims to determine the mediator effect of ethics between the relationship of Mathematics Teacher Cooperation and Readiness in Facing the Era of Industrial Revolution. The study involved a total of 231 mathematics teachers in Indonesia and a total of 384 mathematics teachers in Malaysia using simple random sampling. A survey was conducted to determine the readiness of mathematics teachers in facing the industrial revolution. This study used SEM analysis (using AMOS software) to determine the model of teacher readiness facing the era of industrial revolution such as the direct effect of mathematics teacher cooperation and readiness in facing the era of industrial revolution, the essence of ethics as mediators of the relationship between mathematics teacher cooperation and readiness in facing the era of industrial revolution. The study findings showed collaboration has significant effect on IR4.0 readiness and the direct effect of collaboration on ethics was also significant. Indirectly, ethics has a significant mediating effect in the contribution between collaboration on the readiness of IR4.0 among mathematics teachers in Indonesia and Malaysia. A partial mediator occurred in the results of this study. In conclusion, the study's implications and recommendations emphasize the importance of collaborative practices, ethics, and cross-cultural considerations in preparing mathematics teachers for the Industrial Revolution era in Indonesia and Malaysia. These recommendations highlight the significance of policy support, professional development, ethical guidelines, and research-informed practices to enhance readiness for the challenges brought about by technological advancements.}
}
@article{YU2024e24289,
title = {The application and challenges of ChatGPT in educational transformation: New demands for teachers' roles},
journal = {Heliyon},
volume = {10},
number = {2},
pages = {e24289},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e24289},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024003207},
author = {Hao Yu},
keywords = {ChatGPT, Artificial intelligence, Teacher education, Teacher literacy, Educational transformation},
abstract = {With the rapid development of information technology, artificial intelligence has demonstrated great potential in promoting educational transformation. In November 2022, the release of the artificial intelligence product ChatGPT attracted widespread attention, particularly in the field of education, sparking heated discussions among scholars. As a language processing tool, ChatGPT can not only answer user questions but also complete user-specified tasks and even continuously optimize task performance. However, while possessing powerful features, ChatGPT also has some shortcomings that need improvement, such as the accuracy of answering questions, data pollution issues, ethical and safety concerns, and the risk of knowledge plagiarism. In the process of promoting school education reform, the application of ChatGPT brings both opportunities and challenges. Moreover, ChatGPT's emergence offers teachers an opportunity to reflect on their professional value and sets higher demands for them.}
}
@incollection{KEESMAN2002415,
title = {Chapter 18 Parametric change as the agent of control},
editor = {M.B. Beck},
series = {Developments in Environmental Modelling},
publisher = {Elsevier},
volume = {22},
pages = {415-424},
year = {2002},
booktitle = {Environmental Foresight and Models},
issn = {0167-8892},
doi = {https://doi.org/10.1016/S0167-8892(02)80019-7},
url = {https://www.sciencedirect.com/science/article/pii/S0167889202800197},
author = {K.J. Keesman},
abstract = {Publisher Summary
This chapter discusses the variation on the conventional theme of control system design, transposing the roles fulfilled by the model's parameters (α) and the system's controls (u) in that problem formulation. There is a trade-off between tracking and disturbance rejection, which is a trade-off between making both the sensitivity and the complementary sensitivity functions small. The chapter concludes that for a restricted class of environmental problems with structurally changing dynamics, appropriate controllers can be designed. The class is restricted, because it only covers systems with expected or desired changes and, in practice, more complex design requirements usually have to be met. The forms of logic and ways of conceptual thinking mobilized in control theory have rarely been turned to advantage on the subjects of environmental foresight and management, such as those of how communities adapt their understanding of their cherished pieces of the environment. The use and significance of any result that can be so computed for a given environmental system, thus, remain to be discerned.}
}
@article{ARQUB202210539,
title = {Development of the reproducing kernel Hilbert space algorithm for numerical pointwise solution of the time-fractional nonlocal reaction-diffusion equation},
journal = {Alexandria Engineering Journal},
volume = {61},
number = {12},
pages = {10539-10550},
year = {2022},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2022.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S1110016822002605},
author = {Omar Abu Arqub and Mohamed S. Osman and Choonkil Park and Jung Rye Lee and Hamed Alsulami and Mohammed Alhodaly},
keywords = {Fractional nonlocal reaction-diffusion equation, Reproducing kernel Hilbert space, Caputo time-fractional partial derivative, Numerical pointwise solution},
abstract = {It is notable that, the nonlocal reaction-diffusion equation carries math and computational physics to the core of extremely dynamic multidisciplinary studies that emerge from a huge assortment of uses. In this investigation, a totally new methodology for building a locally numerical pointwise solution is given by the agent the reproducing kernel algorithm. This is done utilizing a couple of generalized Hilpert spaces and their corresponding Green functions. The proposed calculation algorithm is applied to certain scalar issues problems to figure the arrangement solutions with Dirichlet constraints. By applying the procedures of the Gram–Schmidt process, orthonormalizing the basis, and truncating the optimized series, the approximate solutions are drawn, tabulated, and sketched. Introduced mathematical outcomes not only show the hidden superiority of the algorithm but also show its accurate efficiency. Finally, focused notes and futures planning works are mentioned with the most-used references.}
}
@article{TYLER202017,
title = {Real-time, personalized medicine through wearable sensors and dynamic predictive modeling: A new paradigm for clinical medicine},
journal = {Current Opinion in Systems Biology},
volume = {20},
pages = {17-25},
year = {2020},
issn = {2452-3100},
doi = {https://doi.org/10.1016/j.coisb.2020.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S2452310020300068},
author = {Jonathan Tyler and Sung Won Choi and Muneesh Tewari},
keywords = {Early disease detection, Nonlinear disease evolution, Wearable technologies, Landscape dynamic network biomarker, Personalized medicine, Acute graft-versus-host disease, Sensors, Devices, Stem cell transplant},
abstract = {Accurately predicting the onset and course of a disease in an individual is a major unmet challenge in medicine due to the complex and dynamic nature of disease progression. Continuous data from wearable technologies and biomarker data with a fine time resolution provide a unique opportunity to learn more about disease evolution and to usher in a new era of personalized and real-time medicine. Herein, we propose the potential of real-time, continuously measured physiological data as a noninvasive biomarker approach for detecting disease transitions, using allogeneic hematopoietic stem cell transplant patient care as an example. In addition, we review a recent computational technique, the landscape dynamic network biomarker method, that uses biomarker data to identify transition states in disease progression and explore how to use it with both biomarker and physiological data for earlier detection of graft-versus-host disease specifically. Throughout, we argue that increased collaboration across multiple fields is essential to realizing the full potential of wearable and biomarker data in a new paradigm of personalized and real-time medicine.}
}
@article{KOSTER201823,
title = {Is perception of placement universal? A mixed methods perspective on linguistic relativity},
journal = {Lingua},
volume = {207},
pages = {23-37},
year = {2018},
issn = {0024-3841},
doi = {https://doi.org/10.1016/j.lingua.2018.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0024384117304941},
author = {Dietha Koster and Teresa Cadierno},
keywords = {Sapir-Whorf hypothesis, Thinking for Speaking, Grounded Cognition, Placement events, Object orientation},
abstract = {This paper aims to advance theory on how speakers of different languages perceive the act of placement. German and Spanish verbs for example, differ in the specification of object position (e.g., He stands/lays-puts the binoculars on the shelf). Do speakers of these languages perceive placement events differently? This question relates to the notion of linguistic relativity. We report empirical data obtained with methods not yet applied to placement. These methods stem from three popular theoretical paradigms on language and thought. We examine whether placement verbs affect how speakers categorize (Experiment 1); memorize (Experiment 2) and mentally simulate (Experiment 3) object orientation. For three behavioral tasks, we compare accuracy and reaction time data of native speakers of German (N=80) and Spanish (N=50). Results suggest that German speakers do not categorize object position differently or make mental simulations of object orientation. They do show that German speakers have better recognition memory for object position than Spanish speakers. These findings suggest that language-specific effects may occur for some but not all mental processes. Future work should fine-tune reported methods to advance theory on perception of placement and should strive to combine methods to gain a multifaceted perspective on linguistic relativity.}
}
@incollection{HWU2023515,
title = {Chapter 23 - Conclusion and outlook},
editor = {Wen-mei W. Hwu and David B. Kirk and Izzat {El Hajj}},
booktitle = {Programming Massively Parallel Processors (Fourth Edition)},
publisher = {Morgan Kaufmann},
edition = {Fourth Edition},
pages = {515-518},
year = {2023},
isbn = {978-0-323-91231-0},
doi = {https://doi.org/10.1016/B978-0-323-91231-0.00020-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323912310000203},
author = {Wen-mei W. Hwu and David B. Kirk and Izzat {El Hajj}},
keywords = {Computational thinking, parallel patterns, golden age of computing, self-driving cars, individualized medicine},
abstract = {This chapter summarizes the main parts of the book. It then concludes the book by offering an outlook of how parallel programming is contributing and will continue to contribute to new innovations in science and technology.}
}
@article{SALO2024103815,
title = {The art of the ‘common good’: Property and nature values in strategic land-use planning in Finland},
journal = {Environmental Science & Policy},
volume = {159},
pages = {103815},
year = {2024},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2024.103815},
url = {https://www.sciencedirect.com/science/article/pii/S1462901124001497},
author = {Matti Salo and Sari Puustinen and Pekka Jounela and Harri Hänninen and Juha Hiedanpää},
keywords = {Common good, Commons, Finland, Land-use, Property, Strategic planning},
abstract = {Cutting across many biophysical, institutional, cultural, and psychological boundaries, the quest for the ‘common good’ is an enduring legitimation for land-use planning interventions that go beyond statutory planning, even supporting the emergence of new commons. We analyse a body of qualitative and semi-quantitative data from a recent strategic land-use plan process in Southwest Finland, including a series of planning documents and the results of a Q study. We describe how planners, citizens, and stakeholder organisations co-created a regional land-use plan and, focusing on the relationships between the practice of land-use planning and the legal structures of private property, ask how the commons were advanced in relation to private land ownership and how the different interpretations of the common good were reflected in the process. In the studied process, the planners strove to emphasise the commons and the common good by introducing new strategic land-use symbols. However, the emergence of new commons was seen as a threat by many landowners, their advocacy organisations, and regional decision makers. Instead of an unavoidable impasse, we urge that the situation should be seen as a call for novel solutions in the face of the ambitious and spatially explicit nature conservation commitments that increasingly contest the prevailing perceptions of the relationships of nature, property, and the distinct interpretations of common good.}
}
@incollection{CSN20201,
title = {Chapter 1 - Approaches from cognitive neuroscience and comparative cognition},
editor = {G.R. Sinha and Jasjit S. Suri},
booktitle = {Cognitive Informatics, Computer Modelling, and Cognitive Science},
publisher = {Academic Press},
pages = {1-19},
year = {2020},
isbn = {978-0-12-819445-4},
doi = {https://doi.org/10.1016/B978-0-12-819445-4.00001-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128194454000011},
author = {Koushik CSN and Shruti Bhargava Choubey and Abhishek Choubey},
keywords = {Python, cognitive science, tree-like structure, artificial intelligence, neuroscience, Bayesian framework},
abstract = {Cognitive science is basically the study of the psychological behavior of the tasks and the processes that human mind usually performs. It can be used to develop the computational models that are used to study the next sequential step, taken with the help of intelligence (artificial intelligence) which is derived mathematically. Python is a powerful tool that can be used for this cognitive science research where it is used for modeling the intelligence either mathematically or analytically. The tool can help increase the understanding of the developers of data by using a tree-like structure. The tree-like structure can be empathized as a neuron that can form clusters and have complex activities, which can be used for implementation. There are many other methods to represent data such as matrices and graphs, used in order to represent neural network. The neurons can be considered transistors or silicon chips that can be used to process data on the basis of programing, and similar results can be achieved what a normal human brain does. A Bayesian framework has been used to compare between the users and the data that can be processed cognitively by neurons.}
}
@article{BANDEIRA2023236,
title = {Estimation under group actions: Recovering orbits from invariants},
journal = {Applied and Computational Harmonic Analysis},
volume = {66},
pages = {236-319},
year = {2023},
issn = {1063-5203},
doi = {https://doi.org/10.1016/j.acha.2023.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1063520323000465},
author = {Afonso S. Bandeira and Ben Blum-Smith and Joe Kileel and Jonathan Niles-Weed and Amelia Perry and Alexander S. Wein},
keywords = {Signal processing, Biomedical imaging, Statistical aspects of information theory, Applications of lie groups, Applications of invariant theory, Applications of commutative algebra},
abstract = {We study a class of orbit recovery problems in which we observe independent copies of an unknown element of Rp, each linearly acted upon by a random element of some group (such as Z/p or SO(3)) and then corrupted by additive Gaussian noise. We prove matching upper and lower bounds on the number of samples required to approximately recover the group orbit of this unknown element with high probability. These bounds, based on quantitative techniques in invariant theory, give a precise correspondence between the statistical difficulty of the estimation problem and algebraic properties of the group. Furthermore, we give computer-assisted procedures to certify these properties that are computationally efficient in many cases of interest. The model is motivated by geometric problems in signal processing, computer vision, and structural biology, and applies to the reconstruction problem in cryo-electron microscopy (cryo-EM), a problem of significant practical interest. Our results allow us to verify (for a given problem size) that if cryo-EM images are corrupted by noise with variance σ2, the number of images required to recover the molecule structure scales as σ6. We match this bound with a novel (albeit computationally expensive) algorithm for ab initio reconstruction in cryo-EM, based on invariant features of degree at most 3. We further discuss how to recover multiple molecular structures from mixed (or heterogeneous) cryo-EM samples.}
}
@article{GAGLIARDICOZMAN2017298,
title = {On the complexity of propositional and relational credal networks},
journal = {International Journal of Approximate Reasoning},
volume = {83},
pages = {298-319},
year = {2017},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2016.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X16302031},
author = {Fabio {Gagliardi Cozman} and Denis {Deratani Mauá}},
keywords = {Credal networks, Propositional logic, Function-free first-order logic, Complexity theory},
abstract = {A credal network associates a directed acyclic graph with a collection of sets of probability measures. Usually these probability measures are specified by tables containing probability values. Here we examine the complexity of inference in credal networks when probability measures are specified through formal languages. We focus on logical languages based on propositional logic and on the function-free fragment of first-order logic. We show that sub-Boolean and relational logics lead to interesting complexity results. In short, we explore the relationship between specification language and computational complexity in credal networks.}
}
@article{KARWOWSKI201325,
title = {Threshold hypothesis: Fact or artifact?},
journal = {Thinking Skills and Creativity},
volume = {8},
pages = {25-33},
year = {2013},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2012.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S1871187112000429},
author = {Maciej Karwowski and Jacek Gralewski},
keywords = {Intelligence, Creative abilities, Threshold hypothesis},
abstract = {The threshold hypothesis (TH) assumes the existence of complex relations between creative abilities and intelligence: linear associations below 120 points of IQ and weaker or lack of associations above the threshold. However, diverse results have been obtained over the last six decades – some confirmed the hypothesis and some rejected it. In this paper the threshold hypothesis was examined on a large sample of middle and high school Polish students (N=921). Intelligence was measured by Raven's Progressive Matrices (RPM) and creative abilities by the Test for Creative Thinking–Drawing Production (TCT-DP). Results were analyzed as raw test scores, Item Response Theory (IRT) scores and Confirmatory Factor Analysis (CFA) scores to examine three thresholds (+.50 IQ SD, +1 IQ SD, +1.33 IQ SD). It was found that confirmation or rejection of the TH depends strongly on both an analytical strategy and theoretical decisions required for acceptance/rejection of the TH. If significant correlations between intelligence and creative abilities below the threshold and non-significant correlations above the threshold are treated as confirming the TH, such confirmation is observed at 115 points on the IQ scale. However, if confirmation requires higher correlations below the threshold than above it, the TH is less likely to be confirmed. We discuss theoretical and empirical issues which may lead to the conclusion that threshold hypothesis may actually be an artifact of data analysis.}
}
@article{BARILE2012451,
title = {Reflections on service systems boundaries: A viable systems perspective: The case of the London Borough of Sutton},
journal = {European Management Journal},
volume = {30},
number = {5},
pages = {451-465},
year = {2012},
note = {Research Perspectives in the Management of Complex Service Systems},
issn = {0263-2373},
doi = {https://doi.org/10.1016/j.emj.2012.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0263237312000606},
author = {Sergio Barile and Marialuisa Saviano and Francesco Polese and Primiano {Di Nauta}},
keywords = {Service systems, Viable Systems Approach (), Complexity, System’s boundaries},
abstract = {Summary
The aim of this paper is to propose a systems interpretation of the concept of complexity and its implications for a theoretical discussion of the concept of boundary in complex service systems. The proposal highlights the interpretative contribution of a dual perspective of observation that distinguishes between a structure-based view and a systems-based view. When dealing with complexity, the phenomenon under investigation cannot be addressed through management approaches that aim to measure and control it in a vain attempt to find the best solution. Due to the inner nature of complexity, a more rewarding approach to a full understanding of problematic situations should place consolidated management models within a more general interpretation framework that suggests preliminary insights about the real nature of the investigated phenomenon. First, this paper outlines the theoretical background of the literature on service, service systems and complex service systems, providing evidence of the contribution of recent service research advances such as service science and service-dominant logic. Next, the paper focuses on the basic principles of systems thinking to introduce the Viable Systems Approach (vSa) as a general framework of reference for both the investigation and the governance of social organisations. The vSa conceptual framework is adopted for proposing some reflections from a systems perspective in the investigation of the case of the London Borough of Sutton (LBS). The focus is on interpreting the paradoxical situation of an increased fear of crime among LBS residents despite the evidence of reduction in the crime rate. Although the incidence has fallen for most types of crimes, a recent poll confirmed that crime is still rated as the most important issue for residents. Therefore, improving safety and reducing crime remain the top priorities for the Safer Sutton Partnership Service. In short, this study proposes to consider “reducing the fear of crime in a community” as a complex service system.}
}
@incollection{MACHINMASTROMATTEO2024,
title = {Information Literacy and the Information Science Curriculum},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95689-5.00191-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956895001917},
author = {Juan D. Machin-Mastromatteo and César Saavedra-Alamillas and Alejandro Villegas-Muro},
keywords = {Advocacy, Critical competences, Curricular implementation, Curriculum, Curriculum methodologies, Digital literacy, Information literacy, Information literacy programs, Library and information science, Media literacy, Professional education and training, Social implications, Teaching and learning methodologies, Workplace implications},
abstract = {This entry provides a background to information literacy education in the library and information science (LIS) curriculum by presenting a summary of the elements and characteristics that information literacy courses should include. These are divided into six categories: (1) curricular implementation and general challenges; (2) topics the curriculum must include; (3) inclusion of education-related topics; (4) integration of other literacies; (5) methodologies for the information literacy curriculum; and (6) workplace and social implications. Then, it includes a brief and non-exhaustive review of 41 LIS programs, including courses on information literacy and related subjects. Finally, we offer some brief considerations for the future perspectives of this topic.}
}
@article{GREGG19971173,
title = {Skill-biassed change, unemployment and wage inequality},
journal = {European Economic Review},
volume = {41},
number = {6},
pages = {1173-1200},
year = {1997},
issn = {0014-2921},
doi = {https://doi.org/10.1016/S0014-2921(96)00054-2},
url = {https://www.sciencedirect.com/science/article/pii/S0014292196000542},
author = {Paul Gregg and Alan Manning},
keywords = {Skill-biassed technical change, Unemployment, Wage inequality},
abstract = {The analytical frameworks used for thinking about the determinants of the level of unemployment are not well-suited to thinking about the effects of skill-biassed technical change. In this paper, we argue for making relative wages as an argument of the labour supply function (or wage curve) in order to explain the observed patterns of wage inequality and unemployment. We argue that if this is done, we would expect labour market institutions to be much less important than is generally assumed in determining labour market outcomes in the longer-run and that policies towards education are likely to be much more important. We illustrate this by arguing that the British public education system effectively rations access to education and prevents market incentives from working.}
}
@article{KHOSRAVAN2017124,
title = {The effects of fluorine substitution on the chemical properties and inhibitory capacity of Donepezil anti-Alzheimer drug; density functional theory and molecular docking calculations},
journal = {Journal of Molecular Graphics and Modelling},
volume = {71},
pages = {124-134},
year = {2017},
issn = {1093-3263},
doi = {https://doi.org/10.1016/j.jmgm.2016.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S1093326316304089},
author = {Azita Khosravan and Safora Marani and Mohammad Sadegh {Sadeghi Googheri}},
keywords = {Alzheimer disease, Donepezil, Fluorination, Molecular docking, Density functional theory, AIM, NBO},
abstract = {Drug fluorination has the potential to reproduce useful drugs with decreasing the side effect of them. Identifying the effect of this improvement on the chemical properties and biological interactions of drug symbolizes a meaningful progress in drug design. Here the fluorination of Donepezil as an anti-Alzheimer drug, including 7 fluorinated derivatives of it, was investigated computationally. In the first part of our calculations, the most important chemical properties of drug that affects the drug efficiency were investigated by applying the M06/6–31g (d, p) and M062X/6–31g (d, p) levels of theories. Findings showed that the fluorine substitution changed the drug stability as altered the solubility and molecular polarity. Furthermore, the intramolecular hydrogen bonding, charge distribution and electron delocalization of the drug were affected by this replacement. In the second section, the effect of fluorination on the drug⋯enzyme interactions was evaluated by using two effective methods Based on the molecular docking and density functional theory (DFT) calculations fluorine substitution influenced the Donepezil⋯Acetylcholinesterase interactions. Calculated binding energies by two computational methods displayed that the fluorine replacement changed the binding affinity of drug. Finally, the most significant non-bonded interactions between drugs and involved residues were investigated by bond length data analysis.}
}
@article{MEMMERT2009263,
title = {Analysis and simulation of creativity learning by means of artificial neural networks},
journal = {Human Movement Science},
volume = {28},
number = {2},
pages = {263-282},
year = {2009},
issn = {0167-9457},
doi = {https://doi.org/10.1016/j.humov.2008.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167945708000821},
author = {Daniel Memmert and Jürgen Perl},
keywords = {Soft-modelling, Process orientation, Pattern recognition, Divergent thinking, Team sport},
abstract = {The paper presents a new neural network approach for analysis and simulation of creative behavior. The used concept of Dynamically Controlled Neural Gas (DyCoNG) entails a combination of Dynamically Controlled Network [Perl, J. (2004a). A neural network approach to movement pattern analysis. Human Movement Science, 23, 605–620] and Growing Neural Gas (Fritzke, 1995) by quality neurons. A quality neuron reflects the rareness of a piece of information and therefore can measure the originality of a recorded activity that was assigned to the neuron during the network training. The DyCoNG approach was validated using data from a longitudinal field-based study. The creative behavior of 42 participants in standardized test situations was tested in a creative training program lasting six months. The results from the DyCoNG-based simulation show that the network is able to separate main process types and reproduce recorded creative learning processes by means of simulation. The results are discussed in connection with practical implications in team sports and with a view to future investigations.}
}
@article{NAUMANN2022109237,
title = {Insights from system dynamics applications in addiction research: A scoping review},
journal = {Drug and Alcohol Dependence},
volume = {231},
pages = {109237},
year = {2022},
issn = {0376-8716},
doi = {https://doi.org/10.1016/j.drugalcdep.2021.109237},
url = {https://www.sciencedirect.com/science/article/pii/S0376871621007328},
author = {Rebecca B. Naumann and Isabella Guynn and Hannah Margaret Clare and Kristen Hassmiller Lich},
keywords = {Addiction, Substance use disorder, Systems thinking, Opioids, Alcohol, Polysubstance use},
abstract = {Background and aims
Substance misuse and use disorders are dynamic and complex problems, situated within systems of interacting social, environmental, and neurobiological factors. System dynamics (SD) methods broaden, test, and improve understanding of complex systems and can help inform effective action. We sought to systematically review the use of SD tools in addiction-related research.
Methods
Following PRISMA guidelines, we searched several databases from 1958 to 2019. We included studies focused on addiction-related screening and diagnosis, treatment, and return to use, as well as studies focused on earlier stages that may begin a path to addiction (e.g., experimentation, misuse onset).
Results
We extracted information from 59 articles with a median publication year of 2014. In addition to using SD to understand the underlying complexity driving addiction-related trends, other commonly cited reasons for use of SD included assessing impacts of potential actions (n = 35), predicting future trends (n = 28), and supporting strategic planning processes (n = 22). Most studies included simulation models (n = 43); however, some presented insights from qualitative SD diagrams (n = 9) and concept models (n = 6). The majority of studies focused on stages leading to potential addiction: initiation/ experimentation (n = 42) and misuse onset (n = 38). One-third (n = 20) engaged persons with lived experience or other stakeholders during the modeling process.
Conclusions
Addiction-related SD research has increased over the last few decades with applications varying in several ways, from model purpose and types of data used to stakeholder involvement. Future applications should consider the benefits of stakeholder engagement throughout the modeling process and expanding models to include concomitant substance use.}
}
@article{202477,
title = {AATS 2022 Annual Meeting},
journal = {Seminars in Thoracic and Cardiovascular Surgery},
volume = {36},
number = {1},
pages = {77-79},
year = {2024},
issn = {1043-0679},
doi = {https://doi.org/10.1053/j.semtcvs.2022.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S1043067922002283}
}
@article{BECATTINI2012961,
title = {Model and algorithm for computer-aided inventive problem analysis},
journal = {Computer-Aided Design},
volume = {44},
number = {10},
pages = {961-986},
year = {2012},
note = {Fundamentals of Next Generation CAD/E Systems},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2011.02.013},
url = {https://www.sciencedirect.com/science/article/pii/S0010448511000583},
author = {Niccolò Becattini and Yuri Borgianni and Gaetano Cascini and Federico Rotini},
keywords = {Computer-Aided Innovation, Problem solving, OTSM-TRIZ, Conceptual design, Dialogue-based system},
abstract = {The paper presents the research activity developed by the authors in the field of computer-aided inventive problem solving: an original model and a dialogue-based software application have been developed by integrating the logic of ARIZ (Algorithm for the Inventive Problem Solving) with some OTSM-TRIZ (General Theory of Powerful Thinking) models in order to guide a user also with no TRIZ education to the analysis of inventive problems. The paper demonstrates that through a dialogue-based interaction it is possible to guide the user towards a proper formulation of the problem statement, which is an essential step of any conceptual design activity. The proposed software system, although still at a prototype stage, has been tested with students at Politecnico di Milano and at the University of Florence. The paper details the structure of the algorithm and the results of the first validation activity; then, it discusses about the possibility to integrate the proposed approach into a new generation of CAD systems.}
}
@article{LIU2015170,
title = {Verification of a new quantum simulation approach through its application to two-dimensional Ising lattices},
journal = {Physica E: Low-dimensional Systems and Nanostructures},
volume = {66},
pages = {170-175},
year = {2015},
issn = {1386-9477},
doi = {https://doi.org/10.1016/j.physe.2014.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S1386947714003567},
author = {Z.-S. Liu and V. Sechovský and M. Diviš},
keywords = {Simulation approach, Algorithm, Quantum theory, Ising model},
abstract = {A new quantum simulation approach has been applied in the present work to the two-dimensional (2D) ferromagnetic and antiferromagnetic Ising lattices to calculate their magnetic structures, magnetizations, free energies and specific heats in the absence of an external magnetic field. Surprisingly, no size effects could be observed in our simulations performed for the Ising lattices of different sizes. Most importantly, our calculated spontaneous thermally averaged spins for the two kinds of systems are exactly same as those evaluated with quantum mean field theory, and the magnetic structures simulated at all chosen temperatures are perfectly ferromagnetic or antiferromagnetic, verifying the correctness and applicability of our quantum model and computational algorithm. On the other hand, if the classical Monte Carlo (CMC) method is applied to the ferromagnetic 2D Ising lattice with S=1, it is able to generate correct magnetization well consistent with Onsager's theory; but in the case of S=1/2, the computational results of CMC are incomparable to those predicted with the quantum mean field theory, giving rise to very much reduced magnetization and considerably underestimated Curie temperature. The difficulty met by the CMC method is mainly caused by its improperly calculated exchange energy of the randomly selected spin in every simulation step, especially immediately below the transition temperature, where the thermal averages of spins are much less than 1/2, however they are assigned to ±1/2 by CMC to evaluate the exchange energies of the spins, such improper manipulation is obviously impossible to lead the code to converge to the right equilibrium states of the spin systems.}
}
@article{MARQUES201655,
title = {Improved near-exact distributions for the product of independent Generalized Gamma random variables},
journal = {Computational Statistics & Data Analysis},
volume = {102},
pages = {55-66},
year = {2016},
issn = {0167-9473},
doi = {https://doi.org/10.1016/j.csda.2016.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167947316300809},
author = {Filipe J. Marques and Florence Loingeville},
keywords = {Characteristic functions, Gamma distribution, Generalized Integer Gamma distribution, Generalized Near-Integer Gamma distribution, LogGamma distribution, Near-exact distributions, Quality control},
abstract = {The Generalized Gamma distribution is an important distribution in Statistics since it has as particular cases many well known and important distributions and also due to its very interesting modeling properties, which makes it an attractive tool. The distribution of the product of independent Generalized Gamma distributions is investigated. Most of the results available for this distribution are based on Meijer-G or H functions which may still be very difficult to handle. Therefore, near-exact distributions which are based on the Generalized Near-Integer Gamma distribution and which have density and cumulative distribution functions easily implementable and computationally appealing are developed. Numerical studies with computationally intensive analyses are carried out to study the accuracy of these approximations in different scenarios. Also computational modules are provided for the implementation of these approximations. Finally, an example of application to quality control in microbiology is provided.}
}
@article{ZHANG2021132828,
title = {Computing with non-orientable defects: Nematics, smectics and natural patterns},
journal = {Physica D: Nonlinear Phenomena},
volume = {417},
pages = {132828},
year = {2021},
issn = {0167-2789},
doi = {https://doi.org/10.1016/j.physd.2020.132828},
url = {https://www.sciencedirect.com/science/article/pii/S0167278920308290},
author = {Chiqun Zhang and Amit Acharya and Alan C. Newell and Shankar C. Venkataramani},
keywords = {Defects in materials, Non-orientability, Effective theories, Liquid crystals, Pattern formation, Computation of defects},
abstract = {Defects are a ubiquitous feature of ordered media. They have certain universal features, independent of the underlying physical system, reflecting their topological origins. While the topological properties of defects are robust, they appear as ‘unphysical’ singularities, with non-integrable energy densities in coarse-grained macroscopic models. We develop a principled approach for enriching coarse-grained theories with enough of the ‘micro-physics’ to obtain thermodynamically consistent, well-set models that allow for the investigations of dynamics and interactions of defects in extended systems. We also develop associated numerical methods that are applicable to computing energy driven behaviors of defects across the amorphous-soft-crystalline materials spectrum. Our methods can handle order parameters that have a head-tail symmetry, i.e. director fields, in systems with a continuous translation symmetry, as in nematic liquid crystals, and in systems where the translation symmetry is broken, as in smectics and convection patterns. We illustrate our methods with explicit computations.}
}
@incollection{ANTHONY20161,
title = {Chapter 1 - Introduction},
editor = {Richard John Anthony},
booktitle = {Systems Programming},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {1-20},
year = {2016},
isbn = {978-0-12-800729-7},
doi = {https://doi.org/10.1016/B978-0-12-800729-7.00001-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128007297000017},
author = {Richard John Anthony},
keywords = {Transparency, QoS metrics, Functional requirements, Nonfunctional requirements, Software architectures, Case studies.},
abstract = {This chapter introduces the book, its structure, the way the technical material is organised and the motivation behind this. It provides a historical perspective and explains the significance of distributed systems in modern computing. It also explains the integrative, cross-discipline nature of the presentation used in the book and the underlying ‘systems thinking’ approach. It describes and justifies the way that material has been presented from four carefully selected viewpoints (ways of looking at systems structure, organisation and behaviour). These viewpoints have been chosen to overcome the artificial boundaries that are introduced when material is divided for the purposes of teaching into traditional categorisations of operating systems, networking, distributed systems, and programming; whereas many of the key concepts pertinent to the design and development of distributed applications overlap several of these areas or reside in the margins between these areas. This chapter also provides an essential concise introductory discussion of distributed systems to set the scene for the four core chapters which follow. Distributed systems are then examined in depth in Chapter 6. There is also an introduction to the three case studies and the extensive supplemental technical resources.}
}
@article{STEINMANN201437,
title = {Developmental changes of neuronal networks associated with strategic social decision-making},
journal = {Neuropsychologia},
volume = {56},
pages = {37-46},
year = {2014},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2013.12.025},
url = {https://www.sciencedirect.com/science/article/pii/S0028393214000037},
author = {Elisabeth Steinmann and Antonia Schmalor and Alexander Prehn-Kristensen and Stephan Wolff and Andreas Galka and Jan Möhring and Wolf-Dieter Gerber and Franz Petermann and Ulrich Stephani and Michael Siniatchkin},
keywords = {Ultimatum game, Normal development, Strategic social behavior, Decision-making, fMRI},
abstract = {Objectives
One of the important prerequisites for successful social interaction is the willingness of each individual to cooperate socially. Using the ultimatum game, several studies have demonstrated that the process of decision-making to cooperate or to defeat in interaction with a partner is associated with activation of the dorsolateral prefrontal cortex (DLPFC), anterior cingulate cortex (ACC), anterior insula (AI), and inferior frontal cortex (IFC). This study investigates developmental changes in this neuronal network.
Methods
15 healthy children (8–12 years), 15 adolescents (13–18 years) and 15 young adults (19–28 years) were investigated using the ultimatum game. Neuronal networks representing decision-making based on strategic thinking were characterized using functional MRI.
Results
In all age groups, the process of decision-making in reaction to unfair offers was associated with hemodynamic changes in similar regions. Compared with children, however, healthy adults and adolescents revealed greater activation in the IFC and the fusiform gyrus, as well as the nucleus accumbens. In contrast, healthy children displayed more activation in the AI, the dorsal part of the ACC, and the DLPFC. There were no differences in brain activations between adults and adolescents.
Conclusion
The neuronal mechanisms underlying strategic social decision making are already developed by the age of eight. Decision-making based on strategic thinking is associated with age-dependent involvement of different brain regions. Neuronal networks underlying theory of mind and reward anticipation are more activated in adults and adolescents with regard to the increasing perspective taking with age. In relation to emotional reactivity and respective compensatory coping in younger ages, children have higher activations in a neuronal network associated with emotional processing and executive control.}
}
@article{MA2020606,
title = {Disaster analysis on cultural sites using fuzzy based online open provision geographic data frameworks},
journal = {Computer Communications},
volume = {153},
pages = {606-613},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2019.12.036},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419312526},
author = {Liya Ma and Dongmei Wei and Pei Wang},
keywords = {Geographic information system, Cultural site information, OOGIS innovation fuzzy sets, Fuzzy membership function, Fuzzy logics},
abstract = {This paper speaks about the utilization of online open provision geographic data system (OOGIS) for the security of the well known recorded cultural sites and social urban communities in China. As indicated by the qualities of public interest and geographic information system (GIS), Most of the Cultural sites information in china are not accurately traced out by the archaeologist during their research on sculptures in various provinces of China. This paper exhibits a Fuzzy based application structure of OOGIS at various degrees during the time of participating in the assurance of the well known authentic and social urban areas. Further this paper analysis The fuzzy based approach to detect the disaster in cultural cites. This computational technique uses Fuzzy lexical values which uses high security fuzzy membership functions in OOGIS which can furnish archaeologist with more capacities to get all the information about cultural sites, including the publics, specialists and chiefs and so on, Likewise the degree of its application depends on the degree of OOGIS innovation and light of various conditions has been experimentally verified at lab scale using fuzzy based OOGIS software system. Then the introduced system ensures 97.43% of accuracy compared to existing methods.}
}
@article{ALESSI2021105057,
title = {Adjoint shape optimization coupled with LES-adapted RANS of a U-bend duct for pressure loss reduction},
journal = {Computers & Fluids},
volume = {228},
pages = {105057},
year = {2021},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2021.105057},
url = {https://www.sciencedirect.com/science/article/pii/S0045793021002218},
author = {G. Alessi and T. Verstraete and L. Koloszar and B. Blocken and J.P.A.J. {van Beeck}},
keywords = {Adjoint shape optimization, RANS, LES, Cahotic flow motion},
abstract = {Nowadays, as industrial designs are close to their optimal configurations, the challenge lies in the extraction of the last percentages of improvement. This necessitates accurate evaluations of the performance and represents a significant higher computational cost. The present work aims at integrating Large Eddy Simulations in the optimization framework for an accurate evaluation of the flow field. The number of expensive evaluations is kept to a minimum by using the adjoint method for the evaluation of the gradient of the objective function. Divergence of the gradients due to the chaotic flow motion is avoided by an additional step which decouples the Large Eddy Simulations from the gradient calculations. An adaptation process based on a Reynolds Averaged Navier-Stokes simulation is therefore sought to mimic the more accurate Large Eddy Simulation results. The obtained field is then used in combination with an adjoint shape optimization routine. The method is tested on the design of a U-bend for internal cooling channels by minimizing its pressure loss. Starting from an optimized geometry obtained through a classical approach based on RANS evaluations, further improvements of the design are achieved with the application of the proposed strategy when performances are evaluated by means of LES.}
}
@article{DAYAN20121068,
title = {How to set the switches on this thing},
journal = {Current Opinion in Neurobiology},
volume = {22},
number = {6},
pages = {1068-1074},
year = {2012},
note = {Decision making},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2012.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0959438812000992},
author = {Peter Dayan},
abstract = {Reinforcement learning (RL) has become a dominant computational paradigm for modeling psychological and neural aspects of affectively charged decision-making tasks. RL is normally construed in terms of the interaction between a subject and its environment, with the former emitting actions, and the latter providing stimuli, and appetitive and aversive reinforcement. However, there is recent emphasis on redrawing the boundary between the two, with the organism constructing its own notion of reward, punishment and state, and with internal actions, such as the gating of working memory, being treated on an equal footing with external manipulation of the environment. We review recent work in this area, focusing on cognitive control.}
}
@article{CLYNE2021313,
title = {Endothelial response to glucose: dysfunction, metabolism, and transport},
journal = {Biochemical Society Transactions},
volume = {49},
number = {1},
pages = {313-325},
year = {2021},
issn = {1470-8752},
doi = {https://doi.org/10.1042/BST20200611},
url = {https://www.sciencedirect.com/science/article/pii/S1470875221001586},
author = {Alisa Morss Clyne},
keywords = {blood brain barrier, diabetes, extracellular vesicles, glucose transport, glycolysis, SGLT2},
abstract = {The endothelial cell response to glucose plays an important role in both health and disease. Endothelial glucose-induced dysfunction was first studied in diabetic animal models and in cells cultured in hyperglycemia. Four classical dysfunction pathways were identified, which were later shown to result from the common mechanism of mitochondrial superoxide overproduction. More recently, non-coding RNA, extracellular vesicles, and sodium-glucose cotransporter-2 inhibitors were shown to affect glucose-induced endothelial dysfunction. Endothelial cells also metabolize glucose for their own energetic needs. Research over the past decade highlighted how manipulation of endothelial glycolysis can be used to control angiogenesis and microvascular permeability in diseases such as cancer. Finally, endothelial cells transport glucose to the cells of the blood vessel wall and to the parenchymal tissue. Increasing evidence from the blood-brain barrier and peripheral vasculature suggests that endothelial cells regulate glucose transport through glucose transporters that move glucose from the apical to the basolateral side of the cell. Future studies of endothelial glucose response should begin to integrate dysfunction, metabolism and transport into experimental and computational approaches that also consider endothelial heterogeneity, metabolic diversity, and parenchymal tissue interactions.}
}
@incollection{MACCARTHY20223,
title = {Chapter 1 - The Digital Supply Chain—emergence, concepts, definitions, and technologies},
editor = {Bart L. MacCarthy and Dmitry Ivanov},
booktitle = {The Digital Supply Chain},
publisher = {Elsevier},
pages = {3-24},
year = {2022},
isbn = {978-0-323-91614-1},
doi = {https://doi.org/10.1016/B978-0-323-91614-1.00001-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323916141000010},
author = {Bart L. MacCarthy and Dmitry Ivanov},
keywords = {Blockchain, Digital supply chain, Digital twins, Internet of things, Smart factory, Supply chain analytics, Cloud computing},
abstract = {Advances in technology, rapid globalization, trade liberalization, and increased regulation have shaped supply chains in the last four decades. We examine the impact of digitalization on contemporary and future supply chains. Digitalization potentially enables a strong digital thread connecting and mirroring an entire physical supply chain. We provide an overview of the principal technologies and systems enabling the Digital Supply Chain, including Smart Factories, Smart Warehouses, Smart Logistics, Cloud-based systems, and digital platforms. We discuss the computational engines enabled by Analytics, Data Science, and Artificial Intelligence and the emerging technologies likely to influence future supply chains—Blockchain, Digital Twins, Internet of Things, 5G, Edge, and Fog computing. The technologies offering the most promise in linking the virtual and physical worlds to improve supply chain performance are noted. We describe an evolving spectrum from digitally immature to digitally enabled and digitally transformed supply chains. We provide both narrow and broad definitions for future Digital Supply Chains. The transformative effects of the digitalization of supply chains will affect supply systems in diverse ways. Data-rich supply chain ecosystems will provide many new opportunities but will also give rise to many challenges that require continued analysis and evaluation by researchers and practitioners.}
}
@article{KINCH20151288,
title = {New drug discovery: extraordinary opportunities in an uncertain time},
journal = {Drug Discovery Today},
volume = {20},
number = {11},
pages = {1288-1292},
year = {2015},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2014.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S1359644614004796},
author = {Michael S. Kinch and Richard Flath},
abstract = {The way in which new medicines are discovered has irreversibly changed and the future sustainability of the enterprise is characterized by an unprecedented period of uncertainty. Herein, we convey that these changes provide unprecedented opportunities for many different players within the private and public sectors to work together and develop new models that ensure the sustainability of activities that have had an extraordinary impact; in terms of promoting public health and driving economic value. Specific examples of experiments are provided to demonstrate some of the new thinking that will be needed to ensure continuation of new drug discovery.}
}
@article{SCHOLZ2024123281,
title = {Transdisciplinary knowledge integration – PART I: Theoretical foundations and an organizational structure},
journal = {Technological Forecasting and Social Change},
volume = {202},
pages = {123281},
year = {2024},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2024.123281},
url = {https://www.sciencedirect.com/science/article/pii/S0040162524000775},
author = {Roland W. Scholz and Jana Zscheischler and Heike Köckler and Reiner Czichos and Klaus-Markus Hofmann and Cornelia Sindermann},
keywords = {Transdisciplinary problems, Science–practice complementarity, Knowledge integration, Complexity management, Digital data},
abstract = {Transdisciplinary processes deal with transdisciplinary problems that are (i) complex, (ii) societally relevant, (iii) ill-defined, and (iv) real-world problems which often show a high degree of ambiguity resulting in contested perceptions and evaluations among and between scientists and practitioners. Therefore, they are susceptible to multiple trade-offs. Transdisciplinary processes construct socially robust orientations (SoROs) particularly for sustainable transitioning. The integration of science and practice knowledge on equal footing (1) is considered the core of transdisciplinary processes. Yet other forms of knowledge integration contribute essentially to construct SoROs. Individuals may (2) use different modes of thought; (3) refer to various cultures with diverse value and belief systems; and (4) problems are perceived and prioritized based on roles and interests. Coping with transdisciplinary problems, (5) purposeful differentiation and integration and (6) an integration of evolutionary evolving codes of representing knowledge are necessary. Finally, (7) what systems to integrate requires consensus-building among participating scientists and practitioners. This paper is Part I of a two-part publication. It provides a conceptualization of the different types of knowledge integration. Part II analyzes tasks, challenges, and barriers related to different types of knowledge integration in five transdisciplinary processes which developed SoROs for sensitive subsystems of Germany affected by the irresponsible use of digital data.}
}
@article{KONG2021104267,
title = {Investigating primary school principals’ programming perception and support from the perspective of reasoned action: A mixed methods approach},
journal = {Computers & Education},
volume = {172},
pages = {104267},
year = {2021},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2021.104267},
url = {https://www.sciencedirect.com/science/article/pii/S0360131521001445},
author = {Siu-Cheung Kong and Yi-Qing Wang},
keywords = {Mixed methods design, Pedagogical issues, Primary school, Programming education, Theory of reasoned action},
abstract = {Programming is perceived to be an indispensable type of literacy in the digital era. To effectively promote and implement programming in K–12 education, it is necessary to understand school principals' perception of programming education. This study adopted a mixed methods design to explain principals’ understanding, expectations, and support for programming education in primary schools using Theory of Reasoned Action (TRA). In study 1, survey questionnaires were distributed to all principals from public primary schools in Hong Kong. Two hundred and sixty-six principals responded to the survey (response rate = 55.6%). In study 2, a follow-up interview study with 13 principals was conducted to further explore their perception of programming education. The results of study 1 indicated that principals with a better understanding of programming education tend to have clearer expectations of how to implement programming education in their schools, which consequently leads to greater support for the implementation of programming education. In study 2, the thematic analysis further supported the results obtained in study 1. Specifically, the results of study 2 demonstrated that most principals show understanding, expectation, and support for the implementation of programming education, which in turn results in various positive student and teacher outcomes. The results also showed that challenges are inevitable during implementation, principals show capabilities and willingness to adjust their expectation and support to better integrate programming education into their school curricula.}
}
@article{YANG2013809,
title = {Rational preparation of dibenzothiophene-imprinted polymers by surface imprinting technique combined with atom transfer radical polymerization},
journal = {Applied Surface Science},
volume = {282},
pages = {809-819},
year = {2013},
issn = {0169-4332},
doi = {https://doi.org/10.1016/j.apsusc.2013.06.063},
url = {https://www.sciencedirect.com/science/article/pii/S0169433213011744},
author = {Wenming Yang and Lukuan Liu and Zhiping Zhou and Hong Liu and Binze Xie and Wanzhen Xu},
keywords = {Surface imprinted polymers, Atom transfer radical polymerization, Computational simulation, Dibenzothiophene, Adsorbent},
abstract = {A computational simulation method is introduced to simulate the dibenzothiophene-monomer pre-assembly system of molecular imprinted polymers. The interaction type and intensity between dibenzothiophene and monomer are discussed from the binding energy and spatial position distribution. The simulation and analysis results indicate that the amount of the function monomer is not the more the better in preparing molecular imprinted polymers. Based on the above results, a novel dibenzothiophene-imprinted polymers with the favorable specific adsorption effect was prepared by surface imprinting technique combined with atom transfer radical polymerization. This combined technologies are used for preparing a desulfurization adsorbent for the first time. Various measures were selected to characterize the structure and morphology of the prepared adsorbent. The characterization results show that the adsorbent has suitable features for further adsorption process. A series of static adsorption experiments were conducted to analyze its adsorption performance. The adsorption process follows Elovich model by the kinetic analysis and Sips equation by the isothermal analysis. The approach we described will provide another opportunity in the deep desulfurization field.}
}
@incollection{BOUCHER2006369,
title = {Chapter 9 - Unified Modeling Language},
editor = {Thomas O. Boucher and Ali Yalçin},
booktitle = {Design of Industrial Information Systems},
publisher = {Academic Press},
address = {Burlington},
pages = {369-400},
year = {2006},
isbn = {978-0-12-370492-4},
doi = {https://doi.org/10.1016/B978-012370492-4/50009-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012370492450009X},
author = {Thomas O. Boucher and Ali Yalçin},
abstract = {Publisher Summary
Object-oriented (OO) software development is a significant new approach that owes its genesis to developments in software engineering. The OO approach stresses the encapsulation of data and procedures within segments of software code called “objects.” A specific set of OO design tools that evolved from the trend in object thinking is known as Unified Modeling Language (UML). UML is a set of tools developed to assist analysts in uncovering the important features of a design project, finally arriving at a set of models that will be used to design, document, and implement the project, whether it is an information system or other software development projects. UML is a recent development based on OO thinking and has gained popularity because many design projects that use databases also use software components such as Web pages, intelligent agents, and other software objects. UML enables the analyst to document a more complete design by showing all the interactions among the software components, including the database or data objects. This chapter introduces the basic ideas underlying UML in contrast to other traditional techniques.}
}
@article{ELKIND2020193,
title = {Cognitive hierarchy and voting manipulation in k-approval voting},
journal = {Mathematical Social Sciences},
volume = {108},
pages = {193-205},
year = {2020},
issn = {0165-4896},
doi = {https://doi.org/10.1016/j.mathsocsci.2020.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0165489620300615},
author = {Edith Elkind and Umberto Grandi and Francesca Rossi and Arkadii Slinko},
keywords = {Strategic voting, Bounded rationality, Computational complexity},
abstract = {By the Gibbard–Satterthwaite theorem, every reasonable voting rule for three or more alternatives is susceptible to manipulation: there exist elections where one or more voters can change the election outcome in their favour by unilaterally modifying their vote. When a given election admits several such voters, strategic voting becomes a game among potential manipulators: a manipulative vote that leads to a better outcome when other voters are truthful may lead to disastrous results when other voters choose to manipulate as well. We consider this situation from the perspective of a boundedly rational voter, using an appropriately adapted cognitive hierarchy framework to model voters’ limitations. We investigate the complexity of algorithmic questions that such a voter faces when deciding on whether to manipulate. We focus on k-approval voting rules, with k≥1. We provide polynomial-time algorithms for k=1,2 and hardness results for k≥4 (NP and co-NP), supporting the claim that strategic voting, albeit ubiquitous in collective decision making, is computationally hard if the manipulators try to reason about each other’s actions.}
}
@article{MARX20181,
title = {A piecewise linear contour to avoid critical points in inviscid flow stability analyses},
journal = {Computers & Fluids},
volume = {172},
pages = {1-7},
year = {2018},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2018.06.018},
url = {https://www.sciencedirect.com/science/article/pii/S0045793018303499},
author = {David Marx},
keywords = {Inviscid flow stability analysis, Critical point, Spectral method, Complex mapping},
abstract = {In inviscid flow stability analyses critical points are encountered which spoil the convergence of spectral methods for the computation of unstable modes as they become neutral. One way to alleviate this problem is to make a detour in the complex plane, which is often done by using a parabolic or cubic mapping. A piecewise linear profile has also been proposed in the literature for shooting methods. Its use with a spectral collocation method is investigated in the present paper. The method is applied to solve the linearized Euler equations for the computation of a stable surface mode in a channel flow with a lined wall, modelled as a mechanical oscillator.}
}
@article{TAKATSUKA20021131,
title = {GeoVISTA Studio: a codeless visual programming environment for geoscientific data analysis and visualization},
journal = {Computers & Geosciences},
volume = {28},
number = {10},
pages = {1131-1144},
year = {2002},
note = {Shareware and freeware in the Geosciences II. A special issue in honour of John Butler},
issn = {0098-3004},
doi = {https://doi.org/10.1016/S0098-3004(02)00031-6},
url = {https://www.sciencedirect.com/science/article/pii/S0098300402000316},
author = {Masahiro Takatsuka and Mark Gahegan},
keywords = {Visual programming, Exploratory data analysis (EDA), Knowledge construction, Java, Component-oriented programming (COP)},
abstract = {The fundamental goal of the GeoVISTA Studio project is to improve geoscientific analysis by providing an environment that operationally integrates a wide range of analysis activities, including those both computationally and visually based. Improving the infrastructure used in analysis has far-reaching potential to better integrate human-based and computationally based expertise, and so ultimately improve scientific outcomes. To address these challenges, some difficult system design and software engineering problems must be overcome. This paper illustrates the design of a component-oriented system, GeoVISTA Studio, as a means to overcome such difficulties by using state-of-the-art component-based software engineering techniques. Advantages described include: ease of program construction (visual programming), an open (non-proprietary) architecture, simple component-based integration and advanced deployment methods. This versatility has the potential to change the nature of systems development for the geosciences, providing better mechanisms to coordinate complex functionality, and as a consequence, to improve analysis by closer integration of software tools and better engagement of the human expert. Two example applications are presented to illustrate the potential of the Studio environment for exploring and better understanding large, complex geographical datasets and for supporting complex visual and computational analysis.}
}
@article{YU20111509,
title = {Hybrid dynamic iterations for the solution of initial value problems},
journal = {Journal of Parallel and Distributed Computing},
volume = {71},
number = {11},
pages = {1509-1517},
year = {2011},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2011.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S0743731511000852},
author = {Yanan Yu and Ashok Srinivasan},
keywords = {Time parallelization, Hybrid dynamic iterations, ODE solver},
abstract = {Many scientific problems are posed as Ordinary Differential Equations (ODEs). A large subset of these are initial value problems, which are typically solved numerically. The solution starts by using a known state space of the ODE system to determine the state at a subsequent point in time. This process is repeated several times. When the computational demand is high due to large state space, parallel computers can be used efficiently to reduce the time to solution. Conventional parallelization strategies distribute the state space of the problem amongst cores and distribute the task of computing for a single time step amongst the cores. They are not effective when the computational problems have fine granularity, for example, when the state space is relatively small and the computational effort arises largely from the long time span of the initial value problem. We propose a hybrid dynamic iterations method11This paper is an extended version of a conference paper (Yu and Srinivasan, 2009) [20]. In this paper, we have considered an additional underlying ODE solver for the hybrid method, empirically evaluated with more ODE systems, and also evaluated the relative performance with low and high accuracy requirements. which combines conventional sequential ODE solvers with dynamic iterations to parallelize the time domain. Empirical results demonstrate a factor of two to four improvement in performance of the hybrid dynamic iterations method over a conventional ODE solver on an 8 core processor. Compared to Picard iterations (also parallelized in the time domain), the proposed method shows better convergence and speedup results when high accuracy is required.}
}
@article{KHAN20214704,
title = {Fire hazard assessment, performance evaluation, and fire resistance enhancement of bridges},
journal = {Structures},
volume = {34},
pages = {4704-4714},
year = {2021},
issn = {2352-0124},
doi = {https://doi.org/10.1016/j.istruc.2021.10.080},
url = {https://www.sciencedirect.com/science/article/pii/S2352012421010407},
author = {Mustesin Ali Khan and Aatif Ali Khan and Ramakanth Domada and Asif Usmani},
keywords = {Fire risk assessment, Performance-based design, Fire load, Fire resistance, Fire protection, CFD},
abstract = {Although the performance of bridge structures under prescriptive fire scenarios has been the subject of numerous studies, performance-based approaches are yet to be developed to achieve an efficient and economical design. This paper presents a performance-based framework that identifies bridges at high fire risk, produces realistic fire scenarios, provides an open source tool to apply the realistic fire load to the thermomechanical model and evaluate the structural performance of the bridge. It also provides guidance to improve the fire resistance of the bridge. The proposed framework is implemented by simulating the I-65 overpass fire accident in 2002, Birmingham, Alabama, USA. Firstly, fire risk of the bridge is estimated by considering various criteria such as the social and economic impact of fire, structural vulnerability, and the likelihood of fire. Secondly, a realistic fire scenario is developed using the real fire accident data by conducting computational fluid dynamics (CFD) simulations. Thirdly, the newly developed open source FSDM framework is utilised to apply the realistic fire load to the thermomechanical model and finally, the fire resistance of the bridge structure is estimated. The unprotected bridge failed after 12 min of fire exposure which is found in compliance with the actual failure time of the bridge during the accident. Further thermomechanical analyses are performed applying different thicknesses of fire protection to estimate the suitable amount of fire protection to achieve improved fire resistance. It is observed that the fire resistance of the bridge can be enhanced up to 60 min by providing a fire protection of 12 mm thickness. This framework presents an important methodology for the highway department and bridge engineers to identify bridges at high fire risk and accurately determine the amount of fire protection required to reduce the fire risk and enhance the fire resistance of these bridges.}
}
@article{BRYAN2013295,
title = {High-performance computing tools for the integrated assessment and modelling of social–ecological systems},
journal = {Environmental Modelling & Software},
volume = {39},
pages = {295-303},
year = {2013},
note = {Thematic Issue on the Future of Integrated Modeling Science and Technology},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2012.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S1364815212000540},
author = {Brett A. Bryan},
keywords = {Graphics processing unit (GPU), Parallel programming, Multi-core, Cluster, Grid, GIS, Environmental, Concurrency, Global challenges},
abstract = {Integrated spatio-temporal assessment and modelling of complex social–ecological systems is required to address global environmental challenges. However, the computational demands of this modelling are unlikely to be met by traditional Geographic Information System (GIS) tools anytime soon. I evaluated the potential of a range of high-performance computing (HPC) hardware and software tools to overcome these computational barriers. Performance advantages were quantified using a synthetic model. Four tests were compared, using: a) an Arc Macro Language (AML) GIS script on a single central processing unit (CPU); b) Python/NumPy on 1–256 CPU cores; c) Python/NumPy on 1–64 graphics processing units (GPUs) with high-level PyCUDA abstraction (GPUArray); and d) Python/NumPy on 1–64 GPUs with low-level PyCUDA abstraction (ElementwiseKernel). The GIS implementation effectively took 15.5 weeks to run. Python/NumPy on a single CPU core led to a speed-up of 59× compared to the GIS. On a single GPU, speed-ups of 1473× were achieved using GPUArray and 4881× using ElementwiseKernel. Parallel processing led to further performance enhancements. At best, the ElementwiseKernel module in parallel over 64 GPUs achieved a speed-up of 63,643×. Open source tools such as Python applied across a spectrum of HPC resources offer transformational and accessible performance improvements for integrated assessment and modelling. By reducing the computational barrier, HPC can lead to a step change in modelling sophistication, including the better representation of uncertainty, and perhaps even new modelling paradigms. However, migration to new hardware and software environments also has significant costs. Costs and benefits of HPC are discussed and code tools are provided to help others migrate to HPC and transform our ability to address global challenges through integrated assessment and modelling.}
}
@article{SINGH2022118670,
title = {Exploration of experimental, theoretical, Hirshfeld surface, molecular docking and electronic excitation studies of Menadione: A potent anti-cancer agent},
journal = {Journal of Molecular Liquids},
volume = {351},
pages = {118670},
year = {2022},
issn = {0167-7322},
doi = {https://doi.org/10.1016/j.molliq.2022.118670},
url = {https://www.sciencedirect.com/science/article/pii/S0167732222002070},
author = {Neha Singh and Aysha Fatima and Meenakshi Singh and Mukesh kumar and Indresh Verma and S. Muthu and Nazia Siddiqui and Saleem Javed},
keywords = {DFT, TD-DFT, MEP, Molecular docking, Molecular dynamics},
abstract = {In this report, experimental, Computational analysis of menadione (2 methyl-1,4 nathoquinone) has been carried out theoretically by (DFT) density functional theory using B3LYP method with 6-311++G (d,p) basis set. Vibrational spectroscopic study and various other parameters have been accomplished. AIM theory (Atoms in molecules) is used to calculate the ellipticity, iso-surface projection by electron localization function, and binding energies. The computational theoretical spectra of FT-IR showed great agreement with the experimental results. A detailed description of crystal surface intermolecular interactions was carried out and Hirshfeld surface analysis, fingerprint plots were drawn via crystal explorer software. The NBO study helped in analyzing the donor and acceptor interaction. The nucleophilic and electrophilic interactions of the molecule were determined by the Fukui function and Molecular Electrostatic Potential (MEP). TD-DFT with PCM model was done with different solvents. Exploration of electron excitation from occupied to unoccupied orbitals in a single pair of electrons takes place. With DMSO and MeOH as solvents, hole and electron density distribution maps (EDD and HDD) were drawn in an excited state. The HOMO → LUMO energy gap showed the strength and stability of the molecule. With the help of the electrophilicity index and other parameters, the biological potency of the molecule is theoretically estimated. The drug-likeness was also studied and molecular docking was done using different proteins and with binding energy −9.5, −8.3, and −6.2. The biomolecular stability was investigated using a molecular dynamics simulation.}
}
@article{BETANCOUR2023306,
title = {Design and optimization of a runner for a gravitational vortex turbine using the response surface methodology and experimental tests},
journal = {Renewable Energy},
volume = {210},
pages = {306-320},
year = {2023},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2023.04.045},
url = {https://www.sciencedirect.com/science/article/pii/S0960148123004950},
author = {Johan Betancour and Fredys Romero-Menco and Laura Velásquez and Ainhoa Rubio-Clemente and Edwin Chica},
keywords = {Gravitational vortex turbine, Non-conventional energy generation, Optimization, Design of experiments, Runner},
abstract = {In this work, four runners for a gravitational vortex turbine (GVT) were initially analyzed numerically with the aim of selecting the one with the best efficiency (η). The selected rotor was optimized using the response surface methodology (RSM). Four design factors were considered: the number of blades (M), the twist angle (λ), the relationships between the runner upper (D) and lower (d) diameters, and the upper chamber diameter (Dcd); i.e., D/Dcd and d/Dcd, respectively. For the numerical analysis, a three-dimensional (3D) computational domain in ANSYS Fluent software with the K−ϵ RNG turbulence model and the six degrees of freedom (6-DoF) user defined function (UDF) method was utilized for the unsteady flow simulations. The η versus (vs.) the angular velocity (ω) curve was monitored during the CCD for all the treatments tested. The highest η was 0.522 under optimal design conditions; i.e., for M, λ, D/Dcd and d/Dcd equal to 6, 55°, 0.5 and 0.23, respectively. The optimal runner was built using a 3D printing and was experimentally tested utilizing a hydraulic bench. The experimental and numerical η vs. ω curves were compared. A difference of 5.1% between the maximum values of η was found.}
}
@article{SMART20189,
title = {Human-extended machine cognition},
journal = {Cognitive Systems Research},
volume = {49},
pages = {9-23},
year = {2018},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2017.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041717301407},
author = {Paul R. Smart},
keywords = {Extended cognition, Active externalism, Artificial intelligence, Cognitive computing, Human–computer interaction, Human computation},
abstract = {Human-extended machine cognition is a specific form of artificial intelligence in which the casually-active physical vehicles of machine-based cognitive states and processes include one or more human agents. Human-extended machine cognition is thus a specific form of extended cognition that sees human agents as constituent parts of the physical fabric that realizes machine-based cognitive capabilities. This idea is important, not just because of its impact on current philosophical debates about the extended character of human cognition, but also because it helps to focus scientific attention on the potential role of the human social environment in realizing novel forms of artificial intelligence. The present paper provides an overview of human-extended machine cognition and situates the concept within the broader theoretical framework of active externalism. The paper additionally shows how the concept of human-extended machine cognition can be applied to existing forms of human–machine interaction, especially those that occur in the context of the contemporary Internet and Web.}
}
@article{KAZEMIVASH2022109478,
title = {A novel 5D brain parcellation approach based on spatio-temporal encoding of resting fMRI data from deep residual learning},
journal = {Journal of Neuroscience Methods},
volume = {369},
pages = {109478},
year = {2022},
issn = {0165-0270},
doi = {https://doi.org/10.1016/j.jneumeth.2022.109478},
url = {https://www.sciencedirect.com/science/article/pii/S016502702200005X},
author = {Behnam Kazemivash and Vince D. Calhoun},
keywords = {Brain parcellation, Residual deep neural network, ICA, FMRI, Neuroimaging},
abstract = {Objective
Brain parcellation is an essential aspect of computational neuroimaging research and deals with segmenting the brain into (possibly overlapping) sub-regions employed to study brain anatomy or function. In the context of functional parcellation, brain organization which is often measured via temporal metrics such as coherence, is highly dynamic. This dynamic aspect is ignored in most research, which typically applies anatomically based, fixed regions for each individual, and can produce misleading results.
Methods
In this work, we propose a novel spatio-temporal-network (5D) brain parcellation scheme utilizing a deep residual network to predict the probability of each voxel belonging to a brain network at each point in time. Results: We trained 53 4D brain networks and evaluate the ability of these networks to capture spatial and temporal dynamics as well as to show sensitivity to individual or group-level variation (in our case with age).
Conclusion
The proposed system generates informative spatio-temporal networks that vary not only across individuals but also over time and space.
Significance
The dynamic 5D nature of the developed approach provides a powerful framework that expands on existing work and has potential to identify novel and typically ignored findings when studying the healthy and disordered brain.}
}
@article{MENZIES2012796,
title = {The causal structure of mechanisms},
journal = {Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences},
volume = {43},
number = {4},
pages = {796-805},
year = {2012},
note = {Causality in the Biomedical and Social Sciences},
issn = {1369-8486},
doi = {https://doi.org/10.1016/j.shpsc.2012.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S1369848612000647},
author = {Peter Menzies},
keywords = {Mechanisms, Mechanistic explanation, Causal explanation, Interventionism, Structural equations, Modularity},
abstract = {Recently, a number of philosophers of science have claimed that much explanation in the sciences, especially in the biomedical and social sciences, is mechanistic explanation. I argue the account of mechanistic explanation provided in this tradition has not been entirely satisfactory, as it has neglected to describe in complete detail the crucial causal structure of mechanistic explanation. I show how the interventionist approach to causation, especially within a structural equations framework, provides a simple and elegant account of the causal structure of mechanisms. This account explains the many useful insights of traditional accounts of mechanism, such as Carl Craver’s account in his book Explaining the Brain (2007), but also helps to correct the omissions of such accounts. One of these omissions is the failure to provide an explicit formulation of a modularity constraint that plays a significant role in mechanistic explanation. One virtue of the interventionist/structural equations framework is that it allows for a simple formulation of a modularity constraint on mechanistic explanation. I illustrate the role of this constraint in the last section of the paper, which describes the form that mechanistic explanation takes in the computational, information-processing paradigm of cognitive psychology.}
}
@article{MANTOVANI2020104446,
title = {Ontology-driven representation of knowledge for geological maps},
journal = {Computers & Geosciences},
volume = {139},
pages = {104446},
year = {2020},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2020.104446},
url = {https://www.sciencedirect.com/science/article/pii/S0098300419302420},
author = {Alizia Mantovani and Fabrizio Piana and Vincenzo Lombardo},
keywords = {Geology, Geologic knowledge encoding, Geological map, Geographical information science & systems, Geological structure ontology, Geological unit ontology},
abstract = {This paper presents an ontology-driven representation of knowledge for geological maps. The ontological formal language allows for a machine-readable encoding of the Earth scientist's interpretation through semantic categories and properties and is credited to support knowledge sharing and interoperability. We introduce an ontology-driven method for the interpretation and the encoding of the map data that employs shared vocabularies and resources encoded through ontologies in order to prevent the use of ambiguous terms. The approach relies on a computational ontology of the geological knowledge (OntoGeonous), which formalizes a number of geological knowledge sources (including GeoScienceML), to guide the interpretation process. The design of the database underlying the map (OntoGeoBase) constrains the process of data entry to refer to the terminology conveyed by the taxonomic-axiomatic nature of the ontology. This reduces the amount of implicit knowledge favouring a conceptual alignment of the ancillary documentation with the map, leading to a better comprehension of map and allowing the traceability of the interpretation.}
}
@article{CHENGKUIHUANG2010616,
title = {An integrated decision model for evaluating educational web sites from the fuzzy subjective and objective perspectives},
journal = {Computers & Education},
volume = {55},
number = {2},
pages = {616-629},
year = {2010},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2010.02.022},
url = {https://www.sciencedirect.com/science/article/pii/S0360131510000655},
author = {Tony {Cheng-Kui Huang} and Chih-Hong Huang},
keywords = {Evaluation methodologies, Human-computer interface, Architectures for educational technology system, Media in education},
abstract = {With advances in information and network technologies, lots of data have been digitized to reveal information for users by the construction of Web sites. Unfortunately, they are both overloading and overlapping in Internet so that users cannot distinguish their quality. To address this issue in education, Hwang, Huang, and Tseng proposed a group decision system to evaluate the quality of educational Web sites by users’ and experts’ opinions. Their investigative source is solely stemmed from human intention, called the subjective perspective, to make judgments on the quality of Web sites. However, the nature of human beings in making decisions has a gap between intention and behavior. Asking people for eliciting thought is arduous to cause this gap. Human behavior, namely the objective perspective, is the other essential source to obtain human thinking and real doings. For this reason, we can use data mining approaches to acquire the objective source. In this research, we propose an integrated decision model applied in evaluating educational Web sites from the fuzzy subjective and objective perspectives. The former source is extracted by inquiring human opinion using a questionnaire, while the latter is gained automatically by a data mining technique, fuzzy clustering. An empirical study is carried out to validate the model capability.}
}
@article{NIEDER2016830,
title = {Representing Something Out of Nothing: The Dawning of Zero},
journal = {Trends in Cognitive Sciences},
volume = {20},
number = {11},
pages = {830-842},
year = {2016},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2016.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S1364661316301255},
author = {Andreas Nieder},
keywords = {number, mathematics, abstraction, association cortex},
abstract = {Zero stands for emptiness, for nothing, and yet it is considered to be one of the greatest achievements of humankind. This review first recapitulates the discovery of the number zero in human history, then follows its progression in human development, traces its evolution in the animal kingdom, and finally elucidates how the brain transforms ‘nothing’ into an abstract zero category. It is argued that the emergence of zero passes through four corresponding representations in all of these interrelated realms: first, sensory ‘nothing’; then categorical ‘something’; then quantitative empty sets; and finally the number zero. The concept of zero shows how the brain, originally evolved to represent stimuli (‘something’), detaches from empirical properties to achieve ultimate abstract thinking.}
}
@article{CASTELOBRANCO2022527,
title = {Digital representation methods: The case of algorithmic design},
journal = {Frontiers of Architectural Research},
volume = {11},
number = {3},
pages = {527-541},
year = {2022},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2021.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S2095263522000012},
author = {Renata Castelo-Branco and Inês Caetano and António Leitão},
keywords = {Representation methods, Algorithmic design, Computer-aided drafting, Building information modeling},
abstract = {Architectural representation encompasses the means used to describe architectural entities. This discipline has long been under constant change due to architects' ever-present desire for innovation. Algorithmic design (AD) is currently making its way into the plethora of representation methods that integrate the architect's day-to-day work tools. However, it provides its fair share of controversy and hardship as it goes. This paper assesses whether AD is suitable as a representation method for architectural design by making a systematic analysis of this medium as a contemporary representation method. Specifically, we investigate (1) its birth and evolution as a means of representation, (2) the characteristics that make it simultaneously appealing and off-putting to the architectural community, (3) the influence of technological evolution and education on its proliferation, and (4) its capacity to represent design problems in comparison to the currently predominant means of digital architectural representation, that is, computer-aided drafting and building information modeling.}
}
@article{NOCERAALVESJUNIOR2024103856,
title = {Efficiency analysis of engineering classes: A DEA approach encompassing active learning and expositive classes towards quality education},
journal = {Environmental Science & Policy},
volume = {160},
pages = {103856},
year = {2024},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2024.103856},
url = {https://www.sciencedirect.com/science/article/pii/S1462901124001904},
author = {Paulo {Nocera Alves Junior} and Paul Leger and Isotilia {Costa Melo}},
keywords = {Sustainable Development Goals, Quality education, DEA SBM-VRS-BoD, Learning and teaching efficiency, Active learning, Problem-based learning (PBL)},
abstract = {The science, technology, engineering, and mathematics (STEM) education research delves into the core of sustainable development goals (SDGs), including the pillars of quality education (SDG4), robust economic growth (SDG8), and diminished inequalities (SDG10). These pursuits stand as keystones in sculpting inclusive societies and bridging societal gaps. While previous studies utilising data envelopment analysis (DEA) have explored educational performance mainly from a macro-perspective, there is a lack of micro-perspective investigation. Our study aims to fill this gap by proposing a DEA approach to assess the relative efficiency of engineering classes. We analysed 70 classes covering 38 subjects in the first semester of 2022 at a South American school. Methodologically, we employed the slack-based measure (SBM) model under the benefit of doubt (BoD) condition. Unlike prior research, we analysed classes' relative performance considering different pedagogical approaches - 11 active-learning classes (15.7 %) and 59 passive-learning classes (84.3 %). Our results showed that 18 classes were efficient (25.7 %). Active classes were more efficient, but few subjects maintained similar efficiencies for all classes. Moreover, efficient classes were concentrated in the last two years prior to graduation (57.9 %). This may represent an additional barrier for low-income students, who tend to drop out in the first years. The findings support several improvement recommendations, such as integrating digital technologies, boosting active learning opportunities, and bolstering classes in foundational subjects. Also, implications for researchers, decision- and policy-makers are discussed. Our approach can be replicated in diverse educational contexts, enabling the identification of strengths and weaknesses for more efficient educational management.}
}
@article{KUBE2020101829,
title = {Understanding persistent physical symptoms: Conceptual integration of psychological expectation models and predictive processing accounts},
journal = {Clinical Psychology Review},
volume = {76},
pages = {101829},
year = {2020},
issn = {0272-7358},
doi = {https://doi.org/10.1016/j.cpr.2020.101829},
url = {https://www.sciencedirect.com/science/article/pii/S0272735820300179},
author = {Tobias Kube and Liron Rozenkrantz and Winfried Rief and Arthur Barsky},
keywords = {Persistent physical symptoms, Medically unexplained symptoms, Expectation, Cognitive immunization, Belief updating, Predictive processing},
abstract = {Persistent physical symptoms (PPS) are distressing, difficult to treat, and pose a major challenge to health care providers and systems. In this article, we review two disparate bodies of literature on PPS to provide a novel integrative model of this elusive condition. First, we draw on the clinical-psychological literature on the role of expectations to suggest that people with PPS develop dysfunctional expectations about health and disease that become increasingly immune to disconfirmatory information (such as medical reassurance) through cognitive reappraisal. Second, we invoke neuroscientific predictive processing accounts and propose that the psychological process of ‘cognitive immunization’ against disconfirmatory evidence corresponds, at the neurobiological and computational level, to too much confidence (i.e. precision) afforded to prior predictions. This can lead to an attenuation of disconfirming sensory information so that strong priors override benign bodily signals and make people believe that something serious is wrong with the body. Combining these distinct accounts provides a unifying framework for persistent physical symptoms and shifts the focus away from their causes to the sustaining mechanisms that prevent symptoms from subsiding spontaneously. Based on this integrative model, we derive new avenues for future research and discuss implications for treating people with PPS in clinical practice.}
}
@incollection{VANLEEMPUT2015373,
title = {Tissue Classification},
editor = {Arthur W. Toga},
booktitle = {Brain Mapping},
publisher = {Academic Press},
address = {Waltham},
pages = {373-381},
year = {2015},
isbn = {978-0-12-397316-0},
doi = {https://doi.org/10.1016/B978-0-12-397025-1.00308-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780123970251003080},
author = {K. {Van Leemput} and O. Puonti},
keywords = {Bayesian modeling, Bias field correction, Expectation–maximization, Gaussian mixture model, Generative models, Markov random field, Probabilistic atlases, Segmentation, Tissue classification},
abstract = {Computational methods for automatically segmenting magnetic resonance images of the brain have seen tremendous advances in recent years. So-called tissue classification techniques, aimed at extracting the three main brain tissue classes (white matter, gray matter, and cerebrospinal fluid), are now well established. In their simplest form, these methods classify voxels independently based on their intensity alone, although much more sophisticated models are typically used in practice. This article aims to give an overview of often-used computational techniques for brain tissue classification. Although other methods exist, we concentrate on Bayesian modeling approaches, in which generative image models are constructed and subsequently ‘inverted’ to obtain automated segmentations. This general framework encompasses a large number of segmentation methods, including those implemented in widely used software packages such as SPM, FSL, and FreeSurfer.}
}
@article{GUO2023329,
title = {AIGC challenges and opportunities related to public safety: A case study of ChatGPT},
journal = {Journal of Safety Science and Resilience},
volume = {4},
number = {4},
pages = {329-339},
year = {2023},
issn = {2666-4496},
doi = {https://doi.org/10.1016/j.jnlssr.2023.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666449623000397},
author = {Danhuai Guo and Huixuan Chen and Ruoling Wu and Yangang Wang},
keywords = {Generative artificial intelligence， Artificial intelligence generated content, ChatGPT, Public safety, Strong artificial intelligence},
abstract = {Artificial intelligence generated content (AIGC) is a production method based on artificial intelligence (AI) technology that finds rules through data and automatically generates content. In contrast to computational intelligence, generative AI, as exemplified by ChatGPT, exhibits characteristics that increasingly resemble human-level comprehension and creation processes. This paper provides a detailed technical framework and history of ChatGPT, followed by an examination of the challenges posed to political security, military security, economic security, cultural security, social security, ethical security, legal security, machine escape problems, and information leakage. Finally, this paper discusses the potential opportunities that AIGC presents in the realms of politics, military, cybersecurity, society, and public safety education.}
}
@article{KUTAKA2024101964,
title = {How story problems strengthen arithmetic problem-solving strategy sophistication: Evidence from a learning trajectory teaching experiment in kindergarten},
journal = {Learning and Instruction},
volume = {93},
pages = {101964},
year = {2024},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2024.101964},
url = {https://www.sciencedirect.com/science/article/pii/S0959475224000914},
author = {Traci Shizu Kutaka and Pavel Chernyavskiy and Menglong Cong and Kayla McCreadie and Julie Sarama and Douglas H. Clements},
keywords = {Problem-solving strategies, Microgenetic coding, Early childhood mathematics, Arithmetic story problems},
abstract = {Background
The sophistication of young children's arithmetic problem-solving strategies can be influenced through experience and instructional intervention. One potential pathway is through encountering story problems where the location of the unknown quantity varies.
Aims
The goal of the present study is to characterize how arithmetic problem-solving strategy sophistication can evolve through opportunities to solve story problems.
Sample
We used microgenetic principles to guide the coding of arithmetic problem-solving behavior (8843 attempts) across three timescales (time within-session, attempt to solve, and between sessions) for nine story problem structures (N = 40, 19 girls). Data come from a teaching experiment conducted in a Mountain West US state in Spring 2018.
Methods
We employed a Bayesian hierarchical ordinal regression with a nine-level response variable. The model contained fixed effects for session, attempt, story problem structure; a smooth time within session effect; and random effects for student, instructor, and equation.
Results
Our analysis indicates which transitions from less to more sophisticated strategies are better supported by additional attempts to solve the same problem vs. additional instructional sessions. Strategy sophistication also varied by the location of the unknown quantity (result unknown, find difference, start unknown), but not operation (join, separate, part-whole).
Conclusions
If confirmed by other studies, including experiments, what teachers offer children in terms of learning opportunities (more attempts within the same problem or more problems across work sessions) should vary based on the transition they are making.}
}
@article{ALGERAFI2024101018,
title = {Promoting inclusivity in education amid the post-COVID-19 challenges: An interval-valued fuzzy model for pedagogy method selection},
journal = {The International Journal of Management Education},
volume = {22},
number = {3},
pages = {101018},
year = {2024},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2024.101018},
url = {https://www.sciencedirect.com/science/article/pii/S1472811724000892},
author = {Mohammed A.M. Al-Gerafi and Shankha Shubhra Goswami and Sushil Kumar Sahoo and Raman Kumar and Vladimir Simic and Nebojsa Bacanin and Quadri Noorulhasan Naveed and Ayodele Lasisi},
keywords = {Pedagogy method selection, Inclusivity, Post-COVID-19, Interval-valued fuzzy, Multi-criteria decision-making},
abstract = {The advent of the COVID-19 pandemic has brought about unprecedented disruptions in the field of education, necessitating a reevaluation of pedagogical approaches in the post-COVID era. This research paper introduces a novel interval-valued fuzzy simple additive weighting (SAW), weighted product model (WPM), and weighted aggregates sum product assessment (WASPAS) multi-criteria decision-making (MCDM) framework to address the challenge of promoting inclusivity in education amidst the post-COVID era. Leveraging the uncertainty inherent in the post-pandemic educational landscape, the proposed method offers a comprehensive pedagogy selection approach incorporating interval-valued fuzzy sets to account for imprecise and ambiguous data. By integrating the principles of inclusivity and diversity, the method evaluates various pedagogical approaches and their effectiveness in fostering an inclusive learning environment for diverse student populations. The study showcases the application of pedagogy selection in real-world educational scenarios, demonstrating its potential to inform policy decisions and enable educational institutions to adapt and cater to the evolving needs of learners in the aftermath of the COVID-19 pandemic.}
}
@article{KANG2023114835,
title = {Numerical evaluation and analysis of highly oscillatory singular Bessel transforms with a particular oscillator},
journal = {Journal of Computational and Applied Mathematics},
volume = {420},
pages = {114835},
year = {2023},
issn = {0377-0427},
doi = {https://doi.org/10.1016/j.cam.2022.114835},
url = {https://www.sciencedirect.com/science/article/pii/S0377042722004332},
author = {Hongchao Kang and Meijuan Zhang},
keywords = {Singular Bessel transform, Two-point Taylor interpolation polynomial, Modified Filon-type method, Special Hermite interpolation polynomial, Clenshaw-Curtis-Filon-type method, Error analysis},
abstract = {This paper proposes and analyzes two affordable and efficient quadrature rules for the numerical approximation of the oscillatory Bessel transform ∫0bxα(b−x)βf(x)Jν(ωxγ)dx with algebraic singularities, where b,α,β,ν,ω,γ denote the given constants. Firstly, we derive the explicit formula and asymptotic estimation of the generalized moments ∫0bxα′(b−x)β′Jν(ωxγ)dx with α′,β′>−1 by means of the Meijer G function. Furthermore, we design a modified Filon-type method based on a two-endpoint Taylor interpolation polynomial. In particular, we also give a more efficient Clenshaw–Curtis–Filon-type method in view of a special Hermite interpolation polynomial at the Clenshaw–Curtis points. Moreover, this method is easily implemented by the fast Fourier transform and fast computation of the modified moments. The useful homogeneous recurrence relation of the required modified moments is derived by the Bessel equation and the properties of the Chebyshev polynomial. Importantly, the rigorous error analyses in inverse powers of ω for the proposed numerical methods are carried out in details. Some primary numerical experiments can confirm our theoretical analysis, and verify the accuracy and efficiency of the proposed numerical methods.}
}
@incollection{ARTOPOULOS2023280,
title = {Knowledge economy meets development imaginaries},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {280-289},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.01040-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012818630501040X},
author = {Alejandro Artopoulos},
keywords = {Knowledge economy, Knowledge society, Educational change, Sociotechnical imaginaries},
abstract = {Born in the mid-1990s, the term Knowledge Economy has been a driving force in discussion around development and education in the last three decades. It became the main legitimating argument for various programs promoting educational change and innovation by international organizations, national governments and social actors from industry, civil society, and academia. From a sociotechnical imaginaries approach, this paper aims to critically review the concept by mapping theoretical developments resulting from the dialog between applied research, policy proposals, and case studies. I will examine KE building as an endeavor that comprises symbolic and socio-material dimensions, analyzing discontinuities in specific structural and sociotechnical transitions. I propose conclude that there is no single, universal KE as a desirable development destination per se.}
}
@incollection{LOTTO2009381,
title = {Statistical Analysis of Visual Perception},
editor = {Larry R. Squire},
booktitle = {Encyclopedia of Neuroscience},
publisher = {Academic Press},
address = {Oxford},
pages = {381-386},
year = {2009},
isbn = {978-0-08-045046-9},
doi = {https://doi.org/10.1016/B978-008045046-9.01428-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780080450469014285},
author = {R.B. Lotto},
keywords = {Coding theoryEfficiency hypothesisEmpirical visionImage statisticsInformation theoryRate codingSparse codingVisual cortexVisual illusion},
abstract = {Although it is possible to describe a code in great detail, deciphering it requires understanding the nature of the information encoded. Similarly, explaining how we see what we do requires not only quantitative descriptions of the visual brain’s functional architecture but also a clear understanding of what is represented in that architecture. Theoretical and computational neuroscience attempts to explain what that information might be. This article reviews the rationale and evidence for the hypothesis that the brain encodes the statistics of natural images and the behavioral significance of natural images in past visual experience.}
}
@article{THORNQUIST2021675,
title = {Biochemical evidence accumulates across neurons to drive a network-level eruption},
journal = {Molecular Cell},
volume = {81},
number = {4},
pages = {675-690.e8},
year = {2021},
issn = {1097-2765},
doi = {https://doi.org/10.1016/j.molcel.2020.12.029},
url = {https://www.sciencedirect.com/science/article/pii/S1097276520309503},
author = {Stephen C. Thornquist and Maximilian J. Pitsch and Charlotte S. Auth and Michael A. Crickmore},
keywords = {neural networks, eruption, time, motivation, sexual behavior, PKA, cAMP, FLIM, evidence accumulation, },
abstract = {Summary
Neural network computations are usually assumed to emerge from patterns of fast electrical activity. Challenging this view, we show that a male fly’s decision to persist in mating hinges on a biochemical computation that enables processing over minutes to hours. Each neuron in a recurrent network contains slightly different internal molecular estimates of mating progress. Protein kinase A (PKA) activity contrasts this internal measurement with input from the other neurons to represent accumulated evidence that the goal of the network has been achieved. When consensus is reached, PKA pushes the network toward a large-scale and synchronized burst of calcium influx that we call an eruption. Eruptions transform continuous deliberation within the network into an all-or-nothing output, after which the male will no longer sacrifice his life to continue mating. Here, biochemical activity, invisible to most large-scale recording techniques, is the key computational currency directing behavior and motivational state.}
}
@incollection{KELLY201238,
title = {Empirical Challenges to Conventional Mind–Brain Theory},
editor = {V.S. Ramachandran},
booktitle = {Encyclopedia of Human Behavior (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {38-45},
year = {2012},
isbn = {978-0-08-096180-4},
doi = {https://doi.org/10.1016/B978-0-12-375000-6.00394-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780123750006003943},
author = {E.F. Kelly and E.W. Kelly},
keywords = {Automatisms, Consciousness, Free will, Genius, Memory, Mind–body problem, Mystical experience, Psi phenomena, Self},
abstract = {Despite many significant accomplishments, mainstream scientific psychology has not provided a satisfactory theory of the mind, or solved the mind–body problem, and physicalist accounts of the mind are approaching their limits without fully accounting for its properties. The computational theory of the mind has collapsed, forcing physicalism to retreat into what necessarily constitutes its final frontier, the unique biology of the brain, but this biological naturalism seems destined to fare little better. Some critical properties of human mental life can already be recognized as irreconcilable in principle with physical operations of the brain, and others appear likely to prove so as well.}
}
@article{YANG20227130,
title = {Topological Co-indices of Hydroxyethyl Starch Conjugated with Hydroxychloroquine Used for COVID-19 Treatment},
journal = {Polycyclic Aromatic Compounds},
volume = {42},
number = {10},
pages = {7130-7142},
year = {2022},
issn = {1040-6638},
doi = {https://doi.org/10.1080/10406638.2021.1996407},
url = {https://www.sciencedirect.com/science/article/pii/S1040663822006777},
author = {Jun Yang and Mehwish Hussain Muhammad and Muhammad Kamran Siddiqui and Muhammad Farhan Hanif and Muhammad Nasir and Safdar Ali and Jia-Bao Liu},
keywords = {Topological co-index, Forgotten co-index, Hydroxyethyl starch with Hydroxychloroquine conjugate, corona virus disease 2019, multiplicative co-index},
abstract = {In the era of 2019 the most dangerous disease that have affected more than 200 countries is Covid-19 (i.e. corona virus disease 2019) spread by the virus (i.e. novel coronavirus) which is transmitted from susceptible person to survivors in his surroundings. Such an epidemicity occurs first time in the world and number of cases are in excess of normal expectancy for the different areas that’s why it is not easy to overcome this pandemic circumstances immediately. However, it is a challenge to develop anti-virus drugs for the suspected patients at global level. The scientists are interested in detailed experiments on Chloroquine (CQ) and Hydroxychloroquine (HCQ) as one of the unique substance used in the manufacturing for an anti virus drugs. For instance the Hydroxychloroquine conjugated molecular structure became an interesting chemical by the scientist. The significance of both substances can be examine by the number of publications which are leading us to introduce an effective treatment. In this paper, we are interested to discussed physcio-chemical properties of Hydroxychloroquine structure and computational work for the specific topological descriptors named as the F-co-index, first Zagreb co-index and first multiplicative Zagreb co-index, second Zagreb co-index and second multiplicative Zagreb co-index.}
}
@incollection{DU201687,
title = {Chapter 4 - Object Classification Methods},
editor = {Da-Wen Sun},
booktitle = {Computer Vision Technology for Food Quality Evaluation (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {87-110},
year = {2016},
isbn = {978-0-12-802232-0},
doi = {https://doi.org/10.1016/B978-0-12-802232-0.00004-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128022320000049},
author = {C.-J. Du and H.-J. He and D.-W. Sun},
keywords = {Artificial neural network (ANN), Classification, Decision tree, Fuzzy logic, Statistical approaches, Support vector machine (SVM)},
abstract = {Classification is one of the essential features for food quality evaluation using computer vision. By applying classification, computer vision can simulate the human thinking process to make complicated judgments accurately, quickly, and very consistently. Generally, classification identifies objects by classifying them into one of the finite sets of classes. A wide variety of methods, such as the artificial neural network (ANN), statistical approaches, fuzzy logic, decision tree, and support vector machine (SVM), have been developed to implement the classification in the food quality evaluation. In this chapter, the fundamentals of these classification techniques in the computer vision for the food quality evaluation are elaborated.}
}
@article{THOMAS20111285,
title = {Web Wisdom: An essay on how Web 2.0 and Semantic Web can foster a global knowledge society},
journal = {Computers in Human Behavior},
volume = {27},
number = {4},
pages = {1285-1293},
year = {2011},
note = {Social and Humanistic Computing for the Knowledge Society},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2010.07.023},
url = {https://www.sciencedirect.com/science/article/pii/S0747563210002190},
author = {Christopher Thomas and Amit Sheth},
keywords = {Human and social computation, Social networking, Problem solving},
abstract = {Admittedly this is a presumptuous title that should never be used when reporting on individual research advances. Wisdom is just not a scientific concept. In this case, though, we are reporting on recent developments on the web that lead us to believe that the web is on the way to providing a platform for not only information acquisition and business transactions but also for large scale knowledge development and decision support. It is likely that by now every web user has participated in some sort of social function or knowledge accumulating function on the web, many times without even being aware of it, simply by searching and browsing, other times deliberately by e.g. adding a piece of information to a Wikipedia article or by voting on a movie on IMDB.com. In this paper we will give some examples of how Web Wisdom is already emerging, some ideas of how we can create platforms that foster Web Wisdom and a critical evaluation of types of problems that can be subjected to Web Wisdom.}
}
@article{FEI2022281,
title = {Hierarchical model updating strategy of complex assembled structures with uncorrelated dynamic modes},
journal = {Chinese Journal of Aeronautics},
volume = {35},
number = {3},
pages = {281-296},
year = {2022},
issn = {1000-9361},
doi = {https://doi.org/10.1016/j.cja.2021.03.023},
url = {https://www.sciencedirect.com/science/article/pii/S100093612100114X},
author = {Chengwei FEI and Haotian LIU and Rhea {PATRICIA LIEM} and Yatsze CHOY and Lei HAN},
keywords = {Aeroengine casings, Assembled structures, Correlated mode pair, Hierarchical model updating, Objective function, Uncorrelated modes},
abstract = {In structural simulation and design, an accurate computational model directly determines the effectiveness of performance evaluation. To establish a high-fidelity dynamic model of a complex assembled structure, a Hierarchical Model Updating Strategy (HMUS) is developed for Finite Element (FE) model updating with regard to uncorrelated modes. The principle of HMUS is first elaborated by integrating hierarchical modeling concept, model updating technology with proper uncorrelated mode treatment, and parametric modeling. In the developed strategy, the correct correlated mode pairs amongst the uncorrelated modes are identified by an error minimization procedure. The proposed updating technique is validated by the dynamic FE model updating of a simple fixed–fixed beam. The proposed HMUS is then applied to the FE model updating of an aeroengine stator system (casings) to demonstrate its effectiveness. Our studies reveal that (A) parametric modeling technique is able to build an efficient equivalent model by simplifying complex structure in geometry while ensuring the consistency of mechanical characteristics; (B) the developed model updating technique efficiently processes the uncorrelated modes and precisely identifies correct Correlated Mode Pairs (CMPs) between FE model and experiment; (C) the proposed HMUS is accurate and efficient in the FE model updating of complex assembled structures such as aeroengine casings with large-scale model, complex geometry, high-nonlinearity and numerous parameters; (D) it is appropriate to update a complex structural FE model parameterized. The efforts of this study provide an efficient updating strategy for the dynamic model updating of complex assembled structures with experimental test data, which is promising to promote the precision and feasibility of simulation-based design optimization and performance evaluation of complex structures.}
}
@article{RAO2020106014,
title = {Quickly calculating reduct: An attribute relationship based approach},
journal = {Knowledge-Based Systems},
volume = {200},
pages = {106014},
year = {2020},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2020.106014},
url = {https://www.sciencedirect.com/science/article/pii/S0950705120303142},
author = {Xiansheng Rao and Xibei Yang and Xin Yang and Xiangjian Chen and Dun Liu and Yuhua Qian},
keywords = {Approximation quality, Attribute reduction, Attribute relationship, Granularity, Rough set},
abstract = {Presently, attribute reduction, as one of the most important topics in the field of rough set, has been widely explored from different perspectives. To derive the qualified reduct defined in attribute reduction, forward greedy searching is frequently used. However, the previous researches indicate that such searching strategy may be still computationally expensive if the volume of data is large. In view of this, two frameworks are proposed by considering the relationships between attributes, which aim to accelerate the process of searching reducts. Our consideration is actually realized based on the dissimilarity and similarity between attributes, respectively. The main mechanisms are: (1) for the dissimilarity based approach, the combination of attributes with significant difference instead of one and only one attribute will be added into potential reduct in the process of searching reduct; (2) for the similarity based approach, the candidate attributes which are similar to those attributes in potential reduct will be tentatively ignored instead of being evaluated in the process of searching reduct. The experimental results over 16 UCI data sets demonstrate that whether single granularity or multi-granularity attribute reduction is considered, our proposed approaches can not only generate the reducts which may not lead to poorer performances, but also provide superior time efficiency of calculating reducts. This study suggests new trends for quickly computing reducts.}
}
@article{BENINGER2024595,
title = {Artificial Intelligence, Drug Development and Frameworks: An Opportunity to Enhance Understanding},
journal = {Clinical Therapeutics},
volume = {46},
number = {8},
pages = {595-596},
year = {2024},
issn = {0149-2918},
doi = {https://doi.org/10.1016/j.clinthera.2024.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0149291824002042},
author = {Paul Beninger}
}
@article{ZHANG2023106193,
title = {A customized two-stage parallel computing algorithm for solving the combined modal split and traffic assignment problem},
journal = {Computers & Operations Research},
volume = {154},
pages = {106193},
year = {2023},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2023.106193},
url = {https://www.sciencedirect.com/science/article/pii/S0305054823000576},
author = {Kai Zhang and Honggang Zhang and Qixiu Cheng and Xinyuan Chen and Zewen Wang and Zhiyuan Liu},
keywords = {Traffic Assignment Problem, Modal Split, Gradient Projection, Distributed Parallel computing},
abstract = {Efficiently solving the traffic assignment problem (TAP) for large-scale transport networks is a critical problem for transportation studies. Most of the existing algorithms for TAP are serial ones based on single-computer mode, which has inherently limited the computational efficiency, compared with parallel computing methods. Thus, this paper aims to propose an efficient distributed multi-computer cluster resource allocation method for the parallel computing of TAP. Previous studies on the parallel computing of TAP are mainly based on a single-mode, which is extended to a more complex combined modal split and traffic assignment (CMSTA) case in this paper. In order to decompose the CMSTA problem, we proposed a block-decomposed model for solving the CMSTA problem. Then we designed an optimal parallel computing resource schedule for solving each block problem more quickly on the huge transportation network. Therefore, we implemented a customized two-stage parallel (TP) algorithm that can fully use parallel resources. The first parallel stage of the TP algorithm is used in the path generation phase, and the second parallel stage is used in the path flow adjustment phase. Besides, the parallel slowdown is uncovered in calculating each block problem of the path flow adjustment phase by using parallel resources. Numerical examples are taken to validate the efficiency and robustness of the proposed TP algorithm.}
}
@article{PARVIZ2024100190,
title = {AI in education: Comparative perspectives from STEM and Non-STEM instructors},
journal = {Computers and Education Open},
volume = {6},
pages = {100190},
year = {2024},
issn = {2666-5573},
doi = {https://doi.org/10.1016/j.caeo.2024.100190},
url = {https://www.sciencedirect.com/science/article/pii/S2666557324000302},
author = {Muhammed Parviz},
keywords = {Artificial Intelligence, Education, STEM, Non-STEM, Perspective},
abstract = {The integration of artificial intelligence into education has emerged as a promising avenue for enriching teaching and learning experiences. Nevertheless, the successful implementation of artificial intelligence in educational contexts hinges upon various factors, one of which is the perspective of instructors. With this in mind, this study aimed to examine the perspectives of 536 instructors in STEM and non-STEM disciplines regarding AI integration. The respondents’ thoughts, opinions, and concerns regarding advantages, disadvantages and challenges were gathered through an online questionnaire featuring both closed and open-ended questions. Additionally, a series of semi-structured interview sessions were conducted with a cohort of instructors to collect qualitative and quantitative data. The findings revealed that both STEM and non-STEM instructors expressed positive attitudes toward the integration of AI technologies into education. However, notable differences in responses and concerns were also identified in relation to the perceived capabilities and limitations of AI technologies within educational contexts. The results further elucidated a spectrum of opinions on the benefits (e.g., scalability and tirelessness), drawbacks (e.g., deepfake technology and comfort-seeking behavior), and potential challenges (e.g., educational disillusionment and espionage) associated with AI integration. The study concluded by discussing the implications of these findings for STEM and non-STEM education and offering recommendations for the effective and ethical integration of AI technologies in classrooms.}
}
@article{TONG2023102181,
title = {Examining the unique contributions and developmental stability of individual forms of relational reasoning to mathematical problem solving},
journal = {Contemporary Educational Psychology},
volume = {73},
pages = {102181},
year = {2023},
issn = {0361-476X},
doi = {https://doi.org/10.1016/j.cedpsych.2023.102181},
url = {https://www.sciencedirect.com/science/article/pii/S0361476X23000358},
author = {Christine Kong-Yan Tong and Eason Sai-Kit Yip and Terry Tin-Yau Wong},
keywords = {Relational reasoning, Mathematical problem solving, Analogy, Antithesis},
abstract = {Relational reasoning, a higher-order cognitive ability that identifies meaningful patterns among information streams, has been suggested to underlie STEM development. This study attempted to explore the potentially unique contributions of four forms of relational reasoning (i.e., analogy, anomaly, antinomy, and antithesis) to mathematical problem solving. Two separate samples, fifth graders (n = 254) and ninth graders (n = 198), were assessed on their mathematical problem solving ability and the different forms of relational reasoning ability. Linear regression analysis was conducted, with participants’ age, working memory, and spatial skills as covariates. The results showed that analogical and antithetical reasoning abilities uniquely predicted mathematical problem solving. This pattern demonstrated developmental stability across a four-year time frame. The findings clarify the unique significance of individual forms of relational reasoning to mathematical problem solving and call for a shift of research direction to reasoning abilities when exploring dissimilarity-based relations (opposites in particular).}
}
@article{WANG2024122909,
title = {A parallel differential learning ensemble framework based on enhanced feature extraction and anti-information leakage mechanism for ultra-short-term wind speed forecast},
journal = {Applied Energy},
volume = {361},
pages = {122909},
year = {2024},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2024.122909},
url = {https://www.sciencedirect.com/science/article/pii/S0306261924002927},
author = {Jujie Wang and Yafen Liu and Yaning Li},
keywords = {Wind speed prediction, Feature extraction, Preventing information leakage, Parallel differential learning network},
abstract = {Accurate ultra-short-term prediction plays a very important role in maintaining power equipment, preventing accidents, and optimizing dispatch effectiveness. Currently, the decomposition-integration method is widely used in ultra-short-term wind speed prediction. However, most of the existing models ignore the problem of information leakage that occurs during data processing and the effect of discrepancies between multiple decomposition sequences on the prediction results, which poses a great challenge to the accuracy of wind speed prediction. Therefore, this study proposes an improved hybrid wind speed prediction framework based on an improved decomposition method, an anti-information leakage mechanism and an enhanced deep learning algorithm. First, the original sequences are processed using improved singular spectrum analysis (ISSA) to achieve an effective mining of deep features. Second, Transformer is selected to construct the input-output relationship model between the original sequence and the feature components to form an anti-information leakage mechanism. Finally, an enhanced hybrid deep learning model is built using the concept of parallel processing, which can simultaneously process subsequences of different complexity and effectively reduce the prediction error of the model. Simulation experiments are conducted using four sets of data from wind farms located in Liaoning Province, China. The results of the simulations demonstrate that the model performs better in predictions than the benchmark model.}
}
@article{CHALMERS20233341,
title = {David J. Chalmers},
journal = {Neuron},
volume = {111},
number = {21},
pages = {3341-3343},
year = {2023},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2023.10.018},
url = {https://www.sciencedirect.com/science/article/pii/S0896627323007997},
author = {David J. Chalmers},
abstract = {David Chalmers is a philosopher who studies consciousness. After sketching his background in mathematics, science, and philosophy, he describes the problems of consciousness and his collaboration with neuroscientists. He also discusses the roles of neuroscience and philosophy in studying consciousness and other topics as well as the future of these fields.}
}
@article{SUN2023106847,
title = {CTMLP: Can MLPs replace CNNs or transformers for COVID-19 diagnosis?},
journal = {Computers in Biology and Medicine},
volume = {159},
pages = {106847},
year = {2023},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2023.106847},
url = {https://www.sciencedirect.com/science/article/pii/S0010482523003128},
author = {Junding Sun and Pengpeng Pi and Chaosheng Tang and Shui-Hua Wang and Yu-Dong Zhang},
keywords = {COVID-19, CNNs, Transformers, MLPs, Transfer learning, Guided self-supervised learning},
abstract = {Background
Convolutional Neural Networks (CNNs) and the hybrid models of CNNs and Vision Transformers (VITs) are the recent mainstream methods for COVID-19 medical image diagnosis. However, pure CNNs lack global modeling ability, and the hybrid models of CNNs and VITs have problems such as large parameters and computational complexity. These models are difficult to be used effectively for medical diagnosis in just-in-time applications.
Methods
Therefore, a lightweight medical diagnosis network CTMLP based on convolutions and multi-layer perceptrons (MLPs) is proposed for the diagnosis of COVID-19. The previous self-supervised algorithms are based on CNNs and VITs, and the effectiveness of such algorithms for MLPs is not yet known. At the same time, due to the lack of ImageNet-scale datasets in the medical image domain for model pre-training. So, a pre-training scheme TL-DeCo based on transfer learning and self-supervised learning was constructed. In addition, TL-DeCo is too tedious and resource-consuming to build a new model each time. Therefore, a guided self-supervised pre-training scheme was constructed for the new lightweight model pre-training.
Results
The proposed CTMLP achieves an accuracy of 97.51%, an f1-score of 97.43%, and a recall of 98.91% without pre-training, even with only 48% of the number of ResNet50 parameters. Furthermore, the proposed guided self-supervised learning scheme can improve the baseline of simple self-supervised learning by 1%–1.27%.
Conclusion
The final results show that the proposed CTMLP can replace CNNs or Transformers for a more efficient diagnosis of COVID-19. In addition, the additional pre-training framework was developed to make it more promising in clinical practice.}
}
@article{ABELLO2004345,
title = {Hierarchical graph maps},
journal = {Computers & Graphics},
volume = {28},
number = {3},
pages = {345-359},
year = {2004},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2004.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0097849304000275},
author = {James Abello},
keywords = {Visualization, Massive data sets, Graphs, Hierarchy trees},
abstract = {Graphs and maps are powerful abstractions. Their combination, Hierarchical Graph Maps, provide effective tools to process a graph that is too large to fit on the screen. They provide hierarchical visual indices (i.e. maps) that guide navigation and visualization. Hierarchical graph maps deal in a unified manner with both the screen and I/O bottlenecks. This line of thinking adheres to the Visual Information Seeking Mantra: Overview first, zoom and filter, then details on demand (Information Visualization: dynamic queries, star field displays and lifelines, in www.cr.umd.edu, 1997). We highlight the main tasks behind the computation of Graph Maps and provide several examples. The techniques have been used experimentally in the navigation of graphs defined on vertex sets ranging from 100 to 250 million vertices.}
}
@article{SU2024100319,
title = {A comparative review of technology-assisted and non-technology concept mapping-based language learning},
journal = {International Journal of Educational Research Open},
volume = {6},
pages = {100319},
year = {2024},
issn = {2666-3740},
doi = {https://doi.org/10.1016/j.ijedro.2024.100319},
url = {https://www.sciencedirect.com/science/article/pii/S2666374024000013},
author = {Fan Su and Di Zou},
keywords = {Concept mapping, Technology-based concept mapping, Language learning, Systematic review},
abstract = {Concept mapping-based language learning (CMLL) has attracted increasing attention from the research community. Many studies have investigated non-technology-based CMLL (NTCMLL) and technology-based CMLL (TCMLL); however, the literature reveals no reviews comparing the two, which is needed because this can identify the differentiated applicability of technology-and non-technology-based CM activities for assisting language learning. Accordingly, the present study reviews 26 studies comparing NTCMLL with TCMLL regarding publication nature, theoretical framework, target language, learning outcomes, CM activities, and technologies used for concept mapping. The results show that (a) NTCMLL and TCMLL studies have become popular since 2016; (b) meaningful learning was the most common theoretical support; (c) English was the most commonly investigated language; (d) the most discussed learning outcomes were language acquisition and psychological states; (e) individual concept mapping was frequently used; and (f) ready-made tools were applied more than researchers’ self-developed systems. We also identify the similarities and differences between NTCMLL and TCMLL studies while discussing the important implications for their future design.}
}
@article{FETSCH201616,
title = {The importance of task design and behavioral control for understanding the neural basis of cognitive functions},
journal = {Current Opinion in Neurobiology},
volume = {37},
pages = {16-22},
year = {2016},
note = {Neurobiology of cognitive behavior},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2015.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0959438815001804},
author = {Christopher R Fetsch},
abstract = {The success of systems neuroscience depends on the ability to forge quantitative links between neural activity and behavior. Traditionally, this process has benefited from the rigorous development and testing of hypotheses using tools derived from classical psychophysics and computational motor control. As our capacity for measuring neural activity improves, accompanied by powerful new analysis strategies, it seems prudent to remember what these traditional approaches have to offer. Here I present a perspective on the merits of principled task design and tight behavioral control, along with some words of caution about interpretation in unguided, large-scale neural recording studies. I argue that a judicious combination of new and old approaches is the best way to advance our understanding of higher brain function in health and disease.}
}
@article{AUBERT2022100799,
title = {Processes against tests: On defining contextual equivalences},
journal = {Journal of Logical and Algebraic Methods in Programming},
volume = {129},
pages = {100799},
year = {2022},
issn = {2352-2208},
doi = {https://doi.org/10.1016/j.jlamp.2022.100799},
url = {https://www.sciencedirect.com/science/article/pii/S2352220822000529},
author = {Clément Aubert and Daniele Varacca},
keywords = {Process algebra, Concurrency, Testing equivalences, Process semantics},
abstract = {In this paper, we would like to offer and defend a template to study equivalences between programs—in the particular framework of process algebras for concurrent computation. We believe that our layered model of development will clarify the distinction that is too often left implicit between the tasks and duties of the programmer and of the tester. It will also enlighten pre-existing issues that have been running across process algebras such as the calculus of communicating systems, the π-calculus—also in its distributed version—or mobile ambients. Our distinction starts by subdividing the notion of process in three conceptually separated entities, that we call process terms, (completed) processes and tests, and by stressing the importance of formalizing the completion of process terms and the instrumentation that results from placing a (completed) process into a test. While the role of what can be observed and the subtleties in the definitions of congruences have been intensively studied, the fact that not every term can be tested, and that the tester should have access to a different set of tools than the programmer is curiously left out, or at least not often formally discussed–in this respect, the theory of monitor is a counter-example that we discuss and compare to our approach. We argue that this blind spot comes from the under-specification of contexts—environments in which comparisons occur—that play multiple distinct roles but are generally—at least, on the surface of it—given only one definition that fails to capture all of their aspects.}
}
@incollection{2024xiii,
title = {Preface},
editor = {Preetha Evangeline David and P. Anandhakumar},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {132},
pages = {xiii-xiv},
year = {2024},
booktitle = {Applying Computational Intelligence for Social Good},
issn = {0065-2458},
doi = {https://doi.org/10.1016/S0065-2458(24)00009-3},
url = {https://www.sciencedirect.com/science/article/pii/S0065245824000093}
}
@incollection{PURZER202336,
title = {Engineering education},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {36-41},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.13020-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305130209},
author = {Senay Purzer},
keywords = {Design reasoning, Teachers as design coaches},
abstract = {Engineering entered into pre-college education with a heightened position in the early years of the 21st century. This integration took place in diverse ways and with a diverse range of motivation. This article articulates this diversity and argues for a careful examination of pre-college curriculum and practices in engineering. In particular, an understanding on the engineering philosophy and practices of the engineering profession must guide curriculum decisions to enable authentic and engaging learning and inclusive education.}
}
@article{DENG2023104360,
title = {Cognitive flexibility and emotion regulation: Dual layers of resilience against the emergence of paranoia},
journal = {Behaviour Research and Therapy},
volume = {167},
pages = {104360},
year = {2023},
issn = {0005-7967},
doi = {https://doi.org/10.1016/j.brat.2023.104360},
url = {https://www.sciencedirect.com/science/article/pii/S0005796723001092},
author = {Wisteria Deng and Kwaku Acquah and Jutta Joormann and Tyrone D. Cannon},
abstract = {Cognitive inflexibility has been linked to difficulties in revising paranoid beliefs, whereas cognitive flexibility may protect against the development and maintenance of paranoid beliefs by allowing for troubleshooting in light of available evidence. While less discussed in the context of paranoia research, better regulation of affective states may reduce the likelihood of biased beliefs developing in the first place, reducing the burden on belief updating mechanisms. The present study hypothesized that high cognitive flexibility and strong emotion regulation ability may act as a reciprocal protective shield against the risk associated with lower ability in the other domain. Participants were recruited from the general population (N = 221) to complete the Ambiguous Interpretation Inflexibility Task, as well as self-report measures for paranoia and emotion regulation ability. The results show an interaction between cognitive flexibility and emotion regulation ability as related to less severe paranoia. Better emotion regulation ability is associated with lower paranoia in individuals with lower cognitive flexibility, whereas higher cognitive flexibility is associated with less severe paranoia in individuals with greater emotion regulation difficulties. These findings highlight the importance of emotion regulation in early interventions of paranoia, especially how emotion regulation relates to known cognitive vulnerabilities such as inflexibility.}
}