@article{RASUL2024101041,
title = {Enhancing academic integrity among students in GenAI Era:A holistic framework},
journal = {The International Journal of Management Education},
volume = {22},
number = {3},
pages = {101041},
year = {2024},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2024.101041},
url = {https://www.sciencedirect.com/science/article/pii/S1472811724001125},
author = {Tareq Rasul and Sumesh Nair and Diane Kalendra and M.S. Balaji and Fernando de Oliveira Santini and Wagner Junior Ladeira and Raouf Ahmad Rather and Naveed Yasin and Raul V. Rodriguez and Panagiotis Kokkalis and Md Wahid Murad and Md Uzir Hossain},
keywords = {Generative AI, Academic integrity, Higher education, Students, Stakeholders},
abstract = {The introduction of Artificial Intelligence (AI), specifically Generative AI (GenAI), has significantly transformed the higher education landscape. Despite the opportunities GenAI offers to students, they pose significant challenges for academic integrity. Thus, it is crucial for higher education institutions (HEI) to balance the use of GenAI for enhancing the learning experience of students with its ethical and responsible use in their educational journey. The present study proposes a comprehensive academic integrity framework focusing on three key stakeholders: students, educators, and institutions. We propose eight strategies ranging from collaborative learning for students to developing a comprehensive GenAI policy for institutions in maintaining academic integrity among students in HEI. Furthermore, we identified four challenges, namely financial, strategic, operational, and cultural, in the implementing a comprehensive academic integrity framework in the GenAI era. This study offers significant insights for HEI to maintain academic integrity among students in the GenAI era.}
}
@article{KHOONG2024124091,
title = {Evaluating the growth of Singapore's solar electricity capacity towards Green Plan 2030 targets and beyond using system dynamics modelling approach},
journal = {Applied Energy},
volume = {376},
pages = {124091},
year = {2024},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2024.124091},
url = {https://www.sciencedirect.com/science/article/pii/S0306261924014740},
author = {Wei Kit Khoong and Sreenivasulu Bellam},
keywords = {Solar electricity capacity, Singapore's energy mix, Systems thinking, System dynamics modelling, Carbon savings, Singapore Green Plan 2030},
abstract = {Having no native energy resources of fossil fuels, with poor wind resource and scarcity of land, the Solar Photovoltaic (PV) roadmap identified solar electricity as the most feasible source of renewable energy for Singapore's energy mix and supply. Moving towards net-zero emissions and to combat climate change, the Singapore government is aiming to achieve 2-Gigawatt-peak (GWp) of solar electricity target by 2030. Accordingly, the share of solar energy in the national grid is targeted to be between ∼2–6% in 2030 and ∼ 3.5–8% in 2040, and carbon emission savings to be ∼0.5–1.4 and ∼ 0.8–2.1 million tonnes per annum in 2030 and 2040 respectively. Although these ambitious targets align with the government's plans for mitigating emissions, Singapore faces great challenge in terms of land availability to install ground-mounted solar PV panels. In this paper, a system dynamics model is developed to study- to what extent can Singapore achieve the targeted solar electricity goals by 2030 or even beyond based on Green Plan 2030, what policies can be identified to achieve these targets, and how much carbon savings can be achieved through Solar electricity deployment. Accordingly, this paper presents systems thinking and system dynamics (ST&SD) methodology to model the growth of Singapore's solar capacity, carbon emission savings and share of electricity demand met by solar electricity while focusing on key complex factors such as area utilisation, subsidies, PV panel efficiency etc. Results of our model simulations and projections, based on the key data and assumptions, and policy scenarios show that Singapore's solar capacity can be accelerated by the implementation of the proposed policies to reach 2GWp goal towards 2030 or even slightly ahead of this timeline. However, should the government revise its solar capacity targets higher for the years past 2030 i.e. to achieve 8% share of total electricity generation, perhaps by 2040, policies such as an increased area utilisation, subsidies and higher panel efficiency need to be introduced. Our model simulations incorporating and evaluating these policy scenarios yielded the results aligning with the projections mentioned above. The results and insights presented in this paper offer useful recommendations to the researchers and policy makers in the field of solar electricity system in Singapore, and to study further for better policy making.}
}
@article{MA201542,
title = {Towards computational models of animal cognition, an introduction for computer scientists},
journal = {Cognitive Systems Research},
volume = {33},
pages = {42-69},
year = {2015},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2014.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041714000357},
author = {Zhanshan (Sam) Ma},
keywords = {Animal cognition, Cognitive ecology, Social learning, Bioinspired computing and communication, Behavioral informatics, Computational behavior biology},
abstract = {The last few years of the twentieth century witnessed the emerging convergence of biology and computer science and this trend has been accelerating since then. The study of animal behavior or behavior biology has been one of the major contributors for this convergence. Behavior is fascinating because it is the response of an organism to internal and external signals and it is controlled by complex interactions among nerves, the sensory and the motor systems. To some extent, behavior is similar to the output (or response) of a computer system or a network node if we consider an animal brain as a computer node. This paper is the first in a two-part series in which I review the state-of-the-art research in behavior biology inspired computing and communication, with the first part focusing on animal cognition and the second part on animal communication (Ma, 2014). The present article also assumes the task of presenting a general introduction on behavior biology literature, which sets a foundation for synthesizing both parts of the series but the synthesis will be performed in the second part of the series. I sets three objectives in this ‘cognition’ part: (i) to present a brief overview on the literature of behavior biology for computer scientists; (ii) to summarize the state-of-the-art studies in several cognitive aspects of animal behavior: focusing on emerging research in cognitive ecology, social learning and innovation, as well as animal logics; (iii) to review some important existing studies inspired by animal behavior and further present a perspective on the future research. These cognition-related topics offer insights for research fields such as machine learning, human computer interactions (HCI), brain computer interfaces (BCIs), evolutionary computing, pervasive computing, etc. In perspective, I suggest that the interaction between behavioral biology and computer science should be bidirectional, and a new subject, behavioral informatics, or more general computational behavior biology, should be developed by the cooperative efforts between biologists and computer scientists.}
}
@incollection{GALLEGATI20173,
title = {Chapter 1 - An Introduction to Agent-Based Computational Macroeconomics},
editor = {Mauro Gallegati and Antonio Palestrini and Alberto Russo},
booktitle = {Introduction to Agent-Based Economics},
publisher = {Academic Press},
pages = {3-11},
year = {2017},
isbn = {978-0-12-803834-5},
doi = {https://doi.org/10.1016/B978-0-12-803834-5.00002-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128038345000023},
author = {Mauro Gallegati and Antonio Palestrini and Alberto Russo}
}
@incollection{PHIPPEN2024,
title = {Digital Literacy},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95689-5.00097-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956895000973},
author = {Andy Phippen},
keywords = {AI literacy, Critical thinking, Data literacy, Digital literacy, Digital wellbeing, DQ standard, Education for a connected world, Information literacy, Media literacy, Online safety, Social media literacy, Stakeholders},
abstract = {Digital literacy is a crucial skill set in the contemporary era, encompassing technical proficiency, information and media literacy, data literacy, and more. There are further disciplines that are incorporated into the broad concept of digital literacy, including cybersecurity, online safety, and responsible communication. The importance of critical thinking in digital contexts and the emerging field of digital wellbeing are addressed. There are challenges in achieving digital literacy including the lack of common frameworks and diverse barriers such as access to technology, affordability, and cultural differences. Ultimately digital literacy is something that requires the input of various stakeholders, including educators, governments, technology providers, and community organizations. There is a clear need for a collaborative, multi-pronged approach to address these challenges and the need for common agreement on what digital literacy is.}
}
@article{LOONG2014237,
title = {Tourism and Simulacrum: The Computational Economy of Algorithmic Destinations},
journal = {Procedia - Social and Behavioral Sciences},
volume = {144},
pages = {237-246},
year = {2014},
note = {5th Asia-Euro Conference 2014 in Tourism, Hospitality & Gastronomy},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2014.07.292},
url = {https://www.sciencedirect.com/science/article/pii/S1877042814042207},
author = {Bernard Lew Shian Loong},
keywords = {gamified tourism, algorithmic destinations, simulacrum, computational economics, tourism computability, reflexivity},
abstract = {The paper establishes a conceptual and methodological link between destinations and simulacrum through gamified tourism. As a paradigm, gamified tourism provides a rationale and a setting within which to apply computational economics to tourism, an approach amounting to tourism computability. Algorithmic destinations serve as “petri dishes” for real destinations. Utilizing rule sets that embody destination growth dynamics and visitor behavioural norms, seeding points in a cellular automata model (CA) were grown into algorithmic destinations. This is followed by a morphological transformation of geo-tagged satellite images into spatial points. The overlap of this additive and subtractive approach is at the core of tourism computability. Finally, the spatio-temporal dynamics of economic resilience was traced out through a visual phenomenology of algorithmic destinations. The gamification of tourism should be embraced as it holds up a flicker of hope for mature destinations, amidst the onset of museumification and increased commoditization of heritage sites. Gamification is treated as part of the reflexive cycle for destination authenticity; a notion that that Cohen (1988) alluded to in his discussion of emergent authenticity in destination image formation. Seen in this light, the museumification of Venice and the proliferation of its simulacrum, such as the Venetian Hotel in Macao and Venice-themed hotels across the globe, are prefigures and archetypes of a glorious age of gamified tourism.}
}
@article{NAKHLEH2013719,
title = {Computational approaches to species phylogeny inference and gene tree reconciliation},
journal = {Trends in Ecology & Evolution},
volume = {28},
number = {12},
pages = {719-728},
year = {2013},
issn = {0169-5347},
doi = {https://doi.org/10.1016/j.tree.2013.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0169534713002139},
author = {Luay Nakhleh},
abstract = {An intricate relation exists between gene trees and species phylogenies, due to evolutionary processes that act on the genes within and across the branches of the species phylogeny. From an analytical perspective, gene trees serve as character states for inferring accurate species phylogenies, and species phylogenies serve as a backdrop against which gene trees are contrasted for elucidating evolutionary processes and parameters. In a 1997 paper, Maddison discussed this relation, reviewed the signatures left by three major evolutionary processes on the gene trees, and surveyed parsimony and likelihood criteria for utilizing these signatures to elucidate computationally this relation. Here, I review progress that has been made in developing computational methods for analyses under these two criteria, and survey remaining challenges.}
}
@article{OISHI2017327,
title = {Computational mechanics enhanced by deep learning},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {327},
pages = {327-351},
year = {2017},
note = {Advances in Computational Mechanics and Scientific Computation—the Cutting Edge},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2017.08.040},
url = {https://www.sciencedirect.com/science/article/pii/S0045782517306199},
author = {Atsuya Oishi and Genki Yagawa},
keywords = {Deep learning, Artificial neural network, Numerical quadrature, Element stiffness matrix},
abstract = {The present paper describes a method to enhance the capability of, or to broaden the scope of computational mechanics by using deep learning, which is one of the machine learning methods and is based on the artificial neural network. The method utilizes deep learning to extract rules inherent in a computational mechanics application, which usually are implicit and sometimes too complicated to grasp from the large amount of available data A new method of numerical quadrature for the FEM stiffness matrices is developed by using the proposed method, where a kind of optimized quadrature rule superior in accuracy to the standard Gauss–Legendre quadrature is obtained on the element-by-element basis. The detailed formulation of the proposed method is given with the sample application above, and an acceleration technique for the proposed method is discussed}
}
@incollection{WARD201140,
title = {Analogies},
editor = {Mark A. Runco and Steven R. Pritzker},
booktitle = {Encyclopedia of Creativity (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {40-45},
year = {2011},
isbn = {978-0-12-375038-9},
doi = {https://doi.org/10.1016/B978-0-12-375038-9.00009-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780123750389000091},
author = {T.B. Ward},
keywords = {ACME, Analogy, Case study, Computational modeling,  study, Laboratory study, Mapping, Multiconstraint theory, One-to-one correspondence, Parallel connectivity, Retrieval, SME, Source domain, Structure-mapping theory, Systematicity, Target domain},
abstract = {Analogical thinking is a fundamental cognitive process underlying creativity. Analogies map structured knowledge from one domain to another and serve as information for understanding, explaining and creating. Analogy is studied through case study, laboratory, in vivo and computational modeling approaches. The use of analogy is often suggested to be a helpful technique in applied approaches to creativity.}
}
@article{SAVIN2021106878,
title = {Free associations of citizens and scientists with economic and green growth: A computational-linguistics analysis},
journal = {Ecological Economics},
volume = {180},
pages = {106878},
year = {2021},
issn = {0921-8009},
doi = {https://doi.org/10.1016/j.ecolecon.2020.106878},
url = {https://www.sciencedirect.com/science/article/pii/S0921800920309484},
author = {Ivan Savin and Stefan Drews and Jeroen {van den Bergh}},
keywords = {Structural topic modelling, Growth-vs-environment debate, Public opinion, Scientific opinion, Green growth},
abstract = {The debate about the relationship between economic growth and environmental sustainability triggers a range of associations. Here we analyze open-ended textual responses of citizens and scientists concerning their associations with the terms “economic growth” and “green growth”. We derive from the responses a number of topics and examine how associations differ across distinct opinion segments of people, namely supporters of Green growth, Agrowth and Degrowth. The results indicate that the general public is more critical of the notion of economic growth than academic researchers. Citizens stress problems of corruption, social inequality, unemployment and poverty, with less variation among the three opinion segments compared to scientists. The latter more strongly emphasize the environmental consequences of economic growth. Concerning associations of scientists with the term “green growth”, we find topics questioning its feasibility to be more likely expressed by Degrowth supporters, while topics stressing the possibility of sustainable economic growth by Green growth supporters. We find that topic polarization is stronger for scientists than citizens. Our results provide further validation for opinion clusters identified in previous studies and uncover additional insights about related views on growth and sustainability.}
}
@article{KHAN2022200147,
title = {An effective approach to address processing time and computational complexity employing modified CCT for lung disease classification},
journal = {Intelligent Systems with Applications},
volume = {16},
pages = {200147},
year = {2022},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2022.200147},
url = {https://www.sciencedirect.com/science/article/pii/S2667305322000849},
author = {Inam Ullah Khan and Sami Azam and Sidratul Montaha and Abdullah Al Mahmud and A.K.M. Rakibul Haque Rafid and Md. Zahid Hasan and Mirjam Jonkman},
keywords = {COVID-19, Chest X-rays, Image Preprocessing, Modified compact convolutional transformer, Deep convolutional GAN, Hyper-parameter Tuning},
abstract = {Early identification and adequate treatment can help prevent lung disorders from becoming chronic, severe, and life-threatening. X-ray images are commonly used and an automated and effective method involving deep learning techniques can potentially contribute to quick and accurate diagnosis of lung disorders. However, in the study of medical imaging using deep learning, two obstacles limit interpretability. One is an insufficient and imbalanced number of training samples in most medical datasets. The other is excessive training time. Although training time can be reduced by decreasing the number of pixels in the images, training with low resolution images tends to result in poor performance. This study represents a solution to overcome these impediments by balancing the number of images and reducing overall processing time while preserving accuracy. The dataset used in this research contains an unequal number of images in the different classes. The quantity of data in the classes is balanced by creating synthetic images based on the patterns and characteristics of the original images, using a Deep Convolutional Generative Adversarial Network (DCGAN). Unwanted regions are removed from the X-ray images, the brightness and contrast of the images are enhanced, and the abnormalities are highlighted by using different artifact removal, noise reduction, and enhancement techniques. We propose a Modified Compact Convolutional Transformer (MCCT) model using 32 × 32 sized images for the categorization of lung disorders into four classes. An ablation study of eleven cases is employed to adjust several hyper parameters and layer topologies. This reduces training time while preserving accuracy. Six transfer learning models, VGG19, VGG16, ResNet152, ResNet50, ResNet50V2, and MobileNet are applied with the same image size the performance is compared with the proposed MCCT model. Our MCCT model records the greatest test accuracy of 95.37%, requiring a short training time, 10-12 s/epoch, whereas the other models only reach near-moderate performance with accuracies ranging from 43% to 79% and training times of 80-90 s/epoch. The robustness of the model with regards to the number of training samples is validated by training the model multiple times reducing the number of training images gradually from 49621 images to 6204 images. Results suggest that even with a smaller dataset, the performance is sustained. Our proposed approach may contribute to an effective CAD based diagnostic system by addressing the issues of insufficient and imbalanced numbers of medical images, excessive training times and low-resolution images.}
}
@article{KANAMORI2024103319,
title = {Kunen the expositor},
journal = {Annals of Pure and Applied Logic},
volume = {175},
number = {1, Part B},
pages = {103319},
year = {2024},
note = {Kenneth Kunen (1943-2020)},
issn = {0168-0072},
doi = {https://doi.org/10.1016/j.apal.2023.103319},
url = {https://www.sciencedirect.com/science/article/pii/S0168007223000763},
author = {Akihiro Kanamori},
keywords = {Handbook chapters, Set theory text, Late texts},
abstract = {Kunen's expository work is described, bringing out both his way of assimilating and thinking about set theory and how it had a meaningful hand in its promulgation into the next generations.}
}
@article{MATENCIO2021129639,
title = {A physicochemical, thermodynamical, structural and computational evaluation of kynurenic acid/cyclodextrin complexes},
journal = {Food Chemistry},
volume = {356},
pages = {129639},
year = {2021},
issn = {0308-8146},
doi = {https://doi.org/10.1016/j.foodchem.2021.129639},
url = {https://www.sciencedirect.com/science/article/pii/S0308814621006452},
author = {Adrián Matencio and Fabrizio Caldera and Alberto {Rubin Pedrazzo} and Yousef {Khazaei Monfared} and Nilesh {K. Dhakar} and Francesco Trotta},
keywords = {Kynurenic acid, Cyclodextrin, Inclusion complex, Physicochemical, Stability},
abstract = {In this work, the interaction between Kynurenic acid (KYNA) and several natural and modified cyclodextrins (CDs) is carried out. Among all the CD tested, HPβ-CD showed the strongest complexation constant (KF), with a value of 270.94 ± 29.80 M−1. Between natural (α- and β-) CDs, the complex of KYNA with β-CD was the most efficient. The inclusion complex of KYNA with CDs showed a strong influence of pH and temperature. The KF value decreased at high pH values, when the pKa was passed. Moreover, an increase of the temperature caused a decrease in the KF values. The thermodynamic parameters of the complexation (ΔH°, ΔS° and ΔG°) were studied with negative entropy, enthalpy and spontaneity of the process at 25 °C. Moreover, the inclusion complex was also characterized using FTIR and TGA. Finally, molecular docking calculations provided different interactions and their influence in the complexation constant.}
}
@article{ZHAN2022100096,
title = {The effectiveness of gamification in programming education: Evidence from a meta-analysis},
journal = {Computers and Education: Artificial Intelligence},
volume = {3},
pages = {100096},
year = {2022},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2022.100096},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X22000510},
author = {Zehui Zhan and Luyao He and Yao Tong and Xinya Liang and Shihao Guo and Xixin Lan},
keywords = {Programming education, Gamification, Meta-analysis, Game-based learning},
abstract = {This paper aimed at constructing a systematic framework and examining the effect of gamification in programming education through a meta-analysis conducted on 21 empirical studies published in the last decade. We examined the effects of game types, gamification applications, pedagogical agents, programming types, and schooling levels on students' academic achievement, cognitive load, motivation, and thinking skills in programming education by cross-tabulation analysis. Results verified the positive impact of gamification in programming education. Gamification has the largest effect on students' motivation, followed by academic achievement, whereas it has the least effect on students' cognitive load. As for game types, the reasoning strategy game is most effective on academic achievement, while the puzzle game is most effective on motivation. As for gamification application, the games as a competitive mechanism has the greatest impact on students’ thinking skills and motivation. However, when games were adopted as teaching tools or student works, the effects are mainly represented in academic achievement. Pedagogical agents have a limited effect on programming education. With regard to programming types, the effect of gamification is more pronounced in text-based programming rather than graphical programming. This study provided an analytic framework and shed light on potential directions for further studies in the field.}
}
@article{WANG2007126,
title = {Nature-inspired Computation — Effective Realization of Artificial Intelligence},
journal = {Systems Engineering - Theory & Practice},
volume = {27},
number = {5},
pages = {126-134},
year = {2007},
issn = {1874-8651},
doi = {https://doi.org/10.1016/S1874-8651(08)60034-4},
url = {https://www.sciencedirect.com/science/article/pii/S1874865108600344},
author = {Lei WANG and Qi KANG and Qi-di WU},
keywords = {nature-inspired computation, general mode, uniform framework mode, neural networks, swarm intelligence},
abstract = {In nature-inspired computation, different intelligent computation modes of agents usually have different extrinsic forms; but can they take on some relative uniform characteristics? To validate this idea, further systematic study on nature-inspired computation from a more macroscopical angle is made in this article and the uniform framework mode of nature-inspired computation is consequently summarized and presented, as well as described with feedback neural network and swarm intelligence algorithms. On the basis of the defined general mode framework, agents of the algorithms in a nature-inspired computation field can show a type of uniform intelligent computation mode.}
}
@article{SU2021100862,
title = {Is the Text-Based Cognitive Tool More Effective Than the Concept Map on Improving the Pre-Service Teachers’ Argumentation Skills?},
journal = {Thinking Skills and Creativity},
volume = {41},
pages = {100862},
year = {2021},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2021.100862},
url = {https://www.sciencedirect.com/science/article/pii/S1871187121000778},
author = {Guo Su and Taotao Long},
keywords = {Argumentation skills, cognitive tools, pre-service teachers},
abstract = {How to improve pre-service teachers’ argumentation skills has been receiving more and more attention from teacher educators. Visual cognitive tool refers to tools which users can learn with and creatively use to construct knowledge online. Current research revealed that it could help to improve learners’ higher-order thinking skills. This experimental study aimed to investigate the effect of two kinds of cognitive tools, the text-based online visual cognitive tool and the visual concept map, on improving the pre-service teachers’ skills on constructing and evaluating arguments. Post-test argumentation measurement scores and attitude questionnaire showed that the text-based cognitive tool was more effective than the concept map on improving pre-service teachers’ argumentation skills. However, the concept map was useful for externalizing the pre-service teachers’ thinking process as well as collaborative learning. This study also found that the pre-service teachers with teaching experience were inferior to the ones without any teaching experience in the ability on constructing arguments.}
}
@article{KORIYAMA2021458,
title = {Inclusive cognitive hierarchy},
journal = {Journal of Economic Behavior & Organization},
volume = {186},
pages = {458-480},
year = {2021},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2021.04.016},
url = {https://www.sciencedirect.com/science/article/pii/S0167268121001578},
author = {Yukio Koriyama and Ali I. Ozkes},
keywords = {Cognitive hierarchy, Collective decision-making, Level- model, Strategic thinking},
abstract = {Cognitive hierarchy theory, a collection of structural models of non-equilibrium thinking, in which players’ best responses rely on heterogeneous beliefs on others’ strategies including naïve behavior, proved powerful in explaining observations from a wide range of games. We propose an inclusive cognitive hierarchy model, in which players do not rule out the possibility of facing opponents at their own thinking level. Our theoretical results show that inclusiveness is crucial for asymptotic properties of deviations from equilibrium behavior in expansive games. We show that the limiting behaviors are categorized in three distinct types: naïve, Savage rational with inconsistent beliefs, and sophisticated. We test the model in a laboratory experiment of collective decision-making. The data suggests that inclusiveness is indispensable with regard to explanatory power of the models of hierarchical thinking.}
}
@article{HALEEM2024100006,
title = {Perspective of leadership 4.0 in the era of fourth industrial revolution: A comprehensive view},
journal = {Journal of Industrial Safety},
volume = {1},
number = {1},
pages = {100006},
year = {2024},
issn = {2950-2764},
doi = {https://doi.org/10.1016/j.jinse.2024.100006},
url = {https://www.sciencedirect.com/science/article/pii/S2950276424000060},
author = {Abid Haleem and Mohd Javaid and Ravi Pratap Singh},
keywords = {Leadership 4.0, Industry 4.0, Industrial Safety, Technologies, Management},
abstract = {Leadership 4.0 focuses on leaders developing their digital transformation strategy and ensuring alignment with the organization’s business and development ambitions. This is accomplished by successfully displaying disruptive digital leadership characteristics, which include emotional and social intelligence abilities such as empathy and relationship management, cognitive preparedness, critical thinking, inventive thinking, agility, and resilience. Academics and consultants increasingly use Leadership 4.0 to describe the new leadership style required for the Fourth Industrial Revolution (Industry 4.0). It strategically addresses people’s concerns, which are crucial for the effective integration of Industry 4.0, and plays a significant and crucial role in integrating Industry 4.0 into modern workplaces. The primary purpose of this paper is to explore Leadership 4.0 and its needs. Several quality characteristics associated with Digital Leadership 4.0 are investigated, and two-dimensional style matrix presentations for Leadership 4.0 are briefed. Finally, this study identifies and addresses the role of Leadership 4.0 in upcoming industrial management systems. Because digital technologies now impact the entire business, advancing digital strategies requires strong leadership at all levels. With the increasing prevalence of digital transformation in the business sector and the intensification of the "battle for talent," organizations need to consider a more methodical approach to building a solid leadership pipeline with the capabilities required to lead in the digital era. They may place future leaders in positions that require them to go beyond their current competencies and skills to instruct and motivate them to promptly acquire new digital skills. In a new working setting, effectively managing the dynamic interactions between machines, technology, and people is essential for influential digital leaders. Leadership 4.0 is expected to foster an open and innovative culture that welcomes change and progress. This will encourage and inspire their teams to adapt to the ongoing changes in the market.}
}
@article{DALLACHIARA201894,
title = {A many-valued approach to quantum computational logics},
journal = {Fuzzy Sets and Systems},
volume = {335},
pages = {94-111},
year = {2018},
note = {Special Issue: Selected Papers from the 36th Linz Seminar on Fuzzy Set Theory},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2016.12.015},
url = {https://www.sciencedirect.com/science/article/pii/S0165011416304560},
author = {M.L. {Dalla Chiara} and R. Giuntini and G. Sergioli and R. Leporini},
keywords = {Quantum logics, Quantum tomography, Logical gates},
abstract = {Quantum computational logics are special examples of quantum logic where formulas are supposed to denote pieces of quantum information (qubit-systems or mixtures of qubit-systems), while logical connectives are interpreted as reversible quantum logical gates. Hence, any formula of the quantum computational language represents a synthetic logical description of a quantum circuit. We investigate a many-valued approach to quantum information, where the basic notion of qubit has been replaced by the more general notion of qudit. The qudit-semantics allows us to represent as reversible gates some basic logical operations of Łukasiewicz many-valued logics. In the final part of the article we discuss some problems that concern possible implementations of gates by means of optical devices.}
}
@incollection{LAWSON1990108,
title = {9 - Creative thinking},
editor = {Bryan Lawson},
booktitle = {How Designers Think (Second Edition)},
publisher = {Butterworth-Heinemann},
edition = {Second Edition},
pages = {108-120},
year = {1990},
isbn = {978-0-7506-0268-6},
doi = {https://doi.org/10.1016/B978-0-7506-0268-6.50013-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780750602686500139},
author = {Bryan Lawson},
abstract = {Publisher Summary
This chapter discusses creative thinking in design. Design is a creative occupation, and good designers are creative people. One of the most vexing and perennial questions in design education concerns the balance between the free, open-ended, and expressive work demanded of the student and attention to the acquisition of knowledge, discipline, and experience. The effects of experience on problem solving are not always beneficial. In industry, the need to improve already successful products provides the ultimate test of creative thinking. When using personal analogy, the problem solver identifies personally with some part of the problem or solution, thus acting out the situation. Fantasy analogy allows the designer to suspend the sense of credulity and to explore the seemingly fantastic or impossible. Creativity is not only skill or talent but is also related to context—the situation within which the person perceives the problem and performs the process.}
}
@article{PERFORS2012486,
title = {When do memory limitations lead to regularization? An experimental and computational investigation},
journal = {Journal of Memory and Language},
volume = {67},
number = {4},
pages = {486-506},
year = {2012},
issn = {0749-596X},
doi = {https://doi.org/10.1016/j.jml.2012.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0749596X12000800},
author = {Amy Perfors},
keywords = {Regularization, Less is More, Computational modeling, Language acquisition},
abstract = {The Less is More hypothesis suggests that one reason adults and children differ in their ability to learn language is that they also differ in other cognitive capacities. According to one version of this hypothesis, children’s relatively poor memory may make them more likely to regularize inconsistent input (Hudson Kam and Newport, 2005, Hudson Kam and Newport, 2009). This paper reports the result of an experimental and computational investigation of one aspect of this version of the hypothesis. A series of seven experiments in which adults were placed under a high cognitive load during a language-learning task reveal that in adults, increased load during learning (as opposed to retrieval) does not result in increased regularization. A computational model offers a possible explanation for these results. It demonstrates that, unless memory limitations distort the data in a particular way, regularization should occur only in the presence of both memory limitations and a prior bias for regularization. Taken together, these findings suggest that the difference in regularization between adults and children may not be solely attributable to differences in memory limitations during learning.}
}
@article{MYERSCOUGH2014e143,
title = {Tracking the development of atherosclerosis in silico: a computational model for early inflammatory events},
journal = {Atherosclerosis},
volume = {235},
number = {2},
pages = {e143},
year = {2014},
issn = {0021-9150},
doi = {https://doi.org/10.1016/j.atherosclerosis.2014.05.404},
url = {https://www.sciencedirect.com/science/article/pii/S0021915014006406},
author = {M. Myerscough and A. Chalmers}
}
@article{KARPOVA2016v,
title = {Editorial overview: Neurobiology of cognitive behavior: Complexity of neural computation and cognition},
journal = {Current Opinion in Neurobiology},
volume = {37},
pages = {v-viii},
year = {2016},
note = {Neurobiology of cognitive behavior},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2016.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0959438816300125},
author = {Alla Karpova and Roozbeh Kiani}
}
@article{VALENTINOV2015491,
title = {Nonprofit organizations, institutional economics, and systems thinking},
journal = {Economic Systems},
volume = {39},
number = {3},
pages = {491-501},
year = {2015},
note = {Symposium: Financial System and Development in China},
issn = {0939-3625},
doi = {https://doi.org/10.1016/j.ecosys.2014.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0939362515000278},
author = {Vladislav Valentinov and Stefan Hielscher and Ingo Pies},
keywords = {Nonprofit organizations, John Kenneth Galbraith, Countervailing power, Niklas Luhmann},
abstract = {The present paper applies the logic of John Kenneth Gailbraith's institutional economics analysis of corporate power to inquiring into the societal role of the nonprofit sector. Building on Galbraith's insight that corporations cause subtle but pervasive societal imbalances, the paper locates the role of nonprofit organizations in compensating for these imbalances, thus showing corporations and nonprofit organizations to be mutually complementary rather than antagonistic actors. This argument is supported by Niklas Luhmann's vision of the precarious relationship between the complexity and sustainability of social systems as well as by Kenneth Boulding's analysis of the farmer and labor movement. Luhmann's and Boulding's perspectives show profit-seeking corporations to be social systems developing high technological complexity at the cost of sacrificing their societal sustainability, while the improvement of the latter constitutes the rationale of many nonprofit organizations. The same systems-theoretic logic suggests, however, that nonprofit organizations may tend to underestimate the technological complexity of implementing their mission-related activities, thereby undermining their own effectiveness.}
}
@article{SUN200912529,
title = {A computational model of an intuitive reasoner for ecosystem control},
journal = {Expert Systems with Applications},
volume = {36},
number = {10},
pages = {12529-12536},
year = {2009},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2009.04.037},
url = {https://www.sciencedirect.com/science/article/pii/S0957417409003686},
author = {Yung-Chien Sun and Grant Clark},
keywords = {Artificial Intelligence, Intuition, Knowledge acquisition, Limited certainty},
abstract = {Intuition is the human capacity to make decisions under novel, complex situations where knowledge is incomplete and of variable levels of certainty. We take the view that intuition can be modeled as a rational and deductive mode of information processing which is suited to novel, complex situations. In this research, a computational algorithm, or “intuitive reasoner”, is proposed which mimics some aspects of human intuition by combining established mathematical tools, such as fuzzy set theory, and some novel innovations. A rule-based scheme is followed and a rule-learning module that allows rules to be learned from incomplete datasets is developed. The input and the rules drawn by the reasoner are allowed to be fuzzy, multi-valued, and low in certainty. A measure of the certainty level, Strength of Belief, is attached to each input as well as each rule. Solutions are formulated through iterations of consolidating intermediate reasoning results, during which the Strength of Belief of corroborating intermediate results is combined. An experimental implementation of the proposed intuitive reasoner is reported, in which the reasoner was used to solve a classification problem. The results showed that, when given increasingly sparse input data, the rule-learning module generated more rules of lower associated certainty than when presented with more complete data. The intuitive reasoner was able to make use of these low-certainty rules to solve the classification problems with an accuracy that compared favorably to that of traditional methods based on complete datasets.}
}
@article{DEHOLLANDER2016101,
title = {Different Ways of Linking Behavioral and Neural Data via Computational Cognitive Models},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {1},
number = {2},
pages = {101-109},
year = {2016},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2015.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S2451902215000166},
author = {Gilles {de Hollander} and Birte U. Forstmann and Scott D. Brown},
keywords = {Cognition, Computational models, Functional neuroimaging, Joint modeling, Linking, Mathematical models},
abstract = {Cognitive neuroscientists sometimes apply formal models to investigate how the brain implements cognitive processes. These models describe behavioral data in terms of underlying, latent variables linked to hypothesized cognitive processes. A goal of model-based cognitive neuroscience is to link these variables to brain measurements, which can advance progress in both cognitive and neuroscientific research. However, the details and the philosophical approach for this linking problem can vary greatly. We propose a continuum of approaches that differ in the degree of tight, quantitative, and explicit hypothesizing. We describe this continuum using four points along it, which we dub qualitative structural, qualitative predictive, quantitative predictive, and single model linking approaches. We further illustrate by providing examples from three research fields (decision making, reinforcement learning, and symbolic reasoning) for the different linking approaches.}
}
@article{TRNKOVA2019106900,
title = {Rigorous computations with an approximate Dirichlet domain},
journal = {Topology and its Applications},
volume = {268},
pages = {106900},
year = {2019},
issn = {0166-8641},
doi = {https://doi.org/10.1016/j.topol.2019.106900},
url = {https://www.sciencedirect.com/science/article/pii/S0166864119303116},
author = {Maria Trnková},
keywords = {Length spectrum, Dirichlet domain, Hyperbolic 3-manifold},
abstract = {In this paper we address some problems concerning an approximate Dirichlet domain. We show that under some assumptions an approximate Dirichlet domain can work equally well as an exact Dirichlet domain. In particular, we consider a problem of tiling a hyperbolic ball with copies of the Dirichlet domain. This problem arises in the construction of the length spectrum algorithm which is implemented by the computer program SnapPea. Our result explains the empirical fact that the program works surprisingly well despite it does not use exact data. Also we demonstrate a rigorous verification whether two words of the fundamental group of a hyperbolic 3-manifold are the same or not.}
}
@article{MARUYAMA1987437,
title = {New economic thinking: Morphogenetic causal loops and product adaptation strategy},
journal = {Futures},
volume = {19},
number = {4},
pages = {437-441},
year = {1987},
issn = {0016-3287},
doi = {https://doi.org/10.1016/0016-3287(87)90005-X},
url = {https://www.sciencedirect.com/science/article/pii/001632878790005X},
author = {Magoroh Maruyama},
abstract = {This article sets out to dispel two widespread economic superstitions—the belief in an inherent equilibrium of the economic system, and the perception of international trade as a zero-sum game. The author argues that morphogenetic causal loops disprove the first assumption, and should be used to aid policy making; and that positive-sum results could be obtained by lifting import restrictions and adapting products for foreign markets.}
}
@article{ZETTERLUND2023104508,
title = {Computational modelling to advise and inform optimization for aeration and nutrient-dosing in wastewater treatment: Case study from pulp and paper mill in south-central Sweden},
journal = {Journal of Water Process Engineering},
volume = {56},
pages = {104508},
year = {2023},
issn = {2214-7144},
doi = {https://doi.org/10.1016/j.jwpe.2023.104508},
url = {https://www.sciencedirect.com/science/article/pii/S2214714423010280},
author = {Selma Zetterlund and Olivia Schwartz and Maria Sandberg and G. Venkatesh},
keywords = {Aeration, Biological wastewater treatment, Energy use optimisation, Nutrients, Pulp and paper mills},
abstract = {Sweden's pulp and paper sector accounts for a significant proportion of national energy usage, besides generating wastewater that causes eutrophication of nearby sinks. In this paper, the possibility of optimizing biological wastewater treatment at the Stora Enso Skoghall mill south of the city of Karlstad in central Sweden, with respect to electricity usage and the addition of nutrients, has been investigated. A computational model of the treatment process was developed, based on process data obtained from the said mill, and nine different scenarios were compared subsequently, with energy use, environmental impacts and operational expenses, as criteria. The most energy-efficient and cost-effective alternative was a combination of measures such as lowering the oxygen level in the MBBR (Moving Bed Bio-Reactor) from 3 mg/l to 2 mg/l and using the Hyperclassic aerator in the aerated lagoon. This arrangement yielded a 48.5 % reduction in operational expenses, and a 60 % decrease in the energy use, vis-à-vis the reference case, without affecting the efficiency of the treatment process. This also uncovered an opportunity to mitigate the annual global warming and eutrophication impacts, by approximately 100 tons CO2-eq. and 140 kg PO43−-eq. respectively. All attempts to optimise the use of resources and decrease the anthropogenic environmental footprint ought to be made to come closer to the targets set by the United Nations' sustainable development goals (SDGs). The authors' conclusion predicated on the results of the modelling and analysis done in this study is that the potential of seemingly small process modifications, such as lowering the oxygen level in the MBBR, and applying a more optimal dosage of nutrient salts, must not be overlooked by wastewater treatment plants in general (and those in pulp and paper mills in particular).}
}
@article{SHARMA2022132755,
title = {Conformational stability, quantum computational, spectroscopic, molecular docking and molecular dynamic simulation study of 2-hydroxy-1-naphthaldehyde},
journal = {Journal of Molecular Structure},
volume = {1259},
pages = {132755},
year = {2022},
issn = {0022-2860},
doi = {https://doi.org/10.1016/j.molstruc.2022.132755},
url = {https://www.sciencedirect.com/science/article/pii/S0022286022004288},
author = {Arun Sharma and Ghazala Khanum and Anuj Kumar and Aysha Fatima and Meenakshi Singh and Khamael M. Abualnaja and Khaled Althubeiti and S. Muthu and Nazia Siddiqui and Saleem Javed},
keywords = {DFT studies, Fukui Function, MEP, ELF, Hirshfeld, Molecular docking},
abstract = {Experimental FTIR, NMR and UV-visible spectrum analyses were used to describe the title compound 2-Hydroxy-1-Naphthaldehyde. The optimized molecular geometry and vibrational wave numbers were determined by using the DFT approach and B3LYP/6-311++G(d, p) basis set. VEDA was used to determine the vibrational assignments. The GIAO technique was used to compute carbon and proton NMR chemical shifts in CDCl3. The most reactive location of the 2H1NA molecule, according to MEP map analysis, is the site containing the oxygen atom. TD-DFT approach was used to produce the theoretical UV-visible spectrum in MeOH and gas phase. HOMO-LUMO and Donor-Acceptor (NBO) interactions were investigated for the title compound. In addition, nonlinear optical characteristics, ELF and Fukui activity were investigated. Temperature-dependent thermodynamic characteristics were also computed. The 3D intermolecular interactions of the crystal surface were characterised using Hirshfeld surface analysis, whereas the 2D interactions were explained using fingerprint plots. 2H1NA was stabilized by the development of H—H/H—C/H—O contacts. The bioactive probability of the title molecule was theoretically demonstrated by computing the electrophilicity index. In a biological study six different receptors, molecular docking was performed to evaluate the best ligand-protein interactions and likeness to the active substance. Biomolecular stability was investigated using a molecular dynamics simulation.}
}
@article{READ2017237,
title = {Virtual personalities: Using computational modeling to understand within-person variability},
journal = {Journal of Research in Personality},
volume = {69},
pages = {237-249},
year = {2017},
note = {Within-Person Variability in Personality},
issn = {0092-6566},
doi = {https://doi.org/10.1016/j.jrp.2016.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0092656616301738},
author = {Stephen J. Read and Benjamin J. Smith and Vitaliya Droutman and Lynn C. Miller},
keywords = {Virtual personalities, Within-person variability, Between-person variability, Social computational modeling},
abstract = {How can the same underlying psychological/neurobiological system result in both stable between-individual differences and high levels of within-individual variability in personality states over time and situations? We argue that both types of variability result from a psychological system based on structured, chronic motivations, where behavior at a specific point in time is a joint function of the current availability of motive affordances in the situation, current motivationally relevant bodily or interoceptive states, and the result of the competition among alternative active motives. Here we present a biologically-based theoretical framework, embodied in two different computational models, that shows how individuals with stable personality characteristics, can nevertheless exhibit considerable within-person variability in personality states across time and situations.}
}
@article{OESCH2021990,
title = {How REM sleep shapes hypothalamic computations for feeding behavior},
journal = {Trends in Neurosciences},
volume = {44},
number = {12},
pages = {990-1003},
year = {2021},
issn = {0166-2236},
doi = {https://doi.org/10.1016/j.tins.2021.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0166223621001831},
author = {Lukas T. Oesch and Antoine R. Adamantidis},
keywords = {sleep, feeding, goal-directed behavior, hypothalamus, population coding},
abstract = {The electrical activity of diverse brain cells is modulated across states of vigilance, namely wakefulness, non-rapid eye movement (NREM) sleep, and rapid eye movement (REM) sleep. Enhanced activity of neuronal circuits during NREM sleep impacts on subsequent awake behaviors, yet the significance of their activation, or lack thereof, during REM sleep remains unclear. This review focuses on feeding-promoting cells in the lateral hypothalamus (LH) that express the vesicular GABA and glycine transporter (vgat) as a model to further understand the impact of REM sleep on neural encoding of goal-directed behavior. It emphasizes both spatial and temporal aspects of hypothalamic cell dynamics across awake behaviors and REM sleep, and discusses a role for REM sleep in brain plasticity underlying energy homeostasis and behavioral optimization.}
}
@article{CAPUTO2023113309,
title = {Building T-shaped professionals for mastering digital transformation},
journal = {Journal of Business Research},
volume = {154},
pages = {113309},
year = {2023},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2022.113309},
url = {https://www.sciencedirect.com/science/article/pii/S0148296322007640},
author = {Francesco Caputo and Valentina Cillo and Fabio Fiano and Marco Pironti and Marco Romano},
keywords = {Digital transformation, T-shaped professionals, Systems thinking, Soft skills, Cognitive domain},
abstract = {Digital transformation is a multidimensional challenge that is requiring to change consolidated approaches and managerial models. New organized entities are emerging because of this ongoing transformation and new competences are required for managing and living them. Thanks to the interpretative contribution provided by the systems thinking, the paper focuses the attention on the paradigm shift required for defining t-shaped professionals able to master digital transformation in emerging dynamics. A conceptual model is proposed and discussed building upon the T-shaped model with the aim to enrich current theoretical and managerial debates about strategies for supporting both individuals and organizations in facing the challenges imposed by the digital transformation.}
}
@incollection{VOINOV202427,
title = {Participatory Modeling for Sustainability},
editor = {Martin A. Abraham},
booktitle = {Encyclopedia of Sustainable Technologies (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {27-35},
year = {2024},
isbn = {978-0-443-22287-0},
doi = {https://doi.org/10.1016/B978-0-323-90386-8.00020-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323903868000206},
author = {Alexey Voinov},
keywords = {Biases, Modeling process, Social media, Stakeholders, Wicked problem},
abstract = {Sustainability is a wicked problem, which is hard to define in a unique way. It cannot be solved and should be treated in a participatory approach involving as many stakeholders in the process as possible. Participatory modeling is an efficient method for dealing with wicked problems. It involves stakeholders in an open-ended process of shared learning and can be essential for developing sustainable technologies. While there may be various levels of participation, the process evolves around a model of the system at stake. The model is built in interaction with the stakeholders, it provides to formalism to synchronize stakeholder thinking and knowledge about the system and to move towards consensus about the possible decision-making.}
}
@article{MEACHAM2023103902,
title = {Fire safety of existing residential buildings: Building regulatory system gaps and needs},
journal = {Fire Safety Journal},
volume = {140},
pages = {103902},
year = {2023},
issn = {0379-7112},
doi = {https://doi.org/10.1016/j.firesaf.2023.103902},
url = {https://www.sciencedirect.com/science/article/pii/S0379711223001704},
author = {Brian J. Meacham},
keywords = {Regulatory system, Existing buildings, Fire risk, Systems thinking},
abstract = {Considerable cost and effort are invested in government and private-sector activities aimed at providing a societally tolerable level of fire safety of the built environment. This is particularly true with respect to fire safety of new building construction. On the government side, this includes activities associated with building and fire regulations, material performance and test standards, design guidance, competency requirements, review and approvals, and more. On the private side, activities include product development, analysis and design, construction and installation, as well as education and training of practitioners. In some cases there are overlaps (e.g., private building control). However, once buildings become occupied, the system faces several challenges. Oversight of building use and modification often gets lost. Different actors come into play. Competing objectives become more significant. Occupants often lack understanding and ability to recognize problems and make adjustments. The net result is an increase in fire safety risk over the life of a building, with less opportunities for the regulatory system to make interventions prior to an unwanted fire event. However, this can be changed if the approach to regulating existing buildings changes, and importantly, embodies whole-of-life, multi-agency, holistic, systems-based thinking.}
}
@article{MAITY2016152,
title = {A Computational Model to Predict Aesthetic Quality of Text Elements of GUI},
journal = {Procedia Computer Science},
volume = {84},
pages = {152-159},
year = {2016},
note = {Proceeding of the Seventh International Conference on Intelligent Human Computer Interaction (IHCI 2015)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.04.081},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916300965},
author = {Ranjan Maity and Akshay Madrosiya and Samit Bhattacharya},
keywords = {Aesthetics, web page, text elements, aesthetic score, categorization},
abstract = {The role of aesthetics in determining usability of interactive systems has come under focus in recent time. The issue is relevant for Graphical User Interfaces (GUI) containing elements of widely varying nature. It is important to evaluate GUI aesthetically to determine their acceptability to the users. Computational models have been reported in the literature to perform objective assessment of interface aesthetics. However, the existing models only consider geometric features at the highest level, without considering the content inside the geometry. To address this issue, we propose a computational model to evaluate aesthetics of textual contents present on a GUI. The proposed model is based on empirical data collected from user studies. The model is a weighted sum of six features characterizing text: chromatic contrast, luminance contrast, font size, letter spacing, line height and word spacing. A separate validation study demonstrates the feasibility and potential of the model (showing 87% accuracy in model prediction), which is expected to be useful in predicting usability of a web page in a more refined way. Such modeling has its obvious implications in the context of engineering interactive systems. The proposed model along with the user studies are presented in this paper.}
}
@article{NOWICKI2012324,
title = {Improving the computational efficiency of metric-based spares algorithms},
journal = {European Journal of Operational Research},
volume = {219},
number = {2},
pages = {324-334},
year = {2012},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2011.12.033},
url = {https://www.sciencedirect.com/science/article/pii/S0377221711011271},
author = {David R. Nowicki and Wesley S. Randall and Jose Emmanuel Ramirez-Marquez},
keywords = {Inventory, Performance based logistics, Logistics, Supply chain, Optimization, Outcome based contracting},
abstract = {We propose a new heuristic algorithm to improve the computational efficiency of the general class of Multi-Echelon Technique for Recoverable Item Control (METRIC) problems. The objective of a METRIC-based decision problem is to systematically determine the location and quantity of spares that either maximizes the operational availability of a system subject to a budget constraint or minimizes its cost subject to an operational availability target. This type of sparing analysis has proven essential when analyzing the sustainment policies of large-scale, complex repairable systems such as those prevalent in the defense and aerospace industries. Additionally, the frequency of these sparing studies has recently increased as the adoption of performance-based logistics (PBL) has increased. PBL represents a class of business strategies that converts the recurring cost associated with maintenance, repair, and overhaul (MRO) into cost avoidance streams. Central to a PBL contract is a requirement to perform a business case analysis (BCA) and central to a BCA is the frequent need to use METRIC-based approaches to evaluate how a supplier and customer will engage in a performance based logistics arrangement where spares decisions are critical. Due to the size and frequency of the problem there exists a need to improve the efficiency of the computationally intensive METRIC-based solutions. We develop and validate a practical algorithm for improving the computational efficiency of a METRIC-based approach. The accuracy and effectiveness of the proposed algorithm are analyzed through a numerical study. The algorithm shows a 94% improvement in computational efficiency while maintaining 99.9% accuracy.}
}
@article{BRAMLEY2023105471,
title = {Active inductive inference in children and adults: A constructivist perspective},
journal = {Cognition},
volume = {238},
pages = {105471},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2023.105471},
url = {https://www.sciencedirect.com/science/article/pii/S0010027723001051},
author = {Neil R. Bramley and Fei Xu},
keywords = {Hypothesis generation, Active learning, Inductive inference, Developmental change, Concept learning, Program induction},
abstract = {A defining aspect of being human is an ability to reason about the world by generating and adapting ideas and hypotheses. Here we explore how this ability develops by comparing children’s and adults’ active search and explicit hypothesis generation patterns in a task that mimics the open-ended process of scientific induction. In our experiment, 54 children (aged 8.97±1.11) and 50 adults performed inductive inferences about a series of causal rules through active testing. Children were more elaborate in their testing behavior and generated substantially more complex guesses about the hidden rules. We take a ‘computational constructivist’ perspective to explaining these patterns, arguing that these inferences are driven by a combination of thinking (generating and modifying symbolic concepts) and exploring (discovering and investigating patterns in the physical world). We show how this framework and rich new dataset speak to questions about developmental differences in hypothesis generation, active learning and inductive generalization. In particular, we find children’s learning is driven by less fine-tuned construction mechanisms than adults’, resulting in a greater diversity of ideas but less reliable discovery of simple explanations.}
}
@article{MOUTOUSSIS20212025,
title = {Decision-making ability, psychopathology, and brain connectivity},
journal = {Neuron},
volume = {109},
number = {12},
pages = {2025-2040.e7},
year = {2021},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2021.04.019},
url = {https://www.sciencedirect.com/science/article/pii/S0896627321002853},
author = {Michael Moutoussis and Benjamín Garzón and Sharon Neufeld and Dominik R. Bach and Francesco Rigoli and Ian Goodyer and Edward Bullmore and Peter Fonagy and Peter Jones and Tobias Hauser and Rafael Romero-Garcia and Michelle {St Clair} and Petra Vértes and Kirstie Whitaker and Becky Inkster and Gita Prabhu and Cinly Ooi and Umar Toseeb and Barry Widmer and Junaid Bhatti and Laura Villis and Ayesha Alrumaithi and Sarah Birt and Aislinn Bowler and Kalia Cleridou and Hina Dadabhoy and Emma Davies and Ashlyn Firkins and Sian Granville and Elizabeth Harding and Alexandra Hopkins and Daniel Isaacs and Janchai King and Danae Kokorikou and Christina Maurice and Cleo McIntosh and Jessica Memarzia and Harriet Mills and Ciara O’Donnell and Sara Pantaleone and Jenny Scott and Pasco Fearon and John Suckling and Anne-Laura {van Harmelen} and Rogier Kievit and Marc Guitart-Masip and Raymond J. Dolan},
keywords = {decision acuity, computational psychiatry, functional connectivity, adolescence, development},
abstract = {Summary
Decision-making is a cognitive process of central importance for the quality of our lives. Here, we ask whether a common factor underpins our diverse decision-making abilities. We obtained 32 decision-making measures from 830 young people and identified a common factor that we call “decision acuity,” which was distinct from IQ and reflected a generic decision-making ability. Decision acuity was decreased in those with aberrant thinking and low general social functioning. Crucially, decision acuity and IQ had dissociable brain signatures, in terms of their associated neural networks of resting-state functional connectivity. Decision acuity was reliably measured, and its relationship with functional connectivity was also stable when measured in the same individuals 18 months later. Thus, our behavioral and brain data identify a new cognitive construct that underpins decision-making ability across multiple domains. This construct may be important for understanding mental health, particularly regarding poor social function and aberrant thought patterns.}
}
@article{CHEN2021101001,
title = {Instructed concept appropriation for developing knowledge of second language academic discourse context},
journal = {Journal of English for Academic Purposes},
volume = {52},
pages = {101001},
year = {2021},
issn = {1475-1585},
doi = {https://doi.org/10.1016/j.jeap.2021.101001},
url = {https://www.sciencedirect.com/science/article/pii/S147515852100045X},
author = {Jing Chen and Danli Li},
keywords = {Concept-based language instruction, , Mediation, Concept appropriation, Writing activity, Academic literacy},
abstract = {Recent studies from sociocultural perspectives have explored the effects of Concept-based Language Instruction (C-BLI) on L2 development through the explicit teaching of scientific concepts. However, there has been little research into the effects of C-BLI on the development of L2 academic literacy. This article reports on a case study of how C-BLI mediated a Chinese doctoral student's development of conceptual knowledge of context and subsequent context-specific performance in academic writing. Drawing on data from writing tutorials and interviews, the learner's drafts and invited comments, and think-aloud protocols, the study revealed that the C-BLI interventions that integrated symbolic and dialogic mediation helped the learner attain and enhance awareness of contextual components. The learner appropriated the concept as a tool for thinking in judging appropriateness of rules of thumb and choices of exclusive discourse features in specific contexts of use, which consequently mediated his planning for writing and resulted in the development of performance. The study demonstrates the potential of C-BLI as a driving force for the development of conceptual knowledge and context-specific performance in the academic literacy of L2 learners. It has pedagogical implications for curriculum design, C-BLI-informed literacy and concept-based materials, and teacher development to stimulate teacher awareness in C-BLI.}
}
@article{MARINHO2021e06079,
title = {Quantum computational investigations and molecular docking studies on amentoflavone},
journal = {Heliyon},
volume = {7},
number = {1},
pages = {e06079},
year = {2021},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2021.e06079},
url = {https://www.sciencedirect.com/science/article/pii/S2405844021001845},
author = {Márcia M. Marinho and Francisco Wagner Q. Almeida-Neto and Emanuelle M. Marinho and Leonardo P. {da Silva} and Ramon R.P.P.B. Menezes and Ricardo P. {dos Santos} and Emmanuel S. Marinho and Pedro {de Lima-Neto} and Alice M.C. Martins},
keywords = {Antichagasic agent, Biflavonoid, DFT, Fukui analysis, NLO},
abstract = {Chagas disease is a neglected tropical disease caused by the protozoan parasite Trypanosoma cruzi, with approximately 6–7 million people infected worldwide, becoming a public health problem in tropical countries, thus generating an increasing demand for the development of more effective drugs, due to the low efficiency of the existing drugs. Aiming at the development of a new antichagasic pharmacological tool, the density functional theory was used to calculate the reactivity descriptors of amentoflavone, a biflavonoid with proven anti-trypanosomal activity in vitro, as well as to perform a study of interactions with the enzyme cruzain, an enzyme key in the evolutionary process of T-cruzi. Structural properties (in solvents with different values of dielectric constant), the infrared spectrum, the frontier orbitals, Fukui analysis, thermodynamic properties were the parameters calculated from DFT method with the monomeric structure of the apigenin used for comparison. Furthermore, molecular docking studies were performed to assess the potential use of this biflavonoid as a pharmacological antichagasic tool. The frontier orbitals (HOMO-LUMO) study to find the band gap of compound has been extended to calculate electron affinity, ionization energy, electronegativity electrophilicity index, chemical potential, global chemical hardness and global chemical softness to study the chemical behaviour of compound. The optimized structure was subjected to molecular Docking to characterize the interaction between amentoflavone and cruzain enzyme, a classic pharmacological target for substances with anti-gas activity, where significant interactions were observed with amino acid residues from each one's catalytic sites enzyme. These results suggest that amentoflavone has the potential to interfere with the enzymatic activity of cruzain, thus being an indicative of being a promising antichagasic agent.}
}
@article{TELIKANI2020318,
title = {A survey of evolutionary computation for association rule mining},
journal = {Information Sciences},
volume = {524},
pages = {318-352},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.02.073},
url = {https://www.sciencedirect.com/science/article/pii/S002002552030164X},
author = {Akbar Telikani and Amir H. Gandomi and Asadollah Shahbahrami},
keywords = {Data mining, Association rule mining, Evolutionary computation, Swarm intelligent},
abstract = {Association Rule Mining (ARM) is a significant task for discovering frequent patterns in data mining. It has achieved great success in a plethora of applications such as market basket, computer networks, recommendation systems, and healthcare. In the past few years, evolutionary computation-based ARM has emerged as one of the most popular research areas for addressing the high computation time of traditional ARM. Although numerous papers have been published, there is no comprehensive analysis of existing evolutionary ARM methodologies. In this paper, we review emerging research of evolutionary computation for ARM. We discuss the applications on evolutionary computations for different types of ARM approaches including numerical rules, fuzzy rules, high-utility itemsets, class association rules, and rare association rules. Evolutionary ARM algorithms were classified into four main groups in terms of the evolutionary approach, including evolution-based, swarm intelligence-based, physics-inspired, and hybrid approaches. Furthermore, we discuss the remaining challenges of evolutionary ARM and discuss its applications and future topics.}
}
@article{JOHNSON2024,
title = {Minds and markets as complex systems: an emerging approach to cognitive economics},
journal = {Trends in Cognitive Sciences},
year = {2024},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2024.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S1364661324001748},
author = {Samuel G.B. Johnson and Patrick R. Schotanus and J.A. Scott Kelso},
keywords = {decision-making, behavioral economics, narratives, agent-based models, extended mind, Coordination Dynamics},
abstract = {Cognitive economics is an emerging interdisciplinary field that uses the tools of cognitive science to study economic and social decision-making. Although most strains of cognitive economics share commitments to bridging levels of analysis (cognitive, behavioral, and systems) and embracing interdisciplinary approaches, we review a newer strand of cognitive economic thinking with a further commitment: conceptualizing minds and markets each as complex adaptive systems. We describe three ongoing research programs that strive toward these goals: (i) studying narratives as a cognitive and social representation used to guide decision-making; (ii) building cognitively informed agent-based models; and (iii) understanding markets as an extended mind – the Market Mind Hypothesis – analyzed using the concepts, methods, and tools of Coordination Dynamics.}
}
@article{SLOOT2010189,
title = {Computational science: A kaleidoscopic view into science},
journal = {Journal of Computational Science},
volume = {1},
number = {4},
pages = {189},
year = {2010},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2010.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S1877750310000694},
author = {Peter M.A. Sloot}
}
@article{FERNYHOUGH20231180,
title = {Inner speech as language process and cognitive tool},
journal = {Trends in Cognitive Sciences},
volume = {27},
number = {12},
pages = {1180-1193},
year = {2023},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2023.08.014},
url = {https://www.sciencedirect.com/science/article/pii/S1364661323002103},
author = {Charles Fernyhough and Anna M. Borghi},
keywords = {inner dialogue, inner monologue, verbal thinking, self-talk, self-regulation, phenomenology},
abstract = {Many people report a form of internal language known as inner speech (IS). This review examines recent growth of research interest in the phenomenon, which has broadly supported a theoretical model in which IS is a functional language process that can confer benefits for cognition in a range of domains. A key insight to have emerged in recent years is that IS is an embodied experience characterized by varied subjective qualities, which can be usefully modeled in artificial systems and whose neural signals have the potential to be decoded through advancing brain–computer interface technologies. Challenges for future research include understanding individual differences in IS and mapping form to function across IS subtypes.}
}
@article{MANTELERO2014643,
title = {The future of consumer data protection in the E.U. Re-thinking the “notice and consent” paradigm in the new era of predictive analytics},
journal = {Computer Law & Security Review},
volume = {30},
number = {6},
pages = {643-660},
year = {2014},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2014.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S026736491400154X},
author = {Alessandro Mantelero},
keywords = {Data protection, Consent, Data protection impact assessment, Big Data, Data protection authorities},
abstract = {The new E.U. proposal for a general data protection regulation has been introduced to give an answer to the challenges of the evolving digital environment. In some cases, these expectations could be disappointed, since the proposal is still based on the traditional main pillars of the last generation of data protection laws. In the field of consumer data protection, these pillars are the purpose specification principle, the use limitation principle and the “notice and consent” model. Nevertheless, the complexity of data processing, the power of modern analytics and the “transformative” use of personal information drastically limit the awareness of consumers, their capability to evaluate the various consequences of their choices and to give a free and informed consent. To respond to the above, it is necessary to clarify the rationale of the “notice and consent” paradigm, looking back to its origins and assessing its effectiveness in a world of predictive analytics. From this perspective, the paper considers the historical evolution of data protection and how the fundamental issues coming from the technological and socio-economic contexts have been addressed by regulations. On the basis of this analysis, the author suggests a revision of the “notice and consent” model focused on the opt-in and proposes the adoption of a different approach when, such as in Big Data collection, the data subject cannot be totally aware of the tools of analysis and their potential output. For this reason, the author sustains the provision of a subset of rules for Big Data analytics, which is based on a multiple impact assessment of data processing, on a deeper level of control by data protection authorities, and on the different opt-out model.}
}
@article{SIMMONS2022103318,
title = {Freedom from what? Separating lay concepts of freedom},
journal = {Consciousness and Cognition},
volume = {101},
pages = {103318},
year = {2022},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2022.103318},
url = {https://www.sciencedirect.com/science/article/pii/S1053810022000502},
author = {Claire Simmons and Paul Rehren and John-Dylan Haynes and Walter Sinnott-Armstrong},
abstract = {Debates about freedom of will and action and their connections with moral responsibility have raged for centuries, but the opposing sides might disagree because they use different concepts of freedom. Based on previous work, we hypothesized that people who assert freedom in a determined (D) or counterfactual-intervener (CI) scenario assert this because they are thinking about freedom from constraint and not about freedom from determination (in D) or from inevitability (in CI). We also hypothesized that people who deny that freedom in D or in CI deny this because they are thinking about freedom from determination or from inevitability, respectively, and not about freedom from constraint. To test our hypotheses, we conducted two main online studies. Study I supported our hypotheses that people who deny freedom in D and CI are thinking about freedom from determinism and from inevitability, respectively, but these participants seemed to think about freedom from constraint when they were later considering modified scenarios where acts were not determined or inevitable. Study II investigated a contrary bypassing hypothesis that those who deny freedom in D denied this because they took determinism to exclude mental causation and hence to exclude freedom from constraint. We found that participants who took determinism to exclude freedom generally did not deny causation by mental states, here represented by desires and decisions. Their responses regarding causation by desires and decisions at most weakly mediated the relation between determinism and freedom or responsibility among this subgroup of our participants. These results speak against the bypassing hypothesis and in favor of our hypothesis that these participants were not thinking about freedom from constraint.}
}
@article{LIEFGREEN2020101332,
title = {Strategies for selecting and evaluating information},
journal = {Cognitive Psychology},
volume = {123},
pages = {101332},
year = {2020},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2020.101332},
url = {https://www.sciencedirect.com/science/article/pii/S001002852030061X},
author = {Alice Liefgreen and Toby Pilditch and David Lagnado},
keywords = {Information search, OED framework, Utility functions, Inquiry, Question asking, Strategies, Probabilistic reasoning, Bayesian Networks},
abstract = {Within the domain of psychology, Optimal Experimental Design (OED) principles have been used to model how people seek and evaluate information. Despite proving valuable as computational-level methods to account for people’s behaviour, their descriptive and explanatory powers remain largely unexplored. In a series of experiments, we used a naturalistic crime investigation scenario to examine how people evaluate queries, as well as outcomes, in probabilistic contexts. We aimed to uncover the psychological strategies that people use, not just to assess whether they deviated from OED principles. In addition, we explored the adaptiveness of the identified strategies across both one-shot and stepwise information search tasks. We found that people do not always evaluate queries strictly in OED terms and use distinct strategies, such as by identifying a leading contender at the outset. Moreover, we identified aspects of zero-sum thinking and risk aversion that interact with people’s information search strategies. Our findings have implications for building a descriptive account of information seeking and evaluation, accounting for factors that currently lie outside the realm of information-theoretic OED measures, such as context and the learner’s own preferences.}
}
@article{MARTINEZLEDESMA20203567,
title = {Computational methods for detecting cancer hotspots},
journal = {Computational and Structural Biotechnology Journal},
volume = {18},
pages = {3567-3576},
year = {2020},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2020.11.020},
url = {https://www.sciencedirect.com/science/article/pii/S2001037020304876},
author = {Emmanuel Martinez-Ledesma and David Flores and Victor Trevino},
keywords = {Mutations, Cancer, Hotspots, Recurrent mutations, Algorithms, Genomics, Sequencing, Exome, Whole genome sequencing},
abstract = {Cancer mutations that are recurrently observed among patients are known as hotspots. Hotspots are highly relevant because they are, presumably, likely functional. Known hotspots in BRAF, PIK3CA, TP53, KRAS, IDH1 support this idea. However, hundreds of hotspots have never been validated experimentally. The detection of hotspots nevertheless is challenging because background mutations obscure their statistical and computational identification. Although several algorithms have been applied to identify hotspots, they have not been reviewed before. Thus, in this mini-review, we summarize more than 40 computational methods applied to detect cancer hotspots in coding and non-coding DNA. We first organize the methods in cluster-based, 3D, position-specific, and miscellaneous to provide a general overview. Then, we describe their embed procedures, implementations, variations, and differences. Finally, we discuss some advantages, provide some ideas for future developments, and mention opportunities such as application to viral integrations, translocations, and epigenetics.}
}
@incollection{GEYER2020125,
title = {Chapter 6 - Physical meets digital: Blending reality and computational power with digital sticky notes},
editor = {Bo T. Christensen and Kim Halskov and Clemens N. Klokmose},
booktitle = {Sticky Creativity},
publisher = {Academic Press},
pages = {125-151},
year = {2020},
series = {Explorations in Creativity Research},
isbn = {978-0-12-816566-9},
doi = {https://doi.org/10.1016/B978-0-12-816566-9.00006-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128165669000069},
author = {Florian Geyer and Johannes Zagermann and Harald Reiterer},
keywords = {Affinity diagramming, Blended interaction, Post-WIMP user interface, Interaction design, Tangible user interface, User interface design framework, Creativity tool},
abstract = {The high utility and usability of paper sticky notes support workflows and social dynamics of collaborative design activities and methods like affinity diagramming. In this chapter, we show how these natural collaboration activities can be blended with computational power by applying our framework Blended Interaction for a case study on affinity diagramming. Based on four domains of design, we embed our design solutions in a specific physical environment, preserve workflows, and emphasize individual and social interaction. Our proposed design solution to augment sticky notes with digital power blends the benefits of physical materials with the digital power of interactive surfaces, tangibles, and digital pens in an outstanding way. We hope that our design solutions inspire other researchers and practitioners to find innovative solutions that carefully blend real-world practices with the power of digital computing.}
}
@article{BULLOCK2009757,
title = {Computational perspectives on forebrain microcircuits implicated in reinforcement learning, action selection, and cognitive control},
journal = {Neural Networks},
volume = {22},
number = {5},
pages = {757-765},
year = {2009},
note = {Advances in Neural Networks Research: IJCNN2009},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2009.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608009001117},
author = {Daniel Bullock and Can Ozan Tan and Yohan J. John},
keywords = {Basal ganglia, Acetylcholine, Dopamine, Striatum, Decision making},
abstract = {Abundant new information about signaling pathways in forebrain microcircuits presents many challenges, and opportunities for discovery, to computational neuroscientists who strive to bridge from microcircuits to flexible cognition and action. Accurate treatment of microcircuit pathways is especially critical for creating models that correctly predict the outcomes of candidate neurological therapies. Recent models are trying to specify how cortical circuits that enable planning and voluntary actions interact with adaptive subcortical microcircuits in the basal ganglia. The basal ganglia are strongly implicated in reinforcement learning, and in all behavior and cognition over which the frontal lobes exert flexible control. The persisting role of the basal ganglia shows that ancient vertebrate designs for motivated action selection proved adaptable enough to support many “modern” behavioral innovations, including fluent generation of language and speech. This paper summarizes how recent models have incorporated realistic representations of microcircuit features, and have begun to trace their computational implications. Also summarized are recent empirical discoveries that provide guidance regarding how to formulate the rules for synaptic modification that govern learning in cortico-striatal pathways. Such efforts are contributing to an emerging synthesis based on an interlocking set of computational hypotheses regarding cortical interactions with basal ganglia and thalamic nuclei. These hypotheses specify how specialized microcircuits solve learning and control problems inherent to the brain’s parallel design.}
}
@article{BROM20121,
title = {A computational model of the allocentric and egocentric spatial memory by means of virtual agents, or how simple virtual agents can help to build complex computational models},
journal = {Cognitive Systems Research},
volume = {17-18},
pages = {1-24},
year = {2012},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2011.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041711000404},
author = {Cyril Brom and Jan Vyhnánek and Jiří Lukavský and David Waller and Rudolf Kadlec},
keywords = {Spatial cognition, Paradigm of pointing, Disorientation effect, Intelligent virtual agent},
abstract = {The ability to acquire, remember and use information about locations of objects in one’s proximal surrounding is a fundamental aspect of human spatial cognition. In this paper, we present a computational model of this ability. The model provides a possible explanation of contradictory results from experimental psychology related to this ability, namely explanation of why some experiments have reproduced the so-called “disorientation effect” while others have failed to do so. Additionally, in contrast to other computational models of various aspects of spatial cognition, our model is integrated within an intelligent virtual agent. Thus, on a more general level, this paper also demonstrates that it is possible to use intelligent virtual agents as a powerful research tool in computational cognitive sciences.}
}
@article{NUGRAHA2023406,
title = {A SEM-neural network approach for understanding the entrepreneurial competence development of freshmen engineering and computing students},
journal = {Procedia Computer Science},
volume = {216},
pages = {406-414},
year = {2023},
note = {7th International Conference on Computer Science and Computational Intelligence 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.152},
url = {https://www.sciencedirect.com/science/article/pii/S187705092202230X},
author = {Rendika Nugraha and Nanang Ali Sutisna and Adhi Setyo Santoso and Ihsan Hadiansah and Johan Krisnanto Runtuk},
keywords = {Entrepreneurial competence, techno-entrepreneurship, creativity, ethical, sustainable thinking, motivation, perseverance, mobilizing others, learning through experience taking initiative, cope with uncertainty},
abstract = {The discussion of enhancing entrepreneurial competence in Higher Education Institution (HEI), especially in engineering and computing major, has increased for the recent years. This study aims to propose and test a structural model of relationship of Indonesian HEI entrepreneurship education with entrepreneurship competence to assess student entrepreneurship competences especially in undergraduate level. Thus, this study provides the contribution in this stream by creating a subject specialized that fit with specific study program to enhance entrepreneurial competence for freshmen student called Integrative Survival Experience especially in engineering and computing major. We measure its output by using EntreComp questionnaires framework from European Commission. A combination of Structural Equation Modelling (SEM) and neural network was implemented as analytic approach in this study. The results show that the freshmen engineering and computing students develop entrepreneurial competence by enhancing the specific sets of ideas and opportunities as well as the capability to manage resources for taking the action afterwards. Apparently, the entrepreneurial competence development process of engineering and computing students differs with that of business and management students.}
}
@article{LIN2021103944,
title = {Lessons learned from critical accidental fires in tunnels},
journal = {Tunnelling and Underground Space Technology},
volume = {113},
pages = {103944},
year = {2021},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2021.103944},
url = {https://www.sciencedirect.com/science/article/pii/S0886779821001358},
author = {Chien Liang Lin and Chao Fu Chien},
keywords = {Systems thinking, Lessons learned, Accidental tunnel fires, Causal loop diagram},
abstract = {Historical data indicate that tunnel fires often cause casualties and damage to both vehicles and tunnels. These severe consequences suggest that (1) humans seldom effectively learn from history, and (2) people lack optimal safety response strategies for tunnel fires. To investigate the root causes of accidental tunnel fires and learn from them, we first surveyed the literature on historical tunnel accidents and described the common timeline of accidental tunnel fires. We employed systems thinking, based on the past research, to depict a causal loop diagram of common accidental tunnel fires. We arrived at the following three findings: (1) the literature review proved that the causes of tunnel fires are far more complex than other types of fires, and the damage they generate is greater; (2) in the context of systems thinking, accidental tunnel fires involve many causal relationships which are both continuous and dynamic, including at least three systems, namely vehicles, tunnel control, and safety response; (3) the mental models “the experience of the operators at the tunnel operation control center is just as vital as the safety response” and “safety is more critical than the traffic volume in the tunnel”, can strengthen safety response systems and ensure safe driving in tunnels. Although the structure of each tunnel and the characteristics of each fire differ and present different causal relationships, this study elucidated lessons from accidental tunnel fires and provided required messages for establishing effective safety measures. The results of this study can be used to establish systems thinking models of tunnel fires and can serve as a reference for policy planning and establishing standard operating procedures for safety responses.}
}
@article{LIU2024108212,
title = {Analysis of translation teaching skills in colleges and universities based on deep learning},
journal = {Computers in Human Behavior},
volume = {157},
pages = {108212},
year = {2024},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2024.108212},
url = {https://www.sciencedirect.com/science/article/pii/S0747563224000803},
author = {Yan Liu and Shuhua Li and Dan Cui},
keywords = {Deep learning, Colleges and universities, Translation education, Machine learning applications, Teaching strategy},
abstract = {With the progress of the times and the improvement of science and technique, network message technique has occupied a vital position in people's lives. At the same time, society has been implementing university English education reform in recent years, and the “internet plus” wisdom education model is the product of the improvement of the times. This new education model has gradually integrated into the education of various subjects. Introducing the concept of message technique and wisdom education into university translation education can innovate education mode, optimize education content, and integrate high-quality education resources. Cultivating applied translators has become the trend of educational reform. Based on deep learning, this paper studies translation education skills in universities. In-depth education enables learners to acquire systematic knowledge, critical spirit, creative thinking, etc. This kind of learning fully taps individual potential to cultivate a complete personality. According to the research in this paper, wisdom education is 12% better than traditional education, and it is suitable to be widely put into practice.}
}
@article{BAN2020102789,
title = {3D Computational Sketch Synthesis Framework: Assisting Design Exploration Through Generating Variations of User Input Sketch and Interactive 3D Model Reconstruction},
journal = {Computer-Aided Design},
volume = {120},
pages = {102789},
year = {2020},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2019.102789},
url = {https://www.sciencedirect.com/science/article/pii/S0010448518301726},
author = {Seonghoon Ban and Kyung Hoon Hyun},
keywords = {Intelligent design system, Assisted creativity, Sketch-based modeling, Computational design, Virtual reality},
abstract = {A framework is proposed for facilitating the exploration process during the early design phase through computational sketch synthesis and interactive 3D reconstruction. In that phase, designers concentrate on developing concepts through numerous alternatives. Therefore, they constantly sketch so that they can rapidly visualize their ideas. Recently, the design industry has attempted to streamline the design process by implementing 3D model generation in the early design phase so that ideas may be more thoroughly explored, thus improving concept and final design conformance; however, efficiency issues have arisen. In this study, a 3D computational sketch synthesis framework was developed comprising two major components. First, a robust method was proposed to synthesize design alternatives by interpolating an input sketch with sketches in a database so that unvisited combinations may be explored. Secondly, a novel interactive 3D model reconstruction method was developed to facilitate the shape transition of design elements so that designers can quickly evaluate the potential of a large number of design variations. Finally, an interface for design refinement was developed so that designs may be embodied by sketching over the 3D model. To test the proposed methodology, expert designers were recruited for a validation experiment with two conditions followed up by in-depth interviews. In the first condition, the participants were asked to sketch based on a design brief in their current working manner. In the second condition, they were asked to create designs using the proposed framework. It was tested whether there was a difference in the design outcomes. It was demonstrated that the proposed framework resulted in more satisfactory and higher-quality designs and generated design alternatives faster and in greater quantities. All participants agreed that the framework could be useful in the early design phase and responded that the proposed system provides more design inspiration than traditional design methods. Most importantly, it was demonstrated that the proposed framework could enhance the reevaluation potential of design concepts and assist in making better-informed design decisions.}
}
@article{LIU20183231,
title = {Nickel catalyzed regio- and stereoselective arylation and methylation of allenamides via coupling reactions. An experimental and computational study11Electronic supplementary information (ESI) available. CCDC 1548725 and 1817608. For ESI and crystallographic data in CIF or other electronic format see DOI: 10.1039/c8qo00729b},
journal = {Organic Chemistry Frontiers},
volume = {5},
number = {22},
pages = {3231-3239},
year = {2018},
issn = {2052-4110},
doi = {https://doi.org/10.1039/c8qo00729b},
url = {https://www.sciencedirect.com/science/article/pii/S2052411022005156},
author = {Yang Liu and Alessandro Cerveri and Assunta {De Nisi} and Magda Monari and Olalla {Nieto Faza} and Carlos Silva Lopez and Marco Bandini},
abstract = {The nickel catalyzed regio- and stereoselective condensation of boronic acids to allenamides is documented as a novel synthetic route to stereochemically defined tri-substituted enamides. The protocol has been implemented into a three-component variant intercepting the in situ formed allyl-Ni intermediate with a range of aldehydes. Additionally, evidence for the effective extension of this methodology to Me2Zn is documented. Full rationale on the mechanism as well as its stereochemical outcome is provided by a synergistic experimental/computational approach.}
}
@article{STEPHENS2021100871,
title = {From “You have to have three numbers and a plus sign” to “It’s the exact same thing”: K–1 students learn to think relationally about equations},
journal = {The Journal of Mathematical Behavior},
volume = {62},
pages = {100871},
year = {2021},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2021.100871},
url = {https://www.sciencedirect.com/science/article/pii/S0732312321000328},
author = {Ana Stephens and Ranza {Veltri Torres} and Yewon Sung and Susanne Strachota and Angela {Murphy Gardiner} and Maria Blanton and Rena Stroud and Eric Knuth},
keywords = {Equal sign, Equations, Elementary grades, Early algebra, Algebraic thinking},
abstract = {This research shares progressions in thinking about equations and the equal sign observed in ten students who took part in an early algebra classroom intervention across Kindergarten and first grade. We report on data from task-based interviews conducted prior to the intervention and at the conclusion of each school year that elicited students’ interpretations of the equal sign and equations of various forms. We found at the beginning of the intervention that most students viewed the equal sign as an operational symbol and did not accept many equations forms as valid. By the end of first grade, almost all students described the symbol as indicating the equivalence of two amounts and were much more successful interpreting and working with equations in a variety of forms. The progressions we observed align with those of other researchers and provide evidence that very young students can learn to reason flexibly about equations.}
}
@article{WANG2024119888,
title = {Progressive reinforcement learning for video summarization},
journal = {Information Sciences},
volume = {655},
pages = {119888},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.119888},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523014731},
author = {Guolong Wang and Xun Wu and Junchi Yan},
keywords = {Video summarization, Progressive reinforcement learning, Hierarchical strategy},
abstract = {Video summarization addresses generating video summaries to help watchers grasp the content of a video without watching it entirely. Many methods have engaged in automatic video summarization. Although these methods have performed well, they still suffer from limited training data and sparse reward problems. We propose a Progressive Reinforcement Learning Video Summarization structure (PRLVS) with an unsupervised reward. The reward measures the information and quality the selected frames convey without annotations. Striving to earn higher rewards, our PRLVS adopts a “T”-type human thinking paradigm: choosing some key frames and checking if their adjacent frames are better than them. To simulate this paradigm, we decompose the flat strategy into a hierarchical strategy consisting of a horizontal policy and a vertical policy. These two policies are optimized alternatively, which densifies the reward while reducing the exploration space. Their cooperation also makes the agent capture the context information of the whole video at every step. Extensive experimental results on two benchmark databases (i.e., SumMe, TVSum) show that our PRLVS outperforms the comparisons and approaches the supervised methods, which indicates that it is significant to integrate our unsupervised reward into the progressive reinforcement learning structure to address limited annotation and sparse reward problems.}
}
@article{ROLLS2024e31965,
title = {The memory systems of the human brain and generative artificial intelligence},
journal = {Heliyon},
volume = {10},
number = {11},
pages = {e31965},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e31965},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024079969},
author = {Edmund T. Rolls},
keywords = {The brain and AI, Generative Pre-trained Transformer, Generative artificial intelligence, Episodic memory, Semantic memory, Hippocampal memory system, Chat-GPT},
abstract = {Generative Artificial Intelligence foundation models (for example Generative Pre-trained Transformer – GPT – models) can generate the next token given a sequence of tokens. How can this ‘generative AI’ be compared with the ‘real’ intelligence of the human brain, when for example a human generates a whole memory in response to an incomplete retrieval cue, and then generates further prospective thoughts? Here these two types of generative intelligence, artificial in machines and real in the human brain are compared, and it is shown how when whole memories are generated by hippocampal recall in response to an incomplete retrieval cue, what the human brain computes, and how it computes it, are very different from generative AI. Key differences are the use of local associative learning rules in the hippocampal memory system, and of non-local backpropagation of error learning in AI. Indeed, it is argued that the whole operation of the human brain is performed computationally very differently to what is implemented in generative AI. Moreover, it is emphasized that the primate including human hippocampal system includes computations about spatial view and where objects and people are in scenes, whereas in rodents the emphasis is on place cells and path integration by movements between places. This comparison with generative memory and processing in the human brain has interesting implications for the further development of generative AI and for neuroscience research.}
}
@article{YEH2011794,
title = {Evaluation approach to stock trading system using evolutionary computation},
journal = {Expert Systems with Applications},
volume = {38},
number = {1},
pages = {794-803},
year = {2011},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2010.07.035},
url = {https://www.sciencedirect.com/science/article/pii/S0957417410006639},
author = {I-Cheng Yeh and Che-hui Lien and Yi-Chen Tsai},
keywords = {Genetic algorithms, Neural networks, Decision system, Stock market, Over-learning},
abstract = {The past researches emphasize merely the avoidance of over-learning at the system level and ignore the problem of over-learning at the model level, which lead to the poor performance of the evolutionary computation based stock trading decision-making system. This study presents a new evaluation approach to focus on evaluating the generalization capability at the model level. An empirical study was provided and the results reveal four important findings. First, the decision-making system generated at the model design stage outperforms the system generated at the model validation stage, which shows over-learning at the model level. Secondly, for the decision-making system generated either at the model design stage or at the model validation stage, the investment performance in the training period is much better than that in the testing period, exhibiting over-learning at the system level. Third, employing moving timeframe approach is unable to improve the investment performance at the model validation stage. Fourth, reducing the evolution generation and input variables are unable to avoid the over-learning at the model level. The major contribution of this study is to clarify the issue of over-learning at the model and the system level. For future research, this study developed a more reliable evaluation approach in examining the generalization capability of evolutionary computation based decision-making system.}
}
@incollection{WARE20221,
title = {Chapter 1 - Visual Queries},
editor = {Colin Ware},
booktitle = {Visual Thinking for Information Design (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
pages = {1-22},
year = {2022},
isbn = {978-0-12-823567-6},
doi = {https://doi.org/10.1016/B978-0-12-823567-6.00001-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012823567600001X},
author = {Colin Ware},
keywords = {Visual queries, visual search, distributed cognition, predictive cognition, visual system, visual thinking},
abstract = {The mechanisms and processes of visual thinking are introduced together with how this knowledge can help us make design decisions. We begin with a review of the evidence that we actually take in very little information with each glance and the implication that seeing is a process exquisitely tuned to our cognitive task of the moment. As a key part of this process, our brains execute visual queries using eye movements; visual features are detected in parallel to pick out just what is needed to resolve part of a cognitive problem and move on the next step. We begin to understand how seeing can be a distributed cognitive process executed partly in the brain and partly using a visualization as a tool. In particular, when the visualization is part of an interactive computer application, it provides the primary interface between cognitive operations in the human brain and computational operations. The theory of predictive cognition is introduced as a basis for how we should design presentations.}
}
@article{GIROTTO1991111,
title = {Event controllability in counterfactual thinking},
journal = {Acta Psychologica},
volume = {78},
number = {1},
pages = {111-133},
year = {1991},
issn = {0001-6918},
doi = {https://doi.org/10.1016/0001-6918(91)90007-M},
url = {https://www.sciencedirect.com/science/article/pii/000169189190007M},
author = {Vittorio Girotto and Paolo Legrenzi and Antonio Rizzo},
abstract = {The counterfactual assessment of events, i.e. is the mental construction of alternatives to factual events, is a pervasive mental process that is quite natural for people. For example, people easily make counterfactual statements when reflecting on dramatic events (‘If only I hadn't drunk alcohol the night of the car accident…’). The way in which people select the events to mutate when requested to undo a scenario outcome seems to be governed by general rules. One is that subjects tend to select exceptional (i.e. unusual or surprising) rather than normal events (Kahneman and Tversky 1982a,b; Kahneman and Miller 1986). Another is that subjects prefer to select the first rather than the subsequent events in a causal chain (Wells, Taylor and Turtle 1987). We hypothesized that events corresponding to controllable actions (i.e. voluntary decisions) by the protagonist of a scenario are more mentally mutable than events which occur in the surrounding background. In experiment 1, we manipulated the order and the controllability of four events in a scenario. Contrary to the causal order effect hypothesis, subjects preferred to change the event corresponding to a coluntary decision of the scenario actor, regardless of its relative position in the scenario. Experiment 2 showed that subjects made this choice regardless of the normal vs. exceptional status of the voluntary action event. Experiment 3 gave evidence that an unconstrained action performed by the focal actor of a story is more mutable than a constrained action performed by the same actor. The implications of these findings for the analysis of accidents involving human errors are discussed.}
}
@article{EGBERT2021104173,
title = {“It's a chance to make mistakes”: Processes and outcomes of coding in 2nd grade classrooms},
journal = {Computers & Education},
volume = {168},
pages = {104173},
year = {2021},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2021.104173},
url = {https://www.sciencedirect.com/science/article/pii/S0360131521000506},
author = {Joy Egbert and Seyed Abdollah Shahrokni and Reima Abobaker and Nataliia Borysenko},
keywords = {Elementary education, Robotics, Coding, Teacher learning, Computational thinking},
abstract = {Several gaps exist in the literature on coding. First, little exploration has focused on early elementary school students. In addition, close description of the overall context of coding tasks at this level is rare. Further, there is a need for both teacher and student voices around coding experiences to be heard. Moreover, a task engagement framework has not been used to evaluate the process or outcomes of early elementary coding tasks. Therefore, an exploratory holistic case study design was used to investigate student and teacher processes and outcomes of coding lessons in order to fill gaps in the literature. In this study, forty-six 2nd grade students, two teachers, and four researchers completed two one-week units on basic coding. Multiple descriptive and numeric data sources were employed to describe the process and outcomes of learning coding. Conclusions include: (1) teachers should start learning about coding first with short awareness sessions and then move to their own classrooms with knowledge brokers and other forms of assistance; (2) a focus on content and process, including problem-solving, is effective for coding with young children; (3) there can be a high level of engagement for teachers and students with the use of robots and welldesigned, age-appropriate coding tasks, and; (4) multiple data sources and the inclusion of both teacher and student data are essential in exploring coding in classrooms.}
}
@article{GUHE2011249,
title = {A computational account of conceptual blending in basic mathematics},
journal = {Cognitive Systems Research},
volume = {12},
number = {3},
pages = {249-265},
year = {2011},
note = {Special Issue on Complex Cognition},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2011.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S1389041711000155},
author = {Markus Guhe and Alison Pease and Alan Smaill and Maricarmen Martinez and Martin Schmidt and Helmar Gust and Kai-Uwe Kühnberger and Ulf Krumnack},
keywords = {Mathematical cognition, Metaphor, Mathematical reasoning, Analogy, Anti-unification, Conceptual blending, HDTP},
abstract = {We present an account of a process by which different conceptualisations of number can be blended together to form new conceptualisations via recognition of common features, and judicious combination of their distinctive features. The accounts of number are based on Lakoff and Núñez’s cognitively-based grounding metaphors for arithmetic. The approach incorporates elements of analogical inference into a generalised framework of conceptual blending, using some ideas from the work of Goguen. The ideas are worked out using Heuristic-Driven Theory Projection (HDTP, a method based on higher-order anti-unification). HDTP provides generalisations between domains, giving a crucial step in the process of finding commonalities between theories. In addition to generalisations, HDTP can also transfer concepts from one domain to another, allowing the construction of new conceptual blends. Alongside the methods by which conceptual blends may be constructed, we provide heuristics to guide this process.}
}
@article{ISLAM2021104757,
title = {EEG Channel Correlation Based Model for Emotion Recognition},
journal = {Computers in Biology and Medicine},
volume = {136},
pages = {104757},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.104757},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521005515},
author = {Md. Rabiul Islam and Md. Milon Islam and Md. Mustafizur Rahman and Chayan Mondal and Suvojit Kumar Singha and Mohiuddin Ahmad and Abdul Awal and Md. Saiful Islam and Mohammad Ali Moni},
keywords = {Emotion, Convolutional neural network, Feature extraction, EEG, Pearson's correlation coefficient, Complexity},
abstract = {Emotion recognition using Artificial Intelligence (AI) is a fundamental prerequisite to improve Human-Computer Interaction (HCI). Recognizing emotion from Electroencephalogram (EEG) has been globally accepted in many applications such as intelligent thinking, decision-making, social communication, feeling detection, affective computing, etc. Nevertheless, due to having too low amplitude variation related to time on EEG signal, the proper recognition of emotion from this signal has become too challenging. Usually, considerable effort is required to identify the proper feature or feature set for an effective feature-based emotion recognition system. To extenuate the manual human effort of feature extraction, we proposed a deep machine-learning-based model with Convolutional Neural Network (CNN). At first, the one-dimensional EEG data were converted to Pearson's Correlation Coefficient (PCC) featured images of channel correlation of EEG sub-bands. Then the images were fed into the CNN model to recognize emotion. Two protocols were conducted, namely, protocol-1 to identify two levels and protocol-2 to recognize three levels of valence and arousal that demonstrate emotion. We investigated that only the upper triangular portion of the PCC featured images reduced the computational complexity and size of memory without hampering the model accuracy. The maximum accuracy of 78.22% on valence and 74.92% on arousal were obtained using the internationally authorized DEAP dataset.}
}
@article{ASHTIANI201618,
title = {A hesitant fuzzy model of computational trust considering hesitancy, vagueness and uncertainty},
journal = {Applied Soft Computing},
volume = {42},
pages = {18-37},
year = {2016},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2016.01.023},
url = {https://www.sciencedirect.com/science/article/pii/S1568494616300102},
author = {Mehrdad Ashtiani and Mohammad Abdollahi Azgomi},
keywords = {Trust modeling, Hesitant fuzzy sets (HFS), Comparative linguistic expressions, Vagueness, Uncertainty, Multi-criteria decision making (MCDM)},
abstract = {The aim of this work is to introduce a trust model, which is highly consistent with the social nature of trust in computational domains. To this end, we propose a hesitant fuzzy multi-criteria decision making based computational trust model capable of taking into account the fundamental building blocks corresponding to the concept of trust. The proposed model is capable of considering the contextuality property of trust and the subjective priorities of the trustor regarding the chosen goal. This is due to viewing trust not as a single label or an integrated concept, but as a collection of trustworthiness facets that may form the trust decision in various contexts and toward different goals. The main benefit of the proposed model is the consideration of the hesitancy of recommenders and the trustor in the process of trust decision making which can create a more flexible mapping between the social and computational requirements of trust. This type of formulation also allows for taking into account the vagueness of the provided opinions. In addition to the vagueness of the provided opinions, the model is capable of considering the certainty of recommendations and its effect on the aggregation process of gathered opinions. In the proposed model, the taste of the recommenders and the similarity of opinions are also considered. This will allow the model to assign more weight to recommendations that have a similar taste compared to the trustor. Finally, taking into consideration the attitudes of the trustors toward change of personality that may occur for various entities in the environment is another advantage of the proposed model. A step-by-step illustrative example and the results of several experimental evaluations, which demonstrate the benefits of the proposed model, are also presented in this paper.}
}
@article{KINLEY2022105843,
title = {Pathologies of precision: A Bayesian account of goals, habits, and episodic foresight in addiction},
journal = {Brain and Cognition},
volume = {158},
pages = {105843},
year = {2022},
issn = {0278-2626},
doi = {https://doi.org/10.1016/j.bandc.2022.105843},
url = {https://www.sciencedirect.com/science/article/pii/S027826262200001X},
author = {Isaac Kinley and Michael Amlung and Suzanna Becker},
keywords = {Addiction, Goals, Habits, Free energy, Bayesian statistics, Episodic future thinking},
abstract = {The brain is thought to implement two decision-making systems: a goal-directed system in which decisions are made through planning on the basis of action–outcome relationships, and a habitual system in which behaviour reflects stimulus–response associations. A prominent theory of addiction sees it as arising due to an extreme dominance of habit over goal-directed action. The balance between these systems is thought to be arbitrated by the relative precision of their separate predictions of reward. In this paper, we argue that various factors in addiction create hyper-precise reward predictions in the habitual system and hypo-precise reward predictions in the goal-directed system, shifting the balance of behavioural control in favour of habit. Based on this, we offer a theoretical account of the utility of episodic future thinking in addiction, interpreting it as increasing the precision of reward estimates in the goal-directed system, thereby enhancing the control of this system over behaviour.}
}
@article{CARLEY2002253,
title = {Computational organizational science and organizational engineering},
journal = {Simulation Modelling Practice and Theory},
volume = {10},
number = {5},
pages = {253-269},
year = {2002},
note = {Organisational Processes},
issn = {1569-190X},
doi = {https://doi.org/10.1016/S1569-190X(02)00119-3},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X02001193},
author = {Kathleen M Carley},
keywords = {Computational modeling, Simulation, Organization science, Organizational design, Organizational learning},
abstract = {The past decade has witnessed the emergence of a new scientific discipline––computational social and organizational science. Within organization science in particular, and social science more generally, scientists and practitioners are turning to computational analysis to address fundamental socio-technical problems that are so complex and dynamic that they cannot be fully addressed by traditional techniques. Consequently, there is an explosion of computational models, computationally generated findings, interest in doing simulation, and a dearth of support for this enterprise. This paper contains discussions of the underlying fundamental perspective, the relation of models to empirical data and characteristics of necessary infrastructure.}
}
@article{ZHAO2015194,
title = {Bring CS2013 Recommendations into c Programming Course},
journal = {Procedia - Social and Behavioral Sciences},
volume = {176},
pages = {194-199},
year = {2015},
note = {International Educational Technology Conference, IETC 2014, 3-5 September 2014, Chicago, IL, USA},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.01.461},
url = {https://www.sciencedirect.com/science/article/pii/S187704281500498X},
author = {Lingling Zhao and Xiaohong Su and Tiantian Wang},
keywords = {CS2013, C programming course, CS curriculum planning, CS major ;},
abstract = {Computer Science Curriculum 2013 has become the guidance of computing education since it was released in 2013by the ACM/IEEE-Computer Society. This paper analyzes the CS curriculum development trend, trying to dig the programming-related core from CS2013 with respect to the knowledge areas, topics, organization of teaching, and the building of students’ capability. Considering the characteristic of our local institution and undergraduates, we present an updated teaching curriculum and lab curriculum for C Programming Language course in relation to CS2013 recommendations, which highlight the development of the students’ abilities on programming, problem-solving, self-regulated learning, and computational thinking. Finally, we present and assess the implementation of the resulting curriculum.}
}
@article{KLEINSCHMIDT2004842,
title = {Thinking Big: Many Modules or Much Cortex?},
journal = {Neuron},
volume = {41},
number = {6},
pages = {842-844},
year = {2004},
issn = {0896-6273},
doi = {https://doi.org/10.1016/S0896-6273(04)00154-0},
url = {https://www.sciencedirect.com/science/article/pii/S0896627304001540},
author = {Andreas Kleinschmidt},
abstract = {Is there a neural system dedicated to generic magnitude judgments? In this issue of Neuron, Pinel et al. report qualitative spatial overlap of fMRI responses during judgments of luminance, size, and numerical magnitude but also quantitative response differences in intraparietal cortex that mirror behavioral interference between perceptual and symbolic magnitude.}
}
@article{HUMPHREYS1991315,
title = {Vol. 3: Thinking: edited by Daniel N. Osherson and Edward E. Smith (x + 308 pages) ISBN 0 262 65035 5},
journal = {Trends in Neurosciences},
volume = {14},
number = {7},
pages = {315},
year = {1991},
issn = {0166-2236},
doi = {https://doi.org/10.1016/0166-2236(91)90148-N},
url = {https://www.sciencedirect.com/science/article/pii/016622369190148N},
author = {Glyn W. Humphreys}
}
@article{CROLLEN2020290,
title = {How visual is the « number sense »? Insights from the blind},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {118},
pages = {290-297},
year = {2020},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2020.07.022},
url = {https://www.sciencedirect.com/science/article/pii/S0149763420304851},
author = {Virginie Crollen and Olivier Collignon},
keywords = {Blindness, Mathematical cognition, Brain plasticity},
abstract = {Is vision a necessary building block for the foundations of mathematical cognition? A straightforward model to test the causal role visual experience plays in the development of numerical abilities is to study people born without sight. In this review we will demonstrate that congenitally blind people can develop numerical abilities that equal or even surpass those of sighted individuals, despite representing numbers using a qualitatively different representational format. We will also show that numerical thinking in blind people maps onto regions typically involved in visuo-spatial processing in the sighted, highlighting how intrinsic computational biases may constrain the reorganization of numerical networks in case of early visual deprivation. More generally, we will illustrate how the study of arithmetic abilities in congenitally blind people represents a compelling model to understand how sensory experience scaffolds the development of higher-level cognitive representations.}
}
@article{BATEMAN2021100502,
title = {What are digital media?},
journal = {Discourse, Context & Media},
volume = {41},
pages = {100502},
year = {2021},
issn = {2211-6958},
doi = {https://doi.org/10.1016/j.dcm.2021.100502},
url = {https://www.sciencedirect.com/science/article/pii/S2211695821000386},
author = {John A. Bateman},
keywords = {Digital media, Digital information, Literacy, Models of communication, Multimodality, Medium, Development of media, Computational media},
abstract = {This essay addresses the nature of so–called ‘digital media’ in a literacy context from the perspectives of semiotics, theories of the ‘medium’, and computation. It argues that most accounts that attempt to work with some notion of ‘digital media’ anchor themselves insufficiently in semiotics and computation and the essential combination of these that is necessary when discussing digital media as an object of study. This weakens approaches, particularly when the concern is to develop ways of teaching engagement with contemporary communication practices at any level, i.e., improving ‘digital literacies’ of various kinds. Achieving more robust foundations is important for interventions which are not only more effective but also sustainable, minimizing the danger of obsolescence with each new technological turn of the screw. Foundations are also essential for a more balanced perspective on learning situations that does not dichotomize allegedly ‘digital’ and ‘non–digital’ practices and skills. Many such boundaries are deeply misleading and so unnecessarily compartmentalize thinking and restrict the application of relevant research results and methods. The focus of this essay is therefore to consider how a closer examination of media and their development, combined with the contributions made by information technologies, may help articulate notions of digital media that are more supportive of productive engagements with research and issues of literacy.}
}
@incollection{ELNAKIB202159,
title = {3 - Computational methods for identifying left ventricle heart pathologies},
editor = {Ayman S. El-Baz and Jasjit S. Suri},
booktitle = {Diabetes and Cardiovascular Disease},
publisher = {Elsevier},
pages = {59-93},
year = {2021},
volume = {3},
series = {Computer-Assisted Diagnosis},
isbn = {978-0-12-817428-9},
doi = {https://doi.org/10.1016/B978-0-12-817428-9.00003-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128174289000036},
author = {Ahmed Elnakib and Mohammed Ghazal and Fatma Taher and Ali H. Mahmoud and Ayman El-Baz},
keywords = {Computational methods, Left ventricle, Heart, Pathologies, Cardiac MRI (CMRI), Segmentation},
abstract = {Globally, the cardiovascular diseases are the first cause of death. The early detection and quantification of these diseases can significantly reduce the mortality rate. Recent advances in cardiac MRI (CMRI) enable the detection of the left ventricle (LV) wall pathologies and the estimation of different quantification metrics that characterize the working of the heart. Examples of these metrics include the area of pathological tissue in the LV wall, the transmural extent of pathology, and other indexes such as wall thickening, functional strain, and the ejection fraction metrics. In the literature, several computational methods have been proposed in order to estimate these metrics based on using different CMRI acquisition techniques, such as cardiac-enhanced CMRI (CE-CMRI) and cine CMRI. This chapter overviews these computational methods and explains their basic ideas, focusing on the metrics extracted using CE-CMRI and cine CMRI.}
}
@article{MAHMOUDZAKIALI2022100579,
title = {The computation intelligent in teaching using digital communication},
journal = {Measurement: Sensors},
volume = {24},
pages = {100579},
year = {2022},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2022.100579},
url = {https://www.sciencedirect.com/science/article/pii/S2665917422002136},
author = {Hossam {Mahmoud Zaki Ali} and Mohammed Hasan Ali Al-Abyadh},
keywords = {Computation intelligent, Digital communication, Skills, Non-verbal communication skills, School health promotion, Teachers},
abstract = {Teaching is all about communication. Teachers who sharpen their communication skills are prepared to instruct, advise, and mentor their students. They communicate well to effectively collaborate within a healthy educational process. This research aims to Make sure the communication scale prepared for the current research has statistical validity to be applied in the current research, identify the teachers' verbal non-verbal communication skills level, to explore the differences in these variables due to gender. A sample of (376) elementary and preparatory stage teachers, Minia Governorate, Egypt (188 male, 188 female) was chosen. For data collection, the researchers utilized the verbal and non-verbal communication scale (prepared by the researchers). They were applied electronically during the 2020 academic year. The research was a descriptive research design. Results demonstrated The communication scale prepared for the current research has statistical validity to be applied in the current research, there was a high level of verbal and nonverbal communication skills among the research sample. Besides, there were no statistically significant differences between male and female teachers in the levels of verbal communication skills, non-verbal communication skills. Some recommendations regarding the necessity to specify courses for pre-service teachers on verbal communication skills, nonverbal communication skills, were presented. Also, suggestions for those in charge of the educational administration process to improve teachers and school health promotion were illustrated.}
}
@article{EYSENCK19921359,
title = {Thinking clearly about psychology volume 2: Personality and psychopathology: William M. Grove and Dante Cicchetti: Minneapolis: University of Minnesota Press (1991). pp. v–vi, 3–467. Cloth, ISBN 0-8166-1892-5 v. 2.$45.00.},
journal = {Personality and Individual Differences},
volume = {13},
number = {12},
pages = {1359-1360},
year = {1992},
issn = {0191-8869},
doi = {https://doi.org/10.1016/0191-8869(92)90185-R},
url = {https://www.sciencedirect.com/science/article/pii/019188699290185R},
author = {H.J. Eysenck}
}
@article{JUHOLA2021106367,
title = {On computational classification of genetic cardiac diseases applying iPSC cardiomyocytes},
journal = {Computer Methods and Programs in Biomedicine},
volume = {210},
pages = {106367},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2021.106367},
url = {https://www.sciencedirect.com/science/article/pii/S0169260721004417},
author = {Martti Juhola and Henry Joutsijoki and Kirsi Penttinen and Disheet Shah and Katriina Aalto-Setälä},
keywords = {Genetic cardiac diseases, Induced pluripotent stem cells, Cardiomyocytes, Transient profiles, Machine learning, Classification, Leave-one-out, -fold cross-validation},
abstract = {Background
Cardiomyocytes differentiated from human induced pluripotent stem cells (iPSC-CMs) can be used to study genetic cardiac diseases. In patients these diseases are manifested e.g. with impaired contractility and fatal cardiac arrhythmias, and both of these can be due to abnormal calcium transients in cardiomyocytes. Here we classify different genetic cardiac diseases using Ca2+ transient data and different machine learning algorithms.
Methods
By studying calcium cycling of disease-specific iPSC-CMs and by using calcium transients measured from these cells it is possible to classify diseases from each other and also from healthy controls by applying machine learning computation on the basis of peak attributes detected from calcium transient signals.
Results
In the current research we extend our previous study having Ca-transient data from four different genetic diseases by adding data from two additional diseases (dilated cardiomyopathy and long QT Syndrome 2). We also study, in the light of the current data, possible differences and relations when machine learning modelling and classification accuracies were computed by using either leave-one-out test or 10-fold cross-validation.
Conclusions
Despite more complex classification tasks compared to our earlier research and having more different genetic cardiac diseases in the analysis, it is still possible to attain good disease classification results. As excepted, leave-one-out test and 10-fold cross-validation achieved virtually equal results.}
}
@article{CATENACCIVOLPI20141,
title = {How active perception and attractor dynamics shape perceptual categorization: A computational model},
journal = {Neural Networks},
volume = {60},
pages = {1-16},
year = {2014},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2014.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608014001440},
author = {Nicola {Catenacci Volpi} and Jean Charles Quinton and Giovanni Pezzulo},
keywords = {Hopfield networks, Perceptual categorization, Prediction, Active vision, Dynamic choice},
abstract = {We propose a computational model of perceptual categorization that fuses elements of grounded and sensorimotor theories of cognition with dynamic models of decision-making. We assume that category information consists in anticipated patterns of agent–environment interactions that can be elicited through overt or covert (simulated) eye movements, object manipulation, etc. This information is firstly encoded when category information is acquired, and then re-enacted during perceptual categorization. The perceptual categorization consists in a dynamic competition between attractors that encode the sensorimotor patterns typical of each category; action prediction success counts as “evidence” for a given category and contributes to falling into the corresponding attractor. The evidence accumulation process is guided by an active perception loop, and the active exploration of objects (e.g., visual exploration) aims at eliciting expected sensorimotor patterns that count as evidence for the object category. We present a computational model incorporating these elements and describing action prediction, active perception, and attractor dynamics as key elements of perceptual categorizations. We test the model in three simulated perceptual categorization tasks, and we discuss its relevance for grounded and sensorimotor theories of cognition.}
}
@article{BENTLEY20131240,
title = {Predicting the future: Towards symbiotic computational and experimental angiogenesis research},
journal = {Experimental Cell Research},
volume = {319},
number = {9},
pages = {1240-1246},
year = {2013},
note = {Special Issue: Endothelial Biology},
issn = {0014-4827},
doi = {https://doi.org/10.1016/j.yexcr.2013.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0014482713000426},
author = {Katie Bentley and Martin Jones and Bert Cruys},
keywords = {Computational modelling, Interdisciplinary, Angiogenesis, Prediction, Simulation},
abstract = {Understanding the fundamental organisational principles underlying the complex and multilayered process of angiogenesis is the mutual aim of both the experimental and theoretical angiogenesis communities. Surprisingly, these two fields have in the past developed in near total segregation, with neither fully benefiting from the other. However, times are changing and here we report on the new direction that angiogenesis research is taking, where from well-integrated collaborations spring new surprises, experimental predictions and research avenues. We show that several successful ongoing collaborations exist in the angiogenesis field and analyse what aspects of their approaches led them to achieve novel and impactful biological insight. We conclude that there are common elements we can learn from for the future, and provide a list of guidelines to building a successful collaborative venture. Specifically, we find that a near symbiosis of computation with experimentation reaps the most impactful results by close cyclical feedback and communication between the two disciplines resulting in continual refinement of models, experimental directions and our understanding. We discuss high impact examples of predictive modelling from the wider, more established integrated scientific domains and conclude that the angiogenesis community can do nothing but benefit from joining this brave new, integrated world.}
}
@article{DASH201640,
title = {An evolutionary hybrid Fuzzy Computationally Efficient EGARCH model for volatility prediction},
journal = {Applied Soft Computing},
volume = {45},
pages = {40-60},
year = {2016},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2016.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S1568494616301624},
author = {Rajashree Dash and P.K. Dash},
keywords = {Volatility prediction, Stock markets, GARCH model variants, Fuzzy logic based hybrids, Fuzzy inference with nonlinear functions, Multistep prediction, Differential evolution, Super predictive ability test},
abstract = {Accurate modeling for forecasting of stock market volatility is a widely interesting research area both in academia as well as financial markets. This paper proposes an innovative Fuzzy Computationally Efficient EGARCH model to forecast the volatility of three stock market indexes. The proposed model represents a joint estimation of the membership function parameters of a TSK-type fuzzy inference system along with the leverage effect, asymmetric shock by leverage effect of EGARCH model in forecasting highly nonlinear and complicated financial time series model more accurately. Further unlike the conventional TSK type fuzzy neural network the proposed model uses a functional link neural network (FLANN) in the consequent part of the fuzzy rules to provide an improved mapping. Moreover, a differential evolution (DE) algorithm is suggested to solve the parameters estimation problem of Fuzzy Computationally Efficient EGARCH model. Being a parallel direct search algorithm, DE has the strength of finding global optimal solutions regardless of the initial values of its few control parameters. Furthermore, the DE based algorithm aims to achieve an optimal solution with a rapid convergence rate. The proposed model has been compared with some GARCH family models and hybrid fuzzy systems and GARCH models based on three performance metrics: MSFE, RMSFE, and MAFE. The results indicate that the proposed method offers significant improvements in volatility forecasting performance in comparison with all other specified models.}
}
@article{WANG2014638,
title = {Computational Psychiatry},
journal = {Neuron},
volume = {84},
number = {3},
pages = {638-654},
year = {2014},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2014.10.018},
url = {https://www.sciencedirect.com/science/article/pii/S0896627314009167},
author = {Xiao-Jing Wang and John H. Krystal},
abstract = {Psychiatric disorders such as autism and schizophrenia, arise from abnormalities in brain systems that underlie cognitive, emotional, and social functions. The brain is enormously complex and its abundant feedback loops on multiple scales preclude intuitive explication of circuit functions. In close interplay with experiments, theory and computational modeling are essential for understanding how, precisely, neural circuits generate flexible behaviors and their impairments give rise to psychiatric symptoms. This Perspective highlights recent progress in applying computational neuroscience to the study of mental disorders. We outline basic approaches, including identification of core deficits that cut across disease categories, biologically realistic modeling bridging cellular and synaptic mechanisms with behavior, and model-aided diagnosis. The need for new research strategies in psychiatry is urgent. Computational psychiatry potentially provides powerful tools for elucidating pathophysiology that may inform both diagnosis and treatment. To achieve this promise will require investment in cross-disciplinary training and research in this nascent field.}
}
@article{AALTO2019145,
title = {Modeling of biomass supply system by combining computational methods – A review article},
journal = {Applied Energy},
volume = {243},
pages = {145-154},
year = {2019},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2019.03.201},
url = {https://www.sciencedirect.com/science/article/pii/S0306261919306178},
author = {Mika Aalto and Raghu KC and Olli-Jussi Korpinen and Kalle Karttunen and Tapio Ranta},
keywords = {Biomass, Supply chain, Life cycle assessment, Geographical information system, Agent-based modeling and simulation, Discrete-event simulation},
abstract = {As computing power increases, more complex computational models are utilized for biomass supply system studies. The paper describes three commonly used modeling methods in this context, geographic information systems, life-cycle assessment, and discrete-time simulation and presents bibliometric analysis of work using these three study methods. Of the 498 publications identified in searches of the Scopus and Web of Science databases, 17 reported on combinations of methods: 10 on life-cycle assessment and geographic information systems, six on joint use of life-cycle assessment and discrete-time simulation, and one on use of geographic information systems jointly with discrete-time simulation. While no articles dealt directly with simultaneous use of all three methods, several acknowledged the potential of this. The authors discuss numerous challenges identified in the review that arise in combining methods, among them computational load, the increasing number of assumptions, guaranteeing coherence between the models used, and the large quantities of data required. Discussion of issues such as the complexity of reporting and the need for standard procedures and terms becomes more critical as repositories bring together research materials, including entire models, from various sources. Efforts to mitigate many of modeling’s challenges have involved phase-specific modeling and use of such methods as expressions or uncertainty analysis in place of a complex secondary model. The authors conclude that combining modeling methods offer considerable potential for taking more variables into account; improving the results; and benefiting researchers, decision–makers, and operation managers by producing more reliable information.}
}
@article{KRZHIZHANOVSKAYA2015288,
title = {Russian-Dutch double-degree Master’s programme in computational science in the age of global education},
journal = {Journal of Computational Science},
volume = {10},
pages = {288-298},
year = {2015},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2015.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S1877750315000812},
author = {Valeria V. Krzhizhanovskaya and Alexey V. Dukhanov and Anna Bilyatdinova and Alexander V. Boukhanovsky and Peter M.A. Sloot},
keywords = {Computational science, Master’s programme, Graduate program, Double degree, Curriculum, Enrollment, Student research, Funding opportunities},
abstract = {We present a new double-degree graduate (Master’s) programme developed together by the ITMO University, Russia and University of Amsterdam, The Netherlands. First, we look into the global aspects of integration of different educational systems and list some funding opportunities. Then, we describe our double-degree program curriculum, suggest the timeline of enrollment and studies, and give some examples of student research topics. Finally, we discuss the issues of joint programs with Russia and suggest possible solutions, analyze the results of the first three student intakes and reflect on the lessons learnt, and share our thoughts and experiences that could be of interest to the international community expanding the educational markets to the vast countries like Russia, China or India. The paper is written for education professionals and contains useful information for potential students. This is an extended version of a conference paper (http://dx.doi.org/10.1016/j.procs.2014.05.130) invited to this special issue of the Journal of Computational Science.}
}
@article{KANIZSA198523,
title = {Seeing and thinking},
journal = {Acta Psychologica},
volume = {59},
number = {1},
pages = {23-33},
year = {1985},
issn = {0001-6918},
doi = {https://doi.org/10.1016/0001-6918(85)90040-X},
url = {https://www.sciencedirect.com/science/article/pii/000169188590040X},
author = {G. Kanizsa},
abstract = {According to ratiomorphic theories of perception every visual phenomenon would be the result of unconscious inferences through which the visual system, starting from a set of axioms and premises, reaches certain conclusions (which constitute actually the visual phenomena) by a process analogous to a reasoning process. The author presents some examples from the area of amodal completion which, according to him, hardly support a ratiomorphic theory. Instead they constitute counterexamples that rather support the hypothesis that seeing and thinking function according to different rules.}
}
@incollection{MILLER201455,
title = {Chapter 5 - Managing and Integrating Exposome Data: Maps, Models, Computation, and Systems Biology},
editor = {Gary W. Miller},
booktitle = {The Exposome},
publisher = {Academic Press},
address = {San Diego},
pages = {55-69},
year = {2014},
isbn = {978-0-12-417217-3},
doi = {https://doi.org/10.1016/B978-0-12-417217-3.00005-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780124172173000057},
author = {Gary W. Miller},
keywords = {Bioinformatics, systems biology, computational biology, machine learning, Bayesian methods},
abstract = {Exposome-related data will come from a myriad of sources. The huge amounts of data must be organized in some manner that allows appropriate interpretations and associations to be drawn. Models and maps are often used to provide organization to complex data sets. Maps are quite appropriate for exposome research as the location of the sources and exposures is a critical component, and spatial statistics could play a major role in exposome data organization. The complex types of data will undoubtedly require mathematical approaches, including bioinformatics, computational, and systems biology-based techniques. This chapter reviews some of the possible strategies that can be used to keep track of the diverse and massive data sets that will result from exposome research.}
}
@article{VALTON2017631,
title = {Comprehensive review: Computational modelling of schizophrenia},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {83},
pages = {631-646},
year = {2017},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2017.08.022},
url = {https://www.sciencedirect.com/science/article/pii/S0149763416307357},
author = {Vincent Valton and Liana Romaniuk and J. {Douglas Steele} and Stephen Lawrie and Peggy Seriès},
keywords = {Psychotic symptoms, Schizophrenia, Computational models, Computational psychiatry},
abstract = {Computational modelling has been used to address: (1) the variety of symptoms observed in schizophrenia using abstract models of behavior (e.g. Bayesian models – top-down descriptive models of psychopathology); (2) the causes of these symptoms using biologically realistic models involving abnormal neuromodulation and/or receptor imbalance (e.g. connectionist and neural networks – bottom-up realistic models of neural processes). These different levels of analysis have been used to answer different questions (i.e. understanding behavioral vs. neurobiological anomalies) about the nature of the disorder. As such, these computational studies have mostly supported diverging hypotheses of schizophrenia's pathophysiology, resulting in a literature that is not always expanding coherently. Some of these hypotheses are however ripe for revision using novel empirical evidence. Here we present a review that first synthesizes the literature of computational modelling for schizophrenia and psychotic symptoms into categories supporting the dopamine, glutamate, GABA, dysconnection and Bayesian inference hypotheses respectively. Secondly, we compare model predictions against the accumulated empirical evidence and finally we identify specific hypotheses that have been left relatively under-investigated.}
}
@article{POSTAN2018111,
title = {Dioids for Computational Effects},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {339},
pages = {111-134},
year = {2018},
note = {The XLII Latin American Computing Conference},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2018.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S1571066118300525},
author = {Ezequiel Postan and Exequiel Rivas and Mauro Jaskelioff},
keywords = {dioid, monad, Haskell, computational effect},
abstract = {There are different algebraic structures that one can use to model notions of computation. The most well- known are monads, but lately, applicative functors have been gaining popularity. These two structures can be understood as instances of the unifying notion of monoid in a monoidal category. When dealing with non-determinism, it is usual to extend monads and applicative functors with additional structure. However, depending on the desired non-determinism, there are different options of interaction between the existing and the additional structure. This article studies one of those options, which is captured algebraically by dioids. We generalise dioids to dioid categories and show how dioids in such a category model non- determinism in monads and applicative functors. Moreover, we study the construction of free dioids in a programming context.}
}
@article{KUO201232,
title = {Conceptual study of micro-tab device in airframe noise reduction: (II) 3D computation},
journal = {Aerospace Science and Technology},
volume = {17},
number = {1},
pages = {32-39},
year = {2012},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2011.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S127096381100040X},
author = {Brian C. Kuo and Nesrin Sarigul-Klijn},
keywords = {Computational aeroacoustics, High-lift devices, Micro-tab, Airframe noise},
abstract = {A three-dimensional numerical study is conducted to better understand noise reduction results seen in the previous two-dimensional investigation of the acoustic effects of micro-tab device on airframe noise reduction. Without sacrificing the aerodynamic performance, it is possible to achieve high-lift noise reduction with the application of the micro-tab device attached to the pressure side of the flap surface near its trailing-edge. This study was carried out by numerical hybrid method, which combines Computational Fluid Dynamics and acoustic analogy to predict the farfield noise spectrum. The near-full-scale computational results show that the micro-tab device with reduced deflection of the high-lift devices achieves noise reduction in mid-to-high frequency domain, in particular the range that human beings are most sensitive to. In addition, a parametric study in terms of geometric variation of the micro-tab was also investigated and reported. The three-dimensional results obtained thus far show reduction in noise levels with use of micro-tab.}
}
@article{RICHARD2019136,
title = {CastLab: an object-oriented finite element toolbox within the Matlab environment for educational and research purposes in computational solid mechanics},
journal = {Advances in Engineering Software},
volume = {128},
pages = {136-151},
year = {2019},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2018.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S0965997818301303},
author = {Benjamin Richard and Giuseppe Rastiello and Cédric Giry and Francesco Riccardi and Romili Paredes and Eliass Zafati and Santosh Kakarla and Chaymaa Lejouad},
keywords = {Matlab toolbox, Nonlinear solid mechanics, Computational mechanics, Educational tools, Finite elements},
abstract = {The Matlab environment has become widely used among the computational mechanics community, not only for research purposes but also to teach either undergraduate or graduate classes. This paper aims to present a new toolbox devoted to computational mechanics and in particular to solid mechanics. Both recent and well-established numerical formulations have been implemented in it. One of its strengths resides in the fact that it was developed within an object-oriented framework. This key feature makes the CastLab toolbox easy-to-use and with extensive capabilities for customized user developments. After a brief description of the theoretical background related to the problems that can be solved by means of the toolbox, several representative case-studies are presented. These examples have been selected to illustrate not only the numerical efficiency of the toolbox, which is of primary importance for research purposes, but also its strong educational and pedagogic potential.}
}
@article{KUMAR2006806,
title = {Applying computational modeling to drug discovery and development},
journal = {Drug Discovery Today},
volume = {11},
number = {17},
pages = {806-811},
year = {2006},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2006.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S1359644606002868},
author = {Neil Kumar and Bart S. Hendriks and Kevin A. Janes and David {de Graaf} and Douglas A. Lauffenburger},
abstract = {Computational models of cells, tissues and organisms are necessary for increased understanding of biological systems. In particular, modeling approaches will be crucial for moving biology from a descriptive to a predictive science. Pharmaceutical companies identify molecular interventions that they predict will lead to therapies at the organism level, suggesting that computational biology can play a key role in the pharmaceutical industry. We discuss pharmaceutically-relevant computational modeling approaches currently used as predictive tools. Specific examples demonstrate how companies can employ these computational models to improve the efficiency of transforming targets into therapies.}
}
@incollection{PHIPPEN2024,
title = {Artificial Intelligence},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95689-5.00098-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956895000985},
author = {Andy Phippen},
keywords = {Artificial intelligence, Computer science, Deep learning, Digital literacy, Ethics, Information science, Large language models, Machine learning, Natural language processing},
abstract = {Artificial Intelligence (AI) is attracting considerable, and justified, attention about its potential and impact on information systems. However, it is important to look at this evolution against its history. AI’s historical evolution has been beset with underperformance and ethical concerns in data training and responsible deployment. Information science has undergone significant changes with AI׳s integration, impacting information retrieval, classification, and library automation. More specifically Machine Learning plays a crucial role in understanding human requirements for information and processing large data set, but challenges like bias persist. Large Language Models (LLMs) like ChatGPT represent the vanguard of public adoption of AI driven information systems and have exhibited remarkable performance in natural language processing. While they enhance information searching and content creation, users must understand limitations, biases, and practice critical thinking for responsible utilisation in a digital age.}
}
@article{ATSALAKIS2016107,
title = {Using computational intelligence to forecast carbon prices},
journal = {Applied Soft Computing},
volume = {43},
pages = {107-116},
year = {2016},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2016.02.029},
url = {https://www.sciencedirect.com/science/article/pii/S1568494616300801},
author = {George S. Atsalakis},
keywords = {ANFIS forecasting, Carbon allowance, Carbon price forecasting, Computational intelligent forecasting, Neuro-fuzzy forecasting, PATSOS forecasting},
abstract = {European Union has introduced the European Trading System (ETS) as a tool for developing and implementing international treaties related to climate changes and to identify the most cost-effective methods for reducing greenhouse gas emissions, in particular carbon dioxide (CO2), which is the most substantial. Companies producing carbon emissions must effectively manage associated costs by buying or selling carbon emission futures. Viewed from this perspective, this paper provides a model for managing the risk by buying and selling carbon emission futures by implementing techniques that leverage computational intelligence. Three computational intelligence techniques are proposed to provide accurate and timely forecasts for changes in the price of carbon: a novel hybrid neuro-fuzzy controller that forms a closed-loop feedback mechanism called PATSOS; an artificial neural network (ANN) based system; an adaptive neuro-fuzzy inference system (ANFIS). Results are based on 1074 daily carbon price observations collected to comprise a useful time-series dataset and for evaluation of the proposed techniques. The extra-sample performance of the proposed techniques is calculated. Analysis results are compared with those produced by other models. Comparison studies reveal that PATSOS is the most accurate and promising methodology for predicting the price of carbon. It is stated that this paper registers a first attempt to apply a hybrid neuro-fuzzy controller to forecasting carbon prices.}
}
@article{SHARMA20221,
title = {The design and evaluation of an AR-based serious game to teach programming},
journal = {Computers & Graphics},
volume = {103},
pages = {1-18},
year = {2022},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2022.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0097849322000024},
author = {Vandit Sharma and Kaushal Kumar Bhagat and Huai-Hsuan Huang and Nian-Shing Chen},
keywords = {Augmented Reality, Computational thinking, Learning Analytics, Gamification, Feedback design, System usability},
abstract = {The ubiquity of smartphone and tablet devices, combined with the increasing availability of serious games, has enabled students to learn various abstract concepts in an appealing and convenient manner. While several researchers have explored the use of Augmented Reality (AR) in serious games, many of these games have not been critically explained or evaluated. To that end, we employed game-based learning methodologies and Game Learning Analytics (GLA) to systematize the design and evaluation of an AR-based serious game to teach programming. We evaluated our game for usability and effectiveness by conducting a user study on twenty-seven undergraduate students. The evaluation primarily consisted of a learning test conducted twice – before and after playing the game – along with a usability questionnaire that players completed after playing the game. Our results showed that players made significant progress after playing the game. The game helped players improve their basic programming skills, especially for the group having lower prior programming skills. The results highlighted various ways in which GLA can be used to benefit different stakeholders in the game. Based on players’ qualitative responses, we also identified several areas of improvement, most prominently the trade-off between ease of use and game complexity. We provide suggestions and discuss implications for future work.}
}
@article{LASO2018428,
title = {Finding an economic and environmental balance in value chains based on circular economy thinking: An eco-efficiency methodology applied to the fish canning industry},
journal = {Resources, Conservation and Recycling},
volume = {133},
pages = {428-437},
year = {2018},
issn = {0921-3449},
doi = {https://doi.org/10.1016/j.resconrec.2018.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0921344918300429},
author = {Jara Laso and Isabel García-Herrero and María Margallo and Ian Vázquez-Rowe and Pére Fullana and Alba Bala and Cristina Gazulla and Ángel Irabien and Rubén Aldaco},
keywords = {Life cycle assessment, Life cycle costing, Eco-efficiency, Engraulis encrasicolus, Linear programming},
abstract = {The production of food that is environmentally friendly and presents a high economic return is one of the current concerns for the food industry. Eco-efficiency links the environmental performance of a product to its economic value. In this context, this study combines Life Cycle Assessment (LCA) and Life Cycle Costing (LCC) to propose a two-step eco-efficiency methodology assessment for the fish canning industry. An eco-label rating system based on a descriptive weighting of environmental (Global Warming Potential, Acidification Potential, Eutrophication Potential and the ReCiPe Single Score Endpoint) and economic (Value Added) indicators was applied to the canned anchovy. Secondly, LCA-LCC results were coupled to linear programming (LP) tools in order to define a composite eco-efficiency index. This approach enables translation into economic terms of the environmental damage caused when a given alternative is chosen. In particular, different origins for anchovy species (South American vs. Cantabrian) and related waste management alternatives (landfill, incineration and valorization) were evaluated under this cradle to gate approach. Results indicated that substantial differences can be observed depending on the origin of the fish. Anchovies landed in Cantabria show a higher value added score at the expense of larger environmental impacts, mainly due to fuel use intensity. Moreover, its environmental scores are lowered when fish residues are valorized into marketable products, while increasing the value added. This study demonstrates the environmental and economic benefits of applying circular economy. According to this, it is possible to introduce the cradle-to-cradle concept in the fish canned industry. The methodology proposed is intended to be useful to decision-makers in the anchovy canning sector and can be applied to other regions and industrial sectors.}
}
@article{NESI2024,
title = {Enactivism: A contemporary perspective of a reconceptualization of osteopathy},
journal = {Advances in Integrative Medicine},
year = {2024},
issn = {2212-9588},
doi = {https://doi.org/10.1016/j.aimed.2024.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212958824001186},
author = {Jacson Nesi and Michele Benites and Filipe Boeira Schedler},
keywords = {Enactivism, Osteopathy, Medical rationalities, Reconceptualization},
abstract = {Enactivism is a philosophical and scientific approach that emphasizes the role of the body and its interactions with the environment in shaping cognitive processes and subjective experiences. Meanwhile, osteopathy is a person-centered health care discipline, highlighting the structure-function interrelationship of the body and its selfregulation mechanisms. Both approaches value the body and the environment in health. Several authors have been discussing the urgent need for a reconceptualization of osteopathy and also suggesting integrate biological, psychological and social aspects. Thinking osteopathy as a Therapeutic Rationality, implies recognize its fundamental dimensions: Human Morphology, Vital Dynamics, Medical Doctrine, Diagnostic System and Therapeutic System, all integrated by a philosophical Cosmology, as the original term Medical rationality states, but also embrace a broader perspective allowing an individual and unique process of each person, reflecting the transformation of contemporary medicine to a person approach. Enactivism principles can serve as a basis for a reconceptualization of osteopathy, integrating environmental, psychological, social, and spiritual factors. Osteopathic concepts can probably be updated through the convergence between enactivism and osteopathy, promoting more meaningful and evidence-based clinical practice. Advancing in this direction requires a collaborative dialogue between researchers, health professionals and interested people, seeking an integrated understanding of the relationship between body, mind, environment and health.}
}
@article{ANGELAKI2009452,
title = {Multisensory integration: psychophysics, neurophysiology, and computation},
journal = {Current Opinion in Neurobiology},
volume = {19},
number = {4},
pages = {452-458},
year = {2009},
note = {Sensory systems},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2009.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0959438809000725},
author = {Dora E Angelaki and Yong Gu and Gregory C DeAngelis},
abstract = {Fundamental observations and principles derived from traditional physiological studies of multisensory integration have been difficult to reconcile with computational and psychophysical studies that share the foundation of probabilistic (Bayesian) inference. We review recent work on multisensory integration, focusing on experiments that bridge single-cell electrophysiology, psychophysics, and computational principles. These studies show that multisensory (visual–vestibular) neurons can account for near-optimal cue integration during the perception of self-motion. Unlike the nonlinear (superadditive) interactions emphasized in some previous studies, visual–vestibular neurons accomplish near-optimal cue integration through subadditive linear summation of their inputs, consistent with recent computational theories. Important issues remain to be resolved, including the observation that variations in cue reliability appear to change the weights that neurons apply to their different sensory inputs.}
}
@article{LEONELLI201229,
title = {Re-thinking organisms: The impact of databases on model organism biology},
journal = {Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences},
volume = {43},
number = {1},
pages = {29-36},
year = {2012},
note = {Data-Driven Research in the Biological and Biomedical Sciences On Nature and Normativity: Normativity, Teleology, and Mechanism in Biological Explanation},
issn = {1369-8486},
doi = {https://doi.org/10.1016/j.shpsc.2011.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S1369848611000793},
author = {Sabina Leonelli and Rachel A. Ankeny},
keywords = {Database, Data, Model organism, Data-intensive science, Curator},
abstract = {Community databases have become crucial to the collection, ordering and retrieval of data gathered on model organisms, as well as to the ways in which these data are interpreted and used across a range of research contexts. This paper analyses the impact of community databases on research practices in model organism biology by focusing on the history and current use of four community databases: FlyBase, Mouse Genome Informatics, WormBase and The Arabidopsis Information Resource. We discuss the standards used by the curators of these databases for what counts as reliable evidence, acceptable terminology, appropriate experimental set-ups and adequate materials (e.g., specimens). On the one hand, these choices are informed by the collaborative research ethos characterising most model organism communities. On the other hand, the deployment of these standards in databases reinforces this ethos and gives it concrete and precise instantiations by shaping the skills, practices, values and background knowledge required of the database users. We conclude that the increasing reliance on community databases as vehicles to circulate data is having a major impact on how researchers conduct and communicate their research, which affects how they understand the biology of model organisms and its relation to the biology of other species.}
}
@article{DELOERA20161,
title = {Random sampling in computational algebra: Helly numbers and violator spaces},
journal = {Journal of Symbolic Computation},
volume = {77},
pages = {1-15},
year = {2016},
issn = {0747-7171},
doi = {https://doi.org/10.1016/j.jsc.2016.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S074771711600002X},
author = {Jesús A. {De Loera} and Sonja Petrović and Despina Stasi},
keywords = {Violator spaces, Ideal generators, Solving polynomial systems, Randomized algorithm in algebra, Large sparse systems of equations},
abstract = {This paper transfers a randomized algorithm, originally used in geometric optimization, to computational problems in commutative algebra. We show that Clarkson's sampling algorithm can be applied to two problems in computational algebra: solving large-scale polynomial systems and finding small generating sets of graded ideals. The cornerstone of our work is showing that the theory of violator spaces of Gärtner et al. applies to polynomial ideal problems. To show this, one utilizes a Helly-type result for algebraic varieties. The resulting algorithms have expected runtime linear in the number of input polynomials, making the ideas interesting for handling systems with very large numbers of polynomials, but whose rank in the vector space of polynomials is small (e.g., when the number of variables and degree is constant).}
}
@incollection{THIEL2005559,
title = {Chapter 21 - Semiempirical quantum-chemical methods in computational chemistry},
editor = {Clifford E. Dykstra and Gernot Frenking and Kwang S. Kim and Gustavo E. Scuseria},
booktitle = {Theory and Applications of Computational Chemistry},
publisher = {Elsevier},
address = {Amsterdam},
pages = {559-580},
year = {2005},
isbn = {978-0-444-51719-7},
doi = {https://doi.org/10.1016/B978-044451719-7/50064-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780444517197500640},
author = {Walter Thiel},
abstract = {Publisher Summary
This chapter focuses on semiempirical quantum-chemical methods describing their development over the past 40 years. One of the first semiempirical approaches in quantum chemistry was the p-electron method proposed by Hǖckel (1931) that generates molecular orbitals (MOs) essentially from the connectivity matrix of a molecule and provides valuable qualitative insights into the structure, stability, and spectroscopy of unsaturated molecules. Hoffmann extended this approach to include all valence electrons and applied in many qualitative studies of inorganic and organometallic compounds. These early semiempirical methods had a lasting impact on chemical thinking as they guided the development of qualitative MO theory that is commonly employed for rationalizing chemical phenomena in terms of orbitals interactions. They are normally not used any longer as computational tools. After a survey of the established methods such as MNDO, AM1, and PM3, recent methodological advances are described including the development of improved semiempirical models, new general-purpose and special-purpose parametrizations, and linear scaling approaches.}
}