@article{SMITH2018325,
title = {The Developing Infant Creates a Curriculum for Statistical Learning},
journal = {Trends in Cognitive Sciences},
volume = {22},
number = {4},
pages = {325-336},
year = {2018},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2018.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S1364661318300275},
author = {Linda B. Smith and Swapnaa Jayaraman and Elizabeth Clerkin and Chen Yu},
keywords = {statistical learning, egocentric vision, face perception, object perception},
abstract = {New efforts are using head cameras and eye-trackers worn by infants to capture everyday visual environments from the point of view of the infant learner. From this vantage point, the training sets for statistical learning develop as the sensorimotor abilities of the infant develop, yielding a series of ordered datasets for visual learning that differ in content and structure between timepoints but are highly selective at each timepoint. These changing environments may constitute a developmentally ordered curriculum that optimizes learning across many domains. Future advances in computational models will be necessary to connect the developmentally changing content and statistics of infant experience to the internal machinery that does the learning.}
}
@article{KLEINMUNTZ1992227,
title = {Computers as clinicians: An update},
journal = {Computers in Biology and Medicine},
volume = {22},
number = {4},
pages = {227-237},
year = {1992},
issn = {0010-4825},
doi = {https://doi.org/10.1016/0010-4825(92)90062-R},
url = {https://www.sciencedirect.com/science/article/pii/001048259290062R},
author = {Benjamin Kleinmuntz},
keywords = {Clinical information processing, Clinical decision making, Clinical reasoning, Diagnostic reasoning, Computers as clinicians, Medical decision making, Human judgment, Human inference, Computer simulation of thinking, Artificial intelligence, Expert systems},
abstract = {Computers as clinicians entered medical settings relatively recently, but with limited success because they lack general intelligence—that is, though they can be experts in domain specific specialties, they cannot yet deal with clinical problems never before encountered. SOAR, a novel AI computer programming architecture, can learn from past encounters with prior problems and can generalize its learning to new ones. It may therefore take computers to a higher level of clinical performance.}
}
@article{ZHANG2019610,
title = {Resolution, energy and time dependency on layer scaling in finite element modelling of laser beam powder bed fusion additive manufacturing},
journal = {Additive Manufacturing},
volume = {28},
pages = {610-620},
year = {2019},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2019.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S2214860418309576},
author = {Wenyou Zhang and Mingming Tong and Noel M. Harrison},
keywords = {Powder bed fusion, Optimisation, Process modelling, Interdependencies, Additive manufacturing},
abstract = {The Laser Beam Powder Bed Fusion (PBF-LB) category of Additive Manufacturing (AM) is currently receiving much attention for computational process modelling. Major challenges exist in how to reconcile resolution, energy and time in a real build, with the practical limitations of resolution (layer height and mesh resolution), energy (heat format and magnitude) and time (heating and cooling step times) in the computational space. A novel thermomechanical PBF-LB process model including an efficient powder-interface heat loss mechanism was developed. The effect of variations in layer height (layer scaling), energy and time on the temperature and stress evolution was investigated. The influence of heating step time and cooling step time was characterised and the recommended ratio of element size to layer scaling was presented, based on a macroscale 2D model. The layer scaling method was effective when scaling up to 4 times the layer thickness and appropriately also scaling the cooling step time. This research provides guidelines and a framework for layer scaling for finite element modelling of the PBF-LB process.}
}
@article{VIDALPINO2021104249,
title = {Introducing the structural bases of typicality effects in deep learning},
journal = {Image and Vision Computing},
volume = {113},
pages = {104249},
year = {2021},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2021.104249},
url = {https://www.sciencedirect.com/science/article/pii/S0262885621001542},
author = {Omar {Vidal Pino} and Erickson R. Nascimento and Mario F.M. Campos},
keywords = {Typicality effects, Category semantic representation, Image semantic representation, Semantic classification, Global features description, Prototype theory},
abstract = {In this paper, we hypothesize that the effects of the degree of typicality in natural semantic categories can be generated based on the structure of artificial categories learned with deep learning models. Motivated by the human approach to representing natural semantic categories and based on the Prototype Theory foundations, we propose a novel Computational Prototype Model (CPM) to represent the internal structure of semantic categories. Unlike other prototype learning approaches, our mathematical framework proposes a first approach to provide deep neural networks with the ability to model abstract semantic concepts such as category central semantic meaning, typicality degree of an object's image, and family resemblance relationship. We proposed several methodologies based on the typicality's concept to evaluate our CPM-model in image semantic processing tasks such as image classification, a global semantic description, and transfer learning. Our experiments on different image datasets, such as ImageNet and Coco, showed that our approach might be an admissible proposition in the effort to endow machines with greater power of abstraction for the semantic representation of objects' categories.}
}
@incollection{PETROVIC2024,
title = {A Historical and Current Look at Chemical Design for Reduced Hazard},
booktitle = {Reference Module in Chemistry, Molecular Sciences and Chemical Engineering},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-12-409547-2},
doi = {https://doi.org/10.1016/B978-0-443-15742-4.00072-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780443157424000727},
author = {Predrag V. Petrovic and Philip Coish and Paul T. Anastas},
keywords = {Benign by design, CAMD, First do no harm, Green chemistry, Hazard reduction, Safer chemical design, Sustainable chemistry},
abstract = {Green chemistry aims to design chemical products and processes that reduce or eliminate the use and generation of hazardous substances. The design of chemicals for reduced hazard selects the properties and attributes that provide the desired function while avoiding unintended consequences for human health and the environment. The tools and knowledge that are available to designers have increased considerably in recent years. Importantly, the approach and goals of design have evolved and become more holistic, multi-disciplinary, and inclusive. This is notable as the global population׳s continued economic and social well-being cannot be maintained with measures that destroy our natural environment. Sustainable chemistry promotes, advances, enables, and empowers the implementation of the chemistry of sustainability and includes chemical design for reduced hazard. In this chapter, we consider the design of chemicals for reduced hazard (safer chemical design) focusing on the past and present, and importantly, looking to the future on what lies ahead.}
}
@article{MOALLEMI2023102727,
title = {Knowledge co-production for decision-making in human-natural systems under uncertainty},
journal = {Global Environmental Change},
volume = {82},
pages = {102727},
year = {2023},
issn = {0959-3780},
doi = {https://doi.org/10.1016/j.gloenvcha.2023.102727},
url = {https://www.sciencedirect.com/science/article/pii/S0959378023000936},
author = {Enayat A. Moallemi and Fateme Zare and Aniek Hebinck and Katrina Szetey and Edmundo Molina-Perez and Romy L. Zyngier and Michalis Hadjikakou and Jan Kwakkel and Marjolijn Haasnoot and Kelly K. Miller and David G. Groves and Peat Leith and Brett A. Bryan},
keywords = {Co-production, Stakeholder, Sustainability, Socio-ecological system, Transdisciplinary},
abstract = {Decision-making under uncertainty is important for managing human-natural systems in a changing world. A major source of uncertainty is linked to the multi-actor settings of decisions with poorly understood values, complex relationships, and conflicting management approaches. Despite general agreement across disciplines on co-producing knowledge for viable and inclusive outcomes in a multi-actor context, there is still limited conceptual clarity and no systematic understanding on what co-production means in decision-making under uncertainty and how it can be approached. Here, we use content analysis and clustering to systematically analyse 50 decision-making cases with multiple time and spatial scales across 26 countries and in 9 different sectors in the last decade to serve two aims. The first is to synthesise the key recurring strategies that underpin high quality decision co-production across many cases of diverse features. The second is to identify important deficits and opportunities to leverage existing strategies towards flourishing co-production in support of decision-making. We find that four general strategies emerge centred around: promoting innovation for robust and equitable decisions; broadening the span of co-production across interacting systems; fostering social learning and inclusive participation; and improving pathways to impact. Additionally, five key areas that should be addressed to improve decision co-production are identified in relation to: participation diversity; collaborative action; power relationships; governance inclusivity; and transformative change. Characterising the emergent strategies and their key areas for improvement can help guide future works towards more pluralistic and integrated science and practice.}
}
@article{KEIL2005163,
title = {Adaptation and Evolution in Dynamic Persistent Environments},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {141},
number = {5},
pages = {163-179},
year = {2005},
note = {Proceedings of the Workshop on the Foundations of Interactive Computation (FInCo 2005)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2005.05.021},
url = {https://www.sciencedirect.com/science/article/pii/S1571066105051947},
author = {David Keil and Dina Goldin},
keywords = {adaptive systems, coordination, dynamic persistent environments, evolutionary computation, interactive computing, models of computation, multiagent systems},
abstract = {Optimization (adaptation) of agents interacting with dynamic persistent environments (DPEs) poses a separate class of problems from those of static optimization. Such environments must be incorporated into models of interactive computation. By the No Free Lunch Theorem (NFLT), no general-purpose function-optimization algorithm can exist that is superior to random search. But interactive adaptation in environments with persistent state falls outside the scope of the NFLT, and useful general-purpose interactive optimization protocols for DPEs exist, as we show. Persistence of state supports indirect interaction. Based on the observation that mutual causation is inherent to interactive computation, and on the key role of persistent state in multiagent systems, we establish that indirect interaction is essential to multiagent systems (MASs). This work will be useful to researchers in coordination, evolutionary computation, and design of multiagent and adaptive systems.}
}
@article{SCHIFFMAN20041079,
title = {Mainstream economics, heterodoxy and academic exclusion: a review essay},
journal = {European Journal of Political Economy},
volume = {20},
number = {4},
pages = {1079-1095},
year = {2004},
issn = {0176-2680},
doi = {https://doi.org/10.1016/j.ejpoleco.2004.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0176268004000643},
author = {Daniel A. Schiffman},
keywords = {Academic exclusion, Pluralism, Economics education, Historical specificity, Heterodoxy},
abstract = {Does the mainstream of economic thinking and analysis tend systematically to exclude ideas and approaches that could enrich the field, and, as a consequence, have important questions and issues been shunted aside for nonobjective reasons? Two recent volumes by heterodox economists that address these questions are Geoffrey Hodgson's How Economics Forgot History: The Problem of Historical Specificity in Social Science, and Steve Keen's Debunking Economics: The Naked Emperor of the Social Sciences. I evaluate their claims of academic exclusion and assess the current state of (selective) pluralism within mainstream economics.}
}
@article{SPROULE2002412,
title = {Fuzzy pharmacology: theory and applications},
journal = {Trends in Pharmacological Sciences},
volume = {23},
number = {9},
pages = {412-417},
year = {2002},
issn = {0165-6147},
doi = {https://doi.org/10.1016/S0165-6147(02)02055-2},
url = {https://www.sciencedirect.com/science/article/pii/S0165614702020552},
author = {Beth A. Sproule and Claudio A. Naranjo and I.Burhan Türksen},
keywords = {fuzzy logic, pharmacodynamics, fuzzy sets, modelling, predictions, pharmacokinetics},
abstract = {Fuzzy pharmacology is a term coined to represent the application of fuzzy logic and fuzzy set theory to pharmacological problems. Fuzzy logic is the science of reasoning, thinking and inference that recognizes and uses the real world phenomenon that everything is a matter of degree. It is an extension of binary logic that is able to deal with complex systems because it does not require crisp definitions and distinctions for the system components. In pharmacology, fuzzy modeling has been used for the mechanical control of drug delivery in surgical settings, and work has begun evaluating its use in other pharmacokinetic and pharmacodynamic applications. Fuzzy pharmacology is an emerging field that, based on these initial explorations, warrants further investigation.}
}
@article{MAJEED2023919,
title = {Numerical Study for the First Phase of the Miraah Solar Well Plant in Oman},
journal = {Procedia Structural Integrity},
volume = {47},
pages = {919-931},
year = {2023},
note = {27th International Conference on Fracture and Structural Integrity (IGF27)},
issn = {2452-3216},
doi = {https://doi.org/10.1016/j.prostr.2023.07.023},
url = {https://www.sciencedirect.com/science/article/pii/S245232162300433X},
author = {Amani J. Majeed},
keywords = {Solar energy, parabolic trough collector, Enhance oil recovery, Amal oil field},
abstract = {Offshore oil and gas production utilizes several renewable energy technologies to meet its energy requirements and to enhance oil recovery (EOR). Using renewable energy can diminish fuel consumption and maintenance costs in upstream processes. In addition to reducing noise, emissions, and improving safety, renewable energy can reduce pollution. As an alternative to natural gas, solar thermal EOR utilizes concentrated solar power (CSP). For the Miraah project in the Amal oil field in Oman, solar energy was used to generate steam. 10% of the total steam are produced during phase one of the project. This study presents a computational simulation of the phase one Miraah project, where the total amount of heat necessary to convert 100 gals of cold water to steam at 450 degrees Fahrenheit and 414.7 psi are calculated. According to the results, the total steam injection per hour is 637 tons with a 3.46 % error percentage from that in Miraah project.}
}
@article{DEPARDIEU1994110,
title = {Optical properties of cermets: 3D real space renormalization using the connection machine},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {207},
number = {1},
pages = {110-114},
year = {1994},
issn = {0378-4371},
doi = {https://doi.org/10.1016/0378-4371(94)90360-3},
url = {https://www.sciencedirect.com/science/article/pii/0378437194903603},
author = {G. Depardieu and P. Fiorini and S. Berthier},
abstract = {We propose a 3D position space renormalization approach based on Kadanoff's block method, for the calculation of the effective dielectric function of cermet type film. This model has been applied to simulated cubic lattices with particle aggregation. Our calculations were performed on a Thinking Machine CM5 massively parallel supercomputer. The renormalization process maps very naturally on to hypercube parallel architecture. We obtain good agreement with most of the experimental data.}
}
@article{FERRAZDEARRUDA2022127387,
title = {Finding contrasting patterns in rhythmic properties between prose and poetry},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {598},
pages = {127387},
year = {2022},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2022.127387},
url = {https://www.sciencedirect.com/science/article/pii/S0378437122002965},
author = {Henrique {Ferraz de Arruda} and Sandro Martinelli Reia and Filipi Nascimento Silva and Diego Raphael Amancio and Luciano {da Fontoura Costa}},
keywords = {Complex systems, Text analysis, Text classification, Time Series, Machine learning, Neural networks},
abstract = {Poetry and prose are written artistic expressions that help us appreciate the reality we live in. Each of these styles has its own set of subjective properties, such as rhyme and rhythm, which are easily caught by a human reader’s eye and ear. With the recent advances in artificial intelligence, the gap between humans and machines may have decreased, and today we observe algorithms mastering tasks that were once exclusively performed by humans. In this paper, we propose a computational method to distinguish between poetry and prose based solely on aural and rhythmic properties. In order to compare prose and poetry rhythms, we represent the rhymes and phonemes as temporal sequences, and thus, we propose a procedure for extracting rhythmic features from these sequences. The performance of this procedure is evaluated by the use of popular machine learning classifiers, and the best accuracy was obtained with a multilayer perceptron neural network. Interestingly, by using an approach based on complex networks to visualize the similarities between the different texts considered, we found that the patterns of poetry vary more than prose. Consequently, a richer and more complex set of rhythmic possibilities tends to be found in that modality.}
}
@article{PUTTASWAMY2025106910,
title = {Fine DenseNet based human personality recognition using english hand writing of non-native speakers},
journal = {Biomedical Signal Processing and Control},
volume = {99},
pages = {106910},
year = {2025},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2024.106910},
url = {https://www.sciencedirect.com/science/article/pii/S1746809424009686},
author = {B S Puttaswamy and N Thillaiarasu},
keywords = {English Handwriting, Deep learning, Histogram equalization, Contrast enhancement, Attention mechanism, Fine-tuning, Classification},
abstract = {This research addresses the challenge of accurately classifying personalities based on English handwriting input images. Traditional approaches to personality classification often rely on subjective assessments or limited datasets, leading to inconsistencies and inaccuracies. To overcome these limitations, this paper proposes a novel methodology that leverages advanced deep learning techniques. Specifically, the proposed approach involves feature extraction using the Fine DenseNet model, which excels at capturing intricate and multidimensional characteristics from handwriting samples due to its dense connectivity and deep architecture. These features are then input into the Adaptive Multidimensional Long Short-Term Memory-Connectionist Temporal Classification (AMDLSTM-CTC) model, a hybrid architecture that integrates MDLSTM, attention mechanisms, and a CTC layer. While lightweight CNNs and RNNs might be considered for their simplicity and efficiency, they may not capture the detailed and nuanced information essential for accurate personality classification. The Fine DenseNet and AMDLSTM-CTC models are chosen specifically for their ability to handle complex temporal dependencies and extract critical features from handwriting data. The proposed model achieves impressive results with an accuracy of 97.6%, specificity of 98.50%, and precision of 92.26%, demonstrating its effectiveness over existing methods. This indicates its potential to significantly enhance personality classification accuracy in Human Personality Recognition applications.}
}
@article{FRADE2021102186,
title = {Advertising in streaming video: An integrative literature review and research agenda},
journal = {Telecommunications Policy},
volume = {45},
number = {9},
pages = {102186},
year = {2021},
issn = {0308-5961},
doi = {https://doi.org/10.1016/j.telpol.2021.102186},
url = {https://www.sciencedirect.com/science/article/pii/S0308596121000902},
author = {João Lucas Hana Frade and Jorge Henrique Caldeira de Oliveira and Janaina de Moura Engracia Giraldi},
keywords = {Streaming video advertising, Online video advertising, YouTube advertising, Integrative literature review, Research agenda},
abstract = {Streaming video (SV), such as YouTube, is a new media widely used nowadays. Nevertheless, there is a lack of knowledge about advertising in SV. In view of this, through a search in the rich depository of the Scopus database, this article presents the first integrative literature review about advertising in SV. Searching every article and conference paper related to the topic published until May 04, 2020, 59 studies were found and classified into two main topics: marketing studies (35), mostly focused on evaluating or exploring advertising in SV, and computational studies (24), focusing on the development of systems for the insertion of ads into SV. All knowledge present in these studies was summarized so that readers (both scholars and practitioners) could have easy access to the main contributions and information present in each study. Moreover, future research directions in six main themes are presented through a research agenda.}
}
@article{LU2024101454,
title = {The application of educational technology to develop problem-solving skills: A systematic review},
journal = {Thinking Skills and Creativity},
volume = {51},
pages = {101454},
year = {2024},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101454},
url = {https://www.sciencedirect.com/science/article/pii/S1871187123002213},
author = {Dan Lu and Ya-Nan Xie},
keywords = {Technological tools, Problem-solving skills, Systematic review},
abstract = {Fostering problem-solving skills is vital for students to tackle the complexities of the 21st century. The integration of educational technology has spurred scholarly interest in cultivating students’ problem-solving skills. A systematic examination of these scholarly pursuits can yield invaluable insights and serve as a reference for both researchers and practitioners. In this context, the current study conducted a systematic review, aiming to answer the following questions: (1) What and how are educational technological tools applied to promote problem-solving skills? (2) What instructional strategies are used? (3) What is the impact of the application on students’ development of problem-solving skills? Web of Science was the database used to search literature, and 69 articles were included after the screening procedure. After analyzing the included studies, key findings and implications include: (1) Technological tools deployed in these studies serve diverse functions, often combined to establish immersive learning environments, which indicates that incorporating diverse cognitive and practical technology elements into curriculum design is essential for students’ successful problem-solving; (2) Instructional strategies like scaffolding, guidance, tool training, course orientation, and peer collaboration are employed to facilitate students’ problem-solving proficiencies. This finding stresses the significance of a comprehensive approach to developing problem-solving skills, involving guidance, practical training, situational awareness, and collaboration; (3) The incorporation of educational tools in purposefully designed instruction has a positive impact on students’ problem-solving skills. Consequently, educators and institutions are urged to strategically embrace these tools to foster students’ problem-solving abilities.}
}
@article{ELZANFALY201579,
title = {[I3] Imitation, Iteration and Improvisation: Embodied interaction in making and learning},
journal = {Design Studies},
volume = {41},
pages = {79-109},
year = {2015},
note = {Special Issue: Computational Making},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2015.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X1500071X},
author = {Dina El-Zanfaly},
keywords = {computational making, design education, design technology, interaction design, reflective practice},
abstract = {I introduce in this paper a new learning and making process that fosters a new ability to make things through the body's direct, iterative engagement with materials, tools, machines and objects. Tested in a variety of educational settings, this method, which I call ‘I3’ for its three-layer operation of ‘Imitation, Iteration and Improvisation’, allows learners to develop their sensory experiences to improvise and create on their own. I introduce case studies in order to test I3. I challenge the separation of design and construction often reinforced by the use of digital fabrication. I show that learning to make and learning from making emerge together through a situated and embodied interaction among the learner, the materials, the tools and the object in-the-making.}
}
@article{LEHRACH201126,
title = {ITFoM – The IT Future of Medicine},
journal = {Procedia Computer Science},
volume = {7},
pages = {26-29},
year = {2011},
note = {Proceedings of the 2nd European Future Technologies Conference and Exhibition 2011 (FET 11)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2011.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S1877050911006776},
author = {Hans Lehrach and Ralf Subrak and Peter Boyle and Markus Pasterk and Kurt Zatloukal and Heimo Müller and Tim Hubbard and Angela Brand and Mark Girolami and Daniel Jameson and Frank J. Bruggeman and Hans V. Westerhoff},
keywords = {nonlinear information and communication technology, distributive computing, high throughput data analysis, personalized medicine, systems medicine, healthcare revolution: virtual human},
abstract = {Molecular medicine is undergoing a revolution, creating a data fog that may obscure understanding. The functioning human is analogous to a biological factory controlled by an incredibly complex Information and Communication (IC) network. It is proposed that 7 billion computational replicas be made of those 7 billion human IC networks to enable interrogation and manipulation, for understanding and personalized healthcare. This requires a revolutionary ICT that follows the organization of the biological information and communication flows, with implications for hardware, software and connectivity.}
}
@incollection{2024xxv,
title = {Preface},
editor = {Jordan Henry Grafman},
booktitle = {Encyclopedia of the Human Brain (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {xxv},
year = {2024},
isbn = {978-0-12-820481-8},
doi = {https://doi.org/10.1016/B978-0-12-820480-1.05001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204801050014}
}
@article{COBELLI1984291,
title = {A model of glucose kinetics and their control by insulin, compartmental and noncompartmental approaches},
journal = {Mathematical Biosciences},
volume = {72},
number = {2},
pages = {291-315},
year = {1984},
issn = {0025-5564},
doi = {https://doi.org/10.1016/0025-5564(84)90114-7},
url = {https://www.sciencedirect.com/science/article/pii/0025556484901147},
author = {Claudio Cobelli and Gianna Toffolo and Eleuterio Ferrannini},
abstract = {Compartmental and noncompartmental models are used to quantify, from multiple steady-state tracer experiments, glucose kinetics and the effect of insulin upon them. Some aspects of experiment design are discussed. A physiological three-compartment model of glucose kinetics is proposed which provides a new quantitative picture of insulin control of glucose metabolism. Noncompartmental modeling is shown to have structural errors which prevent physiological insight. Compartmental models make a better use of the informational content of kinetic data, even if more demanding both in terms of modeling and computational effort and in terms of physiological thinking.}
}
@article{CHASTAIN2002237,
title = {Square peg in a round hole or horseless carriage? Reflections on the use of computing in architecture},
journal = {Automation in Construction},
volume = {11},
number = {2},
pages = {237-248},
year = {2002},
note = {ACADIA '99},
issn = {0926-5805},
doi = {https://doi.org/10.1016/S0926-5805(00)00095-9},
url = {https://www.sciencedirect.com/science/article/pii/S0926580500000959},
author = {Thomas Chastain and Yehuda E Kalay and Christopher Peri},
keywords = {Square peg in a round hole, Horseless carriage, Architecture},
abstract = {We start with two paradigms that have been used to describe the relationship of computation methods and tools to the production of architecture. The first is that of forcing a square peg into a round hole — implying that the use of a tool is misdirected, or at least poorly fits the processes that have traditionally been part of an architectural design practice. In doing so, the design practice suffers from the use of new technology. The other paradigm describes a state of transformation in relationship to new technology as a horseless carriage in which the process is described in obsolete and ‘backward’ terms. The implication is that there is a lack of appreciation for the emerging potentials of technology to change our relationship with the task. The paper demonstrates these two paradigms through the invention of drawings in the 14th Century, which helped to define the profession of architecture. It then goes on to argue that modern computational tools follow the same paradigms, and like drawings, stand to bring profound changes to the profession of architecture as we know it.}
}
@article{POHJONEN2018244,
title = {Modelling of austenite transformation along arbitrary cooling paths},
journal = {Computational Materials Science},
volume = {150},
pages = {244-251},
year = {2018},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2018.03.052},
url = {https://www.sciencedirect.com/science/article/pii/S0927025618302052},
author = {Aarne Pohjonen and Mahesh Somani and David Porter},
keywords = {Phase transformations, Bainite, Martensite, Thermomechanical processing, Steel},
abstract = {A computational model based on the Johnson-Mehl-Avrami-Kolmogorov equation for simulating the onset and kinetics of austenite to bainite and martensite transformation has been fitted to experimental continuous cooling data for two different steels. We investigated how deformation below recrystallization temperature affected the transformation onset and kinetics in comparison to the same steel in the undeformed state. The fitted model can be used to simulate phase transformations occurring when the steel is cooled along any cooling path. The model can be fully coupled to heat transfer and conduction simulations in order to optimize cooling practice, for example in industrial thermomechanical processing of steel. The fitted model can also be used to predict the hardness of the steel after cooling.}
}
@article{PAYANDEH2006328,
title = {TOWARD UNIFICATION OF CONSTRAINED MECHANICS AND VIRTUAL FIXTURES IN HAPTIC RENDERING},
journal = {IFAC Proceedings Volumes},
volume = {39},
number = {15},
pages = {328-333},
year = {2006},
note = {8th IFAC Symposium on Robot Control},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20060906-3-IT-2910.00056},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016385354},
author = {Shahram Payandeh and Pierre Joli and Zheng Feng},
keywords = {constrained mechanics, virtual fixtures (VF), computational mechanics, haptic rendering},
abstract = {Virtual Fixtures (VF) are defined as haptic or visual aids in assisting users of virtual and tele-operation environments during various tasks. They have been mostly defined similar to physical fixtures which are used in performing daily tasks such as holding-on to the railing when climbing the stairs or using a ruler to draw straight lines. Such analogous definitions can be even extended to the case of learning certain manual tasks under supervision, e.g. when one holds the child's hand in order to teach the association between the mouse movements and positions of cursor on the screen. This paper presents how notions from the computational mechanics for solving constrained dynamical systems can be extended for stable implementation of a class of VF in a haptic rendered environment. It is shown that some of the solution methodologies from computational mechanics can have direct implication in haptic rendering of stiff-constrained environment. Examples of such implementations are also presented using a haptic device interacting with a class of VF.}
}
@article{AMATI2007358,
title = {On the emergence of modern humans},
journal = {Cognition},
volume = {103},
number = {3},
pages = {358-385},
year = {2007},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2006.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0010027706000710},
author = {Daniele Amati and Tim Shallice},
keywords = {Prefrontal cortex, Latching, Paleoanthropology, Human evolution},
abstract = {The emergence of modern humans with their extraordinary cognitive capacities is ascribed to a novel type of cognitive computational process (sustained non-routine multi-level operations) required for abstract projectuality, held to be the common denominator of the cognitive capacities specific to modern humans. A brain operation (latching) that allows this novel computational process is proposed as well as a physics-inspired mechanism that could explain its rather recent emergence without invoking unlikely genetic or structural changes.}
}
@article{PALHARESDEMELO200121,
title = {Recommendation for fertilizer application for soils via qualitative reasoning},
journal = {Agricultural Systems},
volume = {67},
number = {1},
pages = {21-30},
year = {2001},
issn = {0308-521X},
doi = {https://doi.org/10.1016/S0308-521X(00)00044-5},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X00000445},
author = {L.A.M. {Palhares de Melo} and D.J. Bertioli and E.V.M. Cajueiro and R.C. Bastos},
keywords = {Fertilizer application, Qualitative reasoning, Approximate reasoning},
abstract = {In Brazil, liming and fertilization are essential practices in agriculture due to the soils being acidic and poor in nutrients. Distinct decision tables that serve as support for recommendation of fertilizer application are used, but one of their features is that they are based on a strictly quantitative analysis of input variables. This sometimes causes dificulties in their use when calculating the output (recommended fertilizer application). This work presents a model for the use and interpretation of decision tables for fertilizer application. It is based on a qualitative characterization for the rules and input variables used. The results have shown that this approach gives feasible results which more accurately reflect human thinking about the decision table.}
}
@incollection{JAIPALJAMANI2023103,
title = {Makerspace and robotics as/for STEM education},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {103-111},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.13034-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305130349},
author = {Kamini Jaipal-Jamani},
keywords = {STEM, Integrated STEM education, Robotics, Makerspace, K-12 education, Informal education},
abstract = {STEM is recognized as a sustainable strategy in education to promote global economic development. However, the goals for STEM education encompass broader outcomes such as STEM literacy. This article positions STEM Education as an integrated field and argues for interdisciplinary and transdisciplinary approaches. The integrated STEM approach does present challenges for implementation such as practitioners having insufficient knowledge of STEM disciplines. Robotics and Makerspace are suggested as viable strategies for integrated STEM education. Studies indicate these two approaches provide a relevant context in informal and K-12 educational settings to show connections between STEM subjects and promote learning of STEM content.}
}
@article{ZHANG2022105421,
title = {An efficient smoothness indicator mapped WENO scheme for hyperbolic conservation laws},
journal = {Computers & Fluids},
volume = {240},
pages = {105421},
year = {2022},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2022.105421},
url = {https://www.sciencedirect.com/science/article/pii/S0045793022000846},
author = {Xin Zhang and Chao Yan and Feng Qu},
keywords = {WENO, Smoothness indicators, Mapping process, Hyperbolic conservation laws},
abstract = {The mapping function method is a common approach to improve the accuracy of WENO type schemes. However, with the demand of the accuracy improvement higher, the mapping function becomes more and more complex, and the calculation cost also increases significantly. In this study, a novel smoothness indicator mapping method called WENO-ISM is proposed based on the WENO scheme. It employs a simple function which amplifies the original smoothness to improve the scheme's accuracy. In addition to the difference of the mapping function, the superior innovation of the designed scheme is that the mapping object is changed to the smoothness indicator instead of the nonlinear weight calculated from the WENO-JS scheme. The underlying idea of this method is to use this mapping function to directly regard the optimal weight of the stencil in the relatively smooth region as the nonlinear weight. For these, the calculation cost can be greatly reduced without loss of accuracy. In the meanwhile, the simple mapping function itself is with higher accuracy and lower cost. Numerical experiments with one-dimensional linear advection and ADR analysis show that this scheme is superior to other mapped WENO schemes in accuracy and especially in computation cost. Furthermore, improved results of this scheme are obtained by several typical one-dimensional and two-dimensional numerical cases.}
}
@incollection{CH202421,
title = {Chapter Two - The chemometric models in metabolomics},
editor = {Ratnasekhar Ch},
series = {Comprehensive Analytical Chemistry},
publisher = {Elsevier},
volume = {104},
pages = {21-42},
year = {2024},
booktitle = {Metabolomics in Health and Disease Biology},
issn = {0166-526X},
doi = {https://doi.org/10.1016/bs.coac.2023.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0166526X23001046},
author = {Ratnasekhar CH and Priya Rathor and Nicholas Birse},
keywords = {Multivariate analysis, PCA, PLS-DA, OPLS-DA, KNN},
abstract = {The metabolomic analysis provides a powerful approach for delving into intricate biological metabolism. Given metabolomics data's complexity and high-dimensional nature, it necessitates applying advanced analytical techniques for meaningful interpretation. This chapter centers on the pivotal role of chemometric tools in metabolomic analysis. These tools encompass a wide range of statistical and computational methods that empower us to extract valuable insights from extensive and intricate metabolomics datasets. They play a itical role in tasks such as data preprocessing, noise reduction, feature selection, and multivariate analysis, thereby enhancing our ability to unveil biologically relevant information. Moreover, these tools facilitate the integration of data from diverse analytical platforms, allowing researchers to identify and validate metabolites that are indicative of specific biological conditions. Additionally, chemometric methods aid in elucidating metabolic pathways and exploring interactions among metabolites, shedding light on the underlying biology. Given the intricacies involved, it is ucial to utilize specific analytical tools, including but not limited to Principal Component Analysis (PCA), Partial Least Squares Projection to Latent Structures (PLS), permutation tests, Random Forest, and K-Nearest Neighbors (KNN). Hence, this chapter is dedicated to navigating the multifaceted landscape of multivariate metabolomics analysis, highlighting both its advantages and limitations.}
}
@article{CORDELL20231442,
title = {Milligrams to kilograms: making microbes work at scale},
journal = {Trends in Biotechnology},
volume = {41},
number = {11},
pages = {1442-1457},
year = {2023},
issn = {0167-7799},
doi = {https://doi.org/10.1016/j.tibtech.2023.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S016777992300149X},
author = {William T. Cordell and Gennaro Avolio and Ralf Takors and Brian F. Pfleger},
keywords = {scale down, genome minimization, cell heterogeneity, strain engineering, computational fluid dynamics, bioprocess engineering},
abstract = {If biomanufacturing can become a sustainable route for producing chemicals, it will provide a critical step in reducing greenhouse gas emissions to fight climate change. However, efforts to industrialize microbial synthesis of chemicals have met with varied success, due, in part, to challenges in translating laboratory successes to industrial scale. With a particular focus on Escherichia coli, this review examines the lessons learned when studying microbial physiology and metabolism under conditions that simulate large-scale bioreactors and methods to minimize cellular waste through reduction of maintenance energy, optimizing the stress response and minimizing culture heterogeneity. With general strategies to overcome these challenges, biomanufacturing process scale-up could be de-risked and the time and cost of bringing promising syntheses to market could be reduced.}
}
@article{LI2023133,
title = {Optimization of an axial coarse powder separator for low-density lignite based on the optimal seeking method},
journal = {Particuology},
volume = {79},
pages = {133-142},
year = {2023},
issn = {1674-2001},
doi = {https://doi.org/10.1016/j.partic.2022.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S1674200122002401},
author = {Haixia Li and Zhiheng Song and Anchao Zhang and Zhijun Sun and Leying Jin},
keywords = {Lignite particle, Computational fluid dynamics, Separator, Optimal seeking method, Isokinetic sampling},
abstract = {As a major equipment for thermal power plants, the main function of coarse powder separators is to separate particles into size groups according to particle diameter. With the rising coal prices, power plants are using more low-density lignite. Consequently, the original equipment does not operate under normal conditions. Therefore, to return the equipment to normal operating conditions, the isokinetic sampling method, computational fluid dynamics (CFD), and discrete phase model (DPM) based on the Euler–Lagrange method are applied in this research to analyze the effect of baffle shapes, baffle numbers, and centrifugal blades on the equipment performance index, like R90. The shape of the baffle plate was optimized and improved. Preferential method was applied to determine the optimal number of baffles to ensure normal working conditions. Results show that curved baffles can suppress the axial negative gradient field below themselves better than rectangular baffles. The curved baffles selected by the optimal seeking method make full use of the negative axial gradient field and provide the particles entering the separation zone at tangential velocity in advance with the addition of centrifugal blades. Thus, pre-separation can be realized. The R90 is 5 at the separator outlet and 95 at the powder return port, indicating that the output capacity of the separator returned to a reasonable operating performance.}
}
@article{SOLTOGGIO201848,
title = {Born to learn: The inspiration, progress, and future of evolved plastic artificial neural networks},
journal = {Neural Networks},
volume = {108},
pages = {48-67},
year = {2018},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018302120},
author = {Andrea Soltoggio and Kenneth O. Stanley and Sebastian Risi},
keywords = {Artificial neural networks, Lifelong learning, Plasticity, Evolutionary computation},
abstract = {Biological neural networks are systems of extraordinary computational capabilities shaped by evolution, development, and lifelong learning. The interplay of these elements leads to the emergence of biological intelligence. Inspired by such intricate natural phenomena, Evolved Plastic Artificial Neural Networks (EPANNs) employ simulated evolution in-silico to breed plastic neural networks with the aim to autonomously design and create learning systems. EPANN experiments evolve networks that include both innate properties and the ability to change and learn in response to experiences in different environments and problem domains. EPANNs’ aims include autonomously creating learning systems, bootstrapping learning from scratch, recovering performance in unseen conditions, testing the computational advantages of particular neural components, and deriving hypotheses on the emergence of biological learning. Thus, EPANNs may include a large variety of different neuron types and dynamics, network architectures, plasticity rules, and other factors. While EPANNs have seen considerable progress over the last two decades, current scientific and technological advances in artificial neural networks are setting the conditions for radically new approaches and results. Exploiting the increased availability of computational resources and of simulation environments, the often challenging task of hand-designing learning neural networks could be replaced by more autonomous and creative processes. This paper brings together a variety of inspiring ideas that define the field of EPANNs. The main methods and results are reviewed. Finally, new opportunities and possible developments are presented.}
}
@article{PATTON20051082,
title = {The role of scanning in open intelligence systems},
journal = {Technological Forecasting and Social Change},
volume = {72},
number = {9},
pages = {1082-1093},
year = {2005},
note = {New Horizons and Challenges for Future-Oriented Technology Analysis: The 2004 EU-US Seminar},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2004.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0040162504001441},
author = {Kermit M. Patton},
keywords = {Scanning process, Open intelligence systems, SRIC-BI},
abstract = {Every month, SRI Consulting Business Intelligence (SRIC-BI) professionals assemble more than 100 short abstracts of developments that they perceive to be signals of change, discontinuities, inflection points, outliers, or disruptive developments. The effort is part of a continuous scanning process and Scan program that allows SRIC-BI to gauge the ongoing turbulent confluence of culture, commerce, and technology that defines today's business environment. For more than 25 years, scanning has played an essential role in SRIC-BI's and SRI International's foresight capabilities by providing a systematic means for surveying the broad external environment for change vectors. Traditional monitoring processes in most organizations are largely arbitrary, depending on what concerned individuals or leaders in the organization are reading, thinking about, and sharing informally with each other. But in today's world, arbitrary is insufficient. No foresight function can operate with confidence without a disciplined process for spotting new patterns of change and bringing those issues into the organization for early consideration and action. This article describes the scanning process as SRIC-BI practices it, the importance of open intelligence systems, what benefits the scanning process can provide to organizations, and what problems organizations typically run into when setting up scanning systems.}
}
@article{DU2023106421,
title = {Resilience and sustainability-informed probabilistic multi-criteria decision-making framework for design solutions selection},
journal = {Journal of Building Engineering},
volume = {71},
pages = {106421},
year = {2023},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2023.106421},
url = {https://www.sciencedirect.com/science/article/pii/S2352710223006009},
author = {Jiajun Du and Wei Wang and Ting Lou and Hongyu Zhou},
keywords = {Building holistic design, Seismic resilience, Sustainability, Life-cycle thinking, Multi-criteria decision-making, Monte Carlo simulation},
abstract = {In this paper, a novel probabilistic multi-criteria decision-making (MCDM) framework for sustainable and resilient building design solutions selection is developed to optimize alternatives or rehabilitation strategies in a life cycle context. The proposed framework holistically takes into account the buildings' resilience metrics, environmental sustainability, and energy consumption. The life cycle economic cost, global warming potential (GWP), primary energy use (PEU), and social impact are used to represent the buildings’ resilience and sustainability. The second-generation Performance-based Earthquake Engineering procedure and FEMA P-58 method for time-based seismic performance analyses are used to evaluate earthquake-induced impact. The whole-building energy modeling (BEM) is adopted to assess energy use during the operational phase. In addition, life cycle analysis (LCA) and life cycle cost analysis (LCCA) are carried out to assess the life cycle performance of different design alternatives regarding environmental impacts and cost, respectively. Aiming to consider the various uncertainties in the decision-making process, a novel MCS-MCDM method is developed by combining the Monte Carlo simulation (MCS) with the traditional MCDM method. The proposed framework is applied to analyze hypothetical cases considering different geographic locations, structural systems, and building envelope systems, and recommendations are given according to the analysis results. The case study demonstrates the advantages of the proposed framework over traditional frameworks and its guidance for architectural design decisions.}
}
@article{DUMONTHEIL20101574,
title = {Taking perspective into account in a communicative task},
journal = {NeuroImage},
volume = {52},
number = {4},
pages = {1574-1583},
year = {2010},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2010.05.056},
url = {https://www.sciencedirect.com/science/article/pii/S1053811910007895},
author = {Iroise Dumontheil and Olivia Küster and Ian A. Apperly and Sarah-Jayne Blakemore},
keywords = {Theory of mind, Perspective taking, Social brain, Social cognition, Decision making, Inhibition},
abstract = {Previous neuroimaging studies of spatial perspective taking have tended not to activate the brain's mentalising network. We predicted that a task that requires the use of perspective taking in a communicative context would lead to the activation of mentalising regions. In the current task, participants followed auditory instructions to move objects in a set of shelves. A 2×2 factorial design was employed. In the Director factor, two directors (one female and one male) either stood behind or next to the shelves, or were replaced by symbolic cues. In the Object factor, participants needed to use the cues (position of the directors or symbolic cues) to select one of three possible objects, or only one object could be selected. Mere presence of the Directors was associated with activity in the superior dorsal medial prefrontal cortex (MPFC) and the superior/middle temporal sulci, extending into the extrastriate body area and the posterior superior temporal sulcus (pSTS), regions previously found to be responsive to human bodies and faces respectively. The interaction between the Director and Object factors, which requires participants to take into account the perspective of the director, led to additional recruitment of the superior dorsal MPFC, a region activated when thinking about dissimilar others' mental states, and the middle temporal gyri, extending into the left temporal pole. Our results show that using perspective taking in a communicative context, which requires participants to think not only about what the other person sees but also about his/her intentions, leads to the recruitment of superior dorsal MPFC and parts of the social brain network.}
}
@article{MISCHKOWSKI201885,
title = {Think it through before making a choice? Processing mode does not influence social mindfulness},
journal = {Journal of Experimental Social Psychology},
volume = {74},
pages = {85-97},
year = {2018},
issn = {0022-1031},
doi = {https://doi.org/10.1016/j.jesp.2017.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0022103117302858},
author = {Dorothee Mischkowski and Isabel Thielmann and Andreas Glöckner},
keywords = {Social mindfulness, Processing mode, Intuition versus deliberation, Spontaneous cooperation, Prosocial personality},
abstract = {Social mindfulness has recently been introduced as a type of prosocial behavior that emphasizes the importance of a skill to see other people's needs beyond the will to act accordingly. Correspondingly, social mindfulness has been proposed to involve processes of executive functioning and thus of deliberate thinking. In four studies, we tested the influence of processing mode on social mindfulness using different experimental manipulations (i.e., instructions to decide intuitively vs. deliberately, time pressure, and cognitive load). Contrary to the idea that social mindfulness requires conscious processing – and unlike recent findings suggesting intuitive cooperation – we consistently found negligible effect sizes for the influence of processing mode on social mindfulness. This was observable for both, prosocial and selfish individuals alike (i.e., those with high vs. low levels in Social Value Orientation or Honesty-Humility, respectively). Overall, the findings suggest that social mindfulness constitutes a general tendency to perceive and act prosocially in social situations that is unaffected by processing mode and, by implication, distinguishable from other types of prosocial behavior.}
}
@article{SCARR2024111257,
title = {Moving beyond Vesalius: Why anatomy needs a mapping update},
journal = {Medical Hypotheses},
volume = {183},
pages = {111257},
year = {2024},
issn = {0306-9877},
doi = {https://doi.org/10.1016/j.mehy.2023.111257},
url = {https://www.sciencedirect.com/science/article/pii/S0306987723002530},
author = {Graham Scarr and Leonid Blyum and Stephen M Levin and Susan Lowell {de Solórzano}},
keywords = {Biomechanics, Cellular, Compartment, Fascia, Parenchyma, Tissue},
abstract = {The improvement of the human condition is the driver behind a vast amount of ongoing research and naturally employs the most up-to-date methods in its endeavours. It has contributed greatly to our understanding of the body and benefitted our healthcare systems in remarkable ways, but there is a problem. The mapping of anatomy to its physiological functions is essentially derived from the work of Vesalius and traditionally favoured mobility over stability, and as a consequence has allowed the entrenched and simplifying assumptions of the musculoskeletal duality to persist to the present day, despite advances in technology. The lever model of motion, for example, assumes that the body is an intrinsically unstable system that requires an external controller (e.g. neural) to provide the necessary ‘catch-up’ stability for transient muscular latencies, and it is likely that the vulnerabilities inherent within such a mechanism would severely compromise living tissues. The foundational biomechanical assumptions of steady-state forces and kinematics has meant that the disproportionate and potentially damaging consequences of transient peak loadings have been largely overlooked, and which added to the long healing times required for post-traumatic recovery, suggests that such a mechanism would lead to material fatigue and destructive tissue failure. The musculoskeletal duality, however, was not always so dominant but conceptually rivalled in the 17th and 18th centuries by Hooke’s ‘cells’ and Malpighi’s ‘cellular tissues’, both of which have been largely forgotten but now deserve a re-evaluation. The definition of the term ‘cell’ as a small compartment within a larger structure had quite different connotations then than it does today, but this compartmental aspect of connective tissue anatomy gradually faded and is now only recognized for its pathological significance. This paper examines musculoskeletal anatomy from both historical and more recent viewpoints and highlights the concept of the fascial system as a distinct and intrinsically stable functional entity. It is a perspective that enables every anatomical ‘part’ to be included within a ‘cellular’ framework that differs substantially from the mobility-driven machine model: a tensioned fibrous network encompassing a complex heterarchy of regionally specialized compartments under compression, each of which has its own physical and parenchyma-driven characteristics that contribute to the functional whole. In other words, an updated fascia-centric interpretation of architectural anatomy that maps muscles and bones in a substantially different way from traditional models, renders the term musculoskeletal obsolete and greatly expands the meaning of compartment syndrome.}
}
@article{LI2024e30770,
title = {On the correlation among the students’ epistemic cognition, academic emotions, and academic achievement in higher education},
journal = {Heliyon},
volume = {10},
number = {9},
pages = {e30770},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e30770},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024068014},
author = {Xiangyang Li and Zhiyong Liu and Linchong Ji},
keywords = {Academic emotions, Epistemic cognition, Academic achievement, Chinese undergraduate students, Higher education},
abstract = {Students' academic achievement relies on a variety of pedagogical, affective, and individual factors. The investigation of academic emotions and epistemic cognition has been a focal point in existing research. Previous studies have predominantly delved into the essence of students' epistemic cognition and academic emotions. Nonetheless, the correlation between the epistemic cognition, academic emotions, and academic success of Chinese undergraduate students remains inadequately explored. This research delves into the interconnectedness of these variables and examines which facets of epistemic cognition and academic emotions can forecast students' academic performance. A total of three hundred and eighty (380) Chinese undergraduate students were chosen via random sampling for this study. Their self-reported academic achievements were taken into account. Additionally, they completed questionnaires tailored to evaluate their epistemic cognition and academic emotions. The participants' scores underwent Pearson correlation and multiple regression analyses. The findings indicate that positive emotions correlate positively, while negative emotions correlate negatively with students' academic success. Furthermore, positive emotions and three categories of epistemic cognition were found to be predictors of students' academic accomplishments. In conclusion, it is deduced that both epistemic cognition and positive emotions play a role in enhancing students' academic success. The implications of these findings extend to educational psychologists, educators, and students, both theoretically and practically.}
}
@article{BROOKS202440,
title = {‘The current situation is a golden opportunity’},
journal = {New Scientist},
volume = {261},
number = {3475},
pages = {40-43},
year = {2024},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(24)00182-9},
url = {https://www.sciencedirect.com/science/article/pii/S0262407924001829},
author = {Michael Brooks},
abstract = {Our model of the evolution of the universe is an amazing achievement. But astronomical anomalies point the way to a deeper theory, if not a complete one, cosmologist Jim Peebles tells Michael Brooks}
}
@article{BHARDWAJ2022103712,
title = {Internet of things based smart city design using fog computing and fuzzy logic},
journal = {Sustainable Cities and Society},
volume = {79},
pages = {103712},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2022.103712},
url = {https://www.sciencedirect.com/science/article/pii/S2210670722000440},
author = {Kartik Krishna Bhardwaj and Siddhant Banyal and Deepak Kumar Sharma and Waleed Al-Numay},
keywords = {Internet of things, Opportunistic networks, Smart city architecture, Edge computing, Fog computing, Firefly algorithm, Fuzzy logic, Nature inspired algorithm, Probabilistic routing},
abstract = {The prevalence of IoT based technology in contemporary society allows for the planning of smart city infrastructure with individually owned devices as participating nodes in a social smart network. Edge computing application into such an IoT network facilitates the distributed computing framework by bringing the computational application closer to the end-users in form of Fog sub-networks, comprising the local participating end-user IoT nodes as service requesters and specialized servers catering to the requests, modelled for the infrastructure category they are delivering (Smart healthcare, ITS, Smart architecture, etc.). Opportunistic-based last-mile connectivity between the Fog servers and participating nodes aides the dynamicity of the topology for efficient information. Aiding the same, the proposed edge computing enabled, sustainable, opportunistic IoT-based intuitive data exchange, collection and operation scheme that employs a dynamic routing procedure that considers human interactions, their movement patterns and their decision-making process. The proposed scheme employs probabilistic history-based parameters that keep track of node interactions. The interactions are modelled on the Firefly PSO, allowing exploitation of the social mobility. The Fuzzy logic is used for hop decision making based on the social and attribute-based parameters which mimic intuitive and pragmatic decision making of humans in terms of social interactions and mobility.}
}
@article{BELL200163,
title = {Futures studies comes of age: twenty-five years after The limits to growth},
journal = {Futures},
volume = {33},
number = {1},
pages = {63-76},
year = {2001},
issn = {0016-3287},
doi = {https://doi.org/10.1016/S0016-3287(00)00054-9},
url = {https://www.sciencedirect.com/science/article/pii/S0016328700000549},
author = {Wendell Bell},
abstract = {Twenty-five years ago, the publication of The limits to growth marked a period of accomplishments in the futures field. Today, futures studies is experiencing another burst of development and is ready to move more fully into mainstream intellectual life and the standard educational curriculum. In addition to continued work on methods, theory, and empirical research, the resolution of three issues might help persuade established academic communities of the serious purposes and sound intellectual contributions of futurists. They are (1) the adoption of an adequate theory of knowledge (critical realism is proposed), (2) the recognition that prediction does play a role in futures studies (so we can deal explicitly with the philosophical challenges it poses), and (3) the formulation and justification of core values (so we have a valid basis by which to judge the desirability of alternative futures). I propose a critical discourse among futurists in order to resolve each issue. The desire to make futures thinking a part of everyone's education is not, of course, mere futurist chauvinism, but is based on the conviction that futures studies has important contributions to make to human well-being.}
}
@article{CANBAY2024111460,
title = {Predicting discriminative personality profile of haters from digital texts},
journal = {Knowledge-Based Systems},
volume = {287},
pages = {111460},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111460},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124000959},
author = {Pelin Canbay},
keywords = {Big five, Hate speech, MBTI, Personality computing, Profiling},
abstract = {The rate of malicious behavior in online social media is increasing daily. The increase in malignant social behaviors, especially those involving hate speech, makes it necessary to identify haters from digital texts as quickly and accurately as possible. Recently, many studies have been conducted to identify such behaviors; however, profiling the haters’ personality has not been paid sufficient attention. Retrieving the personality profiles of suspected haters from digital texts is one of the most effective ways to distinguish them from others. This study proposes a novel hatebase-aided personality recognition model that gives more successful results than plain recognition models and predicts the discriminative personality traits of online haters. The proposed model contains the combination of two effective sub-models; a deep neural network model, and a fine-tuned BERT model. While the deep neural network model trained with hate indicators provides an interpretable relation between personality and hate indicators, the fine-tuned BERT model provides relationships between text semantics and personality. Combining these sub-models, the proposed model gives hatebase-related personality recognition results. This study evaluates two popular personality models: the Big Five and the MBTI. According to experiments, compared with other users, online haters are less agreeable regarding the Big Five and fewer thinkers regarding the MBTI.}
}
@article{SMITH2017274,
title = {The hierarchical basis of neurovisceral integration},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {75},
pages = {274-296},
year = {2017},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2017.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S014976341630673X},
author = {Ryan Smith and Julian F. Thayer and Sahib S. Khalsa and Richard D. Lane},
keywords = {Neurovisceral integration, Cardiac vagal control, Heart rate variability, Interoception, Emotion, Predictive coding, Cognitive control},
abstract = {The neurovisceral integration (NVI) model was originally proposed to account for observed relationships between peripheral physiology, cognitive performance, and emotional/physical health. This model has also garnered a considerable amount of empirical support, largely from studies examining cardiac vagal control. However, recent advances in functional neuroanatomy, and in computational neuroscience, have yet to be incorporated into the NVI model. Here we present an updated/expanded version of the NVI model that incorporates these advances. Based on a review of studies of structural/functional anatomy, we first describe an eight-level hierarchy of nervous system structures, and the contribution that each level plausibly makes to vagal control. Second, we review recent work on a class of computational models of brain function known as “predictive coding” models. We illustrate how the computational dynamics of these models, when implemented within our proposed vagal control hierarchy, can increase understanding of the relationship between vagal control and both cognitive performance and emotional/physical health. We conclude by discussing novel implications of this updated NVI model for future research.}
}
@article{KOUTAMANIS199340,
title = {Computer vision in architectural design},
journal = {Design Studies},
volume = {14},
number = {1},
pages = {40-57},
year = {1993},
issn = {0142-694X},
doi = {https://doi.org/10.1016/S0142-694X(05)80004-3},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X05800043},
author = {Alexander Koutamanis and Vicky Mitossi},
keywords = {computer vision, visual design representations, computer-aided design and drafting},
abstract = {The visual representations traditionally used by architects form a primary source for the analysis and understanding of architectural design. Computerization and, in particular, computer vision offer a new opportunity for making explicit the general cognitive mechanisms and the domain knowledge involved in these representations, as well as for developing subsequently intelligent computer tools which support and facilitate the representation of design thinking and its products.}
}
@article{CHIRIMUUTA201934,
title = {Synthesis of contraries: Hughlings Jackson on sensory-motor representation in the brain},
journal = {Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences},
volume = {75},
pages = {34-44},
year = {2019},
issn = {1369-8486},
doi = {https://doi.org/10.1016/j.shpsc.2019.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S1369848618300359},
author = {M. Chirimuuta},
abstract = {This paper examines the concept of representation in the brain which occurs in the writings of the neurologist John Hughlings Jackson (1835–1911). Jackson was immersed in Victorian physiological psychology, a hybrid of British associationism and a reflex theory of the operation of the nervous system. Furthermore, Jackson was deeply influenced by Herbert Spencer, and I argue that Spencer's progressivist evolutionary ideas are in tension with the more mechanistic approach of the reflex theory. I also discuss Jackson's legacy in the 20th century and the longstanding debate about localisation of function in the brain.}
}
@article{VARATHAN202438,
title = {Role of different omics data in the diagnosis of schizophrenia disorder: A machine learning study},
journal = {Schizophrenia Research},
volume = {271},
pages = {38-46},
year = {2024},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2024.07.026},
url = {https://www.sciencedirect.com/science/article/pii/S0920996424003323},
author = {Aarthy Varathan and Suntharalingam Senthooran and Pratheeba Jeyananthan},
keywords = {Schizophrenia diagnosis, Transcriptome, DNA methylation, miRNA, Machine learning, Feature selection},
abstract = {Schizophrenia is a serious mental disorder that affects millions of people worldwide. This disorder slowly disintegrates thinking ability and changes behaviours of patients. These patients will show some psychotic symptoms such as hallucinations, delusions, thought disorder and movement disorder. These symptoms are in common with some other psychiatric disorders such as bipolar disorder, major depressive disorder and mood spectrum disorder. As patients would require immediate treatment, an on-time diagnosis is critical. This study explores the use of omics data in diagnosis of schizophrenia. Transcriptome, miRNA and epigenome data are used in diagnosis of patients with schizophrenia with the aid of machine learning algorithms. As the data is in high dimension, mutual information and feature importance are independently used for selecting relevant features for the study. Selected sets of features (biomarkers) are individually used with different machine learning algorithms and their performances are compared to select the best-performing model. This study shows that the top 140 miRNA features selected using mutual information along with support vector machines give the highest accuracy (0.86 ± 0.07) in the diagnosis of schizophrenia. All reported accuracies are validated using 5-fold cross validation. They are further validated using leave one out cross validation and the accuracies are reported in the supplementary material.}
}
@article{SCHWITZGEBEL202432,
title = {Strange… but true?},
journal = {New Scientist},
volume = {261},
number = {3483},
pages = {32-35},
year = {2024},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(24)00558-X},
url = {https://www.sciencedirect.com/science/article/pii/S026240792400558X},
author = {Eric Schwitzgebel},
abstract = {Theories of fundamental reality are inescapably weird, but that doesn't mean they are wrong – and we can all figure out how seriously to take them, says philosopher Eric Schwitzgebel}
}
@article{GRAZZINI201726,
title = {Bayesian estimation of agent-based models},
journal = {Journal of Economic Dynamics and Control},
volume = {77},
pages = {26-47},
year = {2017},
issn = {0165-1889},
doi = {https://doi.org/10.1016/j.jedc.2017.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S0165188917300222},
author = {Jakob Grazzini and Matteo G. Richiardi and Mike Tsionas},
keywords = {Agent-based, Estimation, Bayes, Approximate Bayesian computation, Likelihood},
abstract = {We consider Bayesian inference techniques for agent-based (AB) models, as an alternative to simulated minimum distance (SMD). Three computationally heavy steps are involved: (i) simulating the model, (ii) estimating the likelihood and (iii) sampling from the posterior distribution of the parameters. Computational complexity of AB models implies that efficient techniques have to be used with respect to points (ii) and (iii), possibly involving approximations. We first discuss non-parametric (kernel density) estimation of the likelihood, coupled with Markov chain Monte Carlo sampling schemes. We then turn to parametric approximations of the likelihood, which can be derived by observing the distribution of the simulation outcomes around the statistical equilibria, or by assuming a specific form for the distribution of external deviations in the data. Finally, we introduce Approximate Bayesian Computation techniques for likelihood-free estimation. These allow embedding SMD methods in a Bayesian framework, and are particularly suited when robust estimation is needed. These techniques are first tested in a simple price discovery model with one parameter, and then employed to estimate the behavioural macroeconomic model of De Grauwe (2012), with nine unknown parameters.}
}
@article{CARDONE202260,
title = {A fuzzy partition-based method to classify social messages assessing their emotional relevance},
journal = {Information Sciences},
volume = {594},
pages = {60-75},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.02.028},
url = {https://www.sciencedirect.com/science/article/pii/S002002552200161X},
author = {Barbara Cardone and Ferdinando {Di Martino} and Sabrina Senatore},
keywords = {Fuzzy partition, Emotional categories, Classification, TF-IDF, Fuzzy linguistic labels},
abstract = {With the surge of the large volume of data availability, Machine Learning and mainly Deep Learning techniques are the leading solutions in classification and predictive tasks, targeted at data-efficient learning. These models learn by training on many diversified samples in a process that is computationally expensive or time-consuming. Moreover, in many real-world scenarios, the amount of available data for training is unsuitable, because it is unlabeled or covers only portions of the whole reference domain cases. This paper proposes an alternative approach for document classification that leverages the distribution of the data projected in the multi-dimensional feature space to assess the weight of features in the final classification. The approach does not rely on traditional iterative methods for classification but builds a relevance measure to assess the relevance/importance of the features describing the domain of interest. The idea is to harness this metric to select relevant features and then express the values calculated by these metrics in natural language by exploiting fuzzy variables and linguistic labels to make human comprehension more immediate. The approach has been employed for emotion extraction from social media messages. The novelty of this approach is twofold: first, the well-known TF-IDF measure was reinterpreted as a relevance measure of emotions discovered in text content. Then, the discovered emotion relevance was described by fuzzy linguistic labels, defined on an ad-hoc-designed fuzzy partition, to express the data classification in natural language, more suitable to human understanding.}
}
@incollection{CARDONE2023205,
title = {11 - Emotion-based classification through fuzzy entropy-enhanced FCM clustering},
editor = {Tilottama Goswami and G.R. Sinha},
booktitle = {Statistical Modeling in Machine Learning},
publisher = {Academic Press},
pages = {205-225},
year = {2023},
isbn = {978-0-323-91776-6},
doi = {https://doi.org/10.1016/B978-0-323-91776-6.00010-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323917766000105},
author = {Barbara Cardone and Ferdinando {Di Martino} and Sabrina Senatore},
keywords = {EwFCM, FCM, Fuzzy clustering, Fuzzy entropy, Sentiment analysis, Twitter},
abstract = {In the era of the social web, microblogging is a popular social media that enables users to share their feelings, ideas, and comments about everyday-life events, trends, and advertisements. Great emphasis has been placed on the investigation of human behavior in social networks by analyzing user-generated content to capture feelings and emotions from natural language. Delving into human reactions to events from the social message streams allows understanding collective moods and preferences of web users and supporting market strategies as well as political consensus analysis. This paper proposes a novel approach to the emotion-based classification of microblogging messages such as Twitter. The classification method is unsupervised and exploits the well-known fuzzy c-means (FCM) clustering algorithm, proposing an enhanced version called entropy weighted FCM (shortly, EwFCM) that overcomes the main drawback of the FCM, viz., the sensitivity to the random cluster initialization by leveraging a fuzzy measure to evaluate the entropy in the data distribution. The fuzzy entropy measure allows minimizing the fuzziness of clustering, speeding up the convergence of the clustering algorithm. The proposed approach aims at analyzing microblogging-based trends to detect the main emotions expressed in the textual content, according to a well-known computational analysis of emotions whose main models are attributable to Ekman and Plutchik. The experiments reveal that the presented approach offers high accuracy in the emotion-based classification of textual resources; comparisons with the FCM algorithm applied for emotion classification show that the proposed method converges faster and provides promising classification performance, as evaluated by common metrics such as accuracy, precision, and F1-score.}
}
@article{TAHIR2023111837,
title = {Test flakiness’ causes, detection, impact and responses: A multivocal review},
journal = {Journal of Systems and Software},
volume = {206},
pages = {111837},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111837},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223002327},
author = {Amjed Tahir and Shawn Rasheed and Jens Dietrich and Negar Hashemi and Lu Zhang},
keywords = {Flaky tests, Non-deterministic tests, Test bugs, Software testing, Multivocal review},
abstract = {Flaky tests (tests with non-deterministic outcomes) pose a major challenge for software testing. They are known to cause significant issues, such as reducing the effectiveness and efficiency of testing and delaying software releases. In recent years, there has been an increased interest in flaky tests, with research focusing on different aspects of flakiness, such as identifying causes, detection methods and mitigation strategies. Test flakiness has also become a key discussion point for practitioners (in blog posts, technical magazines, etc.) as the impact of flaky tests is felt across the industry. This paper presents a multivocal review that investigates how flaky tests, as a topic, have been addressed in both research and practice. Out of 560 articles we reviewed, we identified and analysed a total of 200 articles that are focused on flaky tests (composed of 109 academic and 91 grey literature articles/posts) and structured the body of relevant research and knowledge using four different dimensions: causes, detection, impact and responses. For each of those dimensions, we provide categorization and classify existing research, discussions, methods and tools With this, we provide a comprehensive and current snapshot of existing thinking on test flakiness, covering both academic views and industrial practices, and identify limitations and opportunities for future research.}
}
@article{VIEIRA2024113859,
title = {What future for marine renewable energy in Portugal and Spain up to 2030? Forecasting plausible scenarios using general morphological analysis and clustering techniques},
journal = {Energy Policy},
volume = {184},
pages = {113859},
year = {2024},
issn = {0301-4215},
doi = {https://doi.org/10.1016/j.enpol.2023.113859},
url = {https://www.sciencedirect.com/science/article/pii/S0301421523004445},
author = {Mário Vieira and Ana Macedo and António Alvarenga and Marcos Lafoz and Isabel Villalba and Marcos Blanco and Rodrigo Rojas and Alejandro Romero-Filgueira and Adriana García-Mendoza and Miguel Santos-Herran and Marco Alves},
keywords = {Marine renewables, Scenarios, Forecast, Clustering, Morphological analysis},
abstract = {Marine renewables – which include mainly wave, tidal and current energy – have been hailed, for the past decades, as a potential solution to support the decarbonization of the society. Portugal and Spain have been traditionally avid for the testing and demonstration of such technologies, but the implementation of marine capacity is yet marginal, and there are many uncertainties regarding the future of the sector in the region. The main objective of this article is to show a future projection of marine renewable energies in both Iberian and Macaronesian regions for 2030 to research and technological development communities. To obtain this future projection, General Morphological Analysis and advanced clustering techniques have been used. The results are divided into five groups of potential scenarios, which vary significantly due to different political, social and technological parameters. The influence of variables such as innovation speed, infrastructure implementation, and comprehensive metocean data availability emerges as pivotal determinants shaping the sector's course. The knowledge from this systematization is expected to be used by researchers, technicians, governments or by any other agency involved in marine renewable energies in Spain and Portugal, as a guidance for their new projects and research lines.}
}
@article{ENRIZ2005163,
title = {The legacy of the past, the reality of the present and the hopes of the future},
journal = {Journal of Molecular Structure: THEOCHEM},
volume = {731},
number = {1},
pages = {163-172},
year = {2005},
issn = {0166-1280},
doi = {https://doi.org/10.1016/j.theochem.2004.10.048},
url = {https://www.sciencedirect.com/science/article/pii/S0166128004008553},
author = {R.D. Enriz},
keywords = {Computational medicinal chemistry, Molecular modelling, Rational drug design},
abstract = {This view point paper has attempted to penetrate a field of research in which the efforts of a large number of the most varied modern computational techniques are converging, that of computational medicinal chemistry. We have never been so close to realising the dream of the pharmaceutical industry: that we may be able to design new drugs from first principles. However, there are still enough unknown areas where computational medicinal chemists can successfully use their sagacity and creativity. A personal point of view on the present approaches aimed for a rational drug design is given here. Perspectives on future developments in this field are also outlined in this assessment.}
}
@article{MORRISON2024100209,
title = {Promising practices for online professional learning},
journal = {Computers and Education Open},
volume = {7},
pages = {100209},
year = {2024},
issn = {2666-5573},
doi = {https://doi.org/10.1016/j.caeo.2024.100209},
url = {https://www.sciencedirect.com/science/article/pii/S2666557324000491},
author = {Laura Morrison and Janette Hughes},
keywords = {Professional learning, Coding, Math, Making, Online learning},
abstract = {This study took place at the beginning of the COVID-19 pandemic when most schools worldwide were making the transition to online teaching and learning. Through this single-case study design, the study examined the learning experiences of a group of teachers engaged in interactive, inquiry-based professional learning focused on math, making and coding during a shift to emergency remote teaching. The primary objective was to identify promising practices for online professional learning (PL) focused on math and coding using a maker-pedagogies approach to teaching and learning, based on the teachers’ learning experiences. Study participants included 20 teachers from a rural school board in Northern Ontario, Canada. Findings indicated that the following may be considered as promising practices when developing and implementing virtual math and coding PL from a maker perspective. It is important to: a) balance sessions focused on specific math and coding content with more general sessions focused on learning the various maker-technology tools; b) include both synchronous and asynchronous learning opportunities for the variety of teachers involved in the learning; c) include collaborative learning in the teacher PL and a virtual platform that can support this type of social learning; d) ensure the PL sessions are on-going as opposed to one-off or isolated sessions. This research suggests that online professional learning sessions need to consider three elements: the teacher, the content, and the learning environment and offers important recommendations for future work in this area.}
}
@article{HOOD2012181,
title = {Systems Approaches to Biology and Disease Enable Translational Systems Medicine},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {10},
number = {4},
pages = {181-185},
year = {2012},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2012.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S1672022912000526},
author = {Leroy Hood and Qiang Tian},
keywords = {Systems biology, P4 medicine, Family genome sequencing, Targeted proteomics, Single-cell analysis},
abstract = {The development and application of systems strategies to biology and disease are transforming medical research and clinical practice in an unprecedented rate. In the foreseeable future, clinicians, medical researchers, and ultimately the consumers and patients will be increasingly equipped with a deluge of personal health information, e.g., whole genome sequences, molecular profiling of diseased tissues, and periodic multi-analyte blood testing of biomarker panels for disease and wellness. The convergence of these practices will enable accurate prediction of disease susceptibility and early diagnosis for actionable preventive schema and personalized treatment regimes tailored to each individual. It will also entail proactive participation from all major stakeholders in the health care system. We are at the dawn of predictive, preventive, personalized, and participatory (P4) medicine, the fully implementation of which requires marrying basic and clinical researches through advanced systems thinking and the employment of high-throughput technologies in genomics, proteomics, nanofluidics, single-cell analysis, and computation strategies in a highly-orchestrated discipline we termed translational systems medicine.}
}
@article{GIANNAKOS2022100469,
title = {‘Lots done, more to do’: The current state of interaction design and children research and future directions},
journal = {International Journal of Child-Computer Interaction},
volume = {33},
pages = {100469},
year = {2022},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2022.100469},
url = {https://www.sciencedirect.com/science/article/pii/S2212868922000125},
author = {Michail Giannakos and Panos Markopoulos and Juan Pablo Hourcade and Alissa N. Antle},
abstract = {Child–computer interaction (CCI)11In this editorial the term child–computer interaction is used as a synonym to interaction design and children. is a multidisciplinary field of research that concerns the phenomena surrounding the interaction between children and computational and communication technologies. During the last two decades, several endeavors and contributions have significantly furthered CCI research. The pace of children-relevant technological advances and establishment of CCI venues has increased CCI scholarship, and furthered our knowledge and understanding about the relationship between childhood and technology. This editorial article provides an introduction and overview of the special Issue on “review articles in child–computer​ interaction research”. The motivation of this special issue is to highlight review and survey papers that portray contemporary developments in the CCI literature. The contributions come from diverse contexts, cover a wide range of technological affordances, and address a variety of objectives and stakeholders (e.g., designers, learning scientists, policymakers, technologists). In this editorial, we present the background of CCI research, giving a brief overview of the contributions of the special issue, and conclude by highlighting potential emerging issues and challenges of the field.}
}
@article{VANRINSVELD201717,
title = {Mental arithmetic in the bilingual brain: Language matters},
journal = {Neuropsychologia},
volume = {101},
pages = {17-29},
year = {2017},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2017.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0028393217301756},
author = {Amandine {Van Rinsveld} and Laurence Dricot and Mathieu Guillaume and Bruno Rossion and Christine Schiltz},
keywords = {Mathematics, Neuroimaging, Bilingualism, Numerical cognition, Arithmetics},
abstract = {How do bilinguals solve arithmetic problems in each of their languages? We investigated this question by exploring the neural substrates of mental arithmetic in bilinguals. Critically, our population was composed of a homogeneous group of adults who were fluent in both of their instruction languages (i.e., German as first instruction language and French as second instruction language). Twenty bilinguals were scanned with fMRI (3T) while performing mental arithmetic. Both simple and complex problems were presented to disentangle memory retrieval occuring in very simple problems from arithmetic computation occuring in more complex problems. In simple additions, the left temporal regions were more activated in German than in French, whereas no brain regions showed additional activity in the reverse constrast. Complex additions revealed the reverse pattern, since the activations of regions for French surpassed the same computations in German and the extra regions were located predominantly in occipital regions. Our results thus highlight that highly proficient bilinguals rely on differential activation patterns to solve simple and complex additions in each of their languages, suggesting different solving procedures. The present study confirms the critical role of language in arithmetic problem solving and provides novel insights into how highly proficient bilinguals solve arithmetic problems.}
}
@article{JU2017180,
title = {Single image haze removal based on the improved atmospheric scattering model},
journal = {Neurocomputing},
volume = {260},
pages = {180-191},
year = {2017},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.04.034},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217307051},
author = {Mingye Ju and Zhenfei Gu and Dengyin Zhang},
keywords = {Improved atmospheric scattering model, Linear model, Gaussian–Laplacian pyramid, Image haze removal, Haze aware density feature},
abstract = {In this paper, we propose an improved atmospheric scattering model (IASM) to overcome the inherent limitation of the traditional atmospheric scattering model. Based on the IASM, a fast single image dehazing algorithm is also presented. In this algorithm, by constructing a linear model between the transmission and the haze aware density feature, the transmission map can be directly estimated through a linear operation on three components: luminance, saturation and gradient. Combining the sky-relevant feature and the proposed guided energy model (GEM), we can accurately estimate the atmospheric light and scene incident light, and can further restore the scene albedo via the IASM. Finally, an accelerating framework (AF) based on the Gaussian–Laplacian pyramid is proposed to increase the computational speed. Experimental results demonstrate that the proposed algorithm outperforms most of the prevalent algorithms in terms of visual effect and computational efficiency. Besides, it is also capable of processing various types of degraded images in addition to hazy images.}
}
@article{KUBE2020448,
title = {Rethinking post-traumatic stress disorder – A predictive processing perspective},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {113},
pages = {448-460},
year = {2020},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2020.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S0149763419311029},
author = {Tobias Kube and Max Berg and Birgit Kleim and Philipp Herzog},
keywords = {active inference, Bayesian brain, belief updating, expectation, post-traumatic stress disorder, predictive processing},
abstract = {Predictive processing has become a popular framework in neuroscience and computational psychiatry, where it has provided a new understanding of various mental disorders. Here, we apply the predictive processing account to post-traumatic stress disorder (PTSD). We argue that the experience of a traumatic event in Bayesian terms can be understood as a perceptual hypothesis that is subsequently given a very high a-priori likelihood due to its (life-) threatening significance; thus, this hypothesis is re-selected although it does not fit the actual sensory input. Based on this account, we re-conceptualise the symptom clusters of PTSD through the lens of a predictive processing model. We particularly focus on re-experiencing symptoms as the hallmark symptoms of PTSD, and discuss the occurrence of flashbacks in terms of perceptual and interoceptive inference. This account provides not only a new understanding of the clinical profile of PTSD, but also a unifying framework for the corresponding pathologies at the neurobiological level. Finally, we derive directions for future research and discuss implications for psychological and pharmacological interventions.}
}
@article{GOTHAM2016705,
title = {A Suitable Approach in Extracting Non Event Related Potential Sources from Brain of Disabled Patients},
journal = {Procedia Computer Science},
volume = {85},
pages = {705-712},
year = {2016},
note = {International Conference on Computational Modelling and Security (CMS 2016)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.05.257},
url = {https://www.sciencedirect.com/science/article/pii/S187705091630607X},
author = {Solomon Gotham and G. Sasibushana Rao},
keywords = {Electroencephalogram (EEG) ;Magneto encephalogram (EMG) ;Non Gaussianity, Evoked potentials (EVP) ;Non Event Related Potentials (NERP)},
abstract = {Brain is the most important, astonishing and complicated part of human body which is responsible for controlling and functioning of all other human organs. The physical movements and thinking capability (Cognition) of humans depend on the brain activity. Based on certain changes that occur within the brain, electric fields will be generated within the brain. Analyzing brain signals plays vital role in diagnosis and treatment of brain disorders. Brain signals are obtained from electrodes of Electroencephalogram (EEG) or Magneto encephalogram (EMG). These are linear mixture of evoked potentials (EVP) of large number of neurons due to variations in conductive and geometric properties in the layers of 3 layer head model or 4 layer head model. Earlier work1-5 considered processing these mixed signals for analyzing brain functioning of brain disabled patients. But working on the source signals gives an authoritative result. Hence there is a need to separate the source signals from the measured (electrode) signals. This work will suggest a suitable approach in extracting source signals of disabled patients while they were used as subjects under experiment of retrieving event related potentials (ERP). This work retrieved the signals of non target trails i.e., non event related potentials (NERP) and extracted original source signals by the best Gaussian estimate and the algorithm proposed.}
}
@article{NADERPOUR2014209,
title = {The explosion at institute: Modeling and analyzing the situation awareness factor},
journal = {Accident Analysis & Prevention},
volume = {73},
pages = {209-224},
year = {2014},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2014.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S0001457514002644},
author = {Mohsen Naderpour and Jie Lu and Guangquan Zhang},
keywords = {Situation awareness, Situation assessment, Abnormal situations, Methomyl unit, Accident analysis},
abstract = {In 2008 a runaway chemical reaction caused an explosion at a methomyl unit in West Virginia, USA, killing two employees, injuring eight people, evacuating more than 40,000 residents adjacent to the facility, disrupting traffic on a nearby highway and causing significant business loss and interruption. Although the accident was formally investigated, the role of the situation awareness (SA) factor, i.e., a correct understanding of the situation, and appropriate models to maintain SA, remain unexplained. This paper extracts details of abnormal situations within the methomyl unit and models them into a situational network using dynamic Bayesian networks. A fuzzy logic system is used to resemble the operator’s thinking when confronted with these abnormal situations. The combined situational network and fuzzy logic system make it possible for the operator to assess such situations dynamically to achieve accurate SA. The findings show that the proposed structure provides a useful graphical model that facilitates the inclusion of prior background knowledge and the updating of this knowledge when new information is available from monitoring systems.}
}
@article{CRAIG201810447,
title = {Lessons from my undergraduate research students},
journal = {Journal of Biological Chemistry},
volume = {293},
number = {27},
pages = {10447-10452},
year = {2018},
issn = {0021-9258},
doi = {https://doi.org/10.1074/jbc.RA118.003722},
url = {https://www.sciencedirect.com/science/article/pii/S0021925820338023},
author = {Paul A. Craig},
keywords = {structure-function, enzyme, gel electrophoresis, chromatography, bioinformatics, computational biology, CURE, mentor-student relationship, protein function prediction, teaching lab, undergraduate research, mentor-student relationship},
abstract = {From very early on, my personal/professional life has been shaped by teachers in many different settings. Teaching and learning form a two-way street. In the process of teaching undergraduate students, particularly in the research lab, I have learned some profound lessons about the importance of listening to them, challenging them, giving them autonomy, and allowing them to enjoy success and to risk failure. I am now working with a team of faculty members to implement these lessons in a course-based undergraduate research experience in the biochemistry teaching laboratory. Our goal is to seek answers to the question “How do students become scientists?” and to implement those answers with our future students.}
}
@article{ALBERT2008401,
title = {A formal framework for modelling the developmental course of competence and performance in the distance, speed, and time domain},
journal = {Developmental Review},
volume = {28},
number = {3},
pages = {401-420},
year = {2008},
issn = {0273-2297},
doi = {https://doi.org/10.1016/j.dr.2008.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0273229708000257},
author = {Dietrich Albert and Michael D. Kickmeier-Rust and Fumiko Matsuda},
keywords = {Distance–speed–time system, Cognitive development, Overgeneralization, Competence-based Knowledge Space Theory},
abstract = {The developmental course in the distance–speed–time domain is still a matter of debate. Traditional stage models are contested by theories of continuous development and adaptive thinking. In the present work, we introduce a formal framework for modelling the developmental course in this domain, grounding on Competence-based Knowledge Space Theory. This framework, as a more general case, widely includes assumptions and facets of previous models and covers empirical findings collected based on different experimental paradigms. By a distinction of latent competences and observable performance, model validation is not bound to a certain experimental paradigm and no one-to-one correspondence between competences and tasks is required. Therefore, the framework has the potential to bridge the gap between stage models and models of continuous development. The approach also precisely defines misconceptions, for example overgeneralization, and empirically investigates their occurrence. In the present work, we established a prototypical model for the development of understanding the distance–speed–time system. We extended this model with definitions based on different perspectives of overgeneralization. The assumptions of the model and its extensions were examined on the basis of the results of two empirical investigations using six judgment task types. The results yielded a reasonably good fit of model and data. No evidence was found for the occurrence of overgeneralization in this domain. The theoretical model and empirical results are discussed with respect to their relationship to other developmental models and theories.}
}
@incollection{GENTER2024641,
title = {Careers in toxicology},
editor = {Philip Wexler},
booktitle = {Encyclopedia of Toxicology (Fourth Edition)},
publisher = {Academic Press},
edition = {Fourth Edition},
address = {Oxford},
pages = {641-648},
year = {2024},
isbn = {978-0-323-85434-4},
doi = {https://doi.org/10.1016/B978-0-12-824315-2.00174-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128243152001743},
author = {Mary Beth Genter},
keywords = {Certification in toxicology, Computational toxicology, High-throughput methods, Postdoctoral training in toxicology, Risk assessment, Toxicology careers, Toxicology education, Toxicology job opportunities, Toxicology salaries, Toxicology training programs},
abstract = {Toxicology is the study of harmful effects of agents on people, animals, other living organisms, and the environment. Toxicology education requires not only a solid background in the fundamental principles of the field, but also an understanding of bench work and its possible translation for human benefit as well as its use in risk assessment. Employment opportunities are available in multiple job sectors, including academia, the chemical and pharmaceutical industries, contract research organizations, as consultants, and in various governmental and non-profit organizations. Training in toxicology should be broad in both the science and skills that impact effective experimental design, as well as the use, interpretation and communication of toxicology data, and decision-making processes.}
}
@article{RAMAN2002135,
title = {Coordinating informal and formal aspects of mathematics: student behavior and textbook messages},
journal = {The Journal of Mathematical Behavior},
volume = {21},
number = {2},
pages = {135-150},
year = {2002},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(02)00119-0},
url = {https://www.sciencedirect.com/science/article/pii/S0732312302001190},
author = {Manya Raman},
keywords = {Informal mathematics, Formal mathematics, Precalculus textbooks, Calculus textbooks},
abstract = {In this paper I illustrate difficulties students have coordinating informal and formal aspects of mathematics. I also discuss two ways in which precalculus and calculus textbooks treat mathematics that may make this coordination difficult: emphasizing the informal at the expense of the formal and emphasizing the formal at the expense of the informal. By looking at student difficulties in light of textbook treatments, we see evidence that student difficulties are not merely developmental. Students are not given many opportunities to make the kinds of connections which, while difficult, are an essential component of mathematical thinking.}
}
@article{DEBNATH2020101704,
title = {Grounded reality meets machine learning: A deep-narrative analysis framework for energy policy research},
journal = {Energy Research & Social Science},
volume = {69},
pages = {101704},
year = {2020},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2020.101704},
url = {https://www.sciencedirect.com/science/article/pii/S2214629620302796},
author = {Ramit Debnath and Sarah Darby and Ronita Bardhan and Kamiar Mohaddes and Minna Sunikka-Blank},
keywords = {Energy policy, Narratives, Topic modelling, Computational social science, Text analysis, Methodological framework},
abstract = {Text-based data sources like narratives and stories have become increasingly popular as critical insight generator in energy research and social science. However, their implications in policy application usually remain superficial and fail to fully exploit state-of-the-art resources which digital era holds for text analysis. This paper illustrates the potential of deep-narrative analysis in energy policy research using text analysis tools from the cutting-edge domain of computational social sciences, notably topic modelling. We argue that a nested application of topic modelling and grounded theory in narrative analysis promises advances in areas where manual-coding driven narrative analysis has traditionally struggled with directionality biases, scaling, systematisation and repeatability. The nested application of the topic model and the grounded theory goes beyond the frequentist approach of narrative analysis and introduces insight generation capabilities based on the probability distribution of words and topics in a text corpus. In this manner, our proposed methodology deconstructs the corpus and enables the analyst to answer research questions based on the foundational element of the text data structure. We verify theoretical compatibility through a meta-analysis of a state-of-the-art bibliographic database on energy policy, narratives and computational social science. Furthermore, we establish a proof-of-concept using a narrative-based case study on energy externalities in slum rehabilitation housing in Mumbai, India. We find that the nested application contributes to the literature gap on the need for multidisciplinary methodologies that can systematically include qualitative evidence into policymaking.}
}
@incollection{COLLINS2018105,
title = {Chapter 5 - Learning Structures Through Reinforcement},
editor = {Richard Morris and Aaron Bornstein and Amitai Shenhav},
booktitle = {Goal-Directed Decision Making},
publisher = {Academic Press},
pages = {105-123},
year = {2018},
isbn = {978-0-12-812098-9},
doi = {https://doi.org/10.1016/B978-0-12-812098-9.00005-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012812098900005X},
author = {Anne G.E. Collins},
keywords = {Exploration, Generalization, Hierarchical reinforcement learning, Reinforcement learning, Representation learning, Rule learning, Structure learning},
abstract = {How the brain uses reinforcement feedback to make simple choices that lead to reward is well understood. However, this ability is often considered insufficient to account for the flexibility and efficiency of human decision-making. In this chapter, we show that the computations of model-free reinforcement learning (RL) can in fact account for complex human learning abilities, such as generalization, transfer, and fast learning in high-dimensional, dynamic environments. Specifically, we show that humans structure their current information and choices into useful state and action spaces and that applying simple RL computations to these spaces—sometimes hierarchically—enables rich decision-making. Thus, RL computations enable humans to learn to represent the information they acquire in structured ways. Such structured RL simplifies complex problems (through representation learning), affords transfer of information (by building abstract rules and relating them to relevant contexts), and enables efficient exploration (by grouping together subsequences or identifying subpolicies).}
}
@article{LEVINSON2002155,
title = {Returning the tables: language affects spatial reasoning},
journal = {Cognition},
volume = {84},
number = {2},
pages = {155-188},
year = {2002},
issn = {0010-0277},
doi = {https://doi.org/10.1016/S0010-0277(02)00045-8},
url = {https://www.sciencedirect.com/science/article/pii/S0010027702000458},
author = {Stephen C Levinson and Sotaro Kita and Daniel B.M Haun and Björn H Rasch},
keywords = {Language, Spatial reasoning, Linguistic relativity},
abstract = {Li and Gleitman (Turning the tables: language and spatial reasoning. Cognition, in press) seek to undermine a large-scale cross-cultural comparison of spatial language and cognition which claims to have demonstrated that language and conceptual coding in the spatial domain covary (see, for example, Space in language and cognition: explorations in linguistic diversity. Cambridge: Cambridge University Press, in press; Language 74 (1998) 557): the most plausible interpretation is that different languages induce distinct conceptual codings. Arguing against this, Li and Gleitman attempt to show that in an American student population they can obtain any of the relevant conceptual codings just by varying spatial cues, holding language constant. They then argue that our findings are better interpreted in terms of ecologically-induced distinct cognitive styles reflected in language. Linguistic coding, they argue, has no causal effects on non-linguistic thinking – it simply reflects antecedently existing conceptual distinctions. We here show that Li and Gleitman did not make a crucial distinction between frames of spatial reference relevant to our line of research. We report a series of experiments designed to show that they have, as a consequence, misinterpreted the results of their own experiments, which are in fact in line with our hypothesis. Their attempts to reinterpret the large cross-cultural study, and to enlist support from animal and infant studies, fail for the same reasons. We further try to discern exactly what theory drives their presumption that language can have no cognitive efficacy, and conclude that their position is undermined by a wide range of considerations.}
}
@article{SHAIKH2023102692,
title = {Machine intelligence and medical cyber-physical system architectures for smart healthcare: Taxonomy, challenges, opportunities, and possible solutions},
journal = {Artificial Intelligence in Medicine},
volume = {146},
pages = {102692},
year = {2023},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2023.102692},
url = {https://www.sciencedirect.com/science/article/pii/S0933365723002063},
author = {Tawseef Ayoub Shaikh and Tabasum Rasool and Prabal Verma},
keywords = {Medical cyber-physical systems, Internet of things, Big data, CPS Architectures, Digital twin, Dew computing, Security and privacy},
abstract = {Hospitals use medical cyber-physical systems (MCPS) more often to give patients quality continuous care. MCPS isa life-critical, context-aware, networked system of medical equipment. It has been challenging to achieve high assurance in system software, interoperability, context-aware intelligence, autonomy, security and privacy, and device certifiability due to the necessity to create complicated MCPS that are safe and efficient. The MCPS system is shown in the paper as a newly developed application case study of artificial intelligence in healthcare. Applications for various CPS-based healthcare systems are discussed, such as telehealthcare systems for managing chronic diseases (cardiovascular diseases, epilepsy, hearing loss, and respiratory diseases), supporting medication intake management, and tele-homecare systems. The goal of this study is to provide a thorough overview of the essential components of the MCPS from several angles, including design, methodology, and important enabling technologies, including sensor networks, the Internet of Things (IoT), cloud computing, and multi-agent systems. Additionally, some significant applications are investigated, such as smart cities, which are regarded as one of the key applications that will offer new services for industrial systems, transportation networks, energy distribution, monitoring of environmental changes, business and commerce applications, emergency response, and other social and recreational activities.The four levels of an MCPS's general architecture—data collecting, data aggregation, cloud processing, and action—are shown in this study. Different encryption techniques must be employed to ensure data privacy inside each layer due to the variations in hardware and communication capabilities of each layer. We compare established and new encryption techniques based on how well they support safe data exchange, secure computing, and secure storage. Our thorough experimental study of each method reveals that, although enabling innovative new features like secure sharing and safe computing, developing encryption approaches significantly increases computational and storage overhead. To increase the usability of newly developed encryption schemes in an MCPS and to provide a comprehensive list of tools and databases to assist other researchers, we provide a list of opportunities and challenges for incorporating machine intelligence-based MCPS in healthcare applications in our paper's conclusion.}
}
@incollection{OGRADY2014167,
title = {Chapter Six - The Development and Implementation of a Biomolecular Docking Exercise for the General Chemistry Laboratory},
editor = {Ralph A. Wheeler},
series = {Annual Reports in Computational Chemistry},
publisher = {Elsevier},
volume = {10},
pages = {167-187},
year = {2014},
issn = {1574-1400},
doi = {https://doi.org/10.1016/B978-0-444-63378-1.00006-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780444633781000069},
author = {Clare E. O’Grady and Peter Talpey and Timothy E. Elgren and Adam W. {Van Wynsberghe}},
keywords = {Docking, Laboratory exercise, Computational chemistry, Undergraduate, BPA, Estrogenic effects},
abstract = {Computational chemistry has become a widely used tool in nearly all subdisciplines of chemistry. Therefore, it is essential that students be exposed to these techniques throughout the undergraduate curriculum. In this work, we present a biomacromolecular docking laboratory exercise aimed at introducing computational physical chemistry in the context of protein-ligand binding to first-semester freshmen introductory chemistry students. The exercise focuses on investigating the binding properties of persistent organic pollutants, in particular, bisphenol A and its metabolite bisphenol-A glucuronide to estrogen receptors. This report gives a description of the exercise, its goals, and the software tools that were used to implement the laboratory activities. Importantly, all aspects of the exercise can be accomplished using freely accessible software and web servers. The success of the exercise was assessed using pre- and posttesting of conceptual and factual content and Likert scale student attitudinal surveys.}
}
@article{MARCHAND1995179,
title = {Policy analysis as a tool for habitat restoration: A case study of a Danube river floodplain, Hungary},
journal = {Water Science and Technology},
volume = {31},
number = {8},
pages = {179-186},
year = {1995},
note = {Integrated Water Resources Management},
issn = {0273-1223},
doi = {https://doi.org/10.1016/0273-1223(95)00399-8},
url = {https://www.sciencedirect.com/science/article/pii/0273122395003998},
author = {M. Marchand* and E.C.L. Marteijn** and P. Bakonyi***},
keywords = {Floodplain rehabilitation, Danube, policy analysis, water quality modelling},
abstract = {This paper will elaborate a policy analysis approach especially designed for habitat restoration. It will be illustrated by a case study example of a floodplain area along the Danube river, Hungary. The case study used hydrodynamic and water quality models and expertise from a range of disciplines. This made it possible to unravel the complex relations between the environment and human interventions. Crucial was the participation of local experts in the design and screening of measures, as well as the feedback from local interest groups at several occasions during the project. This resulted in the formulation of rehabilitation ideas, most of which have hitherto not been discussed. The combination of creative thinking with practical possibilities and limitations has been worked out in a cyclic process from which three different alternatives emerged. These have been analyzed for their feasibility with regard to the goals to be achieved, their costs and their impacts on other interests.}
}
@article{ASIF2024,
title = {Machine learning-driven catalyst design, synthesis and performance prediction for CO2 hydrogenation},
journal = {Journal of Industrial and Engineering Chemistry},
year = {2024},
issn = {1226-086X},
doi = {https://doi.org/10.1016/j.jiec.2024.09.035},
url = {https://www.sciencedirect.com/science/article/pii/S1226086X24006269},
author = {Muhammad Asif and Chengxi Yao and Zitu Zuo and Muhammad Bilal and Hassan Zeb and Seungjae Lee and Ziyang wang and Taesung Kim},
keywords = {Heterogeneous catalysis, DFT calculation, Machine learning, 3D printing, CO hydrogenation},
abstract = {Atmospheric concentrations of CO2 must be lowered to mitigate climate change and rising global temperatures. CO2 utilization is the most promising approach for the sustainable reduction of CO2 emissions. Interdisciplinary research is gaining increasing attention due to its broader application potential and the promising results of combining various fields. Artificial intelligence in catalytic research could be cost-effective and environmentally friendly. Machine Learning (ML) and 3D printing technologies may soon be able to produce nanoscale raw materials to synthesize the catalyst for commercial-scale applications. In this review article, recent advances in catalyst synthesis using 3D printing technologies and ML-based catalytic reactions, particularly those in CO2 hydrogenation, are critically analyzed, with a focus on the function of ML model prediction. ML approaches with high prediction accuracies are discussed comprehensively. Based on the literature Gray-box models can provide useful insights by revealing the essential catalytic traits, factors, and circumstances that affect the results. They can also provide a practical solution by fusing the benefits of black-box algorithms, such as ensemble models and NNs, with feature importance analysis. Finally, suggestions and recommendations for the potential applications of ML in chemical science, especially in heterogeneous catalysis, are provided along with future research directions.}
}
@article{SOYLU2024104416,
title = {A new ontology for numerical cognition: Integrating evolutionary, embodied, and data informatics approaches},
journal = {Acta Psychologica},
volume = {249},
pages = {104416},
year = {2024},
issn = {0001-6918},
doi = {https://doi.org/10.1016/j.actpsy.2024.104416},
url = {https://www.sciencedirect.com/science/article/pii/S0001691824002932},
author = {Firat Soylu},
keywords = {Numerical cognition, Mathematics, Embodied cognition, Evolution, Neuroimaging, Ontology},
abstract = {Numerical cognition is a field that investigates the sociocultural, developmental, cognitive, and biological aspects of mathematical abilities. Recent findings in cognitive neuroscience suggest that cognitive skills are facilitated by distributed, transient, and dynamic networks in the brain, rather than isolated functional modules. Further, research on the bodily and evolutionary bases of cognition reveals that our cognitive skills harness capacities originally evolved for action and that cognition is best understood in conjunction with perceptuomotor capacities. Despite these insights, neural models of numerical cognition struggle to capture the relation between mathematical skills and perceptuomotor systems. One front to addressing this issue is to identify building block sensorimotor processes (BBPs) in the brain that support numerical skills and develop a new ontology connecting the sensorimotor system with mathematical cognition. BBPs here are identified as sensorimotor functions, associated with distributed networks in the brain, and are consistently identified as supporting different cognitive abilities. BBPs can be identified with new approaches to neuroimaging; by examining an array of sensorimotor and cognitive tasks in experimental designs, employing data-driven informatics approaches to identify sensorimotor networks supporting cognitive processes, and interpreting the results considering the evolutionary and bodily foundations of mathematical abilities. New empirical insights on the BBPs can eventually lead to a revamped embodied cognitive ontology in numerical cognition. Among other mathematical skills, numerical magnitude processing and its sensorimotor origins are discussed to substantiate the arguments presented. Additionally, an fMRI study design is provided to illustrate the application of the arguments presented in empirical research.}
}
@article{ZHANG2022102548,
title = {Routing optimization of shared autonomous electric vehicles under uncertain travel time and uncertain service time},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {157},
pages = {102548},
year = {2022},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2021.102548},
url = {https://www.sciencedirect.com/science/article/pii/S1366554521003069},
author = {Li Zhang and Zhongshan Liu and Lan Yu and Ke Fang and Baozhen Yao and Bin Yu},
keywords = {Shared autonomous electric vehicles, Routing optimization, Charging schedules, Travel time uncertainty, Service time uncertainty, Branch-and-price algorithm},
abstract = {The emerging autonomous electric vehicles have facilitated the implementation of the shared autonomous electric vehicle (SAEV) service. In real-life applications, the operations of SAEVs may be affected by various uncertain factors, such as uncertain travel time caused by traffic congestion and uncertain service time caused by unpredictable customer delay. It is essential to consider uncertain factors to design conservative and robust routes for SAEVs. In this paper, we study a routing optimization problem of SAEVs, where the charging schedules, uncertain travel time, and uncertain service time are considered. The objective of the problem is to minimize the total operational cost that consists of fixed cost and travel cost of SAEVs. A branch-and-price algorithm is developed to solve the problem. Specifically, a tailored label setting algorithm is introduced to identify the robust feasible routes with accessible charging schedules for the pricing subproblem. The proposed algorithm is tested on a set of generated instances. The computational results indicate that the proposed algorithm outperforms the commercial solver CPLEX in terms of both solution quality and computational time. Besides, based on the sensitivity analyses, we show the impact of the budgets of uncertainty, maximum deviations of uncertain parameters, battery capacity, fixed cost, and charging speed on the SAEV service. This work can also provide inspirations of both models and algorithms for many other application areas, such as urban logistics.}
}
@article{ALBEROLA20161,
title = {An artificial intelligence tool for heterogeneous team formation in the classroom},
journal = {Knowledge-Based Systems},
volume = {101},
pages = {1-14},
year = {2016},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2016.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S0950705116000964},
author = {Juan M. Alberola and Elena {del Val} and Victor Sanchez-Anguix and Alberto Palomares and Maria {Dolores Teruel}},
keywords = {Team formation, Artificial intelligence, Belbin roles, Computational intelligence},
abstract = {Nowadays, there is increasing interest in the development of teamwork skills in the educational context. This growing interest is motivated by its pedagogical effectiveness and the fact that, in labour contexts, enterprises organise their employees in teams to carry out complex projects. Despite its crucial importance in the classroom and industry, there is a lack of support for the team formation process. Not only do many factors influence team performance, but the problem becomes exponentially costly if teams are to be optimised. In this article, we propose a tool whose aim it is to cover such a gap. It combines artificial intelligence techniques such as coalition structure generation, Bayesian learning, and Belbin’s role theory to facilitate the generation of working groups in an educational context. This tool improves current state of the art proposals in three ways: i) it takes into account the feedback of other teammates in order to establish the most predominant role of a student instead of self-perception questionnaires; ii) it handles uncertainty with regard to each student’s predominant team role; iii) it is iterative since it considers information from several interactions in order to improve the estimation of role assignments. We tested the performance of the proposed tool in an experiment involving students that took part in three different team activities. The experiments suggest that the proposed tool is able to improve different teamwork aspects such as team dynamics and student satisfaction.}
}
@incollection{HABASH2022161,
title = {6 - Building as a control system},
editor = {Riadh Habash},
booktitle = {Sustainability and Health in Intelligent Buildings},
publisher = {Woodhead Publishing},
pages = {161-189},
year = {2022},
series = {Woodhead Publishing Series in Civil and Structural Engineering},
isbn = {978-0-323-98826-1},
doi = {https://doi.org/10.1016/B978-0-323-98826-1.00006-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323988261000065},
author = {Riadh Habash},
keywords = {Control systems, Computational intelligence, Thermal modeling, Analog control, Digital control, Intelligent control, Artificial neutral networks, Fuzzy logic, HVAC control, Lighting system control},
abstract = {One of the most resilient features in the design of intelligent buildings is control. When implemented and used properly, buildings with proper control systems can perform very well. Achieving this varying objective requires advanced automation and monitoring tools where buildings rely less on human control and more on the embedded intelligence. This entails an integration platform that employs sensing technologies, cloud computing, big data, artificial intelligence, and various proactive and predictive digital capabilities. This chapter focuses on the role of modeling and control in tackling challenges for sustainability and health. It raises questions and provokes a debate about data-driven and computational design innovation and its position in buildings. Various modeling and control approaches including conventional and nonconventional are presented. The next-generation highly integrated building automation and control systems are described. Different approaches used in the design of major building technical systems are discussed. The main research trends in the field of advanced control that evolve around several approaches including model-based, model-predictive, data-based, agent-based, and exergy-based have been highlighted.}
}
@article{KOIVISTO2024103698,
title = {Cognitive flexibility moderates the relationship between openness-to-experience and perceptual reversals of Necker cube},
journal = {Consciousness and Cognition},
volume = {122},
pages = {103698},
year = {2024},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2024.103698},
url = {https://www.sciencedirect.com/science/article/pii/S1053810024000655},
author = {Mika Koivisto and Cypriana Pallaris},
keywords = {Big Five, Bistable perception, Cognitive flexibility, Openness},
abstract = {It is not clear whether personality is related to basic perceptual processes at the level of automatic bottom-up processes or controlled top-down processes. Two experiments examined how personality influences perceptual dynamics, focusing on how cognitive flexibility moderates the relationship between personality and perceptual reversals of the Necker cube. The participants viewed stimuli either passively or with the intent to either hold or switch the orientation of the Necker cube. The influence of openness was predominantly evident in conditions necessitating intentional control over perceptual reversals. The link between openness and intentional perceptual reversals was always moderated by cognitive flexibility, which was measured in three different ways. No relationship was detected between personality traits and reversals in the passive viewing condition, suggesting that relatively spontaneous adaptation-inhibition processes may not be personality-dependent. Overall, our research sheds light on the nuanced influence of personality traits on perceptual experiences, mediated by cognitive flexibility.}
}
@article{JACOB2020102142,
title = {Neural correlates of rumination in major depressive disorder: A brain network analysis},
journal = {NeuroImage: Clinical},
volume = {25},
pages = {102142},
year = {2020},
issn = {2213-1582},
doi = {https://doi.org/10.1016/j.nicl.2019.102142},
url = {https://www.sciencedirect.com/science/article/pii/S2213158219304887},
author = {Yael Jacob and Laurel S Morris and Kuang-Han Huang and Molly Schneider and Sarah Rutter and Gaurav Verma and James W Murrough and Priti Balchandani},
keywords = {Default mode network, Depression, Entropy, Graph Theory, High-field MRI, Precuneus},
abstract = {Patients with major depressive disorder (MDD) exhibit higher levels of rumination, i.e., repetitive thinking patterns and exaggerated focus on negative states. Rumination is known to be associated with the cortical midline structures / default mode network (DMN) region activity, although the brain network topological organization underlying rumination remains unclear. Implementing a graph theoretical analysis based on ultra-high field 7-Tesla functional MRI data, we tested whether whole brain network connectivity hierarchies during resting state are associated with rumination in a dimensional manner across 20 patients with MDD and 20 healthy controls. Applying this data-driven approach we found a significant correlation between rumination tendency and connectivity strength degree of the right precuneus, a key node of the DMN. In order to interrogate this region further, we then applied the Dependency Network Analysis (DEPNA), a recently developed method used to quantify the connectivity influence of network nodes. This revealed that rumination was associated with lower connectivity influence of the left medial orbito-frontal cortex (MOFC) cortex on the right precuneus. Lastly, we used an information theory entropy measure that quantifies the cohesion of a network's correlation matrix. We show that subjects with higher rumination scores exhibit higher entropy levels within the DMN i.e. decreased overall connectivity within the DMN. These results emphasize the general DMN involvement during self-reflective processing related to maladaptive rumination in MDD. This work specifically highlights the impact of the MOFC on the precuneus, which might serve as a target for clinical neuromodulation treatment.}
}
@article{LAWLER1996241,
title = {Thinkable models},
journal = {The Journal of Mathematical Behavior},
volume = {15},
number = {3},
pages = {241-259},
year = {1996},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(96)90004-8},
url = {https://www.sciencedirect.com/science/article/pii/S0732312396900048},
author = {Robert W. Lawler},
abstract = {A primary objective of technical education should be the development of thinkable models in the minds of students. Thinkable models are representations of things and processes simple enough that people can use them in thought experiments. The organization of cognitive structures for technical domains can be imagined to be a network of appropriately connected thinkable models. Artificial intelligence (AI), as the science of representations, has focused in the main on languagelike representations. If we can enrich our vision of representations to include a greater variety of ways of thinking that are useful to people, we may hope to broaden access to scientific ideas. To pursue these notions in some detail, a taxonomy of models is developed and the issue of how representations relate to human modes of perception and action is raised. The notions are explored first through contrasting of several approaches to the Pythagorean Theorem.}
}
@incollection{SUN2020139,
title = {7 - 3D printing technologies: current applications, future trends, and challenges},
editor = {Joanne Yip},
booktitle = {Latest Material and Technological Developments for Activewear},
publisher = {Woodhead Publishing},
pages = {139-151},
year = {2020},
series = {The Textile Institute Book Series},
isbn = {978-0-12-819492-8},
doi = {https://doi.org/10.1016/B978-0-12-819492-8.00007-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128194928000077},
author = {Lushan Sun},
keywords = {3D printing wearable, 3D CAD modeling, selective laser sintering, fused deposition modeling, material jetting, activewear, footwear},
abstract = {The increasing popularity of three-dimensional printing (3DP) technologies in recent decades has resulted in their exponential advancement so that they now provide products with the quality of those that are mass manufactured and mass customized. Today, they are considered to be a form of additive manufacturing and direct digital manufacturing (DDM), and the printing methods have since advanced to create higher quality output. Fashion designers and maker enthusiasts have since found that 3D printing technologies can be modified for apparel, thus allowing process and workflow disruption, or even hacking of the technologies, so that it is now important to rethink traditional skill sets and evolve them. Although 3D printing applications are still in their infancy, this chapter focuses on a few cases around activewear apparel and footwear with the use of 3D printing as they have been unique in their development approaches, direct and computational 3D modeling applications, material uses (e.g., nylon, thermoplastic polyurethane, or TPU), and applied technologies (e.g., fused deposition modeling, selective laser sintering, PolyJet printing). The intention is to shed light on the challenges and the potential future integration of 3D printing technologies in the activewear industry.}
}
@article{HYUN2018113,
title = {Balancing homogeneity and heterogeneity in design exploration by synthesizing novel design alternatives based on genetic algorithm and strategic styling decision},
journal = {Advanced Engineering Informatics},
volume = {38},
pages = {113-128},
year = {2018},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2018.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S1474034617305657},
author = {Kyung Hoon Hyun and Ji-Hyun Lee},
keywords = {Computational design, Design alternative, Design exploration, Styling strategy, Design synthesis, Family look},
abstract = {Designers constantly and consistently draft and develop both general concepts and directions to identify the solution that best fits the styling objectives of the lead designer. Designers often confront design fixations that cognitively clash to explore different design combinations. As design teams explore the range of possible design spaces of a certain design strategy, there is an opportunity for computational approaches to improve the styling process. By implementing product appearance similarity and styling strategy in computational design synthesis, it is possible to discover combinations that would otherwise remain unexplored by human designers. Numerous studies on design synthesis have been conducted. However, there has been no focus on the morphological synthesis of designs with strategic styling decisions. Considering this, the proposed study develops a method to synthesize car styling based on product appearance similarity for effective design exploration in the concept generation phase. The similarities of products across different generations, product portfolios, and competitors’ products are calculated to evaluate the strategic styling decision. The results of the strategic styling decision are used to formulate a fitness function. Car styling is then synthesized with a genetic algorithm based on this fitness function to generate car styling in accordance with the target strategic styling decision. In this respect, designers can computationally synthesize novel design alternatives that consider both homogeneity (family look in design) and heterogeneity (design trend in the market) by pinpointing the desired design exploration area. Ultimately, the style synthesis methodology proposed in this research can help designers to utilize the gradual visualization of styling strategies for more effective and efficient managerial design decisions. To do this, we conduct five major tasks: first, car design data are collected for design synthesis; second, the product appearance similarity is calculated to measure the strategic styling decision; third, synthesis validation is conducted to test whether the proposed methodology can create outside-the-box designs; fourth, a genetic algorithm is used to synthesize car designs in consideration of the strategic styling decision; finally, a series of in-depth interviews with experts and validation experiments are conducted with in-house automobile designers to examine the impact of the proposed methodology. The results showed that designers can quantitatively measure and compare the styling strategies of each car brand, then implement design upgrades, while still maintaining that specific style. Correspondingly, computationally generated design alternatives improve the satisfaction in ease, time, objective reflection and novelty of design outcomes when formulating design strategies in the concept generation phase.}
}
@incollection{GORI2018122,
title = {Chapter 3 - Linear Threshold Machines},
editor = {Marco Gori},
booktitle = {Machine Learning},
publisher = {Morgan Kaufmann},
pages = {122-184},
year = {2018},
isbn = {978-0-08-100659-7},
doi = {https://doi.org/10.1016/B978-0-08-100659-7.00003-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780081006597000038},
author = {Marco Gori},
keywords = {Linear machines, Least mean square, Linear-threshold machines, Normal equations, Ridge regression, Linear separability, Predicate order, Gradient descent, Perceptron algorithm, Terminal attractors},
abstract = {One of the simplest ways of modeling the interactions of intelligent agents with the environment is to expose them to a collection of supervised pairs (example, target). This chapter is about the learning mechanisms that arise from the assumption of dealing with linear and linear-threshold machines. In most cases, the covered topics nicely intercept different disciplines, and are of remarkable importance to better grasp many approaches to machine learning. The chapter covers classic topics, like normal equations and ridge regression, as well as representational issues in pattern recognition that are connected with the notion of predicate order. Linear-threshold machines are described along with related computational geometry issues, and the view that arises from Bayesian decision. Classic gradient learning algorithms, including the stochastic version, are described in the continuum setting, as well as the Rosenblatt perceptron algorithm. Finally, some complexity issues are covered in both the discrete and continuous setting of computation.}
}
@article{SKOLICKI2008801,
title = {Co-evolution of terrorist and security scenarios for water distribution systems},
journal = {Advances in Engineering Software},
volume = {39},
number = {10},
pages = {801-811},
year = {2008},
note = {Selected papers from Civil-Comp 2005 and AICivil-Comp 2005},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2008.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0965997808000689},
author = {Z. Skolicki and T. Arciszewski and M.H. Houck and K. {De Jong}},
keywords = {Homeland security, Patterns, Terrorism, Co-evolution, Evolutionary computation, Water distribution system},
abstract = {Identification of vulnerabilities of water distribution systems and identification of appropriate counter-measures are important components of homeland security. These are difficult and time consuming tasks. This paper provides a new approach to resolve these problems in complex infrastructure systems. It is based on the use of co-evolutionary computation for the generation of both terrorist and security scenarios. The basic concepts of co-evolutionary computation are briefly explained. The concept of co-evolutionary generation of terrorist and security scenarios is introduced in the context of a hypothetical water distribution system for a small town. A tool developed at George Mason University is used for a number of experiments that reveal a variety of emerging security patterns. The experiments show that these patterns may be helpful in effectively protecting the network.}
}
@article{LI20112824,
title = {Human Action Recognition Based on Template Matching},
journal = {Procedia Engineering},
volume = {15},
pages = {2824-2830},
year = {2011},
note = {CEIS 2011},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2011.08.532},
url = {https://www.sciencedirect.com/science/article/pii/S1877705811020339},
author = {Chengyou Li and Tao Hua},
keywords = {computer vison ;key frame, ℜ transform, string matching},
abstract = {This paper presents a new method of human action recognition, which is based on ℜ transform and template matching after the key frame is extracted from a cycle. For a key binary human silhouette, ℜ transform is employed to represent low-level features. The advantage of the ℜ transform lies in its low computational complexity and geometric invariance. We utilize a novel string matching scheme based on edit distance is proposed to analyze different human actions. Compared with other methods, ours is superior because the descriptor is robust to frame loss in the video sequence, disjoint silhouettes and holes in the shape, and thus achieves better performance in similar activities recognition, simple representation, computational complexity and template generalization. Sufficient experiments have proved the efficiency.}
}
@incollection{MURTY201981,
title = {5 - Alloy design in the 21st century: ICME, materials genome, and artificial intelligence strategies},
editor = {B.S. Murty and J.W. Yeh and S. Ranganathan and P.P. Bhattacharjee},
booktitle = {High-Entropy Alloys (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
pages = {81-101},
year = {2019},
isbn = {978-0-12-816067-1},
doi = {https://doi.org/10.1016/B978-0-12-816067-1.00005-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128160671000059},
author = {B.S. Murty and J.W. Yeh and S. Ranganathan and P.P. Bhattacharjee},
keywords = {Integrated Computational Materials Engineering (ICME), Calculation of phase diagrams (CALPHAD), Ab initio calculations, Molecular dynamic simulations, Monte Carlo simulations, Artificial intelligence},
abstract = {In recent years, there has been intense activity in the prediction of phases formed in high-entropy alloys (HEAs) through various means, and strategies based on Integrated Computational Materials Engineering (ICME) are taking prominent position in comparison with parametric approaches. While parametric approaches discussed in Chapter 3 are useful in rationalizing the phase obtained in HEAs and can also be useful in identifying the window of parameters that can lead to the formation of HEAs with a particular structure, the computational methods can be more predictive in nature. Among the computational approaches, the most prominent ones are CALPHAD method, ab initio calculations, molecular dynamics and Monte Carlo simulations, and phase-field modeling. The introduction of artificial intelligence is a disruptive addition and at the same time an extremely promising innovation. The current chapter gives an account of the status of these approaches in the HEA research.}
}
@article{MUDJAHIDIN2019968,
title = {Testing Methods on System Dynamics: A Model of Reliability, Average Reliability, and Demand of Service},
journal = {Procedia Computer Science},
volume = {161},
pages = {968-975},
year = {2019},
note = {The Fifth Information Systems International Conference, 23-24 July 2019, Surabaya, Indonesia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.206},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919319179},
author = { Mudjahidin and Rully Agus Hendrawan and Andre Parvian Aristio and Joko Lianto Buliali and Muhammad Nur Yuniarto},
keywords = {System dynamics, Testing method, structural testing, algorithms testing, behavioural testing},
abstract = {As a model used to simulate policies by creating scenarios, system dynamics must have similarities with real systems. Therefore, the system dynamics model should test so declare as the right model and representing the behaviour of a system. Thus, in this article, we propose three test methods to ensure the system dynamics model have appropriate structure, correct value according to the specified equation, and can use to establish the parameter of the model. We study articles to propose the testing methods (the structural testing, algorithms testing, and behavioural testing) and present the case study about reliability, average reliability, and its affected demands. In this article, we prove that the testing methods can be used to show the system dynamics model appropriates and represents the real system, all computation generated by the simulation output is proper to the specified equation and can use to choose the best parameter.}
}
@article{SHEFFIELD2024S66,
title = {Longitudinal Associations Between Belief Updating and Delusions in Individuals Recovering From an Acute Psychotic Episode},
journal = {Biological Psychiatry},
volume = {95},
number = {10, Supplement },
pages = {S66-S67},
year = {2024},
note = {Abstract Supplement},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2024.02.164},
url = {https://www.sciencedirect.com/science/article/pii/S0006322324002658},
author = {Julia Sheffield and Lauren Hall and Simon Vandekar and Jinyuan Liu and Neil Woodward and Stephan Heckers and Philip Corlett}
}
@article{DONG2012609,
title = {Effect of escape device for Submerged Floating Tunnel (SFT) on hydrodynamic loads applied to SFT},
journal = {Journal of Hydrodynamics, Ser. B},
volume = {24},
number = {4},
pages = {609-616},
year = {2012},
issn = {1001-6058},
doi = {https://doi.org/10.1016/S1001-6058(11)60284-9},
url = {https://www.sciencedirect.com/science/article/pii/S1001605811602849},
author = {Man-sheng DONG and Guo-ping MIAO and Long-chang YONG and Zhong-rong NIU and Huan-ping PANG and Chao-qun HOU},
keywords = {Submerged Floating Tunnel (SFT), conceptual design, flow, Airy wave, escape device, hydrodynamic load},
abstract = {This paper presents a potential approach to settle the problem of surviving major safety accidents in Submerged Floating Tunnel (SFT) that detachable emergency escape devices are set up outside SFT. The Computational Fluid Dynamics (CFD) technology is used to investigate the effect of emergency escape devices on the hydrodynamic load acting on SFT in uniform and oscillatory flows and water waves by numerical test. The governing equations, i.e., the Reynolds-Averaged Navier-Stokes (RANS) equations and k – ɛ standard turbulence equations, are solved by the Finite Volume Method (FVM). Analytic solutions for the Airy wave are applied to set boundary conditions to generate water wave. The VOF method is used to trace the free surface. In uniform flow, hydrodynamic loads, applied to SFT with emergency escape device, reduce obviously. But, in oscillatory flow, it has little influence on hydrodynamic loads acting on SFT. Horizontal and vertical wave loads of SFT magnify to some extend due to emergency escape devices so that the influence of emergency escape devices on hydrodynamic loads of SFT should be taken into consideration when designed.}
}
@incollection{SUBHASWARAJ2022207,
title = {Chapter 11 - Molecular docking and molecular dynamic simulation approaches for drug development and repurposing of drugs for severe acute respiratory syndrome-Coronavirus-2},
editor = {Arpana Parihar and Raju Khan and Ashok Kumar and Ajeet Kumar Kaushik and Hardik Gohel},
booktitle = {Computational Approaches for Novel Therapeutic and Diagnostic Designing to Mitigate SARS-CoV-2 Infection},
publisher = {Academic Press},
pages = {207-246},
year = {2022},
isbn = {978-0-323-91172-6},
doi = {https://doi.org/10.1016/B978-0-323-91172-6.00007-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323911726000078},
author = {Pattnaik Subhaswaraj and Busi Siddhardha},
keywords = {Computational tools, drug repurposing, reverse pharmacology, SARS-CoV-2, molecular docking, molecular dynamics simulation},
abstract = {The world is witnessing the impact of deadly disease outbreaks in the form of the novel severe acute respiratory syndrome (SARS) like Coronavirus disease-2019 (COVID-19) pandemic. Since the beginning of the 21st century, we have witnessed several other disease outbreaks (e.g., SARS, H1N1, Zika virus disease, and Ebola virus epidemics), which have significantly affected our healthcare systems and socioeconomic profile. To fight against these disease outbreaks, the research and development (R&D) sectors are continuously working to develop putative therapeutic modules across the globe. In this context, computational tools further corroborated the drug design and development process. The in silico approaches particularly molecular docking and molecular dynamics simulation received considerable attention in identifying putative drug candidates and thus critically revolutionized the drug development pipelines. As compared to classical pharmacology-based approaches, the reverse pharmacology-based approaches in combination with high throughput computational tools (computer-aided drug designing, quantitative structure–activity relationship studies) have transformed the drug screening timeline thereby minimizing the manpower, cost, and drug development process by 50%–60%. The concept of drug repurposing, that is, utilizing old and clinically-approved drug molecules become a household proposition in R&D sectors by minimizing the drug development cost and timelines. In this review, we emphasized the role of computational-based approaches particularly molecular docking and simulations in designing novel drug candidates to mitigate potential health hazards associated with SARS-CoV-2. Apart from this, we have also emphasized the translational research pipelines being developed to translate the in silico data into the market for human consumption.}
}
@article{LIU1995367,
title = {Some phenomena of seeing shapes in design},
journal = {Design Studies},
volume = {16},
number = {3},
pages = {367-385},
year = {1995},
issn = {0142-694X},
doi = {https://doi.org/10.1016/0142-694X(94)00001-T},
url = {https://www.sciencedirect.com/science/article/pii/0142694X9400001T},
author = {Yu-Tung Liu},
keywords = {restructuring shapes, emergent subshapes, shape recognition, design cognition, design computation},
abstract = {An empirical study was designed and conducted not only to reveal the importance of restructuring shapes in design but to determine some of the variables embedded in such visual behaviour. Four phenomena of seeing shapes in design have been found. Firstly, both experienced and inexperienced designers can recognize explicit subshapes, however, only the former group can recognize implicit subshapes. Secondly, when people look at a shape, explicit, close, namable subshapes are the first to emerge. Thirdly, the time needed to find emergent subshapes is in proportion to the complexity of the shape, namely the number of subshapes it subsumes. Finally, due to their prior experience and professional training, experienced designers have lowered their thresholds of recognizing activation so that they are able to discover implicit emergent subshapes. Implications for design cognition and design computation are drawn from this experiment.}
}
@incollection{AHAMED2017465,
title = {Chapter 29 - The Architecture of a Mind-Machine},
editor = {Syed V. Ahamed},
booktitle = {Evolution of Knowledge Science},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {465-479},
year = {2017},
isbn = {978-0-12-805478-9},
doi = {https://doi.org/10.1016/B978-0-12-805478-9.00029-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128054789000297},
author = {Syed V. Ahamed},
keywords = {Mind, Knowledge, Machines, Technology, Human Needs, Knowledge Windows, Perceptual Spaces},
abstract = {Chapter Summary
In this chapter, we take bold step and propose the unthinkable: The genesis of a Customizable Mind-Machine. Thought that stems from the mind is deeply seated in a biological framework of neurons. The biological origin lies in the marvel of evolution over the eons and refined ever so fast, faster than in the prior centuries. Three (a, b, and c), triadic objects are ceaselessly at work. At a personal level (a) mind, knowledge, and machines have been intertwined like inspiration, words, and language since the dawn of the human evolution and more recently, (b) technology, manufacturing, and economics have formed a hub of progress, (c) wealth, global marketing, and insatiable needs of humans and civilization. These triadic cycles of nine essential objects of human existence are spinning quicker and quicker every year. The Internet offers the mind no choice but to leap and soar over history and over the globe. Alternatively, human mind can sink deeper and deeper into ignorance and oblivion. More recently, the Artificial Intelligence at work in the Internet had challenged the natural intelligence at the cognizance level in the mind to find its way to breakthroughs and innovations. We integrate functions of the mind with the processing of knowledge in the hardware of machines by freely traversing the neural, mental, physical, psychological, social, knowledge, and computational spaces. The laws of neural biology and mind, laws of knowledge and social sciences, and finally the laws of physics and mechanics in each of the spaces are unique and executed by distinctive processors for each space. Much as mind rules over matter, the triad of mind, space, and time creates a human-space that rules over the Relativistic-space of matter, space, and time.}
}
@article{YISHOU200932,
title = {Knowledge Fusion Design Method: Satellite Module Layout},
journal = {Chinese Journal of Aeronautics},
volume = {22},
number = {1},
pages = {32-42},
year = {2009},
issn = {1000-9361},
doi = {https://doi.org/10.1016/S1000-9361(08)60066-7},
url = {https://www.sciencedirect.com/science/article/pii/S1000936108600667},
author = {Wang Yishou and Teng Hongfei},
keywords = {complex engineering system, satellite module layout design, knowledge fusion, human-computer cooperation, evolutionary algorithms, prior knowledge, human intelligence},
abstract = {As a complex engineering problem, the satellite module layout design (SMLD) is difficult to resolve by using conventional computation-based approaches. The challenges stem from three aspects: computational complexity, engineering complexity, and engineering practicability. Engineers often finish successful satellite designs by way of their plenty of experience and wisdom, lessons learnt from the past practices, as well as the assistance of the advanced computational techniques. Enlightened by the ripe patterns, this article puts forward a knowledge fusion approach, which fuses online human knowledge, prior knowledge, and computational knowledge by using evolutionary computation to fully explore the advantages of human and computers. This article highlights the way to represent aforementioned three types of design knowledge, the model to describe problem and the method to fuse, and the roles human plays. The numerical experiments have demonstrated the feasibility of the proposed approach.}
}
@article{WU20183,
title = {The five key questions of human performance modeling},
journal = {International Journal of Industrial Ergonomics},
volume = {63},
pages = {3-6},
year = {2018},
note = {Human Performance Modeling},
issn = {0169-8141},
doi = {https://doi.org/10.1016/j.ergon.2016.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0169814116300427},
author = {Changxu Wu},
keywords = {Human performance modeling},
abstract = {Via building computational (typically mathematical and computer simulation) models, human performance modeling (HPM) quantifies, predicts, and maximizes human performance, human-machine system productivity and safety. This paper describes and summarizes the five key questions of human performance modeling: 1) Why we build models of human performance; 2) What the expectations of a good human performance model are; 3) What the procedures and requirements in building and verifying a human performance model are; 4) How we integrate a human performance model with system design; and 5) What the possible future directions of human performance modeling research are. Recent and classic HPM findings are addressed in the five questions to provide new thinking in HPM's motivations, expectations, procedures, system integration and future directions.}
}
@incollection{JEYAKUMAR2022231,
title = {Chapter 9 - Brain-computer interface in Internet of Things environment},
editor = {Akash Kumar Bhoi and Victor Hugo Costa {de Albuquerque} and Samarendra Nath Sur and Paolo Barsocchi},
booktitle = {5G IoT and Edge Computing for Smart Healthcare},
publisher = {Academic Press},
pages = {231-255},
year = {2022},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-323-90548-0},
doi = {https://doi.org/10.1016/B978-0-323-90548-0.00012-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323905480000127},
author = {Vijay Jeyakumar and Palani Thanaraj Krishnan and Prema Sundaram and Alex Noel Joseph Raj},
keywords = {Artificial intelligence, brain-computer interface, brain–to–things, deep learning, edge computing, human–machine interface},
abstract = {Brain-computer interface (BCI) also referred to as Brain-machine interface (BMI) is a buzz word in the world of neuroscience, translating the human brain thoughts into the chip. These devices may be surgically implanted or placed externally. Such components allow the user to control the actuators or sense the input data through bilateral communication to achieve the task. Most of the current applications focus on neural-prosthetics, artificial limbs, cochlear implants, and assistive devices for people affected by neurological disorders such as Alzheimer’s, Parkinson’s and more. Initially, it was developed to assist persons with neurological disorders. Due to the evolution of noninvasive imaging components, BCI is being extended for public communication like the brain–brain interfacing. The implementation of BCI on neuromorphic hardware components would further improve the computational complexity, execution speed, energy efficiency, and robustness against local failure. Machine learning and deep learning algorithms are contributing to computer vision, speech recognition, game control, autonomous vehicle systems, disease classification/prediction, and many more. Though BCI has improved the lifestyle of the end-users, the responsiveness of those devices is not alike natural elements. Hence, to create an effective pathway from a brain to the external world through mapping, augmenting, assisting and troubleshooting, many computational intelligence methods have been proposed. In this chapter, the translation of brain waves into features and further classified to control any applications in an open/closed environment with a secure mechanism along with adaptive learning algorithms will be discussed.}
}
@article{XU2018309,
title = {A Platform for Fault Diagnosis of High-Speed Train based on Big Data⁎⁎Project supported by the National Natural Science Foundation, China(61490704, 61440015) and the National High-Tech. R&D Program, China (No. 2015AA043802).},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {18},
pages = {309-314},
year = {2018},
note = {10th IFAC Symposium on Advanced Control of Chemical Processes ADCHEM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.09.318},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318320007},
author = {Quan Xu and Peng Zhang and Wenqin Liu and Qiang Liu and Changxin Liu and Liangyong Wang and Anthony Toprac and S. {Joe Qin}},
keywords = {Fault Diagnosis, High-Speed Train, Big Data, Cloud Computing, Edge Computing},
abstract = {High-speed trains are very fast (e.g. 350km/h) and operate at high traffic density, so once a fault has occurred, the consequences are disastrous. In order to better control the train operational status by timely and rapid detection of faults, we need new methods to handle and analyze the huge volumes of high-speed railway data. In this paper, we propose a novel framework and platform for high-speed train fault diagnosis based on big data technologies. The framework aims to allow researchers to focus on fault detection algorithm development and on-line application, with all the complexities of big data import, storage, management, and realtime use handled transparently by the framework. The framework uses a combination of cloud computing and edge computing and a two-level architecture that handles the massive data of train operations. The platform uses Hadoop as its basic framework and combines HDFS, HBase, Redis and MySQL database as the data storage framework. A lossless data compression method is presented to reduce the data storage space and improve data storage efficiency. In order to support various types of data analysis tasks for fault diagnosis and prognosis, the framework integrates online computation, off-line computation, stream computation, real-time computation and so on. Moreover, the platform provides fault diagnosis and prognosis as services to users and a simple case study is given to further illustrate how the basic functions of the platform are implemented.}
}
@article{GOLDIN1998137,
title = {Representational systems, learning, and problem solving in mathematics},
journal = {The Journal of Mathematical Behavior},
volume = {17},
number = {2},
pages = {137-165},
year = {1998},
note = {Representations and the Psychology of Mathematics Education: Part II},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0364-0213(99)80056-1},
url = {https://www.sciencedirect.com/science/article/pii/S0364021399800561},
author = {Gerald A. Goldin},
abstract = {This article explores aspects of a unified psychological model for mathematical learning and problem solving, based on several different types of representational systems and their stages of development. The goal is to arrive at a scientifically adequate theoretical framework, complex enough to account for diverse empirical results but sufficiently simple to be accessible and useful in mathematics education practice. Some perspectives on representational systems are discussed, and components of the model are described in relation to these ideas—including constructs related to imagistic thinking, heuristics and strategies, affect, and the fundamental role of ambiguity.}
}
@article{CAI20094343,
title = {The non-synergy field of convection},
journal = {International Journal of Heat and Mass Transfer},
volume = {52},
number = {19},
pages = {4343-4349},
year = {2009},
issn = {0017-9310},
doi = {https://doi.org/10.1016/j.ijheatmasstransfer.2009.03.063},
url = {https://www.sciencedirect.com/science/article/pii/S0017931009002567},
author = {Ruixian Cai and Qibin Liu and Chenhua Gou},
keywords = {Convection, Non-synergy, Analytical solution, Separating variables with addition, Hybrid separating method},
abstract = {The “field synergy principle”, a theory to approach best convection result proposed by Guo, Tao et al., has been developing since the end of last century. Besides the principle itself, there have been a lot of studies about the field synergy including computational approaches and experimental means to demonstrate and apply this principle. However, an opposite research direction: “ non-synergy” – how can we obtain worst convection result – is also worth researching for the theory and practice. Until now detailed studies have hardly been published in the open literatures. In this paper, a basic theoretical non-synergy research is presented, some algebraically explicit analytical exact solutions for the governing partial differential equation set of two-dimensional laminar incompressible full field non-synergy are derived using the extraordinary methods promoted by the authors, for example, the method of separating variables with the addition and other hybrid method. The obtained solutions include the conditions with the heat source, the mass flow source or no any sources. The physical feature of various solutions are discussed and explained by the means of figures. Besides theoretical meaning, the solutions can be benchmark solutions to develop the computational heat transfer.}
}
@article{ZHANG2023105133,
title = {Risk analysis of people evacuation and its path optimization during tunnel fires using virtual reality experiments},
journal = {Tunnelling and Underground Space Technology},
volume = {137},
pages = {105133},
year = {2023},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2023.105133},
url = {https://www.sciencedirect.com/science/article/pii/S0886779823001530},
author = {Xiaochun Zhang and Linjie Chen and Junhao Jiang and Yixin Ji and Shuyang Han and Ting Zhu and Wenbin Xu and Fei Tang},
keywords = {Tunnel fire, Virtual reality (VR）, Evacuation behavior, Risk analysis, Tunnel lifecycle safety management},
abstract = {The number of urban tunnels has been increasing rapidly, accompanied by frequent tunnel fire accidents owing to the complex tunnel structure and large traffic flow. In this study, a full-size tunnel virtual reality (VR) scenario and computational fluid dynamics (CFD) construction model were established to investigate the evacuation behavior and corresponding risk of people in the early stage of vehicle fires considering four scenarios: normal circumstance, without VR agents, without emergency evacuation signs, and without fire extinguishers. Firstly, the cumulative values of CO, CO2, and temperature along the evacuation path were monitored using CFD. Secondly, the smoke toxicity was calculated using the N-GAS model, and the total risk value was computed based on the analytic hierarchy process which was defined as “smoke hazard: temperature hazard = 7:3.” Thirdly, a multiple regression model was created based on accident data. Finally, to minimize accidents, the design of the evacuation path was optimized using the established mathematical model and A* algorithm to verify the effectiveness of the risk assessment model. The results show that the effects of VR agents, emergency evacuation signs, and fire extinguishers on the evacuation behavior of people are mutual influence. This study combined time and route in the VR evacuation experiments to overcome the limitations of the existing control and VR experiments in quantifying the evacuation results. This research can be utilized to improve emergency evacuation plans and emergency response decision making. Furthermore, it can broaden the application of VR in the field of tunnel lifecycle safety management.}
}
@article{PAPADOPOULOS2008311,
title = {Students’ use of technological tools for verification purposes in geometry problem solving},
journal = {The Journal of Mathematical Behavior},
volume = {27},
number = {4},
pages = {311-325},
year = {2008},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2008.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732312308000497},
author = {Ioannis Papadopoulos and Vassilios Dagdilelis},
keywords = {Verification, Problem solving, Technology},
abstract = {Despite its importance in mathematical problem solving, verification receives rather little attention by the students in classrooms, especially at the primary school level. Under the hypotheses that (a) non-standard tasks create a feeling of uncertainty that stimulates the students to proceed to verification processes and (b) computational environments – by providing more available tools compared to the traditional environment – might offer opportunities for more frequent usage of verification techniques, we posed to 5th and 6th graders non-routine problems dealing with area of plane irregular figures. The data collected gave us evidence that computational environments allow the development of verification processes in a wider variety compared to the traditional paper-and-pencil environment and at the same time we had the chance to propose a preliminary categorization of the students’ verification processes under certain conditions.}
}
@article{TOBACMAN20211,
title = {Troponin Revealed: Uncovering the Structure of the Thin Filament On-Off Switch in Striated Muscle},
journal = {Biophysical Journal},
volume = {120},
number = {1},
pages = {1-9},
year = {2021},
issn = {0006-3495},
doi = {https://doi.org/10.1016/j.bpj.2020.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S0006349520309036},
author = {Larry S. Tobacman},
abstract = {Recently, our understanding of the structural basis of troponin-tropomyosin’s Ca2+-triggered regulation of striated muscle contraction has advanced greatly, particularly via cryo-electron microscopy data. Compelling atomic models of troponin-tropomyosin-actin were published for both apo- and Ca2+-saturated states of the cardiac thin filament. Subsequent electron microscopy and computational analyses have supported and further elaborated the findings. Per cryo-electron microscopy, each troponin is highly extended and contacts both tropomyosin strands, which lie on opposite sides of the actin filament. In the apo-state characteristic of relaxed muscle, troponin and tropomyosin hinder strong myosin-actin binding in several different ways, apparently barricading the actin more substantially than does tropomyosin alone. The troponin core domain, the C-terminal third of TnI, and tropomyosin under the influence of a 64-residue helix of TnT located at the overlap of adjacent tropomyosins are all in positions that would hinder strong myosin binding to actin. In the Ca2+-saturated state, the TnI C-terminus dissociates from actin and binds in part to TnC; the core domain pivots significantly; the N-lobe of TnC binds specifically to actin and tropomyosin; and tropomyosin rotates partially away from myosin’s binding site on actin. At the overlap domain, Ca2+ causes much less tropomyosin movement, so a more inhibitory orientation persists. In the myosin-saturated state of the thin filament, there is a large additional shift in tropomyosin, with molecular interactions now identified between tropomyosin and both actin and myosin. A new era has arrived for investigation of the thin filament and for functional understandings that increasingly accommodate the recent structural results.}
}
@article{SCHOLTE201894,
title = {Toward a systems theatre: Proposal for a program of non-trivial modeling},
journal = {Futures},
volume = {103},
pages = {94-105},
year = {2018},
note = {Futures of Society: The Interactions Revolution},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2018.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0016328717302033},
author = {Tom Scholte},
keywords = {Augusto Boal, Theatre of the Oppressed, Theatre for Living, Systems theory, Cybernetics, Konstantin Stanislavski, Soft Systems Methodology, System Dynamics, Critical systems heurstics, Enactive Management},
abstract = {This paper makes the case for, and calls for participants in, an interdisciplinary research program exploring the development of theatrical methods of social system modeling. It combines argumentation that synthesizes concepts from the theatre and the system sciences with results from a pilot application of some of the modeling methods discussed. Theatrical methods of modeling facilitate surprising insights regarding the impacts of emotion and other non-trivial factors on system behaviour that are difficult to address in purely computational and diagrammatic forms of modeling. While a theoretical relationship between systems approaches and the theatrical techniques discussed has been articulated elsewhere, this paper is the first to propose a more fulsome exploration of the potentialities of this relationship for systems praxis.}
}
@article{EISENHOFER201940,
title = {Steroid metabolomics: machine learning and multidimensional diagnostics for adrenal cortical tumors, hyperplasias, and related disorders},
journal = {Current Opinion in Endocrine and Metabolic Research},
volume = {8},
pages = {40-49},
year = {2019},
note = {Adrenal Cortex},
issn = {2451-9650},
doi = {https://doi.org/10.1016/j.coemr.2019.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S2451965019300341},
author = {Graeme Eisenhofer and Claudio Durán and Triantafyllos Chavakis and Carlo Vittorio Cannistraci},
keywords = {Steroids, Steroidomics, Machine learning, Mass spectrometry, LC-MS/MS, Adrenal, Primary aldosteronism, Cushing's syndrome, Adrenal cortical carcinoma},
abstract = {Steroid profiling applications have a long history primarily directed toward diagnosis of endocrine disorders of childhood. Technological advances in mass spectrometry enabling rapid, sensitive, and specific measurements of multiple steroids in biological fluids are now paving the way for numerous other applications, including diagnosis of adrenocortical carcinoma, primary aldosteronism, and different forms of hypercortisolism. Such analytical procedures that target combinations of steroids in a single biological sample have potential for efficient one-shot methods for diagnosis of multiple disorders of steroidogenesis. Moreover, within a specific disorder, such methods can facilitate subtyping for more rapid therapeutic stratification than allowed by current methods that rely on single measurements per sample at sequential time points in a diagnostic process. Combined with advances in computational mathematics, such as machine learning, it is now possible to move from traditional unidimensional approaches for interpreting diagnostic data to methods that can interpret patterns in data. Mass spectrometry–based steroidomics provides an ideal platform for advancing such multidimensional approaches for disease diagnosis and stratification. Bottlenecks that must be overcome to move forward include needs for laboratory harmonization and method certification combined with ingrained reliance on outmoded, but well-accepted, diagnostic methods and general inertia to take advantage of new technologies.}
}