@article{ALI2024101172,
title = {Physics-informed neural networks in groundwater flow modeling: Advantages and future directions},
journal = {Groundwater for Sustainable Development},
volume = {25},
pages = {101172},
year = {2024},
issn = {2352-801X},
doi = {https://doi.org/10.1016/j.gsd.2024.101172},
url = {https://www.sciencedirect.com/science/article/pii/S2352801X2400095X},
author = {Ahmed Shakir Ali Ali and Farhad Jazaei and T. Prabhakar Clement and Brian Waldron},
keywords = {Artificial intelligence, Physics-informed neural network, PINN, Groundwater modeling, MODFLOW},
abstract = {In recent years, there has been enormous development in soft computing, especially artificial intelligence (AI), which has developed robust methods for solving complex engineering problems. Researchers in the field of water resources engineering have applied these AI methods to solve a variety of hydrological problems. Despite their widespread use in the surface and atmospheric hydrology fields, groundwater hydrologists have not widely used AI methods in their routine field-scale modeling efforts. This is because AI models have been primarily considered black box models that lack physical meaning. Furthermore, using AI models to generate the space-time distribution of transient groundwater level variations is challenging and requires further flux balance and mass transport analyses. More recently, a new type of physics-informed neural network (PINN) model has been developed to address several limitations by integrating governing physics (groundwater flow equations) into the AI tools. This study presents the systematic advantages of the PINN algorithm for solving groundwater problems using a set of classic test problems. As discussed in detail in the article, these advantages and potentials are associated with the meshless nature of PINN, its continuous time and space dimensions, its independence from time-stepping and incremental marching in space, and its efficiency in running time. However, despite PINN's promising attributes, it is important to acknowledge its nascent stage of development and the inherent limitations of all neural network models, such as training challenges and hyperparameter selection. Thus, collaborative efforts between groundwater modelers and computer scientists are imperative to explore and exploit the full potential of PINN in tackling increasingly complex groundwater problems and nurturing PINN into a dependable modeling tool in industry and academia.}
}
@article{YIN2024133163,
title = {Mobileception-ResNet for transient stability prediction of novel power systems},
journal = {Energy},
volume = {309},
pages = {133163},
year = {2024},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2024.133163},
url = {https://www.sciencedirect.com/science/article/pii/S0360544224029384},
author = {Linfei Yin and Wei Ge},
keywords = {Transient stability, Deep learning, MobileNet-v2, Convolutional neural network, Inception-ResNet-v2},
abstract = {Power system transient stability prediction (TSP) is particularly important as power systems change and evolve, including the rapid growth of renewable energy, the proliferation of electric vehicles, and the construction of smart grids. Traditional time-domain simulation methods are time-consuming and cannot achieve online prediction. Direct methods are poorly adapted and cannot be applied to complex power systems. Existing machine learning algorithms only classify the transient stability without providing the degree of transient stability of the system. Therefore, a fast and accurate power system TSP method is needed to assist operators in implementing timely measures to improve the stability of the power system running. This study proposes a Mobileception-ResNet network, Mobileception-ResNet is formed by Inception-ResNet-v2, MobileNet-v2, and a fully connected layer. In this study, Mobileception-ResNet and nine comparison models are experimented on two node systems, i.e., the IEEE 10–39 and 69–300 systems. In the IEEE 10–39 system, the root mean square error, mean absolute error, and mean absolute percentage error of Mobileception-ResNet are 44.13 %, 36.74 %, and 39.96 % lower, and the coefficient of determination is 0.04 % higher, respectively, when compared to the comparative model with the best evaluation indicator; in the IEEE 69–300 system, the corresponding values are 2.6 %, 12.83 %, 12.55 %, and 0.01 %, respectively.}
}
@article{VISWAN2023102808,
title = {Understanding molecular signaling cascades in neural disease using multi-resolution models},
journal = {Current Opinion in Neurobiology},
volume = {83},
pages = {102808},
year = {2023},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2023.102808},
url = {https://www.sciencedirect.com/science/article/pii/S0959438823001332},
author = {Nisha Ann Viswan and Upinder Singh Bhalla},
abstract = {If the genome defines the program for the operations of a cell, signaling networks execute it. These cascades of chemical, cell-biological, structural, and trafficking events span milliseconds (e.g., synaptic release) to potentially a lifetime (e.g., stabilization of dendritic spines). In principle almost every aspect of neuronal function, particularly at the synapse, depends on signaling. Thus dysfunction of these cascades, whether through mutations, local dysregulation, or infection, leads to disease. The sheer complexity of these pathways is matched by the range of diseases and the diversity of their phenotypes. In this review, we discuss how to build computational models, how these models are essential to tackle this complexity, and the benefits of using families of models at different levels of detail to understand signaling in health and disease.}
}
@article{MULLER1987271,
title = {Computational problems in supernova simulations},
journal = {Computer Physics Communications},
volume = {44},
number = {3},
pages = {271-277},
year = {1987},
issn = {0010-4655},
doi = {https://doi.org/10.1016/0010-4655(87)90082-8},
url = {https://www.sciencedirect.com/science/article/pii/0010465587900828},
author = {Ewald Müller},
abstract = {Theoretical models of type I and type II supernova explosions are reviewed from a computational physics point of view. After discussing briefly the underlying physics the numerical problems and challenges encountered in the simulation of type I and type II supernova are addressed.}
}
@article{1991202,
title = {Use of computational methods in drug design},
journal = {Chemometrics and Intelligent Laboratory Systems},
volume = {11},
number = {2},
pages = {202-203},
year = {1991},
issn = {0169-7439},
doi = {https://doi.org/10.1016/0169-7439(91)80072-X},
url = {https://www.sciencedirect.com/science/article/pii/016974399180072X}
}
@article{YANG20163,
title = {Modeling Urban Design with Energy Performance},
journal = {Energy Procedia},
volume = {88},
pages = {3-8},
year = {2016},
note = {CUE 2015 - Applied Energy Symposium and Summit 2015: Low carbon cities and urban energy systems},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2016.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1876610216300662},
author = {Perry Pei-Ju Yang and Jinyue Yan},
keywords = {Urban design computational model, Energy process model, Urban energy system, Urban design, Energy performance},
abstract = {Traditional urban design methods focus on the form-making process and lack performance dimensions such as energy efficiency. There are inherent differences between Urban Design as a model of decision-making for choosing form alternatives and Energy System Modeling as a model of evaluating and assessing system functions. To design a high energy performance city, the gap between the two models must be bridged. We propose a research design that combines the Urban Design Computational Model (UDCM) and the Optimization Model of Energy Process (OMEP) to demonstrate how an urban design computation can be integrated with an energy performance process and system. An evidence-based case study of community-level near zero energy districts will be needed for future work.}
}
@article{ISMAILOVA2021332,
title = {Equalities between Combinators to Evaluate Expressions},
journal = {Procedia Computer Science},
volume = {190},
pages = {332-340},
year = {2021},
note = {2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.06.058},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921013053},
author = {Larisa Ismailova and Viacheslav Wolfengagen and Sergey Kosikov},
keywords = {information process, model structure, indexed expressions, cognitive model, combinator-as-process},
abstract = {One of the aims of this work is to revise the known issues of how to obtain the value of the expression using an applicative computing system. Computation and/or symbolic transformations have undoubtedly become one of the dominant trends in modern computer science. In particular, this refers to the execution of the processes of inference of an object with given properties, which is studied with the formation of sets of equalities leading to a target computational model. At the same time, the mathematical theory of computation gets a target equational description, making the process of studying its properties and capabilities more suitable. A model structure based on the method of indexed expressions is developed and applied using the notions of evaluation mapping and assignments. As shown, this may not be considered as generic notion, but derived using the combinators. Thus, the semantics of computation can be derived from the construction of a combinator-as-process. This is the embedding into a system of combinators. A linking system of equalities between combinatorsl is established, which serving as a generic computational model. This allows us to look differently at the previous cognitive ideas of the semantics of computation disabling an evaluation map and assignments and enabling their replacement by a set of equalities between combinators.}
}
@article{LEACH2024,
title = {The engineering legacy of the FIFA World Cup Qatar 2022: Al Janoub stadium},
journal = {Proceedings of the Institution of Civil Engineers - Structures and Buildings},
year = {2024},
issn = {0965-0911},
doi = {https://doi.org/10.1680/jstbu.22.00127},
url = {https://www.sciencedirect.com/science/article/pii/S0965091124000155},
author = {Jon Leach and Craig Sparrow and Federico Iori and Hamad N. Al-Nuaimi and Mohammed Z. E. B. Elshafie and Nasser A. Al-Nuaimi},
keywords = {buildings, structures & design, thermal effects, wind loading & aerodynamics},
abstract = {Al Janoub was the first new-build stadium designed for the FIFA World Cup Qatar 2022. This paper describes the journey of the engineering design of the 40 000 seat stadium, from the concept and detailed design development stages led by AECOM and Zaha Hadid Architects, through to the design and build contract on site. An architectural jewel located in Al Wakrah, just south of the city of Doha, the stadium was a world-first, using state-of-the-art computational analysis and physical modelling to create a safe, cooled environment that satisfies FIFA's requirements for both player and spectator comfort in the extreme temperatures of the region. A sustainable post-tournament legacy was also a key factor of the design, allowing it to be reduced to a 20 000-capacity stadium for the local football club and community. The task of integrating the stadium's stringent performance requirements on this path-finder project, including extensive scientific research and development, was a challenge that was overcome through close collaboration between the design team and the Supreme Committee's subject matter experts. The project's success as a test-bed helped it to set the standard for other stadia as part of the overall FIFA World Cup Qatar 2022 programme.}
}
@article{WOLFENGAGEN2016359,
title = {Migration of the Individuals},
journal = {Procedia Computer Science},
volume = {88},
pages = {359-364},
year = {2016},
note = {7th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2016, held July 16 to July 19, 2016 in New York City, NY, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.07.449},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916317057},
author = {Viacheslav E. Wolfengagen and Larisa Yu. Ismailova and Sergey V. Kosikov and Irina A. Parfenova and Mikhail Yu. Ermak and Vasiliy D. Petrov and Ilya A. Nikulin and Victor A. Kholodov},
keywords = {data model, computational model, conceptual modeling, stage-by-stage cognition model, variable domains, Big Data, Thick Data},
abstract = {The individuals are modeled by the elements of variable domains. The primitive frame to detect the individual migration from domain to domain is proposed. The supporting computational model is based on a separation of individuals into actual, possible and virtual ones. As was shown, this leads to an adoption of the stage-by-stage cognition model with a pair of evolvents to capture dynamics of the domains – the 2-dimensions model. The first evolvent reflects the generation of the individuals in a domain, the beginning of and canceling out their existence in a domain. The second evolvent reflects the shifts in properties of the individuals. As awaited this unified data model will have the applications to a wide range of models in computer science and Information Technologies.}
}
@article{GRIFFIN1977127,
title = {On the application of boundary conditions to time dependent computations for quasi one-dimensional fluid flows},
journal = {Computers & Fluids},
volume = {5},
number = {3},
pages = {127-137},
year = {1977},
issn = {0045-7930},
doi = {https://doi.org/10.1016/0045-7930(77)90019-6},
url = {https://www.sciencedirect.com/science/article/pii/0045793077900196},
author = {Michael D. Griffin and Anderson {John D.}},
abstract = {A study is made of the influence of boundary and initial conditions on time-dependent finite-difference solutions of quasi-one-dimensional duct flows. Several questions are addressed: (1) Under what conditions will a time-dependent solution converge to a steady-state supersonic flow, (2) Under what conditions will it converge to subsonic flow and (3) What conditions are necessary to insure a particular unique solution for subsonic flows. The results provide an orientation, or way of thinking, about the role of such conditions in time-dependent solutions of steady-state flows. The results also show that supersonic solutions are readily obtained by holding only pressure and temperature fixed at the duct inlet, and allowing velocity to float. However, subsonic solutions require pressure, temperature and velocity to be fixed at both the duct inlet and exit. If no conditions are held fixed at the exit, the results always converge to the supersonic solution, even if the fixed inlet mass flow is less than critical. In such a case, the program appears to generate additional mass flow between the inlet and throat, sufficient to choke the flow. These results also have some impact on two- and three-dimensional time-dependent solutions where subsonic flow is present on some or all portions of the flow boundaries.}
}
@article{BRANASGARZA2012254,
title = {Cognitive effort in the Beauty Contest Game},
journal = {Journal of Economic Behavior & Organization},
volume = {83},
number = {2},
pages = {254-260},
year = {2012},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2012.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S0167268112001278},
author = {Pablo Brañas-Garza and Teresa García-Muñoz and Roberto Hernán González},
keywords = {Beauty Contest Game, Raven, Cognitive Reflection Test},
abstract = {This paper analyzes cognitive effort in 6 different oneshot p-beauty games. We use both Raven and Cognitive Reflection tests to identify subjects’ abilities. We find that the Raven test does not provide any insight on Beauty Contest Game playing but CRT does: subjects with higher scores on this test are more prone to play dominant strategies. The results are confirmed when levels of reasoning instead of entries in the BCG are used.}
}
@article{WORTMANN2017173,
title = {Differentiating parametric design: Digital workflows in contemporary architecture and construction},
journal = {Design Studies},
volume = {52},
pages = {173-197},
year = {2017},
note = {Parametric Design Thinking},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2017.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X17300352},
author = {Thomas Wortmann and Bige Tunçer},
keywords = {parametric design, design automation, architectural design, software design, parametric master model},
abstract = {This paper examines Parametric Design (PD) in contemporary architectural practice. It considers three case studies: The Future of Us pavilion, the Louvre Abu Dhabi and the Morpheus Hotel. The case studies illustrate how, compared to non-parametrically and older parametrically designed projects, PD is employed to generate, document and fabricate designs with a greater level of detail and differentiation, often at the level of individual building components. We argue that such differentiation cannot be achieved with conventional Building Information Modelling and without customizing existing software. We compare the case studies' PD approaches (objected-oriented programming, functional programming, visual programming and distributed visual programming) and decomposition, algorithms and data structures as crucial factors for the practical viability of complex parametric models and as key aspects of PD thinking.}
}
@article{KNIGHT20158,
title = {Making grammars: From computing with shapes to computing with things},
journal = {Design Studies},
volume = {41},
pages = {8-28},
year = {2015},
note = {Special Issue: Computational Making},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2015.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X15000605},
author = {Terry Knight and George Stiny},
keywords = {computational model(s), design theory, perception, reflective practice, shape grammar},
abstract = {Recent interest in making and materiality spans from the humanities and social sciences to engineering, science, and design. Here, we consider making through the lens of a unique computational theory of design: shape grammars. We propose a computational theory of making based on the improvisational, perception and action approach of shape grammars and the shape algebras that support them. We modify algebras for the materials (basic elements) of shapes to define algebras for the materials of objects, or things. Then we adapt shape grammars for computing shapes to making grammars for computing things. We give examples of making grammars and their algebras. We conclude by reframing designing and making in light of our computational theory of making.}
}
@article{CHRISTENSEN2016125,
title = {Towards a formal assessment of design literacy: Analyzing K-12 students' stance towards inquiry},
journal = {Design Studies},
volume = {46},
pages = {125-151},
year = {2016},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2016.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X1530140X},
author = {Kasper Skov Christensen and Mikkel Hjorth and Ole Sejer Iversen and Paulo Blikstein},
keywords = {design education, design research, reflective practices, evaluation},
abstract = {We present a tool for quantitative assessment of K-12 students' stance towards inquiry as an important part of students' development of design literacy. On a basis of design thinking literature, we position designerly stance towards inquiry as a prerequisite for engaging with wicked problems. The Design Literacy (DeL) assessment tool contains design of a qualitative survey question, a coding scheme for assessing aspects of a designerly stance towards inquiry, and a description of how, we have validated the results through a large-scale survey administration in K-12 education. Our DeL tool is meant to provide educators, leaders, and policy makers with strong arguments for introducing design literacy in K-12 schools, which, we posit, function within in an age of measurement.}
}
@article{LUO2023101957,
title = {A design model of FBS based on interval-valued Pythagorean fuzzy sets},
journal = {Advanced Engineering Informatics},
volume = {56},
pages = {101957},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.101957},
url = {https://www.sciencedirect.com/science/article/pii/S147403462300085X},
author = {Yuhan Luo and Minna Ni and Feng Zhang},
keywords = {FBS model, Pythagorean fuzzy sets, AHP, HOQ},
abstract = {The FBS (Function-Behaviour-Structure) model is a research model that stimulates creative thinking of designers in the design process. In order to reduce the influence of user requirement ambiguity on design results in the product design process and improve the accuracy of user requirements in the function-behavior-structure (FBS) design model, this paper proposes an interval-valued Pythagorean fuzzy set-based FBS model integrating AHP and HOQ methods. Firstly, the design model will use IVPF-AHP method to study user requirements and use interval-valued Pythagorean linguistic terms to replace the traditional scoring method of AHP to get the weight of each user requirement. Secondly, the conversion between user requirements and functions will be realized by IVPF-HOQ method, which converts customer requirements into functional characteristics and calculates the weights of each functional characteristic. Finally, the design focus will be filtered according to the order of importance of the functional characteristics, which will be used as functions to guide the development of the FBS model. In this paper, the feasibility and effectiveness of the proposed method will be verified by an application example of a hand-held fluorescence spectrometer. The results show that the proposed FBS model can effectively reduce the subjectivity and ambiguity in the decision-making process, improve the accuracy and information richness of user requirements, and effectively highlight the focus of the design study. The innovation of the proposed method is to provide a more objective and accurate innovative design method for user requirements through the integration of AHP, HOQ and FBS to effectively explore and analyze user requirements. The use of IVPFS to deal with fuzzy information in the design process in a more flexible manner can reduce the ambiguity of requirements when user data is small, and effectively improve the limitations of the FBS design model which is more subjective.}
}
@article{BISWAL2024110150,
title = {Unlocking the potential of signature-based drug repurposing for anticancer drug discovery},
journal = {Archives of Biochemistry and Biophysics},
volume = {761},
pages = {110150},
year = {2024},
issn = {0003-9861},
doi = {https://doi.org/10.1016/j.abb.2024.110150},
url = {https://www.sciencedirect.com/science/article/pii/S0003986124002728},
author = {Sruti Biswal and Bibekanand Mallick},
keywords = {Cancer, Anticancer drug, Drug repurposing, Gene signature},
abstract = {Cancer is the leading cause of death worldwide and is often associated with tumor relapse even after chemotherapeutics. This reveals malignancy is a complex process, and high-throughput omics strategies in recent years have contributed significantly in decoding the molecular mechanisms of these complex events in cancer. Further, the omics studies yield a large volume of cancer-specific molecular signatures that promote the discovery of cancer therapy drugs by a method termed signature-based drug repurposing. The drug repurposing method identifies new uses for approved drugs beyond their intended initial therapeutic use, and there are several approaches to it. In this review, we discuss signature-based drug repurposing in cancer, how cancer omics have revolutionized this method of drug discovery, and how one can use the cancer signature data for repurposed drug identification by providing a step-by-step procedural handout. This modern approach maximizes the use of existing therapeutic agents for cancer therapy or combination therapy to overcome chemotherapeutics resistance, making it a pragmatic and efficient alternative to traditional resource-intensive and time-consuming methods.}
}
@article{HORWICH201323622,
title = {Chaperonin-mediated Protein Folding},
journal = {Journal of Biological Chemistry},
volume = {288},
number = {33},
pages = {23622-23632},
year = {2013},
issn = {0021-9258},
doi = {https://doi.org/10.1074/jbc.X113.497321},
url = {https://www.sciencedirect.com/science/article/pii/S0021925820452524},
author = {Arthur L. Horwich},
keywords = {Protein Folding, Chaperone Chaperonin, Molecular Chaperone, Yeast, Protein Misfolding, Polypeptide},
abstract = {We have been studying chaperonins these past twenty years through an initial discovery of an action in protein folding, analysis of structure, and elucidation of mechanism. Some of the highlights of these studies were presented recently upon sharing the honor of the 2013 Herbert Tabor Award with my early collaborator, Ulrich Hartl, at the annual meeting of the American Society for Biochemistry and Molecular Biology in Boston. Here, some of the major findings are recounted, particularly recognizing my collaborators, describing how I met them and how our great times together propelled our thinking and experiments.}
}
@article{IBEZIM2024e02226,
title = {Potential dual inhibitors of Hexokinases and mitochondrial complex I discovered through machine learning approach},
journal = {Scientific African},
volume = {24},
pages = {e02226},
year = {2024},
issn = {2468-2276},
doi = {https://doi.org/10.1016/j.sciaf.2024.e02226},
url = {https://www.sciencedirect.com/science/article/pii/S2468227624001728},
author = {Akachukwu Ibezim and Emmanuel Onah and Sochi Chinaemerem Osigwe and Peter Ukwu Okoroafor and Onyeoziri Pius Ukoha and Jair Lage {de Siqueira-Neto} and Fidele Ntie-Kang and Karuppasamy Ramanathan},
keywords = {Hexokinases, Mitochondrial complex I, Cancer, MACCS fingerprints, Boruta algorithms, Machine learning, Metabolic plasticity},
abstract = {Hexokinases (Hks) and mitochondrial complex I (MCI) are involved in the energy metabolism of cells; glycolysis/fermentation and oxidative phosphorylation. Both Hks and MCI are known to play critical roles in either division of metabolic plasticity which enables tumor progression and proliferation in the presence of chemotherapies. Therefore, targeting these enzymes are important in cancer drug resistance. Here, computational models for the prediction of inhibition of Hks were developed based on experimental data and an optimal feature subset that was selected by the Boruta algorithm (a wrapper feature selection algorithm coupled with random forest). Out of the seven models that were explored, a random forest classifier gave the best prediction (GA = 0.84, FNR = 0.12 and AUC = 0.96 for the external dataset). Fragmentation analysis led to the identification of the unique structural scaffolds that characterize hexokinase inhibitors and non-inhibitors. The best Hks inhibition model predicted that 23 molecules out of the 191 dataset of MCI actives (IC50 ≤ 10 µM) that were screened, have more than 60 % probability of exhibiting Hk inhibitory activity. Hence, they are possible dual inhibitors of both targets. Furthermore, the 23 molecules’ core structures are members of the scaffolds that are unique to Hk inhibitors earlier predicted by fragment analysis. The need for dual targeting agents in cancer therapy, particularly in combating cancer drug resistance, highlights the relevance of these findings.}
}
@article{YANG2022107728,
title = {Mixed data-driven sequential three-way decision via subjective–objective dynamic fusion},
journal = {Knowledge-Based Systems},
volume = {237},
pages = {107728},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107728},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121009692},
author = {Xin Yang and Yang Chen and Hamido Fujita and Dun Liu and Tianrui Li},
keywords = {Three-way decision, Sequential three-way decision, Mixed data, subjective–objective, Dynamic fusion},
abstract = {In the context of granular computing, sequential three-way decision is a useful tool to triadic thinking, triadic computing and triadic processing from coarser to finer under multilevel and multiview granularity space. In this paper, we mainly explore a novel framework of sequential three-way decision for the fusion of mixed data from the subjective and objective dynamic perspectives. The former focuses on the decision maker’s dynamic behavior without considering the time-evolving data, and the latter emphasizes on dealing with dynamic mixed data over time by multi-stage decision-making. We firstly utilize four T-norm operators and kernel-based similarity relations to integrate different types of dynamic data. Then the subjective and objective models of sequential three-way decision are investigated based on decision thresholds, attribute importance and cost reduction. Finally, the comparative experiments are reported to verify that our proposed models can achieve the lower decision cost and the acceptable accuracy.}
}
@article{ZHU2023110006,
title = {Deep reinforcement learning-based edge computing offloading algorithm for software-defined IoT},
journal = {Computer Networks},
volume = {235},
pages = {110006},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.110006},
url = {https://www.sciencedirect.com/science/article/pii/S1389128623004516},
author = {Xiaojuan Zhu and Tianhao Zhang and Jinwei Zhang and Bao Zhao and Shunxiang Zhang and Cai Wu},
keywords = {Edge computing, Computing offloading, Software defined network, Internet of things, Deep reinforcement learning},
abstract = {Edge computing offloading can effectively solve the problem of insufficient computing resources for terminal devices and improve the performance and efficiency of the system. When network states and tasks change rapidly, data-driven intelligent algorithms have difficulty obtaining comprehensive statistics for accurate prediction, resulting in degraded performance of computational offloading and difficulty in adaptive adjustment. It is a current challenge to improve the environment-aware, intelligent optimization so that the computational offloading algorithm can adapt to the dynamic changes in network state and task demands, thus achieving global multi-objective optimization. This paper presents optimized edge computing offloading algorithm for software-defined IoT. First, to provide global state for making decisions, a software defined edge computing (SDEC) architecture is proposed. The edge layer is integrated into the control layer of software-defined IoT, and multiple controllers share the global network state information via east–west message exchange. Moreover, an edge computing offloading algorithm in software-defined IoT (ECO-SDIoT) based on deep reinforcement learning is proposed. It enables the controllers to offload the computing task to the most appropriate edge server according to the global states, task requirements, and reward. Finally, the performance metrics for edge computing offloading were evaluated in terms of unit task processing latency, load balancing of edge servers, task processing energy consumption, and task completion rate, respectively. Simulation results show that ECO-SDIoT can effectively reduce task completion time and energy consumption compared with other strategies.}
}
@article{ROSSI2011820,
title = {MAPIT: a pedagogical-relational ITS},
journal = {Procedia Computer Science},
volume = {3},
pages = {820-826},
year = {2011},
note = {World Conference on Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2010.12.135},
url = {https://www.sciencedirect.com/science/article/pii/S1877050910005107},
author = {Pier Giuseppe Rossi and Simone Carletti},
keywords = {Teachers’ thinking, Intelligent tutoring system, Multi agent system, Learning management system, Tracking data, Chat-bot},
abstract = {The majority of Intelligent Tutoring System architectures are focused on supporting learners through content retrieval or in one or more given subject matters; examples of this can be found in Baghera [1], MyClass, Andes [2], Gramy, Advanced Geometry Tutor [7]. The implementation of such architectures are time-consuming and are generally not interoperable with other domains [3]. The presented research describes the experimentation of a Open Source, LMS enhanced with elements of AI aiming at supporting online teachers’ and tutors’ work by using a KB specific to relational and pedagogical aspects, not connected to a specific subject matter. Such implementation needs to be provided of an authoring tool easily and readily usable by tutors and teachers of different subjects and with medium level IT training. Starting point of our investigation has been a preliminary analysis of machine-mediated, human-human interactions (MM-HHI) and communications by using the Teachers’ thinking approach [4], [5], [6]. We considered messages exchanged between teachers/tutors and online students in three post-graduate, online courses running at the University of Macerata during 2008–2010 by the Faculty of Education. The study showed that about 30% of messages concerned structured information that could be straightforwardly retrieved by an artificial agent; almost all remaining messages were instead deeply bound to student’s learning path or required a significant input by the teacher/tutor, while the residual part of messages could — to some extents — be delegated to an intelligent agent having access to students’ tracking data in order to display visual information to users or trigger alarms to tutors. The investigation carried out prompted us for the deployment of an Open Source chat-bot system that would retrieve information already coded into the courses or originated by students through the analysis of their activity logs; the chat-bot agent uses this structured information in order to answer students’ most common questions hence relieving teachers and tutors from doing this repetitive task. The system is being implemented on a OLAT ver. 6.3 LMS loosely coupled to a JADE-based Multi Agent System in charge of processing user tracking data and running the ALICE chat-bot integrated with the platform messaging system.}
}
@article{CRANFORD2022100638,
title = {Navigating the “Kessel Run” of digital materials acceleration},
journal = {Patterns},
volume = {3},
number = {11},
pages = {100638},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100638},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922002707},
author = {Steve Cranford},
abstract = {Computational methods such as machine learning, artificial intelligence, and big data in physical sciences, particularly materials science, have been exponentially growing in terms of progress, method development, and number of studies and related publications. This aggregate momentum of the community is palpable, and many exciting discoveries are likely on the horizon. But, like all endeavors, some thought should be given to the current trajectory of the field, ensuring the full potential of the new digital space.}
}
@article{ZHENG2021102174,
title = {An Attention-based Bi-LSTM Method for Visual Object Classification via EEG},
journal = {Biomedical Signal Processing and Control},
volume = {63},
pages = {102174},
year = {2021},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2020.102174},
url = {https://www.sciencedirect.com/science/article/pii/S174680942030313X},
author = {Xiao Zheng and Wanzhong Chen},
keywords = {Deep learning, Attention mechanism, EEG, Bi-LSTM, Visual perception},
abstract = {Background and Objective
Despite many models have been proposed for brain visual perception and content understanding via electroencephalograms (EEGs), due to the lack of research on the inherent temporal relationship, EEG-based visual object classification still demands the improvement on its accuracy and computation complexity.
Methods
To take full advantage of the uneven visual feature saturation between time segments, an end-to-end attention-based Bi-LSTM Method is proposed, named Bi-LSTM-AttGW. Two attention strategies are introduced to Bi-LSTM framework. The attention gate replaces the forget gate in traditional LSTM. It is only relevant to the historical cell state, and not related to the current input. Hence, the attention gate can greatly reduce the number of training parameters. Moreover, the attention weighting method is applied to Bi-LSTM output, and it can explore the most decisive information.
Results
The best classification accuracy achieved by Bi-LSTM-AttGW model is 99.50%. Compared with the state-of-art algorithms and baseline models, the proposed method has great advantages in classification performance and computational complexity. Considering brain region level contribution on visual cognition task, we also verify our method using EEG signals collected from the frontal and occipital regions, that are highly correlated with visual perception tasks.
Conclusions
The results show promise towards the idea that human brain activity related to visual recognition can be more effectively decoded by neural networks with neural mechanism. The experimental results not only could provide strong support for the modularity theory about the brain cognitive function, but show the superiority of the proposed Bi-LSTM model with attention mechanism again.}
}
@incollection{BUTTON199067,
title = {Chapter 4 - Going Up a Blind Alley: Conflating Conversation Analysis and Computational Modelling},
editor = {PAUL LUFF and NIGEL GILBERT and DAVID FROHLICH},
booktitle = {Computers and Conversation},
publisher = {Academic Press},
address = {London},
pages = {67-90},
year = {1990},
isbn = {978-0-08-050264-9},
doi = {https://doi.org/10.1016/B978-0-08-050264-9.50009-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780080502649500099},
author = {Graham Button},
abstract = {Publisher Summary
This chapter discusses the desirability of developing computational models of conversational phenomena, and the supportive role given to conversation analysis (CA) in the development of such models. The arguments presented in this chapter are not an attempt to restrict the range of creative resources that software designers might turn to for inspiration. In particular, it is implicitly endorsed in the attempts to develop descriptively adequate models of conversation for use in computer systems, and explicitly endorsed when it is argued that by providing a simulacrum of conversation one has naturally occurring conversation between computers and humans. The attraction of CA for people who want to develop rules of conversational organization that can be used to program computers is two-fold: (1) CA might seem to provide a ready-made package of conversational rules that they can use or adapt for their purposes; and (2) their models may be authorized by appealing to CA. However, CA is used to authorize computational models of conversation that misrepresent the details of how conversation works.}
}
@article{ESHAGHI2024107342,
title = {Methods for enabling real-time analysis in digital twins: A literature review},
journal = {Computers & Structures},
volume = {297},
pages = {107342},
year = {2024},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2024.107342},
url = {https://www.sciencedirect.com/science/article/pii/S0045794924000713},
author = {Mohammad Sadegh Es-haghi and Cosmin Anitescu and Timon Rabczuk},
abstract = {This paper presents a literature review on methods for enabling real-time analysis in digital twins, which are virtual models of physical systems. The advantages of digital twins are numerous, including cost reduction, risk mitigation, efficiency enhancement, and decision-making support. However, their implementation faces challenges such as the need for real-time data analysis, resource limitations, and data uncertainty. The paper focuses on methods for reducing computational demands, which have not been systematically discussed in the literature. The paper reviews and categorizes methods and tools for accelerating the modeling of physical phenomena and reducing the computational needs of digital twins.}
}
@article{BRADLEY2016277,
title = {Pilot Testing the Debriefing for Meaningful Learning Evaluation Scale},
journal = {Clinical Simulation in Nursing},
volume = {12},
number = {7},
pages = {277-280},
year = {2016},
issn = {1876-1399},
doi = {https://doi.org/10.1016/j.ecns.2016.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S1876139916000104},
author = {Cynthia Sherraden Bradley and Kristina Thomas Dreifuerst},
keywords = {DML, debriefing, effective briefing, debriefing evaluation, measurement},
abstract = {Background
Debriefing for Meaningful Learning (DML), an evidence-based debriefing method, promotes thinking like a nurse through reflective learning. Despite widespread adoption of DML, little is known about how well it is implemented. To assess the effectiveness of DML implementation, an evaluative rubric was developed and tested.
Sample
Three debriefers who had been trained to use DML at least 1 year previously, submitted five recorded debriefings each for evaluation.
Methods
Three raters who were experts in DML scored each of the 15 recorded debriefing session using DML Evaluation Scale (DMLES). Observable behaviors were scored with binary options. These raters also assessed the items in the DMLES for content validity.
Results
Cronbach's alpha, intraclass correlation coefficients, and Content Validity Index scores were calculated to determine reliability and validity.
Conclusion
Use of DMLES could support quality improvement, teacher preparation, and faculty development. Future testing is warranted to investigate the relationship between DML implementation and clinical reasoning.}
}
@article{DEVGUN2023141,
title = {Pre-cath Laboratory Planning for Left Atrial Appendage Occlusion – Optional or Essential?},
journal = {Cardiac Electrophysiology Clinics},
volume = {15},
number = {2},
pages = {141-150},
year = {2023},
note = {Left Atrial Appendage Occlusion},
issn = {1877-9182},
doi = {https://doi.org/10.1016/j.ccep.2023.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S1877918223000205},
author = {Jasneet Devgun and Tom {De Potter} and Davide Fabbricatore and Dee Dee Wang},
keywords = {Left atrial appendage occlusion, Left atrial appendage, Atrial fibrillation, Cardiac CT, 3D printing, Imaging, Structural heart disease}
}
@incollection{BILLEN2023385,
title = {Chapter 16 - Lithosphere–Mantle Interactions in Subduction Zones},
editor = {João C. Duarte},
booktitle = {Dynamics of Plate Tectonics and Mantle Convection},
publisher = {Elsevier},
pages = {385-405},
year = {2023},
isbn = {978-0-323-85733-8},
doi = {https://doi.org/10.1016/B978-0-323-85733-8.00014-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323857338000147},
author = {Magali I. Billen},
keywords = {Subduction dynamics, Rheology, Phase transitions, Numerical modeling, Mantle mixing},
abstract = {How does the interaction of sinking lithosphere with the mantle contribute to the motion of tectonics plates at the Earth's surface and to long-term mixing in the deep mantle? In the decades immediately following the acceptance of the theory of plate tectonics, these questions were pursued vigorously using analytical, laboratory, and numerical models. In the past two decades, attention has turned to building on this foundational knowledge using numerical simulations to more fully integrate the complexity of Earth materials including the effects of deformation mechanisms, composition, fluids, melting, and phase transitions. This ongoing transition to a more system-centered view of geodynamics and plate tectonics not only presents many challenges (computational, experimental, and theoretical) but also promises to bridge the gaps in our current understanding and address the still enigmatic behavior of sinking lithosphere.}
}
@article{CAMARGO20181116,
title = {A method for integrated process simulation in the mining industry},
journal = {European Journal of Operational Research},
volume = {264},
number = {3},
pages = {1116-1129},
year = {2018},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2017.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0377221717306409},
author = {Luis Felipe Riehs Camargo and Luis Henrique Rodrigues and Daniel Pacheco Lacerda and Fabio Sartori Piran},
keywords = {O.R. in natural resources, Production, Simulation, Systems thinking},
abstract = {This paper proposes a method of Integrated Process Simulation (MIPS), which considers the dynamic, stochastic and systemic characteristics of mining operations to support investment decisions in this industry. This MIPS supports development of a Decision Support System (DSS) that considers product quality, process productivity and production costs. A case study is described that used the MIPS to make better investment decisions. The MIPS has proven, in practice, to be effective in several applications; for example, in defining the maintenance policy for critical equipment in an iron ore concentration plant; the process for removing impurities and simulating the company's budget to evaluate the viability of different business plans.}
}
@article{MARITAN2022167351,
title = {Building Structural Models of a Whole Mycoplasma Cell},
journal = {Journal of Molecular Biology},
volume = {434},
number = {2},
pages = {167351},
year = {2022},
issn = {0022-2836},
doi = {https://doi.org/10.1016/j.jmb.2021.167351},
url = {https://www.sciencedirect.com/science/article/pii/S002228362100588X},
author = {Martina Maritan and Ludovic Autin and Jonathan Karr and Markus W. Covert and Arthur J. Olson and David S. Goodsell},
keywords = {whole cell modeling, computational modeling, nucleoid structure, scientific visualization, mycoplasma genitalium},
abstract = {Building structural models of entire cells has been a long-standing cross-discipline challenge for the research community, as it requires an unprecedented level of integration between multiple sources of biological data and enhanced methods for computational modeling and visualization. Here, we present the first 3D structural models of an entire Mycoplasma genitalium (MG) cell, built using the CellPACK suite of computational modeling tools. Our model recapitulates the data described in recent whole-cell system biology simulations and provides a structural representation for all MG proteins, DNA and RNA molecules, obtained by combining experimental and homology-modeled structures and lattice-based models of the genome. We establish a framework for gathering, curating and evaluating these structures, exposing current weaknesses of modeling methods and the boundaries of MG structural knowledge, and visualization methods to explore functional characteristics of the genome and proteome. We compare two approaches for data gathering, a manually-curated workflow and an automated workflow that uses homologous structures, both of which are appropriate for the analysis of mesoscale properties such as crowding and volume occupancy. Analysis of model quality provides estimates of the regularization that will be required when these models are used as starting points for atomic molecular dynamics simulations.}
}
@article{SCHWABER1993126,
title = {Computational modeling of neuronal dynamics for systems analysis: application to neurons of the cardiorespiratory NTS in the rat},
journal = {Brain Research},
volume = {604},
number = {1},
pages = {126-141},
year = {1993},
issn = {0006-8993},
doi = {https://doi.org/10.1016/0006-8993(93)90359-U},
url = {https://www.sciencedirect.com/science/article/pii/000689939390359U},
author = {J.S. Schwaber and E.B. Graves and J.F.R. Paton},
keywords = {Nucleus tractus solitarii, Systems modeling, Cardiovascular reflex, Neuronal dynamics},
abstract = {The study constructs computational models of neurons in order to examine the contribution that their response dynamics may make to functional properties at the system level. As described in the accompanying study, neurons in the cardiorespiratory nucleus tractus solitarii (NTS) of the rat were recorded in vitro. When these cells were intracellularly injected with a constant current pulse, spike discharge patterns and subthreshold voltage trajectories were observed that were time- and voltage-dependent. The accompanying manuscript describes these dynamic responses in 4 classes of putative second-order cells that appear to receive direct primary afferent input, and a previous paper described two populations of rhythmically firing interneurons, one of which is intrinsically auto-active. In the present manuscript experimental neuronal voltage response data was collected across a current injection series for the S3 neuron type described in the accompanying study and for the auto-active neuron described previously. Using this data, computational model neurons have been constructed for these two neurons by using membrane ion channels to produce and match the observed neuronal voltage behavior. The channels were those implicated in the dynamic responses observed in the companion study, and include gNafast, gKdr, gKA, gKCa, gKAHP, gKM, gCaT and gCaL. The description of channel kinetics follows the Hodgkin-Huxley form. Different neuronal sources from the literature of channel kinetics were investigated and assembled into a ‘channel kinetics library’ from which both neuron models were tuned, primarily by adjusting the maximum channel densities, g¯, and time-dependence of kinetics. Methods are described for tuning the channel kinetics library to match various physiological responses. This approach created neuron models that were able to closely replicate the observed complex voltage and spiking responses of the two very different cardiorespiratory NTS neurons. The interaction of voltage- and calcium-dependent conductances were analyzed for their functional contributions by tuning their kinetics. Specific parameters are given that account for the behavior of each model. Sensitivity analyses by perturbing KCa and KA are are shown for both neurons, and I/F curves are presented for the auto-active neuron's simulated and recorded responses. The potential systems-level functional implications resulting from the different kinetics is demonstrated by driving the S3 model neuron in simulation with the pattern of input produced by model primary baroreceptor afferents. The limitations and significance of this approach are discussed. The present study of model neurons are being extended to the larger family of neurons found in the cardiorespiratory NTS (e.g. S1, S2 and S4), are being related to the baroreceptor vagal reflex by in vivo studies, and are being used to explore systems level computation, for example by creating networks reflecting baroreceptor reflex organization. The present kinetics library in principle could be used in this way for other neuronal systems.}
}
@article{PIQUEIRA2016271,
title = {A comparison of LMC and SDL complexity measures on binomial distributions},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {444},
pages = {271-275},
year = {2016},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2015.10.040},
url = {https://www.sciencedirect.com/science/article/pii/S0378437115008882},
author = {José Roberto C. Piqueira},
keywords = {Binomial, Complexity, Information, Measure, Probability},
abstract = {The concept of complexity has been widely discussed in the last forty years, with a lot of thinking contributions coming from all areas of the human knowledge, including Philosophy, Linguistics, History, Biology, Physics, Chemistry and many others, with mathematicians trying to give a rigorous view of it. In this sense, thermodynamics meets information theory and, by using the entropy definition, López-Ruiz, Mancini and Calbet proposed a definition for complexity that is referred as LMC measure. Shiner, Davison and Landsberg, by slightly changing the LMC definition, proposed the SDL measure and the both, LMC and SDL, are satisfactory to measure complexity for a lot of problems. Here, SDL and LMC measures are applied to the case of a binomial probability distribution, trying to clarify how the length of the data set implies complexity and how the success probability of the repeated trials determines how complex the whole set is.}
}
@article{BORDY201329,
title = {Radiotherapy out-of-field dosimetry: Experimental and computational results for photons in a water tank},
journal = {Radiation Measurements},
volume = {57},
pages = {29-34},
year = {2013},
note = {Proceedings of the Workshop: Dosimetry for Second Cancer Risk Estimation EURADOS Annual Meeting Vienna 2012},
issn = {1350-4487},
doi = {https://doi.org/10.1016/j.radmeas.2013.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S1350448713002710},
author = {J.M. Bordy and I. Bessieres and E. d'Agostino and C. Domingo and F. d'Errico and A. {di Fulvio} and Ž. Knežević and S. Miljanić and P. Olko and A. Ostrowsky and B. Poumarede and S. Sorel and L. Stolarczyk and D. Vermesse},
keywords = {Radiotherapy, Photon dosimetry, Out of field doses, Scatter radiation, Leakage},
abstract = {The first objective of this work was to check and select a set of four kinds of passive photon, dosimeters (two thermo-luminescence dosimeter (TLD) types, one radiophotoluminescence (RPL) dosimeter and one optically stimulated luminescence (OSL) dosimeter) together with a common measurement protocol. Dosimeters were calibrated in a reference clinical linear acccelerator beam in a water tank at a reference facility at the Laboratoire National Henri Becquerel (CEA LIST/LNE LNHB, Saclay. Radiation qualities of 6, 12 and 20 MV were used with standard calibration conditions described in IAEA TRS 398 and non-standard conditions. Profile and depth dose ion chamber measurements were also made to provide reference values. Measurements were made in a water tank into which pipes could be inserted which held dosimeters in pre-determined and reproducible positions. The water tank was built to enable investigation of doses up to 60 cm from the beam axis. A first set of experiments was carried out with the beam passing through the tank. From this first experiment, penumbra and out-of-field dose profiles including water and collimator scatter and leakage were found over three orders of magnitude. Two further sets of experiments using the same experimental arrangement with the beam outside the tank, to avoid water scatter, were designed to measure collimator scatter and leakage by closing the jaws of the collimator. Depending on the energy, typical leakage and collimator scatter represents 10–40% and 30–50% of the total out-of-field doses respectively. It was concluded that all dosimeters can be used for out-of-field photon dosimetry. All show good uniformity, good reproducibility, and can be used down to low doses expected at distances remote from the subsequent radiotherapy target volume.}
}
@article{FITZPATRICK2020101942,
title = {The relation between academic abilities and performance in realistic word problems},
journal = {Learning and Individual Differences},
volume = {83-84},
pages = {101942},
year = {2020},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2020.101942},
url = {https://www.sciencedirect.com/science/article/pii/S1041608020301229},
author = {Cheryll L. Fitzpatrick and Darcy Hallett and Kyle R. Morrissey and Nadine R. Yıldız and Rutanya Wynes and Felix Ayesu},
keywords = {Word problems, Academic abilities, Educational psychology, Math cognition},
abstract = {The research on realistic word problems investigates how children (and even adults) largely fail to incorporate real-world knowledge into mathematical word problems. Because of this, most research in this area focuses on improving realistic thinking. However, very little research has explored what other abilities might predict which children actually do take real-world information into account, and what this might imply about the nature of realistic responding. We tested whether general academic abilities, such as verbal skill, reading comprehension, and math calculation skill, previously shown to be related to standard word problem performance, are related to realistic responses, and whether realistic responding is related to standard word problem solving. In our sample of sixth-grade students, only reading comprehension was independently predictive of solving realistic word problems. Performance on realistic word problems, however, was independently predictive of solving standard word problems. As such, realistic word problems may reflect problem solving ability independent of general academic ability, and therefore may be an ability worth fostering.}
}
@incollection{KOCH2009125,
title = {Consciousness: Theoretical and Computational Neuroscience},
editor = {Larry R. Squire},
booktitle = {Encyclopedia of Neuroscience},
publisher = {Academic Press},
address = {Oxford},
pages = {125-130},
year = {2009},
isbn = {978-0-08-045046-9},
doi = {https://doi.org/10.1016/B978-008045046-9.00407-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780080450469004071},
author = {C. Koch and G. Tononi},
abstract = {Consciousness is a puzzling, state-dependent property of certain types of complex, adaptive, and highly interconnected systems. The best example is a healthy and attentive human brain. If the brain is anesthetized, consciousness ceases. Small lesions in the midbrain and thalamus of patients can lead to a complete loss of consciousness, while destruction of circumscribed parts of the cerebral cortex of patients can eliminate very specific aspects of consciousness, such as the ability to be aware of motion or to recognize objects (e.g., faces), without a concomitant loss of consciousness in general. Given the similarity in brain structure and behavior, biologists commonly assume that at least some animals, in particular nonhuman primates, share certain aspects of consciousness with humans. Brain scientists are exploiting a number of empirical approaches that shed light on the neural basis of consciousness. At present, it is not known to what extent artificial systems, such as computers, robots, or the World Wide Web as a whole, are or can become ‘conscious.’ What is needed is a theory of consciousness that explains in quantitative terms what types of systems, with what architecture, can possess conscious states.}
}
@article{MARTINI2022105446,
title = {R_IC: A novel and versatile implementation of the index of connectivity in R},
journal = {Environmental Modelling & Software},
volume = {155},
pages = {105446},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105446},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222001529},
author = {Lorenzo Martini and Tommaso Baggio and Loris Torresani and Stefano Crema and Marco Cavalli},
keywords = {Sediment connectivity, Geomorphometry, R_IC, Open-source},
abstract = {Sediment connectivity is the capability of a system to regulate the exchange of sediment in catchments. The Index of Connectivity (IC) has become a widely used tool, offering a practical way to assess sediment connectivity from hillslopes to downstream channels. We present a novel implementation of IC in R environment to expand the audience of users and encourage alternative applications of the index. The R_IC is an open-source and freely available tool composed by three codes. Standard R_IC runs the IC and it represents the core of the other variants. Custom R_IC offers a more flexible script, allowing the computation of alternative weighting factors and the possibility of running a further profile IC analysis. Batch R_IC performs batch processing of the index. For each code variant, a geomorphological application is presented to illustrate how the R_IC could be used in watershed management and practical issues related to sediment dynamics.}
}
@article{BROWN19921,
title = {Some conceptual issues in the modeling and computational analysis of the Canada-U.S. Free Trade Agreement},
journal = {The North American Journal of Economics and Finance},
volume = {3},
number = {1},
pages = {1-20},
year = {1992},
issn = {1062-9408},
doi = {https://doi.org/10.1016/1062-9408(92)90009-G},
url = {https://www.sciencedirect.com/science/article/pii/106294089290009G},
author = {Drusilla K. Brown and Robert M. Stern},
abstract = {We present an interpretive history of the development of the computational analysis of the Canada-U.S. FTA. Several important conceptual issues are identified, including: perfect competition and national product differentiation; imperfect competition and increasing returns to scale; tariff liberalization and monopolistic competition; adjustment and dynamic effects; macroeconomic effects; and other pertinent aspects of market structure and firm behavior.}
}
@article{PEZZULO2013270,
title = {Action simulation in the human brain: Twelve questions},
journal = {New Ideas in Psychology},
volume = {31},
number = {3},
pages = {270-290},
year = {2013},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2013.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X13000263},
author = {Giovanni Pezzulo and Matteo Candidi and Haris Dindo and Laura Barca},
keywords = {Action simulation, Internal model, Forward model, Motor control, Action understanding},
abstract = {Although the idea of action simulation is nowadays popular in cognitive science, neuroscience and robotics, many aspects of the simulative processes remain unclear from empirical, computational, and neural perspectives. In the first part of the article, we provide a critical review and assessment of action simulation theories advanced so far in the wider literature of embodied and motor cognition. We focus our analysis on twelve key questions, and discuss them in the context of human and (occasionally) primate studies. In the second part of the article, we describe an integrative neuro-computational account of action simulation, which links the neural substrate (as revealed in neuroimaging studies of action simulation) to the components of a computational architecture that includes internal modeling, action monitoring and inhibition mechanisms.}
}
@article{BUTLER2021170,
title = {Expert performance and crowd wisdom: Evidence from English Premier League predictions},
journal = {European Journal of Operational Research},
volume = {288},
number = {1},
pages = {170-182},
year = {2021},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2020.05.034},
url = {https://www.sciencedirect.com/science/article/pii/S037722172030480X},
author = {David Butler and Robert Butler and John Eakins},
keywords = {OR in sports, Prediction, Experts},
abstract = {This paper analyses the forecasting accuracy of experts vis-à-vis laypeople over three seasons of English Premier League matches. We find that former professional football players have superior forecasting ability when compared to laypeople. The results give partial support to the view that a crowd forecast offers the greatest precision. Pundits generate a positive return while both the crowd and laypeople generate losses. As the prediction of multiple score outcomes represents a computationally difficult task, both groups display forecasting biases including a preference toward specific score forecasts. The results are relevant for those concerned with gambling behaviour if the forecasting strategies adopted here generalise to match betting markets.}
}
@article{JOHNSON1989319,
title = {Exploiting parallelism in computational science},
journal = {Future Generation Computer Systems},
volume = {5},
number = {2},
pages = {319-337},
year = {1989},
note = {Grand Challenges to Computational Science},
issn = {0167-739X},
doi = {https://doi.org/10.1016/0167-739X(89)90050-2},
url = {https://www.sciencedirect.com/science/article/pii/0167739X89900502},
author = {Gary M. Johnson},
abstract = {The full exploitation of numerical simulation as an independent approach to the solution of engineering and scientific problems requires computing capability far exceeding that which is presently available. In this paper, the computing requirements posed by challenging problems in several disciplines are examined and contrasted with contemporary supercomputer resources. Of the means available to help fill the gap between the demands of computational science and the performance level of present-generation supercomputer systems, parallel processing appears to have the greatest potential for near-term success. Parallel computer architectures are reviewed and categorized according to processing units, memory, and interconnection scheme. Philosophies of parallel processing are discussed. They are distinguished by the number and size of the parallel tasks which they employ. Scientific problems are examined for parallelism inherent at the physical level. Typical algorithms and their mappings onto parallel architectures are discussed. Computational examples are presented to document the performance of scientific applications on present-generation parallel processors. Projections are made concerning software developments and machine architectures.}
}
@incollection{MILLER2020181,
title = {Chapter eight - Data science and the exposome},
editor = {Gary W. Miller},
booktitle = {The Exposome (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {181-209},
year = {2020},
isbn = {978-0-12-814079-6},
doi = {https://doi.org/10.1016/B978-0-12-814079-6.00008-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128140796000080},
author = {Gary W. Miller},
keywords = {Bioinformatics, systems biology, models, computational biology, machine learning, Bayesian methods, artificial intelligence, causal inference},
abstract = {Data science is focused on extracting meaningful value from complex datasets. Exposome-related data are certainly complex with information coming from a myriad of sources. The huge amounts of data must be organized in some manner that allows appropriate interpretations and associations to be drawn. It is unlikely that unsupervised approaches will allow for causal associations to be made, but with proper study design and appropriate statistical and computational models, it should be possible to derive meaningful connections between complex exposures and specific health outcomes. The complex types of data will undoubtedly require sophistical mathematical approaches, including bioinformatics, computational, machine learning, and systems biology-based techniques. This chapter reviews some of the possible strategies that can be used to keep track of the diverse and massive datasets that will result from exposome research.}
}
@article{FATH2005485,
title = {Elucidating public perceptions of environmental behavior: a case study of Lake Lanier},
journal = {Environmental Modelling & Software},
volume = {20},
number = {4},
pages = {485-498},
year = {2005},
note = {Vulnerability of Water Quality in Intensively Developing Urban Watersheds},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2004.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364815204000611},
author = {Brian D. Fath and M.B. Beck},
keywords = {Cultural theory, Integrated environmental assessment, Stakeholder participation},
abstract = {Participation of stakeholders in stewardship of the aquatic environment, including participation from members of the general public, has become much more widespread than was the case a decade or so ago. With this shift, from a former predominantly technocratic stance to something of a democratic stance on the style of management, it becomes important to elucidate public perceptions of environmental behavior. The paper examines this issue: from a rather specific perspective, where the role of time is significant; with a specific purpose in mind—for defining illustrative stakeholder aspirations for the future, whose plausibility is to be assessed against a computational model of lake behavior; and for a specific case study, Lake Lanier in the Chattahoochee watershed of Georgia, USA. Perturbations and variation in the behavior of the aquatic environment span many time frames, from the very short-term response associated with storms, infrastructure failure, transient pollution events, and so on, to the much longer-term, for instance, the biogeochemical ‘ageing’ of a lake over many decades and more. Our analysis is devoted to data from a survey of stakeholder imagination and perceptions of how the future state of Lake Lanier may evolve in the relatively short term (2–5 years) and in the long term, defined as 25+ years (the span of a generation). Overall, stakeholders are pessimistic and fear that things will be worse in the longer term. Guided largely by thinking on the perspectives of the social solidarities of Cultural Theory, extraction and analysis of sub-samples of the survey responses show that this outlook over the two frames of time is persistent, irrespective of what are, in principle, rather different ‘global’ attitudes towards the man-environment relationship. Of interest inter alia to the foresight generating procedure, by which the ‘reachability’ of stakeholder-derived futures for the lake is to be assessed using a computational model of the relevant parts of the science base, is the question of whether the same small number of priorities for further research on lake behavior is robust in the face of the rich variety of aspirations for the future inevitable in a democratic community of stakeholders.}
}
@article{YU2024107998,
title = {Bridging the gap: Geometry-centric discriminative manifold distribution alignment for enhanced classification in colorectal cancer imaging},
journal = {Computers in Biology and Medicine},
volume = {170},
pages = {107998},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.107998},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524000829},
author = {Weiwei Yu and Nuo Xu and Nuanhui Huang and Houliang Chen},
keywords = {Medical image analysis, Colorectal cancer detection, Domain adaptation, Transfer learning, Manifold learning, Computational oncology},
abstract = {The early detection of colorectal cancer (CRC) through medical image analysis is a pivotal concern in healthcare, with the potential to significantly reduce mortality rates. Current Domain Adaptation (DA) methods strive to mitigate the discrepancies between different imaging modalities that are critical in identifying CRC, yet they often fall short in addressing the complexity of cancer's presentation within these images. These conventional techniques typically overlook the intricate geometrical structures and the local variations within the data, leading to suboptimal diagnostic performance. This study introduces an innovative application of the Discriminative Manifold Distribution Alignment (DMDA) method, which is specifically engineered to enhance the medical image diagnosis of colorectal cancer. DMDA transcends traditional DA approaches by focusing on both local and global distribution alignments and by intricately learning the intrinsic geometrical characteristics present in manifold space. This is achieved without depending on the potentially misleading pseudo-labels, a common pitfall in existing methodologies. Our implementation of DMDA on three distinct datasets, involving several unique DA tasks, has consistently demonstrated superior classification accuracy and computational efficiency. The method adeptly captures the complex morphological and textural nuances of CRC lesions, leading to a significant leap in domain adaptation technology. DMDA's ability to reconcile global and local distributional disparities, coupled with its manifold-based geometrical structure learning, signals a paradigm shift in medical imaging analysis. The results obtained are not only promising in terms of advancing domain adaptation theory but also in their practical implications, offering the prospect of substantially improved diagnostic accuracy and faster clinical workflows. This heralds a transformative approach in personalized oncology care, aligning with the pressing need for early and accurate CRC detection.}
}
@article{CHEN201217,
title = {Data-Brain driven systematic human brain data analysis: A case study in numerical inductive reasoning centric investigation},
journal = {Cognitive Systems Research},
volume = {15-16},
pages = {17-32},
year = {2012},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2010.12.014},
url = {https://www.sciencedirect.com/science/article/pii/S138904171100012X},
author = {Jianhui Chen and Ning Zhong and Peipeng Liang},
keywords = {Data-Brain, Systematic human brain data analysis, Provenance, Brain Informatics},
abstract = {As a crucial step in understanding human intelligence, Brain Informatics (BI) focuses on thinking centric investigations of human cognitive functions with respect to multiple activated brain areas and neurobiological processes for a given task. Although it has been recognized that systematic human brain data analysis is an important issue of BI methodology, the existing expert-driven multi-aspect data analysis excessively depends on individual capabilities and cannot be widely adopted in BI community. In this paper, we propose a Data-Brain driven approach for systematic brain data analysis, which is implemented by using the Data-Brain, Data-Brain based BI provenances and Global Learning Scheme for BI. Furthermore, a human numerical inductive reasoning centric investigation is described to demonstrate significance and usefulness of the proposed approach. Such a Data-Brain driven approach reduces the dependency on individual capabilities and provides a practical way for realizing the systematic human brain data analysis of BI methodology.}
}
@article{BESOLD201597,
title = {Towards integrated neural–symbolic systems for human-level AI: Two research programs helping to bridge the gaps},
journal = {Biologically Inspired Cognitive Architectures},
volume = {14},
pages = {97-110},
year = {2015},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2015.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X15000468},
author = {Tarek R. Besold and Kai-Uwe Kühnberger},
keywords = {Research program, Neural–symbolic integration, Complexity theory, Cognitive architectures, Agent architectures},
abstract = {After a human-level AI-oriented overview of the status quo in neural–symbolic integration, two research programs aiming at overcoming long-standing challenges in the field are suggested to the community: The first program targets a better understanding of foundational differences and relationships on the level of computational complexity between symbolic and subsymbolic computation and representation, potentially providing explanations for the empirical differences between the paradigms in application scenarios and a foothold for subsequent attempts at overcoming these. The second program suggests a new approach and computational architecture for the cognitively-inspired anchoring of an agent’s learning, knowledge formation, and higher reasoning abilities in real-world interactions through a closed neural–symbolic acting/sensing–processing–reasoning cycle, potentially providing new foundations for future agent architectures, multi-agent systems, robotics, and cognitive systems and facilitating a deeper understanding of the development and interaction in human-technological settings.}
}
@article{YU2023114721,
title = {Numerical investigation of splitter blades on the performance of a forward-curved impeller used in a pump as turbine},
journal = {Ocean Engineering},
volume = {281},
pages = {114721},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.114721},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823011058},
author = {He Yu and Tao Wang and Yuancheng Dong and Qiuqin Gou and Lei Lei and Yunqi Liu},
keywords = {Special impeller, Pump as turbine, Splitter blade, Entropy generation, Computational fluid dynamics},
abstract = {Abstract
As a type of economical energy recovery device, pump as turbine (PAT) is generally used in micro-hydropower plants and energy recovery. To study the influence of the splitter blade on a special impeller used in PAT, impellers without and with splitter blades are designed in this paper. The influences of splitter blade on the energy loss, external characteristics and internal flow field distribution of a PAT were simulated via a verified computational fluid dynamics (CFD) method. The consequences present that the shaft power, efficiency, and head corresponding to the BEP of the PAT with splitter blades are 16.4%, 1.3%, and 8.8% better than those of the PAT without splitter blades. The total entropy production of the PAT without splitter blade is higher than that of the PAT with splitter blades at the same flow rate. Adding splitter blade increased the number of effective blades, made the fluid flow more evenly along the impeller flow passage, and reduced the flow separation inside the impeller. This paper displays that adding splitter blades not only obviously increases hydraulic performance under large flow conditions but also significantly widens the high-efficiency range of PATs.}
}
@article{PITTAPANTAZI2007301,
title = {Secondary school students’ levels of understanding in computing exponents},
journal = {The Journal of Mathematical Behavior},
volume = {26},
number = {4},
pages = {301-311},
year = {2007},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2007.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0732312307000508},
author = {Demetra Pitta-Pantazi and Constantinos Christou and Theodossios Zachariades},
keywords = {Exponents, Prototype, Conceptual change},
abstract = {The aim of this study is to describe and analyze students’ levels of understanding of exponents within the context of procedural and conceptual learning via the conceptual change and prototypes’ theory. The study was conducted with 202 secondary school students with the use of a questionnaire and semi-structured interviews. The results suggest that three levels of understanding can be identified. At the first level students’ interpretation of exponents is based upon exponents that symbolize natural numbers. At Level 2, students’ knowledge acquisition process is a process of enrichment of the existing conceptual structures. Students at this level are able to compute exponents with negative numbers by extending the application of prototype examples. Finally, at Level 3 students not only extend the prototype examples but also reorganize their thinking in order to compute and compare exponents with roots, a concept which is quite different from the concept of exponents with natural numbers.}
}
@article{GHAVAM2021128776,
title = {The life cycle environmental impacts of a novel sustainable ammonia production process from food waste and brown water},
journal = {Journal of Cleaner Production},
volume = {320},
pages = {128776},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.128776},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621029747},
author = {Seyedehhoma Ghavam and Caroline M. Taylor and Peter Styring},
keywords = {Green ammonia, Waste utilization, Carbon capture and sequestration, Carbon capture and utilization, Greenhouse gas emissions, Life cycle assessment},
abstract = {To replace existing high impact ammonia production technologies, a new sustainability-driven waste-based technology producing green ammonia with and without urea was devised using life cycle thinking and sustainable design principles, targeting efficiency, carbon emissions, water, and power use competitiveness. We have used life cycle assessment to determine whether cradle-to-gate, multiple configurations of the core waste-based processes integrating several carbon capture/utilization options can compete environmentally with other available ammonia technologies. Our waste-to-ammonia processes reduce potential impacts from abiotic depletion, human toxicity, and greenhouse gas (GHG) emissions relative to fossil-based and renewable technologies. Among the assessed technologies, coupling dark fermentation with anaerobic digestion and capturing CO2 for sequestration or later use is most efficient for GHGs, water, and energy, consuming 27% less energy and reducing GHGs by 98% compared to conventional ammonia. Water use is 38% lower than water electrolysis and GHGs are 94% below municipal waste incineration routes per kg NH3. Additionally, displacing conventional, high impact urea by integrating urea production from process CO2 decreases life cycle environmental impacts significantly despite increased energy demand. On a fertilizer-N basis, the ammonia + urea configuration without dark fermentation performs best on all categories included. Methane and ammonia leakage cause nearly all life cycle impacts, indicating that failing to prevent leakage undermines the effectiveness of new technologies such as these. Our results show that a green ammonia/ammonia + urea process family as designed here can reduce waste and prevent the release of additional CO2 from ammonia production while avoiding fossil-based alternatives and decreasing emissions from biogenic waste sources.}
}
@article{MURRAY2019928,
title = {Center Finding in E. coli and the Role of Mathematical Modeling: Past, Present and Future},
journal = {Journal of Molecular Biology},
volume = {431},
number = {5},
pages = {928-938},
year = {2019},
issn = {0022-2836},
doi = {https://doi.org/10.1016/j.jmb.2019.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S0022283619300269},
author = {Seán M. Murray and Martin Howard},
keywords = {bacterial cell division positioning, plasmid segregation, MinCDE system, ParABS system, mathematical modeling},
abstract = {We review the key role played by mathematical modeling in elucidating two center-finding patterning systems in Escherichia coli: midcell division positioning by the MinCDE system and DNA partitioning by the ParABS system. We focus particularly on how, despite much experimental effort, these systems were simply too complex to unravel by experiments alone, and instead required key injections of quantitative, mathematical thinking. We conclude the review by analyzing the frequency of modeling approaches in microbiology over time. We find that while such methods are increasing in popularity, they are still probably heavily under-utilized for optimal progress on complex biological questions.}
}
@article{LIEVENS2021128,
title = {A service design perspective on the stakeholder engagement journey during B2B innovation: Challenges and future research agenda},
journal = {Industrial Marketing Management},
volume = {95},
pages = {128-141},
year = {2021},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2021.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0019850121000791},
author = {Annouk Lievens and Vera Blažević},
keywords = {Stakeholder engagement, B2B innovation process, Stakeholder engagement journey, Innovation networks, Service design},
abstract = {Innovation in business-to-business (B2B) contexts deals with highly dynamic, complex, and heterogeneous constellations of stakeholders with a diversity of goals, motives, and capabilities that further challenge successful management of B2B innovation processes and outcomes. Complex challenges, such as sustainability and digitization trends, push these B2B firms to embrace new innovation methods that help them manage disruptive change. Service design thinking has emerged as an innovation management practice emphasizing a human-centered innovation process of user interactions, creativity, and learning mindsets. In this article, we aim to evaluate the challenges and develop a research agenda on how service design can effectively enable stakeholders' engagement during the B2B innovation process. We argue that to advance service design opportunities for stakeholder engagement, we need to address the unique complexities and challenges of stakeholder engagement during innovation from a systemic and dynamic process perspective. From a systemic perspective, we zoom in on the building blocks of stakeholder engagement and address multi-level stakeholder engagement platforms (i.e., innovation networks). From a dynamic process perspective, we treat stakeholder engagement as an emerging process and zoom in on the temporal and relational connections and hybrid orchestration to allow for both structural and emerging stakeholder engagement during innovation. We develop a stakeholder engagement journey in which we integrate service and innovation stages and propose how service design activities can support and facilitate the aforementioned challenges and complexities. Finally, we identify concrete research questions and, accordingly, develop a research agenda for future research on stakeholder engagement in B2B innovation trajectories.}
}
@incollection{POULSEN201543,
title = {Chapter 3 - Better Concurrency and SIMD on HBM},
editor = {James Reinders and Jim Jeffers},
booktitle = {High Performance Parallelism Pearls},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {43-67},
year = {2015},
isbn = {978-0-12-802118-7},
doi = {https://doi.org/10.1016/B978-0-12-802118-7.00003-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128021187000030},
author = {Jacob Weismann Poulsen and Per Berg and Karthik Raman},
keywords = {HIROMB-BOOS-Model, Danish Meteorological Institute, Operational ocean models, Parallelization, Validation, Verification, OpenMP, MPI, Intel® VTune™ Amplifier, Performance Monitoring Unit, Vectorization, Nesting, Scaling, Cache layout},
abstract = {This chapter describes some work that is being performed at the Danish Meteorological Institute for optimization of a 3D ocean circulation model code with roots back to the 1990s and which is known as the HIROMB-BOOS-Model. The optimization of this large code is instructive agreeing with the authors’ strong belief that the best performance only comes with a focus on architecting for it starting with appropriate data structures. The thinking process and techniques used in this chapter have wide applicability: focus on data locality and then apply threading and vectorization techniques. This way of thinking should be on the mind of every programmer working to design a high-performance application.}
}
@article{CRILLY2021333,
title = {The Evolution of “Co-evolution” (Part II): The Biological Analogy, Different Kinds of Co-evolution, and Proposals for Conceptual Expansion},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {7},
number = {3},
pages = {333-355},
year = {2021},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2021.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S2405872621000927},
author = {Nathan Crilly},
keywords = {Design process, Design thinking, Creativity, Biological analogy, Interdisciplinarity},
abstract = {Descriptions of problem-solution “co-evolution” either explicitly or implicitly draw an analogy between processes of design and processes of biological evolution. Analogies of this kind are common in research because of their potential to assist in explanation and discovery. However, reviewing the design literature reveals that the discussion of design co-evolution has become disconnected from the biological analogy on which it is founded, and from which other disciplines draw. Here, I explore the function of the co-evolution analogy, provide an illustrative example from biology, and explore the varieties of co-evolution to which design might be compared. By doing so, I propose two possible directions for expanding the design co-evolution concept: (i) examining what co-evolves in addition to, or instead of, problems and solutions, and (ii) examining the different levels at which co-evolution occurs. Both of these proposals are illustrated with a variant of the design co-evolution diagram.}
}
@article{RANGANATHAN201958,
title = {Emotion Mining in Social Media Data},
journal = {Procedia Computer Science},
volume = {159},
pages = {58-66},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.160},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919313389},
author = {Jaishree Ranganathan and Angelina Tzacheva},
keywords = {Data Mining, Emotion, Microblog, Sentiment Analysis, Twitter},
abstract = {Emotions are known to influence the perception of human beings along with their memory, thinking and imagination. Human perception is important in today’s world in a wide range of factors including but not limited to business, education, art, and music. Microblogging and Social networking sites like Twitter, Facebook are challenging sources of information that allow people to share their feelings and thoughts on a daily basis. In this paper we propose an approach to automatically detect emotions on Twitter messages that explores characteristics of the tweets and the writer’s emotion using Support Vector Machine LibLinear model and achieve 98% accuracy. Emotion mining gained attraction in the field of computer science due to the vast variety of systems that can be developed and promising applications like remote health care system, customer care services, smart phones that react based on users’s emotion, vehicles that sense emotion of the driver. These emotions help understand the current state of user. In order to perform suitable actions or provide suggestions on how user’s can enhance their feeling for a better healthy life-style we use actionable recommendations. In this work we extract action rules with respect to the user emotions that help provide suggestions for user’s.}
}
@article{CAGNAC2023,
title = {Codes and methods improvements for safety assessment and LTO: varied approaches},
journal = {EPJ - Nuclear Sciences & Technologies},
volume = {9},
year = {2023},
issn = {2491-9292},
doi = {https://doi.org/10.1051/epjn/2023001},
url = {https://www.sciencedirect.com/science/article/pii/S2491929223000109},
author = {Albannie Cagnac and Denis Verrier and Vladislav Pištora},
abstract = {Nuclear safety has always been at the heart of the concerns of nuclear power plant operators and developers, as well as of various nuclear research organizations and regulatory authorities. Over the last decades, all these nuclear actors have developed and integrated a large number of calculation codes and other tools into their safety work. From the system approach to the local understanding of a phenomenon on a given component, from neutronics to operation optimization for long-term operation, these methods and codes have been constantly evolving since their appearance, in order to be able to integrate new plant designs and components, to improve the results of modeling physical phenomena or quantify and thus reduce the uncertainties on these results. Currently, several H2020 Euratom projects are working on the improvement of these codes and methods. This article will focus on three of these projects: CAMIVVER (Codes And Methods Improvements for VVER comprehensive safety assessment), APAL (Advanced PTS Analysis for LTO), and sCO2-4-NPP (innovative SCO2-based heat removal technology for an increased level of safety of Nuclear Power Plants) in order to illustrate our thinking on the improvement of calculation frameworks. First, we will present the work and the approach adopted with regard to the different calculation codes and methods used in each of these three projects. We will then conclude with an overall analysis of these three approaches, highlighting the difficulties and successes of these three projects, and identifying areas of work for the general improvement of the calculation codes.}
}
@article{SCHULTZ2010174,
title = {Models and methods in motion: Declining the dogma dance},
journal = {Futures},
volume = {42},
number = {2},
pages = {174-176},
year = {2010},
note = {Epistemological pluralism in futures studies},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2009.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0016328709001736},
author = {Wendy Schultz},
abstract = {I take a communicative pragmatist and realist approach to futures studies. This implies a sensitivity to understanding what the audience can absorb and using futures methods effectively to create spaces for new futures. While Wilber's work affords us with new insights to engage with methodology, is not the only path. Indeed, it is intellectual bigotry to demand that everyone master the tools one personally deems most appropriate. Critical conversations about futures must remain open, where post-modernist and integral thinking widen our horizons, they are welcomed, where they straitjacket our thoughts, they are not.}
}
@article{WANG2020256,
title = {Fine-grained neural decoding with distributed word representations},
journal = {Information Sciences},
volume = {507},
pages = {256-272},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.08.043},
url = {https://www.sciencedirect.com/science/article/pii/S0020025519307820},
author = {Shaonan Wang and Jiajun Zhang and Haiyan Wang and Nan Lin and Chengqing Zong},
keywords = {Neural decoding, fMRI word decoding, Word class, Stimuli paradigm, Word embedding models, Informative voxels},
abstract = {fMRI word decoding refers to decode what the human brain is thinking by interpreting functional Magnetic Resonance Imaging (fMRI) scans from people watching or listening to words, representing a sort of mind-reading technology. Existing works decoding words from imaging data have been largely limited to concrete nouns from a relatively small number of semantic categories. Moreover, such studies use different word-stimulus presentation paradigms and different computational models, lacking a comprehensive understanding of the influence of different factors on fMRI word decoding. In this paper, we present a large-scale evaluation of eight word embedding models and their combinations for decoding fine-grained fMRI data associated with three classes of words recorded from three stimulus-presentation paradigms. Specifically, we investigate the following research questions: (1) How does the brain-image decoder perform on different classes of words? (2) How does the brain-image decoder perform in different stimulus-presentation paradigms? (3) How well does each word embedding model allow us to decode neural activation patterns in the human brain? Furthermore, we analyze the most informative voxels associated with different classes of words, stimulus-presentation paradigms and word embedding models to explore their neural basis. The results have shown the following: (1) Different word classes can be decoded most effectively with different word embedding models. Concrete nouns and verbs are more easily distinguished than abstract nouns and verbs. (2) Among the three stimulus-presentation paradigms (picture, sentence and word clouds), the picture paradigm achieves the highest decoding accuracy, followed by the sentence paradigm. (3) Among the eight word embedding models, the model that encodes visual information obtains the best performance, followed by models that encode textual and contextual information. (4) Compared to concrete nouns, which activate mostly vision-related brain regions, abstract nouns activate broader brain regions such as the visual, language and default-mode networks. Moreover, both the picture paradigm and the model that encodes visual information have stronger associations with vision-related brain regions than other paradigms and word embedding models, respectively.}
}
@article{NEMETH2024101385,
title = {The interplay between subcortical and prefrontal brain structures in shaping ideological belief formation and updating},
journal = {Current Opinion in Behavioral Sciences},
volume = {57},
pages = {101385},
year = {2024},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2024.101385},
url = {https://www.sciencedirect.com/science/article/pii/S2352154624000366},
author = {Dezső Németh and Teodóra Vékony and Gábor Orosz and Zoltán Sarnyai and Leor Zmigrod},
abstract = {History illustrates that economic crises and other sociopolitical threats often lead to a rise of polarization and radicalism, whereby people become more susceptible to intolerant political messages, including propaganda and ideological rhetoric. Political science, sociology, economics, and psychology have explored many dimensions of this phenomenon, yet a critical piece of the puzzle is still missing: what cognitive and neural mechanisms in the brain mediate between these threats and responsiveness to political messages? To answer this question, here, we present a theory that combines cognitive neuroscience theories, namely stress-induced memory shift and competitive cognitive processes, with political science. Our Threat-based Neural Switch Theory posits that the processing of political information, similarly to other information processing, is shaped by the competitive interaction between goal-directed and habitual processes. Threats, including resource overload or scarcity, can shift neural networks toward receptiveness to oversimplified political messages. This theory sets out a research program aimed at discovering the cognitive and neural underpinning of how situational factors alter brain functions and modify political information processing.}
}
@article{BERX2022107827,
title = {Identification and classification of risk factors for human-robot collaboration from a system-wide perspective},
journal = {Computers & Industrial Engineering},
volume = {163},
pages = {107827},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107827},
url = {https://www.sciencedirect.com/science/article/pii/S0360835221007312},
author = {Nicole Berx and Wilm Decré and Ido Morag and Peter Chemweno and Liliane Pintelon},
keywords = {Human-robot collaboration, Human factors, Industry 4.0, Safety, Risk factors, Socio-technical},
abstract = {Industry 4.0 systems in general and advanced manufacturing systems such as collaborative robots, in particular, are characterized by a high level of complexity leading to new safety concerns. Safety, specifically for collaborative robots, has been mainly addressed from a technical perspective, to safeguard the physical safety of the operator. Concerns have been raised regarding less focus in Industry 4.0 literature on how other factors, such as psychosocial can produce safety-related risks for the operator in human-robot collaboration. This paper identifies and classifies the risk factors in a human-robot collaboration that have been described in research papers in the last decade. The resulting five classes constitute dimensions that will be used as preliminary building blocks for a safety evaluation framework to be developed in the next step. By evaluating the resulting classes with the underlying dimensions of contemporary socio-technical thinking, this paper demonstrates that these five classes offer a comprehensive, system-wide perspective including risk factors beyond technological considerations. Topics emerging from new risks related to the impact of working with collaborative robots, such as psychosocial, ethical, and cyber risk factors will need to be taken into account in the risk factors that are important to identify, assess and mitigate before working with collaborative robots. Operator involvement and participation, especially throughout the risk assessment and mitigation cycle are recommended as new areas of attention in human-robot collaboration. Going forward, one challenge will be the agility and adaptability of legislation to at least keep track of risk factors emerging from continuously changing technologies and to translate them into practically applicable tools for enterprises and design engineers implementing collaborative applications. Another key challenge will be the measurement of the new emerging and sometimes less technological risks.}
}
@article{COWLEY2019104025,
title = {Wide coding: Tetris, Morse and, perhaps, language},
journal = {Biosystems},
volume = {185},
pages = {104025},
year = {2019},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2019.104025},
url = {https://www.sciencedirect.com/science/article/pii/S0303264719301820},
author = {S J Cowley},
keywords = {Organic codes, Distributed language, Adaptors, Wide cognition, Reading, Languaging},
abstract = {Code biology uses protein synthesis to pursue how living systems fabricate themselves. Weight falls on intermediary systems or adaptors that enable translated DNA to function within a cellular apparatus. Specifically, code intermediaries bridge between independent worlds (e.g. those of RNAs and proteins) to grant functional lee-way to the resulting products. Using this Organic Code (OC) model, the paper draws parallels with how people use artificial codes. As illustrated by Tetris and Morse, human players/signallers manage code functionality by using bodies as (or like) adaptors. They act as coding intermediaries who use lee-way alongside “a small set of arbitrary rules selected from a potentially unlimited number in order to ensure a specific correspondence between two independent worlds” (Barbieri, 2015). As with deep learning, networked bodily systems mesh inputs from a coded past with current inputs. Received models reduce ‘use’ of codes to a run-time or program like process. They overlook how molecular memory is extended by living apparatuses that link codes with functioning adaptors. In applying the OC model to humans, the paper connects Turing’s (1937) view of thinking to Wilson’s (2004) appeal to wide cognition. The approach opens up a new view of Kirsh and Maglio’s (1994) seminal studies on Tetris. As players use an interface that actualizes a code or program, their goal-directed (i.e. ‘pragmatic’) actions co-occur with adaptor-like ‘filling in’ (i.e. ‘epistemic’ moves). In terms of the OC model, flexible functions derive from, not actions, but epistemic dynamics that arise in the human-interface-computer system. Second, I pursue how a Morse radio operator uses dibs and dabs that enable the workings of an artificial code. While using knowledge (‘the rules’) to resemiotize by tapping on a transmission key, bodily dynamics are controlled by adaptor-like resources. Finally, turning to language, I sketch how the model applies to writing and reading. Like Morse operators, writers resemiotize a code-like domain of alphabets, spelling-systems etc. by acting as (or like) bodily adaptors. Further, in attending to a text-interface (symbolizations), a reader relies on filling-in that is (or feels) epistemic. Given that humans enact or mimic adaptor functions, it is likely that the OC model also applies to multi-modal language.}
}
@article{HAREL201758,
title = {Field-based hypotheses on advancing standards for mathematical practice},
journal = {The Journal of Mathematical Behavior},
volume = {46},
pages = {58-68},
year = {2017},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2017.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0732312317300457},
author = {Guershon Harel},
keywords = {Common Core State Standards in Mathematics (CCSSM), Standards for mathematical practice},
abstract = {The Common Core State Standards in Mathematics (CCSSM, 2010) are organized around two types of standards: the standards for mathematical content and standards for mathematical practice. The central goal of this paper is to present cognitive and instructional analyses of standards for mathematical practice through a discussion of field-based activities with inservice secondary mathematics teachers and students. A potential value of the study is that it provides researchers with specific field-based hypotheses on advancing standards for mathematical practice.}
}
@article{WANG2021102528,
title = {Automatic diagnosis of ECG disease based on intelligent simulation modeling},
journal = {Biomedical Signal Processing and Control},
volume = {67},
pages = {102528},
year = {2021},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2021.102528},
url = {https://www.sciencedirect.com/science/article/pii/S1746809421001257},
author = {Xu Wang and Runchuan Li and Shuhong Wang and Shengya Shen and Wenzhi Zhang and Bing Zhou and Zongmin Wang},
keywords = {Intelligent simulation modeling, Rule, ECG diseases, Diagnosis},
abstract = {In order to better assist doctors in diagnosing cardiovascular diseases, a set of end-to-end automatic diagnosis algorithms for ECG diseases based on intelligent simulation modeling are proposed. Firstly, wavelet transform and threshold method are used to denoise the ECG signal and locate the waveform in this paper. Secondly, waveform features are extracted. Finally, the rule method is used to convert the doctors’ thinking of diagnosing the disease into a description of the ECG characteristics of the disease to diagnose the ECG disease, and the algorithm is verified on the public database CCDD and the private data all-in-one machine data. The results show that this method is not inferior to the deep learning method. Now 11 types of diseases and 10 types of rhythm can be diagnosed.}
}
@article{MCCOWN201233,
title = {Farmers use intuition to reinvent analytic decision support for managing seasonal climatic variability},
journal = {Agricultural Systems},
volume = {106},
number = {1},
pages = {33-45},
year = {2012},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2011.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X11001557},
author = {R.L. McCown and P.S. Carberry and N.P. Dalgliesh and M.A. Foale and Z. Hochman},
keywords = {Decision support, Simulation, Information system, Cognitive system, Intuition, Climatic risk},
abstract = {The FARMSCAPE Information System emerged in a long-running research program aimed at making simulation models useful to Australian farmers in managing climatic variability. This paper is about how well it has worked. This is reported in relation to two standards: (1) the value to thinking and action expressed by farmers and their consultants, (2) correspondence with theory about learning and judgement in uncertain external environments. The former utilises recorded narrative interviews with participants over many years. The latter uses a cognitive framework drawn from theory of judgment and decision making featuring the relationship between intuition and analysis (McCown, 2011). The cognitive theory framework makes sense of several evaluation surprises. The first was high enthusiasm by largely-intuitive farmers for an analytic approach to soil water in conjunction with a newly-appreciated “bucket” metaphor for water balance. The second surprise was the virtual absence of soil water measurement 10years later. This had been replaced by various intuitive estimates, calibrated to maintain a heuristic relationship with regard to the “bucket” as a resource. Farmers and their advisers were facilitated in using simulation for thought experiments and planning under climatic uncertainty. Benchmarking enabled problem solving in documented conditions. Scenario analysis using historical climate records supported thought experiments by providing probability distributions that were valued for shaping expectations as a “history of the future”. In retrospective evaluation interviews, researchers were surprised to find that yield forecasting and tactical decision making, anticipated to be analyses that were both site- and season-specific forecasts, had served farmers as “management gaming” simulations to aid formulating action rules for such conditions, thus reducing the need for an on-going decision-aiding service. Equipped with their soil monitoring techniques and with their heuristic rules, farmers still reserved a place for simulation “when you’ve got a planting situation out of the ordinary.”}
}
@article{XI2025106930,
title = {Depression detection based on the temporal-spatial-frequency feature fusion of EEG},
journal = {Biomedical Signal Processing and Control},
volume = {100},
pages = {106930},
year = {2025},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2024.106930},
url = {https://www.sciencedirect.com/science/article/pii/S1746809424009881},
author = {Yang Xi and Ying Chen and Tianyu Meng and Zhu Lan and Lu Zhang},
keywords = {Depression detection, EEG, Temporal-spatial-frequency feature, Channel selection, Attention mechanism},
abstract = {Depression is a prevalent affective psychiatric disorder projected to be the leading contributor to the world’s disease burden by 2030. Due to its high prevalence and low recognition rate, an objective and effective detection method is urgently needed. Deep learning methods based on electroencephalography (EEG) have shown significant potential in depression detection. However, excessive channels can increase redundancy and computational complexity in EEG, while irrelevant channels may reduce accuracy. Additionally, existing models often overlook the complementarity between the temporal-, spatial-, and frequency-domain features of EEG, limiting their detection capabilities. To address these issues, we propose a method that fuses the temporal, spatial, and frequency domain features of EEG to enhance the detection accuracy while eliminating redundant channels. We introduce an EEG channel selection method based on frequency domain weighting that automatically adjusts the channel weights to select the EEG channels that best capture spatial information across the delta, theta, alpha, beta, and gamma bands, thereby optimizing the extraction of spatial-frequency features. In addition, we designed a multiscale spatiotemporal convolutional attention network to extract the spatiotemporal features of EEG. In this network, the multiscale convolutional attention module enhanced the model’s ability to capture spatial features, whereas the temporal trend-aware self-attention module extracted long-term temporal features by analyzing global correlations across different time points. Experimental results on the MODMA dataset show that our method achieved a 97.24% detection accuracy, surpassing current state-of-the-art models. This study offers a novel approach for constructing depression detection models, providing a foundation for future research and application.}
}
@article{KONOVALOV20213323,
title = {Dissecting functional contributions of the social brain to strategic behavior},
journal = {Neuron},
volume = {109},
number = {20},
pages = {3323-3337.e5},
year = {2021},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2021.07.025},
url = {https://www.sciencedirect.com/science/article/pii/S0896627321005699},
author = {Arkady Konovalov and Christopher Hill and Jean Daunizeau and Christian C. Ruff},
keywords = {fMRI, TPJ, dmPFC, social, decision making, strategic},
abstract = {Summary
Social interactions routinely lead to neural activity in a “social brain network” comprising, among other regions, the temporoparietal junction (TPJ) and the dorsomedial prefrontal cortex (dmPFC). But what is the function of these areas? Are they specialized for behavior in social contexts or do they implement computations required for dealing with any reactive process, even non-living entities? Here, we use fMRI and a game paradigm separating the need for these two aspects of cognition. We find that most social-brain areas respond to both social and non-social reactivity rather than just to human opponents. However, the TPJ shows a dissociation from the dmPFC: its activity and connectivity primarily reflect context-dependent outcome processing and reactivity detection, while dmPFC engagement is linked to implementation of a behavioral strategy. Our results characterize an overarching computational property of the social brain but also suggest specialized roles for subregions of this network.}
}
@article{KWON2019109608,
title = {Towards codification of thunderstorm/downburst using gust front factor: Model-based and data-driven perspectives},
journal = {Engineering Structures},
volume = {199},
pages = {109608},
year = {2019},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2019.109608},
url = {https://www.sciencedirect.com/science/article/pii/S0141029619306315},
author = {Dae Kun Kwon and Ahsan Kareem},
keywords = {Wind loads, Nonstationary process, Gust front, Gust front factor, Downburst, Thunderstorm, Codes and standards},
abstract = {Winds associated with gust fronts originating from a thunderstorm/downburst exhibit rapid changes during a short time period which may be accompanied by changes in direction. For several decades, a number of studies have been focused on identifying the characteristics of such nonstationary gust front winds in a variety of manners such as experimental/numerical methods and full-scale measurements. Yet, beginning the dialogue on any guidelines for design practice has thus far not evolved, in part due to a limited consensus on such characteristics among studies in conjunction with paucity of available data needed for vetting and corroborating, which is further impacted by the presence of nonstationarity. In an effort to establishing a new design procedure for this type of wind load effect on structures, the gust front factor (GFF) framework has been proposed by authors that encapsulates both the kinematic and dynamic features of gust front induced wind effects on structures, which distinguish themselves from those experienced in conventional boundary layer flows. This study revisits the gust front factor framework seeking to take the next step toward a possible initial framework for codification of gust front winds from model-based and data-driven perspectives. A modular and extensible web-enabled framework to estimate gust front related wind load effects is envisaged to rationally and holistically quantify design loads. This would promote design practice to enhance disaster resilience of the built environment. In this context, a closed-form expression concerning nonstationary fluctuations for a case of a long pulse duration is derived to facilitate rapid evaluation of nonstationary turbulence effects. A preliminary uncertainty analysis is also carried out to assess the influence of uncertainties associated with the load effects of gust front winds and the reliability of GFF. In addition, a comparison of the model-based gust front factor with a recently introduced thunderstorm response spectrum technique to assess their relative performance is carried out. In view of the lessons learned from the history of the gust loading factor on codes and standards, a possible living codification concept through a learning and updating invoking the emerging “Design Thinking” approach is discussed.}
}
@incollection{MACHINMASTROMATTEO2024,
title = {Literacy of the Future},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95689-5.00197-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956895001978},
author = {Juan D. Machin-Mastromatteo},
keywords = {Adaptation, Collaboration, Critical engagement, Democratic engagement, Digital literacy, Educational integration, Ethical dimensions, Futures Literacy, Information literacy, Lifelong learning, Media literacy, Multiliteracies, Programming skills, Social participation, Technological advancements},
abstract = {This entry summarizes the development of the literacy concepts most commonly associated with LIS, namely information literacy, digital literacy, and media literacy, which frame a synthesis of the future perspectives of these and other literacies that have been proposed in the literature.11An alphabetical and non-exhaustive list could include: academic literacy, artificial intelligence or algorithmic literacy, civic literacy, context literacy, data literacy, emotional literacy, financial literacy, focus literacy, futures literacies, game literacy, graphic literacy, health literacy, literacy education, legal literacy, media literacy, multiliteracies, new literacies, new media literacies, navigation literacy, numerical literacy, participatory/participation literacy, personal literacy, psycho-literacy, scientific literacy, search engine literacy, skepticism literacy, statistical literacy, transliteracy, and visual literacy or visuacy. Note: not all of these are covered in this entry for space limitations. These future perspectives are organized in nine sections: the educational implications of literacy, information literacy, digital literacy, literacy education, multiliteracies and holistic perspectives, media literacy, futures literacy, algorithmic literacy and artificial intelligence implications, and other literacies. The purpose of this entry is to offer a brief overview and commentary on the types of literacies that we need to be aware of and competent in for the near future. As these future trends are derived from the specialized literature, they include some already occurring considerations. However, they might become more salient topics in the upcoming years, and they might entail many different implications for the future of LIS professionals, libraries, and even for education in general.}
}
@article{GREENLEE20201043,
title = {Kinetic and Thermodynamic Control in Dynamic Covalent Synthesis},
journal = {Trends in Chemistry},
volume = {2},
number = {12},
pages = {1043-1051},
year = {2020},
issn = {2589-5974},
doi = {https://doi.org/10.1016/j.trechm.2020.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S258959742030232X},
author = {Andrew J. Greenlee and Chloe I. Wendell and Morgan M. Cencer and Summer D. Laffoon and Jeffrey S. Moore},
keywords = {dynamic, covalent, reversible, kinetic, thermodynamic},
abstract = {In recent years, dynamic covalent chemistry (DCC) has seen the synthesis of increasingly complex cyclooligomers, polymers, and diverse compound libraries. The reversible formation of covalent bonds characteristic of DCC reactions favors thermodynamic product distributions for simple unitopic reactions; however, kinetic effects are increasingly influential in reactions of multitopic precursors. In this review, we explore the interplay between thermodynamic and kinetic considerations when planning a DCC synthesis. Computational models, typically based on reaction thermodynamics, have aided in predicting DCC reaction outcomes with moderate success. A clear direction for the field is to develop more robust computational tools informed by thermodynamic and kinetic driving forces that can predict product distributions in DCC reactions.}
}
@article{VALLVERDU20146,
title = {What are Simulations? An Epistemological Approach},
journal = {Procedia Technology},
volume = {13},
pages = {6-15},
year = {2014},
note = {SLACTIONS 2013: Research conference on virtual worlds – Learning with simulations},
issn = {2212-0173},
doi = {https://doi.org/10.1016/j.protcy.2014.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S2212017314000139},
author = {Jordi Vallverdú},
keywords = {model, computer, simulation, epistemology, representation},
abstract = {Contemporary sciences use a wide and diverse range of computational simulations, including in the areas of aeronautics, chemistry, bioinformatics, social sciences, AI, the physics of elementary particles and most other scientific fields. A simulation is a mathematical model that describes or creates computationally a system process. Simulations are our best cognitive representation of complex reality, that is, our deepest conception of what reality is. In this paper we defend that a simulation is equivalent epistemologically and ontologically with all other types of cognitive models of elements of reality. Therefore, simulations cannot be considered secondary nor weak instruments to approach to the reality analysis.}
}
@article{SANCHIS2023102162,
title = {Towards a general equilibrium theory of allocation of time for the digital revolution era},
journal = {Technology in Society},
volume = {72},
pages = {102162},
year = {2023},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2022.102162},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X22003037},
author = {Raúl G. Sanchis},
keywords = {Household economics, Time allocation, Consumer behaviour, Firm behaviour, General equilibrium},
abstract = {The Digital Revolution we are witnessing has started a new era in modern societies and economies. Time inputs, whether these are from human beings and non-human, electronical or mechanical devices are increasingly more important, especially –but not uniquely– in most advanced economies and societies. Existing economic theory strives to accommodate time inputs into mainstream economic theory. This paper contributes to the existing literature on time allocation theoretical models by suggesting a general equilibrium framework likely to respond to some existing challenges in modern economies. In the general equilibrium modelling process, some improvements are made to time allocation models from the consumer side which concern the inclusion of non-human time inputs and multitasking, and a novel development on a producer theory of allocation of time is designed to determine the underpinnings of a computationally tractable general equilibrium theory of allocation of time. Both the solution and usefulness of this work will require the help of cutting-edge computational techniques in future work.}
}
@incollection{LEVY1989243,
title = {A Computational Approach to Hippocampal Function},
editor = {Robert D. Hawkins and Gordon H. Bower},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {23},
pages = {243-305},
year = {1989},
booktitle = {Computational Models of Learning in Simple Neural Systems},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(08)60113-9},
url = {https://www.sciencedirect.com/science/article/pii/S0079742108601139},
author = {William B Levy},
abstract = {Publisher Summary
This chapter describes the early, formative stages of a theory of hippocampal function. This theory has been stimulated by the psychological observations indicating a role for the hippocampus in short-term working memory and spatial behavior and develops mainly through the consideration of computational issues. These computational issues are related to the psychological viewpoint through physiological and anatomical observations. The hippocampus participates in the prediction of future representations based on past and present representations. All three classes of representations are derived from a multiplicity of sensory modalities, such as auditory, visual, and olfactory signals from neo- and piriform cortices. This fusion of sensory modalities requires recoding because of computational complexity problems. The CA1 region of the hippocampus is postulated to be a prediction-generating layer or tier. This region produces a prediction based on its input from hippocampal region CA3. The combined hippocampal dentate gyrus/CA3 (DG/CA3) system is postulated to be a preprocessor serving the CA1 prediction layer. The computational complexity problems arise from the combinatorial explosion of possible representations resulting when the hippocampus and supporting limbic structures mix representations from multiple sensory modalities.}
}
@article{WANG19951,
title = {Peripheral dynamics of the Cl + CH4 → HCl + CH3 reaction. A classical trajectory computation},
journal = {Chemical Physics},
volume = {197},
number = {1},
pages = {1-17},
year = {1995},
issn = {0301-0104},
doi = {https://doi.org/10.1016/0301-0104(95)00134-A},
url = {https://www.sciencedirect.com/science/article/pii/030101049500134A},
author = {Xuebin Wang and M. Ben-Nun and R.D. Levine},
abstract = {The Cl + CH4 → HCl + CH3 reaction is expected to provide a prototype of a peripheral mechanism. This proposal is examined via a classical trajectory computation using a number of model potentials in which the degrees of freedom which do not take part in the net reaction are, or are not, frozen. The models include a full six-atom potential. The essential features of the dynamics are not sensitive to the level of detail with which the CH3 is described, showing that the intramolecular dynamics of the radical do not significantly affect the dynamics of the reactive event. The reaction is found to proceed by two distinct mechanisms: for trajectories with a large impact parameter, a very short lived complex is formed and dissociates to a rotationally cold HCl product, scattered into the forward direction. At smaller impact parameters, the reaction proceeds via a direct mechanism with a rotationally hot HCl which is scattered backward. The computed angular distribution is in agreement with the experiment, which detects HCl in the j = 1, 3 states and suggests that higher rotational states of HCl, which were not probed in the experiment, will also be scattered backward. The role of the initial vibrational excitation of CH4 is discussed.}
}
@incollection{YELLA2022770,
title = {2.32 - Magic bullets: Drug repositioning and drug combinations},
editor = {Terry Kenakin},
booktitle = {Comprehensive Pharmacology},
publisher = {Elsevier},
address = {Oxford},
pages = {770-788},
year = {2022},
isbn = {978-0-12-820876-2},
doi = {https://doi.org/10.1016/B978-0-12-820472-6.00116-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012820472600116X},
author = {Jaswanth K. Yella and Anil G. Jegga},
keywords = {Artificial intelligence, Computer-aided drug synthesis, COVID-19, De novo drug discovery, Drug combinations, Drug repurposing, Drug synergy, Machine learning, Network analysis},
abstract = {Discovery and development of novel pharmaceuticals continue to be a very costly, time-consuming and uncertain process impacting negatively not only the research and development of pharmaceutical industry but also health care. Although the number of novel drugs approved each year has grown over by as much as 60% compared to the past decade, there are still many diseases that do not have any approved drug. Recent technological advances in the biomedical, genomics, and computational science domains accompanied by multisource and multidimensional data opened new opportunities and challenges. The drug discovery paradigm is increasingly shifting from hypothesis-driven to data-driven approaches. While the search for the magic bullets of medicine continues, the magic—crunching the data deluge into knowledge and hypotheses nuggets—is mostly driven by machines and machine intelligence. This review will primarily focus on three facets of computational drug discovery approaches, namely, drug repositioning, de novo drug discovery, and drug combinations, and reflect on computational approaches which are reproducible and seem most promising for the machine learning-driven drug discovery. Finally, using COVID-19 as an example, we discuss how the computational approaches are aiding and accelerating the process of discovery of magic bullet(s) for this dreadful pandemic.}
}
@article{HO2024124656,
title = {Unraveling the complexity of amorphous solid as direct ingredient for conventional oral solid dosage form: The story of Elagolix Sodium},
journal = {International Journal of Pharmaceutics},
volume = {665},
pages = {124656},
year = {2024},
issn = {0378-5173},
doi = {https://doi.org/10.1016/j.ijpharm.2024.124656},
url = {https://www.sciencedirect.com/science/article/pii/S0378517324008901},
author = {Raimundo Ho and Richard S. Hong and Joseph Kalkowski and Kevin C. Spence and Albert W. Kruger and Jayanthy Jayanth and Nandkishor K. Nere and Samrat Mukherjee and Ahmad Y. Sheikh and Shailendra V. Bordawekar},
keywords = {Amorphous drug substance, Impinging jet precipitation, Scale-up, Glass transition, Microstructure, Physical property control, Multi-scale modeling},
abstract = {Conventional solid oral dosage form development is not typically challenged by reliance on an amorphous drug substance as a direct ingredient in the drug product, as this may result in product development hurdles arising from process design and scale-up, control of physical quality attributes, drug product processability and stability. Here, we present the Chemistry, Manufacturing and Controls development journey behind the successful commercialization of an amorphous drug substance, Elagolix Sodium, a first-in-class, orally active gonadotropin-releasing hormone antagonist. The reason behind the lack of crystalline state was assessed via Molecular Dynamics (MD) at the molecular and inter-molecular level, revealing barriers for nucleation due to prevalence of intra-molecular hydrogen bond, repulsive interactions between active pharmaceutical ingredient (API) molecules and strong solvation effects. To provide a foundational basis for the design of the API manufacturing process, we modeled the solvent-induced plasticization behavior experimentally and computationally via MD for insights into molecular mobility. In addition, we applied material science tetrahedron concepts to link API porosity to drug product tablet compressibility. Finally, we designed the API isolation process, incorporating computational fluid dynamics modeling in the design of an impinging jet mixer for precipitation and solvent-dependent glass transition relationships in the cake wash, blow-down and drying process, to enable the consistent manufacture of a porous, non-sintered amorphous API powder that is suitable for robust drug product manufacturing.}
}
@article{NOURANI2015891,
title = {Predictive Control, Competitive Model Business Planning, and Innovation ERP},
journal = {Procedia Computer Science},
volume = {65},
pages = {891-900},
year = {2015},
note = {International Conference on Communications, management, and Information technology (ICCMIT'2015)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.09.048},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915028781},
author = {Cyrus F. Nourani and Codrina Lauth},
keywords = {Competitive Models, Innovation Management, ERP. Multiplayer Games, Game Trees Computing, Predictive Modeling, Planning, Competitive Models, Dynamic Programming},
abstract = {New optimality principles are put forth based on competitive model business planning. A Generalized MinMax local optimum dynamic programming algorithm is presented and applied to business model computing where predictive techniques can determine local optima. Based on a systems model an enterprise is not viewed as the sum of its component elements, but the product of their interactions. The paper starts with introducing a systems approach to business modeling. A competitive business modeling technique, based on the author's planning techniques is applied. Systemic decisions are based on common organizational goals, and as such business planning and resource assignments should strive to satisfy higher organizational goals. It is critical to understand how different decisions affect and influence one another. Here, a business planning example is presented where systems thinking technique, using Causal Loops, are applied to complex management decisions. Predictive modeling specifics are briefed. A preliminary optimal game modeling technique is presented in brief with applications to innovation and R&D management. Conducting gap and risk analysis can assist with this process. Example application areas to e-commerce with management simulation models are examined.}
}
@article{ZHANG2017123,
title = {Collective decision optimization algorithm: A new heuristic optimization method},
journal = {Neurocomputing},
volume = {221},
pages = {123-137},
year = {2017},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2016.09.068},
url = {https://www.sciencedirect.com/science/article/pii/S0925231216311183},
author = {Qingyang Zhang and Ronggui Wang and Juan Yang and Kai Ding and Yongfu Li and Jiangen Hu},
keywords = {Collective decision optimization algorithm, Artificial neural networks, Meta-heuristic, Decision-making},
abstract = {Recently, inspired by nature, diversiform successful and effective optimization methods have been proposed for solving many complex and challenging applications in different domains. This paper proposes a new meta-heuristic technique, collective decision optimization algorithm (CDOA), for training artificial neural networks. It simulates the social behavior of human based on their decision-making characteristics including experience-based phase, others'-based phase, group thinking-based phase, leader-based phase and innovation-based phase. Different corresponding operators are designed in the methodology. Experimental results carried out on a comprehensive set of benchmark functions and two nonlinear function approximation examples demonstrate that CDOA is competitive with respect to other state-of-art optimization algorithms.}
}
@incollection{WILLIAMS2020341,
title = {Chapter 17 - Begin with the human: Designing for safety and trustworthiness in cyber-physical systems},
editor = {William F. Lawless and Ranjeev Mittu and Donald A. Sofge},
booktitle = {Human-Machine Shared Contexts},
publisher = {Academic Press},
pages = {341-357},
year = {2020},
isbn = {978-0-12-820543-3},
doi = {https://doi.org/10.1016/B978-0-12-820543-3.00017-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128205433000171},
author = {Elizabeth T. Williams and Ehsan Nabavi and Genevieve Bell and Caitlin M. Bentley and Katherine A. Daniell and Noel Derwort and Zac Hatfield-Dodds and Kobi Leins and Amy K. McLennan},
keywords = {Trust, Safety, Autonomy, Agency, Assurance, Metrics, Interfaces, Human-machine interaction, Cyber-physical systems},
abstract = {Control systems are designed and built to manage and regulate the behavior of other systems. The use of artificial intelligence (AI) in control systems has simultaneously created new opportunities and new challenges in how we create, manage, and govern cyber-physical systems. In this paper, we discuss the challenge of defining and developing a model for contemplating how these systems will potentially learn, evolve, and act without human intervention. We present an analytical framework for thinking about trust and safety in these systems—both key factors for shared context in human-machine teams—and demonstrate its application using an example from history.}
}
@incollection{WARE20081,
title = {Chapter 1 - Visual Queries},
editor = {Colin Ware},
booktitle = {Visual Thinking},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {1-22},
year = {2008},
isbn = {978-0-12-370896-0},
doi = {https://doi.org/10.1016/B978-0-12-370896-0.00001-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780123708960000019},
author = {Colin Ware},
abstract = {Publisher Summary
This book about graphic design provides a channel for clear communication that supports visual thinking and acts as an interface to the vast information resources of the modern world. Visual thinking is a process that has the allocation of attention as its very essence. Attention, however, is multifaceted. Making an eye movement is an act of attending. Eye movements are executed to satisfy the need for information and can be thought of as a sequence of visual queries on the visual world. The idea of the visual query is shorthand for what one does when obtaining information either from the world at large or from some kind of information display. Understanding what visual queries are easily executed is a critical skill for the designer. The special skill of designers is not so much skill with drawing or graphic design software, although these are undoubtedly useful, but the talent to analyze a design in terms of its ability to support the visual queries of others. One reason why design is difficult is that the designer already has the knowledge expressed in the design and has seen it develop from inception and therefore cannot see it with fresh eyes. The solution is to be analytic and this is where this book is intended to have value. Effective design should start with a visual task analysis, determine the set of visual queries to be supported by a design, and then use color, form, and space to efficiently serve those queries.}
}
@article{CHANDRA2018306,
title = {New narratives of development work? Making sense of social entrepreneurs’ development narratives across time and economies},
journal = {World Development},
volume = {107},
pages = {306-326},
year = {2018},
issn = {0305-750X},
doi = {https://doi.org/10.1016/j.worlddev.2018.02.033},
url = {https://www.sciencedirect.com/science/article/pii/S0305750X18300780},
author = {Yanto Chandra},
keywords = {Development narrative, Development, Social enterprise, Social entrepreneur, Computational linguistics},
abstract = {This article views social entrepreneurship as a relatively new model for achieving sustainable development. It also identifies development narratives that social entrepreneurs (SEs) construct to represent and promote their work as an important research gap in development studies. Drawing on the development and narratology literature, and employing computational linguistics (CL) techniques, this article compares the development narratives of 1076 Ashoka SEs across two periods (2009–2013 and 1994–1998) and two economies (developing and developed). CL analyses reveal important themes that characterize the identity, framing and orientations of development SEs across time and economies. The findings demonstrate how SE development narratives i) tend to be more pragmatic and solution-centric, and contain less political ideology than conventional development narratives, ii) combine extant development ideas and models but reframe them in new ways to address contemporary, complex development challenges, and iii) reflect a ‘bottom-up’ approach that encourages local ownership and collaborations with various social and economic sectors to achieve development goals. Overall, this study identifies the increasing importance of SEs in the development industry and reveals new aspects of SEs—their latent political framing, collective-utilitarian identities, and topical areas—that require further research via development narratives.}
}
@article{ALBALAWI201712033,
title = {Distributed Economic MPC with Safety-Based Constraints for Nonlinear Systems**Financial support from the National Science Foundation and the Department of Energy is gratefully acknowledged.},
journal = {IFAC-PapersOnLine},
volume = {50},
number = {1},
pages = {12033-12040},
year = {2017},
note = {20th IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2017.08.2098},
url = {https://www.sciencedirect.com/science/article/pii/S2405896317327581},
author = {Fahad Albalawi and Helen Durand and Panagiotis D. Christofides},
keywords = {Process safety, distributed model predictive control, computation time},
abstract = {Promoting process safety of chemical processes while operating them in an economically-optimal manner is a matter of great importance. In Albalawi et al. (2016), a safety-based economic model predictive control methodology (safety-EMPC) was developed to operate nonlinear processes in an economically-optimal manner while maintaining process safety and closed-loop stability. However, the safety-EMPC control strategy was developed with a centralized economic model predictive control (EMPC) structure; thus, computation time limitations within a sampling period may reduce the effectiveness of such a controller design for promoting process safety. Alternatively, we develop in this work sequential and iterative safety-based distributed EMPC schemes (safety-DEMPC) that may overcome the computation time limitations of the centralized safety-EMPC while maintaining similar closed-loop performance. Using a catalytic reactor example, the two proposed safety-DEMPC schemes were demonstrated to achieve similar closed-loop performance to the centralized safety-EMPC while reducing the on-line computation time requirements compared to the centralized safety-EMPC.}
}
@incollection{TANQUE202113,
title = {Chapter 2 - Knowledge Representation and Reasoning in AI-Based Solutions and IoT Applications},
editor = {Gurjit Kaur and Pradeep Tomar and Marcus Tanque},
booktitle = {Artificial Intelligence to Solve Pervasive Internet of Things Issues},
publisher = {Academic Press},
pages = {13-49},
year = {2021},
isbn = {978-0-12-818576-6},
doi = {https://doi.org/10.1016/B978-0-12-818576-6.00002-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128185766000022},
author = {Marcus Tanque},
keywords = {Artificial intelligence, machine learning, intelligent machine, artificial neural networks, cognitive science, deep learning, artificial general networks, knowledge representation, and reasoning, cognitive informatics, Internet of Things},
abstract = {Artificial intelligence (AI)-based solutions, knowledge representation and reasoning, and the Internet of Things applications have transformed how researchers and practitioners view the analytical and computational capabilities. The disruptive evolution of these technologies has encouraged researchers and practitioners to develop integrated AI-based analytical solutions needed for solving pervasive issues affecting computational applications. The capabilities include AI, knowledge Representation and Reasoning and Internet of Things. Such capabilities are designed to support AI-based solutions, knowledge representation and reasoning, and the Internet of Things (IoT) applications. These technology trends involve relevant computational areas, that is, intelligent devices, sensors, autonomous vehicles, robotics, virtual reality, augmented intelligence, and others. The study addresses and validates solutions on how researchers can solve issues that affect AI, knowledge representation and reasoning, and IoT applications.}
}
@article{ROSSITER20237555,
title = {A suite of MATLAB livescript files to support learning of elementary control and feedback concepts},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {7555-7560},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.657},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323010315},
author = {J.A. Rossiter},
keywords = {Virtual laboratories, independent learning, visualisation, livescripts},
abstract = {This paper builds on a body of work in the community which is focussed on sharing learning and teaching resources, especially those which might support a first course in control. Here attention is given to some of the mathematical, analytical and numerical computations which are required to support simple system and feedback analysis and design. The aim is to provide resources which allow students to focus on core concepts and understanding so that the numerical computations are not an obstacle to their investigations. More specifically, this paper focuses on a number of MATLAB livescript files which have been produced to help students visualise the impact of parameter and design choices on system behaviour, while simultaneously empowering them to understand the source code and thus upskill them for the future. The paper gives an overview of the livescripts available so users can decide whether these could be useful in their own context; all are freely available on the author's website (Rossiter, 2021).}
}
@incollection{BRAME201915,
title = {Chapter 2 - Course Design: Making Choices About Constructing Your Course},
editor = {Cynthia J. Brame},
booktitle = {Science Teaching Essentials},
publisher = {Academic Press},
pages = {15-28},
year = {2019},
isbn = {978-0-12-814702-3},
doi = {https://doi.org/10.1016/B978-0-12-814702-3.00002-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128147023000020},
author = {Cynthia J. Brame},
keywords = {Undergraduate, science, education, course design, learning goals, learning objectives, guiding questions, formative assessment},
abstract = {Designing or redesigning a course can be a creative and rewarding effort, but it is always a challenge. Science is characterized by continuous change and an ever-growing (and already large!) body of knowledge, and our courses often seek to help students understand the core knowledge, experimental tools, and ways of thinking in a field. It’s a big task. Further, a course may play a particular role in the curriculum, serving as a prerequisite, a capstone, or the course in which students learn a particular skill. How do you pick on what to focus, and how do you organize your course to help your students be able to transfer their knowledge to a new setting? How can you design the course to help your students build a conceptual framework that can expand and grow as their understanding grows? This chapter describes six principles to guide your course design and provides suggestions for more detailed resources.}
}
@article{ERA2021105070,
title = {Dissociating cognitive, behavioral and physiological stress-related responses through dorsolateral prefrontal cortex inhibition},
journal = {Psychoneuroendocrinology},
volume = {124},
pages = {105070},
year = {2021},
issn = {0306-4530},
doi = {https://doi.org/10.1016/j.psyneuen.2020.105070},
url = {https://www.sciencedirect.com/science/article/pii/S0306453020304935},
author = {Vanessa Era and Luca Carnevali and Julian F. Thayer and Matteo Candidi and Cristina Ottaviani},
keywords = {Dorsolateral prefrontal cortex, Perseverative cognition, Cortisol, Heart rate variability, High-frequency repetitive transcranial magnetic stimulation},
abstract = {The left dorsolateral prefrontal cortex (dlPFC) has been implicated in the regulation of stress-related cognitive processes and physiological responses and is the principal target of noninvasive brain stimulation techniques applied to psychiatric conditions. However, existing studies are mostly correlational and causal evidence on the role of this region in mediating specific psychophysiological mechanisms underpinning stress-related responses are needed to make the application of such techniques more efficient. To fill this gap, this study used inhibitory continuous theta burst stimulation (cTBS) in healthy individuals to examine the extent to which activity of the left dlPFC is associated with cognitive (subjective focus on a tracking task), behavioral (reaction times and variability), and physiological responses (heart rate and its variability and cortisol level) following induction of perseverative cognition. Compared to sham and left ventral PreMotor area stimulation (as active control area), inhibition of left dlPFC determined sustained autonomic and neuroendocrine activation and increased the subjective perception of being task-focused, while not changing the behavioral and self-reported stress-related responses. Adopting a causative approach, we describe a role of left dlPFC in inhibitory control of the physiological stress-response associated to perseverative thinking.}
}
@article{THIEDE201536,
title = {Can teachers accurately predict student performance?},
journal = {Teaching and Teacher Education},
volume = {49},
pages = {36-44},
year = {2015},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2015.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X1500013X},
author = {Keith W. Thiede and Jonathan L. Brendefur and Richard D. Osguthorpe and Michele B. Carney and Amanda Bremner and Sam Strother and Steven Oswalt and Jennifer L. Snow and John Sutton and Dan Jesse},
keywords = {Teacher judgment, Judgment accuracy, Mathematics achievement},
abstract = {In two studies, we examined the effect of professional development to improve mathematics instruction on the accuracy of teachers' monitoring of student learning. Study 1 was conducted with 36 teachers participating in three years of professional development. Judgment accuracy was influenced by the fidelity with which what was learned in the professional development. Study 2 was conducted with 64 teachers from 8 schools, which were randomly assigned to receive professional development or serve as a control. Judgment accuracy was greater for teachers receiving professional development than for teachers who did not and teachers were better to predict students' computational skills.}
}
@incollection{KLATT200719,
title = {Perspectives for process systems engineering – a personal view from academia and industry},
editor = {Valentin Pleşu and Paul Şerban Agachi},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {24},
pages = {19-32},
year = {2007},
booktitle = {17th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/S1570-7946(07)80027-7},
url = {https://www.sciencedirect.com/science/article/pii/S1570794607800277},
author = {Karsten-Ulrich Klatt and Wolfgang Marquardt},
keywords = {Review, critical assessment, emerging fields, modeling, design, optimization, control, operations, numerical algorithms, software.},
abstract = {Process systems engineering (PSE) has been an active research field for almost 50 years. Modeling, simulation and optimization technologies have been developed to a mature state. These technologies have been penetrating all fields of chemical engineering in academia as well as in industrial practice. Systems thinking has been established in industrial practice largely through powerful commercial process simulation software and through mandatory courses in most chemical engineering programs. This contribution reflects on the past, present and future of PSE. Special emphasis will be on the perspectives of this field from an academic and industrial point of view.}
}
@article{JAHEL2023122624,
title = {The future of social-ecological systems at the crossroads of quantitative and qualitative methods},
journal = {Technological Forecasting and Social Change},
volume = {193},
pages = {122624},
year = {2023},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2023.122624},
url = {https://www.sciencedirect.com/science/article/pii/S0040162523003098},
author = {Camille Jahel and Robin Bourgeois and Jérémy Bourgoin and William's Daré and Marie {De Lattre-Gasquet} and Etienne Delay and Patrice Dumas and Christophe {Le Page} and Marc Piraux and Rémi Prudhomme},
keywords = {Quantitative, Qualitative, Anticipation, Foresight, Power relationship, Discontinuities},
abstract = {Urgent calls to transform societies toward more sustainability make the practice of anticipation more and more necessary. The progressive development of computational technologies has opened room for a growing use of quantitative methods to explore the future of social-ecological systems, in addition to qualitative methods. This warrants investigating issues of power relationships and discontinuities and unknowns that arise when mingling quantitative and qualitative anticipatory methods. We first reflected on the semantics attached to these methods. We then conducted a comparative analysis on the way the articulation of quantitative and qualitative methods was conducted, based on an in-depth analysis of a set of eleven anticipatory projects completed by several external case studies. We propose insights to classify projects according to the timing (successive, iterative or convergent) and the purpose of the articulation (imagination, refinement, assessment and awareness raising). We use these insights to explore methodological implications and power relationships and then discuss the ways to inform or frame anticipatory projects that seek to combine these methods.}
}
@article{GROEGER1987295,
title = {Computation—The final metaphor? An interview with Philip Johnson-Laird},
journal = {New Ideas in Psychology},
volume = {5},
number = {2},
pages = {295-304},
year = {1987},
issn = {0732-118X},
doi = {https://doi.org/10.1016/0732-118X(87)90030-4},
url = {https://www.sciencedirect.com/science/article/pii/0732118X87900304},
author = {J.A. Groeger}
}
@incollection{GOMILA201219,
title = {3 - The Relevance of Language for Thought: A Continuum of Possibilities},
editor = {Antoni Gomila},
booktitle = {Verbal Minds},
publisher = {Elsevier},
address = {London},
pages = {19-33},
year = {2012},
isbn = {978-0-12-385200-7},
doi = {https://doi.org/10.1016/B978-0-12-385200-7.00003-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780123852007000035},
author = {Antoni Gomila},
keywords = {Cognitive restructuring, linguistic relativism, Whorf, Vygotsky, thinking for speaking, modular interphase, social scaffolding, categorical effect, perceptual similarity},
abstract = {Publisher Summary
This chapter directs the influence and relevance of language on thoughts. Though there has been no outlined domain on how exactly language effects cognitive architecture, the chapter critically studies five most relevant positions that have attracted defenders, critics since twentieth century to contemporary proposals. It discuses relativism, cognitive restructuring, thinking for speaking, language as modular interface, and language as social scaffolding. Linguistic relativism finds its roots in Romanticism as a reaction to the supremacist attitudes of the “Enlightment thinkers,” who were in the business of establishing hierarchies of languages. Cognitive system, being linguistic, acquires a supplementary system of cognitive representation and processing, which transforms the basic capabilities of system and gives rise to new possibilities. Since language is an interface between the modules it attempts to concede to some cognitive impact without challenging the general cognitive architecture of modules of thought as a successful representational vehicle. Lastly, human minds are socially and culturally constituted minds and therefore linguistic symbols (like other kinds of symbols and other social tools in general) allow the individual to externally discharge cognitive processes through language.}
}
@article{BENARIE1992291,
title = {Air pollution modeling: P. Zannetti, Computational Mechanics Publications, Southampton, U.K. 1990, 444 pp. Price: £59.00},
journal = {Science of The Total Environment},
volume = {119},
pages = {291},
year = {1992},
issn = {0048-9697},
doi = {https://doi.org/10.1016/0048-9697(92)90273-U},
url = {https://www.sciencedirect.com/science/article/pii/004896979290273U},
author = {Michel Benarie}
}
@article{KARVONEN2023101166,
title = {Fundamental concepts of cognitive mimetics},
journal = {Cognitive Systems Research},
volume = {82},
pages = {101166},
year = {2023},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.101166},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723001006},
author = {Antero Karvonen and Tuomo Kujala and Tommi Kärkkäinen and Pertti Saariluoma},
keywords = {AI design, Cognitive mimetics, Design, Artificial intelligence},
abstract = {The rapid development and widespread adoption of Artificial Intelligence (AI) technologies have made the development of AI-specific design methods an important topic to advance. In recent decades, the centre of gravity in AI has shifted away from cognitive science and related fields like psychology. However, there is a clear need and potential for added value in returning to stronger interaction. One potential challenge for this interaction may be the lack of common conceptual grounds and design languages. In this article, we aim to contribute to the development of conceptual interfaces for human-based AI-specific design methods through the idea of cognitive mimetics. We begin by introducing basic concepts from mimetic design and interpret them in the context of this thematic area. These provide some of the basic building blocks for a design language and bring to the surface key questions. These in turn provide a ground for explicating cognitive mimetics. In the second part of this paper, we focus on specifying a key aspect in cognitive mimetics: the contents of information processes. Others engaged in this field can derive value from using or developing the basic conceptual machinery to specify their own approaches in this interdisciplinary field that is still shaping itself. Furthermore, those who resonate with the idea of cognitive mimetics, as specified here, can join in taking this particular approach further.}
}
@article{B2021107538,
title = {A survey on genomic data by privacy-preserving techniques perspective},
journal = {Computational Biology and Chemistry},
volume = {93},
pages = {107538},
year = {2021},
issn = {1476-9271},
doi = {https://doi.org/10.1016/j.compbiolchem.2021.107538},
url = {https://www.sciencedirect.com/science/article/pii/S1476927121001055},
author = {Abinaya B. and Santhi S.},
keywords = {Data sharing, Data access and storage, Data computation, Outsourcing, Privacy-preserving techniques},
abstract = {Nowadays, the purpose of human genomics is widely emerging in health-related problems and also to achieve time and cost-efficient healthcare. Due to advancement in genomics and its research, development in privacy concerns is needed regarding querying, accessing and, storage and computation of the genomic data. While the genomic data is widely accessible, the privacy issues may emerge due to the untrusted third party (adversaries/researchers), they may reveal the information or strategy plans regarding the genome data of an individual when it is requested for research purposes. To mitigate this problem many privacy-preserving techniques are used along with cryptographic methods are briefly discussed. Furthermore, efficiency and accuracy in a secure and private genomic data computation are needed to be researched in future.}
}
@article{PHONAPICHAT20143169,
title = {An Analysis of Elementary School Students’ Difficulties in Mathematical Problem Solving},
journal = {Procedia - Social and Behavioral Sciences},
volume = {116},
pages = {3169-3174},
year = {2014},
note = {5th World Conference on Educational Sciences},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2014.01.728},
url = {https://www.sciencedirect.com/science/article/pii/S1877042814007459},
author = {Prathana Phonapichat and Suwimon Wongwanich and Siridej Sujiva},
keywords = {Mathematical problem solving, mathematical difficulties, mathematical skills, elementary school students},
abstract = {The main purpose of mathematics teaching is to enable students to solve problems in daily life. Unfortunately, according to the latest national test results, most students lack mathematical problem solving skills. This proves to be one of the reasons why overall achievement in mathematics is considered quite low. It also reflects that students have difficulties in comprehending mathematical problems affecting the process of problem-solving. Therefore, in order to allow teachers to establish a proper teaching plan suitable for students’ learning process, this research aims to analyze the difficulties in mathematical problem solving among elementary school students. Samples are divided into two groups, elementary school students and mathematics teachers. Data collection was conducted by structured interview, documentary analysis, and survey tests. Data analysis was conducted by descriptive statistics, and content analysis. The results suggest that there are several difficulties in problem solving, namely 1) Students have difficulties in understanding the keywords appearing in problems, thus cannot interpret them in mathematical sentences. 2) Students are unable to figure out what to assume and what information from the problem is necessary to solving it, 3) Whenever students do not understand the problem, they tend to guess the answer without any thinking process, 4) Students are impatient and do not like to read mathematical problems, and 5) Students do not like to read long problems. Therefore, the results found in this research will lead to the creation and the development of mathematical problem solving diagnostic tests for teachers, in order to improve students’ mathematical problem solving skills.}
}
@article{WEISSLER19991061,
title = {A Perspective on Standardizing the Predictive Power of Noninvasive Cardiovascular Tests by Likelihood Ratio Computation: 1. Mathematical Principles},
journal = {Mayo Clinic Proceedings},
volume = {74},
number = {11},
pages = {1061-1071},
year = {1999},
issn = {0025-6196},
doi = {https://doi.org/10.4065/74.11.1061},
url = {https://www.sciencedirect.com/science/article/pii/S0025619611650933},
author = {Arnold M. Weissler},
abstract = {The current practice of reporting positive and negative predictive value (PV), sensitivity (Se), and specificity (Sp) as measures of the power of noninvasive cardiovascular tests has significant limitations. A test result's PV and its comparison with other test results are highly dependent on the pretest disease prevalence at which it is determined; the citation of sensitivity and specificity provides no succinct or explicit quantitation of the rule-in and rule-out power of a test. This article presents a rationale for the use of an alternative standard for expressing predictive power in the form of positive and negative likelihood ratios, (+)LR and (-)LR. The likelihood ratios are composite expressions of test power, which incorporate the Se and Sp and their respective complements [(1 - Se) and (1 - Sp)], thus yielding single unambiguous measures of positive and negative predictive power. The likelihood ratios are calculated as follows: (+)LR = Se(l- Sp) and (-)LR = Sp/(I- Se). On analysis of the predictive value equations, the likelihood ratios equal the quotients of the posttest predictive value odds to the pretest prevalence odds for disease and no disease, respectively, as follows: (+)LR = (+)PVOd/POD and (-)LR = (-)PVOn/PON, where (+)PVO d is positive predictive value odds for disease, POD is prevalence odds for disease, (-)PVOn is negative predictive value odds for no disease, and PON is prevalence odds for no disease. Thus, the likelihood ratios are measures of the odds advantage in posttest probability of disease or no disease relative to pretest probability, independent of disease prevalence in the tested population. The quotients of the (+)LR or the (-)LR among test results studied in a common population are direct expressions of their relative predictive power in that population, The likelihood ratio principle is applicable to the evaluation of the predictive power of multiple tests performed in a common population and to estimating predictive power at multiple test thresholds.}
}
@article{BERZ1990473,
title = {Computational aspects of optics design and simulation: COSY INFINITY},
journal = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
volume = {298},
number = {1},
pages = {473-479},
year = {1990},
issn = {0168-9002},
doi = {https://doi.org/10.1016/0168-9002(90)90649-Q},
url = {https://www.sciencedirect.com/science/article/pii/016890029090649Q},
author = {Martin Berz},
abstract = {The new differential algebraic (DA) techniques allow very efficient treatment and understanding of nonlinear motion in optical systems as well as circular accelerators. To utilize these techniques in their most general way, a powerful software environment is essential. A language with structure elements similar to Pascal was developed. It has object oriented features to allow for a direct utilization of the elementary operations of the DA package. The compiler of the language is written in Fortran 77 to guarantee wide portability. The language was used to write a very general beam optics code, COSY INFINITY. At its lowest level, it allows the computation of the maps of standard beam line elements including fringe fields and system parameters to arbitrary order. The power of the DA approach coupled with an adequate language environment reveals itself in the very limited length of COSY INFINITY of only a few hundred lines. Grouping of elements as well as structures for optimization and study are readily available through the features of the language. Because of the openness of the approach, it offers a lot of power for more advanced purposes. For example, it is very easy to construct new particle optical elements. There are also many ways to efficiently manipulate and analyze the maps.}
}
@article{CHIU2024100282,
title = {Developing and validating measures for AI literacy tests: From self-reported to objective measures},
journal = {Computers and Education: Artificial Intelligence},
volume = {7},
pages = {100282},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100282},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000857},
author = {Thomas K.F. Chiu and Yifan Chen and King Woon Yau and Ching-sing Chai and Helen Meng and Irwin King and Savio Wong and Yeung Yam},
keywords = {AI literacy, Instrument, K-12 education, AI education, Co-design process, Measures},
abstract = {The majority of AI literacy studies have designed and developed self-reported questionnaires to assess AI learning and understanding. These studies assessed students' perceived AI capability rather than AI literacy because self-perceptions are seldom an accurate account of true measures. International assessment programs that use objective measures to assess science, mathematical, digital, and computational literacy back up this argument. Furthermore, because AI education research is still in its infancy, the current definition of AI literacy in the literature may not meet the needs of young students. Therefore, this study aims to develop and validate an AI literacy test for school students within the interdisciplinary project known as AI4future. Engineering and education researchers created and selected 25 multiple-choice questions to accomplish this goal, and school teachers validated them while developing an AI curriculum for middle schools. 2390 students in grades 7 to 9 took the test. We used a Rasch model to investigate the discrimination, reliability, and validity of the items. The results showed that the model met the unidimensionality assumption and demonstrated a set of reliable and valid items. They indicate the quality of the test items. The test enables AI education researchers and practitioners to appropriately evaluate their AI-related education interventions.}
}
@article{GRUJIC2024,
title = {Neurobehavioral meaning of pupil size},
journal = {Neuron},
year = {2024},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2024.05.029},
url = {https://www.sciencedirect.com/science/article/pii/S0896627324004069},
author = {Nikola Grujic and Rafael Polania and Denis Burdakov},
keywords = {pupil, arousal, cognition, noradrenaline, orexin, hypocretin},
abstract = {Summary
Pupil size is a widely used metric of brain state. It is one of the few signals originating from the brain that can be readily monitored with low-cost devices in basic science, clinical, and home settings. It is, therefore, important to investigate and generate well-defined theories related to specific interpretations of this metric. What exactly does it tell us about the brain? Pupils constrict in response to light and dilate during darkness, but the brain also controls pupil size irrespective of luminosity. Pupil size fluctuations resulting from ongoing “brain states” are used as a metric of arousal, but what is pupil-linked arousal and how should it be interpreted in neural, cognitive, and computational terms? Here, we discuss some recent findings related to these issues. We identify open questions and propose how to answer them through a combination of well-defined tasks, neurocomputational models, and neurophysiological probing of the interconnected loops of causes and consequences of pupil size.}
}
@article{MAYER2024115725,
title = {Site heterogeneity and broad surface-binding isotherms in modern catalysis: Building intuition beyond the Sabatier principle},
journal = {Journal of Catalysis},
volume = {439},
pages = {115725},
year = {2024},
issn = {0021-9517},
doi = {https://doi.org/10.1016/j.jcat.2024.115725},
url = {https://www.sciencedirect.com/science/article/pii/S002195172400438X},
author = {James M. Mayer},
abstract = {Learning the science of heterogeneous catalysis and electrocatalysis always starts with the simple case of a flat, uniform surface with an ideal adsorbate. It has of course been recognized for a century that real catalysts are more complicated. For the increasingly complex catalysts of the 21st century, this Perspective argues that surface heterogeneity and non-ideal binding isotherms are central features, and their implications need to be incorporated in current thinking. A variety of systems are described herein where catalyst complexity leads to broad, non-Langmuirian surface isotherms for the binding of hydrogen atoms – and this occurs even for ideal, flat Pt(111) surfaces. Modern catalysis employs nanoscale materials whose surfaces have substantial step, edge, corner, impurity, and other defect sites, and they increasingly have both metallic and non-metallic elements MnXm, including metal oxides, chalcogenides, pnictides, carbides, doped carbons, etc. The surfaces of such catalysts are often not crystal facets of the bulk phase underneath, and they typically have a variety of potential active sites. Catalytic surfaces in operando are often non-stoichiometric, amorphous, dynamic, and impure, and often vary from one part of the surface to another. Understanding of the issues that arise at such nanoscale, multi-element catalysts is just beginning to emerge. Yet these catalysts are widely discussed using Brønsted/Bell-Evans-Polanyi (BEP) relations, volcano plots, Tafel slopes, the Butler-Volmer equation, and other linear free energy relations (LFERs), which all depend on the implicit assumption that the active sites are “similar” and that surface adsorption is close to ideal. These assumptions underly the ubiquitous intuition based on the Sabatier Principle, that the fastest catalysis will occur when key intermediates have free energies of adsorption that are not too strong nor too weak. Current catalysis research often aims to minimize the complexity of non-ideal isotherms through experimental and computational design (e.g., the use of single crystal surfaces), and these studies are the foundation of the field. In contrast, this Perspective argues that the heterogeneity of binding sites and binding energies is an inherent strength of these catalysts. This diversity makes many nanoscale catalysts inherently a high-throughput screen wrapped in a tiny package. Only by making the heterogeneity part of the foundation of catalysis models, sorting the types of active sites and dissecting non-ideal binding isotherms, will modern catalysis learn to harness the inherent diversity of real catalysts. Controlling and exploiting diversity rather than avoiding it will help to optimize complex modern catalysts and catalytic conditions.}
}
@article{AI2022631,
title = {Reconsidering autistic ‘camouflaging’ as transactional impression management},
journal = {Trends in Cognitive Sciences},
volume = {26},
number = {8},
pages = {631-645},
year = {2022},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2022.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1364661322001061},
author = {Wei Ai and William A. Cunningham and Meng-Chuan Lai},
keywords = {autism, camouflaging, impression management, predictive coding, social alignment, wellbeing},
abstract = {Social performances pervade human interactions. Some autistic people describe their social performances as ‘camouflaging’ and engage in these performances to mitigate social challenges and survive in the neurotypical world. Here, we reconsider autistic camouflaging under the unifying framework of impression management (IM) by examining overlapping and unique motivations, neurocognitive mechanisms, and consequences. Predictive coding and Bayesian principles are synthesized into a computational model of IM that applies to autistic and neurotypical people. Throughout, we emphasize the inherently transactional, context-dependent nature of IM, the distinct computational challenges faced by autistic people, and the psychological toll that compelled IM can take. Viewing camouflaging through this lens highlights the pressing needs to change societal attitudes, destigmatize autism, refine social skills-building programs for autistic individuals, and integrate these programs with environment-focused support.}
}
@article{SIGALA2018151,
title = {New technologies in tourism: From multi-disciplinary to anti-disciplinary advances and trajectories},
journal = {Tourism Management Perspectives},
volume = {25},
pages = {151-155},
year = {2018},
issn = {2211-9736},
doi = {https://doi.org/10.1016/j.tmp.2017.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S2211973617301435},
author = {Marianna Sigala},
abstract = {Technologies transform tourism management and marketing from a static and utilitarian sense (whereby managers and tourists use technologies as tools) to a transformative conceptualization whereby tourism markets and actors both shape and are shaped by technology. This paper unravels the transformative power of technologies on: the tourism actors and resources (both the traditional but also new actors, i.e. the technology agents); the ways actors interact to (co-)create but also (co-)destruct tourism value; and the context in which tourism actors interact from a linear supply chain tourism ‘industry’ to a complex socio-technical smart tourism ecosystem. To study such complex phenomena and transformations, the paper emphasises that research should not only adopt a multi-disciplinary approach, but it also needs to follow an anti-disciplinary thinking whereby new knowledge and constructs do not simply fall within existing paradigms, disciplinary silos and mindsets once developed by studying the ‘pure’ humans and their behaviours.}
}
@article{GOVIL2022103125,
title = {Validation of agile methodology as ideal software development process using Fuzzy-TOPSIS method},
journal = {Advances in Engineering Software},
volume = {168},
pages = {103125},
year = {2022},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2022.103125},
url = {https://www.sciencedirect.com/science/article/pii/S0965997822000357},
author = {Nikhil Govil and Ashish Sharma},
keywords = {Software Development Process, Decision support system, Fuzzy logic, Agile Software Development, Fuzzy TOPSIS, Multi-Criteria Decision Making},
abstract = {Agile methodologies have been an emerging choice of software professionals for the past decade and a half. However, apart from this, some other SDLC models are also available for selection in front of software developers to develop any software. Usually, project managers select any of these models to develop software through their past experiences. There is no logical basis for this selection to be completely correct, as a result of which there is always a risk of software failure or over budget if an inappropriate model has opted. Keeping this problem of software industries in mind, an ideal SDLC model has been identified mathematically in this article. In this article, we applied the Fuzzy TOPSIS method that validates Agile software development as an ideal choice. We have taken a total of six software development processes that are being applied globally. Feedback from five experienced decision-makers has been taken in the form of linguistic terms and further converted into fuzzy values to perform the computation of the closeness coefficient rank of each experimented alternative software development process.}
}