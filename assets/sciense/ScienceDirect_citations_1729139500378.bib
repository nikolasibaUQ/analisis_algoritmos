@article{MIYAMOTO2023869,
title = {An evaluation of homeostatic plasticity for ecosystems using an analytical data science approach},
journal = {Computational and Structural Biotechnology Journal},
volume = {21},
pages = {869-878},
year = {2023},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2023.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S2001037023000016},
author = {Hirokuni Miyamoto and Jun Kikuchi},
keywords = {Biodiversity, Environmental analysis, Network, Machine learning, Statistical inference},
abstract = {The natural world is constantly changing, and planetary boundaries are issuing severe warnings about biodiversity and cycles of carbon, nitrogen, and phosphorus. In other views, social problems such as global warming and food shortages are spreading to various fields. These seemingly unrelated issues are closely related, but it can be said that understanding them in an integrated manner is still a step away. However, progress in analytical technologies has been recognized in various fields and, from a microscopic perspective, with the development of instruments including next-generation sequencers (NGS), nuclear magnetic resonance (NMR), gas chromatography-mass spectrometry (GC/MS), and liquid chromatography-mass spectrometry (LC/MS), various forms of molecular information such as genome data, microflora structure, metabolome, proteome, and lipidome can be obtained. The development of new technology has made it possible to obtain molecular information in a variety of forms. From a macroscopic perspective, the development of environmental analytical instruments and environmental measurement facilities such as satellites, drones, observation ships, and semiconductor censors has increased the data availability for various environmental factors. Based on these background, the role of computational science is to provide a mechanism for integrating and understanding these seemingly disparate data sets. This review describes machine learning and the need for structural equations and statistical causal inference of these data to solve these problems. In addition to introducing actual examples of how these technologies can be utilized, we will discuss how to use these technologies to implement environmentally friendly technologies in society.}
}
@article{LI2021512,
title = {Design and implementation of neural network computing framework on Zynq SoC embedded platform},
journal = {Procedia Computer Science},
volume = {183},
pages = {512-518},
year = {2021},
note = {Proceedings of the 10th International Conference of Information and Communication Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.02.091},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921005676},
author = {Xingying Li and Zhenyu Yin and Fulong Xu and Feiqing Zhang and Guangyuan Xu},
keywords = {Neural network, embedded platform, Zynq SoC, darknet, depthwise separable convolution, MobileNetV2},
abstract = {Limited resources and low computing power of embedded platform make it difficult to apply neural network technology. To overcome this problem, a new neural network computing framework “Zynq-Darknet” was proposed. The framework is based on Darknet, which constructs depthwise separable convolution and a lightweight classiﬁcation model MobileNetV2 and was deployed to Xilinx Zynq-7000 System-on-Chip (SoC) with Linux operating system (OS). In order to verify the performance of the framework and model, experiments were conducted on imagenet-1k dataset using different network structures. The results show that the MobileNetV2 network model based on Zynq-Darknet can effectively perform image classification, and ensure a certain real-time and accuracy while reducing the computational complexity and storage overhead, assuming promising application prospects.}
}
@article{MANLEY201427,
title = {A framework for simulating large-scale complex urban traffic dynamics through hybrid agent-based modelling},
journal = {Computers, Environment and Urban Systems},
volume = {44},
pages = {27-36},
year = {2014},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2013.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0198971513001129},
author = {Ed Manley and Tao Cheng and Alan Penn and Andy Emmonds},
keywords = {Agent-based simulation, Urban complexity, Human cognition, Collective phenomena, Traffic flow, Hybrid simulation},
abstract = {Urban road traffic dynamics are the product of the behaviours and interactions of thousands, often millions of individuals. Traditionally, models of these phenomena have incorporated simplistic representations of individual behaviour, ensuring the maximisation of simulation scale under given computational constraints. Yet, by simplifying representations of behaviour, the overall predictive capability of the model inevitably reduces. In this work a hybrid agent-based modelling framework is introduced that aims to balance the demands of behavioural realism and computational capacity, integrating a descriptive representation of driver behaviour with a simplified, collective model of traffic flow. The hybridisation of these approaches within an agent-based modelling framework yields a representation of urban traffic flow that is driven by individual behaviour, yet, in reducing the computational intensity of simulated physical interaction, enables the scalable expansion to large numbers of agents. A real-world proof-of-concept case study is presented, demonstrating the application of this approach, and showing the gains in computational efficiency made in utilising this approach against traditional agent-based approaches. The paper concludes in addressing how this model might be extended, and exploring the role hybrid agent-based modelling approaches may hold in the simulation of other complex urban phenomena.}
}
@article{CHANG201323,
title = {Discovering Taiwanese design college students’ learning performance and imaginative capacity},
journal = {Thinking Skills and Creativity},
volume = {10},
pages = {23-39},
year = {2013},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2013.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S1871187113000266},
author = {Hsiang-Tang Chang and Tung-I. Lin},
keywords = {Imaginative capacity, Imagination, Learning performance, Design college, RASCH measurement},
abstract = {Imagination affects not only the structure of design ideas at the initial stage but also influences the manifestation of final products. The purpose of this study was to investigate the association between Taiwanese design college students’ imaginative capacity and their learning performance in class. On the basis of recent scholarship, the authors proposed several reasonably related factors, which were classified into three aspects: personality traits, learning atmosphere, and imaginative thinking. They then verified and discussed four research questions through a teaching experiment with 63 junior college students in YunTech, Taiwan. To proceed smoothly without significantly changing the current teaching process, the authors developed a set of supplementary teaching material and two sets of questionnaires which they then used in the teaching experiment. The results of the teaching experiment proved and suggested the following points corresponding to the research questions: (1) students’ senior high school backgrounds have an effect on their imaginative capacities; (2) judges from other schools should be invited to join the judgement to ensure fairness and with a broader scope; (3) students’ imaginative capacity indeed has an effect on the grade of their final products in the judgement; (4) teachers can identify students with higher imaginative capacity through the responses to the proposed supplementary teaching materials and questionnaires used in the study's curricula. Furthermore, the supplementary teaching material is conjectured to be able to inspire students’ imaginative capacity.}
}
@article{KUO2013510,
title = {Cultural Evolution Algorithm for Global Optimizations and its Applications},
journal = {Journal of Applied Research and Technology},
volume = {11},
number = {4},
pages = {510-522},
year = {2013},
issn = {1665-6423},
doi = {https://doi.org/10.1016/S1665-6423(13)71558-X},
url = {https://www.sciencedirect.com/science/article/pii/S166564231371558X},
author = {H.C. Kuo and C.H. Lin},
keywords = {Cultural Algorithm, Genetic Algorithm, Nelder-Mead’s simplex method, Global optimization},
abstract = {The course of socio-cultural transition can neither be aimless nor arbitrary, instead it requires a clear direction. A common goal of social species’ evolution is to move towards an advanced spiritual and conscious state. This study aims to develop a population-based algorithm on the basis of cultural transition goal. In this paper, the socio-cultural model based on a system thought framework could be used to develop a cultural evolution algorithm (CEA). CEA leverage four strategies, each consists of several search methods with similar thinking. Seven benchmark functions are utilized to validate the search performance of the proposed algorithm. The results show that all of the four strategies of cultural evolution algorithm have better performance when compared with relevant literatures. Finally, the CEA was then applied to optimize two different reliability engineering problems, a Serial-Parallel System design and a Bridge System design. For the Serial-Parallel System design, the CEA achieved the exact solution with ease, and for the Bridge System design, the solution obtained by the CEA is superior to those from other literatures.}
}
@article{PANG2022118826,
title = {Uncovering the global task-modulated brain network in chunk decomposition with Chinese characters},
journal = {NeuroImage},
volume = {247},
pages = {118826},
year = {2022},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2021.118826},
url = {https://www.sciencedirect.com/science/article/pii/S1053811921010971},
author = {Jiaoyan Pang and Hanning Guo and Xiaochen Tang and Yu Fu and Zhengwu Yang and Yongchao Li and Na An and Jing Luo and Zhijun Yao and Bin Hu},
keywords = {Cognitive pattern, Beta-series correlation, Chinese characters, Thalamus, Hippocampus},
abstract = {Chunk decomposition, which requires the mental representation transformation in accordance with behavioral goals, is of vital importance to problem solving and creative thinking. Previous studies have identified that the frontal, parietal, and occipital cortex in the cognitive control network selectively activated in response to chunk tightness, however, functional localization strategy may overlook the interaction brain regions. Based on the notion of a global brain network, we proposed that multiple specialized regions have to be interconnected to maintain goal representation during the course of chunk decomposition. Therefore, the present study applied a beta-series correlation method to investigate interregional functional connectivity in the event-related design of chunk decomposition tasks using Chinese characters, which would highlight critical nodes irrespective to chunk tightness. The results reveal a network of functional hubs with highly within or between module connections, including the orbitofrontal cortex, superior/inferior parietal lobule, hippocampus, and thalamus. We speculate that the thalamus integrates information across modular as an integrative hub while the orbitofrontal cortex tracks the mental states of chunk decomposition on a moment-to-moment basis. The superior and inferior parietal lobule collaborate to manipulate the mental representation of chunk decomposition and the hippocampus associates the relationship between elements in the question and solution phase. Furthermore, the tightness of chunks is not only associated with different processors in visual systems but also leads to increased intermodular connections in right superior frontal gyrus and left precentral gyrus. To summary up, the present study first reveals the task-modulated brain network of chunk decomposition in addition to the tightness-related nodes in the frontal and occipital cortex.}
}
@article{FOO201410,
title = {Evolution of acquired resistance to anti-cancer therapy},
journal = {Journal of Theoretical Biology},
volume = {355},
pages = {10-20},
year = {2014},
issn = {0022-5193},
doi = {https://doi.org/10.1016/j.jtbi.2014.02.025},
url = {https://www.sciencedirect.com/science/article/pii/S0022519314001003},
author = {Jasmine Foo and Franziska Michor},
keywords = {Drug resistance, Cancer, Evolution, Mathematical modeling, Optimal dosing strategies},
abstract = {Acquired drug resistance is a major limitation for the successful treatment of cancer. Resistance can emerge due to a variety of reasons including host environmental factors as well as genetic or epigenetic alterations in the cancer cells. Evolutionary theory has contributed to the understanding of the dynamics of resistance mutations in a cancer cell population, the risk of resistance pre-existing before the initiation of therapy, the composition of drug cocktails necessary to prevent the emergence of resistance, and optimum drug administration schedules for patient populations at risk of evolving acquired resistance. Here we review recent advances towards elucidating the evolutionary dynamics of acquired drug resistance and outline how evolutionary thinking can contribute to outstanding questions in the field.}
}
@article{SATO2019293,
title = {Statistical analysis of word usage in biological publications since 1965: Historical delineation highlighting an emergence of function-oriented discourses in contemporary molecular and cellular biology},
journal = {Journal of Theoretical Biology},
volume = {462},
pages = {293-303},
year = {2019},
issn = {0022-5193},
doi = {https://doi.org/10.1016/j.jtbi.2018.11.017},
url = {https://www.sciencedirect.com/science/article/pii/S0022519318305708},
author = {Naoki Sato and Kaoru Sato},
keywords = {Contemporary biology, Function, History of biology, Statistical text analysis, Role, Social responsibility of research},
abstract = {Typical studies on the history of science, or particularly of biology, have been focused on a particular scientist or book, but this selection has a risk of being arbitrary. To find a more objective way of studying history of biology, we applied a statistical method. First, we downloaded from the PubMed database all available titles and abstracts of 934,807 articles in 32 selected journals from 1965 to 2014, and extracted most frequently used 322 terms by text mining. Clustering of these terms according to the annual frequency of usage resulted in three main clusters: Cluster 1 represented terms that were no longer used frequently, Cluster 3 included terms that became abundantly used recently, and Cluster 2 contained terms constantly used. Three phases were delineated in the history of biology over the past 50 years, with transitions in 1987 and 1997. In contrast with our tacit understanding that “function” is a key notion in biological thinking, the results suggest that function-oriented discourses are a new habit of biologists in the genomic era after 1997, in which biological researches focus on identifying a link between a molecule or a structure with its function. We hypothesize that, in spite of repeated warnings, function-related discourses have a teleological connotation, which is easily misunderstood by general audience and, with emphatic expressions such as “important” and “essential”, fit to the need for justification of researches as part of researcher's responsibility for public funding.}
}
@article{PICCININI2004811,
title = {Functionalism, computationalism, and mental states},
journal = {Studies in History and Philosophy of Science Part A},
volume = {35},
number = {4},
pages = {811-833},
year = {2004},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2004.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0039368104000883},
author = {Gualtiero Piccinini},
keywords = {Functionalism, Computationalism, Computational functionalism, Mental states, Computational theory of mind, Functional analysis},
abstract = {Some philosophers have conflated functionalism and computationalism. I reconstruct how this came about and uncover two assumptions that made the conflation possible. They are the assumptions that (i) psychological functional analyses are computational descriptions and (ii) everything may be described as performing computations. I argue that, if we want to improve our understanding of both the metaphysics of mental states and the functional relations between them, we should reject these assumptions.}
}
@article{KIM2024105050,
title = {G-TRACE: Grouped temporal recalibration for video object segmentation},
journal = {Image and Vision Computing},
volume = {147},
pages = {105050},
year = {2024},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2024.105050},
url = {https://www.sciencedirect.com/science/article/pii/S0262885624001549},
author = {Jiyun Kim and JooHo Kim and Sungeun Hong},
keywords = {Semi-supervised video object segmentation, Memory attention, Hierarchical grouping},
abstract = {In Semi-supervised Video Object Segmentation (SVOS), there is a critical emphasis on enhancing the memory and readout mechanisms for frame matching, especially in relation to temporal dynamics. Current methods predominantly use 2D CNNs for encoding video frames, which unfortunately neglects the crucial aspect of addressing temporal variations in individual frames and their associated masks during the encoding process. One potential solution would be to implement temporal models such as 3D CNNs instead of 2D CNNs, but this significantly increases computational requirements, making it impractical for real-world SVOS applications. In this paper, we introduce the Grouped Temporal Recalibration with Attention for Convolutional Encoders (G-TRACE), a novel plug-and-play module that is compatible with various existing SVOS frameworks. G-TRACE uses hierarchical memory-centric attention and integrates effortlessly with 2D CNNs, offering a novel approach to temporal modeling that operates orthogonally to traditional frame matching methods. Extensive evaluations on four widely-used benchmarks demonstrate that our method consistently delivers significant performance improvements over various baseline models.}
}
@article{GROOS2024105420,
title = {Combining user-centered design and behavioral theory to enhance health technologies: A personas-based approach for a primary-care based multifactorial falls risk assessment tool},
journal = {International Journal of Medical Informatics},
volume = {186},
pages = {105420},
year = {2024},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2024.105420},
url = {https://www.sciencedirect.com/science/article/pii/S1386505624000832},
author = {Sara S. Groos and Annemiek J. Linn and Judith I. Kuiper and Natasja M. {van Schoor} and Nathalie {van der Velde} and Julia C.M. {van Weert}},
keywords = {Personas, User-centered design, Behavioral theory, Multifactorial falls risk assessment tools, Implementation},
abstract = {Introduction
Multifactorial falls risk assessment tools (FRATs) can be an effective falls prevention method for older adults, but are often underutilized by health care professionals (HCPs). This study aims to enhance the use and implementation of multifactorial FRATs by combining behavioral theory with the user-centered design (UCD) method of personas construction. Specifically, the study aimed to (1) construct personas that are based on external (i.e., needs, preferences) and intrinsic user characteristics (i.e., behavioral determinants); and (2) use these insights to inform requirements for optimizing an existing Dutch multifactorial FRAT (i.e., the ‘Valanalyse’).
Methods
Survey data from HCPs (n = 31) was used to construct personas of the ‘Valanalyse.’ To examine differences between clusters on 68 clustering variables, a multivariate cluster analysis technique with non-parametric analyses and computational methods was used. The aggregated external and intrinsic user characteristics of personas were used to inform key design and implementation requirements for the ‘Valanalyse,’ respectively, whereby intrinsic user characteristics were matched with appropriate behavior change techniques to guide implementation.
Results
Significant differences between clusters were observed in 20 clustering variables (e.g., behavioral beliefs, situations for use). These variables were used to construct six personas representing users of each cluster. Together, the six personas helped operationalize four key design requirements (e.g., guide treatment-related decision making) and 14 implementation strategies (e.g., planning coping responses) for optimizing the ‘Valanalyse’ in Dutch geriatric, primary care settings.
Conclusion
The findings suggest that theory- and evidence-based personas that encompass both external and intrinsic user characteristics are a useful method for understanding how the use and implementation of multifactorial FRATs can be optimized with and for HCPs, providing important implications for developers and eHealth interventions with regards to encouraging technology adoption.}
}
@article{CHIHAB2023106810,
title = {Thermal performance and energy efficiency of the composite clay and hemp fibers},
journal = {Journal of Building Engineering},
volume = {73},
pages = {106810},
year = {2023},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2023.106810},
url = {https://www.sciencedirect.com/science/article/pii/S2352710223009890},
author = {Yassine Chihab and Najma Laaroussi and Mohammed Garoum},
keywords = {Dynamic thermal, Composite materials, Clay bricks, Hemp fibers, Energy-savings},
abstract = {The purpose of this research is to demonstrate that incorporating hemp fibers into earth bricks can provide an adequate level of thermal comfort by enhancing the material's dynamic thermal characteristics. The main goal is to find the optimal thickness of a clay-hemp wall to attain the highest thermal inertia values. First, the flash method was used to estimate thermal diffusivity, and the state hot plate method was used to measure the thermal conductivity of a clay brick. Using the experimental data as input, computational analysis is performed to investigate the relationship between the thermal performance of composite materials and their microstructures, with the goal of predicting the effective thermal conductivity of the composite clay-hemp. Using the Random Sequential Addition algorithm, a two-phase, three-dimensional composite clay-hemp microstructure was produced. The effective thermal conductivity of these composites was assessed using the finite volume method. The predicted thermophysical characteristics were then utilized to simulate the transient heat transfer across clay-hemp walls. The results demonstrate that when the hemp volume fraction increased, the thermal conductivity, thermal diffusivity, and thermal volumetric capacity all decreased by approximately 52%, 27%, and 35%, respectively. Furthermore, incorporating hemp fibers improved the bricks' dynamic thermal characteristics. Finally, this study has revealed that a wall composed of clay-hemp bricks of 22 cm thick and an insulating layer of 6 cm thick allows for limiting the risk of overheating during the summer months (time lag between 10 and 12 h) while also satisfying the Moroccan Thermal Construction Regulation requirement for Marrakech city.}
}
@article{HANSEN2022101637,
title = {From newspaper supplement to data company: Tracking rhetorical change in the Times Higher Education’s rankings coverage},
journal = {Poetics},
volume = {92},
pages = {101637},
year = {2022},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2021.101637},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X21001352},
author = {Morten Hansen and Astrid {Van den Bossche}},
keywords = {Computational hermeneutics, Rhetoric, Ratio, University rankings, Times Higher Education},
abstract = {Despite their importance, little is known about the companies behind global university rankings and how they have legitimized the use of league tables as structuring devices in the higher education sector. Taking a computational approach to Burke's dramatistic pentad, we analyse a corpus of 3,296 articles printed between 1994 and 2020 in the Times Higher Education magazine, publisher of the World University Rankings. We show how coverage of the rankings is subject to shifts in rhetorical strategy as Times Higher Education has developed into a ranking powerhouse. Over time, the magazine has spectacularized higher education by making changes in the rankings newsworthy, and has thereby cemented the company's position as an arbiter, reporter, and consultant in the sector.}
}
@article{MAFTEI2022107032,
title = {Using fake news as means of cyber-bullying: The link with compulsive internet use and online moral disengagement},
journal = {Computers in Human Behavior},
volume = {127},
pages = {107032},
year = {2022},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2021.107032},
url = {https://www.sciencedirect.com/science/article/pii/S0747563221003551},
author = {Alexandra Maftei and Andrei-Corneliu Holman and Ioan-Alex Merlici},
keywords = {Fake news, Online moral disengagement, Cyberbullying, Compulsive internet use},
abstract = {Online moral disengagement and cyberbullying can enhance fake news spreading. We explored the links between these variables and compulsive Internet use in a sample of 509 teenagers and adults aged 11 to 67. We investigated the effect of compulsive Internet use on cyberbullying through fake news creation and/or distribution, both direct and via moral disengagement, and the related differences between adults and teenagers. The indirect effect of compulsive Internet use on cyberbullying through moral disengagement was significant in adolescents, but not in adults. As assumed, teenagers scored significantly higher than adults on all the primary variables. Contrary to our expectations, no significant gender differences emerged, regardless of participants' age, in terms of compulsive Internet use, moral disengagement, nor cyberbullying. The results emphasize the importance of relevant online education programs designed to engage both teenagers and adults in critical thinking that might help in the fake news detection process, especially during the COVID-19 pandemic.}
}
@article{KOCHEN1958267,
title = {The acquisition and utilization of information in problem solving and thinking},
journal = {Information and Control},
volume = {1},
number = {3},
pages = {267-288},
year = {1958},
issn = {0019-9958},
doi = {https://doi.org/10.1016/S0019-9958(58)80005-4},
url = {https://www.sciencedirect.com/science/article/pii/S0019995858800054},
author = {Manfred Kochen and Eugene H. Galanter},
abstract = {Some of the logical consequences of drawing a distinction between the following two aspects of problem-solving behavior are explored: (a) actions directed toward the acquisition of information to guide future actions toward valuable goals; (b) actions directed toward the utilization of accumulated information to attain a valuable goal. An experimental paradigm accomplishing this separation is described for the case of an environment of periodic sequences of binary events. A general way of describing behavioral strategies is developed in terms of: (a) a plan for when to acquire information, to guess an outcome, or to guess at the solution; and (b) a program for how to compute guesses from the information accumulated. The structure of the binary environmental sequences, the structure of these behavioral strategies, and the relations between them are analyzed, and certain strategies which maximize value are suggested. Computing machine interpretations of certain specific strategies for a restricted kind of experiment are displayed, and predictions from these are compared with experimental data from pilot studies performed with human subjects.}
}
@article{PESSOA2019158,
title = {Neural dynamics of emotion and cognition: From trajectories to underlying neural geometry},
journal = {Neural Networks},
volume = {120},
pages = {158-166},
year = {2019},
note = {special Issue in Honor of the 80th Birthday of Stephen Grossberg},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019302242},
author = {Luiz Pessoa},
keywords = {Emotion, Cognition, Dynamics, Trajectories, Manifold},
abstract = {How can we study, characterize, and understand the neural underpinnings of cognitive-emotional behaviors as inherently dynamic processes? In the past 50 years, Stephen Grossberg has developed a research program that embraces the themes of dynamics, decentralized computation, emergence, selection and competition, and autonomy. The present paper discusses how these principles can be heeded by experimental scientists to advance the understanding of the brain basis of behavior. It is suggested that a profitable way forward is to focus on investigating the dynamic multivariate structure of brain data. Accordingly, central research problems involve characterizing “neural trajectories” and the associated geometry of the underlying “neural space.” Finally, it is argued that, at a time when the development of neurotechniques has reached a fever pitch, neuroscience needs to redirect its focus and invest comparable energy in the conceptual and theoretical dimensions of its research endeavor. Otherwise we run the risk of being able to measure “every atom” in the brain in a theoretical vacuum.}
}
@article{ROLLS2024102636,
title = {A theory of hippocampal function: New developments},
journal = {Progress in Neurobiology},
volume = {238},
pages = {102636},
year = {2024},
issn = {0301-0082},
doi = {https://doi.org/10.1016/j.pneurobio.2024.102636},
url = {https://www.sciencedirect.com/science/article/pii/S0301008224000728},
author = {Edmund T. Rolls and Alessandro Treves},
keywords = {Hippocampus, Episodic memory, Attractor network, Memory recall, Neocortical memory, Consolidation},
abstract = {We develop further here the only quantitative theory of the storage of information in the hippocampal episodic memory system and its recall back to the neocortex. The theory is upgraded to account for a revolution in understanding of spatial representations in the primate, including human, hippocampus, that go beyond the place where the individual is located, to the location being viewed in a scene. This is fundamental to much primate episodic memory and navigation: functions supported in humans by pathways that build ‘where’ spatial view representations by feature combinations in a ventromedial visual cortical stream, separate from those for ‘what’ object and face information to the inferior temporal visual cortex, and for reward information from the orbitofrontal cortex. Key new computational developments include the capacity of the CA3 attractor network for storing whole charts of space; how the correlations inherent in self-organizing continuous spatial representations impact the storage capacity; how the CA3 network can combine continuous spatial and discrete object and reward representations; the roles of the rewards that reach the hippocampus in the later consolidation into long-term memory in part via cholinergic pathways from the orbitofrontal cortex; and new ways of analysing neocortical information storage using Potts networks.}
}
@article{SUCHANTKE2020439,
title = {Space sustainability in Martian orbits — First insights in a technical and regulatory analysis},
journal = {Journal of Space Safety Engineering},
volume = {7},
number = {3},
pages = {439-446},
year = {2020},
note = {Space Debris: The State of Art},
issn = {2468-8967},
doi = {https://doi.org/10.1016/j.jsse.2020.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S2468896720300677},
author = {Isabell Suchantke and Francesca Letizia and Vitali Braun and Holger Krag},
abstract = {Hazards from the outer space environment either natural (space weather and asteroids) or artificial (space debris and the growing number of satellites launched to orbit) pose a rising risk to space flight activities. The awareness for space sustainability and space safety has seen a continuous increase in recent years and does not stop at the Earth's sphere of influence. The first spacefaring nations start thinking about sustainability in cislunar space and the Martian environment. This work deals with the issue of space debris in Martian orbits in the light of planetary protection. A Mars Sustainability Framework has been developed. This includes a study on the orbital and regulatory environment of Mars, long-term propagation of orbits of artificial objects and the two natural moons, and the analysis of objects evolution and first approaches for collision probability computation. With this work, the issue of space debris beyond Earth orbit is analysed at an early stage.}
}
@article{LABO2018185,
title = {Application of low-invasive techniques and incremental seismic rehabilitation to increase the feasibility and cost-effectiveness of seismic interventions},
journal = {Procedia Structural Integrity},
volume = {11},
pages = {185-193},
year = {2018},
note = {XIV INTERNATIONAL CONFERENCE ON BUILDING PATHOLOGY AND CONSTRUCTIONS REPAIR, FLORENCE, ITALY, JUNE 20-22, 2018},
issn = {2452-3216},
doi = {https://doi.org/10.1016/j.prostr.2018.11.025},
url = {https://www.sciencedirect.com/science/article/pii/S2452321618301264},
author = {S. Labò and E. Casprini and C. Passoni and J. Zanni and A. Belleri and A. Marini and P. Riva},
keywords = {Incremental rehabilitation, seismic retrofit, renovation strategy, low-invasive techniques, life cycle thinking, diagrid, school buildings},
abstract = {The high seismic risk connected to the existing construction heritage requires a wide-scale renovation action to ensure structural resilience and avoid future human and economic losses. Given the urgency and the scale of the problem and the lack of available resources, a new strategy for the renovation of the obsolete European building stock should be envisioned, accounting for both safety and environmental, social and economic sustainability. This research aims at exploring new cost-effective seismic retrofit solutions based on the principles of low-invasiveness and incremental seismic rehabilitation, as envisioned by FEMA P-420 (2009). The incremental rehabilitation approach allows to plan repair and retrofit actions along with the maintenance works expected during the building's lifetime, thereby spreading them in time and reducing costs. In addition, low-invasiveness of the solutions is required to reduce the impacts on the functionality of the building, thus cutting the costs connected to downtime. A possible solution is represented by the introduction of an exoskeleton entirely carried out from outside. In this paper, a new sustainable technique is proposed, where the existing structure is connected to a self-supporting exoskeleton adopting demountable dry techniques, which may be assembled and activated in different phases of the building lifetime. As a proof of concept, the approach is then applied to a school building.}
}
@article{LEE2024100211,
title = {A systematic review of AI education in K-12 classrooms from 2018 to 2023: Topics, strategies, and learning outcomes},
journal = {Computers and Education: Artificial Intelligence},
volume = {6},
pages = {100211},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100211},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000122},
author = {Sang Joon Lee and Kyungbin Kwon},
keywords = {Artificial intelligence, AI education, Systematic review, K-12},
abstract = {AI education aims to teach AI concepts, essential knowledge, and skills related to the fundamental ideas in AI. As AI becomes increasingly prevalent in our daily lives, schools and educators have started to recognize the importance of AI education in K-12 schools. However, there have been a limited number of studies reporting on the implementation of AI education in classrooms. This systematic review aimed to provide an overview of the current state of AI education in K-12 schools, exploring topics, instructional approaches, and learning outcomes. Twenty-five peer-reviewed journal articles published between 2018 and 2023 were selected for this systematic review. The findings highlighted that various topics were covered in K-12 AI education, including fundamental AI concepts, different types of AI, AI applications, and ethical considerations related to AI. To facilitate meaningful learning experiences, educators frequently integrated hands-on activities and project-based learning. The findings supported the benefits of AI education in enhancing students' AI literacy, problem-solving skills, and ethical reflections on AI's societal impact. Furthermore, it fostered motivation, positive attitudes toward AI, and an interest in technology while inspiring career aspirations. It is recommended to develop tailored AI curricula, instructional strategies, and appropriate tools and resources that seamlessly integrate into various subjects within the standard school curriculum.}
}
@article{PRINSLOO2021101515,
title = {Sustainability assessment framework and methodology with trans-disciplinary numerical simulation model for analytical floatovoltaic energy system planning assessments},
journal = {Sustainable Energy Technologies and Assessments},
volume = {47},
pages = {101515},
year = {2021},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2021.101515},
url = {https://www.sciencedirect.com/science/article/pii/S2213138821005269},
author = {F.C. Prinsloo and Peter Schmitz and Andrea Lombard},
keywords = {Floatovoltaic system synthesis, WELF-nexus environmental profiling, Sustainability profiling, Floating solar simulation model, FPV sustainability assessment},
abstract = {Floatovoltaics is rapidly emerging as a novel type of sustainable energy technology, in which solar photovoltaic installations are sited directly on open-water spaces. As an agro-renewable energy-generation technology, it makes dual use of water to generate revenue from under-utilised irrigation water surfaces while also offering mutually beneficial layers of land-saving, environmental conservation and water-preservation benefits. Standardised metrics for ground-mounted photovoltaic projects, however, do not properly account for the technology’s extended range of resource-use-efficiencies and impact-effect-positives. Such knowledge gaps hinder evidence-based scientific assessments in regulatory project permissions mandated by law. Technology planning and impact assessment practices can benefit from a computer-aided technique to characterise floatovoltaic performance profiles. This paper introduces a conceptual empirical modelling framework, a holistic system dynamics-thinking methodology and a computer synthesis model to empirically predict the performance and sustainability profiles of prospective floatovoltaic installations. By inherently exploring the techno-economic and techno-environmental externalities of floatovoltaic enterprises, it translates performance profiles into sustainability indicators, articulated as WELF-nexus parameters. The paper details the integrated analytical framework, mathematical modelling formulation and digital computer synthesis model towards quantitative floatovoltaic energy system planning and sustainability assessments. The study’s main finding is that an integrated techno-enviro-economic floatovoltaic assessment methodology can be successfully modelled as a context-sensitive synthesis technique in a system dynamics modelling environment. The proposed technique can find utility in solving real-world problems with assessments in efficiency, feasibility and sustainability for agricultural floatovoltaics.}
}
@article{ALHARRASI201958,
title = {Evidence for the involvement of a GABAergic mechanism in the effectiveness of natural and synthetically modified incensole derivatives in neuropharmacological disorders: A computational and pharmacological approach},
journal = {Phytochemistry},
volume = {163},
pages = {58-74},
year = {2019},
issn = {0031-9422},
doi = {https://doi.org/10.1016/j.phytochem.2019.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0031942218308239},
author = {Ahmed Al-Harrasi and Ajmal Khan and Najeeb Ur Rehman and Sulaiman Al-Shidhani and Nasiara Karim and Imran Khan and Sobia Ahsan Halim and Ahmed Al-Rawahi and Javid Hussain and Rene Csuk},
keywords = {Incensole, Incensone, Incensfuran, Antidepressant, Anxiolytic, Anticonvulsant, Homology modelling, Molecular docking, ADMET prediction},
abstract = {In the course of our continuing exploration for novel bioactive lead compounds (s) from the species Boswellia, we have recently reported incensole derivatives isolated from Boswellia papyrifera Hochst. Given the known antidepressant-like effects of incensole and incensole acetate, we herein present that the low dose intraperitoneal administration of incensole derivatives, namely, incensfuran and incensone, showed significant antidepressant-like effects in the forced swim test (FST) and tail suspension test (TST). Furthermore, these compounds were evaluated for their anxiolytic potential in the elevated plus maze (EPM) and light dark box (LDB) tests and anticonvulsant effects in pentylenetetrazole (PTZ)-induced seizure tests. In the EPM test, administration of these compounds led to dose-dependent increases in open arm entries and in the time spent in EPM open arms. Similar results were obtained in the LDB test, wherein compounds these caused significant increases in the number of transitions between lit and dark compartments and the time spent in the lit compartment. The anxiolytic-like effects in the EPM were not reversed by pretreatment with flumazenil, whereas PTZ and bicuculline (BIC) completely abolished the anxiolytic effects, showing the involvement of the non-benzodiazepine binding sites of GABAA receptors. All four compounds induced significantly elevated brain GABA levels, indicating the involvement of a GABAergic mechanism. Additionally, molecular docking was conducted to elucidate the mode of action for the anxiolytic and anticonvulsant effects of these derivatives. Moreover, these compounds also possess drug-like properties and excellent ADMET profiles.}
}
@article{QIAN2023110898,
title = {A novel granular ball computing-based fuzzy rough set for feature selection in label distribution learning},
journal = {Knowledge-Based Systems},
volume = {278},
pages = {110898},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110898},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123006482},
author = {Wenbin Qian and Fankang Xu and Jintao Huang and Jin Qian},
keywords = {Feature selection, Label distribution learning, Granular ball, Fuzzy rough set, Granular computing},
abstract = {Label distribution learning is a widely studied supervised learning diagram that can handle the problem of label ambiguity. The increasing size of datasets is accompanied by the disaster of dimensionality, which implies that the arrival of redundant and noisy features undermines the effect of label distribution learning. As a crucial data-preprocessing technique, feature selection is capable of choosing discriminative features. However, due to the complex issue of label ambiguity, traditional feature selection approaches for datasets with logical labels cannot be applied to label distribution data. In this paper, a novel granular ball computing-based fuzzy rough set (GBFRS) is proposed for label distribution feature selection. Specifically, the proposed method is first introduced at the finest granularity, i.e., calculating similarity relations between single data points. Considering that the label ambiguity issue is exacerbated by the label imbalance phenomenon, the relative similarity in label distribution space among samples is computed for better generalization of the model. Then, a robust approximation strategy is devised for the target sample by using its true different and partially different class samples. Finally, with the concept of granular balls, the method explores the similarity relations between balls and samples, and the granular ball computing-based fuzzy rough set method is developed , which is endowed with the ability to simulate the characteristics of large-scale priorities in human thinking and considers local consistency. Extensive experiments conducted on twenty-two datasets show that GBFRS can effectively select more significant features than seven state-of-the-art feature selection algorithms.}
}
@article{JI2023119326,
title = {Fast Progressive Differentiable Architecture Search based on adaptive task granularity reorganization},
journal = {Information Sciences},
volume = {645},
pages = {119326},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.119326},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523009118},
author = {Junzhong Ji and Xingyu Wang},
keywords = {Neural Architecture Search, Differentiable Architecture Search, Shrink search space, Granular Computing, Cluster candidate},
abstract = {Shrinkage methods reduce the search space of a Differentiable Architecture Search (DARTS) by progressively discarding candidates, which accelerates the search speed. However, their shrinkage strategy suffers from the vulnerability of too fine task granularity. In other words, they drop only one of the least promising candidates per round of shrinkage, which is suboptimal in terms of performance and efficiency. In this study, we introduce the concept of Granular Computing (GrC) into the shrinkage method and present a Fast Progressive Differentiable Architecture Search (FP-DARTS) method. This method effectively reduces the computational complexity of each round of shrinkage, thereby improving the efficiency and performance of the algorithm. FP-DARTS can be divided into three stages: adaptive granularity division and selection, granular-channel performance evaluation, and progressive shrinkage. In the first stage, to reorganize the task granularity, we cluster the candidate operations into granular-channels and adaptively select the appropriate task granularity. We also propose a dynamic clustering strategy to avoid introducing additional computation. In the second stage, we train the architecture parameters to measure the potential of the granular-channels. In the third stage, to improve the stability of the shrinkage results, we introduce a channel annealing mechanism to smoothly discard unpromising granular-channels. We conducted systematic experiments on CIFAR-10 and ImageNet and achieved a test accuracy of 97.56% on CIFAR-10 with 0.04 GPU-days, and a test accuracy of 75.5% on ImageNet with 1.2 GPU-days. We also conducted experiments on the search space of NAS-Bench-201, and obtained test accuracies of 94.22, 73.07, and 46.23% for CIFAR-10, CIFAR-100 and ImageNet16-120, respectively. The above experimental results demonstrate that FP-DARTS achieves higher search speed and competitive performance compared to other state-of-the-art shrinkage methods and non-shrinkage methods.}
}
@article{SHIEH1993421,
title = {Massively parallel computational methods for finite element analysis of transient structural responses},
journal = {Computing Systems in Engineering},
volume = {4},
number = {4},
pages = {421-433},
year = {1993},
note = {Parallel Computational Methods for Large-Scale Structural Analysis and Design},
issn = {0956-0521},
doi = {https://doi.org/10.1016/0956-0521(93)90011-K},
url = {https://www.sciencedirect.com/science/article/pii/095605219390011K},
author = {R.C. Shieh},
abstract = {With the emphasis on the finitely damped system (e.g. control structure interaction) case, two fully implicit and two semi-implicit sets of finite element method-based numerical algorithms are formulated for transient response analysis of space frame and truss structures in a massively parallel processing (MPP) environment. All algorithm sets use an implicit force calculation/vector equation of motion assembly procedure. The semi-implicit algorithms are based on the explicit central difference (CD) and the fourth-order Runge-Kutta (RK4) schemes, respectively, in conjunction with the use of mass lumping techniques so that solution of the recurrence equations for unknown displacements is reduced to a trivial diagonal matrix inversion operation. The fully implicit algorithm sets are based on the Newmark Beta (NB) and CD schemes, respectively, in conjunction with use of the (iterative) preconditioned conjugate gradient (PCG) method for solving the linear algebraic recurrence equations. The semi-implicit algorithm sets are fully implemented and assessed on an MPP CM-2 computer. A preliminary assessment of the fully implicit sets of algorithms is made on a Sun Workstation. These numerical study results show that the newly formulated MPP algorithms are, to a varying degree, superefficient (or potentially superefficient) on the CM-2 compared with, and even highly competitive against, the conventional sequential algorithms on an advanced serial computer.}
}
@article{RIZZOLATTI1997562,
title = {Parietal cortex: from sight to action},
journal = {Current Opinion in Neurobiology},
volume = {7},
number = {4},
pages = {562-567},
year = {1997},
issn = {0959-4388},
doi = {https://doi.org/10.1016/S0959-4388(97)80037-2},
url = {https://www.sciencedirect.com/science/article/pii/S0959438897800372},
author = {Giacomo Rizzolatti and Leonardo Fogassi and Vittorio Gallese},
abstract = {Recent findings have altered radically our thinking about the functional role of the parietal cortex. According to this view, the parietal lobe consists of a multiplicity of areas with specific connections to the frontal lobe. These areas, together with the frontal areas to which they are connected, mediate distinct sensorimotor transformations related to the control of hand, arm, eye or head movements. Space perception is not unitary, but derives from the joint activity of the fronto-parietal circuits that control actions requiring space computation.}
}
@article{WANG2024100257,
title = {A resource prediction method for air traffic cyber-physical-social system},
journal = {Transportation Engineering},
volume = {17},
pages = {100257},
year = {2024},
issn = {2666-691X},
doi = {https://doi.org/10.1016/j.treng.2024.100257},
url = {https://www.sciencedirect.com/science/article/pii/S2666691X24000320},
author = {Jintao Wang and Huaiqi Chen and Yulong Yin and Zijian Jiang and Meili Chen},
keywords = {Air traffic system, Complex network, Resource prediction, Cyber-physical-social system, Neural network},
abstract = {Air traffic is exhibiting the characteristics of large flow, strong coupling, and high time variation. Therefore, the complex network of air traffic is more vulnerable to disturbances. When it is disturbed, the failure of some nodes spreads through dependency relationships in the network, resulting in cascade failure. In the event of a cascade failure, the network may quickly collapse until it is paralyzed, with widespread delays and flight cancellations. The current flow management and deployment methods still remain in the control-oriented stage, which is mainly completed by air traffic controls (ATCs), and lack of accurate flow adjustment and effective utilization of capacity. The whole air traffic system and its peripheral factors are intricate, so human and social factors must be integrated into the control and decision-making of the system. Considering engineering and social factors such as operation environment, social environment, personnel, rules, equipment, and information processing, we analyse the air traffic in a cyber-physical-social system (CPSS). To reflect the actual system behaviour rules, dynamic response, limit state, and so on, the corresponding computational experiment and comprehensive evaluation system are established. Based on neural networks and other technologies, a resource prediction scheme based on task demand is proposed for multi-dimensional resources such as airports, air routes, and ATC, to reduce the cost of system resource scheduling and improve resource utilization through resource prediction and adjustment. Finally, the accuracy of the proposed resource prediction algorithm is verified by theoretical analysis and simulation.}
}
@article{BOGGS19831,
title = {The integration of structure determination by computation, electron diffraction and microwave spectroscopy},
journal = {Journal of Molecular Structure},
volume = {97},
pages = {1-16},
year = {1983},
note = {Determination of Molecular Structure by Microwave Spectroscopy and Electron Diffraction},
issn = {0022-2860},
doi = {https://doi.org/10.1016/0022-2860(83)90171-0},
url = {https://www.sciencedirect.com/science/article/pii/0022286083901710},
author = {James E. Boggs},
abstract = {The history of the interaction between experimental structure determinations by microwave spectroscopy and by gas phase electron diffraction is briefly reviewed in terms of three eras: (1) competition and antagonism, (2) comparison and correction, and (3) integration of analysis. A similar progression is noted for the relation between experimental and theoretical methods for studying molecular structure, with the present time straddling ages (2) and (3). Examples are given from a variety of studies involving various degrees of methodological interaction. The true integration of experimental and computational structural studies is still in its infancy with the primary illustrations involving the evaluation of theoretical structural offset values from experimental evidence, the transfer of theoretically determined parameters into the fitting of experimental data, and the current development of methods for utilizing vibrational information obtained from the combined analysis of computed theoretical and experimental infrared data in the further analysis of experimental diffraction and microwave information.}
}
@article{PALITTA2023112068,
title = {Stein-based preconditioners for weak-constraint 4D-var},
journal = {Journal of Computational Physics},
volume = {482},
pages = {112068},
year = {2023},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2023.112068},
url = {https://www.sciencedirect.com/science/article/pii/S0021999123001638},
author = {Davide Palitta and Jemima M. Tabeart},
keywords = {4D-var, Data assimilation, Preconditioning, Stein equations},
abstract = {Algorithms for data assimilation try to predict the most likely state of a dynamical system by combining information from observations and prior models. Variational approaches, such as the weak-constraint four-dimensional variational data assimilation formulation considered in this paper, can ultimately be interpreted as a minimization problem. One of the main challenges of such a formulation is the solution of large linear systems of equations which arise within the inner linear step of the adopted nonlinear solver. Depending on the selected approach, these linear algebraic problems amount to either a saddle point linear system or a symmetric positive definite (SPD) one. Both formulations can be solved by means of a Krylov method, like GMRES or CG, that needs to be preconditioned to ensure fast convergence in terms of the number of iterations. In this paper we illustrate novel, efficient preconditioning operators which involve the solution of certain Stein matrix equations. In addition to achieving better computational performance, the latter machinery allows us to derive tighter bounds for the eigenvalue distribution of the preconditioned linear system for certain problem settings. A panel of diverse numerical results displays the effectiveness of the proposed methodology compared to current state-of-the-art approaches.}
}
@article{ZENG2019138,
title = {Iterative optimal control syntheses illustrated on the Brockett integrator},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {16},
pages = {138-143},
year = {2019},
note = {11th IFAC Symposium on Nonlinear Control Systems NOLCOS 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.11.768},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319317719},
author = {Shen Zeng},
keywords = {Nonholonomic systems, Optimal control, Computational methods},
abstract = {In this paper, we investigate computational methods for synthesizing optimal control inputs for nonholonomic control systems. We use the Brockett integrator as a benchmark example to illustrate different aspects of the computational trajectory optimization problem. The main result of this paper is the establishment of a rather attractive iterative scheme for practically constructing optimal control inputs. The functionality and efficiency of the proposed iterative scheme are illustrated on different optimal steering problems centered around the Brockett integrator.}
}
@article{MCINTOSH1988213,
title = {A computational tutor for architectural design},
journal = {Computers, Environment and Urban Systems},
volume = {12},
number = {4},
pages = {213-219},
year = {1988},
issn = {0198-9715},
doi = {https://doi.org/10.1016/0198-9715(88)90028-2},
url = {https://www.sciencedirect.com/science/article/pii/0198971588900282},
author = {Patricia G. McIntosh},
abstract = {The conceptual basis for a computational design tutoring environment whose purpose is to aid student designers in the search for design solutions is presented. Problems that students experience in the course of their design education are presented. These problems are related to previous work in the development of computable representations of designs. A specification for a computational learning environment for design students is outlined.}
}
@article{BESTER2024820,
title = {Complementary conservation of South African crop wild relatives for plant improvement},
journal = {South African Journal of Botany},
volume = {174},
pages = {820-829},
year = {2024},
issn = {0254-6299},
doi = {https://doi.org/10.1016/j.sajb.2024.09.041},
url = {https://www.sciencedirect.com/science/article/pii/S0254629924005945},
author = {C Bester and NC {Le Maitre} and M Visser and WC Botes},
keywords = {Crop wild relatives (CWR), Complementary conservation, Plant breeding, Southern African development community (SADC), CAPFITOGEN},
abstract = {Crop Wild Relatives (CWR) are good sources of unexplored genetic diversity that can assist plant breeders to increase the yield and resilience of their crops. These species are valuable plant genetic resources (PGR) that have been used in more than 4,157 documented cases of plant improvement to date. South Africa has 258 prioritized CWR, selected based on their distribution, threat status and potential as gene donors. In light of ongoing habitat destruction, global warming and mismanagement of resources, the conservation of these PGR is vital. Complementary conservation approaches allow for the continuous development of CWR, while harnessing and applying the available diversity in plant breeding programs. The South African National Biodiversity Strategy and Action Plan (NBSAP) strives to utilize conservation resources to build and maintain an effective complementary, in situ to ex situ conservation pipeline. As part of the Southern African Development Community (SADC), South Africa has access to numerous resources that can assist to protect its rich floral diversity, including the SADC Plant Genetic Resource Centre (SPGRC), the SADC CWR Project and CAPFITOGEN3.}
}
@article{KULIHA2024161,
title = {Secure internet of medical things based electronic health records scheme in trust decentralized loop federated learning consensus blockchain},
journal = {International Journal of Intelligent Networks},
volume = {5},
pages = {161-174},
year = {2024},
issn = {2666-6030},
doi = {https://doi.org/10.1016/j.ijin.2024.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666603024000162},
author = {Megha Kuliha and Sunita Verma},
keywords = {Blockchain, Electronic health records, Federated learning, Healthcare monitoring, Internet of medical things, Security, Privacy, Health monitoring systems, Normalization, MIMIC-III},
abstract = {Electronic Health Records (EHRs) have become an increasingly significant source of information for healthcare professionals and researchers. Two technical challenges are addressed: motivating federated learning members to contribute their time and effort, and ensuring accurate aggregation of the global model by the centralized federated learning server. To overcome these issues and establish a decentralized solution, the integration of blockchain and federated learning proves effective, offering enhanced security and privacy for smart healthcare. The proposed approach includes a gamified element to incentivize and recognize contributions from federated learning members. This research work offers a solution involving resource management within the Internet of Medical Things (IoMT) using a newly proposed trust decentralized loop federated learning consensus blockchain. The obtained raw data is pre-processed by using handling missing values and adaptive min-max normalization. The appropriate features are selected with the aid of hybrid weighted-leader exponential distribution optimization algorithm. Because, data with multiple features exhibits varying levels of variation across each feature. The selected features are then forwarded to the training phase through the proposed pyramid squeeze attention generative adversarial networks to classify the EHR as positive and negative. The proposed classification model demonstrates high flexibility and scalability, making it applicable to a wide range of network architectures for various computer vision tasks. The introduced model provides better outcomes in terms of 98.5% in the training accuracy and 99% in the validation accuracy over Medical Information Mart for Intensive Care III (MIMIC-III) dataset, which is more efficient than the other traditional methods.}
}
@article{MOLINS2022113953,
title = {Stressed individuals exhibit pessimistic bursting beliefs and a lower risk preference in the balloon analogue risk task},
journal = {Physiology & Behavior},
volume = {256},
pages = {113953},
year = {2022},
issn = {0031-9384},
doi = {https://doi.org/10.1016/j.physbeh.2022.113953},
url = {https://www.sciencedirect.com/science/article/pii/S0031938422002591},
author = {Francisco Molins and Mónica Paz and Liza Rozman and Nour {Ben Hassen} and Miguel Ángel Serrano},
keywords = {Decision-making, Balloon analogue risk task, Computational modelling, Stress, Trier social stress test},
abstract = {Stress alters decision-making by usually promoting risk-taking and reward-seeking, which could be advantageous in a context where risk is rewarded, such as the Balloon Analogue Risk Task (BART). However, previous studies addressing this issue showed inconsistencies which could emerge from assessing decision-making as a single dimension. Our aim is to study through computational modelling how stress influences cognitive subprocesses of the decision-making during the BART. For this purpose, 94 healthy participants were submitted to BART, but only half were exposed to the virtual Trier Social Stress Test (TSST-VR). The Experimental-Weight Mean-Variance (EWMV) model was used to gain insight into the subprocesses involved in risk-taking during BART. Rather than reward-seeking, our results showed a pessimistic prior belief about the balloons bursting likelihood, and a lower risk preference in the stressed participants. This cautious attitude could be attributable to an alertness state promoted by stress. Yet, since risk is rewarded in BART, it could also evidence a maladaptive decision-making derived from learning difficulties and altered feedback-processing under stress.}
}
@article{LIU2024110132,
title = {Multimodal brain-controlled system for rehabilitation training: Combining asynchronous online brain–computer interface and exoskeleton},
journal = {Journal of Neuroscience Methods},
volume = {406},
pages = {110132},
year = {2024},
issn = {0165-0270},
doi = {https://doi.org/10.1016/j.jneumeth.2024.110132},
url = {https://www.sciencedirect.com/science/article/pii/S0165027024000773},
author = {Lei Liu and Jian Li and Rui Ouyang and Danya Zhou and Cunhang Fan and Wen Liang and Fan Li and Zhao Lv and Xiaopei Wu},
keywords = {Movement impairment, Rehabilitation, Brain–computer interface, Motor imagery, Steady-state visual evoked potential},
abstract = {Background:
Traditional therapist-based rehabilitation training for patients with movement impairment is laborious and expensive. In order to reduce the cost and improve the treatment effect of rehabilitation, many methods based on human–computer interaction (HCI) technology have been proposed, such as robot-assisted therapy and functional electrical stimulation (FES). However, due to the lack of active participation of brain, these methods have limited effects on the promotion of damaged nerve remodeling.
New method:
Based on the neurofeedback training provided by the combination of brain–computer interface (BCI) and exoskeleton, this paper proposes a multimodal brain-controlled active rehabilitation system to help improve limb function. The joint control mode of steady-state visual evoked potential (SSVEP) and motor imagery (MI) is adopted to achieve self-paced control and thus maximize the degree of brain involvement, and a requirement selection function based on SSVEP design is added to facilitate communication with aphasia patients.
Comparison with existing methods:
In addition, the Transformer is introduced as the MI decoder in the asynchronous online BCI to improve the global perception of electroencephalogram (EEG) signals and maintain the sensitivity and efficiency of the system.
Results:
In two multi-task online experiments for left hand, right hand, foot and idle states, subject achieves 91.25% and 92.50% best accuracy, respectively.
Conclusion:
Compared with previous studies, this paper aims to establish a high-performance and low-latency brain-controlled rehabilitation system, and provide an independent and autonomous control mode of the brain, so as to improve the effect of neural remodeling. The performance of the proposed method is evaluated through offline and online experiments.}
}
@article{GINEITYTE1999205,
title = {On the future of the Hückel model},
journal = {Journal of Molecular Structure: THEOCHEM},
volume = {491},
number = {1},
pages = {205-209},
year = {1999},
issn = {0166-1280},
doi = {https://doi.org/10.1016/S0166-1280(99)00116-5},
url = {https://www.sciencedirect.com/science/article/pii/S0166128099001165},
author = {V. Gineityte},
keywords = {Basis orbitals, Hückel model, Hamiltonian matrix},
abstract = {In an attempt to foresee the prospects of the qualitative trend in quantum chemistry, the place of the Hückel model is analyzed in the broad context of quantum mechanical and chemical perspectives on the molecular world. Quantum mechanics and chemistry are considered as complementary approaches to molecular structure and properties that are irreducible one to another. Arguments are given for the hypothesis that the Hückel model makes a separate level of investigation of molecules situated in between quantum mechanics and chemistry. In this context, the need is emphasized for development of new concepts immanent in the very Hückel model. These concepts are anticipated to play the role of terms for qualitative orbital thinking, the persistent need for which was emphasized recently (R. Hoffmann, J. Mol. Struct. (Theochem), 424 (1998) 1).}
}
@article{ZHAO2013278,
title = {An intelligent chiller fault detection and diagnosis methodology using Bayesian belief network},
journal = {Energy and Buildings},
volume = {57},
pages = {278-288},
year = {2013},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2012.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0378778812005968},
author = {Yang Zhao and Fu Xiao and Shengwei Wang},
keywords = {Fault detection, Fault diagnosis, Centrifugal chiller, Bayesian network},
abstract = {A generic intelligent fault detection and diagnosis (FDD) strategy is proposed in this study to simulate the actual diagnostic thinking of chiller experts. A three-layer Diagnostic Bayesian Network (DBN) is developed to diagnose chiller faults based on the Bayesian Belief Network (BBN) theory. The structure of the DBN is a graphical and qualitative illustration of the intrinsic causal relationships among causal factors in Layer 1, faults in Layer 2 and fault symptoms in Layer 3. The parameters of the DBN represent the quantitative probabilistic relationships among the three layers. To diagnose chiller faults, posterior probabilities of the faults under observed evidences are calculated based on the probability analysis and the graph theory. Compared with other FDD strategies, the proposed strategy can make use of more useful information of the chiller concerned and expert knowledge. It is effective and efficient in diagnosing faults based on uncertain, incomplete and conflicting information. Evaluation of the strategy was made on a 90-ton water-cooled centrifugal chiller reported in ASHRAE RP-1043.}
}
@article{LOWRIE20112244,
title = {Gender differences in students’ mathematics game playing},
journal = {Computers & Education},
volume = {57},
number = {4},
pages = {2244-2248},
year = {2011},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2011.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S0360131511001394},
author = {Tom Lowrie and Robyn Jorgensen},
keywords = {Gender studies, Elementary education, Pedagogical issues, Numeracy practices},
abstract = {The investigation monitored the digital game-playing behaviours of 428 primary-aged students (aged 10–12 years). Chi-square analysis revealed that boys tend to spend more time playing digital games than girls while boys and girls play quite different game genres. Subsequent analysis revealed statistically significant gender differences in terms of the types of mathematics-rich games students prefer to play. Girls preferred to play games that required problem solving, quantitative computations and the interpretation of graphs. Boys preferred games that required visual/spatial engagement. Given the fact that boys outperform girls on spatial tasks and mathematics assessment items that contain graphics, this study has implications for the development of students' mathematics sense making.}
}
@article{LEIVANT198651,
title = {Typing and computational properties of lambda expressions},
journal = {Theoretical Computer Science},
volume = {44},
pages = {51-68},
year = {1986},
issn = {0304-3975},
doi = {https://doi.org/10.1016/0304-3975(86)90109-X},
url = {https://www.sciencedirect.com/science/article/pii/030439758690109X},
author = {Daniel Leivant},
abstract = {We use a perception of second-order typing in the λ-Calculus, as conveying semantic properties of expressions in models over λ-expressions, to exhibit natural and uniform proofs of theorems of Girard (1971/1972) and of Coppo, Dezani and Veneri (1981) about the relations between typing properties and computational properties of λ-expressions (solvability, normalizability, strong normalizability), and of some generalizations of these theorems.}
}
@article{LEVESQUE201427,
title = {On our best behaviour},
journal = {Artificial Intelligence},
volume = {212},
pages = {27-35},
year = {2014},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2014.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0004370214000356},
author = {Hector J. Levesque},
keywords = {IJCAI Research Excellence},
abstract = {The science of AI is concerned with the study of intelligent forms of behaviour in computational terms. But what does it tell us when a good semblance of a behaviour can be achieved using cheap tricks that seem to have little to do with what we intuitively imagine intelligence to be? Are these intuitions wrong, and is intelligence really just a bag of tricks? Or are the philosophers right, and is a behavioural understanding of intelligence simply too weak? I think both of these are wrong. I suggest in the context of question-answering that what matters when it comes to the science of AI is not a good semblance of intelligent behaviour at all, but the behaviour itself, what it depends on, and how it can be achieved. I go on to discuss two major hurdles that I believe will need to be cleared.}
}
@article{BOEING2021102013,
title = {Spatial information and the legibility of urban form: Big data in urban morphology},
journal = {International Journal of Information Management},
volume = {56},
pages = {102013},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0268401219302154},
author = {Geoff Boeing},
keywords = {OpenStreetMap, Urban design, Urban form, Urban morphology, Urban planning, Visualization},
abstract = {Urban planning and morphology have relied on analytical cartography and visual communication tools for centuries to illustrate spatial patterns, conceptualize proposed designs, compare alternatives, and engage the public. Classic urban form visualizations – from Giambattista Nolli’s ichnographic maps of Rome to Allan Jacobs’s figure-ground diagrams of city streets – have compressed physical urban complexity into easily comprehensible information artifacts. Today we can enhance these traditional workflows through the Smart Cities paradigm of understanding cities via user-generated content and harvested data in an information management context. New spatial technology platforms and big data offer new lenses to understand, evaluate, monitor, and manage urban form and evolution. This paper builds on the theoretical framework of visual cultures in urban planning and morphology to introduce and situate computational data science processes for exploring urban fabric patterns and spatial order. It demonstrates these workflows with OSMnx and data from OpenStreetMap, a collaborative spatial information system and mapping platform, to examine street network patterns, orientations, and configurations in different study sites around the world, considering what these reveal about the urban fabric. The age of ubiquitous urban data and computational toolkits opens up a new era of worldwide urban form analysis from integrated quantitative and qualitative perspectives.}
}
@article{BALKHI20051223,
title = {Proteomics of Acute Myeloid Leukemia: Cytogenetic Risk Groups Differ Specifically in Their Proteome, Interactome and Posttranslational Protein Modifications.},
journal = {Blood},
volume = {106},
number = {11},
pages = {1223},
year = {2005},
issn = {0006-4971},
doi = {https://doi.org/10.1182/blood.V106.11.1223.1223},
url = {https://www.sciencedirect.com/science/article/pii/S0006497119761138},
author = {Mumtaz Y. Balkhi and Mulu Geletu and Maximilian Christopeit and Hermann M. Behre and Gerhard Behre},
abstract = {Acute Myeloid Leukemia (AML) is characterized by specific cytogenetic aberrations which are strong determinants of prognostic outcome and therapeutic response. Because the clinical outcome in AML cytogenetic groups differs considerably, we hypothesized that cytogenetic risk groups of AML might differ specifically in their proteome, protein interaction pathways and posttranslational modifications (PTMs). Thus, we determined the proteome of 30 AML patients belonging to various cytogenetic groups based on two-dimensional gel electrophoresis and Nano LC coupled MALDI-TOF-TOF tandem mass spectrometry. We could identify substantial differences in the proteome, protein expression and peak pattern between cytogenetic risk groups of AML. The interactome analysis based on computational bioinformatics using Ingenuity analysis revealed major regulating networks: MAPK8 and MYC for complex aberrant karyotype AML, TP53 for t(8;21)-AML, TP53- MYC- PRKAC for 11q23-AML, JUN and MYC for inv(16)-AML. Most interestingly, peak explorer analysis revealed a modification of O-linked acetyl glucosamine of hnRNPH1 in AML patients with a 11q23 translocation, an acetylation of calreticulin in t(8;21) translocation AML, an increased intensity of dimethylated peptide of hnRNPA2/B1 in AML patients with translocations of t(8;21) and inv(16) in comparison to healthy bone marrow. We show for the first time that cytogenetic risk groups of AML differ specifically both in their proteome, interactome and PTMs. These findings lead to a new thinking about the pathogenesis of AML and has major therapeutic implications because PTMs are the primary drug targets.}
}
@article{HALL198939,
title = {Computational approaches to analogical reasoning: A comparative analysis},
journal = {Artificial Intelligence},
volume = {39},
number = {1},
pages = {39-120},
year = {1989},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(89)90003-9},
url = {https://www.sciencedirect.com/science/article/pii/0004370289900039},
author = {Rogers P. Hall},
abstract = {Analogical reasoning has a long history in artificial intelligence research, primarily because of its promise for the acquisition and effective use of knowledge. Defined as a representational mapping from a known “source” domain into a novel “target” domain, analogy provides a basic mechanism for effectively connecting a reasoner's past and present experience. Using a four-component process model of analogical reasoning, this paper reviews sixteen computational studies of analogy. These studies are organized chronologically within broadly defined task domains of automated deduction, problem solving and planning, natural language comprehension, and machine learning. Drawing on these detailed reviews, a comparative analysis of diverse contributions to basic analogy processes identifies recurrent problems for studies of analogy and common approaches to their solution. The paper concludes by arguing that computational studies of analogy are in a state of adolescence: looking to more mature research areas in artificial intelligence for robust accounts of basic reasoning processes and drawing upon a long tradition of research in other disciplines.}
}
@article{GOLCUK2022159,
title = {An interval type-2 fuzzy axiomatic design method: A case study for evaluating blockchain deployment projects in supply chain},
journal = {Information Sciences},
volume = {602},
pages = {159-183},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.04.034},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522003802},
author = {İlker Gölcük},
keywords = {Blockchain technology, Fuzzy subsethood, Axiomatic design, Best-worst method, Interval type-2 fuzzy sets},
abstract = {This study is concerned with the development of the axiomatic design (AD) method under an interval type-2 fuzzy (IT2F) environment and its application in evaluating blockchain deployment projects in supply chains. Blockchain is a transformative technology that has received significant attention recently. Blockchain technology can process various business transactions by offering a reliable and decentralized infrastructure. Supply chain management is an important application area of blockchains due to its desirable properties, including data security, extended visibility, product traceability, digitalization, and disintermediation. Since blockchain technologies are in their infancy, adopting them to supply chains requires proper design methodologies. Fuzzy AD offers valuable computational mechanisms to evaluate design options in the presence of functional requirements. However, extending AD to different fuzzy extensions is not an easy task, and area-based calculations hinder its widespread applicability. In this study, an IT2F-AD method is developed based on the concept of fuzzy subsethood. The potential of the fuzzy subsethood measure as the main computation engine within type-1 and IT2F-AD is demonstrated. Finally, an integrated multiple criteria decision-making (MCDM) model is proposed by using IT2F Best-Worst Method (IT2F-BWM) and IT2F-AD. The proposed model is used to prioritize blockchain deployment projects in a real-life case study.}
}
@article{TAYLOR1999943,
title = {Towards the networks of the brain: from brain imaging to consciousness},
journal = {Neural Networks},
volume = {12},
number = {7},
pages = {943-959},
year = {1999},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(99)00044-1},
url = {https://www.sciencedirect.com/science/article/pii/S0893608099000441},
author = {J.G. Taylor},
keywords = {Brain imaging, Consciousness, Structural modelling, Motion after-effect, Planning, Thinking, Self},
abstract = {The manner in which the brain computes in various tasks is being probed at a deep level by modern brain imaging techniques, with an increasing appreciation of the different networks being used to solve these tasks. There is simultaneously developing a neural modelling technology, which attempts to explain the underlying computations being performed by this set of networks. This paper describes results from brain imaging and how they may be related to the underlying neural networks by means of structural modelling. It thereby attempts to give an initial glimpse of the emerging picture of the functionality of brain networks. It concludes with a discussion of the role of consciousness in global processing, and how particular styles of neural processing can attain this.}
}
@article{PALMER2024110848,
title = {Assessing between-individual variability in bioenergetics modelling: Opportunities, challenges, and potential applications},
journal = {Ecological Modelling},
volume = {498},
pages = {110848},
year = {2024},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2024.110848},
url = {https://www.sciencedirect.com/science/article/pii/S0304380024002369},
author = {Miquel Palmer and Irene Moro-Martínez and Joaquim Tomàs-Ferrer and Amalia Grau and María Dolores López-Belluga and Marine Herlin and Orestis Stavrakidis-Zachou and Andrea Campos-Candela},
keywords = {Between-individual differences, Dynamic Energy Budget, Bayesian},
abstract = {Population dynamics is influenced by between-individual variability. Dynamic Energy Budget (DEB) theory is an appealing framework for assessing such a variability, yet DEB parameters have rarely been estimated at the individual level. Bayesian hierarchical models show promise for inferring individual variability in DEB parameters, thought computational challenges have limited their use due to the need to solve differential equations. Timely, Stan has emerged as a general-purpose statistical tool for fitting dynamic models. This paper introduces an analytical strategy using Bayesian parametric inference and hierarchical modelling to estimate individual-specific DEB parameters. Two biologically relevant DEB parameters were successfully estimated for 69 Gilt-head breams (Sparus aurata) with up to 11 measures of length and wet weight each. The estimated between-individual variability in these two DEB parameters explained well the observed patterns in length and weight at between- and within-individual levels. Moreover, data-simulation experiments highlighted the potential and limitations of our approach, suggesting that improved data collection could enable to increase precision and the number of DEB parameters that can be estimated at the individual level. This strategy can better represent between-individual variability in DEB parameters, which ultimately may improve forecasting of population dynamics after integrating DEB into population models.}
}
@article{BEZERRAFEITOSA2024843,
title = {Multiple criteria evaluation of hydrogen production processes for use in automotive sector},
journal = {International Journal of Hydrogen Energy},
volume = {49},
pages = {843-861},
year = {2024},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2023.09.232},
url = {https://www.sciencedirect.com/science/article/pii/S0360319923048784},
author = {Francisco Edvan {Bezerra Feitosa} and Antonella Lombardi Costa},
keywords = {Hydrogen, Gasification, Steam reforming, Electrolysis, Solar and nuclear thermochemical, MACBETH},
abstract = {This work evaluates large-scale hydrogen production processes - those capable of providing hydrogen for an eventual automotive hydrogen program in a community and along highways - with the aim to Sidentify which of the production processes of hydrogen is more attractive for an eventual automotive hydrogen program. To perform the research, the MACBETH (Measuring Attractiveness by the Category-Based Assessment Technique) method and the computational code M-MACBETH 3.2.0 are used taking into account multiple criteria. Thus, this work used the computational code M-MACBETH 3.2.0 to evaluate twelve hydrogen production processes and build rankings of attractiveness of hydrogen production processes, considering the following criteria: economic (invested capital and production cost), technical (the purity of the hydrogen produced) and environmental (the amount of energy used to produce 1 kg of hydrogen and the CO2 emissions to produce 1 kg of hydrogen). In the end, three rankings are produced considering three scenarios, which allow to conclude that: if decisions regarding hydrogen are made taking into account that economic and financial aspects have the same weight as sustainability aspects, the more attractive hydrogen plants are those that use the Solar Thermochemical Plants; if decisions are made taking into account that economic and financial aspects are more important than aspects of sustainability, the production process more attractive will be the one that uses the SR technology - Ethanol and Natural Gas Steam Reforming; and that Solar Thermochemical Plants becomes more attractive than all others alternatives when, in decision-making, sustainability environmental aspects prevail over economic and financial aspects.}
}
@article{ROBSON2014287,
title = {When do aquatic systems models provide useful predictions, what is changing, and what is next?},
journal = {Environmental Modelling & Software},
volume = {61},
pages = {287-296},
year = {2014},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2014.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S1364815214000188},
author = {Barbara J. Robson},
keywords = {Modelling philosophy, Biogeochemical modelling, Ecological models, Developments, Progress, Knowledge gaps},
abstract = {This article considers how aquatic systems modelling has changed since 1995 and how it must change in future if we are to continue to advance. A distinction is made between mechanistic and statistical models, and the relative merits of each are considered. The question of “when do aquatic systems models provide accurate and useful predictions?” is addressed, implying some guidelines for model development. It is proposed that, in general, ecological models only provide management-relevant predictions of the behaviour of real systems when there are strong physical (as opposed to chemical or ecological) drivers. Developments over the past 15 years have included changes in technology, changes in the modelling community and changes in the context in which modelling is conducted: the implications of each are briefly discussed. Current trends include increased uptake of best practice guidelines, increasing integration of models, operationalisation, data assimilation, development of improved tools for skill assessment, and application of models to new management questions and in new social contexts. Deeper merging of statistical and mechanistic modelling approaches through such techniques as Bayesian Melding, Bayesian Hierarchical Modelling and surrogate modelling is identified as a key emerging area. Finally, it is suggested that there is a need to systematically identify areas in which our current models are inadequate. We do not yet know for which categories of problems well-implemented aquatic systems models can (or cannot) be expected to accurately predict observational data and system behaviour. This can be addressed through better modelling and publishing practices.}
}
@article{LEUKHIN2018300,
title = {Bio-plausible simulation of three monoamine systems to replicate emotional phenomena in a machine},
journal = {Procedia Computer Science},
volume = {145},
pages = {300-305},
year = {2018},
note = {Postproceedings of the 9th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2018 (Ninth Annual Meeting of the BICA Society), held August 22-24, 2018 in Prague, Czech Republic},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.11.075},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918323639},
author = {Alexey Leukhin and Max Talanov and Jordi Vallverdú and Fail Gafarov},
keywords = {affective computing, affective computation, spiking neural networks, bio-inspired cognitive architecture},
abstract = {In this paper we present the validation of the three-dimensional model of emotions by Hugo Lövheim the “cube of emotion” via neurosimulation in the NEST. We also present the extension of original “cube of emotion” with the bridge to computational processes parameters. The neurosimulation is done via re-implementation of dopamine (DA), serotonin (5-HT) and noradrenaline (NA) subsystems of a rat brain to replicate 8 basic psycho-emotional states according to the “cube of emotion”. Results of neu-rosimulations indicate the incremental influence of DA and NA over computational resources of a psycho-emotional state while 5-HT decreases the computational resources used to calculate a psycho-emotional state. This way we indicate the feasibility of the bio-plausible re-implementation of psycho-emotional states in a computational system. This approach could be useful extension of decision making and load balancing components of modern artificial agents as well as intelligent robotic systems.}
}
@article{DESCIOLI2011204,
title = {The omission effect in moral cognition: toward a functional explanation},
journal = {Evolution and Human Behavior},
volume = {32},
number = {3},
pages = {204-215},
year = {2011},
issn = {1090-5138},
doi = {https://doi.org/10.1016/j.evolhumbehav.2011.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S1090513811000055},
author = {Peter DeScioli and Rebecca Bruening and Robert Kurzban},
keywords = {Omission, Transparency, Moral judgment, Moral psychology},
abstract = {Moral judgment involves much more than computations of the expected consequences of behavior. A prime example of the complexity of moral thinking is the frequently replicated finding that violations by omission are judged less morally wrong than violations by commission, holding intentions constant. Here we test a novel hypothesis: Omissions are judged less harshly because they produce little material evidence of wrongdoing. Evidence is crucial because moral accusations are potentially very costly unless supported by others. In our experiments, the omission effect was eliminated when physical evidence showed that an omission was chosen. Perpetrators who “opted out” by pressing a button that would clearly have no causal effects on the victim, rather than rescuing them, were judged as harshly as perpetrators who directly caused death. These results show that, to reduce condemnation, omissions must not only be noncausal, they must also leave little or no material evidence that a choice was made.}
}
@article{ZHANG202340,
title = {MCHA-Net: A multi-end composite higher-order attention network guided with hierarchical supervised signal for high-resolution remote sensing image change detection},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {202},
pages = {40-68},
year = {2023},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2023.05.033},
url = {https://www.sciencedirect.com/science/article/pii/S0924271623001570},
author = {Haiming Zhang and Guorui Ma and Yongxian Zhang and Bin Wang and Heng Li and Lunjun Fan},
keywords = {Change detection, Higher-order attention, Multi-end network, High-resolution remote sensing image, Hierarchical supervision},
abstract = {Change detection (CD) is the main way to detect changes in the Earth’s surface features in a timely and accurate manner and to understand the interactions between humans and nature, CD is also an important scientific tool for supporting decision-making. Many convolution-based methods and Transformer-based methods have gained remarkable success in the field of CD with high-resolution remote sensing images (HRSIs) due to their powerful feature extraction capability and global information modeling capability, respectively. However, the diversity and complexity of HRSIs render CD methods constantly challenging, e.g., off-nadir angle imaging, seasonal turnover, and simultaneous changes in multiple feature types. Common convolution-based or Transformer-based encoding–decoding networks have a single way of data modeling and a low degree of feature fusion, resulting in poor applicability to different remote sensing data, poor recognition of different semantic targets, and limited accuracy. To improve the generalizability and detection accuracy of the network, we developed a composite higher-order attention network with multiple encoding paths named MCHA-Net. MCHA-Net has four encoding backbones, namely, the Siamese-learning path, Residual-learning path, and Transformer-learning path. Different encoding ways are equivalent to different thinking ends, and the integration can make the network have a stronger feature representation capability, forming a local–global-cross domain data modeling approach and making the network have powerful data sensing and mining capability. The decoding end aggregates the semantic information of each layer and integrates them into a unified linearized feature mapping module to achieve full modeling of the separability of information in the target domain. In addition, we propose a new higher-order attention mechanism to perform adaptive feature refinement for each layer of each encoding end, to guide the network in focusing on the targeted region and to guarantee the boundary integrity and internal compactness of the detection results. Moreover, we design a hierarchical network supervision schema that adds supervision signals at different feature abstraction levels to impose differentiated soft constraints on each layer of the network, ensuring high semantic consistency of features across layers and facilitating fast network fitting. Experimental results on three benchmark datasets (S2Looking, SVCD, and SYSU-CD) and one transfer application dataset (Google Dataset) show that MCHA-Net outperforms state-of-the-art methods in both visual interpretation and quantitative evaluation, and exhibits strong generalization and robustness against few-shot learning.}
}
@incollection{HUANG2006586,
title = {Neo-Gricean Pragmatics},
editor = {Keith Brown},
booktitle = {Encyclopedia of Language & Linguistics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {586-590},
year = {2006},
isbn = {978-0-08-044854-1},
doi = {https://doi.org/10.1016/B0-08-044854-2/04529-6},
url = {https://www.sciencedirect.com/science/article/pii/B0080448542045296},
author = {Y. Huang},
keywords = {classical Gricean pragmatics, conversational implicature, division of pragmatic labor, explicature, Grice's circle, Horn-scales, I-impliciture, implicature, interaction between Q-, I, and M-implicatures, maxims, M-implicature, neo-Gricean pragmatics, pragmatic intrusion, pragmatics, presumptive meaning, Q-implicature, R-implicature},
abstract = {Since its inception, Gricean pragmatics has revolutionized pragmatic theorizing and has to date remained one of the cornerstones of contemporary thinking in linguistic pragmatics and the philosophy of language. This article undertakes to present and assess a neo-Gricean pragmatic theory of conversational implicature, focusing on the bipartite model developed by Laurence Horn and the tripartite model advanced by Stephen Levinson.}
}
@article{HAUSER201778,
title = {The Universal Generative Faculty: The source of our expressive power in language, mathematics, morality, and music},
journal = {Journal of Neurolinguistics},
volume = {43},
pages = {78-94},
year = {2017},
note = {Language Evolution: On the Origin of Lexical and Syntactic Structures},
issn = {0911-6044},
doi = {https://doi.org/10.1016/j.jneuroling.2016.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0911604416300811},
author = {Marc D. Hauser and Jeffrey Watumull},
keywords = {Domain-specificity, Evolution, Generative functions, Language faculty, Recursion, Turing machine, Universal generative faculty},
abstract = {Many have argued that the expressive power of human thought comes from language. Language plays this role, so the argument goes, because its generative computations construct hierarchically structured, abstract representations, covering virtually any content and communicated in linguistic expressions. However, language is not the only domain to implement generative computations and abstract representations, and linguistic communication is not the only medium of expression. Mathematics, morality, and music are three others. These similarities are not, we argue, accidental. Rather, we suggest they derive from a common computational system that we call the Universal Generative Faculty or UGF. UGF is, at its core, a suite of contentless generative procedures that interface with different domains of knowledge to create contentful expressions in thought and action. The representational signatures of different domains are organized and synthesized by UGF into a global system of thought. What was once considered the language of thought is, on our view, the more specific operation of UGF and its interfaces to different conceptual domains. This view of the mind changes the conversation about domain-specificity, evolution, and development. On domain-specificity, we suggest that if UGF provides the generative engine for different domains of human knowledge, then the specificity of a given domain (e.g., language, mathematics, music, morality) is restricted to its repository of primitive representations and to its interfaces with UGF. Evolutionarily, some generative computations are shared with other animals (e.g., combinatorics), both for recognition-learning and generation-production, whereas others are uniquely human (e.g., recursion); in some cases, the cross-species parallels may be restricted to recognition-learning, with no observable evidence of generation-production. Further, many of the differences observed between humans and other animals, as well as among nonhuman animals, are the result of differences in the interfaces: whereas humans promiscuously traverse (consciously and unconsciously) interface conditions so as to combine and analogize concepts across many domains, nonhuman animals are far more limited, often restricted to a specific domain as well as a specific sensory modality within the domain. Developmentally, the UGF perspective may help explain why the generative powers of different domains appear at different stages of development. In particular, because UGF must interface with domain-specific representations, which develop on different time scales, the generative power of some domains may mature more slowly (e.g., mathematics) than others (e.g., language). This explanation may also contribute to a deeper understanding of cross-cultural differences among human populations, especially cases where the generative power of a domain appears absent (e.g., cultures with only a few count words). This essay provides an introduction to these ideas, including a discussion of implications and applications for evolutionary biology, human cognitive development, cross-cultural variation, and artificial intelligence.}
}
@article{VUNJAKNOVAKOVIC20214597,
title = {Organs-on-a-chip models for biological research},
journal = {Cell},
volume = {184},
number = {18},
pages = {4597-4611},
year = {2021},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2021.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0092867421009478},
author = {Gordana Vunjak-Novakovic and Kacey Ronaldson-Bouchard and Milica Radisic},
abstract = {Summary
We explore the utility of bioengineered human tissues—individually or connected into physiological units—for biological research. While much smaller and simpler than their native counterparts, these tissues are complex enough to approximate distinct tissue phenotypes: molecular, structural, and functional. Unlike organoids, which form spontaneously and recapitulate development, “organs-on-a-chip” are engineered to display some specific functions of whole organs. Looking back, we discuss the key developments of this emerging technology. Thinking forward, we focus on the challenges faced to fully establish, validate, and utilize the fidelity of these models for biological research.}
}
@article{LUDGERIO2023e10,
title = {Pedagogical practices developed with children through hospital classes: An integrative literature review},
journal = {Journal of Pediatric Nursing},
volume = {72},
pages = {e10-e18},
year = {2023},
issn = {0882-5963},
doi = {https://doi.org/10.1016/j.pedn.2023.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S0882596323001227},
author = {Muanna Jéssica Batista Ludgério and Cleide Maria Pontes and Bárbara Letícia Cruz {dos Santos} and Eliza Cristina Macedo and Maria Wanderleya {de Lavor Coriolano Marinus} and Luciana Pedrosa Leal},
keywords = {Special education, Hospital education department, Hospitalized child, Child rearing, Teaching},
abstract = {Objective
To analyze the evidence available in the literature on the pedagogical practices developed with children through hospital classes.
Method
An integrative review was conducted on July 20, 2022, in Scopus, MEDLINE/PubMed, CINAHL, LILACS, Web of Science, ERIC, Educ@, and Scielo using the following descriptors in English, Portuguese, and Spanish, extracted from DECS/MeSH, CINAHL, Brased/INEP, and ERIC Thesaurus: “Child, Hospitalized”, “Education, Special”, “Education Department, Hospital”, “Hospital Classroom”, “Hospital Class”, “Child Rearing”, “Educational Practices”, “Early Childhood Education”, “Education”, “Hospital Pedagogy”, and “Hospital Special Class”. No time restriction was applied. The EndNot Web reference manager and the Rayyan software were used to select studies, and later, the methodological rigor and level of evidence were assessed.
Results
The 22 articles described pedagogical practices, including ludic activities, individualized work, working with regular school content, stimulation activities, pedagogical and dialogic listening, learning based on the exchange of knowledge, video games, computational robotics, and theatrical performance.
Conclusion
Although difficulties were identified in implementing pedagogical practices in the hospital, they were shown to allow educational continuity and clinical improvement of hospitalized children.
Implications for practice
Studies on the educational process within the hospital setting can contribute to the development of public policies and the guarantee of the right to education for hospitalized children.
Descriptors
Special education; Hospital education department; Hospitalized child; Child rearing; Teaching.}
}
@article{HICKENDORFF2020101311,
title = {Fourth graders’ adaptive strategy use in solving multidigit subtraction problems},
journal = {Learning and Instruction},
volume = {67},
pages = {101311},
year = {2020},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2020.101311},
url = {https://www.sciencedirect.com/science/article/pii/S0959475219307327},
author = {Marian Hickendorff},
keywords = {Mathematics education, Multidigit subtraction, Strategy flexibility, Strategy adaptivity, Choice/no-choice method},
abstract = {Using the choice/no-choice methodology we investigated Dutch fourth graders’ (N = 124) adaptive use of the indirect addition strategy to solve subtraction problems. Children solved multidigit subtraction problems in one choice condition, in which they were free to choose between direct subtraction and indirect addition, and in two no-choice conditions, in which they had to use either direct subtraction or indirect addition. Furthermore, children were randomly assigned to mental computation, written computation, or free choice between mental and written computation. One third of the children adaptively switched their strategy according to the number characteristics of the problems, whereas the remaining children consistently used the same strategy. The likelihood to adaptively switch strategies decreased when written computation was allowed or required, compared to mandatory mental computation. On average, children were adaptive to their own speed differences but not to the accuracy differences between the strategies.}
}
@article{DO2000483,
title = {Intentions in and relations among design drawings},
journal = {Design Studies},
volume = {21},
number = {5},
pages = {483-503},
year = {2000},
issn = {0142-694X},
doi = {https://doi.org/10.1016/S0142-694X(00)00020-X},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X0000020X},
author = {Ellen Yi-Luen Do and Mark D Gross and Bennett Neiman and Craig Zimring},
keywords = {drawing(s), architectural design, case study/studies, design activity, design research},
abstract = {Designers use drawings to explore alternatives and to test ideas. We report here on two studies on design and drawing. The first study of design drawing symbols aims to determine whether and to what extent it is possible to infer, interpret, or even guess what a designer was thinking about by looking at the drawings she has made. In the second study we examined a collection of drawings for the design of a house to investigate the systems of design transformations. Drawings are characterized by drawing style, projection type, and key elements. We analyzed the relationships among the drawings and developed a notation system for documenting these relationships.}
}
@article{CARPENTER2020100064,
title = {Bridging Domain and Data},
journal = {Patterns},
volume = {1},
number = {4},
pages = {100064},
year = {2020},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2020.100064},
url = {https://www.sciencedirect.com/science/article/pii/S2666389920300842},
author = {Anne E. Carpenter},
abstract = {Dr. Anne Carpenter addresses her career path from cell biology toward computation. Why would a researcher move outside their comfort zone into a different field, from a domain into data science? What is the best way to bridge domain and data? What is challenging about moving from domain toward data? What is amazing about bridging domain and data?}
}
@article{VALLEETOURANGEAU2016195,
title = {Insight with hands and things},
journal = {Acta Psychologica},
volume = {170},
pages = {195-205},
year = {2016},
issn = {0001-6918},
doi = {https://doi.org/10.1016/j.actpsy.2016.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0001691816301755},
author = {Frédéric Vallée-Tourangeau and Sune Vork Steffensen and Gaëlle Vallée-Tourangeau and Miroslav Sirota},
keywords = {Problem solving, Insight, Task ecology, Enactivism, Methodological individualism},
abstract = {Two experiments examined whether different task ecologies influenced insight problem solving. The 17 animals problem was employed, a pure insight problem. Its initial formulation encourages the application of a direct arithmetic solution, but its solution requires the spatial arrangement of sets involving some degree of overlap. Participants were randomly allocated to either a tablet condition where they could use a stylus and an electronic tablet to sketch a solution or a model building condition where participants were given material with which to build enclosures and figurines. In both experiments, participants were much more likely to develop a working solution in the model building condition. The difference in performance elicited by different task ecologies was unrelated to individual differences in working memory, actively open-minded thinking, or need for cognition (Experiment 1), although individual differences in creativity were correlated with problem solving success in Experiment 2. The discussion focuses on the implications of these findings for the prevailing metatheoretical commitment to methodological individualism that places the individual as the ontological locus of cognition.}
}
@article{LI2022107258,
title = {Transformer helps identify kiwifruit diseases in complex natural environments},
journal = {Computers and Electronics in Agriculture},
volume = {200},
pages = {107258},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.107258},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922005713},
author = {Xiaopeng Li and Xiaoyu Chen and Jialin Yang and Shuqin Li},
keywords = {Identify kiwifruit diseases, Deep learning, Vision transformer, Complex environments, Convolutional neural network},
abstract = {The complex background of disease images and the small contrast between the disease area and the background easily confuse, seriously affecting the robustness and accuracy of kiwifruit disease identification models. To address the above problems, this paper proposes a disease identification model based on Vision Transformer and Convolutional Neural Network, ConvViT(Convolutional Neural Network and Vision Transformer), to identify diseases by extracting effective features of kiwifruit disease spots. The proposed ConvViT includes convolutional structure and Transformer structure: The convolutional structure is used to extract the global features of the image, and the Transformer structure is used to obtain the local features of the disease area to help the CNN see better. Meanwhile, the paper designs different models according to the number of parameters and FLOPs (floating-point operations) to improve the model's scalability. The model variants of different sizes are designed to be lightweight to run on devices with different resource constraints. We achieved 98.78% identification accuracy on the self-built kiwifruit disease dataset, with up to 4.53% improvement in identification accuracy compared to the same level of Resnet, ViT, and ResMLP, and more than 10% reduction in the number of parameters and FLOPs. Experimental results on the PlantVillage dataset and the AI Challenger 2018 also show that ConvViT has good generalizability, indicating that the proposed model can solve kiwifruit disease identification problems in complex environments and be valuable a backbone network for other identification tasks with practical applications.}
}
@incollection{POULINDUBOIS2017653,
title = {Chapter 26 - The Development of Object Categories: What, When, and How?},
editor = {Henri Cohen and Claire Lefebvre},
booktitle = {Handbook of Categorization in Cognitive Science (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {San Diego},
pages = {653-671},
year = {2017},
isbn = {978-0-08-101107-2},
doi = {https://doi.org/10.1016/B978-0-08-101107-2.00027-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780081011072000270},
author = {Diane Poulin-Dubois and Sabina Pauen},
keywords = {Categorization, development, infant, child, categories, perceptual, conceptual},
abstract = {From birth, infants are exposed to a wealth of information from their surroundings. This makes early categorization abilities especially important for infants and children to process information and come to understand the world around them. As a result of several sophisticated experimental paradigms, it is well-established that early categorization abilities become refined over the developmental trajectory. Researchers have identified a global-to-basic shift in early categorical thinking, such that preverbal infants discriminate between global-level categories (i.e., dogs, cats, chairs, tables, etc.) before basic-level categories (i.e., different breeds of cats and dogs). However, differences in the literature regarding the timing of this shift emerge depending on the paradigm used to measure categorization. There is evidence to suggest that infants also use dynamic, causal, and functional information to guide their object categorization and discrimination. This chapter provides a comprehensive review of the research and theory on early categorization and concept development.}
}
@article{BAILLIE1989209,
title = {A comparison of the CM with the DAP for lattice gauge theory},
journal = {Parallel Computing},
volume = {12},
number = {2},
pages = {209-220},
year = {1989},
issn = {0167-8191},
doi = {https://doi.org/10.1016/0167-8191(89)90054-9},
url = {https://www.sciencedirect.com/science/article/pii/0167819189900549},
author = {Clive F Baillie and G {Stuart Pawley}},
keywords = {Connection Machine, Distributed Array Processor, SIMD, massively parallel, lattice gauge theory, QED, QCD, performance measurement, performance analysis},
abstract = {Lattice gauge theory is one of the most challenging large-scale scientific computations; a state of the art calculation requires at least 1014 floating-point operations, necessitating the use of advanced architecture massively parallel computers such as the Connection Machine (CM) made by Thinking Machines Corporation (TMC), and the Distributed Array Processor (DAP) made in the past by International Computers Limited (ICL) and currently by active Memory Technology (AMT). The most important gauge theory to be solved is that descrining the sub-nuclear world of high energy physics: Quantum Chromodynamics (QCD). The simples example of a gauge theory is Quantum Electro-dynamics (QED), the theory which describes the interaction of electrons and photons. Simulation of QCD requires computer software very similar to that for the simpler QED problem. Thus, as a first step towards computer simulation of QCD, we have developed code for QED on the CM, and compared this with similar code for the DAP. Experience with the DAP allows us to predict performances for QCD code on the CM, showing the latter to be a very serious proposition for such large-scale scientific computations.}
}
@article{RINO2024102366,
title = {Timed alignments with mixed moves},
journal = {Data & Knowledge Engineering},
volume = {154},
pages = {102366},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102366},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24000909},
author = {Neha Rino and Thomas Chatain},
keywords = {Conformance checking, Alignments, Timestamps, Time Petri nets},
abstract = {We study conformance checking for timed models, that is, process models that consider both the sequence of events that occur, as well as the timestamps at which each event is recorded. Time-aware process mining is a growing subfield of research, and as tools that seek to discover timing-related properties in processes develop, so does the need for conformance-checking techniques that can tackle time constraints and provide insightful quality measures for time-aware process models. One of the most useful conformance artefacts is the alignment, that is, finding the minimal changes necessary to correct a new observation to conform to a process model. In this paper, we extend the notion of timed distance from a previous work where an edit on an event’s timestamp came in two types, depending on whether or not it would propagate to its successors. Here, these different types of edits have a weighted cost each, and the ratio of their costs is denoted by α. We then solve the purely timed alignment problem in this setting for a large class of these weighted distances (corresponding to α∈{1}∪[2,∞)). For these distances, we provide linear time algorithms for both distance computation and alignment on models with sequential causal processes.}
}
@article{RUTHVEN2004259,
title = {Teacher representations of the successful use of computer-based tools and resources in secondary-school English, mathematics and science},
journal = {Teaching and Teacher Education},
volume = {20},
number = {3},
pages = {259-275},
year = {2004},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2004.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X04000113},
author = {Kenneth Ruthven and Sara Hennessy and Sue Brindley},
keywords = {Computer uses in education, Educational technology, Teacher attitude and cognition, Subject teaching and learning, Secondary education, England},
abstract = {This study investigated professional thinking about pedagogical aspects of technology use in mainstream classroom practice. It focuses on the systems of ideas which frame teacher accounts of the successful use of computer-based tools and resources in the core subjects of English, Mathematics and Science at secondary-school level. These accounts were elicited through group interviews with the relevant subject departments in six secondary schools in England. The analysis identifies seven broad themes in which teachers point to the contribution of technology use in: effecting working processes and improving production; supporting processes of checking, trialling and refinement; enhancing the variety and appeal of classroom activity; fostering pupil independence and peer support; overcoming pupil difficulties and building assurance; broadening reference and increasing currency of activity; and focusing on overarching issues and accentuating important features. Further examination of these themes shows how professional thinking about technology use is anchored in well-established representations of pupil motivation and classroom learning, and how contrasting subject profiles reflect corresponding differences in wider subject cultures.}
}
@article{RANA201761,
title = {Dynamic effects in the didehydro‐Diels‐Alder (DDDA) reaction of enyne‐ketoenes: 50% stepwise bond formation in spite of concerted transition state††This article is published as part of a special issue to celebrate the 80th birthday of Professor Waldemar Adam},
journal = {Journal of Physical Organic Chemistry},
volume = {30},
number = {9},
pages = {61-67},
year = {2017},
issn = {0894-3230},
doi = {https://doi.org/10.1002/poc.3732},
url = {https://www.sciencedirect.com/science/article/pii/S0894323022014072},
author = {Anup Rana and Indrajit Paul and Michael Schmittel},
abstract = {The C2─C6/Diels‐Alder cyclization of enyne‐ketoene 1 was studied by experiments, theoretical calculations, and dynamic trajectory computations. The failure to trap possible intermediate(s), indicates a concerted reaction mechanism. A detailed search for stationary points revealed a concerted mechanism and surprisingly a diradical intermediate with no direct connection to the enyne‐ketoene 1. To probe the accessibility of this intermediate quasiclassical trajectories were initiated from the concerted transition state structure. Notably, 36% of the trajectories reach the product zone directly and 5% arrive at the product via the intermediate diradical. Additionally, 31% of the trajectories go to the intermediate zone and stay there within the simulation time limit.}
}
@incollection{EVETT1994115,
title = {Chapter 6 - Providing Computationally Effective Knowledge Representation via Massive Parallelism},
editor = {Laveen N. KANAL and Vipin KUMAR and Hiroaki KITANO and Christian B. SUTTNER},
series = {Machine Intelligence and Pattern Recognition},
publisher = {North-Holland},
volume = {14},
pages = {115-135},
year = {1994},
booktitle = {Parallel Processing for Artificial Intelligence},
issn = {0923-0459},
doi = {https://doi.org/10.1016/B978-0-444-81704-4.50012-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780444817044500120},
author = {Matthew P. Evett and William A. Andersen and James A. Hendler},
abstract = {PARKA is a frame-based knowledge representation system implemented on the Connection Machine. PARKA provides a representation language consisting of concept descriptions (frames) and binary relations on those descriptions (slots). The system is designed explicitly to provide extremely fast property inheritance inference capabilities. In particular, PARKA can perform fast “recognition” queries of the form “find all frames satisfying m property constraints” in O(d + m) time—proportional only to the depth (d) of the knowledge base (KB), and independent of its size. For conjunctive queries of this type, PARKA's performance is measured in tenths of a second, even for KBs with more than 100,000 frames. We show similar results for timings on on large IS-A networks derived from the Cyc commonsense KB, and for queries involving knowledge structure pattern matching in support of case-based planning. With such run-time performance, PARKA is possibly the “fastest knowledge representation system in the world”.}
}
@article{LEITEFILHO2024100381,
title = {Evaluating chatbot user experience (UX) through electroencephalography measures: A systematic literature review},
journal = {Computers in Human Behavior Reports},
volume = {13},
pages = {100381},
year = {2024},
issn = {2451-9588},
doi = {https://doi.org/10.1016/j.chbr.2024.100381},
url = {https://www.sciencedirect.com/science/article/pii/S2451958824000149},
author = {Jaime Ranulfo {Leite Filho} and Thiago Adriano Coleti and Marcelo Morandini},
keywords = {User experience, Chatbot, Virtual assistant, Conversational agent, Natural language interface, Brain-computer interface, Electroencephalography, EEG, Evaluating model methodology approach strategy},
abstract = {Brain activity is a biological signal with unique characteristics that can determine important patterns for recording and processing. The electroencephalogram (EEG) is the most used signal because it measures brain electrical activity, offering greater resolution and data accuracy. When associated to brain activities in user-computer interactions, it can provide information that allows analyzing adequacy and user satisfaction. Thus, the objective of this paper is to identify works that specify which information on electroencephalography assessment may be used to compose an analysis of its interactions with conversational systems. The delimited that the research problems are: (1) What information about user experience by electroencephalography can be used to compose an analysis of their interactions with conversational systems? (2) What techniques are used to present user experience information by EEG to individuals? Based on the Systematic Review method, seven studies were identified that examined commercial EEG devices for UX assessment between 2011 and 2022. The current study found that multiple emotional stimuli were used and reported. The most popular technique among researchers is event induced emotional stimulation, in which participants passively perceive emotional stimuli such as images, music and videos to evoke certain emotions.}
}
@incollection{DUCHATEAU2023147,
title = {Chapter 7 - Machine learning and biophysical models: how to benefit each other?},
editor = {Francisco Chinesta and Elías Cueto and Yohan Payan and Jacques Ohayon},
booktitle = {Reduced Order Models for the Biomechanics of Living Organs},
publisher = {Academic Press},
pages = {147-164},
year = {2023},
series = {Biomechanics of Living Organs},
issn = {25890999},
doi = {https://doi.org/10.1016/B978-0-32-389967-3.00009-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323899673000093},
author = {Nicolas Duchateau and Oscar Camara},
keywords = {Machine learning, Simulations, Biophysical models, Synthetic data, Reduced-order models},
abstract = {Biophysical models and machine learning may be perceived as rather different entities, or on the contrary as very related forms of modelling. In this chapter, we precisely develop the latter idea to provide a didactic and up-to-date overview of some major research tracks where these two fields can collaborate and benefit each other. We specifically articulate contents around two complementary points-of-view on the potential benefits of one field to the other. For biophysical modelling, we focused on accelerating computations, estimating unobservable parameters and examining complex outputs; for machine learning, we laid stress on adding physiologically relevant knowledge and generating synthetic data for training and validation. Throughout this review, we detail specific questions of relevance with examples mostly in the context of computational cardiology, which is our field of interest, and encourage further interaction between these two areas of active research.}
}
@article{ACAR2016861,
title = {Soundscapes of Digital Morphogenesis in Architecture which Created from Musical Algorithm},
journal = {Procedia - Social and Behavioral Sciences},
volume = {216},
pages = {861-873},
year = {2016},
note = {Urban Planning and Architectural Design for Sustainable Development (UPADSD)},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.12.083},
url = {https://www.sciencedirect.com/science/article/pii/S1877042815062631},
author = {Didem Acar},
keywords = {Transcoding, Acoustic, Computational Design, Transdisciplinary framework, Architectural design},
abstract = {Music and architecture have made use of mathematical proportions throughout the history for the purpose of creating acoustic and visual forms. The reason for this is the aesthetic pursuit of both disciplines since centuries. Mathematics is one of the most important factors that influence aesthetic results. While forming their abstract aesthetic compositions the musicians use the musical notes that have definite frequency values. Each of these frequency values are defined by one integer. Every classical music artist uses the fractal sequencing of these frequencies. On the other hand we encounter hundreds of silent formats which are produced using mathematical ideas. In this context if we think of the interdisciplinary interaction between music and architecture no form is ever silent. In this study, the intersection of two disciplines will be examined in the perspective of architecture; a stumper and interrogative start for pursuit of architectural forms of the present day with the transformation of auditory forms to visual forms will be made; and a basis will be provided to be able to discuss the innovations that the spaces, structures and auditory experiences which can be formed by obtaining musical codes bring.}
}
@incollection{VODOVOTZ201589,
title = {Chapter 4.2 - Data-Driven and Statistical Models: Everything Old Is New Again},
editor = {Yoram Vodovotz and Gary An},
booktitle = {Translational Systems Biology},
publisher = {Academic Press},
address = {Boston},
pages = {89-98},
year = {2015},
isbn = {978-0-12-397884-4},
doi = {https://doi.org/10.1016/B978-0-12-397884-4.00012-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780123978844000124},
author = {Yoram Vodovotz and Gary An},
keywords = {Data-driven modeling, statistical modeling, systems biology, computational biology, critical illness, inflammation},
abstract = {In this chapter, data-driven and statistical methods, and the thinking process behind them, are introduced. The development of these methods is associated with the thought process behind their use, both in the context of reductionist research as well as in the context of systems and computational biology. The concepts, advantages, and disadvantages of Big Data are discussed and contrasted with those of dynamic mechanistic modeling. Clinically translational applications of data-driven and statistical methods in the context of critical illness are presented and discussed as a gateway to true mechanistic modeling.}
}
@incollection{HALFORD2008298,
title = {Cognitive Developmental Theories},
editor = {Marshall M. Haith and Janette B. Benson},
booktitle = {Encyclopedia of Infant and Early Childhood Development},
publisher = {Academic Press},
address = {San Diego},
pages = {298-308},
year = {2008},
isbn = {978-0-12-370877-9},
doi = {https://doi.org/10.1016/B978-012370877-9.00039-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780123708779000396},
author = {G.S. Halford},
abstract = {Theories of cognitive development are reviewed, beginning with pioneering theories by Piaget and Vygotsky. Neo-Piagetian theories which integrated Piagetian theory with other conceptions of cognition were developed by McLaughlin, Pascual-Leone, Case, Fischer, and Chapman. Complexity theories propose that children become capable of dealing with more complex relations as they develop. Information processing theories, neural net theories, dynamic systems theories, and theories of reasoning processes all provide models of the reasoning processes employed by children at different ages. Microgenetic analysis methods are used to study the processes of transition from one level of thinking to the next.}
}
@incollection{JANI202491,
title = {13.05 - Recent advances in simulation in fiber-reinforced polymer composites: Mechanical properties and applications},
editor = {Saleem Hashmi},
booktitle = {Comprehensive Materials Processing (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {91-109},
year = {2024},
isbn = {978-0-323-96021-2},
doi = {https://doi.org/10.1016/B978-0-323-96020-5.00182-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323960205001825},
author = {Hasan Rafsan Jani and Md Zillur Rahman},
keywords = {Natural fiber, Composites, FRP composites, Hybrid composites, Mechanical properties, Finite element analysis, Simulation, Industrial applications},
abstract = {This study provides an overview of the recent advances in simulation techniques applied to fiber-reinforced polymer (FRP) composites, focusing on analyzing their mechanical properties and applications. FRP composites have gained significant attention in various industries due to their excellent mechanical properties, lightweight nature, and corrosion resistance. Simulating the behavior of FRP composites allows researchers and engineers to optimize their design, understand their mechanical performance, and predict their response under different loading conditions. This study explores different theoretical models for predicting the mechanical properties of composites. This study also acknowledges the challenges of characterizing these heterogeneous materials, making computational techniques such as finite element analysis indispensable for accurate predictions. In addition, simulated mechanical properties of composites with varying fibers and matrices are explored and compared with experimental results. Furthermore, the applications of simulation in different industries for characterizing FRP composites are discussed.}
}
@article{LOCHAB202116,
title = {An improved flux limiter using fuzzy modifiers for Hyperbolic Conservation Laws},
journal = {Mathematics and Computers in Simulation},
volume = {181},
pages = {16-37},
year = {2021},
issn = {0378-4754},
doi = {https://doi.org/10.1016/j.matcom.2020.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0378475420303207},
author = {Ruchika Lochab and Vivek Kumar},
keywords = {Flux limiters, Hyperbolic equations, High resolution methods, Fuzzy logic},
abstract = {The objective of the work in this paper is to computationally tackle a range of problems in hyperbolic conservation laws, which is an interesting branch of computational fluid dynamics. For the simulation of issues in hyperbolic conservation laws, this work explores the concept of fuzzy logic-based operators. This research presents a unique mixture of fuzzy sets and logic with a new branch of conservation laws from fluid dynamics. The approach considers a computational procedure based on the reconstruction of several high-order numerical methods termed as flux-limited methods using some fuzzy logic operators. With the aid of fuzzy modifiers, these flux limiters are further modified. This approach results in improved convergence of approximations and maintains the problem’s basic properties to be solved. Additionally, to ensure improved results, modified flux-limited methods are imposed on some famous test problems. The application results are provided wherever required. The work has demonstrated that it is possible to use such technique and apply it to complex areas of computational fluid dynamics to produce a more straightforward approach to studying other topics like flux-limited methods and hence opens up an exciting gateway for future work.}
}
@article{BOWLER2016117,
title = {Mindful makers: Question prompts to help guide young peoples' critical technical practices in maker spaces in libraries, museums, and community-based youth organizations},
journal = {Library & Information Science Research},
volume = {38},
number = {2},
pages = {117-124},
year = {2016},
issn = {0740-8188},
doi = {https://doi.org/10.1016/j.lisr.2016.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0740818815300840},
author = {Leanne Bowler and Ryan Champagne},
abstract = {This study examines question prompts as a means to scaffold reflection and reflexivity in the design, development, and use of technological artifacts in maker spaces for youth at public libraries, museums, and community-based organizations. Qualitative analysis is applied to data gathered in four focus groups with teens, three semi-structured interviews with adults who facilitate maker spaces, and six observation sessions. Outcomes include a rich description of critical thinking in the context of technology practice, and secondly, a set of eight activation questions that serve as a tool kit to encourage reflection and scaffold mindful and critical practices in community-based maker spaces for youth. Results from this study support the development of nstruments and practices to support mindful making and critical technical practice in maker spaces for youth.}
}
@article{WANG2024119513,
title = {Real-sea validation of a model predictive controller's inherent robustness for medium-scale unmanned trimaran heading},
journal = {Ocean Engineering},
volume = {313},
pages = {119513},
year = {2024},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2024.119513},
url = {https://www.sciencedirect.com/science/article/pii/S0029801824028518},
author = {Jun Wang and Jian Wang and Xiaofeng Liang and Junjie Liu and Shunzhao Cheng and Hong Yi},
keywords = {Unmanned surface vehicle, Heading control, Path following, Trimaran, Model predictive control, Robustness},
abstract = {The development of medium-to large-scale and high-performance unmanned surface vehicles (USVs) is a burgeoning trend in intelligent marine systems. The heading controller is crucial for USVs to execute diverse missions, especially given their inherent underactuation characteristics and constraints. Recent efforts have focused on enhancing the robustness of controllers against external disturbances and employed two main strategies: one converges the control error to a bounded residual set through robust modifications, while the other eliminates the error by modelling disturbances. Notably, these enhancements have primarily catered to small USVs, where disturbances significantly impact their manoeuvrability, necessitating such robust control strategies. This focus has somewhat overshadowed the inherent robustness of closed-loop control systems. Compared with small USVs, medium-to large-scale USVs are less affected by external disturbances, despite undertaking more complex missions. Leveraging the intrinsic robustness of controllers presents an opportunity to simplify controller design, thereby reallocating computational resources towards enhancing mission capabilities. Model Predictive Control (MPC) has attracted significant attention recently, and its receding horizon and optimality theoretically provides a new level of inherent robustness, which remains under-explored in real sea. This paper focuses on the inherent robustness of MPC in managing the heading of a medium-scale unmanned trimaran subjected to the thrust angle and angular velocity constraints. A model predictive controller considering the constraints is designed based on the identified Nomoto model and the asymptotic stability is ensured with a terminal cost. Conducted real-sea experiments and comparative analyses with a Proportional-Integral-Derivative (PID) controller, the most widespread and dominant control algorithm in practical USV engineering, underscore the superiority of MPC in maintaining satisfactory closed-loop performance. Furthermore, the MPC controller is also successfully applied to real-sea path-following missions and demonstrated good tracking performance in various sea conditions ranging from level 3 to 5 and wind speeds spanning from level 6 to 8. This validation opens up new avenues for motion control strategies in the evolving landscape of larger-scale USVs.}
}
@article{NOORMOHAMMADIASL2024104821,
title = {Human leading or following preferences: Effects on human perception of the robot and the human-robot collaboration},
journal = {Robotics and Autonomous Systems},
pages = {104821},
year = {2024},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2024.104821},
url = {https://www.sciencedirect.com/science/article/pii/S0921889024002057},
author = {Ali Noormohammadi-Asl and Kevin Fan and Stephen L. Smith and Kerstin Dautenhahn},
keywords = {Human–robot collaboration, Adaptive task planning, Leading/following preference, Team performance, Perception of the robot and collaboration},
abstract = {Achieving effective and seamless human–robot collaboration requires two key outcomes: enhanced team performance and fostering a positive human perception of both the robot and the collaboration. This paper investigates the capability of the proposed task planning framework to realize these objectives by integrating human leading/following preferences and performance into its task allocation and scheduling processes. We designed a collaborative scenario wherein the robot autonomously collaborates with participants. The outcomes of the user study indicate that the proactive task planning framework successfully attains the aforementioned goals. We also explore the impact of participants’ leadership and followership styles on their collaboration. The results reveal intriguing relationships between these factors which warrant further investigation in future studies.}
}
@article{FERGUSON20141885,
title = {Training in Minimally Invasive Lobectomy: Thoracoscopic Versus Robotic Approaches},
journal = {The Annals of Thoracic Surgery},
volume = {97},
number = {6},
pages = {1885-1892},
year = {2014},
issn = {0003-4975},
doi = {https://doi.org/10.1016/j.athoracsur.2014.01.055},
url = {https://www.sciencedirect.com/science/article/pii/S0003497514003403},
author = {Mark K. Ferguson and Konstantin Umanskiy and Cindy Warnes and Amy D. Celauro and Wickii T. Vigneswaran and Vivek N. Prachand},
abstract = {Background
Skills required for thoracoscopic and robotic operations likely differ. The needs and abilities of trainees learning these approaches require assessment.
Methods
Trainees performed initial components of minimally invasive lobectomies using thoracoscopic or robotic approaches. Component difficulty was scored by trainees using the NASA task load index (NASATLX). Performance of each component was graded by trainees and attending surgeons on a 5-point ordinal scale (naïve, beginning learner, advanced learner, competent, master).
Results
Eleven surgical trainees performed 87 replications among three lobectomy components (divide pulmonary ligament; dissect level 7/8/9 nodes; dissect level 4/5 nodes). Before performance NASATLX scores did not differ among components or between surgical approaches. Trainees' after performance NASATLX scores appropriately calibrated task load for the components. After performance NASATLX scores were significantly lower for thoracoscopy than before performance estimates; robotic scores were similar before surgery and after performance. Task load was higher for robotic than for thoracoscopic approaches. Trainees rated their performance higher than did attending surgeons in domains of knowledge and thinking, but ratings for other domains were similarly low. Ratings for performance improved significantly as component performance repetitions increased.
Conclusions
Trainees did not differentiate task load among components or surgical approaches before attempting them. Task load scores differentiated difficulty among initial components of lobectomy, and were greater for robotic than for thoracoscopic approaches. Trainees overestimated their level of cognitive performance compared with attending physician evaluation of trainee performance. The study provides insights into how to customize training for thoracoscopic and robotic lobectomy and identifies tools to assess training effectiveness.}
}
@article{HEISS2023107908,
title = {Social media information literacy: Conceptualization and associations with information overload, news avoidance and conspiracy mentality},
journal = {Computers in Human Behavior},
volume = {148},
pages = {107908},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107908},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223002595},
author = {Raffael Heiss and Andreas Nanz and Jörg Matthes},
keywords = {Social media, Media literacy, Information literacy, Information overload, News avoidance, Conspiracy mentality},
abstract = {In this study, we present a novel scale for measuring social media information literacy (SMIL) that encompasses six sub-dimensions: navigation, curation, appraisal, comprehension, creation, and interaction. We also examine antecedents of SMIL, its association with information overload, and possible indirect consequences such as news avoidance and conspiracy thinking. Relying on a two-wave panel dataset (n = 901), we first used factor analysis to test the proposed measurement. The results showed that the six dimensions were empirically distinct and loaded on a higher order SMIL factor. In a second step, we explored antecedents and outcomes of SMIL and its sub-dimensions. We found that not age, but education and frequency of social media use were positively associated with gains in SMIL. Furthermore, SMIL was associated with a decrease in information overload. Information overload, in turn, was associated with a decrease in news avoidance and an increase in conspiracy mentality. Taken together, our results lend support that SMIL may support positive civic outcomes by its potential role in lowering information overload. Helping citizens to acquire SMIL may be one valuable measure to foster democratic resilience.}
}
@article{20244431,
title = {The wide-reaching power of technology},
journal = {Cell},
volume = {187},
number = {17},
pages = {4431-4432},
year = {2024},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2024.07.046},
url = {https://www.sciencedirect.com/science/article/pii/S0092867424008407}
}
@article{GONZALEZFELIU201289,
title = {Modeling Urban Goods Movement: How to be Oriented with so Many Approaches?},
journal = {Procedia - Social and Behavioral Sciences},
volume = {39},
pages = {89-100},
year = {2012},
note = {Seventh International Conference on City Logistics which was held on June 7- 9,2011, Mallorca, Spain},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2012.03.093},
url = {https://www.sciencedirect.com/science/article/pii/S1877042812005605},
author = {Jesus Gonzalez-Feliu and Jean-Louis Routhier},
keywords = {Urban goods movement, modeling approaches, systematic review, multidisciplinarity},
abstract = {This paper proposes an analysis of the different model construction and development approaches in the context of urban goods movement (UGM). We focus on the model development issues more than on the mathematical tools applied in these models. First, we explore the main UGM models in the field, identifying their main construction schemas and their features limits. From this analysis, we propose a classification of UGM modeling frameworks, synthesizing them on a table that illustrates their construction schemas. Second, we analyze their limits and find a first set of synergies between the different thinking schools. This analysis allows us to highlight the strong points and override their weaknesses, and to propose a set of recommendations for planners and modeling schools in order to find co-operative schemas that improve the models’ efficiency.}
}
@article{MELNIKOFF2018280,
title = {The Mythical Number Two},
journal = {Trends in Cognitive Sciences},
volume = {22},
number = {4},
pages = {280-293},
year = {2018},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2018.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S136466131830024X},
author = {David E. Melnikoff and John A. Bargh},
keywords = {dual process, dual system, type 1, type 2, automaticity},
abstract = {It is often said that there are two types of psychological processes: one that is intentional, controllable, conscious, and inefficient, and another that is unintentional, uncontrollable, unconscious, and efficient. Yet, there have been persistent and increasing objections to this widely influential dual-process typology. Critics point out that the ‘two types’ framework lacks empirical support, contradicts well-established findings, and is internally incoherent. Moreover, the untested and untenable assumption that psychological phenomena can be partitioned into two types, we argue, has the consequence of systematically thwarting scientific progress. It is time that we as a field come to terms with these issues. In short, the dual-process typology is a convenient and seductive myth, and we think cognitive science can do better.}
}
@article{JIN2024105113,
title = {An enhanced approach for few-shot segmentation via smooth downsampling mask and label smoothing loss},
journal = {Image and Vision Computing},
volume = {148},
pages = {105113},
year = {2024},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2024.105113},
url = {https://www.sciencedirect.com/science/article/pii/S0262885624002178},
author = {Hailong Jin and Huiying Li},
keywords = {Few-shot segmentation, Segmentation, Few-shot learning, Deep learning},
abstract = {Few-shot semantic segmentation aims to segment new categories with only a small number of annotated images. Previous methods mainly focused on exploiting the pixel-level correlation between the support image and the query image, combined with attention-based methods, resulting in significant advancements. In this paper, we introduce a new perspective to enhance few-shot segmentation. We identify that utilizing the bilinear interpolation method to downsample the mask leads to the loss of fine-grained information from the target features. To address this issue, we propose a Smooth Downsampling Mask (SDM) method. The SDM method is designed to retain more effective target semantic features by employing a cascaded downsampling approach with a smooth kernel for mask processing. Additionally, we propose a label smoothing loss to further enhance the performance, which provides direct guidance for low-resolution feature map optimization. Both methods can be used as plug-and-play modules for existing methods. Notably, our proposed method does not involve additional learnable parameters and is computationally efficient, thus achieving painless gains. To validate the effectiveness of our method, we take three publicly available models as baselines and conduct extensive experiments on three public benchmarks PASCAL-5i, COCO-20i and FSS-1000, and achieve considerable improvement.}
}
@article{BARR1986183,
title = {New Approaches in Water Balance Computations.},
journal = {Journal of Arid Environments},
volume = {11},
number = {2},
pages = {183-184},
year = {1986},
issn = {0140-1963},
doi = {https://doi.org/10.1016/S0140-1963(18)31233-3},
url = {https://www.sciencedirect.com/science/article/pii/S0140196318312333},
author = {D.I.H. Barr}
}
@incollection{BERNINGER2004197,
title = {Chapter 6 - The Reading Brain in Children and Youth: A Systems Approach},
editor = {Bernice Wong},
booktitle = {Learning About Learning Disabilities (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {San Diego},
pages = {197-248},
year = {2004},
isbn = {978-0-12-762533-1},
doi = {https://doi.org/10.1016/B978-012762533-1/50009-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780127625331500093},
author = {Virginia W. Berninger},
abstract = {Publisher Summary
This chapter presents a systems approach to the reading brain in children and youth. A supervisory attentional system in the frontal lobes protects the working brain from external and internal distraction through an inhibitory mechanism that suppresses distraction. Brains are electrochemical computers whose computations create inner mental worlds and overt interactions with the external world. Complete understanding of the functional reading system will require knowledge of regionally specific localized brain activation and interconnectivity of specific regions during the computational processes that create the inner mental worlds as well as the overt reading behavior of reading brains. Visual inspection of the brains of normal and disabled readers reveals no secrets about the structural anomalies that differentiate the neural architecture of those who learn to read easily and those who struggle to learn to read. Domain-general systems that the functional reading system may draw upon include specific sensory systems, fine motor systems for the mouth and hand, attentional systems, networks of supervisory executive functions, the limbic system, and the higher-level thinking and problem solving system.}
}
@article{POLETTI20141803,
title = {Adverse childhood experiences worsen cognitive distortion during adult bipolar depression},
journal = {Comprehensive Psychiatry},
volume = {55},
number = {8},
pages = {1803-1808},
year = {2014},
issn = {0010-440X},
doi = {https://doi.org/10.1016/j.comppsych.2014.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0010440X14001825},
author = {Sara Poletti and Cristina Colombo and Francesco Benedetti},
abstract = {Background
Cognitive distortion is a central feature of depression, encompassing negative thinking, dysfunctional personality styles and dysfunctional attitudes. It has been hypothesized that ACEs could increase the vulnerability to depression by contributing to the development of a stable negative cognitive style. Nevertheless, little research has been carried out on possible associations between adverse childhood experiences (ACEs) and cognitive distortion, and whether any gender differences exist.
Aim
The aim of this study was to examine the association between ACEs and cognitive distortions and possible differences between genders in a sample of patients affected by bipolar disorder.
Method
130 patients with bipolar disorder (BD) (46 men and 84 females), completed the Risky Family Questionnaire to assess ACEs and the Cognition Questionnaire (CQ) to assess cognitive distortions.
Results
A positive association was found between ACE and the CQ total score. Investigating the 5 dimensions assessed through the CQ, only the dimension “generalization across situations” was significantly associated to ACE. An interaction between ACE and gender was found for “generalization across situations”, while no differential effect among females and males was found for CQ total score.
Conclusion
This is the first study to report a relationship between negative past experiences and depressive cognitive distortions in subjects affected by BD. Growing in a family environment affected by harsh parenting seems to a cognitive vulnerability to depression; this effect is especially strong in females.}
}
@article{BYRNE2002426,
title = {Mental models and counterfactual thoughts about what might have been},
journal = {Trends in Cognitive Sciences},
volume = {6},
number = {10},
pages = {426-431},
year = {2002},
issn = {1364-6613},
doi = {https://doi.org/10.1016/S1364-6613(02)01974-5},
url = {https://www.sciencedirect.com/science/article/pii/S1364661302019745},
author = {Ruth M.J. Byrne},
keywords = {counterfactual thinking, reasoning, imagination, emotions, if only},
abstract = {Counterfactual thoughts about what might have been (‘if only…’) are pervasive in everyday life. They are related to causal thoughts, they help people learn from experience and they influence diverse cognitive activities, from creativity to probability judgements. They give rise to emotions and social ascriptions such as guilt, regret and blame. People show remarkable regularities in the aspects of the past they mentally ‘undo’ in their counterfactual thoughts. These regularities provide clues about their mental representations and cognitive processes, such as keeping in mind true possibilities, and situations that are false but temporarily supposed to be true.}
}
@article{WANG201837,
title = {Linguistic terms with weakened hedges: A model for qualitative decision making under uncertainty},
journal = {Information Sciences},
volume = {433-434},
pages = {37-54},
year = {2018},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2017.12.036},
url = {https://www.sciencedirect.com/science/article/pii/S0020025517311593},
author = {Hai Wang and Zeshui Xu and Xiao-Jun Zeng},
keywords = {Decision making, Linguistic hedges, Linguistic term sets, Multi-granularity linguistic decision making, Semantics},
abstract = {When expressing the experts’ opinions in qualitative decision making (QDM), linguistic hedges can be considered to modify the force expressed by a predefined linguistic term. If an expert is not sure to select one term, weakened hedges would be a natural way to express the uncertainty. This is usually implemented by using a hedge to modify the most possible term, like the expression “more or less good”. To model the uncertainty implied by hedges in QDM, this paper presents a novel linguistic representational and computational model in which the linguistic expressions take the form of a weakened hedge and a linguistic term, which is named as linguistic term with weakened hedge (LTWH). The syntax of LTWHs is defined by a set of hedges and a set of linguistic terms. The semantics of a LTWH is determined, objectively, based on the semantics of the term and a similarity measure of the reference domain. Accordingly, the negation, order relations and some basic operations of LTWHs are defined. To illustrate the effectiveness of LTWHs in granular computing, the connection to some multi-granularity linguistic models is exploited and a process for unifying multi-granularity linguistic information is developed. The major contritions of this paper are: (1) The proposed model enables a new manner to express and operate uncertain linguistic information in QDM; (2) it possesses clear syntax and semantics and the computational results are very interpretable; and (3) the proposed solution of multi-granularity linguistic unification maintains the semantics of the original linguistic information.}
}
@article{KOICHU2015233,
title = {Proving as problem solving: The role of cognitive decoupling},
journal = {The Journal of Mathematical Behavior},
volume = {40},
pages = {233-244},
year = {2015},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2015.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0732312315300067},
author = {Boris Koichu and Uri Leron},
keywords = {Proving, Problem solving, Cognitive decoupling, Cycles in problem solving, Drawings and diagrams, Dual process theory},
abstract = {This paper discusses the process of proving from a novel theoretical perspective, imported from cognitive psychology research. This perspective highlights the role of hypothetical thinking, mental representations and working memory capacity in proving, in particular the effortful mechanism of cognitive decoupling: problem solvers need to form in their working memory two closely related models of the problem situation – the so-called primary and secondary representations – and to keep the two models decoupled, that is, keep the first fixed while performing various transformations on the second, while constantly struggling to protect the primary representation from being “contaminated” by the secondary one. We first illustrate the framework by analyzing a common scenario of introducing complex numbers to college-level students. The main part of the paper consists of re-analyzing, from the perspective of cognitive decoupling, previously published data of students searching for a non-trivial proof of a theorem in geometry. We suggest alternative (or additional) explanations for some well-documented phenomena, such as the appearance of cycles in repeated proving attempts, and the use of multiple drawings.}
}
@incollection{GIOVANNONE202441,
title = {Chapter Two - Further steps towards a mechanistic functionalist framework for understanding individual differences in language and cognition},
editor = {Kara D. Federmeier},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {81},
pages = {41-73},
year = {2024},
issn = {0079-7421},
doi = {https://doi.org/10.1016/bs.plm.2024.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0079742124000380},
author = {Nikole Giovannone and Joseph C. Toscano},
keywords = {Individual differences, Cognition, Language processing, Computational modeling, Mechanistic functionalism},
abstract = {Despite a growing focus on individual differences in cognitive psychology, research in this area is complicated by several issues related to how such differences are defined and measured. These challenges create a significant roadblock for the field. To combat this issue, we argue that the next critical step for language and cognitive science is careful and thorough investigation of the specific mechanisms that drive individual differences. In this chapter, our goal is to extend the process-based mechanistic functional normativist framework and to provide a test case for how researchers can leverage computational modeling to investigate individual differences in cognitive mechanisms (using pattern learning in the serial reaction time task as an example). By shifting our focus to characterizing the mechanisms that drive individual differences in language and cognition, the field stands to advance both theoretical frameworks and methodological approaches for studying these processes.}
}
@incollection{BLAGOJEVIC20171,
title = {Chapter One - A Systematic Approach to Generation of New Ideas for PhD Research in Computing},
editor = {Ali R. Hurson and Veljko Milutinović},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {104},
pages = {1-31},
year = {2017},
booktitle = {Creativity in Computing and DataFlow SuperComputing},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2016.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0065245816300572},
author = {V. Blagojević and D. Bojić and M. Bojović and M. Cvetanović and J. Đorđević and Đ. Đurđević and B. Furlan and S. Gajin and Z. Jovanović and D. Milićev and V. Milutinović and B. Nikolić and J. Protić and M. Punt and Z. Radivojević and Ž. Stanisavljević and S. Stojanović and I. Tartalja and M. Tomašević and P. Vuletić},
keywords = {PhD research, Idea generation, Research methodology, Research idea classification, Creative thinking},
abstract = {This article represents an effort to help PhD students in computer science and engineering to generate good original ideas for their PhD research. Our effort is motivated by the fact that most PhD programs nowadays include several courses, as well as the research component, that should result in journal publications and the PhD thesis, all in a timeframe of 3–6 years. In order to help PhD students in computing disciplines to get focused on generating ideas and finding appropriate subject for their PhD research, we have analyzed some state-of-the-art inventions in the area of computing, as well as the PhD thesis research of faculty members of our department, and came up with a proposal of 10 methods that could be implemented to derive new ideas, based on the existing body of knowledge in the research field. This systematic approach provides guidance for PhD students, in order to improve their efficiency and reduce the dropout rate, especially in the area of computing.}
}
@incollection{EDELMAN2015596,
title = {Marr, David (1945–80)},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {596-598},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.61085-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868610851},
author = {Shimon Edelman and Lucia M Vaina},
keywords = {Biological information processing, Brain function, Cognitive psychology, Computational theory and modeling, Neuroscience, Scientific methodology, Vision},
abstract = {David Courtnay Marr was born in 1945 in Essex, England. Marr's dissertation, written at Trinity College, Cambridge and published between 1969 and 1971, presented a theory of mammalian brain function, parts of which remain relevant to the present day, despite vast advances in neurobiology in the past decades. In 1973, Marr joined the Artificial Intelligence Laboratory at the Massachusetts Institute of Technology, where he was made a tenured full professor in 1980. Marr died in November 1980, of leukemia. His highly influential book, Vision: A Computational Investigation into the Human Representation and Processing of Visual Information, which has redefined and revitalized the study of human and machine vision, was published posthumously, in 1982, with a new edition appearing in 2010.}
}
@incollection{KUMBALE2021306,
title = {Models for Personalized Medicine},
editor = {Olaf Wolkenhauer},
booktitle = {Systems Medicine},
publisher = {Academic Press},
address = {Oxford},
pages = {306-317},
year = {2021},
isbn = {978-0-12-816078-7},
doi = {https://doi.org/10.1016/B978-0-12-801238-3.11349-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128012383113492},
author = {Carla M. Kumbale and Jacob D. Davis and Eberhard O. Voit},
keywords = {Dynamic model, Health simplex, Health trajectory, Machine learning, Modeling, Networks, Personalized medicine, Precision medicine, Systems biology, Theranostics},
abstract = {The customization of medicine to specific individuals promises clear improvements in disease treatment, but also faces substantial challenges, many of which have their roots in the complexity of the human body. This complexity cannot be grasped with intuition alone and is not appropriately captured by reductionist methods, which have been dominating biology and medicine for the past decades. Experimental and computational systems biology have the potential of generating adequate datasets and analyzing them in a manner that captures the complexity of health and disease systems in a personalized manner. This potential has not yet fully materialized, but examples and case studies provide a glimpse of the power these approaches are likely to have in the future.}
}
@article{ANTONIOU2024105918,
title = {Realistic simulation of air pollution in an urban area to promote environmental policies},
journal = {Environmental Modelling & Software},
volume = {172},
pages = {105918},
year = {2024},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2023.105918},
url = {https://www.sciencedirect.com/science/article/pii/S1364815223003043},
author = {A. Antoniou and G. Ioannidis and L. Ntziachristos},
keywords = {Optimized visualization, 3D graphics software, Photo-realistic result, Pollutant reduction},
abstract = {Visualizing tools are now capable of synthesizing satellite imagery with Computational Fluid Dynamics (CFD) results. The new method is applied in Augsburg, Germany, and consists of two main phases: pre-processing and post-processing. The pre-processing phase involves creating geometry, mesh, and extracting results from simulations of traffic pollutant dispersion. In the post-processing phase, the results are combined with satellite images to produce visually optimized results. The demonstration of road traffic air pollution is based on real data from local air quality measurements and specific scenarios. The results indicate that using these visualization tools produce understandable and impressive images and videos. This enhances the public's comprehension of scientific results and raises awareness of environmental issues. An increased understanding of scientific results can reinforce the implementation of environmental policies by pressuring responsible authorities to take action. This paper provides a valuable tool for visualizing air pollution and facilitating public engagement with environmental issues.}
}
@article{MAROWKA2020199,
title = {On the performance difference between theory and practice for parallel algorithms},
journal = {Journal of Parallel and Distributed Computing},
volume = {138},
pages = {199-210},
year = {2020},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2019.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S0743731519304964},
author = {Ami Marowka},
keywords = {Python, Teaching parallel programming, Quicksort, Performance modeling},
abstract = {The performance of parallel algorithms is often inconsistent with their preliminary theoretical analyses. Indeed, the difference is increasing between the ability to theoretically predict the performance of a parallel algorithm and the results measured in practice. This is mainly due to the accelerated development of advanced parallel architectures, whereas there is still no agreed model for parallel computation, which has implications for the design of parallel algorithms and for the manner in which parallel programming should be taught. In this study, we examined the practical performance of Cormen’s Quicksort parallel algorithm. We determined the performance of the algorithm with different parallel programming approaches and examine the capacity of theoretical performance analyses of the algorithm for predicting the actual performance. This algorithm is used for teaching theoretical and practical aspects of parallel programming to undergraduate students. We considered the pedagogic implications that may arise when the algorithm is used as a learning resource for teaching parallel programming.}
}
@incollection{FOWLER20131,
title = {Chapter 1 - Introduction},
editor = {Bruce A. Fowler},
booktitle = {Computational Toxicology},
publisher = {Academic Press},
address = {San Diego},
pages = {1-4},
year = {2013},
isbn = {978-0-12-396461-8},
doi = {https://doi.org/10.1016/B978-0-12-396461-8.00001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780123964618000014},
author = {Bruce A. Fowler},
keywords = {Computational toxicology, risk assessment, chemical mixtures, emergency responses, animal–human extrapolations, data extrapolations from  test systems to human risk assessments, systems biology approaches, data mining, hypothesis generation},
abstract = {This book is intended to be an introduction to various applications of computational toxicology and to show how these approaches are currently being used effectively for risk assessment purposes in the near term. It is important to note that the field of computational toxicology is rapidly evolving and that subsequent editions of this book will take up new methods that are currently under development, such as high-throughput screening, and others that are still in a conceptual stage. There are many advantages for including computational toxicology approaches in the risk assessment process. Among these are reducing costs, minimizing use of animals in toxicology testing, improving speed in providing answers regarding chemicals in emergency situations such as the Gulf Oil spill, and dealing with the common problem of decision making for chemical mixtures. In addition, computational methods may be used for extrapolating or translating data from both in vitro and in vivo experimental animal test systems for human risk assessments of chemicals and drugs. In addition, computational methods may be used for focusing laboratory studies into productive areas by data mining the published literature and developing testable hypotheses by application of systems biology approaches to identify chemical interactions with functional molecular pathways to generate a more comprehensive picture of likely primary and secondary modes of chemical or drug activity. In summary, there is much that computational toxicology is now contributing to helping make better societal risk assessment decisions about chemicals and drugs. The future for these approaches is optimistic and limited only by human ingenuity and availability of resources.}
}
@article{BENEDEK2014125,
title = {To create or to recall? Neural mechanisms underlying the generation of creative new ideas},
journal = {NeuroImage},
volume = {88},
pages = {125-133},
year = {2014},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2013.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S1053811913011130},
author = {Mathias Benedek and Emanuel Jauk and Andreas Fink and Karl Koschutnig and Gernot Reishofer and Franz Ebner and Aljoscha C. Neubauer},
keywords = {Creativity, fMRI, Human cognition, Memory retrieval, Inferior parietal cortex},
abstract = {This fMRI study investigated brain activation during creative idea generation using a novel approach allowing spontaneous self-paced generation and expression of ideas. Specifically, we addressed the fundamental question of what brain processes are relevant for the generation of genuinely new creative ideas, in contrast to the mere recollection of old ideas from memory. In general, creative idea generation (i.e., divergent thinking) was associated with extended activations in the left prefrontal cortex and the right medial temporal lobe, and with deactivation of the right temporoparietal junction. The generation of new ideas, as opposed to the retrieval of old ideas, was associated with stronger activation in the left inferior parietal cortex which is known to be involved in mental simulation, imagining, and future thought. Moreover, brain activation in the orbital part of the inferior frontal gyrus was found to increase as a function of the creativity (i.e., originality and appropriateness) of ideas pointing to the role of executive processes for overcoming dominant but uncreative responses. We conclude that the process of idea generation can be generally understood as a state of focused internally-directed attention involving controlled semantic retrieval. Moreover, left inferior parietal cortex and left prefrontal regions may subserve the flexible integration of previous knowledge for the construction of new and creative ideas.}
}
@article{TANG2024101570,
title = {Disruptive content, cross agglomeration interaction, and agglomeration replacement: Does cohesion foster strength?},
journal = {Journal of Informetrics},
volume = {18},
number = {4},
pages = {101570},
year = {2024},
issn = {1751-1577},
doi = {https://doi.org/10.1016/j.joi.2024.101570},
url = {https://www.sciencedirect.com/science/article/pii/S1751157724000828},
author = {Kun Tang and Baiyang Li and Qiyu Zhu and Lecun Ma},
keywords = {Scholar agglomeration, Disruptive knowledge, Coauthor network, Agglomeration size},
abstract = {A trend in the academic field is agglomerations among scholars to generate knowledge with a disruptive influence on science and technology; however, the benefits have not been fully substantiated. This paper analyzes over 660,000 papers on artificial intelligence published from 1961 to 2023. We propose a method to calculate the innovative capacity of disruptive knowledge based on the similarity of historical, current, and future keywords, finding that scholars who commence their scientific endeavors earlier possess a heightened capability for disruptive knowledge innovation as Dkc index. The analysis reveals that multiagglomeration scholars have the highest average number of publications and citations, followed by agglomeration-flow scholars. Moreover, a larger agglomeration results in a lower ability to disrupt and consolidate knowledge innovation. Multiagglomeration and agglomeration-flow scholars harm disruptive/consolidative innovations. However, as the agglomeration effect intensifies, these two types of scholars from the disruptive perspective and multiagglomeration scholars from the consolidation perspective have a diminishing marginal effect on innovation capacity. The agglomeration size acts as a partial intermediary in the Multi→Size→Dkc index from the dual perspective and as a full mediator in the Flow→Size→Dkc index from the disruptive perspective, but only with a direct effect from the consolidative perspective.}
}
@article{CROMWELL20112026,
title = {Rethinking the cognitive revolution from a neural perspective: How overuse/misuse of the term ‘cognition’ and the neglect of affective controls in behavioral neuroscience could be delaying progress in understanding the BrainMind},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {35},
number = {9},
pages = {2026-2035},
year = {2011},
note = {Pioneering Research in Affective Neuroscience: Celebrating the Work of Dr. Jaak Panksepp},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2011.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0149763411000273},
author = {Howard Casey Cromwell and Jaak Panksepp},
keywords = {Cognition, Emotion, Motivation, Perception, Concepts, Neural activity, Behavior},
abstract = {Words such as cognition, motivation and emotion powerfully guide theory development and the overall aims and goals of behavioral neuroscience research. Once such concepts are accepted generally as natural aspects of the brain, their influence can be pervasive and long lasting. Importantly, the choice of conceptual terms used to describe and study mental/neural functions can also constrain research by forcing the results into seemingly useful ‘conceptual’ categories that have no discrete reality in the brain. Since the popularly named ‘cognitive revolution’ in psychological science came to fruition in the early 1970s, the term cognitive or cognition has been perhaps the most widely used conceptual term in behavioral neuroscience. These terms, similar to other conceptual terms, have potential value if utilized appropriately. We argue that recently the term cognition has been both overused and misused. This has led to problems in developing a usable shared definition for the term and to promotion of possible misdirections in research within behavioral neuroscience. In addition, we argue that cognitive-guided research influenced primarily by top-down (cortical toward subcortical) perspectives without concurrent non-cognitive modes of bottom-up developmental thinking, could hinder progress in the search for new treatments and medications for psychiatric illnesses and neurobehavioral disorders. Overall, linkages of animal research insights to human psychology may be better served by bottom-up (subcortical to cortical) affective and motivational ‘state-control’ perspectives, simply because the lower networks of the brain are foundational for the construction of higher ‘information-processing’ aspects of mind. Moving forward, rapidly expanding new techniques and creative methods in neuroscience along with more accurate brain concepts, may help guide the development of new therapeutics and hopefully more accurate ways to describe and explain brain-behavior relationships.}
}
@incollection{CHOQUET1995421,
title = { - Viscous flow computations on the connection machine by a finite element Petrov-Galerkin scheme},
editor = {A. Ecer and J. Hauser and P. Leca and J. Periaux},
booktitle = {Parallel Computational Fluid Dynamics 1993},
publisher = {North-Holland},
address = {Amsterdam},
pages = {421-428},
year = {1995},
isbn = {978-0-444-81999-4},
doi = {https://doi.org/10.1016/B978-044481999-4/50175-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780444819994501754},
author = {Remi Choquet and Penelope Leyland},
abstract = {Publisher Summary
This chapter studies the numerical solution of the compressible Navier–Stokes equations with matrix-free implicit schemes, and its implementation on the Connection Machine CM-200 in the framework of unstructured grids. First, a Petrov Galerkin formulation is chosen following ideas of Hughes, which give at each time step a non-linear problem to be solved F(u)=0. Regarding the cost side of such a resolution, the main difficulties are due to communications and to the nonlinear solver. This chapter considers such problems. In the resolution process, either the jacobian of F is needed or the product of such jacobian by a vector. To obtain either of them, elementary contributions are assembled. Then at some stage of the algorithm of resolution, the new solution obtained globally is explored. Each of these operations involves communications among processors, which are quite costly on a single instruction, multiple data (SIMD) computer. So, existing libraries are selected because they are both easy to use and reasonably efficient compared to more sophisticated approaches. This enables to focus the study on the algorithmic part of the code. The nonlinear equations are solved by Newton-generalized minimal residual method (GMRES). Newton iterations are applied to linearize the problem and the induced linear system is solved by the iterative scheme GMRES, which only needs matrix vector product and is well-suited to nonsymmetric large matrices.}
}
@article{JONES201795,
title = {An exploratory study on student understandings of derivatives in real-world, non-kinematics contexts},
journal = {The Journal of Mathematical Behavior},
volume = {45},
pages = {95-110},
year = {2017},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2016.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0732312316301791},
author = {Steven R. Jones},
keywords = {Calculus, Derivative, Applications, Real-world, Student understanding},
abstract = {Much research on calculus students’ understanding of applied derivatives has been done in kinematics-based contexts (i.e. position, velocity, acceleration). However, given the wide range of applications in science and engineering that are not based on kinematics, nor even explicitly on time, it is important to know how students understand applied derivatives in non-kinematics contexts. In this study, interviews with six students and surveys with 38 students were used to explore students’ “ways of understanding” and “ways of thinking” regarding applied, non-kinematics derivatives. In particular, six categories of ways of understanding emerged from the data as having been shared by a substantial portion of the students in this study: (1) covariation, (2) invoking time, (3) other symbols as constants, (4) other symbols as implicit functions, (5) implicit differentiation, and (6) output values as amounts instead of rates of change.}
}