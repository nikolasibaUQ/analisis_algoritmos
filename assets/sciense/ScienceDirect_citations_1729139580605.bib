@article{MA2024108953,
title = {Recent advances and prospects in hypersonic inlet design and intelligent optimization},
journal = {Aerospace Science and Technology},
volume = {146},
pages = {108953},
year = {2024},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2024.108953},
url = {https://www.sciencedirect.com/science/article/pii/S1270963824000865},
author = {Yue Ma and Mingming Guo and Ye Tian and Jialing Le},
keywords = {Hypersonic inlet, Machine learning, Reinforcement learning, Neural networks, Intellectual evolution},
abstract = {As the “respiratory tract” of the air breathing engine, the hypersonic inlet plays the role of compressing, decelerating, and pressurizing flow. However, research on the traditional inlet design is mainly based on gas dynamics theory and experience, which not only leads to a long design period but also fails to ensure the performance of the inlet at non-design points under broad working conditions. In recent years, with the further development of flow physics, a variety of advanced inlet design methods have been proposed, and a large number of valuable data have been accumulated by applying high-fidelity computational fluid dynamics numerical simulation and ground wind tunnel tests. A new generation of machine learning technologies, typically represented by deep learning, is developing vigorously. By building a deep neural network structure and using data-driven methods to carry out model training, it can realize typical feature extraction automatically, efficiently, and accurately, which is helpful to deeply explore the hidden flow mechanism between data and establish a fast prediction model of inlet performance. The optimal inlet design scheme can be obtained by applying the performance intelligent prediction model and the multi-objective intelligent evolution algorithm. This paper provides a detailed overview of the latest progress in inlet design by applying traditional ideas, expounds the basic principles and typical applications of some representative and prospective machine learning methods, and especially reports the current research status of the combination of machine learning and inlet. Finally, the future development trends and potential applications of several research directions are summarized.}
}
@article{PALIATHANASIS2024101585,
title = {Dipole cosmology in fQ-gravity},
journal = {Physics of the Dark Universe},
volume = {46},
pages = {101585},
year = {2024},
issn = {2212-6864},
doi = {https://doi.org/10.1016/j.dark.2024.101585},
url = {https://www.sciencedirect.com/science/article/pii/S2212686424001675},
author = {Andronikos Paliathanasis},
keywords = {Symmetric teleparallel, Non-metricity gravity, ()-gravity, Dipole cosmology, Dynamics},
abstract = {Symmetric teleparallel f(Q)-gravity allows for the presence of a perfect fluid with a tilted velocity in the Kantowski–Sachs geometry. In this dipole model, we consider an ideal gas and we investigate the evolution of the physical parameters. The tilt parameter is constrained by the nonlinear function f(Q) through the non-diagonal equations of the field equations. We find that the dynamics always reduce to the vacuum solutions of STEGR. This includes the Kasner universe, when no cosmological term is introduced by the f(Q) function, and the isotropic de Sitter universe, where fQ plays the role of the cosmological constant. In the extreme tilt limit, the universe is consistently anisotropic and accelerated. However, the final solution matches that of STEGR.}
}
@incollection{BERMAN201691,
title = {Chapter 3 - Indexing Text},
editor = {Jules J. Berman},
booktitle = {Data Simplification},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {91-133},
year = {2016},
isbn = {978-0-12-803781-2},
doi = {https://doi.org/10.1016/B978-0-12-803781-2.00003-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128037812000035},
author = {Jules J. Berman},
keywords = {Index, Concordance, Autocoding, Autoencoding, Page rank, Search, Retrieval},
abstract = {Data has no value if it cannot be sensibly examined. In past centuries, the index has been the key to searching and retrieving text. Today, it is tempting to think that the index is obsolete, being replaced the by the "find" box that pops onto the screen when we press the "Ctrl-F" keys. This is not the case; simple "find" searches cannot cope with the variations and complexities of textual information. A thoughtful index is a reconceptualization of the document that permits rapid retrieval of terms that are related to the search term. An index, aided by proper annotation of data, permits us to understand data in ways that were not anticipated when the original content was collected. With the use of computers, multiple indexes, designed for different purposes, can be created for a single document or data set. As data accrues, indexes can be updated. When data sets are combined, their respective indexes can be merged. A good way of thinking about indexes is that the document contains all of the complexity; the index contains all of the simplicity. Data scientists who understand how to create and use indexes will be in the best position to search, retrieve, and analyze textual data.}
}
@article{WANG2021104438,
title = {Geophysical electromagnetic modeling and evaluation: A review},
journal = {Journal of Applied Geophysics},
volume = {194},
pages = {104438},
year = {2021},
issn = {0926-9851},
doi = {https://doi.org/10.1016/j.jappgeo.2021.104438},
url = {https://www.sciencedirect.com/science/article/pii/S0926985121001853},
author = {Bochen Wang and Jianxin Liu and Xiangping Hu and Jiawei Liu and Zhenwei Guo and Jianping Xiao},
keywords = {Electromagnetic modeling, Finite difference method, Finite element method, Integral equation method},
abstract = {Electromagnetic forward modeling is the cornerstone of geophysical electromagnetic inversion. During the last 50 years, numerical simulation methods have been rapidly developed and widely used in geophysical area as the computational capacity continued to increase, such as from single-core to the most modern multi-core processing cards. This paper reviews the literature of electromagnetic fields simulation, particularly focusing on the forward modeling methods include finite difference method, finite element method, integral equation method, and several hybrid methods. We also discuss the possibility of deep learning methods for EM modeling. By sorting out the work done by the predecessors, this review briefly introduces the basic principles and traces back the development of these methods. We propose a Qualitative Evaluation Model named STAMP Model and some criterias of qualitative evaluation on these methods will be discussed in this model.}
}
@article{KOSIK2024101011,
title = {Why brain organoids are not conscious yet},
journal = {Patterns},
volume = {5},
number = {8},
pages = {101011},
year = {2024},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2024.101011},
url = {https://www.sciencedirect.com/science/article/pii/S2666389924001363},
author = {Kenneth S. Kosik},
abstract = {Summary
Rapid advances in human brain organoid technologies have prompted the question of their consciousness. Although brain organoids resemble many facets of the brain, their shortcomings strongly suggest that they do not fit any of the operational definitions of consciousness. As organoids gain internal processing systems through statistical learning and closed loop algorithms, interact with the external world, and become embodied through fusion with other organ systems, questions of biosynthetic consciousness will arise.}
}
@article{QIDWAI2013520,
title = {Attracting Students to the Computing Disciplines: A Case Study of a Robotics Contest},
journal = {Procedia - Social and Behavioral Sciences},
volume = {102},
pages = {520-531},
year = {2013},
note = {6th International Forum on Engineering Education (IFEE 2012)},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2013.10.768},
url = {https://www.sciencedirect.com/science/article/pii/S1877042813043048},
author = {Uvais Qidwai and Ryan Riley and Sayed El-Sayed},
keywords = {Robotics, LEGO Mindstorms kits, Logic based computing, Team work, Computing Contest},
abstract = {Guiding high school students towards a specific career choice is one of the challenges that parents, teachers, and career- counselors alike have to participate in with a lot of involvement. While career-fairs, open-houses, and other such activities from universities play an important role in attracting students to specific programs, it has been felt over the years that students tend to choose relatively easy majors and avoid computing and similar disciplines due to the involvement of programming and mathematics. This trend has affected Computer Science programs throughout the world in different proportions. In this paper we would like to share our experiences with an effort to make high school students more aware of the computing disciplines by involving them in doing something that belongs to the core of this discipline, i.e., developing correct logic and computational infrastructure, in a friendly and fun way. One well established fact is that if these young minds are motivated to go for higher education in computing, engineering or STEM (Science, Technology, Engineering & Mathematics)-based programs of study, they can find their own path and would know it better as to what would be a better line of career for them. The presented experiences are related to a set of “Computing Contests” that the Department of Computer Science & Engineering at Qatar University has been holding for the past five years. Initially, it was designed as a website design contest, but after a couple of years’ experience, the interest was observed to be declining. The contest theme was then changed to robotics using the LEGO Mindstorms’ NXT kits and giving simple challenges to the students in which the emphasis was on developing robust logic for computational implementation. Unlike the existing robotic contests in the region, including First LEGO League (FLL), National Robotics Olympiad (NRO), and Bot-Ball, the actual contest track was unknown to the participants until the contest day. This implied that the robots were to be programmed for certain general rules while the actual implementation sequence of the rules was not known. In fact, the emphasis was not given to the mechanical design but on generic sensory-perception based actuation mechanism that was to be programmed in order to solve the given challenge in minimum time. The paper presents results that show a positive change of trend from the schools’ side that reflects the interest taken by the schools, their participation, level of work performed, and what benefits were achieved by the students and their schools. The findings are presented in this paper using quantitative as well as qualitative data and shows that the robotics contests were successful in motivating students to make the computing domain as their major for university studies as well as in making them learn some of the most beneficial skills needed in the engineering and computing programs; objective programming, and outcome-based system design.}
}
@incollection{FORSTER2017159,
title = {Chapter 10 - Effects of the Neuro-Turn: The Neural Network as a Paradigm for Human Self-Understanding},
editor = {Jon Leefmann and Elisabeth Hildt},
booktitle = {The Human Sciences after the Decade of the Brain},
publisher = {Academic Press},
address = {San Diego},
pages = {159-177},
year = {2017},
isbn = {978-0-12-804205-2},
doi = {https://doi.org/10.1016/B978-0-12-804205-2.00010-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128042052000100},
author = {Y. Förster},
keywords = {Neuroscientific turn, embodiment, ontology, film, philosophy of mind, image, art, human self-understanding},
abstract = {This chapter discusses the image of the neural net and its impact on human self-understanding. Neuroscientific imaging techniques produce a wide range of images of the inside of the skull, which not only serve a medical purpose but also begin to fuel our imagination and give the interdisciplinary discussion a new edge. I will elaborate on how philosophy adopts neuroscientific thinking and how art incorporates artificial life. Based on an analysis of the movies Her (2013) and Transcendence (2014), two highly successful Hollywood movies and recent examples from popular culture, I show that these movies invoke a net structure as their leading image, and that it makes a difference whether the image of the brain (body-bound) or of the neural net (not body-bound) is used. Furthermore, the neural net represents an image that suggests the emergence of intelligence, self-organization, and infinity. It is closely intertwined with the Internet, advanced computing, and utopias of immortality. These images, I argue, ultimately suggest a new metaphysical dimension of an omni-present consciousness that permeates being itself.}
}
@article{MACK2020113291,
title = {Attention-based Convolutional Autoencoders for 3D-Variational Data Assimilation},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {372},
pages = {113291},
year = {2020},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2020.113291},
url = {https://www.sciencedirect.com/science/article/pii/S004578252030476X},
author = {Julian Mack and Rossella Arcucci and Miguel Molina-Solana and Yi-Ke Guo},
keywords = {Variational Data Assimilation, Attention networks, Convolutional Autoencoders},
abstract = {We propose a new ‘Bi-Reduced Space’ approach to solving 3D Variational Data Assimilation using Convolutional Autoencoders. We prove that our approach has the same solution as previous methods but has significantly lower computational complexity; in other words, we reduce the computational cost without affecting the data assimilation accuracy. We tested our proposal with data from a real-world application: a pollution model of a site in Elephant and Castle (London, UK) and found that we could (1) reduce the size of the background covariance matrix representation by O(103), and (2) increase our data assimilation accuracy with respect to existing reduced space methods.}
}
@article{MARZANO2024893,
title = {Decision Support Based on Digital Twin Simulation: A case study in Distillery Industry},
journal = {Procedia CIRP},
volume = {126},
pages = {893-897},
year = {2024},
note = {17th CIRP Conference on Intelligent Computation in Manufacturing Engineering (CIRP ICME ‘23)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.08.280},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124008497},
author = {Adelaide Marzano and Alessandra Caggiano},
keywords = {Digital twins, ergonomics, design},
abstract = {This paper presents a complete digital twin framework with the purpose to aid manufacturing ergonomics and control operations in distillery industry. The focus is on the design of a digital twin framework that manages the material flow in the real system based on ergonomics and resource efficient parameters of the manual operations. Preliminary experiments are done by applying the digital twin framework on a lab-scale case study and demonstrate the applicability of the proposed approach.}
}
@article{FU2024107897,
title = {Feature recognition in multiple CNNs using sEMG images from a prototype comfort test},
journal = {Computer Methods and Programs in Biomedicine},
volume = {243},
pages = {107897},
year = {2024},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2023.107897},
url = {https://www.sciencedirect.com/science/article/pii/S0169260723005631},
author = {You-Lei Fu and Wu Song and Wanni Xu and Jie Lin and Xuchao Nian},
keywords = {sEMG imaging, Convolutional neural network, Health informatics, Sternocleidomastoid, Prototype comfort},
abstract = {Objective
Deep learning-based CNN networks have recently been investigated to solve the problem of body posture recognition based on surface electromyographic signals (sEMG). Influenced by these studies, to develop a combined approach of sEMG and CNNs in the study of human-product interactions and the impact of body comfort, and to compare the advantages and disadvantages of various CNNs networks.
Methods
In this study, sEMG measurements were carried out by building a prototype usability experiment, and the data were divided into four categories, with two types of datasets: training and testing. Four CNNs, LeNet-5, VGGNet-11, InceptionNet V4, and DenseNet, were used for the recognition of sEMG images.
Results
DenseNet is another type of convolutional neural network with deep layers, which has a unique advantage over other algorithms. unique advantages over other algorithms. DenseNet has fewer layers and better accuracy than InceptionNet V4, but not only does it bypass enhanced feature reuse, but its network is easier to train and has some regularization effects, while also mitigating the problems of gradient disappearance and model degradation.
Conclusion
These findings could lead to a more appropriate CNN model and a useful tool for developing comfort judgments of surface EMG signals, furthering the development of products that come into contact with the human body without the need for routine retraining.}
}
@article{CARDOSOGRILO20211041,
title = {Fostering long-term care planning in practice: extending objectives and advancing stochastic treatment within location-allocation modelling},
journal = {European Journal of Operational Research},
volume = {291},
number = {3},
pages = {1041-1061},
year = {2021},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2020.09.055},
url = {https://www.sciencedirect.com/science/article/pii/S0377221720308596},
author = {Teresa Cardoso-Grilo and Mónica Duarte Oliveira and Ana Barbosa-Póvoa},
keywords = {OR in health services, Long-term care planning, Multi-objective, Stochastic programming, Scenario reduction methods},
abstract = {Many countries are currently concerned with the planning of networks of Long-Term Care (LTC), which requires considering a multiplicity of policy objectives and anticipating the impact of key uncertainties. Nevertheless, location-allocation literature has not been modelling key health policy objectives, and the use of stochastic planning models entails low practical usability due to prohibitive computational times. This study tackles these issues by proposing an approach that supports the reorganization of LTC networks (in terms of services location, capacity planning and patients allocation) while exploring different health policy objectives and considering uncertainty within a reasonable computational time, leading to the development of a stochastic multi-objective mathematical programming model – the LTCNetPlanner. The LTCNetPlanner builds upon health economics and policy concepts to model the maximization of health and wellbeing together with cost- and equity-related objectives within location-allocation literature. Concerning uncertainty, a scenario-based stochastic approach is developed and alternative scenario reduction methods enabling a faster model resolution are explored within the LTCNetPlanner. Specifically, it is proposed a novel Morphol-KMG method able to reduce the number of scenarios while accounting for experts’ knowledge. A case study in the Great Lisbon region is explored, showing the usefulness of the proposed scenario reduction method to reduce computational times, and how planning decisions change when health and wellbeing benefits are considered.}
}
@incollection{OKU2023123,
title = {Chapter Six - Analyzing teacher–student interactions through graph theory applied to hyperscanning fNIRS data},
editor = {Mariuche Gomides and Isabela Starling-Alves and Flávia H. Santos},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {282},
pages = {123-143},
year = {2023},
booktitle = {Brain and Maths in Ibero-America},
issn = {0079-6123},
doi = {https://doi.org/10.1016/bs.pbr.2023.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0079612323001115},
author = {Amanda Yumi Ambriola Oku and Eneyse Dayane Pinheiro and Raimundo {da Silva Soares} and João Ricardo Sato},
keywords = {Mathematics education, Teaching practice, Graph theory, fNIRS, Hyperscanning},
abstract = {Teacher–student relationships have been found consistently important for student school effectiveness in mathematics in the last three decades. Although this observation is generally made from the teacher's perspective, neuroscience can provide new insights by establishing the neurobiological underpinning of social interactions. This paper further develops this line of research by utilizing graph theory to represent interactions between teachers and students at the neural level. Through hyperscanning with functional near-infrared spectroscopy (fNIRS), we collected data from the prefrontal cortex and the temporoparietal junction of 24 dyads composed of a teacher and a student. Each dyad used a board game to perform a programming logic class that consisted of three steps: independent activities (control), presentation of concepts, and interactive exercises. Graph theory provides results regarding the strength of teacher–student interaction and the main channels involved in these interactions. We combined graph modularity and bootstrap to measure pair coactivation, thus establishing the strength of teacher–student interaction. Also, graph centrality detects the main brain channels involved during this interaction. In general, the teacher's most relevant nodes rely on the regions related to language and number processing, spatial cognition, and attention. Also, the students' most relevant nodes rely on the regions related to task management.}
}
@article{UMAR2024103303,
title = {Physical layer authentication in the internet of vehicles through multiple vehicle-based physical attributes prediction},
journal = {Ad Hoc Networks},
volume = {152},
pages = {103303},
year = {2024},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2023.103303},
url = {https://www.sciencedirect.com/science/article/pii/S1570870523002238},
author = {Mubarak Umar and Jiandong Wang and Feng Li and Shuguang Wang and Minggang Zheng and Zhiwei Zhang and Yulong Shen},
keywords = {Physical layer authentication (PLA), Gaussian process (GP), Machine learning (ML), Internet of vehicles (IoV), GPS spoofing attack},
abstract = {The high security and low complexity of physical layer authentication (PLA) make it a promising complement for complex cryptographic authentication approaches, especially for Internet of Vehicles (IoV) systems constrained by limited computational capabilities and facing increasing security threats due to the broadcast nature of their communications. However, existing PLA schemes exploiting geographical location information to predict channel characteristics for authentication face the challenge of location falsifying/spoofing attacks. Moreover, they either require cryptographic-based initial authentication or cannot be applied to moving vehicle scenarios. To tackle these challenges, we propose a PLA scheme based on the Gaussian process (GP) regression that jointly considers the location and speed attributes of vehicles to enhance the reliability of authentication in the IoV network. Specifically, the historical channel state information attributes together with the location and speed information of transmitters are utilized to establish a mapping and train a GP model to predict the next legitimate location and speed of a transmitter for authentication. First, for vehicle-to-road side unit (RSU) authentication, the trained GP model is stored on RSU and used to authenticate vehicles entering its vicinity by cross-verifying their reported location and speed information with the ones predicted by the model. Next, for vehicle-to-vehicle authentication, we propose an RSU-assisted authentication, where the RSU in the location shared by two vehicles is used to assist in verifying the validity of their reported location and speed information. Finally, for RSU-to-vehicle authentication, we leverage the path loss and angle of arrival of a signal from RSU to estimate and cross-verify its location. We utilize QuaDRiGa, a quasideterministic radio channel generator to generate realistic channels for experimental validations. The results of simulation tests conducted demonstrated that our approach significantly improves authentication performance compared with the existing approaches.}
}
@article{WU2023110852,
title = {Establishing a link between complex courtyard spaces and thermal comfort: A major advancement in evidence-based design},
journal = {Building and Environment},
volume = {245},
pages = {110852},
year = {2023},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2023.110852},
url = {https://www.sciencedirect.com/science/article/pii/S036013232300879X},
author = {Renzhi Wu and Xiaoshan Fang and Robert Brown and Shuang Liu and Huihui Zhao},
keywords = {Spatial indicators, Climate-adaptive design, Courtyard design, Microclimate, Thermal comfort, Rapid simulation technologies},
abstract = {Amidst climate change, the importance of climate-adaptive design in architecture and landscape design has surged, particularly in residential courtyards, where optimizing the microclimate is paramount to residents' well-being. Traditional spatial indices, however, fall short in accurately characterizing complex courtyards and local spatial features. To overcome these limitations, this study introduces pixel-level spatial indicators that effectively overcome these constraints. These indicators are implemented using computational geometry algorithms such as Ray Tracing, Flood Fill, and A*, enabling simulation of various courtyard spatial indicator maps. We also utilize Graphics Processing Unit (GPU)-based rapid thermal comfort simulation technology to generate thermal comfort maps. By applying data mining methods such as Partial Least Squares Regression (PLSR), Pearson correlation, and Nearest-neighbor interpolation, we explore the relationships between spatial indicators and thermal comfort, ultimately identifying key indicators and determining the guiding thresholds and influencing trends corresponding to heat discomfort frequency. Six key indicators and th emerge: Building View Factor (BVF), indicating building coverage visibility (prefer above 0.11); Solar Beam Fraction (BEAM), illustrating Summer solstice sun shading condition (prefer below 0.78); Averaged View Factor (AVF), showing overall visibility (prefer below 0.40); Directional Sky View Factor (DSVF(W)), reflecting sky visibility in a specific orientation (prefer below 0.73); Tree View Factor (TVF), denoting tree coverage visibility (prefer above 0.18); and Plan Water Ratio (PWR), signifying water surface proportion (aim for below 0.44). These insights, integrated into design tools, contribute to evidence-based microclimate regulation strategies, thereby enhancing urban residents' thermal comfort and overall well-being.}
}
@article{STRAUSS2023101782,
title = {Formal vs. intuitive categorization and obsessive-compulsive symptoms},
journal = {Journal of Behavior Therapy and Experimental Psychiatry},
volume = {78},
pages = {101782},
year = {2023},
issn = {0005-7916},
doi = {https://doi.org/10.1016/j.jbtep.2022.101782},
url = {https://www.sciencedirect.com/science/article/pii/S000579162200060X},
author = {Asher Y. Strauss and Isaac Fradkin and Jonathan D. Huppert},
keywords = {Obsessive-compulsive disorder, Reasoning, Categorization, Rule-based, Family resemblance},
abstract = {Background and objectives
Obsessive-compulsive disorder (OCD) is often characterized by rigidity regarding rules and perfectionism, which suggests a formal reasoning style. However, other characterizations suggest an overreliance on internal cues for behavior termination, which suggests a more intuitive reasoning style. We examine reasoning styles in OCD by assessing categorization preferences traditionally classified to rule-based and family resemblance categorization.
Method
An initial study (n = 41) and an online replication (n = 85) were conducted. In both studies, groups scoring high and low on OCD symptoms were compared. Categorization preferences and confidence ratings were examined via a modification of a classic categorization task. The task was administered in three conditions: under time limits, with no time limits, and with explicit explanation of both categorization styles.
Results
Aggregating results from both studies showed that obsessive-compulsive symptoms were associated with a reduced preference for rule-based categorization reflecting a tendency towards a more intuitive, non-formal reasoning style. This preference was apparent even when rules were explicitly described. Group differences regarding confidence were inconclusive.
Limitations
Generalizing results to the clinical population requires further research, and specificity to OC symptoms should be determined.
Conclusions
Challenging the expected association between OCD and rigidity and perfectionism, findings support suggestions that OCD reasoning strays from formal reasoning. This may explain some of the subjective and idiosyncratic rules adopted by individuals with OCD.}
}
@article{LAENG2014263,
title = {Scrutinizing visual images: The role of gaze in mental imagery and memory},
journal = {Cognition},
volume = {131},
number = {2},
pages = {263-283},
year = {2014},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2014.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0010027714000043},
author = {Bruno Laeng and Ilona M. Bloem and Stefania D’Ascenzo and Luca Tommasi},
keywords = {Imagery, Memory, Eye-tracking, Enactment},
abstract = {Gaze was monitored by use of an infrared remote eye-tracker during perception and imagery of geometric forms and figures of animals. Based on the idea that gaze prioritizes locations where features with high information content are visible, we hypothesized that eye fixations should focus on regions that contain one or more local features that are relevant for object recognition. Most importantly, we predicted that when observers looked at an empty screen and at the same time generated a detailed visual image of what they had previously seen, their gaze would probabilistically dwell within regions corresponding to the original positions of salient features or parts. Correlation analyses showed positive relations between gaze’s dwell time within locations visited during perception and those in which gaze dwelled during the imagery generation task. Moreover, the more faithful an observer’s gaze enactment, the more accurate was the observer’s memory, in a separate test, of the dimension or size in which the forms had been perceived. In another experiment, observers saw a series of pictures of animals and were requested to memorize them. They were then asked later, in a recall phase, to answer a question about a property of one of the encoded forms; it was found that, when retrieving from long-term memory a previously seen picture, gaze returned to the location of the part probed by the question. In another experimental condition, the observers were asked to maintain fixation away from the original location of the shape while thinking about the answer, so as to interfere with the gaze enactment process; such a manipulation resulted in measurable costs in the quality of memory. We conclude that the generation of mental images relies upon a process of enactment of gaze that can be beneficial to visual memory.}
}
@article{YAN2021119969,
title = {A mechanistic insight into glucose conversion in subcritical water: Complex reaction network and the effects of acid-base catalysis},
journal = {Fuel},
volume = {289},
pages = {119969},
year = {2021},
issn = {0016-2361},
doi = {https://doi.org/10.1016/j.fuel.2020.119969},
url = {https://www.sciencedirect.com/science/article/pii/S0016236120329653},
author = {Zhifeng Yan and Jie Lian and Yu Feng and Miaoting Li and Feng Long and Ruoqian Cheng and Sheng Shi and Hong Guo and Jianjun Lu},
keywords = {Glucose, Reaction mechanism, Subcritical water, Dispersion-corrected density functional theory, Acid-base bifunctional catalysis},
abstract = {Based on the concept of system thinking and system design, a systematically mechanistic network of glucose conversion toward fructose, 5-hydroxymethyl furfural, 1,6-anhydroglucose, 1,2,4-Benzenetriol, levulinic acid, furfural, erythrose, glyceraldehyde, dihydroxyacetone, pyruvaldehyde, lactic acid, glycolaldehyde in subcritical water were performed by employing dispersion-corrected density functional theory. Fukui functions results predict the most highest reactivity of O(5) of glucose to suffer protonation in subcritical water, which may readily lead to the formation of 1,6-anhydroglucose or fructose with the comparable apparent activation energies (29.441 vs 29.305 kcal/mol). Further dehydration of monosaccharide to 5-HMF is more favorable via cyclic pathway for fructose in comparison to the acyclic pathway for glucose. The formation of levulinic acid has an apparent activation energy of 33.321 kcal/mol but the rate is limited by the numerous steps. The consumption of 5-HMF to 1,2,4-Benzenetriol exhibits a high activation energy of 76.682 kcal/mol. Retro-aldol condensation of C4 compounds prefer to give C2 rather than C3 compounds. The thermodynamic results involving the generation of C2, C3 and C4 compounds by retro-aldol condensation of open-chain C6 intermediates agree with the experimental product distribution and reactivity over temperature at the initial stage of glucose or fructose subcritical hydrolysis. Furthermore, the assistance of H+ may be responsible for the isomerization and retro-aldol condensation in glucose conversion. This comprehensive reaction network provides a fundamental understanding and deeper insight into glucose conversion, which reasonably explains experimental activity and selectivity reported for glucose and fructose conversion in subcritical water.}
}
@article{BENEDETTO2014167,
title = {Rebound effects due to economic choices when assessing the environmental sustainability of wine},
journal = {Food Policy},
volume = {49},
pages = {167-173},
year = {2014},
issn = {0306-9192},
doi = {https://doi.org/10.1016/j.foodpol.2014.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0306919214001250},
author = {Graziella Benedetto and Benedetto Rugani and Ian Vázquez-Rowe},
keywords = {Carbon footprint, Consequential LCA, Indirect effects, Life Cycle Assessment, Sustainable consumption},
abstract = {The identification and working mechanisms of Rebound Effects (REs) have important policy implications. The intensity of these impacts is crucial when it comes to detecting strategies to promote sustainable consumption of food and beverages, as in the case of wine. In fact, neglecting the occurrence of REs in wine production and delivery leads to under- or over-estimating the effects that novel more sustainable technologies may produce. An in-depth analysis on the ways in which the stakeholders may react to the availability of a new product (e.g. wine produced through a process oriented to the reduction of CO2 emissions) may be particularly useful to allow producers and consumers to target the REs with respect to the overall goals of desired sustainability. In this article, we first provide a definition and a classification of different types of REs and then analyse some exemplificative cases applied to the supply and consumption of wine produced through technologies that reduce environmental emissions or resource consumptions. A final step analyses the methodological tools that may be useful when including REs in life cycle thinking as applied to the wine sector.}
}
@article{ARAUJODERESENDE2022169109,
title = {Non-Abelian fusion rules from Abelian systems with SPT phases and graph topological order},
journal = {Annals of Physics},
volume = {446},
pages = {169109},
year = {2022},
issn = {0003-4916},
doi = {https://doi.org/10.1016/j.aop.2022.169109},
url = {https://www.sciencedirect.com/science/article/pii/S0003491622002111},
author = {M.F. {Araujo de Resende} and J.P. {Ibieta Jimenez} and J. {Lorca Espiro}},
keywords = {Two-dimensional lattice model, Non-abelian fusion rule, Condensation mechanism, Global symmetry breaking, Quantum computation},
abstract = {Since Padmanabhan and Teotonio-Sobrinho (2015) show the emergence of non-Abelian fusion rules in some examples of a class of Abelian models, but does not prove whether these rules also exist in other cases, the purpose of this paper is to present such proof emphasizing the importance of the existence of these rules. By the way, as the ground state of these models can be degenerated as a function of their algebra and, hence, they can support some symmetry-protected topological (SPT) phases, we prove that these non-Abelian fusion rules are always necessary for these SPT phase transitions to occur via a condensation mechanism or/and some global symmetry breaking.}
}
@article{PEELEN2017177,
title = {Category selectivity in human visual cortex: Beyond visual object recognition},
journal = {Neuropsychologia},
volume = {105},
pages = {177-183},
year = {2017},
note = {Special Issue: Concepts, Actions and Objects: Functional and Neural Perspectives},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2017.03.033},
url = {https://www.sciencedirect.com/science/article/pii/S0028393217301215},
author = {Marius V. Peelen and Paul E. Downing},
keywords = {Category selectivity, Ventral stream, Object representation, Scene perception},
abstract = {Human ventral temporal cortex shows a categorical organization, with regions responding selectively to faces, bodies, tools, scenes, words, and other categories. Why is this? Traditional accounts explain category selectivity as arising within a hierarchical system dedicated to visual object recognition. For example, it has been proposed that category selectivity reflects the clustering of category-associated visual feature representations, or that it reflects category-specific computational algorithms needed to achieve view invariance. This visual object recognition framework has gained renewed interest with the success of deep neural network models trained to “recognize” objects: these hierarchical feed-forward networks show similarities to human visual cortex, including categorical separability. We argue that the object recognition framework is unlikely to fully account for category selectivity in visual cortex. Instead, we consider category selectivity in the context of other functions such as navigation, social cognition, tool use, and reading. Category-selective regions are activated during such tasks even in the absence of visual input and even in individuals with no prior visual experience. Further, they are engaged in close connections with broader domain-specific networks. Considering the diverse functions of these networks, category-selective regions likely encode their preferred stimuli in highly idiosyncratic formats; representations that are useful for navigation, social cognition, or reading are unlikely to be meaningfully similar to each other and to varying degrees may not be entirely visual. The demand for specific types of representations to support category-associated tasks may best account for category selectivity in visual cortex. This broader view invites new experimental and computational approaches.}
}
@article{LEHMANN20101073,
title = {Core networks for visual-concrete and abstract thought content: A brain electric microstate analysis},
journal = {NeuroImage},
volume = {49},
number = {1},
pages = {1073-1079},
year = {2010},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2009.07.054},
url = {https://www.sciencedirect.com/science/article/pii/S1053811909008441},
author = {Dietrich Lehmann and Roberto D. Pascual-Marqui and Werner K. Strik and Thomas Koenig},
abstract = {Commonality of activation of spontaneously forming and stimulus-induced mental representations is an often made but rarely tested assumption in neuroscience. In a conjunction analysis of two earlier studies, brain electric activity during visual-concrete and abstract thoughts was studied. The conditions were: in study 1, spontaneous stimulus-independent thinking (post-hoc, visual imagery or abstract thought were identified); in study 2, reading of single nouns ranking high or low on a visual imagery scale. In both studies, subjects' tasks were similar: when prompted, they had to recall the last thought (study 1) or the last word (study 2). In both studies, subjects had no instruction to classify or to visually imagine their thoughts, and accordingly were not aware of the studies' aim. Brain electric data were analyzed into functional topographic brain images (using LORETA) of the last microstate before the prompt (study 1) and of the word-type discriminating event-related microstate after word onset (study 2). Conjunction analysis across the two studies yielded commonality of activation of core networks for abstract thought content in left anterior superior regions, and for visual-concrete thought content in right temporal-posterior inferior regions. The results suggest that two different core networks are automatedly activated when abstract or visual-concrete information, respectively, enters working memory, without a subject task or instruction about the two classes of information, and regardless of internal or external origin, and of input modality. These core machineries of working memory thus are invariant to source or modality of input when treating the two types of information.}
}
@incollection{2024xiii,
title = {List of figures},
editor = {Zongxiang Lu and Haibo Li and Ying Qiao and Le Xie and Chanan Singh},
booktitle = {Power System Flexibility},
publisher = {Academic Press},
pages = {xiii-xvi},
year = {2024},
isbn = {978-0-323-99517-7},
doi = {https://doi.org/10.1016/B978-0-323-99517-7.00014-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323995177000142}
}
@article{JIANG2023110445,
title = {Hierarchical multi-UAVs task assignment based on dominance rough sets},
journal = {Applied Soft Computing},
volume = {143},
pages = {110445},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110445},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623004635},
author = {Haihuan Jiang and Guoyin Wang and Qun Liu and Peng Gao and Xin Huang},
keywords = {Dominance rough sets, Multi-UAVs, Task assignment, Optimal ranking strategy},
abstract = {The task assignment of multi-UAVs is important to realize the effective cooperation and enhance the comprehensive performance of multi-UAVs. Heuristic algorithms have become popular methods to solve this problem. As random searching algorithms, the performance of existing heuristic algorithms degrades when solving large-scale or high-dimensional problems, and depends on additional parameter settings. Unlike the idea of heuristic algorithms approximating the optimal solution by searching for the solution space, this paper takes a new approach of gradually acquiring the assignment result based on human cognition, called a hierarchical multi-UAVs task assignment by the optimal ranking strategy based on dominance rough sets (HORD). HORD first establishes the optimal ranking strategy for task assignment of all UAVs based on the dominance rough set theory. Then, a hierarchical multi-UAV task assignment algorithm is built. Comparative experimental results verify that the HORD algorithm speeds up the assignment and increases the total utility without relying on additional parameters.}
}
@article{ZENG2023165061,
title = {Estimation of ground-level O3 concentration in the Yangtze River Delta region based on a high-performance spatiotemporal model MixNet},
journal = {Science of The Total Environment},
volume = {896},
pages = {165061},
year = {2023},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2023.165061},
url = {https://www.sciencedirect.com/science/article/pii/S0048969723036847},
author = {Qiaolin Zeng and Yechen Wang and Jinhua Tao and Meng Fan and Songyan Zhu and Liangfu Chen and Lihui Wang and Yeming Li},
keywords = {O, Point-plane, Spatiotemporal model, Attention mechanism, Yangtze River delta},
abstract = {In recent years, the escalating ozone (O3) concentration has significantly damaged human health. The machine learning models are widely used to estimate ground-level O3 concentrations, but the spatial and temporal features in the data are less considered. To address the issue, this study proposed a novel framework named MixNet to estimate daily O3 concentration from 2020 to 2021 over the Yangtze River Delta. The MixNet utilized image convolution to extract the potential spatial information related to O3 fully. The temporal features were extracted by a Long Short-Term Memory (LSTM). A U-Net, a new jump connection method with an attention mechanism and residual blocks, facilitated a more comprehensive extraction of spatial features in the data. The extracted temporal and spatial features were fused to estimate ground-level O3. Meanwhile, a novel training method was proposed to enhance the accuracy of MixNet. The daily mean O3 maps have high validation results in comparison with ground-level O3 measurement, with R2 (RMSE) of 0.903 (14.511 μg/m3) for sample-based validation, 0.831 (19.036 μg/m3) for site-based validation, and 0.712 (25.108 μg/m3) for time-based validation. The season-average maps indicate that O3 concentration is summer > autumn > spring > winter. The highest value was 137.41 μg/m3 in the summer of 2021 over the Yangtze River Delta urban agglomeration, and the lowest value was 52.73 μg/m3 in winter 2020. The MixNet showed better performance compared with other models, and thus the “point-plane image thinking” will contribute to future studies in developing better methods to estimate atmospheric pollutants.}
}
@article{WALSH2024,
title = {The changing landscape with respect to scientific research and education for second-level students and how they can overlap: the Kefir4All example},
journal = {Journal of Microbiology & Biology Education},
volume = {25},
number = {2},
year = {2024},
issn = {1935-7877},
doi = {https://doi.org/10.1128/jmbe.00058-24},
url = {https://www.sciencedirect.com/science/article/pii/S1935787724000388},
author = {Liam H. Walsh and Cian {O' Mahony} and Paul D. Cotter},
keywords = {citizen science, public engagement, education, fermentation, microbiology, kefir, participant voice},
abstract = {ABSTRACT

Have you ever deeply considered the intersections between research and education, particularly for second-level students? Traditionally, the convergence of these two realms is most often noted when considering the integration of research findings into educational practices or the involvement of, typically a small number, of students in research activities. While these practices have demonstrated efficacy, the fields of scientific research and education are evolving rapidly, necessitating a reevaluation of how we can optimize their convergence. In our discourse, we delve into these evolving trends, uncover the potential for greater integration, and, ultimately, enhance outcomes using the citizen science initiative Kefir4All as an illustrative example.}
}
@article{NALBANT2024e23698,
title = {A methodology for personnel selection in business development: An interval type 2-based fuzzy DEMATEL-ANP approach},
journal = {Heliyon},
volume = {10},
number = {1},
pages = {e23698},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e23698},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023109066},
author = {Kemal Gokhan Nalbant},
keywords = {Fuzzy logic, DEMATEL-ANP (DANP), Personnel selection, Multicriteria decision-making, Sensitivity analysis},
abstract = {The employee selection procedure holds significant importance for every company or organization involved in current economic activities. It is vital to underscore staff selection since it plays a crucial role in determining a firm's success. When a corporation selects a suitable applicant for a position and initiates the recruiting process, it might mark a critical point for the firm and substantially impact its functioning. In the present study, I utilized the trapezoidal interval type-2 (IT2) fuzzy Decision-Making Trial and Evaluation Laboratory (DEMATEL) - Analytic Network Process (ANP) methodology to conduct person selection. Additionally, statistically significant values for both the primary and secondary criteria related to staff recruitment were determined. The employment of square comparison matrices is essential for implementing the DEMATEL-ANP approach. The number of subcriteria for this application is equal to that of the alternative. The approach was successfully executed without facing any obstacles. The aim of this research was to investigate the viability of employing IT2 Fuzzy Systems to handle and control uncertainty efficiently. In addition, the implementation of sensitivity analysis plays a vital role in the context of multicriteria decision-making (MCDM) techniques.}
}
@article{VERNER20242950,
title = {Student learning of engineering systems through simulation-based design using Onshape and Blender},
journal = {Procedia Computer Science},
volume = {232},
pages = {2950-2958},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.02.111},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924002898},
author = {Igor Verner and Dan Cuperman and Matthew Mueller},
keywords = {Introductory engineering education, simulation-based design, cloud based CAD, Onshape, intermediate axis theorem},
abstract = {This study examines how the introductory engineering course can engage students in learning Industry 4.0 concepts and enhance their digital technology skills through simulation-based design activities. The students used cloud-based design and simulation tools to resolve a system operation problem. They examined the motion of a digital model of a satellite under micro-gravity conditions and detected its instability (Dzhanibekov effect). Then, they redesigned the model and verified its motion stability through simulation. The experimental setup combined the cloud-based CAD platform Onshape and the simulation software Blender. The students’ performances, understanding, and perceptions were evaluated using conventional research tools and the data analytics tools of Onshape. Findings showed that the students successfully performed the workshop assignment, correctly answered the questions about the factors that cause motion instability and appreciated the contribution of the workshop. The study demonstrated the potential of online simulation-based design practices for educating novice engineering students.}
}
@article{PEREIRA2014126,
title = {Analytical model for calculating indeterminate results interval of screening tests, the effect on seroconversion window period: A brief evaluation of the impact of uncertain results on the blood establishment budget},
journal = {Transfusion and Apheresis Science},
volume = {51},
number = {2},
pages = {126-131},
year = {2014},
issn = {1473-0502},
doi = {https://doi.org/10.1016/j.transci.2014.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S1473050214001736},
author = {Paulo Pereira and James O. Westgard and Pedro Encarnação and Jerard Seghatchian},
keywords = {Bias, Delta-value, Precision, Seroconversion window period, Total analytical error},
abstract = {The evaluation of measurement uncertainty is not required by the European Union regulation for blood establishments' laboratory tests. However, it is required for tests accredited by ISO 15189. Also, the forthcoming ISO 9001 edition requires “risk based thinking” with risk described as “the effect of uncertainty on an expected result”. ISO recommends GUM models for determination of measurement uncertainty, but their application is not intended for ordinal value measurements, such as what happens with screening test binary results. This article reviews, discusses and proposes concepts intended for measurement uncertainty of screening test results. The precision model focuses on cutoff level allowing the evaluation of the indeterminate interval using analytical sources of variance. The interval is considered in the estimation of the seroconversion window period. The delta-value of patients and healthy subjects' samples allows ranking two tests according to the probability of the two classes of indeterminate results: chance of false negative results and chance of false positive results (waste on budget).}
}
@incollection{GABBIANI20171,
title = {Chapter 1 - Introduction},
editor = {Fabrizio Gabbiani and Steven James Cox},
booktitle = {Mathematics for Neuroscientists (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {1-8},
year = {2017},
isbn = {978-0-12-801895-8},
doi = {https://doi.org/10.1016/B978-0-12-801895-8.00001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128018958000014},
author = {Fabrizio Gabbiani and Steven James Cox},
keywords = {Book structure, Chapter dependencies, Purkinje cell, Brain Facts, Mathematical preliminaries, Units, Neuron, Neuroethology},
abstract = {Faced with the seemingly limitless qualities of the brain, neuroscience has eschewed provincialism and instead pursued a broad tack that openly draws on insights from biology, physics, chemistry, engineering, psychology and mathematics in its construction of technologies and theories with which to probe and understand the brain. These technologies and theories, in turn, continue to attract scientists and mathematicians to questions of neuroscience. As a result, we may trace over one hundred years of fruitful interplay between neuroscience and mathematics. This text aims to prepare the advanced undergraduate or beginning graduate student to take an active part in this dialogue via the application of existing, or the creation of new, mathematics in the interest of a deeper understanding of the brain. This text aims to prepare the advanced undergraduate or beginning graduate student to take an active part in this dialogue via the application of existing, or the creation of new, mathematics in the interest of a deeper understanding of the brain. Requiring no more than one year of Calculus, and no prior exposure to Neuroscience, we prepare the student by (I) introducing mathematical and computational tools in precisely the contexts that first established their importance for neuroscience, and, (II) developing these tools in concrete incremental steps within a common computational environment. As such, the text may also serve to introduce Neuroscience to readers with a mathematical and/or computational background.}
}
@article{TAMM2016251,
title = {Slow sluggish cognitive tempo symptoms are associated with poorer academic performance in children with ADHD},
journal = {Psychiatry Research},
volume = {242},
pages = {251-259},
year = {2016},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2016.05.054},
url = {https://www.sciencedirect.com/science/article/pii/S0165178115304686},
author = {Leanne Tamm and Annie A. Garner and Richard E.A. Loren and Jeffery N. Epstein and Aaron J. Vaughn and Heather A. Ciesielski and Stephen P. Becker},
keywords = {Learning difficulties, School children, Apathy/disinterest, Slowed behavior/thinking},
abstract = {Sluggish cognitive tempo (SCT) symptoms may confer risk for academic impairment in attention-deficit/hyperactivity disorder (ADHD). We investigated SCT in relation to academic performance and impairment in 252 children (ages 6–12, 67% boys) with ADHD. Parents and teachers completed SCT and academic impairment ratings, and achievement in reading, math, and spelling was assessed. Simultaneous regressions controlling for IQ, ADHD, and comorbidities were conducted. Total SCT predicted parent-rated impairments in writing, mathematics, and overall school but not reading. Parent-rated SCT Slow predicted poorer reading and spelling, but not math achievement. Teacher-rated SCT Slow predicted poorer spelling and math, but not reading achievement. Parent-rated SCT Slow predicted greater academic impairment ratings across all domains, whereas teacher-rated SCT Slow predicted greater impairment in writing only. Age and gender did not moderate these relationships with the exception of math impairment; SCT slow predicted math impairment for younger but not older children. Parent and teacher SCT Sleepy and Daydreamy ratings were not significant predictors. SCT Slow appears to be uniquely related to academic problems in ADHD, and may be important to assess and potentially target in intervention. More work is needed to better understand the nature of SCT Slow symptoms in relation to inattention and amotivation.}
}
@incollection{LUO2015401,
title = {18 - Factor Investing and Portfolio Construction Techniques},
editor = {Emmanuel Jurczenko},
booktitle = {Risk-Based and Factor Investing},
publisher = {Elsevier},
pages = {401-433},
year = {2015},
isbn = {978-1-78548-008-9},
doi = {https://doi.org/10.1016/B978-1-78548-008-9.50018-6},
url = {https://www.sciencedirect.com/science/article/pii/B9781785480089500186},
author = {Yin Luo and Spyros Mesomeris},
keywords = {Alpha parameter, Asset-class allocation, Beta portfolios, Copula model, CVaR optimization theory, CVaR portfolio, Efficient frontier analysis, Portfolio construction, Risk-based portfolio, Risk premia},
abstract = {Factor investing has been growing in popularity within institutional investment circles over the last few years. At the same time, risk-based portfolio construction techniques have become more mainstream, particularly following the global financial crisis of 2008. We cannot be blamed for thinking that these two concepts are very closely related: factor investing is about understanding the sources of risk that underlie a particular portfolio and taking investment decisions directly at the factor level, while risk-based portfolio construction techniques can be used to put together risk-factor portfolios. However, we will argue that factor investing is ultimately an “asset allocation” concept (even though it may not be directly used as such), whereas risk-based portfolio construction is a methodology that can be pursued in building risk-factor portfolios; in fact, under certain assumptions, it may actually be the optimal technique. Risk-based portfolio construction methods are readily used outside the risk-factor area, and, at the same time, risk-factor portfolios can be constructed through a number of different approaches such as mean-variance optimization, etc.}
}
@incollection{CLARK2011911,
title = {20 - On the Learnability of Quantifiers**Many thanks for comments from Johan van Benthem, Ed Keenan, Jakub Szymanik and Dag Westerståhl. This work was supported by NIH grant NS44266. (Update of Chapter 19)},
editor = {Johan {van Benthem} and Alice {ter Meulen}},
booktitle = {Handbook of Logic and Language (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {London},
pages = {911-923},
year = {2011},
isbn = {978-0-444-53726-3},
doi = {https://doi.org/10.1016/B978-0-444-53726-3.00020-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780444537263000207},
author = {Robin Clark},
keywords = {generalized quantifiers, identifiability in the limit, PAC-learning, computational complexity},
abstract = {Publisher Summary
This chapter discusses computational properties of quantifiers and, in particular, how a learner could come to associate specific determiners with their denotations on the basis of experience. It also highlights the correspondence between natural language quantifiers and formal automata. Quantifiers are used and understood by finite computational agents. The computational approach to quantifiers promises to advance one's understanding of quantifiers both within theoretical computer science—where the descriptive complexity of quantifiers can tell one about the time and space complexity of queries with different quantifiers—and psycholinguistics—how humans process quantifiers and whether there are hard computational constraints on one's understanding of quantifiers. Recently, Mostowski and Wojtyniak have shown that branching interpretation of quantifiers as NP-complete. More recently, Szymanik has provided a detailed taxonomy of a wide variety of generalized quantifiers in natural language—from different distributive constructions to collective quantification—in terms of both time and space complexity.}
}
@article{BUCAIONI2024100526,
title = {Programming with ChatGPT: How far can we go?},
journal = {Machine Learning with Applications},
volume = {15},
pages = {100526},
year = {2024},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2024.100526},
url = {https://www.sciencedirect.com/science/article/pii/S2666827024000021},
author = {Alessio Bucaioni and Hampus Ekedahl and Vilma Helander and Phuong T. Nguyen},
keywords = {ChatGPT, Large language models, Programming},
abstract = {Artificial intelligence (AI) has made remarkable strides, giving rise to the development of large language models such as ChatGPT. The chatbot has garnered significant attention from academia, industry, and the general public, marking the beginning of a new era in AI applications. This work explores how well ChatGPT can write source code. To this end, we performed a series of experiments to assess the extent to which ChatGPT is capable of solving general programming problems. Our objective is to assess ChatGPT’s capabilities in two different programming languages, namely C++ and Java, by providing it with a set of programming problem, encompassing various types and difficulty levels. We focus on evaluating ChatGPT’s performance in terms of code correctness, run-time efficiency, and memory usage. The experimental results show that, while ChatGPT is good at solving easy and medium programming problems written in C++ and Java, it encounters some difficulties with more complicated tasks in the two languages. Compared to code written by humans, the one generated by ChatGPT is of lower quality, with respect to runtime and memory usage.}
}
@article{SHARMA2021100174,
title = {Curcumin analogs as anti-cathepsins agents: Designing, virtual screening, and molecular docking analysis},
journal = {Computational Toxicology},
volume = {19},
pages = {100174},
year = {2021},
issn = {2468-1113},
doi = {https://doi.org/10.1016/j.comtox.2021.100174},
url = {https://www.sciencedirect.com/science/article/pii/S2468111321000220},
author = {Kavita Sharma and Neera Raghav},
keywords = {Alzheimer, ADME, Cathepsins Inhibitors, Curcumin analogs, Docking, Toxicity},
abstract = {Anti-cathepsin B agents can be potential drug candidates in the prevention and cure of Alzheimer's disease. The enzyme has been found to be involved in amyloid-β, accumulation which has been considered to be one of the possible causes of Alzheimer’s disease. Curcumin, a biologically active ingredient of the natural product turmeric is reported as an anti-Alzheimer agent. Its low solubility and low bioavailability limit its use as a drug. In our present study, we designed and analyzed curcumin analogs as potent anti-cathepsin agents using computational methods. Drug-likeness properties, Bioactivity score, ADME properties were determined. Thereafter toxicity assessment and molecular docking were carried out. In addition, cathepsin B interaction studies with structurally related cathepsin H and L are also reported.}
}
@incollection{JAIN2023183,
title = {Chapter 10 - QSAR and ANN-based molecular modeling},
editor = {Dakeshwar Kumar Verma and Chandrabhan Verma and Jeenat Aslam},
booktitle = {Computational Modelling and Simulations for Designing of Corrosion Inhibitors},
publisher = {Elsevier},
pages = {183-199},
year = {2023},
isbn = {978-0-323-95161-6},
doi = {https://doi.org/10.1016/B978-0-323-95161-6.00006-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323951616000060},
author = {Bhawana Jain and Reena Rawat},
keywords = {Molecular modeling, corrosion, QSAR (quantitative structure and activity relationship), ANN (artificial neural network)},
abstract = {Theoretical assessment of corrosion inhibition potentials of organic compounds has benefited from the molecular modeling of organic molecules utilizing computer tools. Density functional theory, molecular dynamics (MD), and Monte Carlo simulations, artificial neural networks (ANNs), and quantitative structure–activity relationships (QSAR) are some of the typical methodologies used in theoretical research of corrosion inhibition potentials and processes. QSAR is a computer model for describing and forecasting drug interactions and surface interactions. Furthermore, an ANN was utilized to build linear and sigmoidal functionals with the goal of predicting low-carbon steel, copper, and aluminum corrosion rates based on environmental variables. Chemical reactivity and corrosion inhibition actions of organic molecules can be described via computer modeling. The modeling can be regarded as a time-saving and eco-friendly approach of screening organic compounds for corrosion inhibition potentials before their wet laboratory synthesis would be carried out. Another advantage of computational modeling is that molecular sites responsible for interactions with metallic surfaces (active sites or adsorption sites) and the orientation of organic compounds can be easily predicted. Using different theoretical descriptors/parameters, inhibition effectiveness and nature of the metal-inhibitor interactions can also be predicted. This chapter emphasizes the design and testing of corrosion inhibition effectiveness of organic corrosion inhibitors. QSAR aims to draw attention to the link between the effectiveness of prevention (any function) and the structural features (adjectives). It involves the finding of one or more items that, by mathematical equation, link these definitions to their blocking function. ANN shows excellent performance in the prediction (output) for various complex characteristic data (input). The obtained results give deep insight into the corrosion systems by analyzing point of surface corrosion attack, the more stable site of the inhibitor adsorption, and the binding power of the adsorbed coating.}
}
@incollection{SHIELDS1986517,
title = {Dialectics, Dialogue, and Social Transmission of Knowledge},
editor = {I. Kurcz and G.W. Shugar and J.H. Danks},
series = {Advances in Psychology},
publisher = {North-Holland},
volume = {39},
pages = {517-539},
year = {1986},
booktitle = {Knowledge and Language},
issn = {0166-4115},
doi = {https://doi.org/10.1016/S0166-4115(09)60152-7},
url = {https://www.sciencedirect.com/science/article/pii/S0166411509601527},
author = {Mauree M. Shields and Gerard Duveen},
abstract = {Abstract
The study of pragmatics or language in use has opened up the interface between language, thought and the social context in which both are embedded. It is contended in this chapter that traditional cognitive studies with their focus on intra-individual processes are inadequate to account for the nature of our thinking. A theoretical shift is necessary toward the location of our thinking processes in social dialogue. The effort to understand and influence others in conversation is a primary factor in intra-individual thought processes which take the form of an internal dialogue as much affected by wants, needs, and evaluation as by logic and factual inference. Some evidence is presented from work with young children focused on their social thinking.}
}
@article{CONTRERASKALLENS2018765,
title = {Cultural evolution of categorization},
journal = {Cognitive Systems Research},
volume = {52},
pages = {765-774},
year = {2018},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2018.08.026},
url = {https://www.sciencedirect.com/science/article/pii/S1389041718301049},
author = {Pablo Andrés {Contreras Kallens} and Rick Dale and Paul E. Smaldino},
abstract = {Categorization is a fundamental function of minds, with wide ranging implications for the rest of the cognitive system. In humans, categories are shared and communicated between minds, thus requiring explanations at the population level. In this paper, we discuss the current state of research on the cultural evolution of categorization. We begin by delineating key properties of categories in need of evolutionary explanation. We then review computational modeling and laboratory studies of category evolution, including their major insights and limitations. Finally, we discuss remaining challenges for understanding the cultural evolution of categorization.}
}
@article{MITCHELL200666,
title = {Mentalizing and Marr: An information processing approach to the study of social cognition},
journal = {Brain Research},
volume = {1079},
number = {1},
pages = {66-75},
year = {2006},
note = {Multiple Perspectives on the Psychological and Neural Bases of Understanding Other People's Behavior},
issn = {0006-8993},
doi = {https://doi.org/10.1016/j.brainres.2005.12.113},
url = {https://www.sciencedirect.com/science/article/pii/S0006899306000072},
author = {Jason P. Mitchell},
keywords = {Social cognition},
abstract = {To interact successfully, individuals must not only recognize one another as intentional agents driven primarily by internal mental states, but also possess a system for making reliable and useful inferences about the nature of those beliefs, feelings, goals, and dispositions. The ability to make such mental state inferences (i.e., to mentalize or mindread) is the central accomplishment of human social cognition. The present article suggests that our understanding of how humans go about making mental state inferences will benefit from treating social cognition primarily as an information processing system that comprises a set of mechanisms for elaborating more basic social information into an understanding of another's mind. Following Marr's [Marr, D., 1982. Vision. W. H. Freeman, San Francisco, CA] framework for the study of such information processing systems, I suggest that questions about social cognition might profitably be asked at three levels – computation, algorithm, and implementation – and outline a number of ways in which a description of social cognition at the middle level (i.e., the step-by-step processes that give rise to mental state inferences) can be informed by analysis at the other two.}
}
@article{MARK2011354,
title = {Genital Herpes Testing Among Persons Living With HIV},
journal = {Journal of the Association of Nurses in AIDS Care},
volume = {22},
number = {5},
pages = {354-361},
year = {2011},
issn = {1055-3290},
doi = {https://doi.org/10.1016/j.jana.2011.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S1055329011000057},
author = {Hayley D. Mark and Marguerite Lucea and Joy P. Nanda and Jason E. Farley and Lisa Gilbert},
keywords = {genital herpes, herpes simplex virus, HIV, serological screening},
abstract = {This cross-sectional survey explored the frequency of genital herpes testing among 110 people living with HIV (PLWH) and reported barriers and facilitators related to testing. Forty-four percent of the respondents had not been tested for genital herpes since receiving an HIV diagnosis, 34% had been tested, and 22% preferred not to say. Respondents’ most frequently cited factors affecting a decision to not be tested were: (a) testing not being recommended by a provider, (b) not having herpes symptoms, and (c) not thinking they had herpes. Data from this study indicated that PLWH were not frequently tested for genital herpes; there was a limited understanding of the frequently subclinical nature of infection; and provider recommendations for testing, or lack thereof, affected testing decisions.}
}
@article{LOPEZIBOR2008278,
title = {The perception of emotion-free faces in schizophrenia: A magneto-encephalography study},
journal = {Schizophrenia Research},
volume = {98},
number = {1},
pages = {278-286},
year = {2008},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2007.09.016},
url = {https://www.sciencedirect.com/science/article/pii/S0920996407003969},
author = {Juan J. López-Ibor and María-Inés López-Ibor and María-Andreína Méndez and María-Dolores Morón and Laura Ortiz-Terán and Alberto Fernandez and Marina Diaz-Marsá and Tomás Ortiz},
keywords = {Schizophrenia, Delusion, Fusiform gyrus, Fusifom face area, Temporal lobe, Perception of faces, Magneto-encephalography (MEG)},
abstract = {Objective
To analyze how patients suffering from schizophrenia perceive faces of unknown individuals that show no actual emotions in order to investigate the attribution of meanings to a relatively non-significant but complex sensory experience.
Design
Analysis of baseline and poststimulation magneto-encephalographic recordings. The stimuli consisted of four pictures with neutral emotional expression of male and female faces of Spanish individuals unknown to research subjects.
Participants
Twelve right-handed patients suffering from schizophrenia (DSM IV-TR criteria), age 18–65, with active delusional activity (SAPS score in delusions above 39) and 15 right-handed sex- and age-matched control subjects.
Results
Compared to controls patients have a significant higher activity of both hemispheres (0–700 ms) being the activity in the right hemisphere (RH) higher than in the left hemisphere (LH). Patients also have a higher activity on the middle fusiform gyrus (BA 37) in the LH (200–300 ms), on the superior temporal areas (BA 22, 41 and 42) in both hemispheres (100–700 ms) and on the temporal pole (BA 38) in the RH (300–400 ms) and a lower activity in the LH of the latter.
Conclusions
The areas that are more activated in our study are those involved in the process of thinking, in attributing meanings to perceptions and in activities such as theory of mind, which are essential for social interaction. The anterior temporal areas less activated indicate a reduced semantic memory for faces that could explain the social withdrawal of schizophrenia. These alterations are suggestive of a dysfunction of left hemisphere neuronal networks.}
}
@article{ALTHANI2023120,
title = {Tridiagonal maximum-entropy sampling and tridiagonal masks},
journal = {Discrete Applied Mathematics},
volume = {337},
pages = {120-138},
year = {2023},
issn = {0166-218X},
doi = {https://doi.org/10.1016/j.dam.2023.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0166218X2300149X},
author = {Hessa Al-Thani and Jon Lee},
keywords = {Nonlinear combinatorial optimization, Covariance matrix, Differential entropy, Maximum-entropy sampling, Dynamic programming, Local search, Spider, Arrowhead, Tridiagonal, Mask, Correlation matrix},
abstract = {The NP-hard maximum-entropy sampling problem (MESP) seeks a maximum (log-) determinant principal submatrix, of a given order, from an input covariance matrix C. We give an efficient dynamic-programming algorithm for MESP when C (or its inverse) is tridiagonal and generalize it to the situation where the support graph of C (or its inverse) is a spider graph with a constant number of legs (and beyond). We give a class of arrowhead covariance matrices C for which a natural greedy algorithm solves MESP. A mask M for MESP is a correlation matrix with which we pre-process C, by taking the Hadamard product M∘C. Upper bounds on MESP with M∘C give upper bounds on MESP with C. Most upper-bounding methods are much faster to apply, when the input matrix is tridiagonal, so we consider tridiagonal masks M (which yield tridiagonal M∘C). We make a detailed analysis of such tridiagonal masks, and develop a combinatorial local-search based upper-bounding method that takes advantage of fast computations on tridiagonal matrices.}
}
@article{2024i,
title = {Advisory Board and Contents},
journal = {Trends in Cognitive Sciences},
volume = {28},
number = {10},
pages = {i-ii},
year = {2024},
issn = {1364-6613},
doi = {https://doi.org/10.1016/S1364-6613(24)00232-8},
url = {https://www.sciencedirect.com/science/article/pii/S1364661324002328}
}
@article{BONNARDEL2005422,
title = {Towards supporting evocation processes in creative design: A cognitive approach},
journal = {International Journal of Human-Computer Studies},
volume = {63},
number = {4},
pages = {422-435},
year = {2005},
note = {Computer support for creativity},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2005.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S1071581905000455},
author = {Nathalie Bonnardel and Evelyne Marmèche},
keywords = {Creativity, Design, Problem solving, Expertise, Analogy, Evocation process, Support systems},
abstract = {In order to contribute to a better understanding of creativity in non-routine design activities, we conducted an experimental study that focused on a cognitive mechanism involved in creative design, that of the re-use of aspects derived from previous sources of inspiration. Our objective was to determine to what extent designers consider potential sources as useful for solving a specific design problem. Since the relevance of sources of inspiration may be appreciated differently according to the level of expertise in design, the experiment was performed with two groups of participants: experienced designers and inexperienced designers. The results show differences in the number and nature of the aspects selected by each group of designers as well as in the judgments of usefulness they expressed about the different types of suggested sources of inspiration. On this basis, we discuss how these findings may influence the design of a computational system supporting creative design tasks and we consider how to facilitate the progression from novices to experienced designers.}
}
@article{HUANG2012112,
title = {Intron identification approaches based on weighted features and fuzzy decision trees},
journal = {Computers in Biology and Medicine},
volume = {42},
number = {1},
pages = {112-122},
year = {2012},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2011.10.015},
url = {https://www.sciencedirect.com/science/article/pii/S0010482511002113},
author = {Yin-Fu Huang and Ching-Ping Liang and Sing-Wu Liou},
keywords = {Intron identification, Fuzzy decision tree, Intronic sequence feature, Computation-oriented intron definition model, Self-organizing map},
abstract = {Current computational predictions of splice sites largely depend on the sequence patterns of known intronic sequence features (ISFs) described in the classical intron definition model (IDM). The computation-oriented IDM (CO-IDM) clearly provides more specific and concrete information for describing intron flanks of splice sites (IFSSs). In the paper, we proposed a novel approach of fuzzy decision trees (FDTs) which utilize (1) weighted ISFs of twelve uni-frame patterns (UFPs) and forty-five multi-frame patterns (MFPs) and (2) gain ratios to improve the performances in identifying an intron. First, we fuzzified extracted features from genomic sequences using membership functions with an unsupervised self-organizing map (SOM) technique. Then, we brought in different viewpoints of globally weighting and crossly referring in generating fuzzy rules, which are interpretable and useful for biologists to verify whether a sequence is an intron or not. Finally, the experimental results revealed the effectiveness of the proposed method in improving the identification accuracy. Besides, we also implemented an on-line intronic identifier to infer an unknown genomic sequence.}
}
@article{KENNEDY2024101437,
title = {Dynamics of neural activity in early nervous system evolution},
journal = {Current Opinion in Behavioral Sciences},
volume = {59},
pages = {101437},
year = {2024},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2024.101437},
url = {https://www.sciencedirect.com/science/article/pii/S2352154624000883},
author = {Ann Kennedy and Brandon Weissbourd},
abstract = {New techniques for large-scale neural recordings from diverse animals are reshaping comparative systems neuroscience. This growth necessitates fresh conceptual paradigms for comparing neural circuits and activity patterns. Here, we take a systems neuroscience approach to early neural evolution, emphasizing the importance of considering nervous systems as multiply modulated, continuous dynamical systems. We argue that endogenous neural activity likely arose early in evolution to organize behaviors and internal states at the organismal level. This connects to a rich literature on the physiology of endogenous activity in small neural circuits: a field that has built links between data and dynamical systems models. Such models offer mechanistic insight and have robust predictive power. Using these tools, we suggest that the emergence of intrinsically active neurons and periodic dynamics played a critical role in the ascendancy of nervous systems and that dynamical systems present an appealing framework for comparing across species.}
}
@incollection{GUERRERO2023181,
title = {Chapter 7 - Artificial general intelligence},
editor = {José María Guerrero},
booktitle = {Mind Mapping and Artificial Intelligence},
publisher = {Academic Press},
pages = {181-195},
year = {2023},
isbn = {978-0-12-820119-0},
doi = {https://doi.org/10.1016/B978-0-12-820119-0.00009-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128201190000091},
author = {José María Guerrero},
keywords = {AGI, AI, Artificial Intelligence, Artificial superintelligence, ASI, General artificial intelligence},
abstract = {This chapter is a short introduction to artificial general intelligence (AGI). It is complementary to Chapter 6 on narrow artificial intelligence (NAI). It includes a short history or the origins of the ideas about AGI, its foundations, the path to build an AGI, the concept of artificial superintelligence (ASI), the concept of ASI singularity, the risks of AGI and ASI, and a short summary of the possible future applications of mind mapping to AGI and ASI. Considerations have been made for readers who are practitioners of mind mapping and who are not necessarily experts in artificial intelligence (AI).}
}
@article{COENRAAD2024100634,
title = {Making the invisible visible: Youth designs for teaching about technological and algorithmic bias},
journal = {International Journal of Child-Computer Interaction},
volume = {39},
pages = {100634},
year = {2024},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100634},
url = {https://www.sciencedirect.com/science/article/pii/S2212868924000023},
author = {Merijke Coenraad},
keywords = {Cooperative inquiry, Co-design, Techquity, Technological bias},
abstract = {Youth are being exposed to technological and algorithmic bias daily, even if they are not using devices themselves. Drawing from a series of cooperative inquiry co-design sessions with youth designers (ages 8 to 13), this paper investigates how youth who have been introduced to these biases design learning experiences about technological and algorithmic bias for their peers. After having more covert biases revealed to them, when designing to teach peers about technological bias, the youth focused on ways to make the invisible visible using four methods: highlighting or explaining a bias, utilizing an adaptive technology, engaging learners in experiential or interactive learning, and modeling how to have conversations about technological and algorithmic biases. These methods provide a basis on which learning experiences about technological and algorithmic biases can be built to ensure these biases are made visible to the youth they are affecting.}
}
@article{MOSER2024100003,
title = {Good wishes to LabMed Discovery},
journal = {LabMed Discovery},
volume = {1},
number = {1},
pages = {100003},
year = {2024},
issn = {3050-4740},
doi = {https://doi.org/10.1016/j.lmd.2024.100003},
url = {https://www.sciencedirect.com/science/article/pii/S305047402400003X},
author = {Edvard Moser}
}
@article{GOPFERT2024100383,
title = {Opportunities for large language models and discourse in engineering design},
journal = {Energy and AI},
volume = {17},
pages = {100383},
year = {2024},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2024.100383},
url = {https://www.sciencedirect.com/science/article/pii/S2666546824000491},
author = {Jan Göpfert and Jann M. Weinand and Patrick Kuckertz and Detlef Stolten},
keywords = {Product development process, Conceptual design, Design methodology, Design generation, Natural language processing, Foundation models, Multi-modal models},
abstract = {In recent years, large language models have achieved breakthroughs on a wide range of benchmarks in natural language processing and continue to increase in performance. Recently, the advances of large language models have raised interest outside the natural language processing community and could have a large impact on daily life. In this paper, we pose the question: How will large language models and other foundation models shape the future product development process? We provide the reader with an overview of the subject by summarizing both recent advances in natural language processing and the use of information technology in the engineering design process. We argue that discourse should be regarded as the core of engineering design processes, and therefore should be represented in a digital artifact. On this basis, we describe how foundation models such as large language models could contribute to the design discourse by automating parts thereof that involve creativity and reasoning, and were previously reserved for humans. We describe how simulations, experiments, topology optimizations, and other process steps can be integrated into a machine-actionable, discourse-centric design process. As an example, we present a design discourse on the optimization of wind turbine blades. Finally, we outline the future research that will be necessary for the implementation of the conceptualized framework.}
}
@article{CARCASSI2023105541,
title = {The Boolean Language of Thought is recoverable from learning data},
journal = {Cognition},
volume = {239},
pages = {105541},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2023.105541},
url = {https://www.sciencedirect.com/science/article/pii/S0010027723001750},
author = {Fausto Carcassi and Jakub Szymanik},
keywords = {Language of Thought Hypothesis, Bayesian inference, Category learning, Parameter recovery simulation},
abstract = {According to the Language of Thought Hypothesis (LoTH), an influential account in philosophy and cognitive science, human cognition is underlain by symbolic reasoning in a formal language. In this account, concepts are expressions in a Language of Thought, deduction is syntactic manipulation in this language, and learning is an inference of expressions in this language from data. This picture raises the question of what LoT humans have, and how to infer it from behavior. In this paper, we pave the way towards answering this question, by approaching a more fundamental question: to what extent is it possible in principle to recover the human LoT from experimental data? To answer this question, we focus on the fragment of LoT that is concerned with representing Boolean categories and simulate the recovery of the Boolean LoT from category learning experiments. Our findings show that in principle the vast majority of Boolean LoTs can be accurately recovered from experimental data. However, we find that this crucially depends on the employed experimental design. Moreover, we find evidence that LoTs with fewer operators can be recovered from category learning data faster.}
}
@article{POLDRACK201512,
title = {Is “efficiency” a useful concept in cognitive neuroscience?},
journal = {Developmental Cognitive Neuroscience},
volume = {11},
pages = {12-17},
year = {2015},
note = {Proceedings from the inaugural Flux Congress; towards an integrative developmental cognitive neuroscience},
issn = {1878-9293},
doi = {https://doi.org/10.1016/j.dcn.2014.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1878929314000413},
author = {Russell A. Poldrack},
keywords = {Neuroimaging, Neural Energetics, Metabolism, fMRI, Response Time, Networks},
abstract = {It is common in the cognitive neuroscience literature to explain differences in activation in terms of differences in the “efficiency” of neural function. I argue here that this usage of the concept of efficiency is empty and simply redescribes activation differences rather than providing a useful explanation of them. I examine a number of possible explanations for differential activation in terms of task performance, neuronal computation, neuronal energetics, and network organization. While the concept of “efficiency” is vacuous as it is commonly employed in the neuroimaging literature, an examination of brain development in the context of neural coding, neuroenergetics, and network structure provides a roadmap for future investigation, which is fundamental to an improved understanding of developmental effects and group differences in neuroimaging signals.}
}
@incollection{ELYAS201751,
title = {Chapter 3 - Physical Property Estimation for Process Simulation},
editor = {Dominic Chwan {Yee Foo} and Nishanth Chemmangattuvalappil and Denny K.S. Ng and Rafil Elyas and Cheng-Liang Chen and René D. Elms and Hao-Yeh Lee and I-Lung Chien and Siewhui Chong and Chien Hwa Chong},
booktitle = {Chemical Engineering Process Simulation},
publisher = {Elsevier},
pages = {51-79},
year = {2017},
isbn = {978-0-12-803782-9},
doi = {https://doi.org/10.1016/B978-0-12-803782-9.00003-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128037829000030},
author = {Rafil Elyas},
keywords = {Enthalpy, Entropy, Equations of state, Property estimation methods, Separator, Work},
abstract = {Like the foundation of a building, the methods used for physical property estimation determine the integrity of a chemical engineering computation. These days, most engineers rely on commercial simulators to perform their computations, and all commercial simulators these days come with a myriad of property packages, where various property estimation methods have been combined into property packages such as Peng–Robinson, Soave–Redlich–Kwong, BWRS, Grayson–Streed, Braun-K10, NRTL, UNIQUAC, and the list goes on. It is critical to know which property package would be applicable for one's computation. The objective of this chapter is to provide some insight into the workings of those property packages and enable the reader to make the correct selection.}
}
@article{XU2012816,
title = {Intelligent fault inference for rotating flexible rotors using Bayesian belief network},
journal = {Expert Systems with Applications},
volume = {39},
number = {1},
pages = {816-822},
year = {2012},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2011.07.079},
url = {https://www.sciencedirect.com/science/article/pii/S0957417411010414},
author = {Bin Gang Xu},
keywords = {Fault diagnosis, Bayesian belief network, Flexible rotor, Uncertainty inference},
abstract = {Flexible rotor is a crucial mechanical component of a diverse range of rotating machineries and its condition monitoring and fault diagnosis are of particular importance to the modern industry. In this paper, Bayesian belief network (BBN) is applied to the fault inference for rotating flexible rotors with attempt to enhance the reasoning capacity under conditions of uncertainty. A generalized three-layer configuration of BBN for the fault inference of rotating machinery is developed by fully incorporating human experts’ knowledge, machine faults and fault symptoms as well as machine running conditions. Compared with the Naive diagnosis network, the proposed topological structure of causalities takes account of more practical and complete diagnostic information in fault diagnosis. The network tallies well with the practical thinking of field experts in the whole processes of machine fault diagnosis. The applications of the proposed BBN network in the uncertainty inference of rotating flexible rotors show good agreements with our knowledge and practical experience of diagnosis.}
}
@article{GREENBERG2024164,
title = {The Lie coalgebra of multiple polylogarithms},
journal = {Journal of Algebra},
volume = {645},
pages = {164-182},
year = {2024},
issn = {0021-8693},
doi = {https://doi.org/10.1016/j.jalgebra.2024.01.030},
url = {https://www.sciencedirect.com/science/article/pii/S0021869324000565},
author = {Zachary Greenberg and Dani Kaufman and Haoran Li and Christian K. Zickert},
keywords = {Multiple polylogarithms, Motivic Lie coalgebra, Symbols, Polylogarithm relations, Bloch groups},
abstract = {We use Goncharov's coproduct of multiple polylogarithms to define a Lie coalgebra over an arbitrary field. It is generated by symbols subject to inductively defined relations, which we think of as functional relations for multiple polylogarithms. In particular, we have inversion relations and shuffle relations. We relate our definition to Goncharov's Bloch groups, and to the concrete model for L(F)≤4 by Goncharov and Rudenko.}
}
@article{HE2023420,
title = {Towards a pluralistic neurobiological understanding of consciousness},
journal = {Trends in Cognitive Sciences},
volume = {27},
number = {5},
pages = {420-432},
year = {2023},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2023.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1364661323000426},
author = {Biyu J. He},
keywords = {consciousness, conscious content, visual awareness, conscious perception, prior knowledge, stimulus ambiguity, states of consciousness, state space},
abstract = {Theories of consciousness are often based on the assumption that a single, unified neurobiological account will explain different types of conscious awareness. However, recent findings show that, even within a single modality such as conscious visual perception, the anatomical location, timing, and information flow of neural activity related to conscious awareness vary depending on both external and internal factors. This suggests that the search for generic neural correlates of consciousness may not be fruitful. I argue that consciousness science requires a more pluralistic approach and propose a new framework: joint determinant theory (JDT). This theory may be capable of accommodating different brain circuit mechanisms for conscious contents as varied as percepts, wills, memories, emotions, and thoughts, as well as their integrated experience.}
}
@article{KNOBE2023892,
title = {The common effect of value on prioritized memory and category representation},
journal = {Trends in Cognitive Sciences},
volume = {27},
number = {10},
pages = {892-900},
year = {2023},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2023.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364661323001511},
author = {Joshua Knobe and Fiery Cushman},
keywords = {concepts, prioritized memory, sampling, resource rationality}
}
@article{STAHL1994309,
title = {Experimental evidence on players' models of other players},
journal = {Journal of Economic Behavior & Organization},
volume = {25},
number = {3},
pages = {309-327},
year = {1994},
issn = {0167-2681},
doi = {https://doi.org/10.1016/0167-2681(94)90103-1},
url = {https://www.sciencedirect.com/science/article/pii/0167268194901031},
author = {Dale O. Stahl and Paul W. Wilson},
keywords = {Game theory, Laboratory experiments, Semi-parametric methods},
abstract = {We pose a hierarchial model of strategic thinking and conduct an experiment to test this theory as well as other solution concepts for symmetric (3 × 3) games. A level-0 type plays unpredictably, a level-1 type acts as if everyone else were level-0 types, and a level-2 type acts as if all other players were level-0 and level-1 types. In a model with level-0,…, level-2, and Nash types, we estimated that an insignificant portion of the participants were level-0 types, 24% were level-1 types, 49% were level-2 types, and the remaining 27% were Nash types.}
}
@article{WEITZ201630,
title = {Monte Carlo efficiency improvement by multiple sampling of conditioned integration variables},
journal = {Journal of Computational Physics},
volume = {326},
pages = {30-34},
year = {2016},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2016.08.036},
url = {https://www.sciencedirect.com/science/article/pii/S002199911630393X},
author = {Sebastian Weitz and Stéphane Blanco and Julien Charon and Jérémi Dauchet and Mouna {El Hafi} and Vincent Eymet and Olivier Farges and Richard Fournier and Jacques Gautrais},
keywords = {Monte Carlo integration, Monte Carlo efficiency, Monte Carlo in complex geometry, Statistical physics},
abstract = {We present a technique that permits to increase the efficiency of multidimensional Monte Carlo algorithms when the sampling of the first, unconditioned random variable consumes much more computational time than the sampling of the remaining, conditioned random variables while its variability contributes only little to the total variance. This is in particular relevant for transport problems in complex and randomly distributed geometries. The proposed technique is based on an new Monte Carlo estimator in which the conditioned random variables are sampled more often than the unconditioned one. A significant contribution of the present Short Note is an automatic procedure for calculating the optimal number of samples of the conditioned random variable per sample of the unconditioned one. The technique is illustrated by a current research example where it permits to increase the efficiency by a factor 100.}
}
@article{MISHRA2024108508,
title = {An integrated picture fuzzy standard deviation and pivot pairwise assessment method for assessing the drivers of digital transformation in higher education institutions},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {108508},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108508},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624006663},
author = {Arunodaya Raj Mishra and Pratibha Rani and Dragan Pamucar and Adel Fahad Alrasheedi and Vladimir Simic},
keywords = {Digital transformation, Picture fuzzy sets, Higher education institutions, DSS, PIPRECIA},
abstract = {Information and communication technologies play a significant role in all aspects of modern society. It has become a subject of high significance in all perspectives, mainly in the workstation. Consequently, the main objective for universities and schools is to develop future specialists for handling the problems and finding the effective outcomes by their digital competency as a significant ability. In higher education institutions, assessing the key drivers of digital transformation is a complicated decision-making problem as the multiple factors are involved in it. To this aim, a novel picture fuzzy information-based decision-making framework is proposed to assess the key drivers of digital transformation in higher education institutions. In this regard, an improved score function is proposed to rank the picture fuzzy numbers. In addition, it is used to determine the weights of decision experts. Next, some generalized Dombi aggregation operators are proposed to aggregate the picture fuzzy information. Further, an integrated criteria weighting model is presented based on the combination of objective weights through standard deviation-based model and subjective weights through pivot pairwise relative criteria importance assessment (PIPRECIA) model in the context of picture fuzzy sets. To rank the alternatives, a picture fuzzy extension of alternative ranking order model accounting for two step normalization (AROMAN) approach is proposed using the score function, Dombi operators and integrated weighting model. The usefulness of the proposed approach is illustrated using a case study of key drivers' assessment of digital transformation in higher education institutions. Sensitivity and comparative analyses are discussed to reveal the consistency, robustness and efficiency of introduced model. This study offers a new decision-making framework, which makes a significant contribution to the key drivers’ assessment process from uncertainty perspective.}
}
@article{LIN20051585,
title = {Faster optimal parallel prefix circuits: New algorithmic construction},
journal = {Journal of Parallel and Distributed Computing},
volume = {65},
number = {12},
pages = {1585-1595},
year = {2005},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2005.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S0743731505001346},
author = {Yen-Chun Lin and Chin-Yu Su},
keywords = {Combinational circuits, Depth, Depth-size optimal, Fan-out, Parallel algorithms, Prefix computation, Size optimal},
abstract = {Parallel prefix circuits are parallel prefix algorithms on the combinational circuit model. A prefix circuit with n inputs is depth-size optimal if its depth plus size equals 2n-2. Smaller depth implies faster computation, while smaller size implies less power consumption, less VLSI area, and less cost. To be of practical use, the depth and fan-out of a depth-size optimal prefix circuit should be small. A circuit with a smaller fan-out is in general faster and occupies less VLSI area. In this paper, we present a new algorithm to design parallel prefix circuits, and construct a class of depth-size optimal parallel prefix circuits, named SU4, with fan-out 4. When n⩾30, SU4 has the smallest depth among all known depth-size optimal prefix circuits with fan-out 4.}
}
@article{RAGNI2011309,
title = {Generalized dynamic stock and flow systems: An AI approach},
journal = {Cognitive Systems Research},
volume = {12},
number = {3},
pages = {309-320},
year = {2011},
note = {Special Issue on Complex Cognition},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2010.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S1389041710000872},
author = {Marco Ragni and Felix Steffenhagen and Andreas Klein},
keywords = {Complex cognition, Dynamic stock and flow systems, Computational/AI modeling},
abstract = {A well-known problem in complex cognition is the so-called dynamic stocks and flows task (DSF). The challenge in this task is to control different flows, e.g. the inflows and outflows of water to a tank, towards a specified goal configuration, i.e. a certain amount of water in the tank. The problem is that some flows are exogenously controlled with a hidden dynamic. These flows need to be counterbalanced by setting endogenous flows. Since the dynamic underlying the hidden flows can be any computable function, this task can be classified as computationally complex. Psychological findings show that humans have difficulties in dealing with such dynamic systems. In this article, we present a formal generalization of this task and present a computational approach for solving such tasks as a first step towards an assistance system for complex system control.}
}
@article{MIAO2021108327,
title = {Federated deep reinforcement learning based secure data sharing for Internet of Things},
journal = {Computer Networks},
volume = {197},
pages = {108327},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108327},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621003285},
author = {Qinyang Miao and Hui Lin and Xiaoding Wang and Mohammad Mehedi Hassan},
keywords = {Secure data sharing, Federated learning, Deep reinforcement learning, IoT},
abstract = {The increasing number of Internet of Things (IoT) devices motivate the data sharing that improves the quality of IoT services. However, data providers usually suffer from the privacy leakage caused by direct data sharing. To solve this problem, in this paper, we propose a Federated Learning based Secure data Sharing mechanism for IoT, named FL2S. Specifically, to accomplish efficient and secure data sharing, a hierarchical asynchronous federated learning (FL) framework is developed based on the sensitive task decomposition. In addition, to improve data sharing quality, the deep reinforcement learning (DRL) technology is utilized to select participants of sufficient computational capabilities and high quality datasets. By integrating task decomposition and participant selection, reliable data sharing is realized by sharing local data models instead of the source data with data privacy preserved. Experiment results show that the proposed FL2S achieves high accuracy in secure data sharing for various IoT applications.}
}
@article{MACLENNAN2010199,
title = {Morphogenesis as a model for nano communication},
journal = {Nano Communication Networks},
volume = {1},
number = {3},
pages = {199-208},
year = {2010},
note = {Fundamentals of Nanoscale Communications},
issn = {1878-7789},
doi = {https://doi.org/10.1016/j.nancom.2010.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S1878778910000402},
author = {Bruce J. MacLennan},
keywords = {Algorithmic assembly, Embodied computation, Molecular communication, Morphogenesis, Nano communication, Self-organization},
abstract = {The creation of physical objects with a complex hierarchical structure from the nanoscale up to the macroscale presents many challenges that must be met in order to reap the full benefits of nanotechnology. To accomplish this we can learn from a natural process that already accomplishes it: embryological morphogenesis, which teaches us means by which microscopic agents can communicate and coordinate their activity by means of molecular signals in order to create complex physical structures. We call the application of these ideas artificial morphogenesis; it is a kind of embodied computation, which refers to the intimate interaction of physical and information processes. We outline the basis for artificial morphogenesis and present several simple examples in which biologically inspired models can be used to describe the assembly of useful nanostructures.}
}
@article{CAMPBELL2021647,
title = {Analysis and prediction of reaction kinetics using the degree of rate control},
journal = {Journal of Catalysis},
volume = {404},
pages = {647-660},
year = {2021},
issn = {0021-9517},
doi = {https://doi.org/10.1016/j.jcat.2021.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0021951721004097},
author = {Charles T. Campbell and Zhongtian Mao},
keywords = {Mechanisms, Microkinetic modelling, Rate determining step, Reaction energy diagram, Computational search, Catalyst optimization, Process optimization, Apparent activation energy, Tafel slope, Reaction order, Kinetic isotope effect, Site coverage},
abstract = {“Degree of rate control” (DRC) analysis provides a quantitative approach for analysing the kinetics of multi-step reaction mechanisms that has been widely applied to both heterogeneous and homogeneous catalysis research, as well as electrocatalysis. The DRC of any given transition state or intermediate is defined as a partial derivative such that it approximately equals the fractional increase in net rate to the product of interest per differential decrease in its standard-state free energy for that species (÷RT), holding constant the standard-state free energies of all other transition states and intermediates. Even very complex mechanisms usually have only a few species with non-zero DRCs and are thus the species whose interactions with the catalyst most strongly affect the net rate. These key DRC values thus offer a simple and intuitive route to optimize catalyst materials, especially with the assistance of computational methods like density functional theory (DFT). These high-DRC species are also the species whose energetics must be most accurately measured or calculated to achieve an accurate kinetic model for any reaction mechanism. In simple cases with a single “rate-determining step”, the DRC for its transition state (TS) is + 1. Catalyst-bound intermediates, on the other hand, often have negative DRCs equal to a small integer times their fractional occupancy of catalyst sites. The apparent activation energy equals a weighted average of the standard-state enthalpies (relative to reactants) of all the species (intermediates, transition states and products) in the reaction mechanism, each weighted by its DRC (+RT). It has been shown that the apparent transfer coefficient in electrocatalysis, an inverted form of the Tafel slope, is a weighted average of the number of electrons transferred to generate each intermediate or product species in the mechanism, weighted again by the DRC. Quantitative analysis of kinetic isotope effects (KIEs, or the ratio of net rates for different reactant isotopes) in complex mechanisms has shown that the logarithm of the KIE equals the weighted average over all species in the mechanism of the difference between the two isotopes in their standard-state free energies (÷RT), again weighted by the DRC. The reaction orders with respect to fluid-phase concentrations of reactants, products and intermediates have also been proven to be directly related to DRCs. Thus, there are numerous experimental observables which equate to short linear combinations of DRCs, so that combinations of experimental measurements might provide access to DRC values. Since its invention for transition states in 1994 and its generalization to include intermediates in 2009, the DRC has thus far mainly been calculated for microkinetic models based either entirely on DFT or on DFT with the key energies (i.e., those for high-DRC species) being fine-tuned to match experiments. The relationships summarized above provide new opportunities for using experiments earlier in the development and optimization of microkinetic models that require input from computational catalysis.}
}
@article{ZHU2024101058,
title = {Introducing immersive virtual reality in a marketing practical training course: Qualitative evaluation with undergraduates},
journal = {The International Journal of Management Education},
volume = {22},
number = {3},
pages = {101058},
year = {2024},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2024.101058},
url = {https://www.sciencedirect.com/science/article/pii/S1472811724001290},
author = {Dong Hong Zhu},
keywords = {Marketing, Immersive virtual reality, Supermarket design, Practical training course, Qualitative analysis, Undergraduate},
abstract = {In the marketing learning, practical training courses are very important for undergraduates to understand and master theoretical knowledge. Virtual reality (VR) technology has great potential in practical teaching, but its effect on marketing students has not been paid enough attention. This study aims to fill this gap by exploring the impact of immersive VR practical training courses on marketing learning. By using the self-developed “immersive VR supermarket design” system as a platform, this study qualitatively analyzed the evaluations of marketing students who participated in this VR practical training course. The results show that students have perceived novelty, perceived interest, the sense of achievement and perceived usefulness of the VR practical training course, which can enhance their professional identity and thus enhance their willingness to learn. In addition, perceived usefulness includes professional cognition improvement, professional knowledge deepening, practical ability improvement, VR knowledge improvement and team cooperation ability improvement. The conclusions show that it is a positive thing to introduce immersive VR into marketing practical training courses. This study enriches the research of immersive VR practical courses and provides significant implications for the development of VR practical training courses in marketing education.}
}
@article{CONNOLLY20123813,
title = {Toward interactive, Internet-based decision aid for vaccination decisions: Better information alone is not enough},
journal = {Vaccine},
volume = {30},
number = {25},
pages = {3813-3818},
year = {2012},
note = {Special Issue: The Role of Internet Use in Vaccination Decisions},
issn = {0264-410X},
doi = {https://doi.org/10.1016/j.vaccine.2011.12.094},
url = {https://www.sciencedirect.com/science/article/pii/S0264410X11020433},
author = {Terry Connolly and Jochen Reb},
keywords = {Decision making, Decision aiding, Internet, Vaccination decisions},
abstract = {Vaccination decisions, as in choosing whether or not to immunize one's small child against specific diseases, are both psychologically and computationally complex. The psychological complexities have been extensively studied, often in the context of shaping convincing or persuasive messages that will encourage parents to vaccinate their children. The computational complexity of the decision has been less noted. However, even if the parent has access to neutral, accurate, credible information on vaccination risks and benefits, he or she can easily be overwhelmed by the task of combining this information into a well-reasoned decision. We argue here that the Internet, in addition to its potential as an information source, could provide useful assistance to parents in integrating factual information with their own values and preferences – that is, in providing real decision aid as well as information aid. We sketch one approach for accomplishing this by means of a hierarchy of interactive decision aids ranging from simple advice to full-scale decision analysis.}
}
@article{BULLARD2006369,
title = {Coarse-graining approximation for simulating surface reaction kinetics in particulate systems},
journal = {Computational Materials Science},
volume = {38},
number = {2},
pages = {369-373},
year = {2006},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2005.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S0927025606000760},
author = {Jeffrey W. Bullard},
keywords = {Coarse graining, Kinetics, Reactions, Coarsening},
abstract = {When systems of reacting solid or liquid particles are explicitly modeled on a computational mesh, a wide distribution of initial particle sizes can lead to undesirably large numbers of mesh points. A coarse graining method is presented that avoids this problem by essentially grouping the finest particles together into a number of larger clusters. The excess surface area of these clusters is incorporated into a bias factor that can be multiplied by the intrinsic reaction rate constant to approximate the enhanced reaction rate of the clusters relative to actual particles of the same size. The bias factor can be calculated directly from knowledge of mesh point spacing and of the particle size distribution. Examples are given of the value that the bias factor can have for a model distribution of particle sizes, using computational meshes with varying resolution limits.}
}
@incollection{KASHYAP2019165,
title = {Chapter 10 - Miracle of Deep Learning Using IoT},
editor = {Arun Kumar Sangaiah},
booktitle = {Deep Learning and Parallel Computing Environment for Bioengineering Systems},
publisher = {Academic Press},
pages = {165-178},
year = {2019},
isbn = {978-0-12-816718-2},
doi = {https://doi.org/10.1016/B978-0-12-816718-2.00017-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128167182000178},
author = {Ramgopal Kashyap},
keywords = {Profound learning, Internet-of-Things (IoT), Software defined network (SDN), Partially overlapping channel assignment (POCA), Movement stack expectation},
abstract = {Human recognition is still difficult due to the fact we do not yet have a data set sufficiently extensive to oversee the model preparing. This chapter addresses this difficulty from three perspectives to make human recognition adaptable to genuine information and applications. We consider a semi-directed profound learning structure that utilizes boisterous marked, as opposed to very much commented on information, and a huge scale dressing dataset with loud explanations, from which we can learn great portrayals for garments that assists in perceiving a human. A profound learning calculation and a space guided dropout procedure are taken in a solitary model from various human identification datasets with area predispositions. Ongoing advances of profound learning have created urging results tantamount to and even better than obtained by human specialists. Nonetheless, the extensive sum information input has been an overwhelming undertaking for profound figuring out how to be broadly connected in the Internet-of-Things (IoT) with continuous handling. The objective of this chapter is to investigate brilliant and quick information handling plan for more computationally proficient profound figuring out how to help versatile and continuous applications, which will have the capacity to build the range and vital productivity in IoT. This chapter will indicate how singular value decomposition (SVD)-QR calculation is required for preprocessing of profound learning for vast scale information input. Because of the quick increase of detecting information and brisk reaction prerequisite in the IoT conveyance organization; the rapid transmission has risen as a vital issue. Allocating reasonable directs in the remote IoT conveyance organization is an essential certification of rapid transmission. In any case, the high elements of movement stack make the regularly settled channel task calculation inadequate. As of late, the software defined networking based IoT (SDN-IoT) is suggested to be used to enhance the transmission quality. Other than this, the keen method of profound learning is broadly looked into in high computational SDN.}
}
@article{THOMAKOS2023100284,
title = {The origins of forward-looking decision making: Cybernetics, operational research, and the foundations of forecasting},
journal = {Decision Analytics Journal},
volume = {8},
pages = {100284},
year = {2023},
issn = {2772-6622},
doi = {https://doi.org/10.1016/j.dajour.2023.100284},
url = {https://www.sciencedirect.com/science/article/pii/S2772662223001248},
author = {Dimitrios Thomakos and Panos Xidonas},
keywords = {Decision making, Cybernetics, Operational research, Business analytics, Forecasting, Artificial intelligence, Machine learning},
abstract = {The massive explosion of literature, theory, and methods on all aspects of decision-analytics, machine learning and artificial intelligence, over the past 20 or so years has brought a rapid specialization in each of the substrata of the fields that are using them. The sharp focus on empirical usage of these methods across applications, and the consequent trivialization from data only-driven improvements and multiple method comparisons, have diverted attention from foundational and epistemological concerns and questions, leading to pure empiricism — due to, but not exclusively, increases and availability of computing power. Tapping into the, equally massive, history and literature of the earliest developments from pioneers in the fields of cybernetics, operations research, and forecasting, we re-establish the links to the past on the origins of business and predictive analytics. Using interdisciplinary-sourced material we bring attention to the significance of these early developments and to the need for a return to these early sources in re-establishing our connection with the fundamental principles and questions that define meaningful, forward-looking, decision-making.}
}
@incollection{MAIVALI201555,
title = {Chapter 2 - The Basis of Knowledge: Causality and Truth},
editor = {Ülo Maiväli},
booktitle = {Interpreting Biomedical Science},
publisher = {Academic Press},
address = {Boston},
pages = {55-108},
year = {2015},
isbn = {978-0-12-418689-7},
doi = {https://doi.org/10.1016/B978-0-12-418689-7.00002-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780124186897000028},
author = {Ülo Maiväli},
keywords = {Popper, philosophy of science, causality, induction, deduction, correlation, concordance, regression analysis, AIC, Akaike},
abstract = {As a basis for scientific methodology, and to enable sharper thinking on the topic of scientific discovery, concepts like realism, empiricism, instrumentalism, operationalism, and pragmatism are explained. Emphasis is put throughout on truth and causality, which are followed from the historical depths (Hume and Kant) through twentieth-century developments (Popper and Fisher) into modern statistical theories. The connections of causality with observational (correlational) and experimental studies is discussed through integrating real-life examples with theoretical concepts. Statistical methods of correlation, concordance, and regression analysis are discussed with emphasis on their scientific uses and misuses, assumptions, and interpretations in the context of formulating causal theories. Model selection, Granger causality, and convergent cross mapping are briefly touched on. Scientific experiments, with their dependence on defined experimental systems, manipulations and controls, are put into the context of testing of causal hypotheses.}
}
@article{AHMAD2023100568,
title = {Deep learning models for cloud, edge, fog, and IoT computing paradigms: Survey, recent advances, and future directions},
journal = {Computer Science Review},
volume = {49},
pages = {100568},
year = {2023},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2023.100568},
url = {https://www.sciencedirect.com/science/article/pii/S1574013723000357},
author = {Shahnawaz Ahmad and Iman Shakeel and Shabana Mehfuz and Javed Ahmad},
keywords = {Deep learning, Cloud computing, Edge computing, Fog computing, IoT, Models},
abstract = {In recent times, the machine learning (ML) community has recognized the deep learning (DL) computing model as the Gold Standard. DL has gradually become the most widely used computational approach in the field of machine learning, achieving remarkable results in various complex cognitive tasks that are comparable to, or even surpassing human performance. One of the key benefits of DL is its ability to learn from vast amounts of data. In recent years, the DL field has witnessed rapid expansion and has found successful applications in various conventional areas. Significantly, DL has outperformed established ML techniques in multiple domains, such as cloud computing, robotics, cybersecurity, and several others. Nowadays, cloud computing has become crucial owing to the constant growth of the IoT network. It remains the finest approach for putting sophisticated computational applications into use, stressing the huge data processing. Nevertheless, the cloud falls short because of the crucial limitations of cutting-edge IoT applications that produce enormous amounts of data and necessitate a quick reaction time with increased privacy. The latest trend is to adopt a decentralized distributed architecture and transfer processing and storage resources to the network edge. This eliminates the bottleneck of cloud computing as it places data processing and analytics closer to the consumer. Machine learning (ML) is being increasingly utilized at the network edge to strengthen computer programs, specifically by reducing latency and energy consumption while enhancing resource management and security. To achieve optimal outcomes in terms of efficiency, space, reliability, and safety with minimal power usage, intensive research is needed to develop and apply machine learning algorithms. This comprehensive examination of prevalent computing paradigms underscores recent advancements resulting from the integration of machine learning and emerging computing models, while also addressing the underlying open research issues along with potential future directions. Because it is thought to open up new opportunities for both interdisciplinary research and commercial applications, we present a thorough assessment of the most recent works involving the convergence of deep learning with various computing paradigms, including cloud, fog, edge, and IoT, in this contribution. We also draw attention to the main issues and possible future lines of research. We hope this survey will spur additional study and contributions in this exciting area.}
}
@incollection{HOWITT20061605,
title = {Chapter 35 Coordination Issues in Long-Run Growth},
editor = {L. Tesfatsion and K.L. Judd},
series = {Handbook of Computational Economics},
publisher = {Elsevier},
volume = {2},
pages = {1605-1624},
year = {2006},
issn = {1574-0021},
doi = {https://doi.org/10.1016/S1574-0021(05)02035-6},
url = {https://www.sciencedirect.com/science/article/pii/S1574002105020356},
author = {Peter Howitt},
keywords = {growth, coordination, innovation, stability, agent-based systems},
abstract = {Economic growth depends not only on how people make decisions but also upon how their decisions are coordinated. Because of this, aggregate outcomes can diverge from individual intentions. I illustrate this with reference to the modern literature on economic growth, and also with reference to an older literature on the stability of full-employment equilibrium. Agent-based computational methods are ideally suited for studying the aspects of growth most affected by coordination issues.}
}
@article{ADIKARI2021100022,
title = {Value co-creation for open innovation: An evidence-based study of the data driven paradigm of social media using machine learning.},
journal = {International Journal of Information Management Data Insights},
volume = {1},
number = {2},
pages = {100022},
year = {2021},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2021.100022},
url = {https://www.sciencedirect.com/science/article/pii/S266709682100015X},
author = {Achini Adikari and Donna Burnett and Darshana Sedera and Daswin {de Silva} and Damminda Alahakoon},
keywords = {Information models, Data-driven paradigm, Open innovation, Service innovation, S-D logic, Social media, Machine learning},
abstract = {Social media encapsulates one of the most prominent human information behaviours that has rapidly evolved to create a new data-driven paradigm that uses data-intensive digital environments to communicate, collaborate, express opinions and support decisions. This has established social media as a unique information asset for value co-creation as it empowers individuals to actively express opinions and sentiment on all facets of interactions with an external entity.  Despite recent research on the theoretical underpinnings of social media in open service innovation, practical demonstrations of actionable insights are limited, mainly due to the voluminous and unstructured nature of social media data. We address this limitation by presenting an evidence-based study that uses machine learning algorithms to generate actionable insights of strategic value from this data-driven paradigm.  These outcomes provide fresh perspectives and new thinking that advances social media as an emergent information asset for end-to-end open innovation and incremental value co-creation.}
}
@article{LENGYEL2024100046,
journal = {Papers in Regional Science},
volume = {103},
number = {5},
pages = {100046},
year = {2024},
issn = {1056-8190},
doi = {https://doi.org/10.1016/j.pirs.2024.100046},
url = {https://www.sciencedirect.com/science/article/pii/S1056819024000666},
author = {Balázs Lengyel}
}
@article{BORJI2023101027,
title = {On students' understanding of volumes of solids of revolution: An APOS analysis},
journal = {The Journal of Mathematical Behavior},
volume = {70},
pages = {101027},
year = {2023},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2022.101027},
url = {https://www.sciencedirect.com/science/article/pii/S0732312322000955},
author = {Vahid Borji and Rafael Martínez-Planell},
keywords = {Volumes of solids of revolution, Riemann sums, Visualization, APOS, Textbook, Integral},
abstract = {We apply Action-Process-Object-Schema theory (APOS) to (1) examine the textbook used by students in the study to infer mental constructions the book proposes that students could do to understand solids of revolution, (2) use semi-structured interviews with nine students to find out which of the textbook’s proposed constructions students actually do, and what unconjectured or unexpected constructions students do, and (3) use the results of the interviews and the research literature to inform and set forth an alternative proposal for constructing volumes of revolution. Results suggest that many students have not constructed processes to visualize solids of revolution and to relate Riemann sums to the corresponding definite integrals. Implications for curriculum and instruction are discussed.}
}
@article{SOUTHWORTH2023100127,
title = {Developing a model for AI Across the curriculum: Transforming the higher education landscape via innovation in AI literacy},
journal = {Computers and Education: Artificial Intelligence},
volume = {4},
pages = {100127},
year = {2023},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2023.100127},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X23000061},
author = {Jane Southworth and Kati Migliaccio and Joe Glover and Ja’Net Glover and David Reed and Christopher McCarty and Joel Brendemuhl and Aaron Thomas},
keywords = {21st century competencies, Curriculum design, AI literacy, Transformative program development, Interdisciplinary education, Career readiness},
abstract = {Artificial Intelligence (AI) is a ubiquitous concept and tool already found across society and an integral part of everyday life. As such, basic understanding and knowledge of AI should be a critical component of student education to foster successful global citizens. This position paper describes one possible path to address potential gaps in AI education and integrate AI across the curriculum at a traditional research university. The University of Florida (UF) is infusing AI across the curriculum and developing opportunities for student engagement within identified areas of AI literacy regardless of student discipline. The AI Across the Curriculum initiative being developed at UF will make AI education a cornerstone opportunity for all students. The ultimate goal of AI Across the Curriculum is the creation of an AI-ready workforce covering the essential 21st-century competencies identified as workforce and government needs worldwide. Qualified human capital is essential to face the challenges of the 21st-century, and UF is positioning itself to lead in meeting this global societal need. In designing the AI Across the Curriculum model, all students are provided with a suite of AI opportunities and are encouraged to engage. The university is taking advantage of a significant investment in AI campus-wide to innovate curriculum and create activities that nurture interdisciplinary engagement while ensuring student career readiness. As businesses, industry, and governments transform globally within this AI paradigm shift, AI education, innovation, and literacy will become cornerstones of curriculum with UF providing an inclusive example for all undergraduate, graduate, and professional students. While the AI effort at UF is inclusive and broad, the focus of this paper is on undergraduate programs which also represents a Quality Enhancement Plan (or QEP) effort for reaccreditation of UF's undergraduate programs. This program is highly innovative and transformative, creating interdisciplinary AI literacy opportunity for all students.}
}
@incollection{MASCOLO2013185,
title = {Chapter Eight - Developing through Relationships: An Embodied Coactive Systems Framework},
editor = {Richard M. Lerner and Janette B. Benson},
series = {Advances in Child Development and Behavior},
publisher = {JAI},
volume = {45},
pages = {185-225},
year = {2013},
booktitle = {Embodiment and Epigenesis: Theoretical and Methodological Issues in Understanding the Role of Biology within the Relational Developmental System},
issn = {0065-2407},
doi = {https://doi.org/10.1016/B978-0-12-397946-9.00008-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780123979469000087},
author = {Michael F. Mascolo},
keywords = {Coaction, Coconstruction, Joint action, Microdevelopment, Scaffolding, Systems theory, Tying shoes},
abstract = {In recent decades, the developmental sciences have undergone a relational turn. Epigenetic (Gottlieb & Lickliter, 2007), embodied (Thompson, 2007), relational (Lerner & Overton, 2008) and systems (Kelso, 2003) approaches are transforming the ways in which we think about the nature and origins of psychological structures. At their most basic level, relational and systems approaches analyze the developmental origins of order and variability not in terms of sets of separable causal forces but instead in analyses of relations between causal systems. From this view, genes and environment, biology and culture, cognition and emotion, self and other, and so forth are inseparable as causal processes in the development of action and experience. Drawing on these principles, this paper contains an outline of an embodied coactive systems framework for understanding how individual psychological structures develop as a product of socially distributed coactions that occur among elements of the extended person–environment system. Based on these principles, a system for the Developmental Analysis of Joint Action is described. This system provides a set of conceptual and empirical tools for making precise assessments of dynamic structure of jointly constructed patterns of thinking, feeling and acting. By tracking developmental changes in joint action, the system allows researchers to track the origins of higher order psychological structures through particular sequences of coconstructive activity. The holistic analytic system is illustrated through microdevelopmental analyses of (1) the joint construction of shoe-tying skill between a 5-year-old boy and a caregiver, and (2) socioemotional organization developing representations of self in a young adult over the course of a single session of psychotherapy.}
}
@article{VANLANGE2008405,
title = {Collective rationality: The integrative model explains it (as) well},
journal = {Acta Psychologica},
volume = {128},
number = {2},
pages = {405-408},
year = {2008},
issn = {0001-6918},
doi = {https://doi.org/10.1016/j.actpsy.2008.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0001691808000127},
author = {Paul A.M. {Van Lange}},
keywords = {Cooperation, Game theory, Social psychology, Interdependence, Transformation, Social utility},
abstract = {In this commentary, I argue that there is indeed considerable evidence in support of the notion that people tend to reason from a collective (or team) perspective by asking themselves questions such as “What do we want, and what should I do help achieve it?” [Colman, A. M., Pulford, B. D., & Rose, J. (2008). Collective rationality in interactive decisions: Evidence for team reasoning. Acta Psychologica]. As such, in my view, team reasoning – and thinking, feeling, and acting in terms of collective rationality – is consistent with a social utility model (or transformational model) which considers the weights that people attach not only to outcomes for self, but also to outcomes for other, and to equality in outcomes [Van Lange, P. A. M. (1999). The pursuit of joint outcomes and equality in outcomes: An integrative model of social value orientation. Journal of Personality and Social Psychology,77, 337–349]. This commentary provides an illustration demonstrating that the integrative model is well-suited to account for the findings observed by Colman et al. (2008).}
}
@article{BOUGIE2022863,
title = {Data-Efficient Reinforcement Learning from Controller Guidance with Integrated Self-Supervision for Process Control},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {7},
pages = {863-868},
year = {2022},
note = {13th IFAC Symposium on Dynamics and Control of Process Systems, including Biosystems DYCOPS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.07.553},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322009594},
author = {Nicolas Bougie and Takashi Onishi and Yoshimasa Tsuruoka},
keywords = {Reinforcement learning control, Process control, Chemical plant control, Co-Learning, self-learning, Artificial intelligence},
abstract = {Model-free reinforcement learning methods have achieved significant success in a variety of decision-making problems. In fact, they traditionally rely on large amounts of data generated by sample-efficient simulators. However, many process control industries involve complex and costly computations, which limits the applicability of model-free reinforcement learning. In addition, extrinsic rewards are naturally sparse in the real world, further increasing the amount of necessary interactions with the environment. This paper presents a sample-efficient model-free algorithm for process control, which massively accelerates the learning process even when rewards are extremely sparse. To achieve this, we leverage existing controllers to guide the agent's learning — controller guidance is used to drive exploration towards key regions of the state space. To further mitigate the above-mentioned challenges, we propose a strategy for self-supervision learning that lets us improve the agent's policy via its own successful experience. Notably, the method we develop is able to leverage guidance that does not include the actions and remains effective when the existing controllers are suboptimal. We present an empirical evaluation on a vinyl acetate monomer (VAM) chemical plant under disturbances. The proposed method exhibits better performance than baselines approaches and higher sample efficiency. Besides, empirical results show that our method outperforms the existing controllers for controlling the plant and canceling disturbances, mitigating the drop in the production load.}
}
@article{SOHAIL2021103819,
title = {Dengue control measures via cytoplasmic incompatibility and modern programming tools},
journal = {Results in Physics},
volume = {21},
pages = {103819},
year = {2021},
issn = {2211-3797},
doi = {https://doi.org/10.1016/j.rinp.2021.103819},
url = {https://www.sciencedirect.com/science/article/pii/S2211379721000061},
author = {Ayesha Sohail and Mehwish Iftikhar and Robia Arif and Hijaz Ahmad and Khaled A. Gepreel and Sahrish Iftikhar},
keywords = {ANN, Cytoplasmic incompatibility, Computer simulations, Mating, Nonlinear modeling, Stability analysis},
abstract = {The vector borne diseases share a high percentage of the annual deaths reported by the world health organization. Due to the involvement of multiple factors and resistance of the vector to the treatment strategies, it is highly desired to design a safe, efficient and harmless control strategy. A state of the art computational framework for the Wolbachia induced “genetic-control-strategy” has been documented during this research. The process of mating and molecular identification is complex and expensive and different labs around the world are devoted to format a smooth implementation strategy of this approach. During this research, as a first step, data was collected, secondly, dynamic artificial neural networks, with the “tapped delay” lines are used for nonlinear filtering of the data and for the prediction of the parametric values of the novel mathematical model. To study parametric reliance of stable solutions and unstable solution we analyze stability and the corresponding thresholds desired for this study are documented. The study conducted during this research can prove to be useful in future experimental and theoretical studies, aimed to control the vector borne diseases.}
}
@article{LAGEMANN2023103748,
title = {Optimal ship lifetime fuel and power system selection under uncertainty},
journal = {Transportation Research Part D: Transport and Environment},
volume = {119},
pages = {103748},
year = {2023},
issn = {1361-9209},
doi = {https://doi.org/10.1016/j.trd.2023.103748},
url = {https://www.sciencedirect.com/science/article/pii/S1361920923001451},
author = {Benjamin Lagemann and Sotiria Lagouvardou and Elizabeth Lindstad and Kjetil Fagerholt and Harilaos N. Psaraftis and Stein Ove Erikstad},
keywords = {Shipping, Greenhouse gas emissions, Alternative fuels, Stochastic optimization, Flexibility, Retrofit},
abstract = {Ship designers face increasing pressure to comply with global emission reduction ambitions. Alternative fuels, potentially derived from bio-feedstock or renewable electricity, provide promising solutions to this problem. The main challenge is to identify a suitable ship power system, given not only uncertain emission requirements but also uncertain fuel and carbon emission prices. We develop a two-stage stochastic optimization model that explicitly considers uncertain fuel and carbon emission prices, as well as potential retrofits along the lifetime. The bi-objective setup of the model shows how the choice of optimal power system changes with reduced emission levels. Methanol and LNG configurations appear to be relatively robust initial choices due to their ability to run on fuel derived from different feedstocks, andtheir better retrofittability towards ammonia or hydrogen. From a policy perspective, our model provides insight into the effect ofthe different types ofcarbon pricing mechanismson a shipowner’s decisions.}
}
@article{BORGIANNI2015388,
title = {Integration of OTSM-TRIZ and Analytic Hierarchy Process for Choosing the Right Solution},
journal = {Procedia Engineering},
volume = {131},
pages = {388-400},
year = {2015},
note = {TRIZ and Knowledge-Based Innovation in Science and Industry},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2015.12.431},
url = {https://www.sciencedirect.com/science/article/pii/S1877705815043234},
author = {Yuri Borgianni and Francesco Saverio Frillici and Federico Rotini},
keywords = {Network of Problems, best solution selection, Analytic Hierarchy Process, hand steamer},
abstract = {A relevant part of TRIZ literature concerns the steps of the problem solving process, hence the analysis of the troublesome situation, the identification of the core problem and its resolution. Conversely, few efforts have been dedicated to support the last phase of the conceptual design process, which regards the selection of the most promising solutions to be further developed. The lack within TRIZ of an instrument capable to fulfill the abovementioned task led the authors to investigate the classical decision making methods and their applicability in the context of selecting the most valuable concepts downstream of problem solving phases characterized by divergent thinking. Several potential approaches have been surveyed and, among the others, the Weighted Sum Method and the Analytic Hierarchy Process seem to hold some of the characteristics requested by an ideal method to facilitate the decision making. In this paper, both of them have been tested through a real case study in order to verify their actual applicability and to reveal strengths and weaknesses with a particular focus on their capability to guide the decision process when a plurality of parties (e.g. policy makers, domain experts) are involved. The testing activity revealed that the Analytic Hierarchy Process resulted overall more appreciated by the experimenters, thanks to the systematic approach employed to select the best solution among a sample of alternatives developed through the Network of Problems.}
}
@article{SIMON1986241,
title = {The information processing explanation of Gestalt phenomena},
journal = {Computers in Human Behavior},
volume = {2},
number = {4},
pages = {241-255},
year = {1986},
issn = {0747-5632},
doi = {https://doi.org/10.1016/0747-5632(86)90006-3},
url = {https://www.sciencedirect.com/science/article/pii/0747563286900063},
author = {Herbert A. Simon},
abstract = {Gestalt psychology has shown the importance in human thinking and problem solving of the behavior that it labels “intuition,” “insight,” and “understanding.” This paper discusses computer programs, already described in the published literature, that stimulate exactly these kinds of behaviors. It is shown that much of what has been discussed under the heading of “insight” can be explained in terms of recognition processes that are readily simulated. Computer simulation has shown itself a powerful tool for interpreting and explaining a wide range of phenomena associated with the kinds of thinking and understanding that have been so usefully emphasized in the Gestalt literature.}
}
@article{HAMMOND199287,
title = {Cal as a trojan horse for educational change: The case of psychology},
journal = {Computers & Education},
volume = {19},
number = {1},
pages = {87-95},
year = {1992},
issn = {0360-1315},
doi = {https://doi.org/10.1016/0360-1315(92)90014-V},
url = {https://www.sciencedirect.com/science/article/pii/036013159290014V},
author = {Nick Hammond and Annie Trapp},
abstract = {The use of CAL in psychology teaching in higher education in the U.K. is reviewed from a perspective of educational change. Approaches vary in how they challenge the traditional educational and organization assumptions of university teaching, though virtually all require some re-thinking of educational objectives and context of use.}
}
@article{CLEGG201856,
title = {Analysis of a train-operating company's customer service system during disruptions: Conceptual requirements for gamifying frontline staff development},
journal = {Journal of Rail Transport Planning & Management},
volume = {8},
number = {1},
pages = {56-77},
year = {2018},
issn = {2210-9706},
doi = {https://doi.org/10.1016/j.jrtpm.2017.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S2210970617300495},
author = {Ben Clegg and Richard Orme and Chris Owen and Pavel Albores},
keywords = {Customer service systems, Gamification, Systems thinking, Mitigate-Plan-React-Recovery (MPRR) framework, Disruption management, Frontline staff training},
abstract = {This paper provides an account of an action research study into the systemic success factors which help frontline staff react to and recover from a rail service disruption. This study focuses on the effective use of information during a disruption to improve customer service, as this is a priority area for train-operating companies (TOCs) in Great Britain. A novel type of systems thinking, known as Process-Oriented Holonic (PrOH) Modelling, has been used to investigate and model the ‘Passenger Information During Disruption’ (PIDD) system. This paper presents conceptual requirements for a gamified learning environment; it describes ‘what’, ‘how’ and ‘when’ these systemic success factors could be gamified using a popular disruption management reference framework known as the Mitigate, Prepare, React and Recover (MPRR) framework. This paper will interest managers of and researchers into customer service system disruptions, as well as those wishing to develop new gamified learning environments to improve customer service systems.}
}
@incollection{VOINOV2019164,
title = {Modules and Integrated Modeling☆},
editor = {Brian Fath},
booktitle = {Encyclopedia of Ecology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {164-169},
year = {2019},
isbn = {978-0-444-64130-4},
doi = {https://doi.org/10.1016/B978-0-12-409548-9.11142-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012409548911142X},
author = {Alexey Voinov and Paul A. Fishwick},
keywords = {Allometry, Boundary conditions, Constants, Control functions, Experimental data, Forcing functions, Measurements, Monitoring, Stoichiometry},
abstract = {In the modular approach we do not intend to design a unique general model. Instead, the goal is to offer a framework that can be easily extended and is flexible to be modified. A module that performs best in one case may not be sufficient in another. The goals and scale of a particular study may require a completely different set of modules that will be invoked and further translated into a working model. There is a certain disparity between the software developer and the researcher views upon models and modules. For a software developer, a module is an entity, a black box, which should be as independent as possible, and should be as easy as possible to combine with other modules. This is especially true for the federation approach to modular modeling and is well demonstrated by the web-based modeling systems. The utility of such applications may be marginal from the research viewpoint. For a researcher a model is predominantly a tool for understanding the system. By plugging together a number of black boxes, for which specifics and behavior is obscure and hardly understood, we do not significantly increase our knowledge about the system. The results generated are difficult to interpret when there is not enough understanding of the processes that are actually modeled. The decomposition of such systems requires careful analysis of spatial and temporal scales of processes considered and is very closely related to specific goals of the model built. In this context the modular approach can be useful if the focus is shifted from reusability and “plug-and-play,” to transparency, analysis and hierarchical description of various processes and system components. With the modules being transparent and open for experiment and analysis, the researcher can better understand the specifics of the model formalism that is inherited. It is then easier to decide whether a module is suitable or if it should be modified and tuned to the specific goals of a particular study. However in other cases, when the goal is to deliver a system simulation and perhaps a forecast as fast as possible, then we may be less interested in the contents of a module and more concerned about its reliability, accuracy, validation in previous module applications. Modular systems thinking is the way to achieve a number of critical goals: (1) we understand a system better since we can look inside of the system over multiple levels of encapsulation and conceptual abstraction, (2) we foster an economy based on modular systems, allowing exchange of objects via a web-based “module repository,” and (3) we are gradually making modules capable of being interacted-with via special interfaces (i.e., collaborative modeling).}
}
@article{RAKHIMOV20181,
title = {Uncertainty Quantification method for CFD applied to the turbulent mixing of two water layers},
journal = {Nuclear Engineering and Design},
volume = {333},
pages = {1-15},
year = {2018},
issn = {0029-5493},
doi = {https://doi.org/10.1016/j.nucengdes.2018.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0029549318303959},
author = {A. Cutrono Rakhimov and D.C. Visser and E.M.J. Komen},
keywords = {Uncertainty Quantification, CFD simulation, Turbulent mixing, Latin Hypercube Sampling, Richardson extrapolation},
abstract = {Computer codes contain sources of uncertainty. Therefore, one need to quantify the uncertainty in the physical models, the corresponding inputs, and the applied numerical methods used in a computer code, in order to assess the reliability of the results. Uncertainty Quantification (UQ) is therefore common practice for most fast running codes which easily allow to run thousands of simulations. However, for computationally demanding Computational Fluid Dynamics (CFD) codes, UQ is a challenge! Due to the continuous increase in computer power on the one hand and the development of sophisticated UQ methods on the other hand, UQ for CFD is becoming more and more feasible nowadays. This work aims to evaluate the CFD prediction and associated UQ for the OECD/NEA benchmark based on a GEMIX (GEneric MIxing eXperiment) mixing layer test. Mixing problems are often encountered in nuclear systems and are typical single phase issues for which CFD may bring benefits. The presented CFD-UQ methodology is based on (a) the ASME Verification and Validation (V&V) standard for UQ in CFD applications, (b) the propagation of uncertain input parameters, and (c) Richardson extrapolation to evaluate spatial discretization uncertainty. The CFD-UQ methodology is proven to be efficient thanks to the Latin Hypercube Sampling (LHS) approach, which samples the uncertain input parameters prior to CFD propagation, in contrast to the computationally expensive coupling of CFD simulations and Monte Carlo (MC) Sampling. Namely, 20 computations were sufficient to evaluate the propagation of the uncertain input parameters. Moreover, the calculated uncertainty bands, with the presented CFD-UQ methodology, tied up to the CFD results, are effectively enclosing the experimental data. Overall, a good agreement is obtained with the experimental profiles for velocity, concentration and turbulent kinetic energy. For all three quantities, the presented CFD-UQ methodology was ranked by the benchmark organizers within the top 5 (out of 13 submissions). This shows that the applied CFD-UQ methodology is efficient, effective, and robust.}
}
@article{BOUDRY2013660,
title = {The mismeasure of machine: Synthetic biology and the trouble with engineering metaphors},
journal = {Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences},
volume = {44},
number = {4, Part B},
pages = {660-668},
year = {2013},
issn = {1369-8486},
doi = {https://doi.org/10.1016/j.shpsc.2013.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S1369848613000812},
author = {Maarten Boudry and Massimo Pigliucci},
keywords = {Synthetic biology, Adaptationism, Reverse engineering, Organism-machine metaphor, Analogical thinking},
abstract = {The scientific study of living organisms is permeated by machine and design metaphors. Genes are thought of as the “blueprint” of an organism, organisms are “reverse engineered” to discover their functionality, and living cells are compared to biochemical factories, complete with assembly lines, transport systems, messenger circuits, etc. Although the notion of design is indispensable to think about adaptations, and engineering analogies have considerable heuristic value (e.g., optimality assumptions), we argue they are limited in several important respects. In particular, the analogy with human-made machines falters when we move down to the level of molecular biology and genetics. Living organisms are far more messy and less transparent than human-made machines. Notoriously, evolution is an opportunistic tinkerer, blindly stumbling on “designs” that no sensible engineer would come up with. Despite impressive technological innovation, the prospect of artificially designing new life forms from scratch has proven more difficult than the superficial analogy with “programming” the right “software” would suggest. The idea of applying straightforward engineering approaches to living systems and their genomes—isolating functional components, designing new parts from scratch, recombining and assembling them into novel life forms—pushes the analogy with human artifacts beyond its limits. In the absence of a one-to-one correspondence between genotype and phenotype, there is no straightforward way to implement novel biological functions and design new life forms. Both the developmental complexity of gene expression and the multifarious interactions of genes and environments are serious obstacles for “engineering” a particular phenotype. The problem of reverse-engineering a desired phenotype to its genetic “instructions” is probably intractable for any but the most simple phenotypes. Recent developments in the field of bio-engineering and synthetic biology reflect these limitations. Instead of genetically engineering a desired trait from scratch, as the machine/engineering metaphor promises, researchers are making greater strides by co-opting natural selection to “search” for a suitable genotype, or by borrowing and recombining genetic material from extant life forms.}
}
@incollection{SCHUTZE2025395,
title = {Public Health Professionals},
editor = {Stella R. Quah},
booktitle = {International Encyclopedia of Public Health (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Oxford},
pages = {395-402},
year = {2025},
isbn = {978-0-323-97280-2},
doi = {https://doi.org/10.1016/B978-0-323-99967-0.00167-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323999670001678},
author = {Heike Schütze},
keywords = {Public health, Health workforce, Public health professionals, Public health competencies, Public health education, Public health functions},
abstract = {A greater acceptance that health cannot be addressed by one sector alone has seen public health expand and develop as a profession separately identified from medicine. Public health professionals now come from diverse backgrounds, including program management, policy development, research, and surveillance. They form an important part of the health workforce, together with clinical practitioners and other health professionals. Education and training of public health professionals includes bachelor, master, and doctoral university programs, and continuing professional education on specific topics. However, some low-and middle-income countries continue to restrict public health education to community and preventive medicine, offering these degrees to physicians only. Globalization, the multi-sectoral nature of public health, and health professional migration continue to present several challenges in this context.}
}
@article{WANG201081,
title = {On the cognitive process of human problem solving},
journal = {Cognitive Systems Research},
volume = {11},
number = {1},
pages = {81-92},
year = {2010},
note = {Brain Informatics},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2008.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S1389041708000417},
author = {Yingxu Wang and Vincent Chiew},
keywords = {Cognitive informatics, Cognitive computing, Brain informatics, Computational intelligence, Reference model of the brain, Cognitive processes, Problem solving, Mathematical model, Concept algebra, RTPA},
abstract = {One of the fundamental human cognitive processes is problem solving. As a higher-layer cognitive process, problem solving interacts with many other cognitive processes such as abstraction, searching, learning, decision making, inference, analysis, and synthesis on the basis of internal knowledge representation by the object–attribute-relation (OAR) model. Problem solving is a cognitive process of the brain that searches a solution for a given problem or finds a path to reach a given goal. When a problem object is identified, problem solving can be perceived as a search process in the memory space for finding a relationship between a set of solution goals and a set of alternative paths. This paper presents both a cognitive model and a mathematical model of the problem solving process. The cognitive structures of the brain and the mechanisms of internal knowledge representation behind the cognitive process of problem solving are explained. The cognitive process is formally described using real-time process algebra (RTPA) and concept algebra. This work is a part of the cognitive computing project that designed to reveal and simulate the fundamental mechanisms and processes of the brain according to Wang’s layered reference model of the brain (LRMB), which is expected to lead to the development of future generation methodologies for cognitive computing and novel cognitive computers that are capable of think, learn, and perceive.}
}
@article{KAPUS2020100582,
title = {Specifying reversibility with TLA+},
journal = {Journal of Logical and Algebraic Methods in Programming},
volume = {116},
pages = {100582},
year = {2020},
issn = {2352-2208},
doi = {https://doi.org/10.1016/j.jlamp.2020.100582},
url = {https://www.sciencedirect.com/science/article/pii/S2352220820300675},
author = {Tatjana Kapus},
keywords = {Concurrent system, Reversible computation, Temporal logic of actions, Model checking},
abstract = {In the past, action-based, process-algebraic formalisms for the description and analysis of concurrent reversible computations were mainly developed. In this paper, we present a state-based approach to the specification of concurrent systems in which forward-executed actions may either be executed in reverse in a causal-consistent uncontrolled fashion or are irreversible. The basic underlying system semantics is assumed to be a set of possible infinite sequences of states with actions defined as state transitions, which allows us to specify reversibility with the specification language TLA+ and to use its tool support for specification editing and verification. We provide definitions of TLA+ operators for the specification of causal-consistent reversibility and irreversible actions in a uniform way. The reversibility is achieved by remembering as much computation history as necessary with regard to the irreversible actions. The applicability of the approach is illustrated with examples, including the modelling of the influence of the Raf kinase inhibitor protein on the extracellular signal-regulated kinase signalling pathway and parameterised specification of a system of dining philosophers.}
}
@article{GOLUBICKIS2022105207,
title = {Sticky me: Self-relevance slows reinforcement learning},
journal = {Cognition},
volume = {227},
pages = {105207},
year = {2022},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2022.105207},
url = {https://www.sciencedirect.com/science/article/pii/S0010027722001950},
author = {Marius Golubickis and C. Neil Macrae},
keywords = {Self, Learning, Self-prioritization, Probabilistic selection task, Reinforcement learning drift diffusion model},
abstract = {A prominent facet of social-cognitive functioning is that self-relevant information is prioritized in perception, attention, and memory. What is not yet understood, however, is whether similar effects arise during learning. In particular, compared to other people (e.g., best friend) is information about the self acquired more rapidly? To explore this matter, here we used a probabilistic selection task in combination with computational modeling (i.e., Reinforcement Learning Drift Diffusion Model analysis) to establish how self-relevance influences learning under conditions of uncertainty (i.e., choices are based on the perceived likelihood of positive and negative outcomes). Across two experiments, a consistent pattern of effects was observed. First, learning rates for both positive and negative prediction errors were slower for self-relevant compared to friend-relevant associations. Second, self-relevant (vs. friend-relevant) learning was characterized by the exploitation (vs. exploration) of choice selections. That is, in a complex (i.e., probabilistic) decision-making environment, previously rewarded self-related outcomes were selected more often than novel — but potentially riskier — alternatives. The implications of these findings for accounts of self-function are considered.}
}
@article{CHARALAMBOS2024101838,
title = {p5.quadrille.js: P5.js quadrille library},
journal = {SoftwareX},
volume = {27},
pages = {101838},
year = {2024},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2024.101838},
url = {https://www.sciencedirect.com/science/article/pii/S2352711024002097},
author = {Jean Pierre Charalambos},
keywords = {Game-based learning, Creative-coding, p5.js library},
abstract = {This paper introduces p5.quadrille.js, an open-source p5.js library and game-based learning tool for creative coding. It enables board game design, creative code applications like photo and video mosaics, and visualizing complex concepts like image convolution. The design of the library ensures students engage with essential skills in functional and object-oriented programming, set and logical operations, geometric transformations, and game design. Illustrative examples demonstrate its main functionalities, and a preliminary evaluation highlights the potential for future quantitative research to assess its impact on learning outcomes.}
}
@article{MAGALHAES20151157,
title = {Establishment ofAutomatization as a Requirement for Time Management Input Modules in Project Management Information Systems for Academic Activities – A Game Theory Approach},
journal = {Procedia Computer Science},
volume = {64},
pages = {1157-1162},
year = {2015},
note = {Conference on ENTERprise Information Systems/International Conference on Project MANagement/Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN / HCist 2015 October 7-9, 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.08.596},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915027313},
author = {Sérgio Tenreiro de Magalhães and Maria José Magalhães and Vítor J. Sá},
keywords = {Academic Activities, Project Management, Project Management Information Systems, Game Theory.},
abstract = {Academics are expected to engage in several works in several different domains, namely research and development, general management and services to the community, while lecturing a set of courses. Academics might differ in their preference for some of these activities and also in their corresponding performance. Quality assurance in academic institutions implies monitoring performance, what is frequently done by measuring a set of quantitative results at the end of a certain period. Project Management best practices can change this frequent practice, introducing, for instance, the concept of cost efficiency, allowing for objective comparisons between different types of activities. For this to happen there is a need to monitor the time spent by each academic in each activities or, at least, in each set of activities of the same type. The challenge is to know how to do that. Game Theory has been studying decision making in competitive environment, which is increasingly the case in academic institutions. Therefore, there is a primary need to verify if a relevant percentage of the academics have a perception that there is an incentive to lie in their timesheets, due to competitive thinking. This paper presents a pilot study that allowed concluding that time management input modules in project management information systems for academic activities must be automated, eliminating the human factor in timesheet fillings.}
}
@article{MOGILNER2011692,
title = {Modeling cellular processes in 3D},
journal = {Trends in Cell Biology},
volume = {21},
number = {12},
pages = {692-700},
year = {2011},
issn = {0962-8924},
doi = {https://doi.org/10.1016/j.tcb.2011.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0962892411001978},
author = {Alex Mogilner and David Odde},
abstract = {Recent advances in photonic imaging and fluorescent protein technology offer unprecedented views of molecular space–time dynamics in living cells. At the same time, advances in computing hardware and software enable modeling of ever more complex systems, from global climate to cell division. As modeling and experiment become more closely integrated we must address the issue of modeling cellular processes in 3D. Here, we highlight recent advances related to 3D modeling in cell biology. While some processes require full 3D analysis, we suggest that others are more naturally described in 2D or 1D. Keeping the dimensionality as low as possible reduces computational time and makes models more intuitively comprehensible; however, the ability to test full 3D models will build greater confidence in models generally and remains an important emerging area of cell biological modeling.}
}
@incollection{JUNGCK201237,
title = {Chapter Three - Mathematics Make Microbes Beautiful, Beneficial, and Bountiful},
editor = {Sima Sariaslani and Geoffrey M. Gadd},
series = {Advances in Applied Microbiology},
publisher = {Academic Press},
volume = {80},
pages = {37-80},
year = {2012},
issn = {0065-2164},
doi = {https://doi.org/10.1016/B978-0-12-394381-1.00003-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780123943811000039},
author = {John R. Jungck},
keywords = {Mathematics, Modeling, Analysis, Hypothesis testing, Problem solving, Visualization, Ordering, Bioinformatics, Phylogenetics, Epidemiology, Graph theory, Computational geometry, Heuristics, Algorithms, Education},
abstract = {Microbiology is a rich area for visualizing the importance of mathematics in terms of designing experiments, data mining, testing hypotheses, and visualizing relationships. Historically, Nobel Prizes have acknowledged the close interplay between mathematics and microbiology in such examples as the fluctuation test and mutation rates using Poisson statistics by Luria and Delbrück and the use of graph theory of polyhedra by Caspar and Klug. More and more contemporary microbiology journals feature mathematical models, computational algorithms and heuristics, and multidimensional visualizations. While revolutions in research have driven these initiatives, a commensurate effort needs to be made to incorporate much more mathematics into the professional preparation of microbiologists. In order not to be daunting to many educators, a Bloom–like “Taxonomy of Quantitative Reasoning” is shared with explicit examples of microbiological activities for engaging students in (a) counting, measuring, calculating using image analysis of bacterial colonies and viral infections on variegated leaves, measurement of fractal dimensions of beautiful colony morphologies, and counting vertices, edges, and faces on viral capsids and using graph theory to understand self assembly; (b) graphing, mapping, ordering by applying linear, exponential, and logistic growth models of public health and sanitation problems, revisiting Snow’s epidemiological map of cholera with computational geometry, and using interval graphs to do complementation mapping, deletion mapping, food webs, and microarray heatmaps; (c) problem solving by doing gene mapping and experimental design, and applying Boolean algebra to gene regulation of operons; (d) analysis of the “Bacterial Bonanza” of microbial sequence and genomic data using bioinformatics and phylogenetics; (e) hypothesis testing–again with phylogenetic trees and use of Poisson statistics and the Luria–Delbrück fluctuation test; and (f) modeling of biodiversity by using game theory, of epidemics with algebraic models, bacterial motion by using motion picture analysis and fluid mechanics of motility in multiple dimensions through the physics of “Life at Low Reynolds Numbers,” and pattern formation of quorum sensing bacterial populations. Through a developmental model for preprofessional education that emphasizes the beauty, utility, and diversity of microbiological systems, we hope to foster creativity as well as mathematically rigorous reasoning.}
}
@article{FUQUA2022117753,
title = {Commodity demand forecasting using modulated rank reduction for humanitarian logistics planning},
journal = {Expert Systems with Applications},
volume = {206},
pages = {117753},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117753},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422010314},
author = {Donovan Fuqua and Steven Hespeler},
keywords = {Time-series prediction, Data efficient machine learning, Rank reduction, Walk-forward validation},
abstract = {Demand prediction for humanitarian logistics is a complex problem with immediate real-world consequences. This paper examines fuel demand during two regional humanitarian crisis events and the supply chain operated by the US Government as part of Operation Unified Response. Because typical machine learning algorithms require large amounts of training data, our methods for predictive analysis depend on rapid training of a model where re-sampling would not be useful due to dynamic time-series data. We propose an online robust principal components analysis (RPCA) model combined with a long short-term memory (LSTM) recurrent network to address this challenge. Our computational results demonstrate that the proposed model can predict demand efficiently on real-world humanitarian supply datasets and well-known benchmark datasets in the University of California, Irvine (UCI) Machine Learning Repository. This method also allows us to tune training lag in online learning.}
}
@article{GAEL2018239,
title = {A New Micro-Batch Approach for Partial Least Square Clusterwise Regression},
journal = {Procedia Computer Science},
volume = {144},
pages = {239-250},
year = {2018},
note = {INNS Conference on Big Data and Deep Learning},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.525},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918322348},
author = {Beck Gaël and Azzag Hanane and Bougeard Stéphanie and Lebbah Mustapha and Niang Ndèye},
keywords = {Clusterwise, PLS, Spark},
abstract = {Current implementations of Clusterwise methods for regression when applied to massive data either have prohibitive computational costs or produce models that are difficult to interpret. We introduce a new implementation Micro-Batch Clusterwise Partial Least Squares (mb-CW-PLS), which is consists of two main improvements: (a) a scalable and distributed computational framework and (b) a micro-batch Clusterwise regression using buckets (micro-clusters). With these improvements, we are able to produce interpretable regression models with multicollinearity within a reasonable time frame.}
}
@article{RAUFI2024100196,
title = {Exploring Indonesian EFL students’ lexical diversity and its correlation with academic vocabulary use in an online academic writing environment},
journal = {Ampersand},
volume = {13},
pages = {100196},
year = {2024},
issn = {2215-0390},
doi = {https://doi.org/10.1016/j.amper.2024.100196},
url = {https://www.sciencedirect.com/science/article/pii/S2215039024000341},
author = {Beauty Sholeha Raufi and Herri Mulyono and Hamzah Puadi Ilyas and Siti Zulaiha},
keywords = {Lexical diversity, EFL, Online discussion and assignments, Academic writing},
abstract = {This paper reports on a quasi-experimental study designed to explore the relationship between lexical diversity and academic vocabulary use among Indonesian EFL university students engaging in online academic writing discussions and assignments. The study analysed 11,624 tokens and 5437 types collected from students' contributions in online discussions and academic writing assignments. Findings revealed that the level of lexical diversity among the students was significantly varied depending on the text length and the total amount of data. It was found most students were indicated to possess average ESL level of academic writing with a D value ranging from 50 to 70 while some others indicated a high level of adult ESL with developed academic text in the range between 70 and 80 D level. Moreover, it was also found that the degree of lexical diversity was not affected by the writing topic familiarity or students' academic year. Findings revealed a statistically significant negative correlation between measures of lexical diversity and academic vocabulary use. Contrary to expectations, while a diversified vocabulary generally correlates positively with student writing performance, this relationship reverses in contexts emphasizing the academic quality of writing. These results highlight the complex dynamics of lexical usage in academic writing and suggest that mere lexical diversity does not always equate to higher academic writing quality. This study contributes to a critical understanding of how lexical diversity functions in digital academic environments and offers implications for English language teaching practices in higher education settings.}
}
@article{WU1999147,
title = {Path planning and prototype design of an AGV},
journal = {Mathematical and Computer Modelling},
volume = {30},
number = {7},
pages = {147-167},
year = {1999},
issn = {0895-7177},
doi = {https://doi.org/10.1016/S0895-7177(99)00171-5},
url = {https://www.sciencedirect.com/science/article/pii/S0895717799001715},
author = {Kun-Hsiang Wu and Chin-Hsing Chen and Juing-Ming Ko and Jiann-Der Lee},
keywords = {Mobile robot, Path planning, Distance transform, Potential field, Fuzzy logic control},
abstract = {In this paper, a path planning method using fuzzy logic control and potential field for an Automatic Guided Vehicle (AGV) is proposed. The design and implementation of an AGV prototype are also presented. Prom a top-view image of an environment, the chamfer distance transform is used to build the artificial potential field required. The potential field method is used to calculate the repulsive force between the vehicle and the closest obstacle, and the attractive force generated by the goal. Then the resultant force guides the mobile vehicle to its destination. When trap situations occur and the AGV may not reach the goal or even collide the obstacles, a fuzzy logic controller is proposed to modify the direction of the AGV. Based on the angle between the obstacle and the goal, and the status of the AGV, a correction angle is generated by the simple fuzzy rules. Fuzzy logic control with the characteristic of simulating human thinking renders the path of the AGV smoother and safer. A prototype of the experimental AGV is built by modifying a consumer model car. The built mobile vehicle is shown to track the desired trajectories successfully according to the path planning algorithm we proposed. A series of simulations, based on window-frame type environments with geometric obstacles, show that the fuzzy logic control is able to make the AGV escape from trap situations and generate a smoother and safer trajectory.}
}