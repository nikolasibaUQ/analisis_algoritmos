@article{GENT202238,
title = {Making a mind},
journal = {New Scientist},
volume = {253},
number = {3374},
pages = {38-41},
year = {2022},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(22)00294-9},
url = {https://www.sciencedirect.com/science/article/pii/S0262407922002949},
author = {Edd Gent},
abstract = {In the push to make artificial intelligence that thinks like humans, many researchers are focused on fresh insights from neuroscience. Should they be looking to psychology instead, asks Edd Gent}
}
@article{GLOBIG2024101852,
title = {Considering information-sharing motives to reduce misinformation},
journal = {Current Opinion in Psychology},
volume = {59},
pages = {101852},
year = {2024},
issn = {2352-250X},
doi = {https://doi.org/10.1016/j.copsyc.2024.101852},
url = {https://www.sciencedirect.com/science/article/pii/S2352250X24000654},
author = {Laura K. Globig and Tali Sharot},
keywords = {Social media, Misinformation, Social incentives, Reward-learning, Nudges},
abstract = {Misinformation has risen in recent years, negatively affecting domains ranging from politics to health. To curb the spread of misinformation it is useful to consider why, how, and when people decide to share information. Here we suggest that information-sharing decisions are value-based choices, in which sharers strive to maximize rewards and minimize losses to themselves and/or others. These outcomes can be tangible, in the form of monetary rewards or losses, or intangible, in the form of social feedback. On social media platforms these rewards and losses are not clearly tied to the accuracy of information shared. Thus, sharers have little incentive to avoid disseminating misinformation. Based on this framework, we propose ways to nudge sharers to prioritize accuracy during information-sharing.}
}
@article{MOHAMED2010317,
title = {Investigating Number Sense Among Students},
journal = {Procedia - Social and Behavioral Sciences},
volume = {8},
pages = {317-324},
year = {2010},
note = {International Conference on Mathematics Education Research 2010 (ICMER 2010)},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2010.12.044},
url = {https://www.sciencedirect.com/science/article/pii/S187704281002149X},
author = {Mohini Mohamed and Jacinta Johnny},
keywords = {Number sense, Mental computation, Number sense test, Number sense framework},
abstract = {Number sense can be described as good intuition about numbers and their relationships. Individuals with good number sense tend to exhibit the following characteristics when performing mental computations; sense-making approach, planning and control, flexibility and appropriateness sense of reasonableness. This is a very important skill to be mastered by every individual to enable them to handle numerical problems in their daily life. Students rarely face problems with algorithms. Unfortunately, many studies have showed that students have poor understanding in making sense on numbers when tested on their competency in number sense component. This study aims to investigate if there is a relationship between student performance in number sense and mathematics achievement and to explore the components of number sense that students are weak in.}
}
@article{ZHOU2019244,
title = {Lightweight IoT-based authentication scheme in cloud computing circumstance},
journal = {Future Generation Computer Systems},
volume = {91},
pages = {244-251},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.08.038},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18307878},
author = {Lu Zhou and Xiong Li and Kuo-Hui Yeh and Chunhua Su and Wayne Chiu},
keywords = {Internet-of-things (IoT), Cloud computing, Authentication, Proverif, User tracking},
abstract = {Recently, authentication technologies integrated with the Internet of Things (IoT) and cloud computing have been promptly investigated for secure data retrieval and robust access control on large-scale IoT networks. However, it does not have a best practice for simultaneously deploying IoT and cloud computing with robust security. In this study, we present a novel authentication scheme for IoT-based architectures combined with cloud servers. To pursue the best efficiency, lightweight crypto-modules, such as one-way hash function and exclusive-or operation, are adopted in our authentication scheme. It not only removes the computation burden but also makes our proposed scheme suitable for resource-limited objects, such as sensors or IoT devices. Through the formal verification delivered by Proverif, the security robustness of the proposed authentication scheme is guaranteed. Furthermore, the performance evaluation presents the practicability of our proposed scheme in which a user-acceptable computation cost is achieved.}
}
@article{MUNUZURI202264,
title = {Unified representation of Life's basic properties by a 3-species Stochastic Cubic Autocatalytic Reaction-Diffusion system of equations},
journal = {Physics of Life Reviews},
volume = {41},
pages = {64-83},
year = {2022},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2022.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S1571064522000185},
author = {Alberto P. Muñuzuri and Juan Pérez-Mercader},
keywords = {Living systems, Turing instability, Top-down approach, Bottom-up approach, Non-linear reaction-diffusion equations, Properties of life},
abstract = {Today we can use physics to describe in great detail many of the phenomena intervening in the process of life. But no analogous unified description exists for the phenomenon of life itself. In spite of their complexity, all living creatures are out of equilibrium chemical systems sharing four fundamental properties: they (1) handle information, (2) metabolize, (3) self-reproduce and (4) evolve. This small number of features, which in terran life are implemented with biochemistry, point to an underlying simplicity that can be taken as a guide to motivate and implement a theoretical physics style unified description of life using tools from the non-equilibrium physical-chemistry of extended systems. Representing a system with general rules is a well stablished approach to model building and unification in physics, and we do this here to provide an abstract mathematical description of life. We start by reviewing the work of previous authors showing how the properties in the above list can be individually represented with stochastic reaction-diffusion kinetics using polynomial reaction terms. These include “switches” and computation, the kinetic representation of autocatalysis, Turing instability and adaptation in the presence of both deterministic and stochastic environments. Thinking of these properties as existing on a space-time lattice each of whose nodes are subject to a common mass-action kinetics compatible with the above, leads to a very rich dynamical system which, just as natural life, unifies the above properties and can therefore be interpreted as a high level or “outside-in” theoretical physics representation of life. Taking advantage of currently available advanced computational techniques and hardware, we compute the phase plane for this dynamical system both in the deterministic and stochastic cases. We do simulations and show numerically how the system works. We review how to extract useful information that can be mapped into emergent physical phenomena and attributes of importance in life such as the presence of a “membrane” or the time evolution of an individual system's negentropy or mass. Once these are available, we illustrate how to perform some basic phenomenology based on the model's numerical predictions. Applying the above to the idealization of the general Cell Division Cycle (CDC) given almost 25 years ago by Hunt and Murray, we show from the numerical simulations how this system executes a form of the idealized CDC. We also briefly discuss various simulations that show how other properties of living systems such as migration towards more favorable regions or the emergence of effective Lotka-Volterra populations are accounted for by this general and unified view from the “top” of the physics of life. The paper ends with some discussion, conclusions, and comments on some selected directions for future research. The mathematical techniques and powerful simulation tools we use are all well established and presented in a “didactical” style. We include a very rich but concise SI where the numerical details are thoroughly discussed in a way that anyone interested in studying or extending the results would be able to do so.}
}
@article{SHARMA2022100694,
title = {eFeed-Hungers 2.0: Pervasive computing, sustainable feeding to purge global hunger},
journal = {Sustainable Computing: Informatics and Systems},
volume = {35},
pages = {100694},
year = {2022},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2022.100694},
url = {https://www.sciencedirect.com/science/article/pii/S2210537922000361},
author = {Sugam Sharma and Ritu Shandilya and Kihwan Kim and Debasis Mandal and U. Sunday Tim and Johnny Wong},
keywords = {Pervasive, Hunger, Sustainable, Feed, Food waste, Mitigate, Computing, eFeed-Hungers},
abstract = {In this paper, we further strengthen our eFeed-Hungers research efforts in fighting the hunger using the advanced computer technologies. According to the United Nations hunger reports, the amount of the globally produced food is adequate enough to serve the whole world of about 7.5 billion people. Despite this, the world is globally experiencing the growing hunger and growing food waste issues and consequently, the food-deprived suffering. The primary cause of this is dubbed as the unavailability of the proper conduits to efficiently and effectively channel the food waste to the food-deprived population. The eFeed-Hungers is one such globally popular computational framework that provenly addresses these concerns and challenges and smoothly connects the edible food waste to needy. In this research, we have extended the strength of eFeed-Hungers platform to smartly address the mass donations at one location. Further, the required technical artifacts are appropriately implemented to expand the horizon of eFeed-Hungers operations to respond to the global communities with disparate time zones. Additionally, the eFeed-Hungers ecosystem is equipped with the verification and validation system for the incoming food donations to further ensure the consumer’s safely and security. The system is also developed to be responsive especially for the basic pervasive computing devices - mobiles or smartphones of any dimensions to equip the donors as well as consumers to respectively donate and search the food pervasively.}
}
@article{BOLER201875,
title = {The affective politics of the “post-truth” era: Feeling rules and networked subjectivity},
journal = {Emotion, Space and Society},
volume = {27},
pages = {75-85},
year = {2018},
issn = {1755-4586},
doi = {https://doi.org/10.1016/j.emospa.2018.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S1755458617301585},
author = {Megan Boler and Elizabeth Davis},
keywords = {Affect, Emotion, Post-truth, Feeling rules, Truthiness, Digital media, Algorithmic governance, Computational propaganda},
abstract = {This essay maps interdisciplinary lines of inquiry to assess current research on affect and emotion in relation to digital and social media, in the context of the fractured news media landscape and increasingly visible emotionality in political life. The essay sketches the context of polarized emotionality and the crisis of truth characterizing current U.S. politics, centrally engaging Arlie Hochschild's concept of “feeling rules”. We explore the limitations of “affect theory” for researching mediatized politics, contending that the stark differentiation of “affect” from “emotion” reifies the rational, autonomous, liberal conception of the subject, and is of limited value for political communications research. Instead, we emphasize the relational nature of affect and emotion, and the value of feminist politics of emotion research. Our analysis evaluates the limitations of contemporary scholarship on affect, social media, and politics in the context of the grave challenges posed by algorithmic governance and computational propaganda. We conclude by suggesting the concept of “networked subjectivity” for understanding mediatized politics, and the importance of the “affective feedback loop” within the context of the social media “culture of likes.”}
}
@article{BAUERNEGRINI2022105785,
title = {Usability evaluation of circRNA identification tools: Development of a heuristic-based framework and analysis},
journal = {Computers in Biology and Medicine},
volume = {147},
pages = {105785},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105785},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522005522},
author = {Guilherme Bauer-Negrini and Guilherme {Cordenonsi da Fonseca} and Carmem Gottfried and Juliana Herbert},
keywords = {Usability, Bioinformatics, circRNA, Heuristic evaluation, Command-line interface},
abstract = {Background and objective
Circular RNAs (circRNAs) are endogenous molecules of non-coding RNA that form a covalently closed loop at the 3′ and 5′ ends. Recently, the role of these molecules in the regulation of gene expression and their involvement in several human pathologies has gained notoriety. The identification of circRNAs is highly dependent on computational methods for analyzing RNA sequencing data. However, bioinformatics software is known to be problematic in terms of usability. Evidence points out that tools for identifying circRNAs can have such problems, negatively impacting researchers in this field. Here we present a heuristic-based framework for evaluating the usability of command-line circRNA identification software.
Methods
We used heuristics evaluation to comprehensively identify the usability issues in a sample of circRNA identification tools.
Results
We identified 46 usability issues presented individually in four tools. Most of the issues had cosmetic or minor severity. These are unlikely to challenge experienced users but may cause inconvenience for novice users. We also identified severe issues with the potential to harm users regardless of their experience. The areas most affected were the documentation and the installability of the tools.
Conclusions
With the proposed framework, we formally describe, for the first time, the usability problems that can affect users in this area of circRNA research. We hope that our framework can help researchers evaluate their software's usability during development.}
}
@article{STAMMITTI2013e58,
title = {Spreadsheets for assisting Transport Phenomena Laboratory experiences},
journal = {Education for Chemical Engineers},
volume = {8},
number = {2},
pages = {e58-e71},
year = {2013},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2013.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S1749772813000067},
author = {Aurelio Stammitti},
keywords = {Educational spreadsheets, Transport Phenomena Laboratory, Laboratory experience quality, Data processing task, Hands-on learning, Student analytical thinking},
abstract = {Academic laboratories have been traditionally used for complementing and reinforcing in a practical way the theoretical instruction received in classroom lectures. However, data processing and model evaluation tasks are time consuming and do not add much value to the student's learning experience as they reduce available time for result analysis, critical thinking and report writing skills development. Therefore, this project addressed this issue by selecting three experiences of the Transport Phenomena Laboratory, namely: metallic bar temperature profiles, transient heat conduction and fixed and fluidised bed behaviour, and developed a spreadsheet for each one of them. These spreadsheets, without demanding programming skills, easily process experimental data sets, evaluate complex analytical and numerical models and correlations, not formerly considered and, convey results in tables and plots. Chemical engineering students that tested the spreadsheets were surveyed and expressed the added value of the sheets, being user-friendly, helped them to fulfil lab objectives by reducing their workload and, allowed them to complete deeper analyses that instructors could not request before, as they were able to quickly evaluate, compare and validate different model assumptions and correlations. Students also provided valuable suggestions for improving the spreadsheet experience. Through these sheets, students’ lab learning experience was updated.}
}
@article{HASSAN2024128058,
title = {Unfolding Explainable AI for Brain Tumor Segmentation},
journal = {Neurocomputing},
volume = {599},
pages = {128058},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128058},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224008294},
author = {Muhammad Hassan and Ahmed Ameen Fateh and Jieqiong Lin and Yijiang Zhuang and Guisen Lin and Hairui Xiong and Zhou You and Peiwu Qin and Hongwu Zeng},
keywords = {Segmentation, Brain Tumor, Machine Learning, Deep Learning, Explainable AI, Neuro-Symbolic Learning},
abstract = {Brain tumor segmentation (BTS) has been studied from handcrafted engineered features to conventional machine learning (ML) methods, followed by the cutting-edge deep learning approaches. Each recent approach has attempted to overcome the challenges of previous methods and brought conveniences in efficacy, throughput, computation, explainability, investigation, and interpretability. Recently, deep learning (DL) algorithms show excellent performance regarding diverse fields, including image process, computer vision, health analytics, autonomous vehicles, and natural language processes; however, ultimately impediment in making the artificial intelligence explainable and interpretable to clinicians while dealing with critical health informatics and radiomics. Besides the sophisticated deep learning models for brain tumor segmentation, notorious notions like explainability, investigation, trust, and interpretability of DL raised significant concerns for clinicians in their domains. Among many DL methods, the neuro-symbolic learning (NSL) concept has gained more attention as it can contribute to explainable and interpretable AI. In the current study, we survey the prominent approaches, from handcrafted engineering conventional ML to deep learning algorithms, highlight the challenges in DL algorithms, and propose NSL architectures for BTS. Compared to existing surveys, our study not only outlines handcrafted to DL methods for BTS but also proposed explainable and interpretable pipelines appropriate for clinical practices. Our study can better facilitate novice learners in explainable AI and propose efficient, robust, interpretable DL models to facilitate the diagnosis, prognosis, and treatment of BTS.}
}
@article{THELANCETPSYCHIATRY2024481,
title = {Pluralisms in psychiatry},
journal = {The Lancet Psychiatry},
volume = {11},
number = {7},
pages = {481},
year = {2024},
issn = {2215-0366},
doi = {https://doi.org/10.1016/S2215-0366(24)00179-2},
url = {https://www.sciencedirect.com/science/article/pii/S2215036624001792},
author = { {The Lancet Psychiatry}}
}
@article{MILIK201022,
title = {On Efficient Implementation of Search Algorithm for Genome Patterns},
journal = {IFAC Proceedings Volumes},
volume = {43},
number = {24},
pages = {22-27},
year = {2010},
note = {10th IFAC Workshop on Programmable Devices and Embedded Systems},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20101006-2-PL-4019.00006},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015309812},
author = {Adam Milik and Andrzej Pulka},
keywords = {Dynamic programming, Computational methods, Pattern identification, Pattern recognition, Parallel processing, Pipeline processing},
abstract = {The presented paper describes the implementation of the computation algorithm on modern, complex programmable hardware devices. The presented algorithm originates from computation biology and works on very long chains of symbols, which come from reference patterns of the genome. The software solutions in the field are very limited and need large time and space resources. Main research efforts have been done to investigate the properties of the searching algorithm. Especially the influence of the penalty values assigned for the mismatch, the insertion and the deletion on the algorithm has been analyzed. This allows obtaining completely new algorithm that offers extremely efficient implementation and exhibits outstanding performance. The different FPGA generations have been considered as target families for the searching algorithm based on the dynamic programming idea. The obtained results are very promising and show the dominance of the dedicated platforms over the general purpose PC-based systems.}
}
@incollection{WHANGBO2005765,
title = {Chapter 26 - Concepts of perturbation, orbital interaction, orbital mixing and orbital occupation},
editor = {Clifford E. Dykstra and Gernot Frenking and Kwang S. Kim and Gustavo E. Scuseria},
booktitle = {Theory and Applications of Computational Chemistry},
publisher = {Elsevier},
address = {Amsterdam},
pages = {765-784},
year = {2005},
isbn = {978-0-444-51719-7},
doi = {https://doi.org/10.1016/B978-044451719-7/50069-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044451719750069X},
author = {Myung-Hwan Whangbo},
abstract = {Publisher Summary
This chapter describes the concepts of perturbation, orbital interaction, orbital mixing, and orbital occupation work at all levels of electronic structure. These qualitative concepts provide a conceptual framework in which to rationalize experimental/theoretical observations and to generate qualitative predictions. An important role of an electronic structure theory is to provide quantitative predictions. In this role theoretical predictions require developments of efficient programs for theoretical computations. Another important role of an electronic structure theory is to provide a conceptual framework in which to think and organize. In this role, the theoretical predictions need not be quantitative but should provide a bias toward correct thinking about further experimental and theoretical studies. When combined with the ideas of symmetry and overlap, the concepts of perturbation, orbital interaction, orbital mixing, and orbital occupation have been indispensable not only in understanding structure – property relationships in various chemical compounds but also in interpreting results of electronic structure calculations. These qualitative concepts work at all levels of electronic structure descriptions from one-electron theory neglecting self-consistent-field adjustments of orbitals to theories including electron correlation and to those including relativistic effects.}
}
@article{ROSSER202383,
title = {A conjoined intellectual journey: Richard H. Day and the journal he founded},
journal = {Journal of Economic Behavior & Organization},
volume = {210},
pages = {83-90},
year = {2023},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2023.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167268123000781},
author = {J. Barkley Rosser and Marina V Rosser},
keywords = {Adaptive economics, Chaos theory, Complexity economics, Evolutionary economics},
abstract = {In 1980 Richard H. Day co-founded and long edited the Journal of Economic Behavior and Organization (JEBO). It development reflected the intellectual journey that Day himself followed in his career from an early interest in decision-making by farmers using programming methods through studying adaptation in organizations facing bounded rationality and nonlinear dynamics to a broad evolutionary approach to long-run patterns of economic growth and development. A not widely recognized outcome of this was that JEBO would become a leading outlet for evolutionary economics, including all of its various strands, including biological, organizational, NeoSchumpeterian, Alchian efficiency, Simonian bounded rationality, evolutionary game theory, and complexity evolution.}
}
@incollection{FARMER2008228,
title = {Chapter 11 - Fragment-Based Drug Discovery},
editor = {Camille Georges Wermuth},
booktitle = {The Practice of Medicinal Chemistry (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {New York},
pages = {228-243},
year = {2008},
isbn = {978-0-12-374194-3},
doi = {https://doi.org/10.1016/B978-0-12-374194-3.00011-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780123741943000111},
author = {Bennett T. Farmer and Allen B. Reitz},
abstract = {Publisher Summary
Although target proteins are flexible and can adopt one or more of a manifold of induced conformations, binding sites on proteins have evolved to recognize a limited number of endogenous modulators and substrates and to exclude others. This chapter reviews fragment-based drug discovery (FBDD) on the historical and operational level and explains how it can be applied on a case-by-case basis. FBBD determines which molecular substructures or fragments interact with targets of interest and how they bind, and then uses that information to obtain drugs for therapy. It represents a paradigm shift in thinking of how to approach the lead generation process in drug discovery, and is an attempt to get more information rapidly while doing the same amount of work overall. The study draws comparison between the FBDD and HTS/HTL approaches. In the FBDD approach, the medicinal chemist plays the role of a combined synthetic and structural chemist. The emphasis on informatics is greatly reduced because there is less data overall and most of it, such as from NMR or X-ray crystal structures, is visually analyzed, typically being complemented only by functional assay data on just the target itself. Several different computational methods have been developed to prescreen fragment libraries as a way to select members for further study and consideration.}
}
@article{KOH2020106,
title = {Automated detection of Alzheimer's disease using bi-directional empirical model decomposition},
journal = {Pattern Recognition Letters},
volume = {135},
pages = {106-113},
year = {2020},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2020.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167865520300921},
author = {Joel En Wei Koh and Vicnesh Jahmunah and The-Hanh Pham and Shu Lih Oh and Edward J Ciaccio and U Rajendra Acharya and Chai Hong Yeong and Mohd Kamil Mohd Fabell and Kartini Rahmat and Anushya Vijayananthan and Norlisah Ramli},
abstract = {The build-up of beta-amyloid and rapid spread of tau proteins in the brain cause the death of neurons, leading to Alzheimer's disease (AD). AD is a form of dementia, and the symptoms include memory loss and decision-making difficulties. Current advanced diagnostic modalities are costly or unable to detect the histopathological features of AD. Hence a computational intelligence tool (CIT) for AD diagnosis is proposed in this study. The magnetic resonance images (MRI) of the brain are pre-processed using an adaptive histogram, and decomposed into four IMFS using bidirectional empirical mode decomposition (BEMD). Local binary patterns (LBP) are then computed per IMF, and the histograms are concatenated. Adaptive synthetic sampling (ADASYN) is applied to balance the dataset and Student's t-test is utilized for selection of highly significant features, within each fold for ten-fold validation. Amongst other classifiers, SVM-Poly 1 and random forest(RF) were employed for classification, yielding the highest accuracy of 93.9% each. Our study concludes that the recommended CIT is useful for the automatic classification of AD versus normal MRI imagery in hospitals.}
}
@article{BORATYNSKA20165529,
title = {FsQCA in corporate bankruptcy research. An innovative approach in food industry},
journal = {Journal of Business Research},
volume = {69},
number = {11},
pages = {5529-5533},
year = {2016},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2016.04.166},
url = {https://www.sciencedirect.com/science/article/pii/S0148296316303733},
author = {Katarzyna Boratyńska},
keywords = {Complexity theory, fsQCA, Corporate bankruptcy, Food industry},
abstract = {This study focuses on fsQCA in corporate bankruptcy research. This research aims at revealing how an fsQCA approach can overcome the knowledge gap of current conceptual and methodological attempts to expose corporate bankruptcy's architecture of causalities. The article discusses the economic literature concerning using fsQCA in corporate bankruptcy studies through complexity theory and a critical perspective. The study concentrates on implementing fsQCA and asymmetric thinking to corporate bankruptcy cases in food industry. The research examines the main reasons for corporate bankruptcy, namely: lack of financial liquidity, too high level of liabilities, losses, weak management, and too late recovery actions. The study attempts to build theory from food industry cases.}
}
@article{ZHOU202363,
title = {A privacy-preserving logistic regression-based diagnosis scheme for digital healthcare},
journal = {Future Generation Computer Systems},
volume = {144},
pages = {63-73},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.02.022},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23000638},
author = {Yousheng Zhou and Liyuan Song and Yuanni Liu and Pandi Vijayakumar and Brij B. Gupta and Wadee Alhalabi and Hind Alsharif},
keywords = {Digital healthcare, Online diagnosis, Privacy protection, Homomorphic authenticated encryption},
abstract = {In recent years, with the popularity of smart wearable devices, online diagnosis is becoming a promising medical technology and therefore promotes the progress of digital healthcare. Online diagnostic services relieve computing and storage requirements of wearable devices with the help of the cloud, while facilitating remote collaboration and data sharing, providing instant access to major diagnostics that patients can obtain the diagnosis within seconds, thereby saving a lot of time and economic costs. However, the frequent occurrence of security incidents based on wireless transmission of wearable devices further exacerbates the security risks of patient health data, and therefore the security of online diagnosis based on wearable devices should be taken seriously. This paper proposes privacy-preserving logistic regression based online disease diagnosis (LR-DDH), where the privacy of the medical data can be preserved with the use of homomorphic authenticated encryption. Theoretical analysis and experimental results demonstrate that the scheme LR-DDH proposed in this paper achieves efficient computation and communication under the premise of security.}
}
@article{CHERNYSHOV2004535,
title = {A System Identification Approach to Assessing Airline Pilot Skills},
journal = {IFAC Proceedings Volumes},
volume = {37},
number = {6},
pages = {535-540},
year = {2004},
note = {16th IFAC Symposium on Automatic Control in Aerospace 2004, Saint-Petersburg, Russia, 14-18 June 2004},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)32230-9},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017322309},
author = {Kirill Chernyshov},
keywords = {Aircraft control, Skill, Human factors, Performance monitoring, Identification, Stochastic systems, Coupling coefficients, Cross correlation functions, Estimation algorithms, Sampled data},
abstract = {The paper presents a new approach to eliciting information on current professional airline pilot skills and pilotage experience as a decision making person (DMP). Such an approach is regarded to the heuristic regularities of the DMP thinking process. In turn, the regularities are revealed on basis of recording the motions of the pilot eyes over the information field of the flight deck and processing the experimental data obtained. For the data mining, a probability theoretical approach is involved. Such an approach is based on applying the notion of consistency of measures of stochastic dependence of random variables; and a method of deriving almost sure converging estimate of such a measure by sample data is proposed.}
}
@article{DBASTIANI2023120220,
title = {CFD simulation of anaerobic granular sludge reactors: A review},
journal = {Water Research},
volume = {242},
pages = {120220},
year = {2023},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2023.120220},
url = {https://www.sciencedirect.com/science/article/pii/S0043135423006565},
author = {Camila {D' Bastiani} and David Kennedy and Anthony Reynolds},
keywords = {CFD, Anaerobic digestion, Multiphase flow, Granular reactors, Biogas},
abstract = {Anaerobic digestion processes can generate renewable energy in the form of biogas while treating organic wastewater. The generation of biogas within anaerobic digestion systems is directly linked to the mixing conditions inside the reactors. In high-rate reactors such as the upflow anaerobic sludge blanket (UASB) reactor, the expanded granular sludge bed (EGSB) reactor and the internal circulation (IC) reactor, the hydrodynamic behaviour will depend on the interactions between the wastewater, the biogas, and the biomass granules. Over the past few years, various researchers have used computational fluid dynamics (CFD) to study the hydrodynamic behaviour in these types of reactors. This review aims to present and critically discuss the state of the art in the use of CFD applied to anaerobic granular sludge reactors (AGSRs). It briefly introduces and discusses the various aspects of modelling. It also reviews the various papers which used CFD to model these reactors and critically analyses the models used for the simulations in terms of general approaches and single-phase vs multiphase studies. The methods used in the validation of the CFD models are also described and discussed. Based on the findings, the challenges and future perspectives for the CFD modelling of AGSRs are discussed and gaps in the knowledge are identified.}
}
@article{DORRIE2023105075,
title = {Automated force-flow-oriented reinforcement integration for Shotcrete 3D Printing},
journal = {Automation in Construction},
volume = {155},
pages = {105075},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105075},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523003357},
author = {Robin Dörrie and Niklas Freund and Eric Herrmann and Abtin Baghdadi and Inka Mai and Felipe Galli and Martin David and Klaus Dröder and Dirk Lowke and Harald Kloft},
keywords = {Structural design, Digital fabrication, Additive manufacturing in construction, AMC, Force-flow-oriented reinforcement, Automation in construction, 3D concrete printing, Shotcrete 3D Printing},
abstract = {The construction industry faces various challenges, e.g. reducing its carbon footprint and the extensive use of materials. Therefore, Computational Design and Additive Manufacturing gain more importance throughout the industry. In combination, they offer the possibility of manufacturing individually designed building components, which can be less material-consuming and structurally improved. The presented research displays and discusses the effect of force-flow-oriented reinforcement design in concrete beams concerning the flexural strength and the required amount of steel. For this purpose, different reinforcement layouts were designed and integrated into conventionally cast and additively manufactured beam components. For the design of the force-flow-oriented reinforcement layouts, a digital workflow is established, and FEM simulations are utilised. The load-bearing capacity of the beams is compared based on four-point bending tests. Due to the optimised reinforcement layout, an increase of flexural strength of more than 60% was achieved while keeping the reinforcement amount constant. It is also shown that using force-flow-oriented reinforcement layouts can save nearly 60% of the reinforcement needed to achieve the same flexural strength as a beam with a conventional reinforcement cage. Finally, the potential for automated force-flow-oriented reinforcement integration within additively manufactured components is discussed.}
}
@article{KRISHNAN2024153,
title = {Integrating artificial intelligence: A step forward in orthodontic education},
journal = {Journal of the World Federation of Orthodontists},
volume = {13},
number = {4},
pages = {153-154},
year = {2024},
issn = {2212-4438},
doi = {https://doi.org/10.1016/j.ejwf.2024.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S221244382400050X},
author = {Vinod Krishnan}
}
@incollection{PROFILLIDIS2019383,
title = {Chapter 9 - Fuzzy Methods},
editor = {V.A. Profillidis and G.N. Botzoris},
booktitle = {Modeling of Transport Demand},
publisher = {Elsevier},
pages = {383-417},
year = {2019},
isbn = {978-0-12-811513-8},
doi = {https://doi.org/10.1016/B978-0-12-811513-8.00009-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128115138000091},
author = {V.A. Profillidis and G.N. Botzoris},
keywords = {4-step model, Accidents, Airports, Crisp, Fuzzification, Fuzzy logic, Fuzzy model, Fuzzy regression, Gaussian, Linear programming, Membership degree, Membership function, Objective function, Railways, Traffic, Transport economics, Trapezoidal, Triangular},
abstract = {This chapter deals with applications of fuzzy methods, which give the ability to study quantitatively problems characterized by ambiguity, imprecision, uncertainty, linguistic variables, and missing or few or no data. The fuzzy method introduces another way of thinking: a statement, instead of being true or false, may be partially true or false. Thus, instead of taking into account the typically used fixed numerical values (such as, e.g., 2.34), the fuzzy method employs a set of plausible values (e.g., around the value 2.34) within a specific domain. Although this approach may look similar to the error of statistical methods, the fuzzy method can tackle situations (such as missing or vague data), for which classic methods are inefficient. The principles of fuzzy numbers, fuzzy sets, and fuzzy logic are presented. The case of symmetric triangular fuzzy numbers is analyzed in detail. Next, linear regression analysis with the use of fuzzy numbers is explained. A detailed application of fuzzy linear regression for a transport demand problem is surveyed analytically. The chapter includes many applications of fuzzy linear regression for the forecast of a variety of transport demand problems: air transport, rail transport, road transport, transport at urban level, and transport economics. Applications of the fuzzy method to other transport problems are explained: route choice, road safety, accident analysis, logistics and routing of freight vehicles, and the optimization of capacity of airports.}
}
@article{MASSEY2021178,
title = {Sex differences in health and disease: A review of biological sex differences relevant to cancer with a spotlight on glioma},
journal = {Cancer Letters},
volume = {498},
pages = {178-187},
year = {2021},
issn = {0304-3835},
doi = {https://doi.org/10.1016/j.canlet.2020.07.030},
url = {https://www.sciencedirect.com/science/article/pii/S0304383520303876},
author = {Susan Christine Massey and Paula Whitmire and Tatum E. Doyle and Joseph E. Ippolito and Maciej M. Mrugala and Leland S. Hu and Peter Canoll and Alexander R.A. Anderson and Melissa A. Wilson and Susan M. Fitzpatrick and Margaret M. McCarthy and Joshua B. Rubin and Kristin R. Swanson},
keywords = {Sex differences, Sex factors, Precision medicine, Glioma, Patient-specific computational modeling},
abstract = {The influence of biological sex differences on human health and disease, while being increasingly recognized, has long been underappreciated and underexplored. While humans of all sexes are more alike than different, there is evidence for sex differences in the most basic aspects of human biology and these differences have consequences for the etiology and pathophysiology of many diseases. In a disease like cancer, these consequences manifest in the sex biases in incidence and outcome of many cancer types. The ability to deliver precise, targeted therapies to complex cancer cases is limited by our current understanding of the underlying sex differences. Gaining a better understanding of the implications and interplay of sex differences in diseases like cancer will thus be informative for clinical practice and biological research. Here we review the evidence for a broad array of biological sex differences in humans and discuss how these differences may relate to observed sex differences in various diseases, including many cancers and specifically glioblastoma. We focus on areas of human biology that play vital roles in healthy and disease states, including metabolism, development, hormones, and the immune system, and emphasize that the intersection of sex differences in these areas should not go overlooked. We further propose that mathematical approaches can be useful for exploring the extent to which sex differences affect disease outcomes and accounting for those in the development of therapeutic strategies.}
}
@article{ARTEMOV20093884,
title = {A tribute to D.B. Spalding and his contributions in science and engineering},
journal = {International Journal of Heat and Mass Transfer},
volume = {52},
number = {17},
pages = {3884-3905},
year = {2009},
note = {Special Issue Honoring Professor D. Brian Spalding},
issn = {0017-9310},
doi = {https://doi.org/10.1016/j.ijheatmasstransfer.2009.03.038},
url = {https://www.sciencedirect.com/science/article/pii/S0017931009002026},
author = {V. Artemov and S.B. Beale and G. {de Vahl Davis} and M.P. Escudier and N. Fueyo and B.E. Launder and E. Leonardi and M.R. Malin and W.J. Minkowycz and S.V. Patankar and A. Pollard and W. Rodi and A. Runchal and S.P. Vanka},
keywords = {D.B. Spalding, Fluid dynamics, Heat transfer, Mass transfer, Combustion},
abstract = {This paper presents a summary of some of the scientific and engineering contributions of Prof. D.B. Spalding up to the present time. Starting from early work on combustion, and his unique work in mass transfer theory, Spalding’s unpublished “unified theory” is described briefly. Subsequent to this, developments in algorithms by the Imperial College group led to the birth of modern computational fluid dynamics, including the well-known SIMPLE algorithm. Developments in combustion, multi-phase flow and turbulence modelling are also described. Finally, a number of academic and industrial applications of computational fluid dynamics and heat transfer applications considered in subsequent years are mentioned.}
}
@article{OMEARA2024,
title = {Going fishing: how to get what you want from a fungal genetic screen},
journal = {mSphere},
volume = {9},
number = {7},
year = {2024},
issn = {2379-5042},
doi = {https://doi.org/10.1128/msphere.00638-23},
url = {https://www.sciencedirect.com/science/article/pii/S237950422400136X},
author = {Teresa R. O'Meara},
abstract = {ABSTRACT

Five years ago, as I was starting my lab, I wrote about two functional genomic screens in fungi that had inspired me (mSphere 4:e00299-19, https://doi.org/10.1128/mSphere.00299-19). Now, I want to discuss some of the principles and questions that I ask myself and my students as we embark on our own screens. A good screen, whether it is a genetic or chemical screen, can be the starting point for new discovery and an excellent basis for the beginning of a scientific research project. However, screens are often criticized for being “fishing expeditions.” To stretch this metaphor to the extreme, this is because people are worried that we do not know how to fish, that we will come home without any fish, bring home the wrong fish, or not know what to do with a fish if we caught it. How you set up the screen and analyze the results determines whether the screen will be useful. In this mini-review, and in the spirit of teaching a scientist to fish, I will discuss recent excellent fungal genetic and chemical screens that illustrate some of the key aspects of a successful screen.}
}
@article{HUNTER198763,
title = {What is fundamental in an information age? A focus on curriculum},
journal = {Education and Computing},
volume = {3},
number = {1},
pages = {63-73},
year = {1987},
note = {Special Issue on Educational Computer Policy Alternatives in the United States},
issn = {0167-9287},
doi = {https://doi.org/10.1016/S0167-9287(87)80513-7},
url = {https://www.sciencedirect.com/science/article/pii/S0167928787805137},
author = {Beverly Hunter},
keywords = {Curriculum change, Knowledge-creative Learning, Problem Solving Tools, Information Handling, Algorithmic Thinking, Critical Thinking Skills, Higher-order Thinking Skills, Information Age, Computer Literacy, Problem Solving, Decision Making, Inquiry, Reasoning, Valuing},
abstract = {Systematic reassessment of both overt and covert curriculum content and methods is needed in response to broader social change involved in the information revolution. The educational system in the United States is moving (unevenly) through three overlapping stages of curriculum change: (1) focus on technology, (2) integration of technology into curriculum, and (3) focus on fundamental change in curriculum. Indicators of the current state of change in elementary and secondary schools include state education agency mandates, teacher-oriented publications, past and present surveys of computer use in schools, teacher attitudes, private industry initiatives, and recommendations of national study groups and commissions.}
}
@article{ZHANG2022101922,
title = {Multiple-symbol noncoherent learning detection of coded QAM signals in IEEE 802.15.3 Wireless Multi-media Networks},
journal = {Physical Communication},
volume = {55},
pages = {101922},
year = {2022},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2022.101922},
url = {https://www.sciencedirect.com/science/article/pii/S1874490722001999},
author = {Gaoyuan Zhang and Congfang Ma and Kai Chen and Yongen Li and Haiqiong Li and Congzheng Han},
keywords = {Wireless Multi-media Networks, Noncoherent detection, Deep learning, Uniform quantization},
abstract = {We consider the noncoherent deep learning problem for coded signal detection under the phase noncoherent channels for remote home healthcare applications with high data rate. In particular, a multiple-symbol noncoherent learning detection (MNLD) scheme based on neural networks is proposed for low-density parity-check (LDPC) coded noncoherent quadrature amplitude modulation (QAM) signals in IEEE 802.15.3 Wireless Multi-media Networks. Our derivation shows that extensive operations for the first kind zero-order modified Bessel function is unavoidable for the implementation of the optimal bit log-likelihood ratio (LLR) for decoding in traditional multiple-symbol detection (MSD) scheme. The perfect estimation of the channel state information (CSI), i.e., a priori information about the variance of the additive white Gaussian noise (AWGN), is also required for the receiver. This is clearly not computationally practical for Wireless Multi-media Networks. Consequently, we developed an improved approach based on feed-forward neural networks to accurately calculate the bit LLR. Furthermore, to decrease the generation size of training set and thus increase the training speed of the proposed neural networks, we uniformly quantize the continuous carrier phase offset (CPO), which is random and unknown, into discrete status. Our simulation results verify the learning efficiency of this simplified training-set generation configuration. The decoding convergence is successfully accelerated and much performance gain is finally achieved when compared with traditional decoding using the perfect bit LLR. This is clearly critical for high reliable transmission of home healthcare information.}
}
@article{SAFKHANI2021100311,
title = {RSEAP2: An enhanced version of RSEAP, an RFID based authentication protocol for vehicular cloud computing},
journal = {Vehicular Communications},
volume = {28},
pages = {100311},
year = {2021},
issn = {2214-2096},
doi = {https://doi.org/10.1016/j.vehcom.2020.100311},
url = {https://www.sciencedirect.com/science/article/pii/S2214209620300826},
author = {Masoumeh Safkhani and Carmen Camara and Pedro Peris-Lopez and Nasour Bagheri},
keywords = {Vehicular cloud computing, Authentication, Elliptic curve based cryptography, Security analysis, Tag/reader impersonation, Distance bounding attacks},
abstract = {RSEAP is a recently proposed RFID based authentication protocol for vehicular cloud computing whose authors claimed to be secure and efficient. In this article, we challenge these claims. More precisely, we show that RSEAP does not provide the desired security, and it is possible to conduct both tag and reader impersonation attacks efficiently. Besides, despite the use of timestamps, we show how this protocol also suffers from a range of relay attacks. The complexity of any of the proposed attacks is negligible while the success probability is maximum (i.e., the adversary's success probability is ‘1’ since all the proposed attacks are deterministic). To improve the security of RSEAP scheme, we suggest the required patches for fixing the security vulnerabilities mentioned above. We show that the improved protocol, called RSEAP2, is more efficient (computation and communication costs) than the original RSEAP, while provides a higher security level. The security of RSEAP2 is evaluated informally and also formally using the Scyther tool, which is a well-known and automated tool to assess the security of cryptographic protocols. Additionally, we have formally verified the security of the proposed scheme under the Real-or-Random oracle model.}
}
@article{QUINTON2024106526,
title = {Embodied sequential sampling models and dynamic neural fields for decision-making: Why hesitate between two when a continuum is the answer},
journal = {Neural Networks},
volume = {179},
pages = {106526},
year = {2024},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106526},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024004507},
author = {Jean-Charles Quinton and Flora Gautheron and Annique Smeding},
keywords = {Decision-making, Sequential sampling model, Leaky competing accumulator, Dynamic neural field, Embodied decision, Mouse-tracking},
abstract = {As two alternative options in a forced choice task are separated by design, two classes of computational models of decision-making have thrived independently in the literature for nearly five decades. While sequential sampling models (SSM) focus on response times and keypresses in binary decisions in experimental paradigms, dynamic neural fields (DNF) focus on continuous sensorimotor dimensions and tasks found in perception and robotics. Recent attempts have been made to address limitations in their application to other domains, but strong similarities and compatibility between prominent models from both classes were hardly considered. This article is an attempt at bridging the gap between these classes of models, and simultaneously between disciplines and paradigms relying on binary or continuous responses. A unifying formulation of representative SSM and DNF equations is proposed, varying the number of units which interact and compete to reach a decision. The embodiment of decisions is also considered by coupling cognitive and sensorimotor processes, enabling the model to generate decision trajectories at trial level. The resulting mechanistic model is therefore able to target different paradigms (forced choices or continuous response scales) and measures (final responses or dynamics). The validity of the model is assessed statistically by fitting empirical distributions obtained from human participants in moral decision-making mouse-tracking tasks, for which both dichotomous and nuanced responses are meaningful. Comparing equations at the theoretical level, and model parametrizations at the empirical level, the implications for psychological decision-making processes, as well as the fundamental assumptions and limitations of models and paradigms are discussed.}
}
@article{LENNON2022104608,
title = {Young children's social and independent behavior during play with a coding app: Digital game features matter in a 1:1 child to tablet setting},
journal = {Computers & Education},
volume = {190},
pages = {104608},
year = {2022},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2022.104608},
url = {https://www.sciencedirect.com/science/article/pii/S0360131522001798},
author = {Maya Lennon and Sarah Pila and Rachel Flynn and Ellen A. Wartella},
keywords = {Applications in coding, Cooperative/collaborative learning, Games, early years education},
abstract = {The overarching aim of this study was to explore young children's (N = 25, Mage = 5.16 years) play with two coding games (Daisy the Dinosaur and Kodable) in a 1:1 child to tablet setting. We had three research questions focused on children's game play: 1) How does the structure of each game influence children's play? 2) Do children play more or less independently depending on the game they play? 3) Do children who play the games more independently learn more coding skills? Three researchers coded more than 6 h of video data showing children's play with digital coding games. Findings include, that the type of game did influence the different ways that children behaved while playing. However, during both games, children had the same amount of independent play. Children who played more independently during Daisy the Dinosaur learned more coding skills. This may be because these children were focusing more on the game than their peers as we did not find a similar effect for the game Kodable. We discuss the ways that children play structured vs. open structured (i.e., sandbox) digital games with a particular focus on how game play may influence learning. As opportunities for individual device ownership in classrooms increase, future work should continue to explore how game features influence learning.}
}
@article{LI2018122,
title = {Uncertainty, politics, and technology: Expert perceptions on energy transitions in the United Kingdom},
journal = {Energy Research & Social Science},
volume = {37},
pages = {122-132},
year = {2018},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2017.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S2214629617303304},
author = {Francis G.N. Li and Steve Pye},
keywords = {Climate policy, Energy policy, Uncertainty analysis, Decision-making},
abstract = {Energy policy is beset by deep uncertainties, owing to the scale of future transitions, the long-term timescales for action, and numerous stakeholders. This paper provides insights from semi-structured interviews with 31 UK experts from government, industry, academia, and civil society. Participants were asked for their views on the major uncertainties surrounding the ability of the UK to meet its 2050 climate targets. The research reveals a range of views on the most critical uncertainties, how they can be mitigated, and how the research community can develop approaches to better support strategic decision-making. The study finds that the socio-political dimensions of uncertainty are discussed by experts almost as frequently as technological ones, but that there exist divergent perspectives on the role of government in the transition and whether or not there is a requirement for increased societal engagement. Finally, the study finds that decision-makers require a new approach to uncertainty assessment that overcomes analytical limits to existing practice, is more flexible and adaptable, and which better integrates qualitative narratives with quantitative analysis. Policy design must escape from ‘caged’ thinking concerning what can or cannot be included in models, and therefore what types of uncertainties can or cannot be explored.}
}
@article{MARGINEANU20161,
title = {Neuropharmacology beyond reductionism – A likely prospect},
journal = {Biosystems},
volume = {141},
pages = {1-9},
year = {2016},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2015.11.010},
url = {https://www.sciencedirect.com/science/article/pii/S0303264715002026},
author = {Doru Georg Margineanu},
keywords = {Neuropharmacology, Systems pharmacology, Reductionism, Multi-target drug, Phenotypic screening, Emergent properties, Serendipity},
abstract = {Neuropharmacology had several major past successes, but the last few decades did not witness any leap forward in the drug treatment of brain disorders. Moreover, current drugs used in neurology and psychiatry alleviate the symptoms, while hardly curing any cause of disease, basically because the etiology of most neuro-psychic syndromes is but poorly known. This review argues that this largely derives from the unbalanced prevalence in neuroscience of the analytic reductionist approach, focused on the cellular and molecular level, while the understanding of integrated brain activities remains flimsier. The decline of drug discovery output in the last decades, quite obvious in neuropharmacology, coincided with the advent of the single target-focused search of potent ligands selective for a well-defined protein, deemed critical in a given pathology. However, all the widespread neuro-psychic troubles are multi-mechanistic and polygenic, their complex etiology making unsuited the single-target drug discovery. An evolving approach, based on systems biology considers that a disease expresses a disturbance of the network of interactions underlying organismic functions, rather than alteration of single molecular components. Accordingly, systems pharmacology seeks to restore a disturbed network via multi-targeted drugs. This review notices that neuropharmacology in fact relies on drugs which are multi-target, this feature having occurred just because those drugs were selected by phenotypic screening in vivo, or emerged from serendipitous clinical observations. The novel systems pharmacology aims, however, to devise ab initio multi-target drugs that will appropriately act on multiple molecular entities. Though this is a task much more complex than the single-target strategy, major informatics resources and computational tools for the systemic approach of drug discovery are already set forth and their rapid progress forecasts promising outcomes for neuropharmacology.}
}
@article{FAVERO2023112755,
title = {Analysis of subjective thermal comfort data: A statistical point of view},
journal = {Energy and Buildings},
volume = {281},
pages = {112755},
year = {2023},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2022.112755},
url = {https://www.sciencedirect.com/science/article/pii/S0378778822009264},
author = {Matteo Favero and Antonio Luparelli and Salvatore Carlucci},
keywords = {Subjective thermal comfort data, Rating scales, Level of measurement, Ordinal regression, Bayesian analysis, Statistical thinking},
abstract = {Thermal comfort research aims to determine the relationship between the thermal environment and the human sense of warmth. This is usually achieved by measuring the subjective human thermal response to different thermal environments. However, it is common practice to use simple linear regression to analyse data collected using ordinal scales. This practice may lead to severe errors in inference. This study first set the methodological foundations to analyse subjective thermal comfort data from a statistical perspective. Subsequently, we show the practical consequences of fallacious assumptions by utilising a Bayesian approach and show, through an illustrative example, that a linear regression model applied to ordinal data suggests results different from those obtained using ordinal regression. Specifically, linear regression found no difference in means and effect size between genders, while the ordinal regression model led to the opposite conclusion. In addition, the linear regression model distorts the estimated regression coefficient for air temperature compared to the ordinal model. Finally, the ordinal model shows that the distance between adjacent response categories of the ASHRAE 7-point thermal sensation scale is not equidistant. Given the abovementioned issues, we advocate utilising ordinal models instead of metric models to analyse ordinal data.}
}
@incollection{MOTTAMONTESERRAT202189,
title = {Chapter 5 - Computer language and linguistics},
editor = {Dioneia {Motta Monte-Serrat} and Carlo Cattani},
booktitle = {The Natural Language for Artificial Intelligence},
publisher = {Academic Press},
pages = {89-120},
year = {2021},
series = {Cognitive Data Science in Sustainable Computing},
isbn = {978-0-12-824118-9},
doi = {https://doi.org/10.1016/B978-0-12-824118-9.00005-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128241189000059},
author = {Dioneia {Motta Monte-Serrat} and Carlo Cattani},
keywords = {Computer language, Linguistics, Symbolic language, Generative model, Algorithmic core, Axiomatic-logical structure},
abstract = {Language is the ability to use complex communication systems and a set of signals used to encode and decode information. Mathematics and linguistics deal with the representation of thought through symbolic language under a guiding structure of language functioning. Computer language is formal, symbolic, and depends on linguistic principles of natural language. Computing regulates the behavior of the machine in the execution of specific tasks and, for that, generative models were developed. The underlying structure of the latter would be adequate to incorporate complex and high-dimensional data in a latent space with a supposed ability to replicate the original data. The distribution in probabilistic terms, however, does not reflect reality. We suggest the logical-axiomatic principle of natural language as an algorithmic core of computational methods capable of generalizing the execution of a set of factors.}
}
@article{IZMALKOV2011121,
title = {Perfect implementation},
journal = {Games and Economic Behavior},
volume = {71},
number = {1},
pages = {121-140},
year = {2011},
note = {Special Issue In Honor of John Nash},
issn = {0899-8256},
doi = {https://doi.org/10.1016/j.geb.2010.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0899825610000758},
author = {Sergei Izmalkov and Matt Lepinski and Silvio Micali},
keywords = {Mechanism design, Trust, Privacy},
abstract = {Privacy and trust affect our strategic thinking, yet have not been precisely modeled in mechanism design. In settings of incomplete information, traditional implementations of a normal-form mechanism—by disregarding the players' privacy, or assuming trust in a mediator—may fail to reach the mechanism's objectives. We thus investigate implementations of a new type. We put forward the notion of a perfect implementation of a normal-form mechanism M: in essence, a concrete extensive-form mechanism exactly preserving all strategic properties of M, without relying on trusted mediators or violating the players' privacy. We prove that any normal-form mechanism can be perfectly implemented by a verifiable mediator using envelopes and an envelope-randomizing device. Differently from a trusted mediator, a verifiable one only performs prescribed public actions, so that everyone can verify that he is acting properly, and that he never learns any information that should remain private.}
}
@article{OKAMURA2021,
title = {NMB4.0: development of integrated nuclear fuel cycle simulator from the front to back-end},
journal = {EPJ - Nuclear Sciences & Technologies},
volume = {7},
year = {2021},
issn = {2491-9292},
doi = {https://doi.org/10.1051/epjn/2021019},
url = {https://www.sciencedirect.com/science/article/pii/S2491929221000145},
author = {Tomohiro Okamura and Ryota Katano and Akito Oizumi and Kenji Nishihara and Masahiko Nakase and Hidekazu Asano and Kenji Takeshita},
abstract = {Nuclear Material Balance code version 4.0 (NMB4.0) has been developed through collaborative R&D between TokyoTech&JAEA. Conventional nuclear fuel cycle simulation codes mainly analyze actinides and are specialized for front-end mass balance analysis. However, quantitative back-end simulation has recently become necessary for considering R&D strategies and sustainable nuclear energy utilization. Therefore, NMB4.0 was developed to realize the integrated nuclear fuel cycle simulation from front- to back-end. There are three technical features in NMB4.0: 179 nuclides are tracked, more than any other code, throughout the nuclear fuel cycle; the Okamura explicit method is implemented, which contributes to reducing the numerical cost while maintaining the accuracy of depletion calculations on nuclides with a shorter half-life; and flexibility of back-end simulation is achieved. The main objective of this paper is to show the newly developed functions, made for integrated back-end simulation, and verify NMB4.0 through a benchmark study to show the computational performance.}
}
@incollection{FOTOPOULOS2022241,
title = {Chapter 8 - The edge-cloud continuum in wearable sensing for respiratory analysis},
editor = {Rui Pedro Paiva and Paulo de Carvalho and Vassilis Kilintzis},
booktitle = {Wearable Sensing and Intelligent Data Analysis for Respiratory Management},
publisher = {Academic Press},
pages = {241-271},
year = {2022},
isbn = {978-0-12-823447-1},
doi = {https://doi.org/10.1016/B978-0-12-823447-1.00002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234471000026},
author = {Anaxagoras Fotopoulos and Pantelis Z. Lappas and Alexis Melitsiotis},
keywords = {Artificial intelligence, Edge computing, Internet of Medical Things, Multisource fusion, P4 health care},
abstract = {Edge computing is seen as a set of remotely available computer system resources that drive the computing power at the source of data to improve energy efficiency and security, as well as decrease latency. Although the computation capability of biomedical wearables has increased extremely during the past decade, it is still challenging to perform sophisticated artificial intelligence (AI) algorithms in a resource-constrained environment for energy-efficiency and (near) real-time processing, along the edge-cloud continuum. The aim of this chapter is twofold. The first is to outline the role of edge computing on the Internet of Medical Things, in which wearable technologies are used as the sensory equipment for respiratory analysis, at the transition of patient monitoring from hospital to home. The second is to discuss the potential of explainable AI in the P4 health-care context for respiratory analysis, by highlighting computational intelligence and multisource fusion approaches to achieve continuous monitoring of respiratory analysis.}
}
@article{WANDELL2017298,
title = {Diagnosing the Neural Circuitry of Reading},
journal = {Neuron},
volume = {96},
number = {2},
pages = {298-311},
year = {2017},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2017.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0896627317306980},
author = {Brian A. Wandell and Rosemary K. Le},
keywords = {reading, diffusion imaging, development, fMRI, computational modeling},
abstract = {We summarize the current state of knowledge of the brain’s reading circuits, and then we describe opportunities to use quantitative and reproducible methods for diagnosing these circuits. Neural circuit diagnostics—by which we mean identifying the locations and responses in an individual that differ significantly from measurements in good readers—can help parents and educators select the best remediation strategy. A sustained effort to develop and share diagnostic methods can support the societal goal of improving literacy.}
}
@article{BOULGAKOV2020154,
title = {Bringing Microscopy-By-Sequencing into View},
journal = {Trends in Biotechnology},
volume = {38},
number = {2},
pages = {154-162},
year = {2020},
issn = {0167-7799},
doi = {https://doi.org/10.1016/j.tibtech.2019.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167779919301349},
author = {Alexander A. Boulgakov and Andrew D. Ellington and Edward M. Marcotte},
keywords = {DNA microscopy, next-generation sequencing, barcoding, localization, oligonucleotides},
abstract = {The spatial distribution of molecules and cells is fundamental to understanding biological systems. Traditionally, microscopies based on electromagnetic waves such as visible light have been used to localize cellular components by direct visualization. However, these techniques suffer from limitations of transmissibility and throughput. Complementary to optical approaches, biochemical techniques such as crosslinking can colocalize molecules without suffering the same limitations. However, biochemical approaches are often unable to combine individual colocalizations into a map across entire cells or tissues. Microscopy-by-sequencing techniques aim to biochemically colocalize DNA-barcoded molecules and, by tracking their thus unique identities, reconcile all colocalizations into a global spatial map. Here, we review this new field and discuss its enormous potential to answer a broad spectrum of questions.}
}
@article{AGUIRRE2011305,
title = {Geovisual evaluation of public participation in decision making: The grapevine},
journal = {Journal of Visual Languages & Computing},
volume = {22},
number = {4},
pages = {305-321},
year = {2011},
note = {Part Special Issue on Challenging Problems in Geovisual Analytics},
issn = {1045-926X},
doi = {https://doi.org/10.1016/j.jvlc.2010.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X10000790},
author = {Robert Aguirre and Timothy Nyerges},
keywords = {Grapevine, Geovisual analytics, Public participation, Decision making, Spatio-temporal events, Human–computer–human interaction},
abstract = {This article reports on a three-dimensional (time–space) geovisual analytic called a “grapevine.” People often use metaphors to describe the temporal and spatial structure of online discussions, e.g., “threads” growing as a result of message exchanges. We created a visualization to evaluate the temporal and spatial structure of online message exchanges based on the shape of a grapevine naturally cultivated in a vineyard. Our grapevine visualization extends up through time with features like buds, nodes, tendrils, and leaves produced as a result of message posting, replying, and voting. Using a rotatable and fully interactive three-dimensional GIS (Geographic Information System) environment, a geovisual analyst can evaluate the quality of deliberation in the grapevine visualization by looking for productive patterns in fine-grained human–computer–human interaction (HCHI) data and then sub-sampling the productive parts for content analysis. We present an example of how we used the technique in a study of participatory interactions during an online field experiment about improving transportation in the central Puget Sound region of Washington called the Let's Improve Transportation (LIT) Challenge. We conclude with insights about how our grapevine could be applied as a general purpose technique for evaluation of any participatory learning, thinking, or decision making situation.}
}
@article{RIGGI2024122,
title = {3D animation as a tool for integrative modeling of dynamic molecular mechanisms},
journal = {Structure},
volume = {32},
number = {2},
pages = {122-130},
year = {2024},
issn = {0969-2126},
doi = {https://doi.org/10.1016/j.str.2023.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0969212623004483},
author = {Margot Riggi and Rachel M. Torrez and Janet H. Iwasa},
abstract = {Summary
As the scientific community accumulates diverse data describing how molecular mechanisms occur, creating and sharing visual models that integrate the richness of this information has become increasingly important to help us explore, refine, and communicate our hypotheses. Three-dimensional (3D) animation is a powerful tool to capture dynamic hypotheses that are otherwise difficult or impossible to visualize using traditional 2D illustration techniques. This perspective discusses the current and future roles that 3D animation can play in the research sphere.}
}
@article{COOKE2020138,
title = {Diverse perspectives on interdisciplinarity from Members of the College of the Royal Society of Canada},
journal = {FACETS},
volume = {5},
number = {1},
pages = {138-165},
year = {2020},
issn = {2371-1671},
doi = {https://doi.org/10.1139/facets-2019-0044},
url = {https://www.sciencedirect.com/science/article/pii/S2371167120000551},
author = {Steven J. Cooke and Vivian M. Nguyen and Dimitry Anastakis and Shannon D. Scott and Merritt R. Turetsky and Alidad Amirfazli and Alison Hearn and Cynthia E. Milton and Laura Loewen and Eric E. Smith and D. Ryan Norris and Kim L. Lavoie and Alice Aiken and Daniel Ansari and Alissa N. Antle and Molly Babel and Jane Bailey and Daniel M. Bernstein and Rachel Birnbaum and Carrie Bourassa and Antonio Calcagno and Aurélie Campana and Bing Chen and Karen Collins and Catherine E. Connelly and Myriam Denov and Benoît Dupont and Eric George and Irene Gregory-Eaves and Steven High and Josephine M. Hill and Philip L. Jackson and Nathalie Jette and Mark Jurdjevic and Anita Kothari and Paul Khairy and Sylvie A. Lamoureux and Kiera Ladner and Christian R. Landry and François Légaré and Nadia Lehoux and Christian Leuprecht and Angela R. Lieverse and Artur Luczak and Mark L. Mallory and Erin Manning and Ali Mazalek and Stuart J. Murray and Lenore L. Newman and Valerie Oosterveld and Patrice Potvin and Sheryl Reimer-Kirkham and Jennifer Rowsell and Dawn Stacey and Susan L. Tighe and David J. Vocadlo and Anne E. Wilson and Andrew Woolford and Jules M. Blais},
keywords = {interdisciplinarity, academic institutions, universities, funding, scholarly activity, boundary crossing, barriers},
abstract = {Various multiple-disciplinary terms and concepts (although most commonly “interdisciplinarity,” which is used herein) are used to frame education, scholarship, research, and interactions within and outside academia. In principle, the premise of interdisciplinarity may appear to have many strengths; yet, the extent to which interdisciplinarity is embraced by the current generation of academics, the benefits and risks for doing so, and the barriers and facilitators to achieving interdisciplinarity, represent inherent challenges. Much has been written on the topic of interdisciplinarity, but to our knowledge there have been few attempts to consider and present diverse perspectives from scholars, artists, and scientists in a cohesive manner. As a team of 57 members from the Canadian College of New Scholars, Artists, and Scientists of the Royal Society of Canada (the College) who self-identify as being engaged or interested in interdisciplinarity, we provide diverse intellectual, cultural, and social perspectives. The goal of this paper is to share our collective wisdom on this topic with the broader community and to stimulate discourse and debate on the merits and challenges associated with interdisciplinarity. Perhaps the clearest message emerging from this exercise is that working across established boundaries of scholarly communities is rewarding, necessary, and is more likely to result in impact. However, there are barriers that limit the ease with which this can occur (e.g., lack of institutional structures and funding to facilitate cross-disciplinary exploration). Occasionally, there can be significant risk associated with doing interdisciplinary work (e.g., lack of adequate measurement or recognition of work by disciplinary peers). Solving many of the world’s complex and pressing problems (e.g., climate change, sustainable agriculture, the burden of chronic disease, and aging populations) demands thinking and working across long-standing, but in some ways restrictive, academic boundaries. Academic institutions and key support structures, especially funding bodies, will play an important role in helping to realize what is readily apparent to all who contributed to this paper—that interdisciplinarity is essential for solving complex problems; it is the new norm. Failure to empower and encourage those doing this research will serve as a great impediment to training, knowledge, and addressing societal issues.}
}
@incollection{OLSON200116640,
title = {Writing Systems, Psychology of},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {16640-16643},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/01563-1},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767015631},
author = {D.R. Olson},
abstract = {The writing systems of the world differ importantly in how they relate to spoken language. Tokening and pictographic scripts relate to meanings or intentions directly. So-called full writing systems represent properties of the spoken language but in completely different ways. Morphophonemic (logographic) scripts represent the words or morphemes of the language, syllabic scripts represent the syllables of the language whether or not they also represent word boundaries. Alphabetic scripts represent, with varying degrees of success, the phonemes of the language, but also by means of spaces, the words of the language. Not only do these differences have an effect on learning to read, they also have an important effect on the ways in which one thinks about language and consequently about the world and the mind. Writing systems provide models for thinking about speech.}
}
@article{ZHOU2023110513,
title = {Random following ant colony optimization: Continuous and binary variants for global optimization and feature selection},
journal = {Applied Soft Computing},
volume = {144},
pages = {110513},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110513},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623005318},
author = {Xinsen Zhou and Wenyong Gui and Ali Asghar Heidari and Zhennao Cai and Guoxi Liang and Huiling Chen},
keywords = {Feature selection, Ant colony optimization, Global optimization, Machine learning, Classification},
abstract = {Continuous ant colony optimization was a population-based heuristic search algorithm inspired by the pathfinding behavior of ant colonies with a simple structure and few control parameters. However, in the case of multimodal and high-dimensional optimization problems, it was often limited to local regions in the feasible domain space, negatively affecting the computational effort required to find the optimal solution point. To alleviate its limitations in this regard, a random following strategy is proposed to enhance communication among the ant colony search agent and other ant colony members within the search dimension. The proposed algorithm that incorporates this strategy is called Random Following Ant Colony Optimization. Then, to evaluate the global optimization performance of the proposed algorithm, the well-known numerical optimization problem, namely the Congress on Evolutionary Computation 2017 test suite, is used. First, the proposed algorithm’s parameters are analyzed for sensitivity, scalability experiments, and balanced diversity. Second, it is compared experimentally with 11 state-of-the-art algorithms in dimensions 10, 30, 50, and 100, respectively, and Wilcoxon signed-rank test, Friedman test, and Bonferroni-Dunn post-hoc statistical test are used to synthesize the experimental comparison results. Finally, to evaluate the ability of the proposed algorithm to handle discrete feature selection problems, comparative experiments are conducted on 24 datasets with eight well-known classification methods and five high-performance classification methods. The benchmark test results show that the global optimization performance of the proposed algorithm is comparable to the winners of the test suite in 50 and 100 dimensions. The results of the feature selection experiments show that the proposed algorithm is much stronger than the well-known and high-performance classification methods on high-dimensional datasets.}
}
@article{LEITE2024103613,
title = {Land use and environmental impacts: Flood model in a medium-sized Brazilian city as a tool for urban sustainability},
journal = {Environmental Science & Policy},
volume = {151},
pages = {103613},
year = {2024},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2023.103613},
url = {https://www.sciencedirect.com/science/article/pii/S1462901123002629},
author = {Marcos Esdras Leite and Felipe Teixeira Dias and Jefferson William Lopes Almeida and Narciso Ferreira dos Santos-Neto},
keywords = {Urban Sustainability, Urban Geography, Urban Planning, Geotechnologies, Brazilian City},
abstract = {Thinking, articulating, and promoting research focused on themes and issues inherent to urban spaces has posed challenges to various areas of science, particularly encouraging multidisciplinary research. Among the various studies on urban phenomena, those that reflect structural problems, whether of natural origin or resulting from human actions in space, stand out. Examples include recurrent flooding in various cities. In this regard, with the aim of correlating urban problems with the perspectives inherent to Urban Sustainability, this research aims to propose a flood model as a tool for thinking and planning space utilization. The method used consisted of creating a Flood Model, adopting an applied research approach and employing a multidisciplinary theoretical-methodological framework. This research was conducted using a medium-sized city, Montes Claros, in the state of Minas Gerais, Brazil. Geotechnologies and Geographic Information System (GIS) were utilized in constructing the Flood Model. As conclusions, the results point towards new perspectives on the conception of Urban Policies and Planning, seeking to establish guidelines to achieve the sustainability of urban spaces and to prevent flooding disasters and risks based on the structured model in this study. Additionally, it underscored the need for oversight of installation works in subdivisions and public area paving to require the construction of an appropriate drainage system tailored to the local reality, as well as the regulation of subdivisions and occupations in floodplains of watercourses, primarily through technical, management, and policy approaches.}
}
@incollection{POULTON20013,
title = {Chapter 1 A brief history},
editor = {Mary M. Poulton},
series = {Handbook of Geophysical Exploration: Seismic Exploration},
publisher = {Pergamon},
volume = {30},
pages = {3-18},
year = {2001},
booktitle = {Computational neural networks for geophysical data processing},
issn = {0950-1401},
doi = {https://doi.org/10.1016/S0950-1401(01)80015-X},
url = {https://www.sciencedirect.com/science/article/pii/S095014010180015X},
author = {Mary M. Poulton},
abstract = {Publisher Summary
Computational neural networks are not just the grist of science fiction writers anymore nor are they a temporary success that will soon fade from use. The field of computational neural networks has matured in the last decade and found so many industrial applications that the notion of using a neural network to solve a particular problem no longer needs a “sales pitch” to management in many companies. Neural networks are now being routinely used in process control, manufacturing, quality control, product design, financial analysis, fraud detection, loan approval, voice and handwriting recognition, and data mining to name just a few application areas. The resurgence of neural network research is often attributed to the publication of a nonlinear network algorithm that overcame many of the limitations of the Perceptron and ADALINE. Many industrial applications of neural networks can claim significant increases in productivity, reduced costs, improved quality, or new products. This chapter present neural networks to the geophysicists as a serious computational tool—a tool with great potential and great limitations.}
}
@article{MOHAMEDHEN2024e39088,
title = {Towards multi-agent system for learning object recommendation},
journal = {Heliyon},
volume = {10},
number = {20},
pages = {e39088},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e39088},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024151195},
author = {Ahmed Salem Mohamedhen and Abdullah Alfazi and Nouha Arfaoui and Ridha Ejbali and Mohamedade Farouk Nanne},
keywords = {Deep learning, Knowledge level, Learning object, Learning style, Recommender system},
abstract = {The rapid increase of online educational content has made it harder for students to find specific information. E-learning recommender systems help students easily find the learning objects they require, improving the learning experience. The effectiveness of these systems is further improved by integrating deep learning with multi-agent systems. Multi-agent systems facilitate adaptable interactions within the system's various parts, and deep learning processes extensive data to understand learners' preferences. This collaboration results in custom-made suggestions that cater to individual learners. Our research introduces a multi-agent system tailored for suggesting learning objects in line with learners' knowledge levels and learning styles. This system uniquely comprises four agents: the learner agent, the tutor agent, the learning object agent, and the recommendation agent. It applies the Felder and Silverman model to pinpoint various student learning styles and organizes educational content based on the newest IEEE Learning Object Metadata standard. The system uses advanced techniques, such as Convolutional Neural Networks (CNN) and Multilayer Perceptrons (MLP), to propose learning objects. In terms of creating personalized learning experiences, this system is a considerable step forward. It effectively suggests learning objects that closely match each learner's personal profile, greatly enhancing student engagement and making the learning process more efficient.}
}
@incollection{MARCHAND201831,
title = {Chapter 2 - Analogical Mapping in Numerical Development},
editor = {Daniel B. Berch and David C. Geary and Kathleen {Mann Koepke}},
booktitle = {Language and Culture in Mathematical Cognition},
publisher = {Academic Press},
pages = {31-47},
year = {2018},
series = {Mathematical Cognition and Learning},
isbn = {978-0-12-812574-8},
doi = {https://doi.org/10.1016/B978-0-12-812574-8.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012812574800002X},
author = {Elisabeth Marchand and David Barner},
keywords = {Analogy, Number acquisition, Successor function, Numerical estimation, Structure mapping},
abstract = {This chapter outlines the contribution of analogical thinking in numerical cognition and specifically to number-word learning and numerical estimation. We begin with an overview of number-word learning, followed by a description of analogical mapping as defined by Gentner, 1983, Gentner, 2010, and discuss how children might acquire the meaning of counting based on analogical mapping. Next, we review the claim that very similar processes of analogical mapping may support numerical estimation, based on findings from studies of dot-array and number-line estimation. These studies suggest that children's knowledge of how the count list is structured and in particular the ordering and distance between numbers affects their ability to make accurate estimates. Finally, we discuss extensions of this idea to other cases where analogy has been proposed as a source of representational change. We conclude that analogical mappings enrich how humans transcend core numerical abilities to represent abstract content.}
}
@article{LUO2021106873,
title = {Guiding data-driven design ideation by knowledge distance},
journal = {Knowledge-Based Systems},
volume = {218},
pages = {106873},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.106873},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121001362},
author = {Jianxi Luo and Serhad Sarica and Kristin L. Wood},
keywords = {Data-driven design, Concept generation, Knowledge discovery, Knowledge distance, Network analysis, Patent data},
abstract = {Data-driven conceptual design methods and tools aim to inspire human ideation for new design concepts by providing external inspirational stimuli. In prior studies, the stimuli have been limited in terms of coverage, granularity, and retrieval guidance. Here, we present a knowledge-based expert system that provides design stimuli across the semantic, document and field levels simultaneously from all fields of engineering and technology and that follows creativity theories to guide the retrieval and use of stimuli according to the knowledge distance. The system is centered on the use of a network of all technology fields in the patent classification system, to store and organize the world’s cumulative data on the technological knowledge, concepts and solutions in the total patent database according to statistically-estimated knowledge distance between technology fields. In turn, knowledge distance guides the network-based exploration and retrieval of inspirational stimuli for inferences across near and far fields to generate new design ideas by analogy and combination. With two case studies, we showcase the effectiveness of using the system to explore and retrieve multilevel inspirational stimuli and generate new design ideas for both problem solving and open-ended innovation. These case studies also demonstrate the computer-aided ideation process, which is data-driven, computationally augmented, theoretically grounded, visually inspiring, and rapid.}
}
@article{IP2024111435,
title = {Estimating the structural diversity introduced by decision forest algorithms : A probabilistic approach},
journal = {Knowledge-Based Systems},
volume = {286},
pages = {111435},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111435},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124000704},
author = {Ryan H.L. Ip and Michael Bewong and Md. Nasim Adnan and Md. Zahidul Islam},
keywords = {Data mining, Diversity, Machine learning, R index, Root nodes},
abstract = {Structurally diverse decision trees are important for knowledge discovery and classification/prediction accuracy. Over the years, researchers have devoted much effort to the development of algorithms to increase diversity among the trees within an ensemble. While Kappa is commonly used to measure diversity among the decision trees, it does not measure the ability of the tree building algorithms to introduce diversity. Further, Kappa does not consider the structural diversity amongst the trees. Instead, Kappa measures the diversity of the predictions made from the trees produced, and are dependent on the datasets used. This paper presents a novel data-independent metric, called R index, for measuring the diversity that can be introduced by a decision forest algorithm without building the entire decision forest. The proposed measure is applied to five well-known algorithms that involve bagging and random subspacing. An efficient practical approach for calculating the R index empirically – R finder – is also proposed, and is implemented. Both R finder and Kappa were applied to thirty-two publicly available benchmark datasets under various algorithms to estimate the resulting diversity. The results indicate a generally strong negative correlation between R finder and Kappa, implying that R finder is effective at estimating the diversity of trees without the added computational costs associated with calculating Kappa.}
}
@article{ZHANG2024105271,
title = {RFSC-net: Re-parameterization forward semantic compensation network in low-light environments},
journal = {Image and Vision Computing},
volume = {151},
pages = {105271},
year = {2024},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2024.105271},
url = {https://www.sciencedirect.com/science/article/pii/S0262885624003767},
author = {Wenhao Zhang and Huiying Xu and Xinzhong Zhu and Yunzhong Si and Yao Dong and Xiao Huang and Hongbo Li},
keywords = {Low-light detection, Feature fusion, Re-parameterization, Forward semantic compensation},
abstract = {Although detectors currently perform well in well-light conditions, their accuracy decreases due to insufficient object information. In addressing this issue, we propose the Re-parameterization Forward Semantic Compensation Network (RFSC-Net). We propose the Reparameterization Residual Efficient Layer Aggregation Networks (RSELAN) for feature extraction, which integrates the concepts of re-parameterization and the Efficient Layer Aggregation Networks (ELAN). While focusing on the fusion of feature maps of the same dimension, it also incorporates upward fusion of lower-level feature maps, enhancing the detailed texture information in higher-level features. Our proposed Forward Semantic Compensation Feature Fusion (FSCFF) network reduces interference from high-level to low-level semantic information, retaining finer details to improve detection accuracy in low-light conditions. Experiments on the low-light ExDark and DarkFace datasets show that RFSC-Net improves mAP by 2% on ExDark and 0.5% on DarkFace over the YOLOv8n baseline, without an increase in parameter counts. Additionally, AP50 is enhanced by 2.1% on ExDark and 1.1% on DarkFace, with a mere 3.7 ms detection latency on ExDark.}
}
@article{ANTONIDES2022101010,
title = {A learning trajectory for enumerating permutations: Applying and elaborating a theory of levels of abstraction},
journal = {The Journal of Mathematical Behavior},
volume = {68},
pages = {101010},
year = {2022},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2022.101010},
url = {https://www.sciencedirect.com/science/article/pii/S0732312322000785},
author = {Joseph Antonides and Michael T. Battista},
keywords = {Permutations, Learning trajectories, Abstraction, Combinatorics, Teaching experiment, Concreteness fading},
abstract = {Permutations are fundamental to combinatorics and other areas of mathematics, and it is important that students develop efficient and conceptually supported ways of mentally constructing, listing, and enumerating them. To date, there is still much to learn about how students reason about enumerating permutations, and how instruction can support students’ conceptual development. We address this gap in the research literature by carefully tracing the evolution of two preservice middle school teachers’ permutation enumeration strategies and conceptualizations, which led to the formulation of levels of sophistication for combinatorial reasoning. These levels are explained by applying and extending a constructivist theory of levels of abstraction. Additionally, we outline an instructional approach that was instrumental in facilitating student learning. Together, the proposed levels and linked instructional approach constitute an initial learning trajectory for permutations that we believe could be useful for understanding and supporting post-secondary non-STEM students’ meaningful conceptualizations and enumerations of permutations.}
}
@article{MARTIN20102089,
title = {Integrating learning theories and application-based modules in teaching linear algebra},
journal = {Linear Algebra and its Applications},
volume = {432},
number = {8},
pages = {2089-2099},
year = {2010},
note = {Special issue devoted to the 15th ILAS Conference at Cancun, Mexico, June 16-20, 2008},
issn = {0024-3795},
doi = {https://doi.org/10.1016/j.laa.2009.08.030},
url = {https://www.sciencedirect.com/science/article/pii/S0024379509004704},
author = {William Martin and Sergio Loch and Laurel Cooley and Scott Dexter and Draga Vidakovic},
keywords = {Linear algebra, Learning theory, Curriculum, Pedagogy, Constructivist theories, APOS – Action-Process-Object-Schema, Theoretical framework, Encapsulated process, Thematicized schema, Triad – intra, Inter, Trans, Genetic decomposition, Vector addition, Matrix, Matrix multiplication, Matrix representation, Basis, Column space, Row space, Null space, Eigenspace, Transformation},
abstract = {The research team of The Linear Algebra Project developed and implemented a curriculum and a pedagogy for parallel courses in (a) linear algebra and (b) learning theory as applied to the study of mathematics with an emphasis on linear algebra. The purpose of the ongoing research, partially funded by the National Science Foundation, is to investigate how the parallel study of learning theories and advanced mathematics influences the development of thinking of individuals in both domains. The researchers found that the particular synergy afforded by the parallel study of math and learning theory promoted, in some students, a rich understanding of both domains and that had a mutually reinforcing effect. Furthermore, there is evidence that the deeper insights will contribute to more effective instruction by those who become high school math teachers and, consequently, better learning by their students. The courses developed were appropriate for mathematics majors, pre-service secondary mathematics teachers, and practicing mathematics teachers. The learning seminar focused most heavily on constructivist theories, although it also examined socio-cultural and historical perspectives. A particular theory, Action–Process–Object–Schema (APOS) [10], was emphasized and examined through the lens of studying linear algebra. APOS has been used in a variety of studies focusing on student understanding of undergraduate mathematics. The linear algebra courses include the standard set of undergraduate topics. This paper reports the results of the learning theory seminar and its effects on students who were simultaneously enrolled in linear algebra and students who had previously completed linear algebra and outlines how prior research has influenced the future direction of the project.}
}
@article{LUO2025121403,
title = {Sequential three-way group decision-making for double hierarchy hesitant fuzzy linguistic term set},
journal = {Information Sciences},
volume = {687},
pages = {121403},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121403},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524013173},
author = {Nanfang Luo and Qinghua Zhang and Qin Xie and Yutai Wang and Longjun Yin and Guoyin Wang},
keywords = {Granular computing, Sequential three-way group decision-making, Double hierarchy hesitant fuzzy linguistic term sets, Information fusion},
abstract = {Group decision-making (GDM) characterized by complexity and uncertainty is an essential part of various life scenarios. Most existing researches lack tools to fuse information quickly and interpret decision results for partially formed decisions. This limitation is particularly noticeable when there is a need to improve the efficiency of GDM. To address this issue, a novel multi-level sequential three-way decision for group decision-making (S3W-GDM) method is constructed from the perspective of granular computing. This method simultaneously considers the vagueness, hesitation, and variation of GDM problems under double hierarchy hesitant fuzzy linguistic term sets (DHHFLTS) environment. First, for fusing information efficiently, a novel multi-level expert information fusion method is proposed, and the concepts of expert decision table and the extraction/aggregation of decision-leveled information based on the multi-level granularity are defined. Second, the neighborhood theory, outranking relation and regret theory (RT) are utilized to redesign the calculations of conditional probability and relative loss function. Then, the granular structure of DHHFLTS based on the sequential three-way decision (S3WD) is defined to improve the decision-making efficiency, and the decision-making strategy and interpretation of each decision-level are proposed. Furthermore, the algorithm of S3W-GDM is given. Finally, an illustrative example of diagnosis is presented, and the comparative and sensitivity analysis with other methods are performed to verify the efficiency and rationality of the proposed method.}
}
@incollection{GEWEKE20013463,
title = {Chapter 56 - Computationally Intensive Methods for Integration in Econometrics**The authors gratefully acknowledge financial support from National Science Foundation grants SBR-9511280, SBR-9731037, SES-9814342, and SES-9819444.},
editor = {James J. Heckman and Edward Leamer},
series = {Handbook of Econometrics},
publisher = {Elsevier},
volume = {5},
pages = {3463-3568},
year = {2001},
issn = {1573-4412},
doi = {https://doi.org/10.1016/S1573-4412(01)05009-7},
url = {https://www.sciencedirect.com/science/article/pii/S1573441201050097},
author = {John Geweke and Michael Keane},
keywords = {Bayesian inference, discrete choice, dynamic optimization, integration, Markov chain Monte Carlo, multinomial probit, normal mixtures, selection models, Primary C15, Secondary C11},
abstract = {Until recently, inference in many interesting models was precluded by the requirement of high dimensional integration. But dramatic increases in computer speed, and the recent development of new algorithms that permit accurate Monte Carlo evaluation of high dimensional integrals, have greatly expanded the range of models that can be considered. This chapter presents the methodology for several of the most important Monte Carlo methods, supplemented by a set of concrete examples that show how the methods are used. Some of the examples are new to the econometrics literature. They include inference in multinomial discrete choice models and selection models in which the standard normality assumption is relaxed in favor of a multivariate mixture of normals assumption. Several Monte Carlo experiments indicate that these methods are successful at identifying departures from normality when they are present. Throughout the chapter the focus is on inference in parametric models that permit rich variation in the distribution of disturbances. The chapter first discusses Monte Carlo methods for the evaluation of high dimensional integrals, including integral simulators like the GHK method, and Markov Chain Monte Carlo methods like Gibbs sampling and the Metropolis–Hastings algorithm. It then turns to methods for approximating solutions to discrete choice dynamic optimization problems, including the methods developed by Keane and Wolpin, and Rust, as well as methods for circumventing the integration problem entirely, such as the approach of Geweke and Keane. The rest of the chapter deals with specific examples: classical simulation estimation for multinomial probit models, both in the cross sectional and panel data contexts; univariate and multivariate latent linear models; and Bayesian inference in dynamic discrete choice models in which the future component of the value function is replaced by a flexible polynomial.}
}
@article{OVERLAN2017320,
title = {Learning abstract visual concepts via probabilistic program induction in a Language of Thought},
journal = {Cognition},
volume = {168},
pages = {320-334},
year = {2017},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2017.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0010027717302020},
author = {Matthew C. Overlan and Robert A. Jacobs and Steven T. Piantadosi},
keywords = {Concept learning, Visual learning, Language of Thought, Computational modeling, Behavioral experiment},
abstract = {The ability to learn abstract concepts is a powerful component of human cognition. It has been argued that variable binding is the key element enabling this ability, but the computational aspects of variable binding remain poorly understood. Here, we address this shortcoming by formalizing the Hierarchical Language of Thought (HLOT) model of rule learning. Given a set of data items, the model uses Bayesian inference to infer a probability distribution over stochastic programs that implement variable binding. Because the model makes use of symbolic variables as well as Bayesian inference and programs with stochastic primitives, it combines many of the advantages of both symbolic and statistical approaches to cognitive modeling. To evaluate the model, we conducted an experiment in which human subjects viewed training items and then judged which test items belong to the same concept as the training items. We found that the HLOT model provides a close match to human generalization patterns, significantly outperforming two variants of the Generalized Context Model, one variant based on string similarity and the other based on visual similarity using features from a deep convolutional neural network. Additional results suggest that variable binding happens automatically, implying that binding operations do not add complexity to peoples’ hypothesized rules. Overall, this work demonstrates that a cognitive model combining symbolic variables with Bayesian inference and stochastic program primitives provides a new perspective for understanding people’s patterns of generalization.}
}
@article{CETINSAYA2024101626,
title = {From PID to swarms: A decade of advancements in drone control and path planning - A systematic review (2013–2023)},
journal = {Swarm and Evolutionary Computation},
volume = {89},
pages = {101626},
year = {2024},
issn = {2210-6502},
doi = {https://doi.org/10.1016/j.swevo.2024.101626},
url = {https://www.sciencedirect.com/science/article/pii/S2210650224001640},
author = {Berk Cetinsaya and Dirk Reiners and Carolina Cruz-Neira},
keywords = {Unmanned aerial vehicle (UAV), Drone control, Path planning, Swarm intelligence, Nature-inspired swarm algorithms},
abstract = {This systematic literature review synthesizes and evaluates existing research on drone control and path planning, encompassing the principles of swarm intelligence and nature-inspired algorithms. However, it is not limited to these; it also explores other algorithms to provide a comprehensive overview of the state-of-the-art in this rapidly evolving field. The review identifies and analyzes key trends, challenges, and advancements in drone control and path planning. It investigates the evolution of control strategies, ranging from classical proportional-integral-derivative (PID) controllers to modern swarm algorithms and reinforcement learning-based techniques. Additionally, it explores path planning methodologies, including traditional optimization algorithms and heuristic-based approaches, and specifically, swarm algorithms within the context of drone swarms. The emphasis on nature-inspired intelligent computation extends to the exploration of swarm intelligence and cooperative planning as integral components of drone path planning. By synthesizing and critically analyzing the literature, this systematic review not only presents a comprehensive understanding of the current landscape of drone control and path planning, but it also acknowledges the role of various nature-inspired algorithms, including but not limited to swarm intelligence, and identifies avenues for future research in this evolving field.}
}
@article{UCAN2022104878,
title = {Advice hierarchies among finite automata},
journal = {Information and Computation},
volume = {288},
pages = {104878},
year = {2022},
note = {Special Issue: Selected Papers of the 14th International Conference on Language and Automata Theory and Applications, LATA 2020},
issn = {0890-5401},
doi = {https://doi.org/10.1016/j.ic.2022.104878},
url = {https://www.sciencedirect.com/science/article/pii/S0890540122000207},
author = {Ahmet Bilal Uçan and A.C. Cem Say},
keywords = {Formal languages, Automata theory, Advised computation},
abstract = {We examine the effects of supplying increasing amounts of trusted advice to a finite automaton. Previous work has shown that allowing such automata with a single advice tape to make a single pass over their input renders them unable to recognize the palindromes language, whereas both two-way machines reading advice from a single tape and one-way machines with multiple advice tapes can recognize all languages with exponentially bounded amounts of advice. We study several architectural variants and demonstrate the existence of language hierarchies based on increased advice length, runtime (measured in terms of the number of allowed left-to-right passes on the input), and number of advice tapes. We also prove some lower bounds for recognizing certain concrete languages.}
}
@incollection{GALLISTEL2017141,
title = {1.08 - Learning and Representation☆},
editor = {John H. Byrne},
booktitle = {Learning and Memory: A Comprehensive Reference (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {141-154},
year = {2017},
isbn = {978-0-12-805291-4},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.21009-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093245210092},
author = {Randy Gallistel},
keywords = {Associations, Cognitive map, Episodic memory, Information, Memory molecules, Path integration, Read–write memory, Signals, Sun compass, Symbols},
abstract = {Behavioral and electrophysiological evidence implies that brains compute representations of aspects of the experienced world. For example, they compute the animal's position in the world by integrating its velocity with respect to time. Other examples are the learning of the solar ephemeris, the construction of a cognitive map, and episodic memory in food caching. Representations require a symbolic read–write memory that carries information extracted from experience forward in time in a computationally accessible form. The analogy between the architecture of computer memory and the genetic architecture suggests the sort of memory structure to be looked for in the nervous system.}
}
@article{BARTH2009441,
title = {Children’s multiplicative transformations of discrete and continuous quantities},
journal = {Journal of Experimental Child Psychology},
volume = {103},
number = {4},
pages = {441-454},
year = {2009},
note = {Special Issue: Typical Development of Numerical Cognition},
issn = {0022-0965},
doi = {https://doi.org/10.1016/j.jecp.2009.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S0022096509000289},
author = {Hilary Barth and Andrew Baron and Elizabeth Spelke and Susan Carey},
keywords = {Ratio sensitivity, Ratios, Multiplicative operations, Doubling, Halving, Numerical cognition},
abstract = {Recent studies have documented an evolutionarily primitive, early emerging cognitive system for the mental representation of numerical quantity (the analog magnitude system). Studies with nonhuman primates, human infants, and preschoolers have shown this system to support computations of numerical ordering, addition, and subtraction involving whole number concepts prior to arithmetic training. Here we report evidence that this system supports children’s predictions about the outcomes of halving and perhaps also doubling transformations. A total of 138 kindergartners and first graders were asked to reason about the quantity resulting from the doubling or halving of an initial numerosity (of a set of dots) or an initial length (of a bar). Controls for dot size, total dot area, and dot density ensured that children were responding to the number of dots in the arrays. Prior to formal instruction in symbolic multiplication, division, or rational number, halving (and perhaps doubling) computations appear to be deployed over discrete and possibly continuous quantities. The ability to apply simple multiplicative transformations to analog magnitude representations of quantity may form a part of the toolkit that children use to construct later concepts of rational number.}
}
@article{TARAPOULOUZI2022123410,
title = {Heavy metals detection at chemometrics-powered electrochemical (bio)sensors},
journal = {Talanta},
volume = {244},
pages = {123410},
year = {2022},
issn = {0039-9140},
doi = {https://doi.org/10.1016/j.talanta.2022.123410},
url = {https://www.sciencedirect.com/science/article/pii/S0039914022002065},
author = {Maria Tarapoulouzi and Vincenzo Ortone and Stefano Cinti},
keywords = {Multivariate analysis, Design of experiment, Artificial intelligence, Electroanalysis, Sensors, Heavy metals},
abstract = {Heavy metals represent a serious issue regarding both environmental and health status. Their monitoring is necessary and it is necessary the development of decentralized approaches that are able to enforce the risk assessment. Electrochemical sensors and biosensors, with the various architectures, represent a solid reality often involved for this type of analytical determination. Although these approaches offer easy-to-use and portable tools, some limitations are often highlighted in presence of multi-targets and/or real matrices. However, chemometrics- and artificial intelligence-based tools, both for designing and for data analyzing, display the capability in producing novel functionality towards the management of complex matrices which often contain more information than those that are visualized with sensor detection. Design of experiment, exploratory, predictive and regression analysis can push the world of electrochemical (bio)sensors beyond the state of the art, because is still too large the number of analytical chemists that do not deal with multivariate thinking. In this paper, the use of multivariate methods applied to electrochemical sensing of heavy metals is showed, and each approach is described in terms of efficacy and outputs.}
}
@article{MIKITEN1995141,
title = {Intuition-based computing: A new kind of ‘virtual reality’},
journal = {Mathematics and Computers in Simulation},
volume = {40},
number = {1},
pages = {141-147},
year = {1995},
issn = {0378-4754},
doi = {https://doi.org/10.1016/0378-4754(95)00023-1},
url = {https://www.sciencedirect.com/science/article/pii/0378475495000231},
author = {Terry M. Mikiten},
keywords = {Intuition, Mind, Problem-solving, Creativity, Cognition, Computing, Grand challenge},
abstract = {It is helpful to consider the mind and the computer as two separate information domains. Each has a separate system of rules that guide behavior. The interaction between the two is characterized as an interplay between rule systems. In this view, there should be interactions which are optimal and others which are not. To understand this, the first task is to identify the rules that operate in each domain. The next is to see how they interact. It is concluded that rules of the mind which give rise to what is generally termed ‘intuition’ is altogether compatible with rules of computation. This, in turn, suggests computational systems capable of independent ‘intuitive’ processing on the one hand, and other computational systems which can serve to augment human intuition.}
}
@article{HYLAND2007437,
title = {The Category Theoretic Understanding of Universal Algebra: Lawvere Theories and Monads},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {172},
pages = {437-458},
year = {2007},
note = {Computation, Meaning, and Logic: Articles dedicated to Gordon Plotkin},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2007.02.019},
url = {https://www.sciencedirect.com/science/article/pii/S1571066107000874},
author = {Martin Hyland and John Power},
keywords = {Universal algebra, Lawvere theory, monad, computational effect},
abstract = {Lawvere theories and monads have been the two main category theoretic formulations of universal algebra, Lawvere theories arising in 1963 and the connection with monads being established a few years later. Monads, although mathematically the less direct and less malleable formulation, rapidly gained precedence. A generation later, the definition of monad began to appear extensively in theoretical computer science in order to model computational effects, without reference to universal algebra. But since then, the relevance of universal algebra to computational effects has been recognised, leading to renewed prominence of the notion of Lawvere theory, now in a computational setting. This development has formed a major part of Gordon Plotkin's mature work, and we study its history here, in particular asking why Lawvere theories were eclipsed by monads in the 1960's, and how the renewed interest in them in a computer science setting might develop in future.}
}
@incollection{SHEN20231,
title = {Interdisciplinary science learning},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {1-9},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.13030-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305130301},
author = {Ji Shen and Changzhao Wang},
keywords = {Cross-disciplinary learning, Integrated learning, Interdisciplinary science learning, Interdisciplinary understanding, Interdisciplinary practices, Knowledge integration, Multidisciplinary learning, STEM education, STEAM education},
abstract = {This article presents a conceptual review of studies and programs related to interdisciplinary science learning in different boundary-crossing scenarios including within sciences, across STEM, and with non-STEM fields. Specific examples are also included to illuminate the four core interdisciplinary practices, namely, translation, transfer, integration, and transformation, that cut across these interdisciplinary learning contexts. The article also discusses challenges for interdisciplinary science learning and strategies proposed to address these challenges. More empirical studies are called to test the effectiveness of these strategies to facilitate and assess interdisciplinary science learning in different domains and contexts.}
}
@article{TRAUTTEUR2007106,
title = {A note on discreteness and virtuality in analog computing},
journal = {Theoretical Computer Science},
volume = {371},
number = {1},
pages = {106-114},
year = {2007},
note = {Computing and the Natural Sciences},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2006.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S0304397506007699},
author = {Giuseppe Trautteur and Guglielmo Tamburrini},
keywords = {Analog computing, Virtual machine, Cognitive modelling},
abstract = {The need for physically motivated discreteness and finiteness conditions emerges in models of both analog and digital computing that are genuinely concerned with physically realizable computational processes. This is brought out by a critical examination of notional analog superTuring devices which involve physically untenable idealizations about the perfect functioning of analog apparatuses and infinite precision of physical measurements. The capability for virtual behaviour, that is, the capability of interpreting, storing, transforming, creating the code, and thereby mimicking the behaviour of (Turing) machines, is used here to introduce a new dimension in the discussion of the analog–digital watershed. In the light of recent results on the analog simulation of digital computing, we examine the role of virtuality as a discriminating factor between these two species of computing, and immerse this problem in the context of natural computing. Is virtuality instantiated in parts of the natural world other than computer technology? This broad issue is examined in connection with the computational modelling of brain and mental information processing.}
}
@article{DEGRANDE201960,
title = {To add or to multiply? An investigation of the role of preference in children's solutions of word problems},
journal = {Learning and Instruction},
volume = {61},
pages = {60-71},
year = {2019},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2019.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0959475218304511},
author = {Tine Degrande and Lieven Verschaffel and Wim {Van Dooren}},
keywords = {Word problem solving, Additive reasoning, Multiplicative reasoning, Preference, Skill},
abstract = {Previous research has shown that upper primary school children frequently erroneously solve additive word problems multiplicatively, while younger children frequently erroneously solve multiplicative word problems additively. It has been suggested that children's preference for additive or multiplicative relations explains these errors, besides their lacking skills, but this claim has not been tested empirically yet. Therefore, we administered four test instruments (a word problem test, a preference test, and two tests measuring additive and multiplicative computation and discrimination skill) to 246 third to sixth graders. Previous research results on errors in word problems, as well as on preference were replicated and systematized. Further, they were extended by explaining this erroneous word problem solving behavior by preference, for those children who unmistakably had acquired the necessary computation and discrimination skills. This finding provides strong evidence for the unique additional role of children's preference in erroneous additive or multiplicative word problem solving behavior.}
}
@article{OYEFUSI2024107549,
title = {Development of a novel performance evaluation framework for implementing regenerative practices in construction},
journal = {Environmental Impact Assessment Review},
volume = {107},
pages = {107549},
year = {2024},
issn = {0195-9255},
doi = {https://doi.org/10.1016/j.eiar.2024.107549},
url = {https://www.sciencedirect.com/science/article/pii/S0195925524001367},
author = {Oluwatobi Nurudeen Oyefusi and Wallace Imoudu Enegbuma and Andre Brown and Oludolapo Ibrahim Olanrewaju},
keywords = {Regenerative practices, Sustainability, Construction industry, Performance, FAHP},
abstract = {While sustainable construction practices effectively reduce environmental impact, their exclusive focus on environmental, economic, and social goals limits their ability to actively foster positive transformation and ecosystem restoration. Addressing the growing challenges in the built environment necessitates a shift to regenerative practices within the construction industry. Unlike sustainability, regenerative practices go beyond the concept of merely sustaining the status quo; they are geared towards actively enhancing and restoring the built environment over time. However, implementing these practices is less prominent in the construction industry due to the absence of a suitable tool for evaluating their expected performance outcomes. This study bridges this gap by introducing a novel performance evaluation framework for implementing regenerative construction practices, establishing a benchmark for implementation. Through an extensive literature review and data collection from a committee of regenerative outcome leads, we employ the Fuzzy Analytical Hierarchical Process (FAHP) to establish interconnections among key regenerative performance criteria. Results highlight the dominant significance of “Healthy, more resilient, and connected communities,” surpassing other criteria like “Thriving and prosperous natural systems,” “Prosperous and resilient local economies,” and “Net-positive environmental development.” The proposed evaluation framework offers theoretical and practical implications, fostering a new theoretical approach that exceeds sustainability standards and provides tangible guidance for construction decision-makers.}
}
@article{ALAYANDE2020e00436,
title = {Estimating effective rates of protection in Nigeria's protected cement industry},
journal = {Scientific African},
volume = {8},
pages = {e00436},
year = {2020},
issn = {2468-2276},
doi = {https://doi.org/10.1016/j.sciaf.2020.e00436},
url = {https://www.sciencedirect.com/science/article/pii/S2468227620301745},
author = {Folarin Alayande},
keywords = {Trade protection, Effective rate of protection, Trade policy, Nigerian cement},
abstract = {Trade protection for selected products is a key element of smart industrial policy in many developing countries. Understanding quantitative measures of trade protection for industrial and consumer commodities is therefore a key requisite to determining the effectiveness of export-led growth in particular, and overall industrial policy. However, accurate estimates for trade policy incentives provided to industrial products are few and comparable datasets are sparse, with the most recent published country datasets dated as far back as 2012, yet with limited sector indices. This study estimates the effective rate of protection (ERP), a key index of trade protection and industrial policy, for the cement industry, one of the largest manufacturing industries in Nigeria. Time series data for 16 years, on the actual cost of trade protection, including tariff barriers and import prohibition bans, from 2000 to 2015 is used. With the ERP, the true cost of protection of domestic manufactures from imported goods, is computed using input shares and tariff data from the United Nations COMTRADE database. The data show the basis for computation and provides a re-useable template for central planners in the computation of effective rate of protection for similar manufacturing industries and other African countries. The computed ERP for the cement industry in Nigeria show a relatively high protection rate, and the overwhelming impact of trade prohibition on the ERP after the implementation of the Federal Government's incentive-led Backward Integration Programme. The evidence is compared with earlier data on Africa. Preliminary findings and trend analysis indicate a high correlation between the ERP and value added to gross domestic product (GDP).}
}
@article{GROBAS20202903,
title = {Biofilm and swarming emergent behaviours controlled through the aid of biophysical understanding and tools},
journal = {Biochemical Society Transactions},
volume = {48},
number = {6},
pages = {2903-2913},
year = {2020},
issn = {1470-8752},
doi = {https://doi.org/10.1042/BST20200972},
url = {https://www.sciencedirect.com/science/article/pii/S1470875220001245},
author = {Iago Grobas and Dario G. Bazzoli and Munehiro Asally},
keywords = {active matter, biofilms, living materials, pattern engineering, physics of microbes, swarming},
abstract = {Bacteria can organise themselves into communities in the forms of biofilms and swarms. Through chemical and physical interactions between cells, these communities exhibit emergent properties that individual cells alone do not have. While bacterial communities have been mainly studied in the context of biochemistry and molecular biology, recent years have seen rapid advancements in the biophysical understanding of emergent phenomena through physical interactions in biofilms and swarms. Moreover, new technologies to control bacterial emergent behaviours by physical means are emerging in synthetic biology. Such technologies are particularly promising for developing engineered living materials (ELM) and devices and controlling contamination and biofouling. In this minireview, we overview recent studies unveiling physical and mechanical cues that trigger and affect swarming and biofilm development. In particular, we focus on cell shape, motion and density as the key parameters for mechanical cell–cell interactions within a community. We then showcase recent studies that use physical stimuli for patterning bacterial communities, altering collective behaviours and preventing biofilm formation. Finally, we discuss the future potential extension of biophysical and bioengineering research on microbial communities through computational modelling and deeper investigation of mechano-electrophysiological coupling.}
}
@article{DAY2021104431,
title = {Adjoint based optimisation for efficient VAWT blade aerodynamics using CFD},
journal = {Journal of Wind Engineering and Industrial Aerodynamics},
volume = {208},
pages = {104431},
year = {2021},
issn = {0167-6105},
doi = {https://doi.org/10.1016/j.jweia.2020.104431},
url = {https://www.sciencedirect.com/science/article/pii/S016761052030341X},
author = {Harry Day and Derek Ingham and Lin Ma and Mohamed Pourkashanian},
keywords = {Adjoint, Aerodynamics, CFD, Optimisation, VAWT, Vertical axis wind turbine},
abstract = {The objective of this work is to demonstrate the viability of applying Adjoint methods to aerodynamic optimisation of VAWTs. Adjoint methods are very powerful optimisation techniques which have been implemented effectively in other fields, yet there is an absence of such work within VAWT literature. A ‘semi-transient’ optimisation process is proposed, using Adjoint optimisation data from single instances in time to improve VAWT performance. This is challenging due to the unsteady nature of VAWT aerodynamics. A pitching aerofoil model approximates the VAWT flow field, drastically reducing computational cost. Details are given on the necessary CFD model(s), Adjoint solver settings, and optimisation philosophy. The optimisation process was applied to a typical VAWT in the commercial CFD software ANSYS Fluent. A high tip-speed-ratio case is chosen to minimise unsteady flow affects. The results show novel blade geometries which improve the VAWT average power coefficient when compared to the original NACA0018 blade. Such a method is novel in the field of VAWTs, and the use of Adjoint methods with low cost CFD models provides an efficient optimisation methodology that can be readily adopted by the VAWT design community. This work sets the foundation for a new and very promising avenue for VAWT research.}
}
@article{LAKHLIFI2023104181,
title = {Heuristics and biases in medical decision-making under uncertainty: The case of neuropronostication for consciousness disorders},
journal = {La Presse Médicale},
volume = {52},
number = {2},
pages = {104181},
year = {2023},
note = {Disorders of Consciousness},
issn = {0755-4982},
doi = {https://doi.org/10.1016/j.lpm.2023.104181},
url = {https://www.sciencedirect.com/science/article/pii/S0755498223000180},
author = {Camille Lakhlifi and Benjamin Rohaut},
abstract = {Neuropronostication for consciousness disorders can be very complex and prone to high uncertainty. Despite notable advancements in the development of dedicated scales and physiological markers using innovative paradigms, these technical progressions are often overshadowed by factors intrinsic to the medical environment. Beyond the scarcity of objective data guiding medical decisions, factors like time pressure, fatigue, multitasking, and emotional load can drive clinicians to rely more on heuristic-based clinical reasoning. Such an approach, albeit beneficial under certain circumstances, may lead to systematic error judgments and impair medical decisions, especially in complex and uncertain environments. After a brief review of the main theoretical frameworks, this paper explores the influence of clinicians' cognitive biases on clinical reasoning and decision-making in the challenging context of neuroprognostication for consciousness disorders. The discussion further revolves around developing and implementing various strategies designed to mitigate these biases and their impact, aiming to enhance the quality of care and the patient safety.}
}
@article{REHDER201454,
title = {Independence and dependence in human causal reasoning},
journal = {Cognitive Psychology},
volume = {72},
pages = {54-107},
year = {2014},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2014.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0010028514000176},
author = {Bob Rehder},
keywords = {Causal reasoning, Causal inference, Causal Markov condition, Conditional independence, Screening off},
abstract = {Causal graphical models (CGMs) are a popular formalism used to model human causal reasoning and learning. The key property of CGMs is the causal Markov condition, which stipulates patterns of independence and dependence among causally related variables. Five experiments found that while adult’s causal inferences exhibited aspects of veridical causal reasoning, they also exhibited a small but tenacious tendency to violate the Markov condition. They also failed to exhibit robust discounting in which the presence of one cause as an explanation of an effect makes the presence of another less likely. Instead, subjects often reasoned “associatively,” that is, assumed that the presence of one variable implied the presence of other, causally related variables, even those that were (according to the Markov condition) conditionally independent. This tendency was unaffected by manipulations (e.g., response deadlines) known to influence fast and intuitive reasoning processes, suggesting that an associative response to a causal reasoning question is sometimes the product of careful and deliberate thinking. That about 60% of the erroneous associative inferences were made by about a quarter of the subjects suggests the presence of substantial individual differences in this tendency. There was also evidence that inferences were influenced by subjects’ assumptions about factors that disable causal relations and their use of a conjunctive reasoning strategy. Theories that strive to provide high fidelity accounts of human causal reasoning will need to relax the independence constraints imposed by CGMs.}
}
@article{AHMAN201351,
title = {Normalization by Evaluation and Algebraic Effects},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {298},
pages = {51-69},
year = {2013},
note = {Proceedings of the Twenty-ninth Conference on the Mathematical Foundations of Programming Semantics, MFPS XXIX},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2013.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S1571066113000534},
author = {Danel Ahman and Sam Staton},
keywords = {Algebraic effects, Type theory, Normalization by evaluation, Presheaves, Monads},
abstract = {We examine the interplay between computational effects and higher types. We do this by presenting a normalization by evaluation algorithm for a language with function types as well as computational effects. We use algebraic theories to treat the computational effects in the normalization algorithm in a modular way. Our algorithm is presented in terms of an interpretation in a category of presheaves equipped with partial equivalence relations. The normalization algorithm and its correctness proofs are formalized in dependent type theory (Agda).}
}
@article{UENO2011385,
title = {Lichtheim 2: Synthesizing Aphasia and the Neural Basis of Language in a Neurocomputational Model of the Dual Dorsal-Ventral Language Pathways},
journal = {Neuron},
volume = {72},
number = {2},
pages = {385-396},
year = {2011},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2011.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S0896627311008348},
author = {Taiji Ueno and Satoru Saito and Timothy T. Rogers and Matthew A. Lambon Ralph},
abstract = {Summary
Traditional neurological models of language were based on a single neural pathway (the dorsal pathway underpinned by the arcuate fasciculus). Contemporary neuroscience indicates that anterior temporal regions and the “ventral” language pathway also make a significant contribution, yet there is no computationally-implemented model of the dual pathway, nor any synthesis of normal and aphasic behavior. The “Lichtheim 2” model was implemented by developing a new variety of computational model which reproduces and explains normal and patient data but also incorporates neuroanatomical information into its architecture. By bridging the “mind-brain” gap in this way, the resultant “neurocomputational” model provides a unique opportunity to explore the relationship between lesion location and behavioral deficits, and to provide a platform for simulating functional neuroimaging data.}
}
@incollection{VALYAN202025,
title = {Chapter 4 - Decision-making deficits in substance use disorders: cognitive functions, assessment paradigms, and levels of evidence},
editor = {Antonio Verdejo-Garcia},
booktitle = {Cognition and Addiction},
publisher = {Academic Press},
pages = {25-61},
year = {2020},
isbn = {978-0-12-815298-0},
doi = {https://doi.org/10.1016/B978-0-12-815298-0.00004-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128152980000046},
author = {Alireza Valyan and Hamed Ekhtiari and Ryan Smith and Martin P. Paulus},
keywords = {Assessment, Behavioral tasks, Computational models, Decision-making dysfunction, fMRI, Intervention, Substance use disorder},
abstract = {Aberrant decision-making plays an important role in both the onset and maintenance of substance use disorders (SUDs). The current state of research within the field of SUDs can be usefully summarized within three broad dimensions: (1) the goal of characterizing the affected cognitive components that contribute to aberrant decision-making (i.e., value, probability, time, and learning functions), (2) the instruments/methods used to accomplish that goal (i.e., self-reports, behavioral tasks, computational modeling, and brain mapping), and (3) the levels of evidence afforded by those instruments/methods. In this chapter, we review and organize the most recent findings based on this three-dimensional framework. Our aim is to (1) provide a comprehensive synthesis of current research on decision-making in SUDs that can serve as a useful resource to guide future research, (2) highlight current limitations in the field and promising future research directions, and (3) illustrate ways in which the framework that we provide may inform the design and implementation of interventional strategies that can advance the field of addiction medicine.}
}
@incollection{FRANTZ202025,
title = {3 - The “Big 3.” Simon, Katona, Leibenstein},
editor = {Roger Frantz},
booktitle = {The Beginnings of Behavioral Economics},
publisher = {Academic Press},
pages = {25-45},
year = {2020},
series = {Perspectivs in Behavioral Economics and the Economics of Beh},
isbn = {978-0-12-815289-8},
doi = {https://doi.org/10.1016/B978-0-12-815289-8.00003-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128152898000034},
author = {Roger Frantz},
keywords = {ECONS and HUMANS, Behavioral macroeconomics, Intervening variables, Gestalt psychology, Tit-for-tat, Parable of the ant, Bounded rationality, Satisficing, X-efficiency, Non-allocative efficiency, Das John Maynard Keynes rationality problem},
abstract = {Katona, Leibenstein, and Simon are the “Big 3” of the old behavioral economics. Why are they the Big 3? Their names are most often mentioned by others in terms of “early” behavioral economics. They wrote convincingly about homo economicus, and in doing so they began knocking him off his pedestal. Without this behavioral economicus would never exist. With respect to the Big 3’s writings, Leibenstein wrote about, among other things, multiple-selves, gift exchange, social norms, consumer interdependence, non-allocative efficiency, and less than perfect rationality. Katona wrote about, among other things, ECONS vs HUMANS, expectations, aspirations, adaptive behavior, macro-behavioral theory, procedural rationality, and less than perfect rationality. Among other things, Herbert Simon wrote about bounded rationality, intuition (System 1) and logical thinking (System 2), ECONS vs HUMANS, satisficing, rejection of as if theorizing, learning theories in economics and psychology, rationality in economics and psychology, the nature of human knowledge (tacit knowledge), and less than perfect rationality.}
}
@article{XIONG2020180,
title = {Construction of approximate reasoning model for dynamic CPS network and system parameter identification},
journal = {Computer Communications},
volume = {154},
pages = {180-187},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.02.073},
url = {https://www.sciencedirect.com/science/article/pii/S0140366420301225},
author = {Juxia Xiong and Jinzhao Wu},
keywords = {Cyber physical system, Network, Event message modeling, Interactive multi-model algorithm},
abstract = {CPS (Cyber Physical System) is a large and complex real-time feedback system that integrates computing processes, physical processes, communication networks, sensor networks, and control systems. It has the powerful function of sensing and controlling the physical environment, which is a big wave following the Internet technology. Because the forms of communication, interaction, and collaboration between heterogeneous units within the CPS are intricate and complex, a comprehensive model needs to be established to describe and analyze the CPS. This paper analyzes the CPS architecture and proposes a new and more complete CPS architecture, decomposes according to this architecture, and classifies the physical entities in the CPS. At the same time, event-based modeling thinking is used to define, classify and formalize event messages. Considering the higher real-time requirements of CPS, an event weighting algorithm was designed according to the different priorities of real-time events. In order to reduce the congestion caused by the limited network bandwidth in the CPS system, improve the ability to identify abnormal data with great uncertainty, and fully guarantee the response rate of the CPS system to emergencies, this paper analyzes the complexity of the CPS system from the perspective of information theory. The average dynamic complexity of the CPS system is set as a threshold to determine the level of information entropy of the sensor data in a certain period of time. The CPS system selects high information entropy data to send first. The effectiveness is analyzed through experiments.}
}
@article{MEERWIJK2024104582,
title = {Development of a 3-Step theory of suicide ontology to facilitate 3ST factor extraction from clinical progress notes},
journal = {Journal of Biomedical Informatics},
volume = {150},
pages = {104582},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104582},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423003039},
author = {Esther L. Meerwijk and Gabrielle A. Jones and Asqar S. Shotqara and Sofia Reyes and Suzanne R. Tamang and Hyrum S. Eddington and Ruth M. Reeves and Andrea K. Finlay and Alex H.S. Harris},
keywords = {Suicide, Natural language processing, Electronic health records, Controlled vocabulary, Veterans health services, Psychological pain},
abstract = {Objective
Suicide risk prediction algorithms at the Veterans Health Administration (VHA) do not include predictors based on the 3-Step Theory of suicide (3ST), which builds on hopelessness, psychological pain, connectedness, and capacity for suicide. These four factors are not available from structured fields in VHA electronic health records, but they are found in unstructured clinical text. An ontology and controlled vocabulary that maps psychosocial and behavioral terms to these factors does not exist. The objectives of this study were 1) to develop an ontology with a controlled vocabulary of terms that map onto classes that represent the 3ST factors as identified within electronic clinical progress notes, and 2) to determine the accuracy of automated extractions based on terms in the controlled vocabulary.
Methods
A team of four annotators did linguistic annotation of 30,000 clinical progress notes from 231 Veterans in VHA electronic health records who attempted suicide or who died by suicide for terms relating to the 3ST factors. Annotation involved manually assigning a label to words or phrases that indicated presence or absence of the factor (polarity). These words and phrases were entered into a controlled vocabulary that was then used by our computational system to tag 14 million clinical progress notes from Veterans who attempted or died by suicide after 2013. Tagged text was extracted and machine-labelled for presence or absence of the 3ST factors. Accuracy of these machine-labels was determined for 1000 randomly selected extractions for each factor against a ground truth created by our annotators.
Results
Linguistic annotation identified 8486 terms that related to 33 subclasses across the four factors and polarities. Precision of machine-labeled extractions ranged from 0.73 to 1.00 for most factor-polarity combinations, whereas recall was somewhat lower 0.65–0.91.
Conclusion
The ontology that was developed consists of classes that represent each of the four 3ST factors, subclasses, relationships, and terms that map onto those classes which are stored in a controlled vocabulary (https://bioportal.bioontology.org/ontologies/THREE-ST). The use case that we present shows how scores based on clinical notes tagged for terms in the controlled vocabulary capture meaningful change in the 3ST factors during weeks preceding a suicidal event.}
}
@article{DWYER20111021,
title = {An approach to quantitatively measuring collaborative performance in online conversations},
journal = {Computers in Human Behavior},
volume = {27},
number = {2},
pages = {1021-1032},
year = {2011},
note = {Web 2.0 in Travel and Tourism: Empowering and Changing the Role of Travelers},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2010.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0747563210003730},
author = {Paul Dwyer},
keywords = {Collaboration, Cognitive modeling, Collective thinking},
abstract = {Interpersonal dynamics often hinder people from optimizing collaboration. Researchers who monitor the intellectual activity of people as they converse online receive less value when such collaboration is impaired. How can they detect suboptimal collaboration? This study builds on a new metric for measuring collaborative value from the information content of participant contributions to propose a measure of collaborative efficiency, and demonstrates its utility by assessing collaboration around a sample of weblogs. The new collaborative value metric can augment qualitative research by highlighting for deeper investigation conversational themes that triggered elevated collaborative production. Identifying these themes may also define the cognitive box people have built within a collaborative venue. Challenging people to consider fresh ideas by deliberately introducing them into collaborative venues is recommended as the key to overcoming collaborative dysfunction.}
}
@article{CRAIG2018300,
title = {Metaphors of knowing, doing and being: Capturing experience in teaching and teacher education},
journal = {Teaching and Teacher Education},
volume = {69},
pages = {300-311},
year = {2018},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2017.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X17301841},
author = {Cheryl J. Craig},
keywords = {Metaphors, Teachers' experiences, Narrative inquiry, School reform},
abstract = {In this article, Bateson's idea of human beings thinking with metaphors and learning through stories is examined as it played out within accumulated educational research studies. Five storied metaphors illuminating knowing, doing and being are highlighted from five investigations involving different research teams. In the cross-case analysis, the importance of narrative exemplars emerges, along with the significance of metaphors serving as proxies for teachers' experiences. The plotlines of the metaphors, the morals of the metaphors and the truths of the metaphors are also discussed. In the end result, the value of metaphors in surfacing teachers' embedded, embodied knowledge of experience is affirmed as well as the deftness of the narrative inquiry research method in metaphorically capturing pre-service and inservice teachers' storied experiences.}
}
@article{MCDONALD201955,
title = {Cognitive bots and algorithmic humans: toward a shared understanding of social intelligence},
journal = {Current Opinion in Behavioral Sciences},
volume = {29},
pages = {55-62},
year = {2019},
note = {Artificial Intelligence},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2019.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S2352154618301979},
author = {Kelsey R McDonald and John M Pearson},
abstract = {Questions of social behavior are simultaneously among the most fundamental in neuroscience and the most challenging in artificial intelligence. Yet despite decades of work, a unified perspective from the cognitive and computational approaches to the problem has yet to emerge. Recently, however, excitement around the challenges posed to reinforcement learning by multiplayer video games, coupled with the adoption of more complex modeling strategies in social neuroscience, has broadened the interface between the two fields. Here, we review recent progress from both directions, arguing that advances in artificial intelligence provide neuroscientists with valuable tools for modeling social interactions. At the same time, the study of humans as efficient social learners can inform the design of new algorithms for multi-agent systems. We conclude by encouraging a joint approach that incorporates the best of both domains to advance a shared picture of social intelligence.}
}
@incollection{SAHU2022127,
title = {Artificial Intelligence and Machine Learning: New Age Tools for Augmenting Plastic Materials Designing, Processing, and Manufacturing},
editor = {M.S.J. Hashmi},
booktitle = {Encyclopedia of Materials: Plastics and Polymers},
publisher = {Elsevier},
address = {Oxford},
pages = {127-152},
year = {2022},
isbn = {978-0-12-823291-0},
doi = {https://doi.org/10.1016/B978-0-12-820352-1.00108-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128203521001085},
author = {Kisor Kumar Sahu and Shibu Meher and Abhilash M. Menon and M.K. Sridhar and Gangala V. {Harsha Vardhan} and Saurabh Pandey and Ashutosh Kumar and Shreeja Das},
keywords = {Artificial intelligence (AI), Artificial neural network (ANN), Autoencoders, Deep learning, Machine learning (ML), Principal component analysis (PCA)},
abstract = {Plastic and polymers are late entrants in the repository of engineering materials, compared to bronze and iron (early phases of human civilizations are named after them). However, the extent of usage of the former is growing at an exponential rate because of the near-infinite combinatorial possibilities. In fact, there are hardly few engineering disciplines that can potentially offer so large and endless unexploited possibilities. Plastic industries are widespread across the world due to their easy scalability, favorable economics and extremely diverse applications. Plastic manufacturing and recycling are especially important for the economy of a country and provide livelihood for a large population. It is imperative that the current processes in plastic be improved upon by the advantages offered by computational tools and digital technologies. At this stage, we desperately need new age tools that can properly guide the human enterprise of innovation in designing, perfection in processing while maintaining stringent quality requirements in manufacturing. Artificial intelligence (AI) and machine learning (ML) perfectly fits this bill for the new age tools. We are at a very nascent stage of this AI/ML revolution. This article samples some of the pioneering works from the very discreet space of AI/ML applications in the field of plastic and polymer designing, processing and manufacturing and attempts to tie them up in a cohesive narrative. For the sake of completeness, applications of AI/ML for limiting the adverse environmental impact and future outlook have also been covered.}
}
@incollection{NOVOTNY1996149,
title = {Computational Biochemistry of Antibodies and T-Cell Receptors},
editor = {Frederic M. Richards and David E. Eisenberg and Peter S. Kim},
series = {Advances in Protein Chemistry},
publisher = {Academic Press},
volume = {49},
pages = {149-260},
year = {1996},
booktitle = {Antigen Binding Molecules: Antibodies and T-cell Receptors},
issn = {0065-3233},
doi = {https://doi.org/10.1016/S0065-3233(08)60490-8},
url = {https://www.sciencedirect.com/science/article/pii/S0065323308604908},
author = {Jiri Novotny and Jürgen Bajorath}
}
@article{LIU2014330,
title = {Large scale two sample multinomial inferences and its applications in genome-wide association studies},
journal = {International Journal of Approximate Reasoning},
volume = {55},
number = {1, Part 3},
pages = {330-340},
year = {2014},
note = {Theory and applications of belief functions – Belief 2012},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2013.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X13000881},
author = {Chuanhai Liu and Jun Xie},
keywords = {Belief functions, Inference model},
abstract = {Statistical analysis of multinomial counts with a large number K of categories and a small number n of sample size is challenging to both frequentist and Bayesian methods and requires thinking about statistical inference at a very fundamental level. Following the framework of Dempster–Shafer theory of belief functions, a probabilistic inferential model is proposed for this “large K and small n” problem. The inferential model produces a probability triplet (p,q,r) for an assertion conditional on observed data. The probabilities p and q are for and against the truth of the assertion, whereas r=1−p−q is the remaining probability called the probability of “donʼt know”. The new inference method is applied in a genome-wide association study with very high dimensional count data, to identify association between genetic variants to the disease Rheumatoid Arthritis.}
}
@incollection{CHORAFAS200760,
title = {4 - Stress analysis and its tools},
editor = {Dimitris N. Chorafas},
booktitle = {Stress Testing for Risk Control Under Basel II},
publisher = {Butterworth-Heinemann},
address = {Oxford},
pages = {60-79},
year = {2007},
isbn = {978-0-7506-8305-0},
doi = {https://doi.org/10.1016/B978-075068305-0.50005-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780750683050500051},
author = {Dimitris N. Chorafas},
abstract = {Publisher Summary
This chapter explains the need for stress testing to take a scientific approach as an advanced analytical methodology for commendable results. The scientific method of investigation is the only basis for conducting tests and experiments. The chapter examines relatively novel approaches to surveys targeting a qualitative evaluation by experts, such as the Delphi method. The chapter discusses the contributions of the scientific method and financial technology to analytical thinking and testing. The characteristics of a sound methodology are discussed and the fundamentals of stress analysis under normal conditions or under stress are described. The chapter also discusses case studies with scenario analysis and talks about stress evaluation through sensitivity analysis and about the fundamentals of statistical analysis.}
}
@article{ZHU2023116444,
title = {A super-real-time three-dimension computing method of digital twins in space nuclear power},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {417},
pages = {116444},
year = {2023},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2023.116444},
url = {https://www.sciencedirect.com/science/article/pii/S0045782523005686},
author = {Enping Zhu and Tao Li and Jinbiao Xiong and Xiang Chai and Tengfei Zhang and Xiaojing Liu},
keywords = {Digital twin, GPU and multi-core CPU, Machine learning, Super-real-time, Space nuclear reactor},
abstract = {Digital twins (DTs) have attracted widespread attention in academia and industry in recent years. It can accurately reflect the physical world in real-time, enabling online monitoring, control, and prediction operations. Their foundation is super-real-time computing and high data representation capabilities. However, current DTs do not achieve 3D super-real-time computing. This study proposes a novel 3D computational method for solving fluid–solid coupling problems in a super-real-time. The method is based on a mixed solution framework that combines traditional numerical methods with deep learning operators. Specifically, the method employs multi-core CPU parallel acceleration to solve the solid equations while leveraging the computing power of GPU to solve the fluid equations. The fluid–solid coupling is achieved through information exchange between the GPU and the multi-core CPU. In addition, the proposed method introduces a new deep learning operator framework based on the DeepONET. The framework is accompanied by a database structure that facilitates model training and validation and a loss function that guides the training. The space nuclear reactor, an improved TOPAZ-II system, was selected to demonstrate its feasibility. Four non-training transient conditions were simulated to test the generalization performance. The results show that the proposed method achieves an average error between the calculated results and reference values below 2.5%, with the average error of thermodynamic parameters below 1.5%. The average deviation between system parameter peak values during the transient process and the reference value was less than 5 s. The result meets the acceptable error level and satisfies the super-real-time requirements with a time acceleration ratio of approximately 1.17, which is 60 times faster than traditional numerical methods. The results demonstrate the accuracy and efficiency of the proposed method for DT.}
}
@article{GUTIERREZORTIZ2022100164,
title = {Biofuel production from supercritical water gasification of sustainable biomass},
journal = {Energy Conversion and Management: X},
volume = {14},
pages = {100164},
year = {2022},
issn = {2590-1745},
doi = {https://doi.org/10.1016/j.ecmx.2021.100164},
url = {https://www.sciencedirect.com/science/article/pii/S2590174521000891},
author = {F.J. {Gutiérrez Ortiz}},
keywords = {Supercritical water, Gasification, Biofuel, Hydrogen, Sustainability, Process simulation},
abstract = {A review of biofuel production from supercritical water gasification (SCWG) of sustainable biomass has been performed, mainly organic waste, following a critical thinking in this field of knowledge. Thus, sub- and super- critical water properties and hydrothermal processing are briefly commented on. Then, the feedstocks usable in SCWG are fully reviewed and a brief description of the studies on the kinetics and mechanisms of reactions is carried out. Next, thermodynamic and process simulation are discussed, aimed at producing liquid and gas biofuels. After that, a brief comment on the viability of SCWG processes to produce biofuels is provided based on techno-economic and lifecycle assessments. Finally, some remarks on where we are and where we should go are given in order to advance this technology towards its maturity. This review explains some misleading concepts applied to SCWG processes, provides a brief but comprehensive overview of the technology focused on producing biofuels in a sustainable way, allows a better understanding of the SCWG of biomass for biofuel production, and proposes a series of improvements to be made and examined in the future research.}
}
@article{BARTON201242,
title = {Looking for the future in the past: Long-term change in socioecological systems},
journal = {Ecological Modelling},
volume = {241},
pages = {42-53},
year = {2012},
note = {Modeling Across Millennia: Interdisciplinary Paths to Ancient socio-ecological Systems},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2012.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S0304380012000786},
author = {C. Michael Barton and Isaac I.T. Ullah and Sean M. Bergin and Helena Mitasova and Hessam Sarjoughian},
keywords = {Socio-ecological systems, Coupled modeling, Agent-based modeling, Surface process modeling, Simulation, Prehistoric Mediterranean, Archaeology, Agricultural land-use},
abstract = {The archaeological record has been described as a key to the long-term consequences of human action that can help guide our decisions today. Yet the sparse and incomplete nature of this record often makes it impossible to inferentially reconstruct past societies in sufficient detail for them to serve as more than very general cautionary tales of coupled socio-ecological systems. However, when formal and computational modeling is used to experimentally simulate human socioecological dynamics, the empirical archaeological record can be used to validate and improve dynamic models of long term change. In this way, knowledge generated by archaeology can play a unique and valuable role in developing the tools to make more informed decisions that will shape our future. The Mediterranean Landscape Dynamics project offers an example of using the past to develop and test computational models of interactions between land-use and landscape evolution that ultimately may help guide decision-making.}
}
@article{CLEGG2024101007,
title = {Artificial intelligence and management education: A conceptualization of human-machine interaction},
journal = {The International Journal of Management Education},
volume = {22},
number = {3},
pages = {101007},
year = {2024},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2024.101007},
url = {https://www.sciencedirect.com/science/article/pii/S1472811724000788},
author = {Stewart Clegg and Soumodip Sarkar},
keywords = {AI, ChatGPT, Management education, Archetypes},
abstract = {The increasing use of Advanced Natural Language Processing (ANLP) models, particularly ChatGPT-4, presents opportunities and challenges to management education and research. These models can enhance the style, creativity, and analytical power of research papers, potentially shifting human scholars' roles from creators to ‘prompters’. If machines can perform educational and research tasks more effectively the role of human educators becomes a salient question in a world in which ANLP models offer clear, coherent, and polished insights, the use of which has potentially paradoxical possibilities. From one perspective, a new type of high-quality scholarship and education characterized by strong human involvement that synergistically leverages ANLP models' analytical capabilities, enabling human scholars to probe complex phenomena and make management research truly meaningful and impactful for broader audiences, is possible. We explore these questions through an ‘ideal type’ conceptualization of the possible relations between AI and management education and research.}
}
@article{MOSS2013611,
title = {Senior Academic Physicians and Retirement Considerations},
journal = {Progress in Cardiovascular Diseases},
volume = {55},
number = {6},
pages = {611-615},
year = {2013},
note = {Symposium on Psychosocial Factors in Cardiovascular Disease},
issn = {0033-0620},
doi = {https://doi.org/10.1016/j.pcad.2013.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S003306201300056X},
author = {Arthur J. Moss and Henry Greenberg and Edward M. Dwyer and Helmut Klein and Daniel Ryan and Charles Francis and Frank Marcus and Shirley Eberly and Jesaia Benhorin and Monty Bodenheimer and Mary Brown and Robert Case and John Gillespie and Robert Goldstein and Mark Haigney and Ronald Krone and Edgar Lichstein and Emanuela Locati and David Oakes and Poul Erik Bloch Thomsen and Wojciech Zareba},
keywords = {Academic physicians, Retirement issues, Retirement options},
abstract = {An increasing number of academic senior physicians are approaching their potential retirement in good health with accumulated clinical and research experience that can be a valuable asset to an academic institution. Considering the need to let the next generation ascend to leadership roles, when and how should a medical career be brought to a close? We explore the roles for academic medical faculty as they move into their senior years and approach various retirement options. The individual and institutional considerations require a frank dialogue among the interested parties to optimize the benefits while minimizing the risks for both. In the United States there is no fixed age for retirement as there is in Europe, but European physicians are initiating changes. What is certain is that careful planning, innovative thinking, and the incorporation of new patterns of medical practice are all part of this complex transition and timing of senior academic physicians into retirement.}
}
@article{CHEN2022105882,
title = {Neural connectome features of procrastination: Current progress and future direction},
journal = {Brain and Cognition},
volume = {161},
pages = {105882},
year = {2022},
issn = {0278-2626},
doi = {https://doi.org/10.1016/j.bandc.2022.105882},
url = {https://www.sciencedirect.com/science/article/pii/S0278262622000409},
author = {Zhiyi Chen and Tingyong Feng},
keywords = {Procrastination, Neural connectome, Self-control network, DLPFC},
abstract = {Procrastination refers to an irrationally delay for intended courses of action despite of anticipating a negative consequence due to this delay. Previous studies tried to reveal the neural substrates of procrastination in terms of connectome-based biomarkers. Based on this, we proposed a unified triple brain network model for procrastination and pinpointed out what challenges we are facing in understanding neural mechanism of procrastination. Specifically, based on neuroanatomical features, the unified triple brain network model proposed that connectome-based underpinning of procrastination could be ascribed to the abnormalities of self-control network (i.e., dorsolateral prefrontal cortex, DLPFC), emotion-regulation network (i.e., orbital frontal cortex, OFC), and episodic prospection network (i.e., para-hippocampus cortex, PHC). Moreover, based on the brain functional features, procrastination had been attributed to disruptive neural circuits on FPN (frontoparietal network)-SCN (subcortical network) and FPN-SAN (salience network), which led us to hypothesize the crucial roles of interplay between these networks on procrastination in unified triple brain network model. Despite of these findings, poor interpretability and computational model limited further understanding for procrastination from theoretical and neural perspectives. On balance, the current study provided an overview to show current progress on the connectome-based biomarkers for procrastination, and proposed the integrative neurocognitive model of procrastination.}
}
@article{OBRIEN2021104184,
title = {Misplaced trust: When trust in science fosters belief in pseudoscience and the benefits of critical evaluation},
journal = {Journal of Experimental Social Psychology},
volume = {96},
pages = {104184},
year = {2021},
issn = {0022-1031},
doi = {https://doi.org/10.1016/j.jesp.2021.104184},
url = {https://www.sciencedirect.com/science/article/pii/S0022103121000871},
author = {Thomas C. O'Brien and Ryan Palmer and Dolores Albarracin},
keywords = {Misinformation, Trust in science, Critical thinking, Methodological literacy},
abstract = {At a time when pseudoscience threatens the survival of communities, understanding this vulnerability, and how to reduce it, is paramount. Four preregistered experiments (N = 532, N = 472, N = 605, N = 382) with online U.S. samples introduced false claims concerning a (fictional) virus created as a bioweapon, mirroring conspiracy theories about COVID-19, and carcinogenic effects of GMOs (Genetically Modified Organisms). We identify two critical determinants of vulnerability to pseudoscience. First, participants who trust science are more likely to believe and disseminate false claims that contain scientific references than false claims that do not. Second, reminding participants of the value of critical evaluation reduces belief in false claims, whereas reminders of the value of trusting science do not. We conclude that trust in science, although desirable in many ways, makes people vulnerable to pseudoscience. These findings have implications for science broadly and the application of psychological science to curbing misinformation during the COVID-19 pandemic.}
}
@article{ABONIZIO2023105134,
title = {How people interact with a chatbot against disinformation and fake news in COVID-19 in Brazil: The CoronaAI case},
journal = {International Journal of Medical Informatics},
volume = {177},
pages = {105134},
year = {2023},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2023.105134},
url = {https://www.sciencedirect.com/science/article/pii/S1386505623001521},
author = {Hugo Queiroz Abonizio and Ana Paula Ayub da Costa Barbon and Renne Rodrigues and Mayara Santos and Vicente Martínez-Vizcaíno and Arthur Eumann Mesas and Sylvio {Barbon Junior}},
keywords = {Coronavirus, Data Mining, Natural Language Processing, Brazil},
abstract = {Background
The search for valid information was one of the main challenges encountered during the COVID-19 pandemic, which resulted in the development of several online alternatives.
Objectives
To describe the development of a computational solution to interact with users of different levels of digital literacy on topics related to COVID-19 and to map the correlations between user behavior and events and news that occurred throughout the pandemic.
Method
CoronaAI, a chatbot based on Google's Dialogflow technology, was developed at a public university in Brazil and made available on WhatsApp. The dataset with users’ interactions with the chatbot comprises approximately 7,000 hits recorded throughout eleven months of CoronaAI usage.
Results
CoronaAI was widely accessed by users in search of valuable and updated information on COVID-19, including checking the veracity of possible fake news about the spread of cases, deaths, symptoms, tests and protocols, among others. The mapping of users' behavior revealed that as the number of cases and deaths increased and as COVID-19 became closer, users showed a greater need for information applicable to self-care compared to following the statistical data. In addition, they showed that the constant updating of this technology may contribute to public health by enhancing general information on the pandemic and at the individual level by clarifying specific doubts about COVID-19.
Conclusion
Our findings reinforce the potential usefulness of chatbot technology to resolve a wide spectrum of citizens' doubts about COVID-19, acting as a cost-effective tool against the parallel pandemic of misinformation and fake news.}
}
@article{SCHWAB2018500,
title = {A Robust Fault Detection Method using a Zonotopic Kaucher Set-membership Approach},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {24},
pages = {500-507},
year = {2018},
note = {10th IFAC Symposium on Fault Detection, Supervision and Safety for Technical Processes SAFEPROCESS 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.09.623},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318323358},
author = {Stefan Schwab and Vicenç Puig and Soeren Hohmann},
keywords = {Robust fault detection, set-membership approach, Kaucher arithmetic},
abstract = {This paper presents a robust fault detection method using a zonotopic Kaucher set-membership method. The fault detection approach is based on checking the consistency between the model and the data. Consistency is given if there is an intersection between the feasible parameter set and the nominal parameter set. To allow efficient computation the feasible set is approximated by a zonotope. Due to the usage of Kaucher interval arithmetic the results are mathematically guaranteed. The proposed approach is assessed using an illustrative application based on a well-known four-tank case study. The study shows that it is possible to detect even small errors in a noisy setting.}
}
@article{OXENFELDT197783,
title = {The computation of costs for price decisions},
journal = {Industrial Marketing Management},
volume = {6},
number = {2},
pages = {83-90},
year = {1977},
issn = {0019-8501},
doi = {https://doi.org/10.1016/0019-8501(77)90045-1},
url = {https://www.sciencedirect.com/science/article/pii/0019850177900451},
author = {A.R. Oxenfeldt},
abstract = {Business should compute costs in a particular way of pricing purposes. The correct cost computation varies with its purpose, though most executives still believe that an item has a true cost regardless of why it is computed. Even cost estimates made for price decisions will differ according to the type of price decision that is at issue. One finds important differences depending on whether one is estimating costs for a one-shot bid, for a promotional price offer that is to last for a short period, or for a decision concerning long-term price. Although the same basic principles would apply in all three cases, their application is different. The appropriate concept is that of “decision cost”, a very simple but powerful idea that leads to different cost conclusions than are reached by prevailing costing methods.}
}
@article{GOLDSCHMIDT2023101186,
title = {Editorial: Expanding the frontiers of design: A blessing or a curse?},
journal = {Design Studies},
volume = {86},
pages = {101186},
year = {2023},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2023.101186},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X23000273},
author = {Gabriela Goldschmidt}
}
@article{DECKERT2021104930,
title = {Electrophysiological correlates of conventional metaphor, irony, and literal language processing – An event-related potentials and eLORETA study},
journal = {Brain and Language},
volume = {215},
pages = {104930},
year = {2021},
issn = {0093-934X},
doi = {https://doi.org/10.1016/j.bandl.2021.104930},
url = {https://www.sciencedirect.com/science/article/pii/S0093934X21000249},
author = {Matthias Deckert and Michaela Schmoeger and Max Geist and Sarah Wertgen and Ulrike Willinger},
keywords = {Metaphor, Irony, Literal language, Event-related potentials, eLORETA},
abstract = {Conventional metaphor, irony, and literal language processing were compared. Thirty right-handed participants (21–34 years) performed a sequential-statement ERP-paradigm. A left-frontal Late Anterior Negative Slow Wave (LANSW, 450–1000 ms) was significantly greater for metaphors and by visual tendency greater for irony, compared to literal statements. A centroparietal N400 (300–450 ms) and a centroparietal right-dominant “Late N400” (450–600 ms) were by statistical and visual tendency greater for metaphors. Left PCC and left lingual gyrus activity was significantly higher in metaphors compared to literal statements (eLORETA; 450–1000 ms). A statistical trend indicated higher parahippocampal gyrus activity in metaphors and ironies. N400 results are discussed considering changing processing techniques and a renewed semantic conflict. The Late N400 was associated with the construct of “associativeness”. The LANSW was related to metaphorical mapping, frame-shifting processes, integration of meanings, and memory processes. eLORETA results were discussed considering metaphorical mapping, creation of mental images, conventionality, valence, memory processes, and divergent thinking.}
}
@article{WOLFF20124051,
title = {Constraints in the generation of photonic Wannier functions},
journal = {Physica B: Condensed Matter},
volume = {407},
number = {20},
pages = {4051-4055},
year = {2012},
note = {Proceedings of the conference - Wave Propagation: From Electrons to Photonic Crystals and Metamaterials},
issn = {0921-4526},
doi = {https://doi.org/10.1016/j.physb.2012.03.022},
url = {https://www.sciencedirect.com/science/article/pii/S0921452612002554},
author = {Christian Wolff and Kurt Busch},
keywords = {Photonic Crystals, Wannier functions},
abstract = {We report on the generation of maximally localized photonic Wannier functions under constraints. This allows us to impress certain symmetry properties of the underlying Photonic Crystal onto the Wannier functions. This added flexibility enhances the utility of the Wannier function approach to Photonic Crystal circuits by providing deeper physical insight and making computations more efficient.}
}
@article{CHATTERJEE2024111870,
title = {Investigating the association between symptoms and functional activity in brain regions in schizophrenia: A cross-sectional fmri-based neuroimaging study},
journal = {Psychiatry Research: Neuroimaging},
volume = {344},
pages = {111870},
year = {2024},
issn = {0925-4927},
doi = {https://doi.org/10.1016/j.pscychresns.2024.111870},
url = {https://www.sciencedirect.com/science/article/pii/S0925492724000933},
author = {Indranath Chatterjee and Bisma Hilal},
keywords = {Schizophrenia, Medical imaging, Machine learning, Symptoms study, Brain regions, fMRI},
abstract = {Schizophrenia is a persistent neurological disorder profoundly affecting cognitive, emotional, and behavioral functions, prominently characterized by delusions, hallucinations, disordered speech, and abnormal motor activity. These symptoms often present diagnostic challenges due to their overlap with other forms of psychosis. Therefore, the implementation of automated diagnostic methodologies is imperative. This research leverages Functional Magnetic Resonance Imaging (fMRI), a neuroimaging modality capable of delineating functional activations across diverse brain regions. Furthermore, the utilization of evolving machine learning techniques for fMRI data analysis has significantly progressive. Here, our study stands as a novel attempt, focusing on the comprehensive assessment of both classical and atypical symptoms of schizophrenia. We aim to uncover associated changes in brain functional activity. Our study encompasses two distinct fMRI datasets (1.5T and 3T), each comprising 34 schizophrenia patients for the 1.5T dataset and 25 schizophrenia patients for the 3T dataset, along with an equal number of healthy controls. Machine learning algorithms are applied to assess data subsets, enabling an in-depth evaluation of the current functional condition concerning symptom impact. The identified voxels contribute to determining the brain regions most influenced by each symptom, as quantified by symptom intensity. This rigorous approach has yielded various new findings while maintaining an impressive classification accuracy rate of 97 %. By elucidating variations in activation patterns across multiple brain regions in individuals with schizophrenia, this study contributes to the understanding of functional brain changes associated with the disorder. The insights gained may inform differential clinical interventions and provide a means of assessing symptom severity accurately, offering new avenues for the management of schizophrenia.}
}