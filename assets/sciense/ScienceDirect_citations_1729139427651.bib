@article{CHEN20071153,
title = {Computationally intelligent agents in economics and finance},
journal = {Information Sciences},
volume = {177},
number = {5},
pages = {1153-1168},
year = {2007},
note = {Including: The 3rd International Workshop on Computational Intelligence in Economics and Finance (CIEF’2003)},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2006.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0020025506002301},
author = {Shu-Heng Chen},
keywords = {Computational intelligence, Agent-based computational economics},
abstract = {This paper is an editorial guide for the second special issue on Computational Intelligence in economics and finance, which is a continuation of the special issue of Information Sciences, Vol. 170, No. 1. This second issue appears as a part of the outcome from the 3rd International Workshop on Computational Intelligence in Economics and Finance, which was held in Cary, North Carolina, September 26–30, 2003. This paper offers some main highlights of this event, with a particular emphasis on some of the observed progress made in this research field, and a brief introduction to the papers included in this special issue.}
}
@article{SOMMER2013e201302014,
title = {MEMBRANE PACKING PROBLEMS: A SHORT REVIEW ON COMPUTATIONAL MEMBRANE MODELING METHODS AND TOOLS},
journal = {Computational and Structural Biotechnology Journal},
volume = {5},
number = {6},
pages = {e201302014},
year = {2013},
issn = {2001-0370},
doi = {https://doi.org/10.5936/csbj.201302014},
url = {https://www.sciencedirect.com/science/article/pii/S2001037014600428},
author = {Björn Sommer},
abstract = {The use of model membranes is currently part of the daily workflow for many biochemical and biophysical disciplines. These membranes are used to analyze the behavior of small substances, to simulate transport processes, to study the structure of macromolecules or for illustrative purposes. But, how can these membrane structures be generated? This mini review discusses a number of ways to obtain these structures. First, the problem will be formulated as the Membrane Packing Problem. It will be shown that the theoretical problem of placing proteins and lipids onto a membrane area differ significantly. Thus, two sub-problems will be defined and discussed. Then, different – partly historical – membrane modeling methods will be introduced. And finally, membrane modeling tools will be evaluated which are able to semi-automatically generate these model membranes and thus, drastically accelerate and simplify the membrane generation process. The mini review concludes with advice about which tool is appropriate for which application case.}
}
@article{YUNG2021101566,
title = {A visual approach to interpreting the career of the network metaphor},
journal = {Poetics},
volume = {88},
pages = {101566},
year = {2021},
note = {Measure Mohr Culture},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2021.101566},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X21000498},
author = {Vincent Yung},
keywords = {Metaphor, Network, Formal model, Computational hermeneutics, Text analysis},
abstract = {Metaphor has had a prolific presence in sociology: as a subject of empirical study, as a tool for theorizing, and as ideas that live public lives beyond the ivory tower. Despite all this work, tracing the core dynamics of metaphors over time remains a persistent challenge. To address this problem, I propose a systematic analysis of metaphor anchored around four core questions. What do metaphors describe? How do metaphors change over time? What do metaphors do? And how do conventional metaphors emerge? I address these questions in the context of one persistent and prolific metaphor: the network metaphor. To do so, I offer a formal model of the career of metaphor by drawing on cognitive linguistics and sociological theories of public ideas and professional careers. Specifically, I integrate the Google Books Ngram corpus with computational techniques that leverage co-occurrence to visualize and interpret the career of the network metaphor with respect to four areas: its jurisdiction, temporality, ecology, and conventionality. I leverage the network metaphor as both a revelatory case for evaluating my model and a critical case for deepening our historical understanding of the network. My analysis lends validation to the argument that the network metaphor has achieved the status of a broad category for thinking about contemporary life. However, it also demonstrates that the network metaphor had a lively history prior to the twentieth century. This includes significant changes in how it was used in the first half of the nineteenth century, a career standing in for anatomical structures of the body in the mid-nineteenth century, and a predominant expression in simile form prior to the start of the twentieth century.}
}
@incollection{KALBFLEISCH2010641,
title = {2.33 - Genomics, Bioinformatics, and Computational Biology},
editor = {Charlene A. McQueen},
booktitle = {Comprehensive Toxicology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {641-661},
year = {2010},
isbn = {978-0-08-046884-6},
doi = {https://doi.org/10.1016/B978-0-08-046884-6.00236-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780080468846002360},
author = {T.S. Kalbfleisch and G.A. Rempala and K.S. Ramos},
abstract = {In this chapter, we describe how computational biology can be aided by informatics infrastructure to provide the basis for in silico studies that no longer require the generation of data, and instead facilitate the collection, organization, and analysis of existing datasets that can drive discovery. A new reality is that we are awash in data and tools to analyze these data and one of the most significant challenges is that of enabling the researcher to discover datasets relevant to their work, collect these data, assess its quality, and analyze it. The development of an adequate infrastructure within a researcher’s institution greatly facilitates progress on this front, both in terms of the development of tools and the development of local informatics expertise necessary to complement the domain-specific expertise of the researcher. As an informatics community we often ponder the heterogeneity of tools and resources on a global scale at the expense of the more immediate local problems encountered on a routine basis. Here, we suggest that getting our own houses in order by first employing interoperable solutions that support and facilitate collaboration amongst the complementary disciplines within our own institutions places the informatics community in a better position to address global informatics challenges. This approach can ensure that the solutions implemented employ an architecture and standards that support interoperability. Indeed, this is an organizational and cultural challenge rather than a technological one. Organizational structure and practices are described that provide a comprehensive base of talent capable of creating an environment that supports a sustainable informatics infrastructure, and that can quickly grow as needed to support the specific and rapidly evolving needs unique to that institution.}
}
@article{JOKONYA20141533,
title = {Towards a Big Data Framework for the Prevention and Control of HIV/AIDS, TB and Silicosis in the Mining Industry},
journal = {Procedia Technology},
volume = {16},
pages = {1533-1541},
year = {2014},
note = {CENTERIS 2014 - Conference on ENTERprise Information Systems / ProjMAN 2014 - International Conference on Project MANagement / HCIST 2014 - International Conference on Health and Social Care Information Systems and Technologies},
issn = {2212-0173},
doi = {https://doi.org/10.1016/j.protcy.2014.10.175},
url = {https://www.sciencedirect.com/science/article/pii/S2212017314004022},
author = {Osden Jokonya},
keywords = {Tuberculosis, Big Data, HIV/AIDS, Silicosis, Systems Approach, Viable Systems Model, Organizational Cybernetics, Hard Systems Thinking, Soft Systems Thinking, Emancipatory Systems Thinking, Critical Systems Thinking, Epidemiology},
abstract = {This paper proposes a big data integrated framework to assist with prevention and control of HIV/AIDS, TB and silicosis (HATS) in the mining industry. The linkage between HATS presents a major challenge to the mining industry globally. When the immune system is compromised by HIV/AIDS and silicosis, it makes it easier for tuberculosis to infect the body. In addition, the silica dust which affects the lungs may also cause silicosis and tuberculosis. The objective of this paper is to posit a big data integrated framework to assist in the prevention and control of HATS in the mining industry. Literature was reviewed in order to build a conceptual framework. Although this study is not the first to apply big data in healthcare, to the researcher's knowledge, it is the first to apply big data in understanding the linkage between HATS in the mining industry. The literature review indicates only a few studies using big data in healthcare with no research found on big data and HATS. It therefore makes a contribution to existing body of literature on the control of HATS. The proposed big data framework has the potential of addressing the needs of predictive epidemiology which is important in forecasting and disease control in the mining industry. The paper therefore lays a foundation for the use of viable systems model and big data to address the challenges of HATS in the mining industry. As part of future work, the framework will be validated using sequential explanatory mixed methods case study approach in mining organizations.}
}
@article{LI2024141569,
title = {Programming experiment course for innovative and sustainable education: A case study of Java for Millikan Oil-Drop experiment},
journal = {Journal of Cleaner Production},
volume = {447},
pages = {141569},
year = {2024},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2024.141569},
url = {https://www.sciencedirect.com/science/article/pii/S0959652624010175},
author = {Yizheng Li and Guandong Su and Haocheng Pan and Chengwei Tan and Gongsheng Li},
keywords = {Laboratory instruction, Java object-oriented programming, Innovative and sustainable education, Student-centered learning},
abstract = {A teaching approach of programming experiment courses with a new education process is proposed in this study to improve the sustainability of engineering education and extend the accessibility of lab experiments across experiment courses, with the goal of encouraging innovative and scientific thinking among students. The Millikan Oil-Drop experiment combined with Java object-oriented programming is demonstrated as a case study to validate the feasibility and advantages of this teaching approach. The new education process of the experiment course is designed based on Jean Piaget's cognitive development theory, which has high practical potential for popularization among tertiary institutions without additional cost. Additionally, this work discussed the relevance of the general criterion of the Accreditation Board for Engineering and Technology on student outcomes to educational accreditation, further indicating that introducing programming into experiment education can improve students' all-round ability and strengthen the triangular relationship among the three main subjects in tertiary education, namely, students, faculty, and higher educational institutions. This study may serve as an educational guide for teachers and tertiary institutions to pursue innovative and sustainable education.}
}
@article{ZAHRAH2024100481,
title = {Unmasking hate in the pandemic: A cross-platform study of the COVID-19 infodemic},
journal = {Big Data Research},
volume = {37},
pages = {100481},
year = {2024},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2024.100481},
url = {https://www.sciencedirect.com/science/article/pii/S2214579624000558},
author = {Fatima Zahrah and Jason R.C. Nurse and Michael Goldsmith},
keywords = {Social media analysis, Cross-platform analysis, Online hate, COVID-19},
abstract = {The past few decades have established how digital technologies and platforms have provided an effective medium for spreading hateful content, which has been linked to several catastrophic consequences. Recent academic studies have also highlighted how online hate is a phenomenon that strategically makes use of multiple online platforms. In this article, we seek to advance the current research landscape by harnessing a cross-platform approach to computationally analyse content relating to the 2020 COVID-19 pandemic. More specifically, we analyse content on hate-specific environments from Twitter, Reddit, 4chan and Stormfront. Our findings show how content and posting activity can change across platforms, and how the psychological components of online content can differ depending on the platform being used. Through this, we provide unique insight into the cross-platform behaviours of online hate. We further define several avenues for future research within this field so as to gain a more comprehensive understanding of the global hate ecosystem.}
}
@article{GADALLA2023200201,
title = {Concepts and experiments on psychoanalysis driven computing},
journal = {Intelligent Systems with Applications},
volume = {18},
pages = {200201},
year = {2023},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2023.200201},
url = {https://www.sciencedirect.com/science/article/pii/S2667305323000261},
author = {Minas Gadalla and Sotiris Nikoletseas and José Roberto {de A. Amazonas} and José D.P. Rolim},
keywords = {Lacanian discourses, Psychoanalysis computing, GPT-3},
abstract = {This research investigates the effective incorporation of the human factor and user perception in text-based interactive media. In such contexts, the reliability of user texts is often compromised by behavioural and emotional dimensions. To this end, several attempts have been made in the state of the art, to introduce psychological approaches in such systems, including computational psycholinguistics, personality traits and cognitive psychology methods. In contrast, our method is fundamentally different since we employ a psychoanalysis-based approach; in particular, we use the notion of Lacanian discourse types, to capture and deeply understand real (possibly elusive) characteristics, qualities and contents of texts, and evaluate their reliability. As far as we know, this is the first time computational methods are systematically combined with psychoanalysis. We believe such psychoanalytic framework is fundamentally more effective than standard methods, since it addresses deeper, quite primitive elements of human personality, behaviour and expression which usually escape methods functioning at “higher”, conscious layers. In fact, this research is a first attempt to form a new paradigm of psychoanalysis-driven interactive technologies, with broader impact and diverse applications. To exemplify this generic approach, we apply it to the case-study of fake news detection; we first demonstrate certain limitations of the well-known Myers–Briggs Type Indicator (MBTI) personality type method, and then propose and evaluate our new method of analysing user texts and detecting fake news based on the Lacanian discourses psychoanalytic approach.}
}
@article{NIKZAINAL2024101739,
title = {Prof. Serena Nik-Zainal},
journal = {Cell Reports Medicine},
volume = {5},
number = {9},
pages = {101739},
year = {2024},
issn = {2666-3791},
doi = {https://doi.org/10.1016/j.xcrm.2024.101739},
url = {https://www.sciencedirect.com/science/article/pii/S2666379124004695},
author = {Serena Nik-Zainal},
abstract = {Serena Nik-Zainal, MD, PhD, is professor of genomic medicine and bioinformatics and an honorary consultant in clinical genetics at the University of Cambridge. Prof. Nik-Zainal has dedicated her career to studying the physiology of cancer mutagenesis via a combination of computational and experimental work, as well as validation with clinical data. Among the many awards she has earned for her work, she has recently received the 2024 ESMO Award for Translational Research, for the research in the field of mutational signatures and her efforts in translating their use into clinics.}
}
@article{DREANY20181,
title = {Safety engineering of computational cognitive architectures within safety-critical systems},
journal = {Safety Science},
volume = {103},
pages = {1-11},
year = {2018},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2017.10.020},
url = {https://www.sciencedirect.com/science/article/pii/S0925753517301947},
author = {Harry H. Dreany and Robert Roncace and Paul Young},
keywords = {Safety engineering, Artificial intelligence, Cognitive architecture, Decision support model, Intelligent technologies},
abstract = {This paper presents the integration of a cognitive architecture with an intelligent decision support model (IDSM) that is embedded into an autonomous non-deterministic safety critical system. The IDSM will integrate multi-criteria decision making via intelligent technologies like expert systems, fuzzy logic, machine learning and genetic algorithms. Cognitive technology is currently simulated in safety–critical systems to highlight variables of interest, interface with intelligent technologies, and provide an environment that improves a system’s cognitive performance. In this study, the IDSM is being applied to an actual safety–critical system, an unmanned surface vehicle (USV) with embedded artificial intelligence (AI) software. The USV’s safety performance is being researched in a simulated and a real world nautical based environment. The objective is to build a dynamically changing model to evaluate a cognitive architecture’s ability to ensure safe performance of an intelligent safety–critical system. The IDSM does this by finding a set of key safety performance parameters that can be critiqued via safety measurements, mechanisms and methodologies. The uniqueness of this research will be on bounding the decision making associated with the cognitive architecture’s key safety parameters (KSP). Other real-time applications that could benefit from advancing the safety of cognitive technologies are unmanned platforms, transportation technologies, and service robotics. The results will provide cognitive science researchers a reference for safety engineering artificially intelligent safety–critical systems.}
}
@article{KENDON2008187,
title = {Optimal computation with non-unitary quantum walks},
journal = {Theoretical Computer Science},
volume = {394},
number = {3},
pages = {187-196},
year = {2008},
note = {From Gödel to Einstein: Computability between Logic and Physics},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2007.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S0304397507008791},
author = {Viv Kendon and Olivier Maloyer},
keywords = {Quantum computing, Quantum walks, Quantum algorithms},
abstract = {Quantum versions of random walks on the line and the cycle show a quadratic improvement over classical random walks in their spreading rates and mixing times, respectively. Non-unitary quantum walks can provide a useful optimisation of these properties, producing a more uniform distribution on the line, and faster mixing times on the cycle. We investigate the interplay between quantum and random dynamics by comparing the resources required, and examining numerically how the level of quantum correlations varies during the walk. We show numerically that the optimal non-unitary quantum walk proceeds such that the quantum correlations are nearly all removed at the point of the final measurement. This requires only O(logT) random bits for a quantum walk of T steps.}
}
@article{GROSS199653,
title = {The Electronic Cocktail Napkin—a computational environment for working with design diagrams},
journal = {Design Studies},
volume = {17},
number = {1},
pages = {53-69},
year = {1996},
issn = {0142-694X},
doi = {https://doi.org/10.1016/0142-694X(95)00006-D},
url = {https://www.sciencedirect.com/science/article/pii/0142694X9500006D},
author = {Mark D. Gross},
keywords = {conceptual design, computer-based environment, diagrams, sketching},
abstract = {The Electronic Cocktail Napkin is an experimental computer-based environment for sketching and diagramming in conceptual design. The project's goal is to develop a computational drawing environment to support conceptual designing in a way that leads smoothly from diagrams to more formal and structured representations of schematic design. With computational representations for conceptual designs, computer-supported editing, critquing, analysis, and simulation can be employed earlier in the design process, where it can have a greater impact on outcomes. The paper describes the Electronic Cocktail Napkin program-its recognition and parsing of diagrams and management of spatial constraints, its drawing environment, and two experimental query-by-diagram schemes for retrieving information from architectural databases.}
}
@incollection{KRUSHINSKY1981171,
title = {STRUCTURAL ANALYSIS OF NON-VERBAL THINKING IN MAN},
editor = {N.P. BECHTEREVA},
booktitle = {Psychophysiology},
publisher = {Pergamon},
pages = {171-178},
year = {1981},
isbn = {978-0-08-025930-7},
doi = {https://doi.org/10.1016/B978-0-08-025930-7.50018-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780080259307500181},
author = {L.V. Krushinsky and N.P. Popova},
abstract = {Publisher Summary
This chapter discusses the characteristics of nonverbal thinking in man and elementary reasoning activity in animals and children. Verbal thinking is associated with the formation of a structural–functional organization of a pattern-code of verbal signals. Nonverbal thinking is associated with comprehension of the principles that make up the structural organization of environment. This type of thinking occurs in humans and other vertebrates. The chapter also discusses physiological and phenotypic aspects of nonverbal thinking. Both types of thinking have their own characteristics, which are related to the chemistry of the brain, and both reflect the nature of man. The main characteristic of nonverbal thinking is the success in performing the task at the first attempt. Repeated presentations of the task often lead to refusals to resolve it, the appearance of fear of the experimental conditions, and numerous signs of an overexcited state. A study described in the chapter revealed age to be a leading factor in determining the accurate understanding of object movement within the spatial–temporal coordinate system when the exact whereabouts of the object is unknown.}
}
@article{SANTOS2023102749,
title = {Policy entrepreneurs in the global education complex: The case of Finnish education experts working in international organisations},
journal = {International Journal of Educational Development},
volume = {98},
pages = {102749},
year = {2023},
issn = {0738-0593},
doi = {https://doi.org/10.1016/j.ijedudev.2023.102749},
url = {https://www.sciencedirect.com/science/article/pii/S0738059323000263},
author = {Íris Santos and Elias Pekkola},
keywords = {Development cooperation for education, Influence, Finnish education experts, Complexity, Multiple streams approach},
abstract = {This article analyses the perceived role of Finnish education experts working in development cooperation for education. We interviewed 31 education experts working in international organisations representing Finland. A theoretically pluralist approach is utilised combining complexity thinking with a multiple streams approach. The analysis demonstrates that the context of educational development cooperation is ambiguous and complex. Influencing policymaking is a strategic, non-linear task which takes time, resources, and personal skills. Policy entrepreneurs need to understand the dynamics of development cooperation, identify actors that trust them, and recognise when policy windows are likely to open.}
}
@article{THOMSON1994137,
title = {Whither computational materials science? Some thoughts from the mechanical properties front},
journal = {Computational Materials Science},
volume = {2},
number = {1},
pages = {137-142},
year = {1994},
issn = {0927-0256},
doi = {https://doi.org/10.1016/0927-0256(94)90056-6},
url = {https://www.sciencedirect.com/science/article/pii/0927025694900566},
author = {Robb Thomson},
abstract = {A claim is made that analysis will remain important and become a useful ally in helping computational materials science live up to its ultimate potential. Examples are given in the mechanical properties area where numerical simulations have been able to parameterize and mark out areas of validity for elasticity theory. The important role of developing asymptotic paths from one level or category of theory to another is commented on.}
}
@article{ALBUS20101519,
title = {A model of computation and representation in the brain},
journal = {Information Sciences},
volume = {180},
number = {9},
pages = {1519-1554},
year = {2010},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2009.12.031},
url = {https://www.sciencedirect.com/science/article/pii/S0020025510000095},
author = {James S. Albus},
keywords = {Brain modeling, Cognitive modeling, Human neocortex, Image processing, Knowledge representation, Perception, Reverse engineering the brain, Segmentation, Signals to symbols},
abstract = {The brain is first and foremost a control system that is capable of building an internal representation of the external world, and using this representation to make decisions, set goals and priorities, formulate plans, and control behavior with intent to achieve its goals. The internal representation is distributed throughout the brain in two forms: (1) firmware embedded in synaptic connections and axon-dendrite circuitry, and (2) dynamic state-variables encoded in the firing rates of neurons in computational loops in the spinal cord, midbrain, subcortical nuclei, and arrays of cortical columns. It assumes that clusters and arrays of neurons are capable of computing logical predicates, smooth arithmetic functions, and matrix transformations over a space defined by large input vectors and arrays. Feedback from output to input of these neural computational units enable them to function as finite-state-automata (fsa), Markov decision processes (MDP), or delay lines in processing signals and generating strings and grammars. Thus, clusters of neurons are capable of parsing and generating language, decomposing tasks, generating plans, and executing scripts. In the cortex, neurons are arranged in arrays of cortical columns that interact in tight loops with their underlying subcortical nuclei. It is hypothesized that these circuits compute sophisticated mathematical and logical functions that maintain and use complex abstract data structures. It is proposed that cortical hypercolumns together with their underlying thalamic nuclei can be modeled as a cortical computational unit (CCU) consisting of a frame-like data structure (containing attributes and pointers) plus the computational processes and mechanisms required to maintain it and use it for perception cognition, and sensory-motor behavior. In sensory processing areas of the brain, CCU processes enable focus of attention, segmentation, grouping, and classification. Pointers stored in CCU frames define relationships that link pixels and signals to objects and events in situations and episodes. CCU frame pointers also link objects and events to class prototypes and overlay them with meaning and emotional values. In behavior generating areas of the brain, CCU processes make decisions, set goals and priorities, generate plans, and control behavior. In general, CCU pointers are used to define rules, grammars, procedures, plans, and behaviors. CCU pointers also define abstract data structures analogous to lists, frames, objects, classes, rules, plans, and semantic nets. It is suggested that it may be possible to reverse engineer the human brain at the CCU level of fidelity using next-generation massively parallel computer hardware and software.}
}
@incollection{PRATT198497,
title = {A Theoretical Framework for Thinking About Depiction},
editor = {W. Ray Crozier and Antlony J. Chapman},
series = {Advances in Psychology},
publisher = {North-Holland},
volume = {19},
pages = {97-109},
year = {1984},
booktitle = {Cognitive Processes in the Perception of Art},
issn = {0166-4115},
doi = {https://doi.org/10.1016/S0166-4115(08)62347-X},
url = {https://www.sciencedirect.com/science/article/pii/S016641150862347X},
author = {Francis Pratt},
abstract = {Publisher Summary
This chapter provides a chronological account of the steps which describes the present theoretical framework for thinking about depiction. The experimental results provides good evidence for the following assertions: (1) knowledge is a necessary part of all acts of depiction done by people of all ages and of all levels of skill, (2) knowledge is a main determinant of looking strategies, (3) the role of knowledge in the organization of looking strategies is one of determining the level of description to be used as the basis of analytic processes, and (4) "good" copying performance (i.e., "accurate" in terms of scene-specific and view-specific relations) can be equated with level of description accessed. The chapter emphasizes on: (1) each descending level of description implies an increasing disintegration of the analytic task. (2) Analysis for depiction is concerned with variance. It is concerned with relations that change according to viewing circumstances. In effect, they can be considered as novel relations. There is much evidence that people's ability to maintain "novel" relations in memory is severely limited. (3) The model consisting of a group of straight lines is only capable of being analyzed at the lowest levels of description.}
}
@article{WEYDMANN2025111173,
title = {Disentangling negative reinforcement, working memory, and deductive reasoning deficits in elevated BMI},
journal = {Progress in Neuro-Psychopharmacology and Biological Psychiatry},
volume = {136},
pages = {111173},
year = {2025},
issn = {0278-5846},
doi = {https://doi.org/10.1016/j.pnpbp.2024.111173},
url = {https://www.sciencedirect.com/science/article/pii/S0278584624002410},
author = {Gibson Weydmann and Igor Palmieri and Reinaldo A.G. Simões and Samara Buchmann and Eduardo Schmidt and Paulina Alves and Lisiane Bizarro},
keywords = {Overweight, Reinforcement Learning, Working Memory, Computational Modelling},
abstract = {Neuropsychological data suggest that being overweight or obese is associated with a tendency to perseverate behavior despite negative feedback. This deficit might be observed due to other cognitive factors, such as working memory (WM) deficits or decreased ability to deduce model-based strategies when learning by trial-and-error. In the present study, a group of subjects with overweight or obesity (Ow/Ob, n = 30) was compared to normal-weight individuals (n = 42) in a modified Reinforcement Learning (RL) task. The task was designed to control WM effects on learning by manipulating cognitive load and to foster model-based learning via deductive reasoning. Computational modelling and analysis were conducted to isolate parameters related to RL mechanisms, WM use, and model-based learning (deduction parameter). Results showed that subjects with Ow/Ob had a higher number of perseverative errors and used a weaker deduction mechanism in their performance than control individuals, indicating impairments in negative reinforcement and model-based learning, whereas WM impairments were not responsible for deficits in RL. The present data suggests that obesity is associated with impairments in negative reinforcement and model-based learning.}
}
@incollection{MACWHINNEY2008229,
title = {CHAPTER 22 - Neurolinguistic Computational Models},
editor = {BRIGITTE STEMMER and HARRY A. WHITAKER},
booktitle = {Handbook of the Neuroscience of Language},
publisher = {Elsevier},
address = {San Diego},
pages = {229-236},
year = {2008},
isbn = {978-0-08-045352-1},
doi = {https://doi.org/10.1016/B978-0-08-045352-1.00022-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780080453521000227},
author = {BRIAN MACWHINNEY and PING LI}
}
@article{LI2018359,
title = {Experimental and computational Fluid Dynamics study of separation gap effect on gas explosion mitigation for methane storage tanks},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {55},
pages = {359-380},
year = {2018},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2018.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950423018304224},
author = {Jingde Li and Hong Hao and Yanchao Shi and Qin Fang and Zhan Li and Li Chen},
keywords = {Separation gap, Safety gap, External pressure, Vented gas explosion, CFD, FLACS},
abstract = {This paper presented both experimental and numerical assessments of separation gap effect on vented explosion pressure in and around the area of a tank group. A series of vented gas explosion layouts with different separation gaps between tanks were experimentally investigated. In order to qualitatively determine the relationship between the separation gap distance and explosion pressure, intensive computational Fluid Dynamics (CFD) simulations, verified with testing data, were conducted. Good agreement between CFD simulation results and experimental data was achieved. By using CFD simulation, more gas explosion cases were included to consider different gas cloud coverage scenarios. Separation gap effects on internal and external pressures at various locations were investigated.}
}
@article{BAILEY2006793,
title = {Clover: Connecting technology and character education using personally-constructed animated vignettes},
journal = {Interacting with Computers},
volume = {18},
number = {4},
pages = {793-819},
year = {2006},
note = {Special Theme Papers from Special Editorial Board Members (contains Regular Papers)},
issn = {0953-5438},
doi = {https://doi.org/10.1016/j.intcom.2005.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S0953543805001153},
author = {Brian P. Bailey and Sharon Y. Tettegah and Terry J. Bradley},
keywords = {Animation, Character education, Multimedia, Narratives, Vignettes},
abstract = {Schools are increasingly integrating character education to facilitate improved moral thinking and pro social behavior among students. An effective method for delivering character education is problem solving moral and social situations represented visually as animated vignettes. However, schools are rarely able to use animated vignettes since existing tools do not allow them to be easily created and having them created externally is overly expensive. In this paper, we describe the design, use, and evaluation of a computational tool that enables students to construct their own animated vignettes. By building, sharing, and responding to vignettes, students become engaged in problem solving moral and social situations. Evaluations showed that users are able to build meaningful vignettes, our tool is easy to learn and fun to use, and our tool's multimedia features are often used and well-liked. Educators can download and use our tool while researchers can draw upon our design rationale and lessons learned when building similar tools.}
}
@article{GANO201556,
title = {Starting with Universe: Buckminster Fuller's Design Science Now},
journal = {Futures},
volume = {70},
pages = {56-64},
year = {2015},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2014.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S0016328714002055},
author = {Gretchen Gano},
keywords = {Comprehensiveness, Big data, Design science, Buckminster Fuller, Worldviews Network},
abstract = {Increasingly, decision makers seek to harness “big data” to guide choices in management and policy settings as well as in professions that manufacture, build, and innovate. Scholars examining this trend tend to diagnose it at once as techno positivist in its insistence on design yoked to quantifiable variables and computational modeling and, alternatively, as an imperative integral to realizing ecologically sustainable innovation. This article investigates this tension. It reflects on the role of futurists, designers, architects, urban planners, social scientists, and artists in interpreting and utilizing comprehensiveness as a design frame. Among nine experimental foresight workshops at the inaugural Emerge conference at Arizona State University, many focused on producing physical objects or media, one modeled and expanded upon a method pioneered by architect and polymath R. Buckminster Fuller. At a time when many of the capabilities to realize Fuller's specifications for big data have matured, I investigate whether comprehensive design as framed by Fuller's method shows promise as a trend enabling ecologically sustainable innovations. A historical look at Fuller's Design Science and the reflection on it in the Emerge workshop marks an opportunity to highlight and interpret the resurgence of comprehensive thinking in design while navigating the contradictions this orientation engenders.}
}
@article{WOODRUFF1994463,
title = {Some computational challenges of developing efficient parallel algorithms for data-dependent computations in thermal-hydraulics supercomputer applications},
journal = {Nuclear Engineering and Design},
volume = {146},
number = {1},
pages = {463-471},
year = {1994},
issn = {0029-5493},
doi = {https://doi.org/10.1016/0029-5493(94)90351-4},
url = {https://www.sciencedirect.com/science/article/pii/0029549394903514},
author = {S.B. Woodruff},
abstract = {The Transient Reactor Analysis Code (TRAC), which features a two-fluid treatment of thermal-hydraulics, is designed to model transients in water reactors and related facilities. One of the major computational costs associated with TRAC and similar codes is calculating constitutive coefficients. Although the formulations for these coefficients are local, the costs are flow-regime- or data-dependent; i.e., the computations needed for a given spatial node often vary widely as a function of time. Consequently, a fixed, uniform assignment of nodes to parallel processors will result in degraded computational efficiency due to the poor load balancing. A standard method for treating data-dependent models on vector architectures has been to use gather operations (or indirect addressing) to sort the nodes into subsets that (temporarily) share a common computational model. However, this method is not effective on distributed memory data parallel architectures, where indirect addressing involves expensive communication overhead. Another serious problem with this method involves software engineering challenges in the areas of maintainability and extensibility. For example, an implementation that was hand-tuned to achieve good computational efficiency would have to be rewritten whenever the decision tree governing the sorting was modified. Using an example based on the calculation of the wall-to-liquid and wall-to-vapor heat-transfer coefficients for three nonboiling flow regimes, we describe how the use of the Fortran 90 WHERE construct and automatic inlining of functions can be used to ameliorate this problem while improving both efficiency and software engineering. Unfortunately, a general automatic solution to the load-balancing problem associated with data-dependent computations is not yet available for massively parallel architectures. We discuss why developers should either wait for such solutions or consider alternative numerical algorithms, such as a neural network representation, that do not exhibit load-balancing problems.}
}
@article{ALLEN2006428,
title = {Innovations in computational type theory using Nuprl},
journal = {Journal of Applied Logic},
volume = {4},
number = {4},
pages = {428-469},
year = {2006},
note = {Towards Computer Aided Mathematics},
issn = {1570-8683},
doi = {https://doi.org/10.1016/j.jal.2005.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S1570868305000704},
author = {S.F. Allen and M. Bickford and R.L. Constable and R. Eaton and C. Kreitz and L. Lorigo and E. Moran},
keywords = {Martin-Löf type theory, Dependent intersection types, Union types, Polymorphic subtyping, Logic of events, Formal digital libraries, Computational type theory, Proofs as programs, Program extraction, Tactics},
abstract = {For twenty years the Nuprl (“new pearl”) system has been used to develop software systems and formal theories of computational mathematics. It has also been used to explore and implement computational type theory (CTT)—a formal theory of computation closely related to Martin-Löf's intuitionistic type theory (ITT) and to the calculus of inductive constructions (CIC) implemented in the Coq prover. This article focuses on the theory and practice underpinning our use of Nuprl for much of the last decade. We discuss innovative elements of type theory, including new type constructors such as unions and dependent intersections, our theory of classes, and our theory of event structures. We also discuss the innovative architecture of Nuprl as a distributed system and as a transactional database of formal mathematics using the notion of abstract object identifiers. The database has led to an independent project called the Formal Digital Library, FDL, now used as a repository for Nuprl results as well as selected results from HOL, MetaPRL, and PVS. We discuss Howe's set theoretic semantics that is used to relate such disparate theories and systems as those represented by these provers.}
}
@article{KOBSIRIPAT2015227,
title = {Effects of the Media to Promote the Scratch Programming Capabilities Creativity of Elementary School Students},
journal = {Procedia - Social and Behavioral Sciences},
volume = {174},
pages = {227-232},
year = {2015},
note = {International Conference on New Horizons in Education, INTE 2014, 25-27 June 2014, Paris, France},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.01.651},
url = {https://www.sciencedirect.com/science/article/pii/S1877042815007028},
author = {Worarit Kobsiripat},
keywords = {Scratch programming, Higher-order Thinking, Creative Thinking, Computer Multimedia;},
abstract = {Developing creative Promote higher-order thinking processes Give learners specific ability to think on their wide variety and innovative of the original. It led to the discovery and creation of new inventions or form new ideas. Consistent with the educational goals of the program.This research aim to study a guild line of using Scratch Computer Program that leading to creativity. And study the effects of media on the Scratch programming capabilities creativity. The sample consisted of 60 students who were studying in semester 1. 2013 academic year, using purposive sampling (Purposive Sampling) tool used in this research is a lesson plan. Scratch and computer media test innovative ideas. Statistics used Data analysis were percentage, mean, standard deviation and Dependent t-test. The findings indicated that First, Mediums Scratch program can be used as a medium for learning activities. The adoption includes a multimedia interactive media as a tool to support learning. Second, Scratch media performance of computer programs is equal according to the criteria set 82.46/82.25 E1/E2 is 80/80. Creativity of students. Received instruction from the learning activities through the medium of a computer program Scratch by elements of creativity is an idea ingenious ideas flexibility. Initiatives and ideas census. Higher posttest than pretest statistically significant at the .05 level of performance, computer media Scratch equals 82.46/82.25 according to defined criteria E1/E2 is 80 /80. In conclusion the computer program Scratch media can lead creative development of students through the learning activities that promote innovative education that cause the learners’ desirable.}
}
@article{BERKOVICHOHANA2020116626,
title = {Inter-participant consistency of language-processing networks during abstract thoughts},
journal = {NeuroImage},
volume = {211},
pages = {116626},
year = {2020},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2020.116626},
url = {https://www.sciencedirect.com/science/article/pii/S1053811920301130},
author = {Aviva Berkovich-Ohana and Niv Noy and Michal Harel and Edna Furman-Haran and Amos Arieli and Rafael Malach},
keywords = {Abstract-thoughts, Visual imagery, Default mode network, Language, fMRI},
abstract = {Human brain imaging typically employs structured and controlled tasks to avoid variable and inconsistent activation patterns. Here we expand this assumption by showing that an extremely open-ended, high-level cognitive task of thinking about an abstract content, loosely defined as “abstract thinking” - leads to highly consistent activation maps. Specifically, we show that activation maps generated during such cognitive process were precisely located relative to borders of well-known networks such as internal speech, visual and motor imagery. The activation patterns allowed decoding the thought condition at >95%. Surprisingly, the activated networks remained the same regardless of changes in thought content. Finally, we found remarkably consistent activation maps across individuals engaged in abstract thinking. This activation bordered, but strictly avoided visual and motor networks. On the other hand, it overlapped with left lateralized language networks. Activation of the default mode network (DMN) during abstract thought was similar to DMN activation during rest. These observations were supported by a quantitative neuronal distance metric analysis. Our results reveal that despite its high level, and varied content nature - abstract thinking activates surprisingly precise and consistent networks in participants’ brains.}
}
@article{LORE2024105149,
title = {Using multiple, dynamically linked representations to develop representational competency and conceptual understanding of the earthquake cycle},
journal = {Computers & Education},
volume = {222},
pages = {105149},
year = {2024},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2024.105149},
url = {https://www.sciencedirect.com/science/article/pii/S0360131524001635},
author = {Christopher Lore and Hee-Sun Lee and Amy Pallant and Jie Chao},
keywords = {Teaching/learning strategies, Simulations, Pedagogical issues, Applications in subject areas},
abstract = {Using computational methods to produce and interpret multiple scientific representations is now a common practice in many science disciplines. Research has shown students have difficulty in moving across, connecting, and sensemaking from multiple representations. There is a need to develop task-specific representational competencies for students to reason and conduct scientific investigations using multiple representations. In this study, we focus on three representational competencies: 1) linking between representations, 2) disciplinary sensemaking from multiple representations, and 3) conceptualizing domain-relevant content derived from multiple representations. We developed a block code-based computational modeling environment with three different representations and embedded it within an online activity for students to carry out investigations around the earthquake cycle. The three representations include a procedural representation of block codes, a geometric representation of land deformation build-up, and a graphical representation of deformation build-up over time. We examined the extent of students' representational competencies and which competencies are most correlated with students’ future performance in a computationally supported geoscience investigation. Results indicate that a majority of the 431 students showed at least some form of representational competence. However, a relatively small number of students showed sophisticated levels of linking, sensemaking, and conceptualizing from the representations. Five of seven representational competencies, the most prominent being code sensemaking (η2 = 0.053, p < 0.001), were significantly correlated to student performance on a summative geoscience investigation.}
}
@article{HU2024101646,
title = {A flexible BERT model enabling width- and depth-dynamic inference},
journal = {Computer Speech & Language},
volume = {87},
pages = {101646},
year = {2024},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2024.101646},
url = {https://www.sciencedirect.com/science/article/pii/S0885230824000299},
author = {Ting Hu and Christoph Meinel and Haojin Yang},
keywords = {Grafting, Dynamic inference, Large Language Models, Deep learning},
abstract = {Fine-tuning and inference on Large Language Models like BERT have become increasingly expensive regarding memory cost and computation resources. The recently proposed computation-flexible BERT models facilitate their deployment in varied computational environments. Training such flexible BERT models involves jointly optimizing multiple BERT subnets, which will unavoidably interfere with one another. Besides, the performance of large subnets is limited by the performance gap between the smallest subnet and the supernet, despite efforts to enhance the smaller subnets. In this regard, we propose layer-wise Neural grafting to boost BERT subnets, especially the larger ones. The proposed method improves the average performance of the subnets on six GLUE tasks and boosts the supernets on all GLUE tasks and the SQuAD data set. Based on the boosted subnets, we further build an inference framework enabling practical width- and depth-dynamic inference regarding different inputs by combining width-dynamic gating modules and early exit off-ramps in the depth dimension. Experimental results show that the proposed framework achieves a better dynamic inference range than other methods in terms of trading off performance and computational complexity on four GLUE tasks and SQuAD. In particular, our best-tradeoff inference result outperforms other fixed-size models with similar amount of computations. Compared to BERT-Base, the proposed inference framework yields a 1.3-point improvement in the average GLUE score and a 2.2-point increase in the F1 score on SQuAD, while reducing computations by around 45%.}
}
@article{CHENG2024104948,
title = {Exploring differences in self-regulated learning strategy use between high- and low-performing students in introductory programming: An analysis of eye-tracking and retrospective think-aloud data from program comprehension},
journal = {Computers & Education},
volume = {208},
pages = {104948},
year = {2024},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2023.104948},
url = {https://www.sciencedirect.com/science/article/pii/S0360131523002257},
author = {Gary Cheng and Di Zou and Haoran Xie and Fu Lee Wang},
keywords = {Introductory programming, Self-regulated learning strategies, Eye tracking, Retrospective think aloud, Higher education},
abstract = {Previous studies have reported mixed results regarding the relationship between students’ use of self-regulated learning (SRL) strategies and their performance in introductory programming courses. These studies were constrained by their reliance on self-report questionnaires as a means of collecting and analysing data. To address this limitation, this study aimed to employ eye-tracking and retrospective think-aloud techniques to identify differences in SRL strategy use for program comprehension tasks between high-performing students (N = 31) and low-performing students (N = 31) in an undergraduate programming course. All participants attended individual eye-tracking sessions to comprehend two Python program codes with different constructs. Their eye-tracking data and video-recalled retrospective think-aloud data were captured and recorded for analysis. The findings reveal that higher-order cognitive skills, such as elaboration and critical thinking, were mostly adopted by high-performing students, while basic cognitive and resource management strategy, such as rehearsal and help-seeking, were mostly employed by low-performing students when comprehending the program codes. This study not only demonstrates the design of combining eye-tracking and retrospective think-aloud data to explore students’ use of SRL strategies but also provides evidence to support the notion that program comprehension is a complex process that cannot be effectively addressed by employing merely rudimentary strategies, such as repetitively reading the same code segment. In the future, researchers could explore the possibility of using a webcam to monitor and assess students’ online programming processes and provide feedback based on their eye movements. They could also examine the effects of SRL strategies training on students’ motivation, engagement, and performance in various types of programming activities.}
}
@article{HEGER2018177,
title = {We should totally open a restaurant: How optimism and overconfidence affect beliefs},
journal = {Journal of Economic Psychology},
volume = {67},
pages = {177-190},
year = {2018},
issn = {0167-4870},
doi = {https://doi.org/10.1016/j.joep.2018.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167487017305585},
author = {Stephanie A. Heger and Nicholas W. Papageorge},
keywords = {Subjective beliefs, Overconfidence, Optimism, Information, Experiments},
abstract = {Wishful thinking, defined as the tendency to over-estimate the probability of high-payoff outcomes, is a widely-documented phenomenon that can affect decision-making across numerous domains, including finance, management, and entrepreneurship. We design an experiment to distinguish and test the relationship between two easily-confounded biases, optimism and overconfidence, both of which can contribute to wishful thinking. We find that optimism and overconfidence are positively correlated at the individual level and that both help to explain wishful thinking. These findings suggest that ignoring optimism results in an upwardly biased estimate of the role of overconfidence in explaining wishful thinking. To illustrate this point, we show that 30% of our observations are misclassified as under- or overconfident if optimism is omitted from the analysis. Our findings have potential implications for the design of information interventions since how agents incorporate information depends on whether the bias is ego-related.}
}
@article{DOHERTYSNEDDON2013616,
title = {Gaze aversion during social style interactions in autism spectrum disorder and Williams syndrome},
journal = {Research in Developmental Disabilities},
volume = {34},
number = {1},
pages = {616-626},
year = {2013},
issn = {0891-4222},
doi = {https://doi.org/10.1016/j.ridd.2012.09.022},
url = {https://www.sciencedirect.com/science/article/pii/S0891422212002557},
author = {Gwyneth Doherty-Sneddon and Lisa Whittle and Deborah M. Riby},
keywords = {Eye contact, Gaze, Williams syndrome, Gaze aversion, Autism spectrum disorder},
abstract = {During face-to-face interactions typically developing individuals use gaze aversion (GA), away from their questioner, when thinking. GA is also used when individuals with autism (ASD) and Williams syndrome (WS) are thinking during question-answer interactions. We investigated GA strategies during face-to-face social style interactions with familiar and unfamiliar interlocutors. Participants with WS and ASD used overall typical amounts/patterns of GA with all participants looking away most while thinking and remembering (in contrast to listening and speaking). However there were a couple of specific disorder related differences: participants with WS looked away less when thinking and interacting with unfamiliar interlocutors; in typical development and WS familiarity was associated with reduced gaze aversion, however no such difference was evident in ASD. Results inform typical/atypical social and cognitive phenotypes. We conclude that gaze aversion serves some common functions in typical and atypical development in terms of managing the cognitive and social load of interactions. There are some specific idiosyncracies associated with managing familiarity in ASD and WS with elevated sociability with unfamiliar others in WS and a lack of differentiation to interlocutor familiarity in ASD. Regardless of the familiarity of the interlocutor, GA is associated with thinking for typically developing as well as atypically developing groups. Social skills training must take this into account.}
}
@article{BOWMAN201834,
title = {Big questions, informative data, excellent science},
journal = {Statistics & Probability Letters},
volume = {136},
pages = {34-36},
year = {2018},
note = {The role of Statistics in the era of big data},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2018.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300622},
author = {Adrian W. Bowman},
keywords = {Big data, Statistical models},
abstract = {The expression big data is often used in a manner which implies that immediate insight is readily available. Unfortunately, this raises unrealistic expectations. A model which encapsulates the powerful concepts of statistical thinking remains an invaluable component of good analysis.}
}
@article{JOHNSON200337,
title = {Parallel processing in computational stochastic dynamics},
journal = {Probabilistic Engineering Mechanics},
volume = {18},
number = {1},
pages = {37-60},
year = {2003},
issn = {0266-8920},
doi = {https://doi.org/10.1016/S0266-8920(02)00041-3},
url = {https://www.sciencedirect.com/science/article/pii/S0266892002000413},
author = {E.A. Johnson and C. Proppe and B.F. Spencer and L.A. Bergman and G.S. Székely and G.I. Schuëller},
keywords = {Computational stochastic dynamics, Parallel computing, Monte Carlo simulation, Random eigenvalue, Fokker–Planck equation},
abstract = {Studying large complex problems that often arise in computational stochastic dynamics (CSD) demands significant computer power and data storage. Parallel processing can help meet these requirements by exploiting the computational and storage capabilities of multiprocessing computational environments. The challenge is to develop parallel algorithms and computational strategies that can take full advantage of parallel machines. This paper reviews some of the characteristics of parallel computing and the techniques used to parallelize computational algorithms in CSD. The characteristics of parallel processor environments are discussed, including parallelization through the use of message passing and parallelizing compilers. Several applications of parallel processing in CSD are then developed: solutions of the Fokker–Planck equation, Monte Carlo simulation of dynamical systems, and random eigenvector problems. In these examples, parallel processing is seen to be a promising approach through which to resolve some of the computational issues pertinent to CSD.}
}
@article{CHINTA20248181,
title = {Cascade reactions of HDDA-benzynes with tethered cyclohexadienones: strain-driven events originating from ortho-annulated benzocyclobutenes††Electronic supplementary information (ESI) available. CCDC 2302618–2302621. For ESI and crystallographic data in CIF or other electronic format see DOI: https://doi.org/10.1039/d4sc00571f},
journal = {Chemical Science},
volume = {15},
number = {21},
pages = {8181-8189},
year = {2024},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d4sc00571f},
url = {https://www.sciencedirect.com/science/article/pii/S2041652024006813},
author = {Bhavani Shankar Chinta and Dorian S. Sneddon and Thomas R. Hoye},
abstract = {Intramolecular net [2 + 2] cycloadditions between benzyne intermediates and an electron-deficient alkene to give benzocyclobutene intermediates are relatively rare. Benzynes are electrophilic and generally engage nucleophiles or electron-rich π-systems. We describe here reactions in which an alkene of a tethered enone traps thermally generated benzynes in a variety of interesting ways. The number of atoms that link the benzyne to C4 of a cyclohexa-2,5-dienone induces varying amounts of strain in the intermediates and products. This leads to a variety of different reaction outcomes by way of various strain-releasing events that are mechanistically intriguing. This work demonstrates an underappreciated class of strain that originates from the adjacent fusion of two rings to both C1–C2 and C2–C3 of a benzenoid ring – i.e. ‘ortho-annulation strain’. DFT computations shed considerable light on the mechanistic diversions among various reaction pathways as well as allow more fundamental evaluation of the strain in a homologous series of ortho-annulated carbocycles.}
}
@article{BLACK2008723,
title = {Deriving an approximation algorithm for automatic computation of ripple effect measures},
journal = {Information and Software Technology},
volume = {50},
number = {7},
pages = {723-736},
year = {2008},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2007.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584907000791},
author = {Sue Black},
keywords = {Software measurement, Ripple effect, Matrix algebra},
abstract = {The ripple effect measures impact, or how likely it is that a change to a particular module may cause problems in the rest of a program. It can also be used as an indicator of the complexity of a particular module or program. Central to this paper is a reformulation in terms of matrix arithmetic of the original ripple effect algorithm produced by Yau and Collofello in 1978. The main aim of the reformulation is to clarify the component parts of the algorithm making the calculation more explicit. The reformulated algorithm has been used to implement REST (Ripple Effect and Stability Tool) which produces ripple effect measures for C programs. This paper describes the reformulation of Yau and Collofello’s ripple effect algorithm focusing on the computation of matrix Zm which holds intramodule change propagation information. The reformulation of the ripple effect algorithm is validated using fifteen programs which have been grouped by type. Due to the approximation spurious 1s are contained within matrix Zm. It is discussed whether this has an impact on the accuracy of the reformulated algorithm. The conclusion of this research is that the approximated algorithm is valid and as such can replace Yau and Collofello’s original algorithm.}
}
@article{HONG201611,
title = {Ontology-based conceptual design for ultra-precision hydrostatic guideways with human–machine interaction},
journal = {Journal of Industrial Information Integration},
volume = {2},
pages = {11-18},
year = {2016},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2016.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X16300188},
author = {Haibo Hong and Yuehong Yin},
keywords = {Human machine integrated conceptual design, Information integration, High dimensional information integration, Ontology},
abstract = {This paper proposed a human–machine integrated conceptual design method based on ontology, aiming at eliminating the uncertainties and blindness during the design process of ultra-precision grinding machine, especially for its key component–the ultra-precision hydrostatic guideways. Both the required knowledge and the database of hydrostatic guideways are modelled using ontologies to provide a consensual understanding among collaborators. Moreover, a formalized knowledge searching interface is developed to obtain similar instances as references according to the design principles and rules. Based on the imaginal thinking theory, the search process and the results are attempted to be presented in the form of image in order to fit human's customary intuitive thinking frame, facilitating the decision making process. Finally, our design of hydrostatic guideways for an ultra-precision grinding machine is used to validate the effectiveness of the method.}
}
@article{JOHAN1992113,
title = {A data parallel finite element method for computational fluid dynamics on the Connection Machine system},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {99},
number = {1},
pages = {113-134},
year = {1992},
issn = {0045-7825},
doi = {https://doi.org/10.1016/0045-7825(92)90124-3},
url = {https://www.sciencedirect.com/science/article/pii/0045782592901243},
author = {Zdeněk Johan and Thomas J.R. Hughes and Kapil K. Mathur and S.Lennart Johnsson},
abstract = {A finite element method for computational fluid dynamics has been implemented on the Connection Machine systems CM-2 and CM-200. An implicit iterative solution strategy, based on the preconditioned matrix-free GMRES algorithm, is employed. Parallel data structures built on both nodal and elemental sets are used to achieve maximum parallelization. Communication primitives provided through the Connection Machine Scientific Software Library substantially improved the overall performance of the program. Computations of three-dimensional compressible flows using unstructured meshes having close to one million elements, such as a complete airplane, demonstrate that the Connection Machine systems are suitable for these applications. Performance comparisons are also carried out with the vector computers Cray Y-MP and Convex C-1.}
}
@article{BERRUTO2024858,
title = {Engineering agricultural soil microbiomes and predicting plant phenotypes},
journal = {Trends in Microbiology},
volume = {32},
number = {9},
pages = {858-873},
year = {2024},
issn = {0966-842X},
doi = {https://doi.org/10.1016/j.tim.2024.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0966842X2400043X},
author = {Chiara A. Berruto and Gozde S. Demirer},
keywords = {rhizosphere engineering, plant microbiome, machine learning, community modeling, host–microbe interactions, microbiome-associated phenotype},
abstract = {Plant growth-promoting rhizobacteria (PGPR) can improve crop yields, nutrient use efficiency, plant tolerance to stressors, and confer benefits to future generations of crops grown in the same soil. Unlocking the potential of microbial communities in the rhizosphere and endosphere is therefore of great interest for sustainable agriculture advancements. Before plant microbiomes can be engineered to confer desirable phenotypic effects on their plant hosts, a deeper understanding of the interacting factors influencing rhizosphere community structure and function is needed. Dealing with this complexity is becoming more feasible using computational approaches. In this review, we discuss recent advances at the intersection of experimental and computational strategies for the investigation of plant–microbiome interactions and the engineering of desirable soil microbiomes.}
}
@article{SINGER200948,
title = {The dynamic infrastructure of mind—A hypothesis and some of its applications},
journal = {New Ideas in Psychology},
volume = {27},
number = {1},
pages = {48-74},
year = {2009},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2008.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X08000032},
author = {Florence Mihaela Singer},
keywords = {Cognitive architecture, Dynamic infrastructure of mind, Learning, Operational categories},
abstract = {A mechanism underlying the computational properties of the cognitive architecture is construed based on a minimal list of operational clusters. This general processing mechanism constitutes the dynamic infrastructure of mind (DIM). DIM consists in categories of mental operations foundational for learning that contain inborn components called inner operations, which are self-developing in the interaction mind-environment. Within the DIM paradigm, the input cognitive systems are not domain specific or core-knowledge specific, they are operational specific and capable of further developments that become domain specific while experiencing the environment. Arguments for this construal come from three sources: literature review, data collected through classroom observations, and a four-year experimental study of teaching and learning mathematics in primary grades. The outcomes of that experiment led to a methodology of learning based on activating the operational infrastructure of mind, which enhances students' flexibility of thinking and predicts the capacity to solve creatively a variety of problems.}
}
@article{SOTELOMONGE2021869,
title = {Conceptualization and cases of study on cyber operations against the sustainability of the tactical edge},
journal = {Future Generation Computer Systems},
volume = {125},
pages = {869-890},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.07.016},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21002788},
author = {Marco Antonio {Sotelo Monge} and Jorge {Maestre Vidal}},
keywords = {Cyber defense, Economical Denial of Sustainability, Military operations, Situational Awareness, Tactical Denial of Sustainability},
abstract = {The last decade consolidated the cyberspace as fifth domain of military operations, which extends its preliminarily intelligence and information exchange purposes towards enabling complex offensive and defensive operations supported/supportively of parallel kinetic domain actuations. Although there is a plethora of well documented cases on strategic and operational interventions of cyber commands, the cyber tactical military edge is still a challenge, where cyber fires barely integrate to the traditional joint targeting cycle due to, among others, long planning/development times, asymmetric effects, strict target reachability requirements, or the fast propagation of collateral damage; the latter rapidly deriving on hybrid impacts (political, economic, social, etc.) and evidencing significant socio-technical gaps. In this context, it is expected that Tactical Clouds disruptively facilitate cyber operations at the edge while exposing the rest of the digital assets of the operation to them. On these grounds, the main purpose of the conducted research is to review and in depth analyze the risks and opportunities of jeopardizing the sustainability of the military Tactical Clouds at their cyber edge. Along with a 1) comprehensively formulation of the researched problematic, the study 2) formalizes the Tactical Denial of Sustainability (TDoS) concept; 3) introduces the phasing, potential attack surfaces, terrains and impact of TDoS attacks; 4) emphasizes the related human and socio-technical aspects; 5) analyzes the threats/opportunities inherent to their impact on the cloud energy efficiency; 6) reviews their implications at the military cyber thinking for tactical operations; 7) illustrates five extensive CONOPS that facilitate the understanding of the TDoS concept; and given the high novelty of the discussed topics, this paper 8) paves the way for further research and development actions.}
}
@incollection{BRACHMAN20041,
title = {Chapter 1 - Introduction},
editor = {Ronald J. Brachman and Hector J. Levesque},
booktitle = {Knowledge Representation and Reasoning},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {1-14},
year = {2004},
series = {The Morgan Kaufmann Series in Artificial Intelligence},
isbn = {978-1-55860-932-7},
doi = {https://doi.org/10.1016/B978-155860932-7/50086-8},
url = {https://www.sciencedirect.com/science/article/pii/B9781558609327500868},
author = {Ronald J. Brachman and Hector J. Levesque},
abstract = {Publisher Summary
This introductory chapter discusses the main issues associated with Artificial Intelligence (AI) and the prospect of a machine that could think. AI is the study of intelligent behavior that is achieved through computational means. One striking aspect of intelligent behavior is that it is conditioned by knowledge. Knowledge representation and reasoning are the parts of AI that are concerned with how an agent uses what it knows in deciding what to do. It is the study of thinking as a computational process. The book introduces the symbolic structures invented for representing knowledge and the computational processes devised for reasoning with those symbolic structures. The reason why logic is relevant to knowledge representation and reasoning is that logic is the study of entailment relations—languages, truth conditions, and rules of inference… Despite the centrality of knowledge representation and reasoning to AI, there are alternate views. Some authors have claimed that human-level reasoning is not achievable via purely computational means. Others suggest that intelligence derives from computational mechanisms.}
}
@article{MANIKANTAN2009639,
title = {Challenges for the future modifications of the TNM staging system for head and neck cancer: Case for a new computational model?},
journal = {Cancer Treatment Reviews},
volume = {35},
number = {7},
pages = {639-644},
year = {2009},
issn = {0305-7372},
doi = {https://doi.org/10.1016/j.ctrv.2009.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S0305737209000632},
author = {Kapila Manikantan and Suhail I. Sayed and Konstantinos N. Syrigos and Peter Rhys-Evans and Chris M. Nutting and Kevin J. Harrington and Rehan Kazi},
keywords = {TNM stage, Head and neck cancer, Co-morbidity},
abstract = {Summary
The TNM system of staging cancers is a simple and effective method to map the extent of tumours. It had traditionally strived to maintain a balance between being simple and user-friendly on one hand and comprehensive on the other. A number of revisions have taken place over the years with the goal of improving utility. However, numerous controversies surround the TNM system. There is a school of thought that contends that patient co-morbidity and specific tumour-related factors should be incorporated to add further prognostic capabilities in the TNM system, but this raises concerns that such an approach may unnecessarily complicate the system. This review highlights some controversies that surround the TNM system and suggests prognostic indicators that may be added to make it more useful in guiding treatment decisions and predicting outcomes.}
}
@incollection{NUNES2010457,
title = {Learning Outside of School},
editor = {Penelope Peterson and Eva Baker and Barry McGaw},
booktitle = {International Encyclopedia of Education (Third Edition)},
publisher = {Elsevier},
edition = {Third Edition},
address = {Oxford},
pages = {457-463},
year = {2010},
isbn = {978-0-08-044894-7},
doi = {https://doi.org/10.1016/B978-0-08-044894-7.00525-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008044894700525X},
author = {T. Nunes},
keywords = {Guided participation, Informal learning, Informal mathematics, Learning outside school, Nonformal learning, Oral arithmetic, Situated learning, Street mathematics, Thinking in action, Work-based learning},
abstract = {Learning can take place everywhere: in the home, the community, or at work. Learning outside school is often invisible because it is taken for granted, as common sense or cultural knowledge. It happens in the course of activities not designed for learning, so it can be described as thinking in action. The representational tools (number systems, graphs) and objects (crates, coins, bills) used outside school become part of our thinking as we act and think with them. A major process in learning outside school is guided participation, where learners take responsibility for accomplishing tasks guided by a more experienced person.}
}
@incollection{PROCHAZKOVA2020121,
title = {Chapter 6 - Altered states of consciousness and creativity},
editor = {David D. Preiss and Diego Cosmelli and James C. Kaufman},
booktitle = {Creativity and the Wandering Mind},
publisher = {Academic Press},
pages = {121-158},
year = {2020},
series = {Explorations in Creativity Research},
isbn = {978-0-12-816400-6},
doi = {https://doi.org/10.1016/B978-0-12-816400-6.00006-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128164006000067},
author = {Luisa Prochazkova and Bernhard Hommel},
keywords = {Altered states of consciousness (ASC), Cannabis, Convergent thinking, Creativity, Divergent thinking, Hallucinations, Meditation, Metactontrol, Psychedelics},
abstract = {Increasing evidence suggests that altered states of consciousness (ASC) are associated with both positive and negative effects on components of creative performance, and convergent and divergent thinking in particular. We provide a metacontrol framework that allows characterizing factors that induce ASC in terms of their general impact on the information processing style of problem solvers. We discuss behavioral and neuronal findings from three areas that reflect strong connections between ASC and the underlying effects on metacontrol on the one hand and components of creativity on the other hand: drug-induced ASC, meditation-induced ASC, and hallucinations. While more, and especially more systematic research is needed, we identify a general trend, suggesting that factors that induce ASC are likely to alter the metacontrol state by biasing it toward either persistence, which is beneficial for convergent thinking and other persistence-heavy operations, or flexibility, which is beneficial for divergent thinking and other flexibility-heavy operations.}
}
@article{SNOW201462,
title = {Emergent behaviors in computer-based learning environments: Computational signals of catching up},
journal = {Computers in Human Behavior},
volume = {41},
pages = {62-70},
year = {2014},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2014.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0747563214004518},
author = {Erica L. Snow and G. Tanner Jackson and Danielle S. McNamara},
keywords = {Intelligent tutoring systems, Individual differences, Self-regulated learning, Agency, Log data, Dynamic analyses},
abstract = {Self-regulative behaviors are dynamic and evolve as a function of time and context. However, dynamical fluctuations in behaviors are often difficult to measure and therefore may not be fully captured by traditional measures alone. Utilizing system log data and two novel statistical methodologies, this study examined emergent patterns of controlled and regulated behaviors and assessed how variations in these patterns related to individual differences in prior literacy ability and target skill acquisition. Conditional probabilities and Entropy analyses were used to examine nuanced patterns manifested in students’ interaction choices within a computer-based learning environment. Forty high school students interacted with the game-based intelligent tutoring system iSTART-ME, for a total of 11 sessions (pretest, 8 training sessions, posttest, and a delayed retention test). Results revealed that high and low reading ability students differed in their patterns of interactions and the amount of control they exhibited within the game-based system. However, these differences converged overtime along with differences in students’ performance within iSTART-ME. The findings from this study indicate that individual differences in students’ prior reading ability relate to the emergence of controlled and regulated behaviors during learning tasks.}
}
@article{MAHAL2007491,
journal = {Economics & Human Biology},
volume = {5},
number = {3},
pages = {491-493},
year = {2007},
note = {Special Issue on Obesity in Eastern Europe},
issn = {1570-677X},
doi = {https://doi.org/10.1016/j.ehb.2007.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S1570677X07000573},
author = {Ajay Mahal}
}
@article{GARDNER201854,
title = {SMLXL: Scaling the smart city, from metropolis to individual},
journal = {City, Culture and Society},
volume = {12},
pages = {54-61},
year = {2018},
note = {Innovation and identity in next generation smart cities},
issn = {1877-9166},
doi = {https://doi.org/10.1016/j.ccs.2017.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S1877916617301315},
author = {Nicole Gardner and Luke Hespanhol},
keywords = {Smart cities, Architecture, Design, Physical computing, Proxemics, Computational design},
abstract = {The ‘smart city’ is an oft-cited techno-urban imaginary promoted by businesses and governments alike. It thinks big, and is chiefly imagined in terms of large-scale information communications systems that hinge on the collection of real-time and so-called ‘big data’. Less talked about are the human-scale implications and user-experience of the smart city. Much of the current academic scholarship on smart cities offers synoptic and technical perspectives, leaving the users of smart systems curiously unaccounted for. While they purport to empower citizens, smart cities initiatives are rarely focused at the citizen-scale, nor do they necessarily attend to the ways initiatives can be user-led or co-designed. Drawing on the outcomes of a university studio, this article rethinks the smart city as a series of urban scales—metropolis, community, individual, and personal—and proposes an analytical model for classifying smart city initiatives in terms of engagement. Informed by the theory of proxemics, the model proposed analyses smart city initiatives in terms of the scope of their features and audience size; the actors accountable for their deployment and maintenance; their spatial reach; and the ability of design solutions to re-shape and adapt to different urban scenarios and precincts. We argue that the significance of this model lies in its potential to facilitate modes of thinking across and between scales in ways that can gauge the levels of involvement in the design of digitally mediated urban environments, and productively re-situate citizens as central to the design of smart city initiatives.}
}
@article{IYER2024e32546,
title = {Inspiring a convergent engineering approach to measure and model the tissue microenvironment},
journal = {Heliyon},
volume = {10},
number = {12},
pages = {e32546},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e32546},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024085773},
author = {Rishyashring R. Iyer and Catherine C. Applegate and Opeyemi H. Arogundade and Sushant Bangru and Ian C. Berg and Bashar Emon and Marilyn Porras-Gomez and Pei-Hsuan Hsieh and Yoon Jeong and Yongdeok Kim and Hailey J. Knox and Amir Ostadi Moghaddam and Carlos A. Renteria and Craig Richard and Ashlie Santaliz-Casiano and Sourya Sengupta and Jason Wang and Samantha G. Zambuto and Maria A. Zeballos and Marcia Pool and Rohit Bhargava and H. Rex Gaskins},
keywords = {Bioengineering, Interdisciplinary research, Bioimaging, Biomaterials, Biosensing, Computational biology, Biomedical devices, Biotechnology},
abstract = {Understanding the molecular and physical complexity of the tissue microenvironment (TiME) in the context of its spatiotemporal organization has remained an enduring challenge. Recent advances in engineering and data science are now promising the ability to study the structure, functions, and dynamics of the TiME in unprecedented detail; however, many advances still occur in silos that rarely integrate information to study the TiME in its full detail. This review provides an integrative overview of the engineering principles underlying chemical, optical, electrical, mechanical, and computational science to probe, sense, model, and fabricate the TiME. In individual sections, we first summarize the underlying principles, capabilities, and scope of emerging technologies, the breakthrough discoveries enabled by each technology and recent, promising innovations. We provide perspectives on the potential of these advances in answering critical questions about the TiME and its role in various disease and developmental processes. Finally, we present an integrative view that appreciates the major scientific and educational aspects in the study of the TiME.}
}
@article{WANG2022102240,
title = {What is the competence boundary of Algorithms? An institutional perspective on AI-based video generation},
journal = {Displays},
volume = {73},
pages = {102240},
year = {2022},
issn = {0141-9382},
doi = {https://doi.org/10.1016/j.displa.2022.102240},
url = {https://www.sciencedirect.com/science/article/pii/S0141938222000671},
author = {Jun Wang and Sichen Li and Ke Xue and Li Chen},
keywords = {Artificial intelligence, Video, Information fluency, Perception},
abstract = {As automated journalism based on AI came into being, it is important to understand the algorithm competence possibilities and limitations for the institutional facilitating the human–machine collaboration. Meanwhile, videos become mainstream in the advertisement realm. To expand the scope of research from journalism to advertisement, from text news to video, a comparative study was conducted to examine how the users perceive the video created by AI and humans. There is no significant difference explicitly, but the implicit appraisals were in favor of human-generated video. The key discussion is the boundary thinking of AI in both the academic and industrial spheres.}
}
@article{BENSASSI20233123,
title = {Fuzzy knowledge based assessment system for K-12 Scientific Reasoning Competencies},
journal = {Procedia Computer Science},
volume = {225},
pages = {3123-3132},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.306},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923014643},
author = {Manel BenSassi and Henda Ben Ghezala},
keywords = {Learning analytics, educational recommendations, fuzzy competencies assessment, knowledge representation, fuzzy ontology, Scientific Reasoning competencies},
abstract = {Developing Scientific Reasoning (SR) competencies at an early age, are challenging to meet expectation of the 4th sustainable development goal. Hence, educators and educational decision-makers try to embed these competencies into such subjects as the arts, language, technology, economics, mathematics and science, using an inter-disciplinary approach. In this context, this paper proposes a fuzzy knowledge-based solution to build practical pupils, educators, and decision-makers recommender system to support the development of SR competencies in a data driver manner. Our system consists of:(1) inferring and computational module that calculates in a fuzzy manner the global appreciation to each SR-competencies. (2) recommendation module that aims to help learners, educators and decision makers to assess the degree of development of SR competencies and to get alternative suggestion of remediation. The proposed solution has been tested on the last two levels of science education in four Tunisian elementary schools in different regions. A preliminary analysis showed that the learning process should be more focused on Tunisian pupil's profile, and that investigation and collaborative based learning should be applied further in Tunisian classroom.}
}
@article{GREGOR19981481,
title = {A computational study of the focus-of-attention EM-ML algorithm for PET reconstruction1Research supported in part by the National Science Foundation under grant CDA-95-29459.1},
journal = {Parallel Computing},
volume = {24},
number = {9},
pages = {1481-1497},
year = {1998},
issn = {0167-8191},
doi = {https://doi.org/10.1016/S0167-8191(98)00067-2},
url = {https://www.sciencedirect.com/science/article/pii/S0167819198000672},
author = {Jens Gregor and Dean A. Huff},
keywords = {Distributed computing, Expectation-maximization, Image reconstruction, Positron emission tomography},
abstract = {The expectation-maximization maximum-likelihood (EM-ML) algorithm for image reconstruction in positron emission tomography (PET) essentially solves a large linear system of equations. In this paper, we study computational aspects of a recently developed preprocessing scheme for focusing the attention, and thus the computational resources, on a subset of the equations and unknowns in order to reduce the storage, computation, and communication requirements of the EM-ML algorithm. The approach is completely data-driven and uses no prior anatomic knowledge. The experimental results are obtained from runs on a small network of workstations using simulated phantom data as well as data obtained from a clinical ECAT 921 PET scanner.}
}
@article{CAMERON2019102,
title = {Education in Process Systems Engineering: Why it matters more than ever and how it can be structured},
journal = {Computers & Chemical Engineering},
volume = {126},
pages = {102-112},
year = {2019},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2019.03.036},
url = {https://www.sciencedirect.com/science/article/pii/S0098135418311773},
author = {Ian T. Cameron and Sebastian Engell and Christos Georgakis and Norbert Asprion and Dominique Bonvin and Furong Gao and Dimitrios I. Gerogiorgis and Ignacio E. Grossmann and Sandro Macchietto and Heinz A. Preisig and Brent R. Young},
abstract = {This position paper is an outcome of discussions that took place at the third FIPSE Symposium in Rhodes, Greece, between June 20–22, 2016 (http://fi-in-pse.org). The FIPSE objective is to discuss open research challenges in topics of Process Systems Engineering (PSE). Here, we discuss the societal and industrial context in which systems thinking and Process Systems Engineering provide indispensable skills and tools for generating innovative solutions to complex problems. We further highlight the present and future challenges that require systems approaches and tools to address not only ‘grand’ challenges but any complex socio-technical challenge. The current state of Process Systems Engineering (PSE) education in the area of chemical and biochemical engineering is considered. We discuss approaches and content at both the unit learning level and at the curriculum level that will enhance the graduates’ capabilities to meet the future challenges they will be facing. PSE principles are important in their own right, but importantly they provide significant opportunities to aid the integration of learning in the basic and engineering sciences across the whole curriculum. This fact is crucial in curriculum design and implementation, such that our graduates benefit to the maximum extent from their learning.}
}
@article{MATTHEWS201973,
title = {Introducing a computational method to estimate and prioritize systemic body exposure of organic chemicals in humans using their physicochemical properties},
journal = {Computational Toxicology},
volume = {9},
pages = {73-99},
year = {2019},
issn = {2468-1113},
doi = {https://doi.org/10.1016/j.comtox.2018.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S2468111318300276},
author = {Edwin John Matthews},
keywords = {Absorption, Bioavailability, Chemical disposition, Data-gaps, Distribution, Food ingredient, GRAS, Hazard identification, , OCS (optimal chemical space), Pharmacokinetics, Physicochemical property, Preservative, Prioritization, QSAR, QSPR, Read-across, Risk-ranking, Sequestration, Signal-detection, Toxicokinetics},
abstract = {This report describes a computational method developed to predict systemic exposure (s-exposure), chemical disposition {(CD) intestinal absorption, transport, membrane permeability, distribution, sequestration, phospholipidosis and toxicokinetics} of organic chemicals in humans. The method qualitatively and quantitatively estimates a chemical's CD activity profile based upon computed molecular descriptor properties (descriptors), and it facilitates in silico signal-detection of data-gaps, prioritization, risk-ranking, read-across, and re-assessments (if mandated) of large sets of chemicals in a safety evaluation setting. The investigation used a reference set of 2372 marketed human pharmaceuticals to define decision rules for an optimal chemical space (OCS) in which chemicals have high s-exposure, good CD, and a potential for chemical toxicity (CT); conversely, chemicals outside the OCS have low s-exposure, poor CD into the body, and low potential for CT. The method requires computation of 29 descriptors, identification of OCS molecular descriptor property violations (descriptor_violations), and alignment of descriptor_violations with specific decision rules for individual CD endpoint activities. The investigation predicted the CD activities of food and cosmetic preservatives, ingredients in GRAS (generally recognized as safe). Notices submitted to the FDA, reference pharmaceuticals, and it provides prioritization metrics and indices that facilitate prioritization of chemical in silico computed CD activities.}
}
@article{WATKINS200867,
title = {Specifying Properties of Concurrent Computations in CLF},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {199},
pages = {67-87},
year = {2008},
note = {Proceedings of the Fourth International Workshop on Logical Frameworks and Meta-Languages (LFM 2004)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2007.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S1571066108000790},
author = {Kevin Watkins and Iliano Cervesato and Frank Pfenning and David Walker},
keywords = {logical frameworks, type theory, linear logic, concurrency},
abstract = {CLF (the Concurrent Logical Framework) is a language for specifying and reasoning about concurrent systems. Its most significant feature is the first-class representation of concurrent executions as monadic expressions. We illustrate the representation techniques available within CLF by applying them to an asynchronous pi-calculus with correspondence assertions, including its dynamic semantics, safety criterion, and a type system with latent effects due to Gordon and Jeffrey.}
}
@article{RIVEST19931,
title = {On Choosing between Experimenting and Thinking when Learning},
journal = {Information and Computation},
volume = {106},
number = {1},
pages = {1-25},
year = {1993},
issn = {0890-5401},
doi = {https://doi.org/10.1006/inco.1993.1047},
url = {https://www.sciencedirect.com/science/article/pii/S0890540183710473},
author = {R.L. Rivest and R.H. Sloan},
abstract = {We introduce a model of inductive inference, or learning, that extends the conventional Bayesian approach by explicitly considering the computational cost of formulating predictions to be tested. We view the learner as a scientist who must divide her time between doing experiments and deducing predictions from promising theories, and we wish to know how she can do so most effectively. We explore several approaches based on the cost of making a prediction relative to the cost of performing an experiment. The resulting strategies share many qualitative characteristics with "real" science. This model is significant for the following reasons: •It allows us to study how a scientist might go about acquiring knowledge in a world where (as in real life) both performing experiments and making predictions from theories require time and effort.•It lays the foundation for a rigorous machine-implementable notion of "subjective probability." Good (1959, , 443-447) argues persuasively that subjective probability is at the heart of probability theory. Previous treatments of subjective probability do not handle the complication that the learner′s subjective probabilities may change as the result of pure thinking; our model captures this and other effects in a realistic manner. In addition, we begin to answer the question of how to trade off versus -a question that is fundamental for computers that must exist in the world and learn from their experience.}
}
@article{ERVIN201612,
title = {Technology in geodesign},
journal = {Landscape and Urban Planning},
volume = {156},
pages = {12-16},
year = {2016},
note = {Geodesign—Changing the world, changing design},
issn = {0169-2046},
doi = {https://doi.org/10.1016/j.landurbplan.2016.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S0169204616301888},
author = {Stephen M. Ervin},
keywords = {Geodesign, Algorithmic processes, Collaboration, Dynamic modeling, Simulation, Systems thinking},
abstract = {Based on an idealized model with six distinguishing criteria of geodesign projects -- large areas, complex issues, and multi-person teams; digital computing, algorithmic processes, and communications technologies; collaborative, information-based projects; timely feedback about impacts and implications of proposals; dynamic modeling and simulation; and systems thinking -- the technological supports required for each of these criteria are described.}
}
@article{BOTTINI2020606,
title = {Knowledge Across Reference Frames: Cognitive Maps and Image Spaces},
journal = {Trends in Cognitive Sciences},
volume = {24},
number = {8},
pages = {606-619},
year = {2020},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2020.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S1364661320301327},
author = {Roberto Bottini and Christian F. Doeller},
keywords = {conceptual knowledge, space, hippocampus, parietal lobe, conceptual metaphors, analogy},
abstract = {In human and non-human animals, conceptual knowledge is partially organized according to low-dimensional geometries that rely on brain structures and computations involved in spatial representations. Recently, two separate lines of research have investigated cognitive maps, that are associated with the hippocampal formation and are similar to world-centered representations of the environment, and image spaces, that are associated with the parietal cortex and are similar to self-centered spatial relationships. We review evidence supporting cognitive maps and image spaces, and we propose a hippocampal–parietal network that can account for the organization and retrieval of knowledge across multiple reference frames. We also suggest that cognitive maps and image spaces may be two manifestations of a more general propensity of the mind to create low-dimensional internal models.}
}
@article{FOLTZ2023127,
title = {Reflections on the nature of measurement in language-based automated assessments of patients' mental state and cognitive function},
journal = {Schizophrenia Research},
volume = {259},
pages = {127-139},
year = {2023},
note = {Language and Speech Analysis in Schizophrenia and Related Psychoses},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2022.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0920996422002833},
author = {Peter W. Foltz and Chelsea Chandler and Catherine Diaz-Asper and Alex S. Cohen and Zachary Rodriguez and Terje B. Holmlund and Brita Elvevåg},
keywords = {Natural language processing, Speech technologies, Artificial intelligence},
abstract = {Modern advances in computational language processing methods have enabled new approaches to the measurement of mental processes. However, the field has primarily focused on model accuracy in predicting performance on a task or a diagnostic category. Instead the field should be more focused on determining which computational analyses align best with the targeted neurocognitive/psychological functions that we want to assess. In this paper we reflect on two decades of experience with the application of language-based assessment to patients' mental state and cognitive function by addressing the questions of what we are measuring, how it should be measured and why we are measuring the phenomena. We address the questions by advocating for a principled framework for aligning computational models to the constructs being assessed and the tasks being used, as well as defining how those constructs relate to patient clinical states. We further examine the assumptions that go into the computational models and the effects that model design decisions may have on the accuracy, bias and generalizability of models for assessing clinical states. Finally, we describe how this principled approach can further the goal of transitioning language-based computational assessments to part of clinical practice while gaining the trust of critical stakeholders.}
}
@article{CRUJEIRAS2013208,
title = {Challenges in the implementation of a competency-based curriculum in Spain},
journal = {Thinking Skills and Creativity},
volume = {10},
pages = {208-220},
year = {2013},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2013.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S187118711300045X},
author = {Beatriz Crujeiras and María Pilar Jiménez-Aleixandre},
keywords = {Scientific competency, Epistemic practices, Higher-order thinking, Policy},
abstract = {This paper addresses some of the challenges involved in implementing the new approach established in the Spanish National Curriculum in 2006, which brought as a major change a focus on the development of key competencies. The paper focuses on scientific competency and the challenges involved in the itinerary from policy documents to classrooms are addressed in three sections: (i) an analysis is made of the changes in the science curriculum as a consequence of the emphasis on scientific competency, comparing the assessment criteria in the previous and current steering documents; (ii) trends in teacher education are discussed; (iii) the findings of the diagnostic evaluation are analyzed. The paper is framed in a theoretical approach, viewing students’ participation in scientific practices, and the development of higher-order thinking as necessary goals of science education. We argue that the focus on competencies, characterized as the ability to apply knowledge and skills in new contexts, involves a major change towards knowledge transfer and higher-order thinking skills. Some issues emerging from the analysis relate to the implications of assessment criteria and the challenges involved in its implementation, to the trends in teacher professional development and the difficulties related to the current economic crisis and to the results of the diagnostic evaluation and time frame needed for reforms to have an impact. It is argued that the development of both competencies and higher-order thinking requires students’ prolonged engagement.}
}
@article{MEDINAOLIVA201338,
title = {PRM-based patterns for knowledge formalisation of industrial systems to support maintenance strategies assessment},
journal = {Reliability Engineering & System Safety},
volume = {116},
pages = {38-56},
year = {2013},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2013.02.026},
url = {https://www.sciencedirect.com/science/article/pii/S0951832013000616},
author = {G. Medina-Oliva and P. Weber and B. Iung},
keywords = {Maintenance strategies, Performances analysis, Decision-making, Bayesian Networks (BN), Probabilistic Relational Model (PRM)},
abstract = {The production system and its maintenance system must be now developed on “system thinking” paradigm in order to guarantee that Key Performance Indicators (KPI) will be optimized all along the production system (operation) life. In a recursive way, maintenance system engineering has to integrate also KPI considerations with regards to its own enabling systems. Thus this paper develops a system-based methodology wherein a set of KPIs is computed in order to verify if the objectives of the production and maintenance systems are satisfied. In order to help the decision-making process for maintenance managers, a “unified” generic model have been developed. This model integrates (a) the interactions of the maintenance system with its enabling systems, (b) the impact of the maintenance strategies through the computation of some key performance indicators, and (c) different kinds of knowledge regarding the maintenance system and the system of interest, including quantitative and qualitative knowledge. This methodology is based on an executable unified model built with Probabilistic Relational Model (PRM). PRM allows a modular representation and inferences computation of large size models. The methodology added-value is shown on a test-bench.}
}
@article{DOWNES1993229,
title = {Modeling scientific practice: Paul Thagard's computational approach},
journal = {New Ideas in Psychology},
volume = {11},
number = {2},
pages = {229-243},
year = {1993},
issn = {0732-118X},
doi = {https://doi.org/10.1016/0732-118X(93)90036-D},
url = {https://www.sciencedirect.com/science/article/pii/0732118X9390036D},
author = {Stephen Downes},
abstract = {In this paper I examine Paul Thagard's computational approach to studying science, which is a contribution to the cognitive science of science. I present several criticisms of Thagard's approach and use them to motivate some suggestions for alternative approaches in cognitive science of science. I first argue that Thagard does not clearly establish the units of analysis of his study. Second, I argue that Thagard mistakenly applies the same model to both individual and group decision making. Finally, I argue that in attempting to account for psychological and social processes as well as providing a philosophical model of successful reasoning Thagard attempts to explain too much with one model, thus straining the plausibility of his model.}
}
@article{PRINA2024132735,
title = {Machine learning as a surrogate model for EnergyPLAN: Speeding up energy system optimization at the country level},
journal = {Energy},
volume = {307},
pages = {132735},
year = {2024},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2024.132735},
url = {https://www.sciencedirect.com/science/article/pii/S036054422402509X},
author = {Matteo Giacomo Prina and Mattia Dallapiccola and David Moser and Wolfram Sparber},
keywords = {Energy system modelling, Energy scenarios, Energy planning, Machine learning},
abstract = {In the field of energy system modelling, increasing complexity and optimization analysis are essential for understanding the most effective decarbonization options. However, the growing need for intricate models leads to increased computational time, which can hinder progress in research and policy-making. This study aims to address this issue by integrating machine learning algorithms with EnergyPLAN and EPLANopt, a coupling of EnergyPLAN software and a multi-objective evolutionary algorithm, to expedite the optimization process while maintaining accuracy. By saving computational time, we can increase the number of evaluations, thereby enabling deeper exploration of uncertainty in energy system modelling. Although machine learning models have been widely employed as surrogate models to accelerate optimization problems, their application in energy system modeling at the national scale, while preserving high temporal resolution and extensive sector-coupling, remains scarce. Several machine learning models were evaluated, and an artificial neural network was selected as the most effective surrogate model. The findings demonstrate that incorporating this surrogate model within the optimization process reduces computational time by 64 % compared to the conventional EPLANopt approach, while maintaining an accuracy level close to that obtained by running EPLANopt without the surrogate model.}
}
@article{MENGOV2022101944,
title = {Virtual social networking increases the individual's economic predictability},
journal = {Journal of Behavioral and Experimental Economics},
volume = {101},
pages = {101944},
year = {2022},
issn = {2214-8043},
doi = {https://doi.org/10.1016/j.socec.2022.101944},
url = {https://www.sciencedirect.com/science/article/pii/S221480432200115X},
author = {George Mengov and Nikolay Georgiev and Irina Zinovieva and Anton Gerunov},
keywords = {Decision making, Virtual social network, Emotional economic choice, Neural model},
abstract = {Forecasting economic choice is hard because today we still do not know enough about human motivation. A fundamental problem is the lack of knowledge about how the neural networks in the brain give rise to thinking and decision making. One way to address the issue has been to develop simplified economic experiments, in which participants need skills of little complexity and their minds employ cognitive mechanisms, already well understood by mathematical psychology and neuroscience. Here we take a neural model for rudimentary emotion generation and memorizing and use it as a guiding theory to understand decision making in an experimental oligopoly market. For the first time in that line of research, participants are put in a lab virtual social network serving to exchange opinions about deals with companies. On average, choices become significantly more predictable when people participate in the network, in contrast to working alone with expert information. Calibrating the model for each person, we find that some people are predicted with startling precision.}
}
@article{CARTER2021100065,
title = {Comparing manual and computational approaches to theme identification in online forums: A case study of a sex work special interest community},
journal = {Methods in Psychology},
volume = {5},
pages = {100065},
year = {2021},
issn = {2590-2601},
doi = {https://doi.org/10.1016/j.metip.2021.100065},
url = {https://www.sciencedirect.com/science/article/pii/S2590260121000229},
author = {Pelham Carter and Matt Gee and Hollie McIlhone and Harkeeret Lally and Robert Lawson},
keywords = {Sex work, Online forums, Language, gender and sexuality, Mixed methods, Corpus linguistics},
abstract = {Online forums afford individuals opportunities to take part in a community with shared interests and goals. This involves the sharing of experiences and advice (Attard and Coulson, 2012) and can lead to positive effects (Pendry and Salvatore, 2015). Online forums also afford access to rich sources of detailed data, personal experiences, and hard-to-reach or taboo communities. Such online research, though well-suited to qualitative analysis, leads to a number of practical problems in terms of range, depth, and ease of access to data. Even extensive data collection and manual analysis often only engage with a small percentage of the data available in online communities. In this article, we present a traditional manual collection and thematic analysis of data (2631 posts across 60 different threads, approximately 300,000 words) from forums where sex workers and men who pay for sex discuss matters relating to prostitution. This analysis revealed five themes of forum use: preference sharing, personal narrative sharing, practical advice, philosophical issues, and community maintenance. Further automated data collection and corpus analysis, such as keyness and topic modelling, are presented as a potential innovation within online qualitative research. This approach allowed for the analysis of a larger dataset of 255,891 posts, across 14,232 threads (16,472,006 words), revealing additional themes such as sexual hygiene, desire, legality, and ethnicity, as well as differences in the use of terms of address and slang by punters and sex workers. The automated methods presented allow for more comprehensive investigations of online communities than traditional approaches, but we also note that manual interpretation should still be incorporated into the analysis.}
}
@article{ARTHUR2023638,
title = {Economics in nouns and verbs},
journal = {Journal of Economic Behavior & Organization},
volume = {205},
pages = {638-647},
year = {2023},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2022.10.036},
url = {https://www.sciencedirect.com/science/article/pii/S0167268122003936},
author = {W. Brian Arthur},
keywords = {Economic theory, Mathematics in economics, Algorithms, Complexity economics, Computational economics},
abstract = {Standard economic theory uses mathematics as its main means of understanding, and this brings clarity of reasoning and logical power. But there is a drawback: algebraic mathematics restricts economic modeling to what can be expressed only in quantitative nouns, and this forces theory to leave out matters to do with process, formation, adjustment, and creation—matters to do with nonequilibrium. For these we need a different means of understanding, one that allows verbs as well as nouns. Algorithmic expression is such a means. It allows verbs—processes—as well as nouns—objects and quantities. It allows fuller description in economics, and can include heterogeneity of agents, actions as well as objects, and realistic models of behavior in ill-defined situations. The world that algorithms reveal is action-based as well as object-based, organic, possibly ever-changing, and not fully knowable. But it is strangely and wonderfully alive.}
}
@article{MUNSON2019100736,
title = {After eliciting: Variation in elementary mathematics teachers’ discursive pathways during collaborative problem solving},
journal = {The Journal of Mathematical Behavior},
volume = {56},
pages = {100736},
year = {2019},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2019.100736},
url = {https://www.sciencedirect.com/science/article/pii/S073231231930046X},
author = {Jen Munson},
keywords = {Classroom discourse, Eliciting, Responsiveness, Student understanding},
abstract = {Mathematics teachers are called on to craft instruction that centers students’ mathematical ideas and creates consistent, pervasive opportunities for meaning-making through discourse. In the context of collaborative problem solving, teachers can use eliciting and probing to uncover student thinking while students work together to develop mathematical ideas and strategies. After eliciting and probing, teachers can further respond to the student thinking that has been revealed. This study explored the discursive pathways two fourth grade mathematics teachers used after eliciting student thinking, when their aim was to be responsive to and advance student thinking. Drawing on interactions (n = 97) from nine lessons, qualitative analysis identified five distinct discursive pathways after eliciting, two of which, praise and funneling, were associated with the nature of student understanding uncovered during eliciting. Implications for future research and professional development on teacher-student discourse are discussed.}
}
@incollection{PENN2012143,
title = {Computational Linguistics},
editor = {Ruth Kempson and Tim Fernando and Nicholas Asher},
booktitle = {Philosophy of Linguistics},
publisher = {North-Holland},
address = {Amsterdam},
pages = {143-173},
year = {2012},
series = {Handbook of the Philosophy of Science},
issn = {18789846},
doi = {https://doi.org/10.1016/B978-0-444-51747-0.50005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780444517470500056},
author = {Gerald Penn}
}
@article{KONOPKA1994V,
title = {Computational experiments in molecular biology: Searching for the ‘big picture’},
journal = {Computers & Chemistry},
volume = {18},
number = {3},
pages = {V-VIII},
year = {1994},
issn = {0097-8485},
doi = {https://doi.org/10.1016/0097-8485(94)85013-5},
url = {https://www.sciencedirect.com/science/article/pii/0097848594850135},
author = {AndrzejK. Konopka}
}
@article{WESTRA2023645,
title = {Accounting for systemic complexity in the assessment of climate risk},
journal = {One Earth},
volume = {6},
number = {6},
pages = {645-655},
year = {2023},
issn = {2590-3322},
doi = {https://doi.org/10.1016/j.oneear.2023.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S2590332223002063},
author = {Seth Westra and Jakob Zscheischler},
abstract = {Summary
Widespread changes to climate-sensitive systems are placing increased demands on risk assessments as a foundation for managing risk. Recent attention to compounding and cascading risks, deep uncertainty, and “bottom-up” risk assessment frameworks have foregrounded the need to account for systemic complexity in risk assessment methodology. We describe the sources of systemic complexity and highlight the role of risk assessments as a formal sense-making device that enables learning and organizing knowledge of the dynamic interplay between the climate-sensitive system and its (climatological) environment. We highlight boundary judgments as a core concern of risk assessments, helping to create islands of analytical and cognitive tractability in a complex, uncertain, and ambiguous world. We then point to three key concepts—boundary critique, multi-methodology, and second-order learning—as critical elements of contemporary risk assessment practice, and we weave these into an overarching framework to better account for systemic complexity in the assessment of climate risk.}
}
@article{WILLIAMS2009420,
title = {Analysis of externally loaded bolted joints: Analytical, computational and experimental study},
journal = {International Journal of Pressure Vessels and Piping},
volume = {86},
number = {7},
pages = {420-427},
year = {2009},
issn = {0308-0161},
doi = {https://doi.org/10.1016/j.ijpvp.2009.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0308016109000179},
author = {J.G. Williams and R.E. Anley and D.H. Nash and T.G.F. Gray},
keywords = {Bolted joints, Pre-loading, Bolt stiffness, Member stiffness, Member contact},
abstract = {The behaviour of a simple single-bolted-joint under tensile separating loads is analysed using conventional analytical methods, a finite element approach and experimental techniques. The variation in bolt force with external load predicted by the finite element analysis conforms well to the experimental results. It is demonstrated that certain detailed features such as thread interaction do not need to be modelled to ensure useful results. Behaviour during the pre-loading phase of use agrees with previous long-standing studies. However, the pre-loading analysis does not carry over to the stage when external loading is applied, as is normally assumed and it is shown that the current, conventional analytical methods substantially over-predict the proportion of the external load carried by the bolt. The basic reason for this is shown to be related to the non-linear variation in contact conditions between the clamped members during the external loading stage.}
}
@article{FREYBERG20231,
title = {The morphological paradigm in robotics},
journal = {Studies in History and Philosophy of Science},
volume = {100},
pages = {1-11},
year = {2023},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2023.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0039368123000742},
author = {Sascha Freyberg and Helmut Hauser},
keywords = {Bionics, Embodied cognition, Morphology, Morphological computation, Soft robotics, Principles of orientation and control},
abstract = {In the paper, we are going to show how robotics is undergoing a shift in a bionic direction after a period of emphasis on artificial intelligence and increasing computational efficiency, which included isolation and extreme specialization. We assemble these new developments under the label of the morphological paradigm. The change in its paradigms and the development of alternatives to the principles that dominated robotics for a long time contains a more general epistemological significance. The role of body, material, environment, interaction and the paradigmatic status of biological and evolutionary systems for the principles of control are crucial here. Our focus will be on the introduction of the morphological paradigm in a new type of robotics and to contrast the interests behind this development with the interests shaping former models. The article aims to give a clear account of the changes in principles of orientation and control as well as concluding general observation in terms of historical epistemology, suggesting further political-epistemological analysis.}
}
@article{ZENASNI201149,
title = {Pleasantness of creative tasks and creative performance},
journal = {Thinking Skills and Creativity},
volume = {6},
number = {1},
pages = {49-56},
year = {2011},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2010.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S1871187110000404},
author = {Franck Zenasni and Todd Lubart},
keywords = {Creativity, Perceived pleasantness, Emotion, Story writing, Divergent thinking},
abstract = {To examine the impact of emotion on creative potential, experimental studies have typically focused on the impact of induced or spontaneous mood states on creative performance. In this report the relationship between the perceived pleasantness of tasks (using divergent thinking and story writing tasks) and creative performance was examined. Overall perceived pleasantness did not differ between tasks. However, results indicate that the perceived pleasantness of the story writing task increased during task completion whereas the perceived pleasantness of divergent thinking tasks remained stable during task performance. The number of generated ideas in a divergent thinking task (fluency) was significantly related to overall perceived pleasantness of the task.}
}
@article{DONG2022134394,
title = {Understanding robustness in multiscale nutrient abatement: Probabilistic simulation-optimization using Bayesian network emulators},
journal = {Journal of Cleaner Production},
volume = {378},
pages = {134394},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.134394},
url = {https://www.sciencedirect.com/science/article/pii/S095965262203966X},
author = {Feifei Dong and Jincheng Li and Chao Dai and Jie Niu and Yan Chen and Jiacong Huang and Yong Liu},
keywords = {Diffuse nutrient, BMPs, Machine learning, Uncertainty, Simulation-optimization, Bayesian network},
abstract = {Ecosystem management in the face of uncertain disturbances has triggered increasing practices of resilience thinking. A multiscale probabilistic simulation-optimization framework is developed based on the nested nature of watersheds to inform decision robustness for Best Management Practices (BMPs). We presented a novel approach using hybrid Bayesian Networks (BNs) as interpretable and probabilistic emulators of process-based models. The hybrid BNs established at the scale of Hydrologic Response Units (HRUs) are embedded into simulation-optimization, whereby we analyze the cost-effectiveness-robustness of candidate BMP strategies at the subbasin scale. The optimal strategy is identified in compliance with water quality standards using watershed-scale integer programming. We apply the approaches in a typical intensively cultivated plateau watershed adjacent to Lake Dianchi, one of the three most eutrophic lakes in China. Our findings suggest that the hybrid BNs, incorporating both quantitative and qualitative information, are reliable emulators of the Soil and Water Assessment Tool (SWAT) in capturing critical pathways of diffuse phosphorus. Tradeoffs among cost, effectiveness, and robustness follow the law of diminishing marginal benefits. The optimum BMP strategies vary with policymakers’ preference toward robustness levels. Our findings indicate that robustness should be accounted for as an additional decision attribute besides costs and pollution mitigation. The benefits of the modeling framework are to (i) reduce over 99% computation complexity and support efficient decision-making under multifaceted uncertainties; (ii) improve interpretability and reliability of machine learning emulators; and (iii) inform policymakers of robustness with the probability of water quality restoration success.}
}
@article{STONE2004781,
title = {Intention, interpretation and the computational structure of language},
journal = {Cognitive Science},
volume = {28},
number = {5},
pages = {781-809},
year = {2004},
note = {2003 Rumelhart Prize Special Issue Honoring Aravind K. Joshi},
issn = {0364-0213},
doi = {https://doi.org/10.1016/j.cogsci.2004.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S036402130400062X},
author = {Matthew Stone},
keywords = {Dialogue, Pragmatics, Tree adjoining grammar},
abstract = {I show how a conversational process that takes simple, intuitively meaningful steps may be understood as a sophisticated computation that derives the richly detailed, complex representations implicit in our knowledge of language. To develop the account, I argue that natural language is structured in a way that lets us formalize grammatical knowledge precisely in terms of rich primitives of interpretation. Primitives of interpretation can be correctly viewed intentionally, as explanations of our choices of linguistic actions; the model therefore fits our intuitions about meaning in conversation. Nevertheless, interpretations for complex utterances can be built from these primitives by simple operations of grammatical derivation. In bridging analyses of meaning at semantic and symbol-processing levels, this account underscores the fundamental place for computation in the cognitive science of language use.}
}
@article{CEKIRGE199461,
title = {Oil spill modeling using parallel computations},
journal = {Spill Science & Technology Bulletin},
volume = {1},
number = {1},
pages = {61-68},
year = {1994},
issn = {1353-2561},
doi = {https://doi.org/10.1016/1353-2561(94)90008-6},
url = {https://www.sciencedirect.com/science/article/pii/1353256194900086},
author = {H.M. Cekirge and C.P. Giammona and J. Berlin and C. Long and M. Koch and R. Jamail},
abstract = {The current status of oil spill modeling is presented. The physical and chemical processes taking place in oil spills are explained for the design of an ideal oil spill model (IOSM). The requirements of an IOSM for forecasting are rapid response, contingency planning and training. The use of parallel computation techniques in oil spill modeling is introduced and delineated.}
}
@article{SWARUP2006273,
title = {A new evolutionary computation technique for economic dispatch with security constraints},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {28},
number = {4},
pages = {273-283},
year = {2006},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2006.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0142061506000020},
author = {K.S. Swarup and P. Rohit Kumar},
keywords = {Power system optimization, Economic load dispatch, Particle swarm optimization, Line-flow, Voltage constraints},
abstract = {This paper presents an efficient and reliable evolutionary based approach to solve the economic load dispatch (ELD) with security constraints. A new approach is proposed which employs attractive and repulsive particle swarm optimization (ARPSO) algorithm for ELD. Incorporation of ARPSO as a derivative-free optimization technique in solving ELD with security (voltages and line-flows) constraints significantly relieves the assumptions imposed on the optimized objective function. The proposed approach has been implemented on three representative systems, i.e. IEEE 14 bus, IEEE 30 bus and IEEE 57 bus systems, respectively. The feasibility of the proposed method is demonstrated and the results are compared with linear programming, quadratic programming and genetic algorithm, respectively. The premature convergence problem, that is common in all evolutionary computation techniques, is solved in ARPSO by including the diversity factor in the Type 1 PSO algorithm. The developed algorithms are computationally faster (in terms of the number of load flows carried out) than the other methods because only one run is required.}
}
@article{PAZBARUCH2023101294,
title = {Cognitive abilities and creativity: The role of working memory and visual processing},
journal = {Thinking Skills and Creativity},
volume = {48},
pages = {101294},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101294},
url = {https://www.sciencedirect.com/science/article/pii/S1871187123000640},
author = {Nurit Paz-Baruch and Rotem Maor},
keywords = {Cognitive abilities, Creativity, Working memory, Visual processing},
abstract = {Recent studies have revealed the importance of cognitive abilities in creative thinking. However, most research addressed adults and only a few studies have examined the ways these correlations are manifested among young children. The present study explores the role of various cognitive abilities in creativity among school children. Measures of creativity, visual-spatial working memory (VSWM), verbal short-term memory (STM), working memory (WM), and visual processing (VP) were administered to 331 students in Grades 4 and 5. Cluster analysis was used to group students' creativity levels. A multivariate analysis of variance (MANOVA) was conducted to test differences in cognitive abilities across the three clusters. Group differences between high and moderate level creativity students and low creativity students were found regarding VP abilities in the following tests: VSWM, visual discrimination (VD), and Raven's Colored Progressive Matrices (RCPM). Group differences between high creativity students and low creativity students were also found on verbal STM and WM. Additionally, structural equation modeling (SEM) analysis revealed that VP can significantly account for unique variances associated with creativity, while verbal STM and WM are not significantly related to creativity. These findings enlighten the cognitive processes underlying creativity in young children.}
}
@incollection{1991344,
title = {Appendix A - Scientific chaos: a new way of thinking about dynamics},
editor = {Ralph D. Stacey},
booktitle = {The Chaos Frontier},
publisher = {Butterworth-Heinemann},
pages = {344-365},
year = {1991},
isbn = {978-0-7506-0139-9},
doi = {https://doi.org/10.1016/B978-0-7506-0139-9.50021-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780750601399500212}
}
@article{ARKOUDAS2008461,
title = {Computation, hypercomputation, and physical science},
journal = {Journal of Applied Logic},
volume = {6},
number = {4},
pages = {461-475},
year = {2008},
note = {The Philosophy of Computer Science},
issn = {1570-8683},
doi = {https://doi.org/10.1016/j.jal.2008.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S1570868308000414},
author = {Konstantine Arkoudas},
keywords = {Hypercomputation, Church–Turing thesis, Gandy's thesis, Mechanical computability, Algorithms, Turing limit, Physical science, Physical computability},
abstract = {Copeland and others have argued that the Church–Turing thesis (CTT) has been widely misunderstood by philosophers and cognitive scientists. In particular, they have claimed that CTT is in principle compatible with the existence of machines that compute functions above the “Turing limit,” and that empirical investigation is needed to determine the “exact membership” of the set of functions that are physically computable. I argue for the following points: (a) It is highly doubtful that philosophers and cognitive scientists have widely misunderstood CTT as alleged.1 In fact, by and large, computability theorists and mathematical logicians understand CTT in the exact same way. (b) That understanding most likely coincides with what Turing and Church had in mind. Even if it does not, an accurate exegesis of Turing and Church need not dictate how today's working scientists understand the thesis. (c) Even if we grant Copeland's reading of CTT, an orthodox stronger version of it which he rejects (Gandy's thesis) follows readily if we only accept a highly plausible necessary condition for what constitutes a deterministic digital computer. Finally, (d) regardless of whether we accept this condition, the prospects for a scientific theory of hypercomputation are exceedingly poor because physical science does not have the wherewithal to investigate computability or to discover its ultimate “limit.”}
}
@incollection{FEIGENSON201113,
title = {Chapter 2 - Objects, Sets, and Ensembles},
editor = {Stanislas Dehaene and Elizabeth M. Brannon},
booktitle = {Space, Time and Number in the Brain},
publisher = {Academic Press},
address = {San Diego},
pages = {13-22},
year = {2011},
isbn = {978-0-12-385948-8},
doi = {https://doi.org/10.1016/B978-0-12-385948-8.00002-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780123859488000025},
author = {Lisa Feigenson},
abstract = {Publisher Summary
This chapter discusses how different types of entities such as individual objects, sets of objects, and ensembles can function as items to be maintained in working memory (WM). All of these types of representations can be relevant to thinking about quantities, and each supports different kinds of quantity-relevant computations. One way to overcome the three- to four-item limit of WM is to bind together representations of individual objects into representations of sets of objects. Binding multiple individuals into a single higher-order group can increase the number of individual items that can be remembered, as in the well-known demonstrations of chunking by adults. Set representations play a critical role in many numerical processes, including representing the nested relationships between as well as representing the cardinality of an array. Many real-world scenes contain stimuli that do not lend themselves to representation qua individual objects or sets of objects. One can imagine enumerating a flock containing dozens of individual birds.}
}
@article{GOLDBACH2016249,
title = {Computational Cutting Pattern Generation Using Isogeometric B-Rep Analysis},
journal = {Procedia Engineering},
volume = {155},
pages = {249-255},
year = {2016},
note = {TENSINET – COST TU1303 International Symposium 2016 "Novel structural skins - Improving sustainability and efficiency through new structural textile materials and designs"},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2016.08.026},
url = {https://www.sciencedirect.com/science/article/pii/S1877705816321671},
author = {Ann-Kathrin Goldbach and Michael Breitenberger and Armin Widhammer and Kai-Uwe Bletzinger},
keywords = {Cutting pattern generation, Variation of Reference Strategy, Isogeometric Analysis, Isogeometric B-Rep Analysis, Design cycle of structural membranes},
abstract = {The cutting pattern plays a major role for the design of structural membranes, since it influences both their aesthetical appearance and structural behavior. A novel approach towards cutting pattern generation is the so-called Variation of Reference Strategy (VaReS) [1], which minimizes the total potential energy arising from the motion of a planar cutting pattern to its corresponding three-dimensional shape. With non-uniform rational B-Splines (NURBS) being the standard tool for geometry description in CAD, it is only consequent to use these for analysis as well. Isogeometric B-Rep Analysis (IBRA) [2] follows up on this idea and enriches the original Isogeometric Analysis (IGA), which was introduced by Hughes et al. [3], by the possibility of analysing trimmed NURBS geometries. This paper presents cutting pattern generation with the Variation of Reference Strategy in the context of IGA/IBRA. With this approach, the whole design of a membrane structure can be represented by NURBS geometries – including blueprint plans. To use the benefits of IBRA for cutting pattern generation, a NURBS-based membrane-element was developed for the VaReS routine. A developable surface serves as a benchmark example, since its analytical cutting pattern is known. Examples of double-curved geometries show the applicability and benefits of the proposed procedure for real structures.}
}
@article{SAIKIA2021129664,
title = {Study of interacting mechanism of amino acid and Alzheimer's drug using vibrational techniques and computational method},
journal = {Journal of Molecular Structure},
volume = {1227},
pages = {129664},
year = {2021},
issn = {0022-2860},
doi = {https://doi.org/10.1016/j.molstruc.2020.129664},
url = {https://www.sciencedirect.com/science/article/pii/S0022286020319773},
author = {Jyotshna Saikia and Bhargab Borah and Th. Gomti Devi},
keywords = {DL-Alanine, Memantine, Raman, FTIR, DFT},
abstract = {The present work is undertaken to investigate the molecular interaction between Memantine (Alzheimer's drug) and DL-Alanine (amino acid) using DFT and vibrational spectroscopic methods, in particular, Fourier Transform Infrared Spectroscopy (FTIR) and Raman techniques. The DFT calculations of these molecules are carried out using the B3LYP/6–311 ++ G (d, p) level of theory. The experimental FTIR and Raman spectra of the molecules are compared to the respective DFT computed wavenumbers. A satisfactory agreement is obtained between experimental and computed wavenumbers. Further, HOMO-LUMO energy gap, Natural Bond Orbital (NBO) analysis, total energy, zero-point vibrational energy, Molecular Electrostatic Potential (MEP), chemical potential, hardness, ionization energy, global electrophilicity index, dipole moments, and first-order hyperpolarizabilities of the interacting state are reported and compared to the respective parameters of the individual states. The NBO analysis of the molecules indicates the transfer of charge between DL-Alanine and Memantine through NH•••O intermolecular hydrogen bonds. The molecular docking studies of the molecules are  performed to investigate the binding affinity of the ligand with the 6DG7 receptor.}
}
@article{HEYN2023111604,
title = {A compositional approach to creating architecture frameworks with an application to distributed AI systems},
journal = {Journal of Systems and Software},
volume = {198},
pages = {111604},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111604},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222002801},
author = {Hans-Martin Heyn and Eric Knauss and Patrizio Pelliccione},
keywords = {AI systems, Architectural frameworks, Compositional thinking, Requirements engineering, Systems engineering},
abstract = {Artificial intelligence (AI) in its various forms finds more and more its way into complex distributed systems. For instance, it is used locally, as part of a sensor system, on the edge for low-latency high-performance inference, or in the cloud, e.g. for data mining. Modern complex systems, such as connected vehicles, are often part of an Internet of Things (IoT). This poses additional architectural challenges. To manage complexity, architectures are described with architecture frameworks, which are composed of a number of architectural views connected through correspondence rules. Despite some attempts, the definition of a mathematical foundation for architecture frameworks that are suitable for the development of distributed AI systems still requires investigation and study. In this paper, we propose to extend the state of the art on architecture framework by providing a mathematical model for system architectures, which is scalable and supports co-evolution of different aspects for example of an AI system. Based on Design Science Research, this study starts by identifying the challenges with architectural frameworks in a use case of distributed AI systems. Then, we derive from the identified challenges four rules, and we formulate them by exploiting concepts from category theory. We show how compositional thinking can provide rules for the creation and management of architectural frameworks for complex systems, for example distributed systems with AI. The aim of the paper is not to provide viewpoints or architecture models specific to AI systems, but instead to provide guidelines based on a mathematical formulation on how a consistent framework can be built up with existing, or newly created, viewpoints. To put in practice and test the approach, the identified and formulated rules are applied to derive an architectural framework for the EU Horizon 2020 project “Very efficient deep learning in the IoT” (VEDLIoT) in the form of a case study.}
}
@article{DEBARROS2012171,
title = {Quantum-like model of behavioral response computation using neural oscillators},
journal = {Biosystems},
volume = {110},
number = {3},
pages = {171-182},
year = {2012},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2012.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0303264712001736},
author = {J. Acacio {de Barros}},
keywords = {Disjunction effect, Quantum cognition, Quantum-like model, Neural oscillators, Stimulus-response theory},
abstract = {In this paper we propose the use of neural interference as the origin of quantum-like effects in the brain. We do so by using a neural oscillator model consistent with neurophysiological data. The model used was shown elsewhere to reproduce well the predictions of behavioral stimulus-response theory. The quantum-like effects are brought about by the spreading activation of incompatible oscillators, leading to an interference-like effect mediated by inhibitory and excitatory synapses.}
}
@article{PILA201952,
title = {Learning to code via tablet applications: An evaluation of Daisy the Dinosaur and Kodable as learning tools for young children},
journal = {Computers & Education},
volume = {128},
pages = {52-62},
year = {2019},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2018.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0360131518302422},
author = {Sarah Pila and Fashina Aladé and Kelly J. Sheehan and Alexis R. Lauricella and Ellen A. Wartella},
keywords = {Apps, Coding, Computational thinking, Digital games, Educational technology, STEM},
abstract = {Despite the growing number of digital apps designed to teach coding skills to young children, we know little about their effectiveness. To formally explore this question, we conducted a naturalistic observation of a one-week program designed to teach foundational coding skills (i.e., sequencing, conditions, loops) to young children (N = 28, Mage = 5.15 years) using two tablet applications: Daisy the Dinosaur and Kodable. Pre- and post-assessments measured familiarity with technology, appeal of coding apps, knowledge of Daisy commands, ability to play Kodable, and conceptual understanding of coding. Participants improved in their knowledge of Daisy commands (i.e., move, grow, jump) and Kodable gameplay (i.e., placing arrows in the correct sequence to move a character through a maze), but did not improve in their ability to verbally explain what coding is. Appeal of the games was significantly related to children's learning of Daisy commands, but child gender was not related to either Daisy or Kodable learning outcomes. Results suggest that young children can learn foundational coding skills via apps, especially when the apps are appealing to children.}
}
@article{BARBER200798,
title = {Interplay between computational models and cognitive electrophysiology in visual word recognition},
journal = {Brain Research Reviews},
volume = {53},
number = {1},
pages = {98-123},
year = {2007},
issn = {0165-0173},
doi = {https://doi.org/10.1016/j.brainresrev.2006.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0165017306000853},
author = {Horacio A. Barber and Marta Kutas},
keywords = {Language, Reading, Visual word recognition, Computational model, Event-related potential (ERP), EEG, MEG, Cognitive electrophysiology},
abstract = {In this article, we discuss the relevance of electrophysiological data to the enterprise of analyzing and understanding the reading process. Specifically, we detail how the event-related brain potential (ERP) technique (and its magnetic counterpart) can aid in development of models of visual word recognition. Any viable and accurate account of reading must take into account the temporal and anatomical constraints imposed by the fact that reading is a human brain function. We believe that neurophysiological (especially, although not limited to electrophysiological) data can serve an essential reference in the development of biologically realistic models of reading. We assess just how well extant electrophysiological data comport with specific predictions of existing computational models and offer some suggestions for the kinds of research that can address some of the remaining open questions.}
}
@article{REICHLE20062,
title = {Computational models of eye-movement control during reading: Theories of the “eye–mind” link},
journal = {Cognitive Systems Research},
volume = {7},
number = {1},
pages = {2-3},
year = {2006},
note = {Models of Eye-Movement Control in Reading},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2005.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041705000227},
author = {Erik D. Reichle}
}
@article{ZOMAYA2004551,
title = {Parallel and nature-inspired computational paradigms and applications},
journal = {Parallel Computing},
volume = {30},
number = {5},
pages = {551-552},
year = {2004},
note = {Parallel and nature-inspired computational paradigms and applications},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2004.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167819104000304},
author = {Albert Y Zomaya and Fikret Ercal and El-ghazali Talbi}
}
@article{GALLISTEL2021104533,
title = {The physical basis of memory},
journal = {Cognition},
volume = {213},
pages = {104533},
year = {2021},
note = {Special Issue in Honour of Jacques Mehler, Cognition’s founding editor},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2020.104533},
url = {https://www.sciencedirect.com/science/article/pii/S0010027720303528},
author = {C.R. Gallistel},
keywords = {Engram, Communication channel, Plastic synapse, Molecules},
abstract = {Neuroscientists are searching for the engram within the conceptual framework established by John Locke's theory of mind. This framework was elaborated before the development of information theory, before the development of information processing machines and the science of computation, before the discovery that molecules carry hereditary information, before the discovery of the codon code and the molecular machinery for editing the messages written in this code and translating it into transcription factors that mark abstract features of organic structure such as anterior and distal. The search for the engram needs to abandon Locke's conceptual framework and work within a framework informed by these developments. The engram is the medium by which information extracted from past experience is transmitted to the computations that inform future behavior. The information-conveying symbols in the engram are rapidly generated in the course of computations, which implies that they are molecules.}
}
@incollection{HALL2006338,
title = {Computational Approaches to Fibril Structure and Formation},
series = {Methods in Enzymology},
publisher = {Academic Press},
volume = {412},
pages = {338-365},
year = {2006},
booktitle = {Amyloid, Prions, and Other Protein Aggregates, Part B},
issn = {0076-6879},
doi = {https://doi.org/10.1016/S0076-6879(06)12020-0},
url = {https://www.sciencedirect.com/science/article/pii/S0076687906120200},
author = {Carol K. Hall and Victoria A. Wagoner},
abstract = {Assembly of normally soluble proteins into amyloid fibrils is a cause or associated symptom of numerous human disorders. Although some progress toward understanding the molecular‐level details of fibril structure has been made through in vitro experiments, the insoluble nature of fibrils make them difficult to study experimentally. We describe two computational approaches used to investigate fibril formation and structure: intermediate‐resolution discontinuous molecular dynamics simulations and atomistic molecular dynamics simulations. Each method has its strengths and weaknesses, but taken together the two approaches provide a useful molecular‐level picture of fibril structure and formation.}
}
@article{ANANTHASWAMY201742,
title = {That's a termite colony between your ears},
journal = {New Scientist},
volume = {233},
number = {3112},
pages = {42-43},
year = {2017},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(17)30275-0},
url = {https://www.sciencedirect.com/science/article/pii/S0262407917302750},
author = {Anil Ananthaswamy},
abstract = {After wrestling with the nature of the mind for over half a century, Daniel Dennett uploads his latest thinking on consciousness, word-based “mind viruses” and why we must doubt the power of artificial intelligence}
}
@article{JONES20171,
title = {Diversity not quantity in caregiver speech: Using computational modeling to isolate the effects of the quantity and the diversity of the input on vocabulary growth},
journal = {Cognitive Psychology},
volume = {98},
pages = {1-21},
year = {2017},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2017.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0010028516302274},
author = {Gary Jones and Caroline F. Rowland},
keywords = {Input quantity, Lexical diversity, Vocabulary acquisition, CLASSIC, Language acquisition},
abstract = {Children who hear large amounts of diverse speech learn language more quickly than children who do not. However, high correlations between the amount and the diversity of the input in speech samples makes it difficult to isolate the influence of each. We overcame this problem by controlling the input to a computational model so that amount of exposure to linguistic input (quantity) and the quality of that input (lexical diversity) were independently manipulated. Sublexical, lexical, and multi-word knowledge were charted across development (Study 1), showing that while input quantity may be important early in learning, lexical diversity is ultimately more crucial, a prediction confirmed against children’s data (Study 2). The model trained on a lexically diverse input also performed better on nonword repetition and sentence recall tests (Study 3) and was quicker to learn new words over time (Study 4). A language input that is rich in lexical diversity outperforms equivalent richness in quantity for learned sublexical and lexical knowledge, for well-established language tests, and for acquiring words that have never been encountered before.}
}
@article{BOKHOVE2023100497,
title = {Using Social Network Analysis to gain insight into social creativity while designing digital mathematics books},
journal = {Social Sciences & Humanities Open},
volume = {8},
number = {1},
pages = {100497},
year = {2023},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2023.100497},
url = {https://www.sciencedirect.com/science/article/pii/S259029112300102X},
author = {Christian Bokhove and Marios Xenos and Manolis Mavrikis},
keywords = {Technological environment, Co-creation, Social creativity, Social network analysis},
abstract = {Analysing the processes and products of creativity to better understand and support individuals and teams, is a difficult and elusive challenge despite years of research in creativity. In this article, we are particularly interested in social creativity in communities of interest. Building on Guilford's classic model of Divergent Thinking of fluency, flexibility, originality and elaboration, we employ Social Network Analysis to model the creative design process. The creative process in the current study takes place in a technological environment called the ‘MC-squared platform’, in which members of a community of interest collaborate in a social, co-creative process for designing digital, mathematical textbooks. Both the technological environment and the methodology are exemplified through two case examples, one on the design process of a digital book about a bioclimatic amusement park and one on the design process of a digital book about fractions. We conclude that, for these examples, both the technological tool and the data analysis approach provide insight into the social creativity process of the community of interest.}
}
@incollection{GARDNER202427,
title = {Chapter 2 - Ethics and the smart city},
editor = {Nicole Gardner},
booktitle = {Scaling the Smart City},
publisher = {Elsevier},
pages = {27-49},
year = {2024},
series = {Smart Cities},
isbn = {978-0-443-18452-9},
doi = {https://doi.org/10.1016/B978-0-443-18452-9.00004-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780443184529000045},
author = {Nicole Gardner},
keywords = {Design, process, Ethics, Ethics by design, Material ethics, Philosophy of technology, Postphenomenology, Practical ethics, Smart city, Urban technology, Value-sensitive design},
abstract = {Examining who (and what) stands to gain and lose in the smart city, and what kind of urban life smart cities create, are questions of a philosophical and ethical import. Since the early 2010s scholars and journalists have critiqued the smart city paradigm and examined its various projects and experimental initiatives to surface its ethical dimensions and significance. The smart city's rhetorical swing from tech-centric to human-centric and from smart city to smart citizen is construed here as a further effort to create the image of a more ethical smart city. Consequently, the topic of ethics has also intersected with the smart city in particular ways, and predominantly through the lens of data governance and the protection of privacy rights. This chapter argues for an expanded approach to smart city ethics. It proposes a focus on urban technology design to bridge the gap between a micro-ethical focus on data ethics and macro-level political-ethical critique. Bringing philosophical thinking on technology together with a design-led approach to urban technology is argued to provide a further way to draw out a potentially different set of ethical concerns and to explore how urban life can be lived with technology.}
}
@incollection{VASSILOPOULOS2010139,
title = {5 - Novel computational methods for fatig life modeling of composite materials},
editor = {Anastasios P. Vassilopoulos},
booktitle = {Fatigue Life Prediction of Composites and Composite Structures},
publisher = {Woodhead Publishing},
pages = {139-173},
year = {2010},
series = {Woodhead Publishing Series in Composites Science and Engineering},
isbn = {978-1-84569-525-5},
doi = {https://doi.org/10.1533/9781845699796.1.139},
url = {https://www.sciencedirect.com/science/article/pii/B978184569525550005X},
author = {A.P. Vassilopoulos and E.F. Georgopoulos},
keywords = {fatigue, composites, artificial neural network, genetic programming, ANFIS, S–N curves},
abstract = {Abstract:
Novel computational methods such as artificial neural networks, adaptive neuro-fuzzy inference systems and genetic programming are used in this chapter for the modeling of the nonlinear behavior of composite laminates subjected to constant amplitude loading. The examined computational methods are stochastic nonlinear regression tools, and can therefore be used to model the fatigue behavior of any material, provided that sufficient data are available for training. They are material-independent methods that simply follow the trend of the available data, in each case giving the best estimate of their behavior. Application on a wide range of experimental data gathered after fatigue testing glass/epoxy and glass/polyester laminates proved that their modeling ability compares favorably with, and is to some extent superior to, other modeling techniques.}
}
@incollection{2016295,
title = {10 - Computational fluid dynamics in aerospace field and CFD-based multidisciplinary simulations},
editor = {Qun Zhang and Song Cen},
booktitle = {Multiphysics Modeling},
publisher = {Academic Press},
address = {Oxford},
pages = {295-328},
year = {2016},
series = {Elsevier and Tsinghua University Press Computational Mechanics Series},
isbn = {978-0-12-407709-6},
doi = {https://doi.org/10.1016/B978-0-12-407709-6.00010-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780124077096000109},
keywords = {aerospace engineering, finite volume method, ALE formulation, discrete geometric conservation law, mesh deformation, remeshing, flapping wing, store separation, wing flutter},
abstract = {In this chapter, the ALE formulation of the finite volume method is proposed for the simulation of compressible fluid flow. Two major topics of the discrete geometric conservation lawgeometric conservation law and mesh deformationmesh deformation algorithm in this chapter are to handle the moving boundary problem accurately and efficiently. Three examples are given to verify the effectiveness of the presented methods in multiphysics simulation for aerospace engineering problems.}
}
@article{ELABY2022101780,
title = {Does design-build concept improve problem-solving skills? An analysis of first-year engineering students},
journal = {Ain Shams Engineering Journal},
volume = {13},
number = {6},
pages = {101780},
year = {2022},
issn = {2090-4479},
doi = {https://doi.org/10.1016/j.asej.2022.101780},
url = {https://www.sciencedirect.com/science/article/pii/S2090447922000910},
author = {Mohammed F. Elaby and Hesham M. Elwishy and Saeed F. Moatamed and Mahmoud A. Abdelwahed and Ahmed E. Rashiedy},
keywords = {Problem-solving, Critical thinking, Design-build studio, Instructional design},
abstract = {The design-build studio (DBS) is an emergent paradigm in architecture education. Recently, researchers have addressed the success of integrating the design-build concept into the conventional studio in the advanced years of education to improve several issues related to the design process: social, environmental, technological, …etc. However, its efficiency in terms of contribution to the learning experience has not yet been addressed. This paper examines the implementation of the DBS concept as a new teaching model to improve the problem-solving skills (PSS) of first-year students in engineering education. It also discusses the effectson learning experiences and pedagogical outcomes in both quantitative and qualitative terms. The research’s results identify the significance of applying this model as a real-simulated method-based learning experience. It can help students to improve their learning experiences and to enhance students self-confidence regarding PSS.}
}
@article{CORREABAENA20181410,
title = {Accelerating Materials Development via Automation, Machine Learning, and High-Performance Computing},
journal = {Joule},
volume = {2},
number = {8},
pages = {1410-1420},
year = {2018},
issn = {2542-4351},
doi = {https://doi.org/10.1016/j.joule.2018.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S2542435118302289},
author = {Juan-Pablo Correa-Baena and Kedar Hippalgaonkar and Jeroen {van Duren} and Shaffiq Jaffer and Vijay R. Chandrasekhar and Vladan Stevanovic and Cyrus Wadia and Supratik Guha and Tonio Buonassisi},
keywords = {accelerated materials development, machine learning, artificial intelligence, energy materials},
abstract = {Successful materials innovations can transform society. However, materials research often involves long timelines and low success probabilities, dissuading investors who have expectations of shorter times from bench to business. A combination of emergent technologies could accelerate the pace of novel materials development by ten times or more, aligning the timelines of stakeholders (investors and researchers), markets, and the environment, while increasing return on investment. First, tool automation enables rapid experimental testing of candidate materials. Second, high-performance computing concentrates experimental bandwidth on promising compounds by predicting and inferring bulk, interface, and defect-related properties. Third, machine learning connects the former two, where experimental outputs automatically refine theory and help define next experiments. We describe state-of-the-art attempts to realize this vision and identify resource gaps. We posit that over the coming decade, this combination of tools will transform the way we perform materials research, with considerable first-mover advantages at stake.}
}
@article{KELLEY2021439,
title = {Applying Independent Core Observer Model Cognitive Architecture to a Collective Intelligence System},
journal = {Procedia Computer Science},
volume = {190},
pages = {439-449},
year = {2021},
note = {2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.06.052},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921012977},
author = {David Kelley},
keywords = {Collective Intelligence Systems, Independent Core Observer Model, Artificial General Intelligence, mediated Artificial Superintelligence, Hive Mind, AGI, ICOM, mASI.},
abstract = {This paper shows how the Independent Core Observer Model (ICOM) Cognitive Architecture for Artificial General Intelligence (AGI) can be applied to building a collective intelligence system called a mediated Artificial Superintelligence (mASI). The details include breaking down the ICOM implementation in the form of the mASI system and the general performance of initial studies with the mASI. Details of the primary difference between the Independent Core Observer Model Cognitive Architecture and the mASI architecture variant include inserting humanity in the contextual engine components of ICOM, creating a type of collective intelligence. Humans can ‘mediate’ new system-generated thinking keeping the thought process accessible and slow enough for humans to oversee and understand. This also allows the modification of emotional valences of the thought process of the mASI system to help the system generate complex contextual models (knowledge graphs) of new ideas and which speeds up the learning process. With the humans acting as control rods in a reactor and emotional drivers, the mASI system maintains safety where the system would cease to function if humans walked away.}
}
@incollection{RUFFONI201191,
title = {3.307 - Finite Element Analysis in Bone Research: A Computational Method Relating Structure to Mechanical Function},
editor = {Paul Ducheyne},
booktitle = {Comprehensive Biomaterials},
publisher = {Elsevier},
address = {Oxford},
pages = {91-111},
year = {2011},
isbn = {978-0-08-055294-1},
doi = {https://doi.org/10.1016/B978-0-08-055294-1.00093-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780080552941000933},
author = {D. Ruffoni and G.H. {van Lenthe}},
keywords = {Bone imaging, Bone research, Computational modeling, Femur, Finite element analysis, Fracture, Hierarchical structure, Microcomputed tomography, Osteoporosis, Radius, Strength, Vertebra},
abstract = {Bone is probably the most frequently investigated biological material and finite element analysis (FEA) is the computational tool most commonly used for the analysis of bone biomechanical function. FEA has been used in bone research for more than 30years and has had a substantial impact on our understanding of the complex behavior of bone. Bone is structured in a hierarchical way covering many length scales and this chapter reflects this hierarchical organization. In particular, the focus is on the applications of FEA for understanding the relationship between bone structure and its mechanical function at specific hierarchical levels. Depending on the hierarchical level, different issues have been investigated with FEA ranging from more clinically oriented topics related to bone quality (e.g., predicting bone strength and fracture risk) to more fundamental problems dealing with the mechanical aspects of biological processes (e.g., stress and strain around osteocyte lacunae) as well as with the micromechanical behavior of bone at its ultrastructure. A better understanding of the relationship between structure and mechanical function is expected to be important for the current trends in (bio)materials design, where the structure of biological materials is considered as a possible source of inspiration, as well as for more successful approaches in the prevention and treatment of age- and disease-related fractures.}
}