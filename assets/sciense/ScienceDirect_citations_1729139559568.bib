@article{SUN2019104141,
title = {Formal system interactive failure analysis method based on systems theoretic process analysis model},
journal = {Engineering Failure Analysis},
volume = {106},
pages = {104141},
year = {2019},
issn = {1350-6307},
doi = {https://doi.org/10.1016/j.engfailanal.2019.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S1350630718314973},
author = {Rui Sun and Deming Zhong and Weigang Li},
keywords = {Engine failures, Aircraft failures, Failure analysis},
abstract = {Interactive failures are failures caused by two or more components that often occur in complex systems when the system is modified, upgraded, or simply designed inadequately. However, the official guideline provided, namely, common cause analysis, cannot discover these problems. It cannot establish a complex interactive system model, nor can it provide a unified analysis method for all parts of the system. Another method, systems theoretic process analysis, is limited to the control system. To solve this problem, a method called system theoretic formal analysis method (STFAM) is proposed in this paper. STFAM establishes a system-component-interactive model that provides an abundance of interactive information for failure analysis and presents a unified model to support the analysis of multiple components in the system. It is divided into three steps. First, a hierarchical system structure is built and then transformed into a formalized state machine. Next, the interactive failures are determined and converted into a linear temporal logic or computation tree logic model. Finally, NuSMV is used to verify the model and record the results. To evaluate the proposed method, a practical problem that occurred in full-authority digital engine control, in which in some cases, the valve closes for unknown reasons until the system is reset is presented. An analysis of the issue demonstrates the effectiveness of our method.}
}
@article{ZHANG2024100064,
title = {How far has the globe gone in achieving One Health? Current evidence and policy implications based on global One Health index},
journal = {Science in One Health},
volume = {3},
pages = {100064},
year = {2024},
issn = {2949-7043},
doi = {https://doi.org/10.1016/j.soh.2024.100064},
url = {https://www.sciencedirect.com/science/article/pii/S2949704324000039},
author = {Qiyu Zhang and Jingshu Liu and Lefei Han and Xinchen Li and Chensheng Zhang and Zhaoyu Guo and Anqi Chao and Chenxi Wang and Erya Wan and Fumin Chen and Hanqing Zhao and Jiaxin Feng and Jingbo Xue and Lulu Huang and Jin Chen and Zhishan Sun and Zile Cheng and Jingxian Yin and Zhengze He and Liangyu Huang and Logan Wu and Siwei Fei and Siyu Gu and Tiange Jiang and Tianyun Li and Weiye Chen and Nan Zhou and Ne Qiang and Qin Li and Runchao He and Yi Zhang and Min Li and Xiangcheng Wang and Kokouvi Kassegne and Yongzhang Zhu and Leshan Xiu and Qinqin Hu and Kun Yin and Shang Xia and Shizhu Li and Zhaojun Wang and Xiaokui Guo and Xiaoxi Zhang and Xiao-Nong Zhou},
keywords = {Global One Health index (GOHI), Zoonotic diseases, Antimicrobial resistance, Food security, Climate change},
abstract = {Background
In the 21st century, as globalization accelerates and global public health crises occur, the One Health approach, guided by the holistic thinking of human-animal-environment and emphasizing interdisciplinary collaboration to address global health issues, has been strongly advocated by the international community. An immediate requirement exists for the creation of an assessment tool to foster One Health initiatives on both global and national scales.
Methods
Built upon extensive expert consultations and dialogues, this follow-up study enhances the 2022 global One Health index (GOHI) indicator system. The GOHI framework is enriched by covering three indices, e.g. external drivers index (EDI), intrinsic drivers index (IDI), and core drivers index (CDI). The comprehensive indicator system incorporates 13 key indicators, 50 indicators, and 170 sub I-indicators, utilizing a fuzzy analytic hierarchy process to ascertain the weight for each indicator. Weighted and summed, the EDI, IDI, and CDI scores contribute to the computation of the overall GOHI 2022 score. By comparing the ranking and the overall scores among the seven regions and across 160 countries/territories, we have not only derived an overall profile of the GOHI 2022 scores, but also assessed the GOHI framework. We also compared rankings of indicators and sub I-indicators to provide greater clarity on the strengths and weaknesses of each region within the One Health domains.
Results
The GOHI 2022 performance reveals significant disparities between countries/territories ranged from 39.03 to 70.61. The global average score of the GOHI 2022 is 54.82. The average score for EDI, IDI, and CDI are 46.57, 58.01, and 57.25, respectively. In terms of global rankings, countries from North America, Europe and Central Asia, East Asia and Pacific present higher scores. In terms of One Health domains of CDI, the lowest scores are observed in antimicrobial resistance (median: 43.09), followed by food security (median: 53.78), governance (median: 54.77), climate change (median: 64.12) and zoonotic diseases (median: 69.23). Globally, the scores of GOHI vary spatially, with the highest score in North America while lowest in sub-Saharan Africa. In addition, evidence shows associations between the socio–demographic profile of countries/territories and their GOHI performance in certain One Health scenarios.
Conclusion
The objective of GOHI is to guide impactful strategies for enhancing capacity building in One Health. With advanced technology and an annually updated database, intensifying efforts to refine GOHI's data-mining methodologies become imperative. The goal is to offer profound insights into disparities and progressions in practical One Health implementation, particularly in anticipation of future pandemics.}
}
@article{DONKERS2023100275,
title = {De-sounding echo chambers: Simulation-based analysis of polarization dynamics in social networks},
journal = {Online Social Networks and Media},
volume = {37-38},
pages = {100275},
year = {2023},
issn = {2468-6964},
doi = {https://doi.org/10.1016/j.osnem.2023.100275},
url = {https://www.sciencedirect.com/science/article/pii/S2468696423000344},
author = {Tim Donkers and Jürgen Ziegler},
keywords = {Social polarization, Echo chambers, Filter bubbles, Opinion dynamics, Social media, Machine learning, Latent space models, Recommender systems, Agent-based modeling, Knowledge-graph embedding},
abstract = {As online social networks have become dominant platforms for public discourse worldwide, there is growing anecdotal evidence of a concurrent rise in social antagonisms. Yet, while the increase in polarization is evident, the extent to which these digital communication ecosystems are driving this shift remains elusive. A dominant scholarly perspective suggests that digital social media lead to the compartmentalization of information channels, potentially culminating in the emergence of echo chambers. However, a growing body of empirical research suggests that the mechanisms influencing ideological demarcation are more complex than a complete communicative decoupling of user groups. This study introduces two intertwined principles that elucidate the dynamics of digital communication: First, socio-cognitive biases of social group formation enforce internal congruence of ideological communities by demarcation from outsiders. Second, algorithmic personalization of content contributes to ideological network formation by creating social redundancy, wherein the same individuals frequently interact in various roles, such as authors, recipients, or disseminators of messages, leading to a surplus of shared ideological fragments. Leveraging these insights, we pioneer a computational simulation model, integrating machine learning based on behavioral data and established recommendation technologies, to explore the evolution of social network structures in digital exchanges. Utilizing advanced methods in opinion dynamics, our model uniquely captures both the algorithmic delivery and the subsequent dissemination of messages by users. Our findings reveal that in ambiguous debate scenarios, the dual components of our model are essential to accurately capture the emergence of social polarization. Consequently, our model offers a forward-looking perspective on the evolution of network communication, facilitating nuanced comparisons with empirical graph benchmarks.}
}
@article{UMEREZ2001159,
title = {Howard Pattee's theoretical biology — a radical epistemological stance to approach life, evolution and complexity},
journal = {Biosystems},
volume = {60},
number = {1},
pages = {159-177},
year = {2001},
issn = {0303-2647},
doi = {https://doi.org/10.1016/S0303-2647(01)00114-9},
url = {https://www.sciencedirect.com/science/article/pii/S0303264701001149},
author = {Jon Umerez},
keywords = {Epistemological stance, Epistemic cut, Semantic closure, Code, Symbol},
abstract = {This paper offers a short review of Pattee's main contributions to science and philosophy. With no intention of being exhaustive, an account of Pattee's work is presented which discusses some of his ideas and their reception. This is done through an analysis centered in what is thought to be his main contribution: the elaboration of an internal epistemic stance to better understand life, evolution and complexity. Having introduced this core idea as a sort of a posteriori cohesive element of a complex but highly coherent and complete system of thinking, further specific elements are also reviewed.}
}
@article{VENKATESWARLU20201093,
title = {Modeling and fabrication of catalytic converter for emission reduction},
journal = {Materials Today: Proceedings},
volume = {33},
pages = {1093-1099},
year = {2020},
note = {International Conference on Future Generation Functional Materials and Research 2020},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.07.125},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320352159},
author = {K. Venkateswarlu and Revuri Ajay Kumar and Ram Krishna and M. Sreenivasan},
keywords = {Finite element analysis, Catalytic converter, CFD analysis, Thermal analysis, CO, HC, NOx},
abstract = {Automobiles throughout the world are the primary consumers of fossil fuels, which emit toxic gases when burnt; including HC, CO and NOx. Catalytic converters were developed to detoxify these gases into less harmful gases such as carbon dioxide and H2O.In this paper, the development of a catalytic converter for efficient emission reduction is presented. The results are presented after performing Computational Fluid Dynamics (CFD) on the proposed catalytic converter. In this catalytic converter Two Different materials used they are stainless steel wire mesh and ceramic stones at time and we are conducted tests with catalytic converter at different blended fluids and engine speeds such as methane, ethane and 1700,1900 RPM.3D modeling done by CATIA parametric software And analysis done in ANSYS software.}
}
@article{XIAO2021116456,
title = {Global sensitivity analysis of a CaO/Ca(OH)2 thermochemical energy storage model for parametric effect analysis},
journal = {Applied Energy},
volume = {285},
pages = {116456},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.116456},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921000222},
author = {Sinan Xiao and Timothy Praditia and Sergey Oladyshkin and Wolfgang Nowak},
keywords = {Thermochemical energy storage, Variance-based sensitivity analysis, Regional sensitivity analysis, Polynomial chaos expansion},
abstract = {Simulation models have been widely used for thermochemical energy storage to better understand its behavior and consequently to improve operational control of the device. However, incomplete knowledge of system properties leads to a significant number of uncertain parameters in the simulation models, which in turn cause uncertainties in system predictions. In this work, we perform global sensitivity analysis to identify the effect of uncertain parameters on the outputs of a thermochemical energy storage model, so that we can better understand the predictive uncertainties, proceed with targeted data acquisition or even simplify the corresponding uncertainty quantification. To get reliable sensitivity analysis results, we use both variance-based and regional sensitivity analysis since they focus on different probabilistic features of model outputs. Since the simulation model is computationally expensive, we use model reduction via the (arbitrary) polynomial chaos expansion. Then, to further confirm the results, the regional sensitivity index is also estimated based on the original model with the same given sample set. Based on the results of both sensitivity analysis methods, we can find that there are 8 unimportant parameters among 16 analyzed parameters. Thus, we can focus resources on investigating the important uncertain parameters. Also, we can ignore the uncertainty of unimportant parameters to simplify the corresponding uncertainty quantification.}
}
@article{RODRIGUEZPLANAS2022429,
title = {Gender norms in high school: Impacts on risky behaviors from adolescence to adulthood},
journal = {Journal of Economic Behavior & Organization},
volume = {196},
pages = {429-456},
year = {2022},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2022.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S016726812200021X},
author = {Nuria Rodríguez-Planas and Anna Sanz-de-Galdeano and Anastasia Terskaya},
keywords = {Gender norms, short-, medium- and long-run effects, risky behaviors and labor market outcomes, Add health},
abstract = {Engagement in risky behaviors is traditionally more prevalent among males than females, and the gap increases as youths move from adolescence to adulthood. Using the National Longitudinal Study of Adolescent to Adult Health, we identify a causal effect of exposure to high-school grade-mates with mothers who think that important skills for both boys and girls to possess are traditionally masculine ones (such as to think for oneself or work hard) as opposed to traditionally feminine ones (namely, to be well-behaved, popular, or help others) on the gender gap in teenagers’ engagement in risky behaviors. We find that a higher proportion of grade-mates’ mothers with non-traditional or non-stereotypical gender views who believe that independent thinking and working hard matter for either gender is associated with a reduction of the gender gap in risky behaviors both in the short and medium run. These results are driven by males curbing risky behaviors, suggesting that the relaxation of gender stereotypes results in boys behaving “more like girls”. In the long run, being exposed to grade-mates whose mothers have non-stereotypical gender beliefs reduces the gender gap in labor market outcomes by improving women's performance. This evidence, together with our exploration of several potential mechanisms, suggests that the transmission of gender norms is driving our results.}
}
@article{KUMAR2023176,
title = {Variance-capturing forward-forward autoencoder (VFFAE): A forward learning neural network for fault detection and isolation of process data},
journal = {Process Safety and Environmental Protection},
volume = {178},
pages = {176-194},
year = {2023},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2023.07.083},
url = {https://www.sciencedirect.com/science/article/pii/S0957582023006857},
author = {Deepak Kumar and Umang Goswami and Hariprasad Kodamana and Manojkumar Ramteke and Prakash Kumar Tamboli},
keywords = {Forward-Forward Algorithm, Fault Detection and Isolation, Real-time update, Tennessee Eastman Process},
abstract = {Data-driven models have emerged as popular choices for fault detection and isolation (FDI) in process industries. However, real-time updating of these models due to streaming data requires significant computational resources, is tedious and therefore pauses difficulty in fault detection. To address this problem, in this study, we have developed a novel forward-learning neural network framework that can efficiently update data-driven models in real time for high-frequency data without compromising the accuracy. The neural network parameters are updated using a suitably constructed forward-forward learning algorithm instead of the traditional backpropagation algorithm. Firstly, we develop a variance-capturing forward-forward autoencoder (VFFAE) for FDI. Further, we showcase that the previously trained VFFAE model can be quickly adapted to incoming data which demonstrate the efficacy of the proposed framework. We have three process case studies to validate the proposed approach, namely, the Tennesse-Eastman dataset, nuclear power flux dataset, and wastewater plant dataset, to validate the proposed approach. Our findings demonstrate that within the initial 90 s, the model underwent 90 updates using a forward-forward approach and only 10 updates using backpropagation-based methods without compromising accuracy. This highlights the model’s capacity to effectively handle streaming data during the modeling process.}
}
@article{EGELAND2024101108,
title = {Making sense of the modularity debate},
journal = {New Ideas in Psychology},
volume = {75},
pages = {101108},
year = {2024},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2024.101108},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X24000369},
author = {Jonathan Egeland},
keywords = {Cognition, Evolutionary psychology, Modularity, Levels of analysis, Scientific explanation},
abstract = {For several decades scientists and philosophers studying how the mind works have debated the issue of modularity. Their main disagreements concern the massive modularity hypothesis, according to which all (or most) of our cognitive mechanisms are modular in nature. Pietraszewski and Wertz (2022) have recently suggested that the modularity debate is based on a confusion about the levels of analysis at which the mind can be explained. This article argues that their position suffers from three major problems: (1) the argument is unsound, with untrue premises; (2) it glosses over important empirical issues; and (3) the guidelines it offers are not sufficient for avoiding future confusions. As these criticisms are developed, this article will provide a way of making sense of the modularity debate—with an eye for what really is at stake both conceptually and empirically—and, by identifying a false assumption often shared by proponents and opponents of the massive modularity hypothesis alike, it will sketch out some guidelines for moving the debate forward.}
}
@article{INGLIS2022104424,
title = {From viewsheds to viewscapes: Trends in landscape visibility and visual quality research},
journal = {Landscape and Urban Planning},
volume = {224},
pages = {104424},
year = {2022},
issn = {0169-2046},
doi = {https://doi.org/10.1016/j.landurbplan.2022.104424},
url = {https://www.sciencedirect.com/science/article/pii/S0169204622000731},
author = {Nicole C. Inglis and Jelena Vukomanovic and Jennifer Costanza and Kunwar K. Singh},
keywords = {GIS, Landscape aesthetics, Landscape visibility, Line-of-sight, Visual assessment},
abstract = {The study of visibility and visual quality (VVQ) spans scientific disciplines, methods, frameworks and eras. Recent advances in line-of-sight computation and geographic information systems (GIS) have propelled VVQ research into the realm of high performance computing via a cache of geospatial tools accessible to a broad range of research disciplines. However, in the disciplines that use VVQ analysis most (archaeology, architecture, geosciences and planning), methods and terminology can vary markedly, which may encumber interdisciplinary progress. A multidisciplinary systematic review of past VVQ research is timely to assess past efforts and effectively advance the field. In this study, we summarize the state of VVQ research in a systematic review of peer-reviewed publications spanning the past two decades. Our search yielded 528 total studies, 176 of which we reviewed in depth. VVQ analysis in peer-reviewed research increased 21-fold in the last 20 years, applied primarily in archaeology and natural resources research. We found that methods, tools and study designs varied across disciplines and scales. Research disproportionately represented the Global North and primarily employed medium resolution bare-earth elevation models, despite their known limitations. We propose a framework for standardized reporting of methods that emphasizes cross-disciplinary collaboration to propel visibility research into the future.}
}
@article{PALMIERI2020106074,
title = {Dataset of active avoidance in Wistar-Kyoto and Sprague Dawley rats: Experimental data and reinforcement learning model code and output},
journal = {Data in Brief},
volume = {32},
pages = {106074},
year = {2020},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2020.106074},
url = {https://www.sciencedirect.com/science/article/pii/S2352340920309689},
author = {John Palmieri and Kevin M. Spiegler and Kevin C.H. Pang and Catherine E. Myers},
keywords = {Avoidance learning, Reinforcement learning, Neurosciences, Computational modelling, Computational biology, Strain differences, Wistar Kyoto rat},
abstract = {Data were collected from 40 Wistar-Kyoto (WKY) and 40 Sprague Dawley (SD) rats during an active escape-avoidance experiment. Footshock could be avoided by pressing a lever during a danger period prior to onset of shock. If avoidance did not occur, a series of footshocks was administered, and the rat could press a lever to escape (terminate shocks). For each animal, data were simplified to the presence or absence of lever press and stimuli in each 12-second time frame. Using the pre-processed dataset, a reinforcement learning (RL) model, based on an actor-critic architecture, was utilized to estimate several different model parameters that best characterized each rat's behaviour during the experiment. Once individual model parameters were determined for all 80 rats, behavioural recovery simulations were run using the RL model with each animal's “best-fit” parameters; the simulated behaviour generated avoidance data (percent of trials avoided during a given experimental session) that could be compared across simulated rats, as is customarily done with empirical data. The datasets representing both the experimental data and the model-generated data can be interpreted in various ways to gain further insight into rat behaviour during avoidance and escape learning. Furthermore, the estimated parameters for each individual rat can be compared across groups. Thus, possible between-strain differences in model parameters can be detected, which might provide insights into strain differences in learning. The software implementing the RL model can also be applied to or serve as a template for other experiments involving acquisition learning. Reference for Co-Submission: K.M. Spiegler, J. Palmieri, K.C.H. Pang, C.E. Myers, A reinforcement-learning model of active avoidance behavior: Differences between Sprague-Dawley and Wistar-Kyoto rats. Behav. Brain Res. (2020 Jun 22[epub ahead of print])  doi: 10.1016/j.bbr.2020.112784}
}
@article{LILI20171611,
title = {An Inverse Optimization Model for Human Resource Allocation Problem Considering Competency Disadvantage Structure},
journal = {Procedia Computer Science},
volume = {112},
pages = {1611-1622},
year = {2017},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.08.248},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917316575},
author = {Zhang Lili},
keywords = {inverse optimization, linear programming, human resource allocation, competency, evaluation according to disadvantage structure},
abstract = {Most of serious and major accidents that happened during the production procedure of process industry are caused by improper equipment operations, which is further owing to inappropriate human resources allocation and ignorance of individual competencies differences. In order to take both of competency disadvantage and adjustment requirement into consideration, we use an inverse optimization method to solve a human resource allocation problem, and furthermore, adjust equipment operating parameters to make the per-defined settings optimized, such as the total number of jobs, security-related parameters and so on.In the solving process, firstly a standard competence hierarchy system is conducted; secondly we propose an assessment method according to disadvantage structure; thirdly we use inverse optimization method to solve the problem and optimize the predefined allocation plan. Lastly, we give an example to prove its feasibility and effectiveness. In this paper a novel formulation of human resource allocation problem is proposed, in which some of main individual characteristics are considered and described mathematically, including psychology, behaviour and characteristics diged from them such as weakness. The other contribution of this paper is using inverse optimization to adjust parameters based on the given ideal allocation plan. Both of these propositions have a positive significance on promoting development and security construction for process industries.This research incorporates the academic thinking of inverse optimization, it not only puts psychology and behavior into optimization model, but also data mines weakness characteristics under the psychology and behavior data, and find a new way to introducing the weakness characteristics into decision making model. It provides a new thought for the following decision making problem, that is the ideal decision plan is known, and optimization parameters are changeable. It promotes the combining of psychology, behavior and operations research, it is good for process industries to develop in a safety and efficiency way.}
}
@article{KASTELLAKIS201519,
title = {Synaptic clustering within dendrites: An emerging theory of memory formation},
journal = {Progress in Neurobiology},
volume = {126},
pages = {19-35},
year = {2015},
issn = {0301-0082},
doi = {https://doi.org/10.1016/j.pneurobio.2014.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0301008214001373},
author = {George Kastellakis and Denise J. Cai and Sara C. Mednick and Alcino J. Silva and Panayiota Poirazi},
keywords = {Plasticity, Active dendrites, Associative memory, Synapse clustering, Synaptic tagging and capture},
abstract = {It is generally accepted that complex memories are stored in distributed representations throughout the brain, however the mechanisms underlying these representations are not understood. Here, we review recent findings regarding the subcellular mechanisms implicated in memory formation, which provide evidence for a dendrite-centered theory of memory. Plasticity-related phenomena which affect synaptic properties, such as synaptic tagging and capture, synaptic clustering, branch strength potentiation and spinogenesis provide the foundation for a model of memory storage that relies heavily on processes operating at the dendrite level. The emerging picture suggests that clusters of functionally related synapses may serve as key computational and memory storage units in the brain. We discuss both experimental evidence and theoretical models that support this hypothesis and explore its advantages for neuronal function.}
}
@article{SANCHEZ2024,
title = {Treatment of Epistemic Uncertainty in Conjunction Analysis with Dempster-Shafer Theory},
journal = {Advances in Space Research},
year = {2024},
issn = {0273-1177},
doi = {https://doi.org/10.1016/j.asr.2024.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S0273117724009347},
author = {Luis Sánchez and Massimiliano Vasile and Silvia Sanvido and Klaus Merz and Christophe Taillan},
keywords = {Space Traffic Management, Conjunction Data Message, Epistemic Uncertainty, Dempster-Shafer theory of evidence, Conjunction Assessment, Decision-making},
abstract = {The paper presents an approach to the modelling of epistemic uncertainty in Conjunction Data Messages (CDM) and the classification of conjunction events according to the confidence in the probability of collision. The approach proposed in this paper is based on Dempster-Shafer Theory (DSt) of evidence and starts from the assumption that the observed CDMs are drawn from a family of unknown distributions. The Dvoretzky–Kiefer–Wolfowitz (DKW) inequality is used to construct robust bounds on such a family of unknown distributions starting from a time series of CDMs. A DSt structure is then derived from the probability boxes constructed with DKW inequality. The DSt structure encapsulates the uncertainty in the CDMs at every point along the time series and allows the computation of the belief and plausibility in the realisation of a given probability of collision. The methodology proposed in this paper is tested on a number of real events and compared against existing practices in the European and French Space Agencies. We will show that the classification system proposed in this paper is more conservative than the approach taken by the European Space Agency but provides an added quantification of uncertainty in the probability of collision.}
}
@incollection{BOTTGE2010767,
title = {Math Instruction for Children with Special Needs},
editor = {Penelope Peterson and Eva Baker and Barry McGaw},
booktitle = {International Encyclopedia of Education (Third Edition)},
publisher = {Elsevier},
edition = {Third Edition},
address = {Oxford},
pages = {767-773},
year = {2010},
isbn = {978-0-08-044894-7},
doi = {https://doi.org/10.1016/B978-0-08-044894-7.01126-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008044894701126X},
author = {B.A. Bottge},
keywords = {Cognitive strategy instruction, Concrete-to-representations-to-abstract (CRA) sequence, Curriculum-based assessment, Enhanced anchored instruction, K-12 math instruction, Learning disabilities, Meta-cognition, Schema-based instruction},
abstract = {Students with learning disabilities in math (denoted MD) display difficulties in developing conceptual understanding of number, in computation, and in formulating correct strategies for solving problems. Often, these students have concomitant reading difficulty, which severely limits their understanding of text-based problems. While explicit instruction of basic computation skills remains important, a greater emphasis is placed on the ability to solve problems, especially as a growing number of students with MD are included in general education classrooms. This article summarizes a small sample and brief descriptions of instructional interventions that hold promise for educating students with MD.}
}
@article{ZHU2020706,
title = {Cognitive-inspired Computing: Advances and Novel Applications},
journal = {Future Generation Computer Systems},
volume = {109},
pages = {706-709},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20308384},
author = {Rongbo Zhu and Lu Liu and Maode Ma and Hongxiang Li},
keywords = {Cognitive-inspired computing, Systems, Intelligent health analysis, Security and privacy, Novel applications},
abstract = {Cognition is emerging as a new and promising methodology with the development of cognitive-inspired computing and interaction systems, which enables a large class of applications and has emerged with a significance to change our life. However, recent advances on artificial intelligence (AI), edge computing, big data, and cognitive computational theory show that multidisciplinary cognitive-inspired computing still struggles with fundamental, long-standing problems, such as computational models and decision-making mechanisms based on the neurobiological processes of the brain, cognitive sciences, and psychology. How to enhance human cognitive performance with machine learning (ML), common sense, intelligent interaction, privacy security and novel applications is worth exploring. The objective of this special issue is to report high-quality state-of-the-art research contributions that address these key aspects of cognitive-inspired computing and novel applications. By presenting a selection of papers on various topics related to cognitive-inspired computing and applications, we hope to shed light on the multiple aspects of this emerging multidisciplinary paradigm. The papers included in this issue propose solutions for cognitive-inspired systems, AI-assisted computing, intelligent health analysis, security and privacy issues, as well as novel applications.}
}
@article{VELICHKOVSKY201735,
title = {Consciousness and working memory: Current trends and research perspectives},
journal = {Consciousness and Cognition},
volume = {55},
pages = {35-45},
year = {2017},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2017.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S1053810017301654},
author = {Boris B. Velichkovsky},
keywords = {Consciousness, Working memory, Visual masking, Attentional blink, Implicit working memory},
abstract = {Working memory has long been thought to be closely related to consciousness. However, recent empirical studies show that unconscious content may be maintained within working memory and that complex cognitive computations may be performed on-line. This promotes research on the exact relationships between consciousness and working memory. Current evidence for working memory being a conscious as well as an unconscious process is reviewed. Consciousness is shown to be considered a subset of working memory by major current theories of working memory. Evidence for unconscious elements in working memory is shown to come from visual masking and attentional blink paradigms, and from the studies of implicit working memory. It is concluded that more research is needed to explicate the relationship between consciousness and working memory. Future research directions regarding the relationship between consciousness and working memory are discussed.}
}
@article{COONEY1992237,
title = {The influence of verbal protocol methods on children's mental computation},
journal = {Learning and Individual Differences},
volume = {4},
number = {3},
pages = {237-257},
year = {1992},
issn = {1041-6080},
doi = {https://doi.org/10.1016/1041-6080(92)90004-X},
url = {https://www.sciencedirect.com/science/article/pii/104160809290004X},
author = {John B. Cooney and Stephen F. Ladd},
abstract = {The purpose of this study was to investigate the validity of children's verbal reports about the cognitive processes underlying their mental arithmetic. A within-subject comparison was made with respect to the data that could be obtained with retrospective verbal report, concurrent verbal report, and no verbal report conditions. The results of the investigation indicated that children's verbal reports of strategy use may not be veridical. The source of the nonveridicality was incompleteness rather than fabrication. It was also found that immediately retrospective and concurrent verbal reports increased students' solution accuracy relative to a no verbal report condition. Thus, the primary mental operations underlying children's mental arithmetic are reactive to giving verbal reports. It was concluded that empirical checks for reactivity and refinements to protocol procedures to reveal the progression of strategy use are needed in future research.}
}
@article{KLEEBARILLAS2015455,
title = {A comparative study and validation of state estimation algorithms for Li-ion batteries in battery management systems},
journal = {Applied Energy},
volume = {155},
pages = {455-462},
year = {2015},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2015.05.102},
url = {https://www.sciencedirect.com/science/article/pii/S0306261915007357},
author = {Joaquín {Klee Barillas} and Jiahao Li and Clemens Günther and Michael A. Danzer},
keywords = {Lithium-ion battery, Battery management system, State of charge estimation, Robustness analysis, Sliding-mode observer, Kalman-based SOC estimation},
abstract = {To increase lifetime, safety, and energy usage battery management systems (BMS) for Li-ion batteries have to be capable of estimating the state of charge (SOC) of the battery cells with a very low estimation error. The accurate SOC estimation and the real time reliability are critical issues for a BMS. In general an increasing complexity of the estimation methods leads to higher accuracy. On the other hand it also leads to a higher computational load and may exceed the BMS limitations or increase its costs. An approach to evaluate and verify estimation algorithms is presented as a requisite prior the release of the battery system. The approach consists of an analysis concerning the SOC estimation accuracy, the code properties, complexity, the computation time, and the memory usage. Furthermore, a study for estimation methods is proposed for their evaluation and validation with respect to convergence behavior, parameter sensitivity, initialization error, and performance. In this work, the introduced analysis is demonstrated with four of the most published model-based estimation algorithms including Luenberger observer, sliding-mode observer, Extended Kalman Filter and Sigma-point Kalman Filter. The experiments under dynamic current conditions are used to verify the real time functionality of the BMS. The results show that a simple estimation method like the sliding-mode observer can compete with the Kalman-based methods presenting less computational time and memory usage. Depending on the battery system’s application the estimation algorithm has to be selected to fulfill the specific requirements of the BMS.}
}
@incollection{HEILMAN201319,
title = {Chapter 2 - Visual artistic creativity and the brain},
editor = {Stanley Finger and Dahlia W. Zaidel and François Boller and Julien Bogousslavsky},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {204},
pages = {19-43},
year = {2013},
booktitle = {The Fine Arts, Neurology, and Neuroscience},
issn = {0079-6123},
doi = {https://doi.org/10.1016/B978-0-444-63287-6.00002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780444632876000026},
author = {Kenneth M. Heilman and Lealani Mae Acosta},
keywords = {artistic creativity, hemispheric functions, visuospatial skills, creative innovation, divergent thinking, imagery, global and focal attention},
abstract = {Creativity is the development of a new or novel understanding—insight that leads to the expression of orderly relationships (e.g., finding and revealing the thread that unites). Visual artistic creativity plays an important role in the quality of human lives, and the goal of this chapter is to describe some of the brain mechanisms that may be important in visual artistic creativity. The initial major means of learning how the brain mediates any activity is to understand the anatomy and physiology that may support these processes. A further understanding of specific cognitive activities and behaviors may be gained by studying patients who have diseases of the brain and how these diseases influence these functions. Physiological recording such as electroencephalography and brain imaging techniques such as PET and fMRI have also allowed us to gain a better understanding of the brain mechanisms important in visual creativity. In this chapter, we discuss anatomic and physiological studies, as well as neuropsychological studies of healthy artists and patients with neurological disease that have helped us gain some insight into the brain mechanisms that mediate artistic creativity.}
}
@article{YURKOVICH2018130,
title = {Quantitative -omic data empowers bottom-up systems biology},
journal = {Current Opinion in Biotechnology},
volume = {51},
pages = {130-136},
year = {2018},
note = {Systems biology • Nanobiotechnology},
issn = {0958-1669},
doi = {https://doi.org/10.1016/j.copbio.2018.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0958166917302276},
author = {James T Yurkovich and Bernhard O Palsson},
abstract = {The large-scale generation of ‘-omic’ data holds the potential to increase and deepen our understanding of biological phenomena, but the ability to synthesize information and extract knowledge from these data sets still represents a significant challenge. Bottom-up systems biology overcomes this hurdle through the integration of disparate -omic data types, and absolutely quantified experimental measurements allow for direct integration into quantitative, mechanistic models. The human red blood cell has served as a starting point for the application of systems biology approaches and has been the focus of a recent burst of generated quantitative metabolomics and proteomics data. Thus, the red blood cell represents the perfect case study through which to examine our ability to glean knowledge from the integration of multiple disparate data types.}
}
@article{HUANG2021399,
title = {Adaptive process monitoring via online dictionary learning and its industrial application},
journal = {ISA Transactions},
volume = {114},
pages = {399-412},
year = {2021},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2020.12.046},
url = {https://www.sciencedirect.com/science/article/pii/S0019057820305656},
author = {Keke Huang and Yiming Wu and Cheng Long and Hongquan Ji and Bei Sun and Xiaofang Chen and Chunhua Yang},
keywords = {Adaptive process monitoring, Online dictionary learning, Time-varying process, Computational complexity},
abstract = {For industrial processes, one common drawback of conventional process monitoring methods is that they would make an increasing number of false alarms in cases of various factors such as catalyst deactivation, seasonal fluctuation and so forth. To address this issue, the present work proposes an online dictionary learning method, which can fulfill the process monitoring and fault diagnosis task adaptively. The proposed method would incorporate currently available information to update the dictionary and control limit, instead of keeping a fixed monitoring model. The online dictionary learning method are more superior than conventional methods. Firstly, compared with some traditional offline methods based on small amounts of historical data, the proposed method can augment train data with online dictionary updating, thus it copes with time-varying processes well. Secondly, the proposed method enjoys a lower computational complexity than the offline learning method with mass data, which is appealing in the era of industrial big data. Thirdly, the proposed method performs more reliably than the existing recursive principal component analysis-based methods because it can resolve the anomaly of principal component or non-orthogonality of eigenvectors problem which was often confronted in the recursive principal component analysis-based methods. Finally, some experiments were designed and carried out to demonstrate the advantage of the online dictionary learning.}
}
@article{BIRD2004337,
title = {Kuhn, naturalism, and the positivist legacy},
journal = {Studies in History and Philosophy of Science Part A},
volume = {35},
number = {2},
pages = {337-356},
year = {2004},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2004.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0039368104000184},
author = {Alexander Bird},
keywords = {Kuhn, Naturalism, Positivism, Preston},
abstract = {I defend against criticism the following claims concerning Thomas Kuhn: (i) there is a strong naturalist streak in The structure of scientific revolutions, whereby Kuhn used the results of a posteriori enquiry in addressing philosophical questions; (ii) as Kuhn’s career as a philosopher of science developed he tended to drop the naturalistic elements and to replace them with more traditionally philosophical, a priori approaches; (iii) at the same time there is a significant residue of positivist thought in Kuhn, which Kuhn did not recognise as such; (iv) the naturalistic elements referred to in (i) are the most original and fruitful elements of Kuhn’s thinking; (v) the positivistic elements referred to in (iii) vitiated his thought and acted as factors in preventing Kuhn from developing the naturalistic elements and from following the path taken by much subsequent philosophy of science. Preston presents an alternative reading of Kuhn which emphasizes the Wittgensteinian elements in Kuhn. I argue that this alternative view is, descriptively, poorly supported by the textual evidence and the facts of the history of philosophy of science in the twentieth century. I provide some defence of the naturalistic approach and related themes.}
}
@article{SUTTON2004503,
title = {Representation, levels, and context in integrational linguistics and distributed cognition},
journal = {Language Sciences},
volume = {26},
number = {6},
pages = {503-524},
year = {2004},
note = {Distributed cognition and integrational linguistics},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2004.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0388000104000452},
author = {John Sutton},
keywords = {Integrational linguistics, Distributed cognition, Mental representation, Reduction, Context, Memory},
abstract = {Distributed Cognition and Integrational Linguistics have much in common. Both approaches see communicative activity and intelligent behaviour in general as strongly context-dependent and action-oriented, and brains as permeated by history. But there is some tension between the two frameworks on three important issues. The majority of theorists of distributed cognition want to maintain some notions of mental representation and computation, and to seek generalizations and patterns in the various ways in which creatures like us couple with technologies, media, and other agents; many also want to offer explanations at subpersonal levels which may undercut the autonomy of personal-level accounts. In contrast, dominant views in integrational linguistics reject all invocation of representation, resist the explanatory search for similarity across contexts and moments, and see linguistics as a lay discipline which should not offer explanations in terms alien to ordinary agents. On each of these issues, I argue that integrationists could move closer to the distributed cognition framework without losing the most important aspects of their view: integrationist criticisms of mainstream or classical theories can be respected while alliances with revised cognitivist views about representation, context, and explanation are developed.}
}
@article{WOJTAK2023101148,
title = {Adaptive timing in a dynamic field architecture for natural human–robot interactions},
journal = {Cognitive Systems Research},
volume = {82},
pages = {101148},
year = {2023},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.101148},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723000761},
author = {Weronika Wojtak and Flora Ferreira and Luís Louro and Estela Bicho and Wolfram Erlhagen},
keywords = {Temporal cognition, Human–robot interactions, Neurodynamics, Adaptation, Error monitoring},
abstract = {A close temporal coordination of actions and goals is crucial for natural and fluent human–robot interactions in collaborative tasks. How to endow an autonomous robot with a basic temporal cognition capacity is an open question. In this paper, we present a neurodynamics approach based on the theoretical framework of dynamic neural fields (DNF) which assumes that timing processes are closely integrated with other cognitive computations. The continuous evolution of neural population activity towards an attractor state provides an implicit sensation of the passage of time. Highly flexible sensorimotor timing can be achieved through manipulations of inputs or initial conditions that affect the speed with which the neural trajectory evolves. We test a DNF-based control architecture in an assembly paradigm in which an assistant hands over a series of pieces which the operator uses among others in the assembly process. By watching two experts, the robot first learns the serial order and relative timing of object transfers to subsequently substitute the assistant in the collaborative task. A dynamic adaptation rule exploiting a perceived temporal mismatch between the expected and the realized transfer timing allows the robot to quickly adapt its proactive motor timing to the pace of the operator even when an additional assembly step delays a handover. Moreover, the self-stabilizing properties of the population dynamics support the fast internal simulation of acquired task knowledge allowing the robot to anticipate serial order errors.}
}
@article{YU2023119632,
title = {A graph attention network under probabilistic linguistic environment based on Bi-LSTM applied to film classification},
journal = {Information Sciences},
volume = {649},
pages = {119632},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.119632},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523012173},
author = {Bin Yu and Ruipeng Cai and Jing Zhang and Yu Fu and Zeshui Xu},
keywords = {Probabilistic linguistic term set, Bi-LSTM, Graph attention network, Film classification},
abstract = {Film reviews contain rich and complex linguistic information that can reflect the opinions and emotions of the reviewers. However, existing methods for emotion classification of film reviews rely on quantifying qualitative evaluations numerically. This approach can lead to difficulties in interpretation, information loss, and performance degradation under massive data. In this paper, we propose a novel method that utilizes a probabilistic linguistic term set (PLTS) and graph attention network (GAT) to classify films based on their emotional content in long reviews. Firstly, the Bi-directional long short-term memory (Bi-LSTM) method is used to convert film reviews into distributed emotional probabilities. This approach not only captures the emotional information in reviews, but also avoids the limitations of numerical quantification. Secondly, using PLTS to represent emotional information not only considers the relationships of linguistic features but also captures multiple emotional information simultaneously. Finally, we utilize multiple GATs to learn and aggregate the distributed emotional probabilities, enabling our method to fully perceive multiple emotional information in the reviews. Experimental results demonstrate that our method outperforms other models in classification accuracy on the IMDB film review dataset. Our method emulates human thinking to analyze emotional information in reviews and uses a human-like attention mechanism to learn the interrelationship between emotions in film reviews. Therefore, our method exhibits significant improvements in both accuracy and interpretability compared to current models, making it applicable to diverse domains that necessitate the analysis of linguistic data. Overall, the proposed method in this paper presents a novel and effective approach to analyzing and classifying films based on linguistic reviews.}
}
@article{SOHAIL201947,
title = {A videographic assessment of ferrofluid during magnetic drug targeting: An application of artificial intelligence in nanomedicine},
journal = {Journal of Molecular Liquids},
volume = {285},
pages = {47-57},
year = {2019},
issn = {0167-7322},
doi = {https://doi.org/10.1016/j.molliq.2019.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S0167732219315399},
author = {Ayesha Sohail and Maryam Fatima and Rahamt Ellahi and Khush Bakhat Akram},
keywords = {Ferrofluids, Drug targeting, Artificial intelligence, Videographic footage},
abstract = {Forecasting the thresholds via the computational analysis of magnetic drug targeting, is a useful approach since it can help to design the nanoscale experiments to get the best results and efficiency. In such investigations, an artificial intelligence when interlinked with the computational techniques provide better insight specially for rheological problems. In the proposed model mathematical framework for the magnetic drug targeting is adopted while the flow of the ferrofluid, with different concentrations is taken into account. The flow without any obstruction is compared with the flow having obstruction. The nanoscale dynamics sensitive to such obstructions are documented by videographic footage. Nanaoscale approach and the response of the nanomedicine relative to external agents are used. The pressure gradient, the magnetic susceptibility and the velocity profile of the ferrofluid provides useful thresholds to identify the geometry of the obstacle, and to forecast the resulting dynamics.}
}
@article{DAVIES2024100156,
title = {Digital learning, face-to-face learning and climate change},
journal = {Future Healthcare Journal},
volume = {11},
number = {3},
pages = {100156},
year = {2024},
issn = {2514-6645},
doi = {https://doi.org/10.1016/j.fhj.2024.100156},
url = {https://www.sciencedirect.com/science/article/pii/S2514664524015467},
author = {David Liam Davies and AbdulAzeez Lawal and Angela E. Orji and Chloe Tytherleigh and Kieran Walsh},
keywords = {Digital learning, Medical education, Climate change},
abstract = {Debates about digital learning, face-to-face learning and blended learning often focus on their effectiveness in achieving a few core educational outcomes. The cost or convenience of using different methods to achieve certain outcomes have increasingly come into the educational framework over the past two decades. However, only rarely do educators or learners consider the climate footprint of their various activities. This is an important shortcoming, as all learning activities can contribute to our overall climate footprint. Providers of education should do their best to minimise the carbon footprint associated with their learning. But learners also have responsibility to ensure that how they access learning is also associated with minimal environmental cost. Both providers and learners should focus on activities that are likely to have the greatest impact. This is relevant both to face-to-face education and digital learning.}
}
@article{IZMALKOV2011121,
title = {Perfect implementation},
journal = {Games and Economic Behavior},
volume = {71},
number = {1},
pages = {121-140},
year = {2011},
note = {Special Issue In Honor of John Nash},
issn = {0899-8256},
doi = {https://doi.org/10.1016/j.geb.2010.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0899825610000758},
author = {Sergei Izmalkov and Matt Lepinski and Silvio Micali},
keywords = {Mechanism design, Trust, Privacy},
abstract = {Privacy and trust affect our strategic thinking, yet have not been precisely modeled in mechanism design. In settings of incomplete information, traditional implementations of a normal-form mechanism—by disregarding the players' privacy, or assuming trust in a mediator—may fail to reach the mechanism's objectives. We thus investigate implementations of a new type. We put forward the notion of a perfect implementation of a normal-form mechanism M: in essence, a concrete extensive-form mechanism exactly preserving all strategic properties of M, without relying on trusted mediators or violating the players' privacy. We prove that any normal-form mechanism can be perfectly implemented by a verifiable mediator using envelopes and an envelope-randomizing device. Differently from a trusted mediator, a verifiable one only performs prescribed public actions, so that everyone can verify that he is acting properly, and that he never learns any information that should remain private.}
}
@article{VANDECRUYS2021107,
title = {Mental distress through the prism of predictive processing theory},
journal = {Current Opinion in Psychology},
volume = {41},
pages = {107-112},
year = {2021},
note = {Psychopathology},
issn = {2352-250X},
doi = {https://doi.org/10.1016/j.copsyc.2021.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S2352250X21001056},
author = {Sander {Van de Cruys} and Pieter {Van Dessel}},
keywords = {Predictive processing, Mental distress, Psychopathology, Emotion, Depression, Anxiety, Active inference, Addiction, Learning, Psychotherapy, Computational psychiatry},
abstract = {Summary
We review the predictive processing theory’s take on goals and affect, to shed new light on mental distress and how it develops into psychopathology such as in affective and motivational disorders. This analysis recovers many of the classical factors known to be important in those disorders, like uncertainty and control, but integrates them in a mechanistic model of adaptive and maladaptive cognition and behavior. We derive implications for treatment that have so far remained underexposed in existing predictive processing accounts of mental disorder, specifically with regard to the model-dependent construction of value, the importance of model validation (evidence), and the introduction and learning of new, adaptive beliefs that relieve suffering.}
}
@article{OITAVEM2011661,
title = {A recursion-theoretic approach to NP},
journal = {Annals of Pure and Applied Logic},
volume = {162},
number = {8},
pages = {661-666},
year = {2011},
issn = {0168-0072},
doi = {https://doi.org/10.1016/j.apal.2011.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S016800721100011X},
author = {I. Oitavem},
keywords = {Computational complexity, Implicit characterization, Recursion schemes, NP},
abstract = {An implicit characterization of the class NP is given, without using any minimization scheme. This is the first purely recursion-theoretic formulation of NP.}
}
@incollection{VODOVOTZ201563,
title = {Chapter 3.2 - Dynamic Knowledge Representation and the Power of Model Making},
editor = {Yoram Vodovotz and Gary An},
booktitle = {Translational Systems Biology},
publisher = {Academic Press},
address = {Boston},
pages = {63-68},
year = {2015},
isbn = {978-0-12-397884-4},
doi = {https://doi.org/10.1016/B978-0-12-397884-4.00009-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780123978844000094},
author = {Yoram Vodovotz and Gary An},
keywords = {Systems biology, mathematical modeling, computational biology, computational modeling, knowledge representation, conceptual model},
abstract = {This chapter focuses on describing the primary tool used in Translational Systems Biology: dynamic computational modeling. This chapter discusses the conceptual basis and rationale for modeling, with particular emphasis on the role of dynamic computational and mathematical models in biomedical research. We introduce the concept of using models as means of Dynamic Knowledge Representation, with the scientific target of facilitating the visualization, instantiation, evaluation, and falsification of biological hypotheses. We compare and contrast the use of modeling and simulation for this purpose versus the development and use of “engineering grade” quantitative models, noting specifically that given the state of biological knowledge, biomedical Dynamic Knowledge Representation is aimed at facilitating discovery, as opposed to the engineering goal of optimizing solutions. We discuss the fundamental step in model construction, mapping, and explain its role in the use and potential interpretation of both biological proxy models and computational models. We introduce the concept of Conceptual Model Verification, and its role as a means of accelerating the Scientific Cycle.}
}
@article{MA2019367,
title = {An efficient method to compute different types of generalized inverses based on linear transformation},
journal = {Applied Mathematics and Computation},
volume = {349},
pages = {367-380},
year = {2019},
issn = {0096-3003},
doi = {https://doi.org/10.1016/j.amc.2018.12.064},
url = {https://www.sciencedirect.com/science/article/pii/S0096300318311251},
author = {Jie Ma and Feng Gao and Yongshu Li},
keywords = {Generalized inverse, Linear transformation, Rational matrix, MATHEMATICA},
abstract = {In this paper, we present functional definitions of all types of generalized inverses related to the {1}-inverse, which is a continuation of the work of Campbell and Meyer (2009). According to these functional definitions, we further derive novel representations for all types of generalized inverses related to the {1}-inverse in terms of the bases for R(A*), N(A) and N(A*). Based on these representations, we present the corresponding algorithm for computing various generalized inverses related to the {1}-inverse of a matrix and analyze the computational complexity of our algorithm for a constant matrix. Finally, we implement our algorithm and several known algorithms for symbolic computation of the Moore-–Penrose inverse in the symbolic computational package MATHEMATICA and compare their running times. Numerical experiments show that our algorithm outperforms these known algorithms when applied to compute the Moore–Penrose inverse of one-variable rational matrices, but is not the best choice for two-variable rational matrices in practice.}
}
@article{MARTIN2023120366,
title = {An energy future beyond climate neutrality: Comprehensive evaluations of transition pathways},
journal = {Applied Energy},
volume = {331},
pages = {120366},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.120366},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922016233},
author = {Nick Martin and Laura Talens-Peiró and Gara Villalba-Méndez and Rafael Nebot-Medina and Cristina Madrid-López},
keywords = {Sustainable energy transition, Renewable energy, Energy modelling, Integrated assessment, Life cycle assessment, Critical raw materials},
abstract = {Many of the long-term policy decisions surrounding the sustainable energy transition rely on models that fail to consider environmental impacts and constraints beyond direct greenhouse gas emissions and land occupation. Such assessments offer incomplete and potentially misleading information about the true sustainability issues of transition pathways. Meanwhile, although decision-makers desire greater access to a broader range of environmental, material and socio-economic indicators, few tools currently address this gap. Here, we introduce ENBIOS, a framework that integrates a broader range of such indicators into energy modelling and policymaking practices. By calculating sustainability-related indicators across hierarchical levels, we reach deeper understandings of the potential energy systems to be derived. With ENBIOS, we analyse a series of energy pathways designed by the Calliope energy system optimization model for the European energy system in 2030 and 2050. Although overall emissions will drop significantly, considerable rises in land, labour and critical raw material requirements are likely. These outcomes are further reflected in unfavourable shifts in key metabolic indicators during this period; energy metabolic rate of the system will drop by 25.6%, land requirement-to-energy will quadruple, while the critical raw material supply risk-to-energy ratio will rise by 74.2%. Heat from biomass and electricity from wind and solar are shown to be the dominant future processes across most indicator categories.}
}
@incollection{PIOT2006163,
title = {Gross, Maurice (1934–2001)},
editor = {Keith Brown},
booktitle = {Encyclopedia of Language & Linguistics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {163-164},
year = {2006},
isbn = {978-0-08-044854-1},
doi = {https://doi.org/10.1016/B0-08-044854-2/05135-X},
url = {https://www.sciencedirect.com/science/article/pii/B008044854205135X},
author = {M. Piot},
keywords = {comparative linguistics, computational linguistics, computer dictionaries, French language, linguistic theory, machine translation, mathematical models, syntax-based lexicon},
abstract = {Gross, Maurice (1934–2001) was a pioneer thinker in the field of modern linguistics. Long before computers could facilitate large-scale, lexically-based language study, he built an exhaustive, empirically based inventory of the ‘lexicon-grammar’ of French: the world's first lexical grammar. Since then, researchers in other countries have adopted the Gross model of description, which serves as a computational model for any language.}
}
@article{WENZLAFF200127,
title = {Mental control after dysphoria: Evidence of a suppressed, depressive bias},
journal = {Behavior Therapy},
volume = {32},
number = {1},
pages = {27-45},
year = {2001},
issn = {0005-7894},
doi = {https://doi.org/10.1016/S0005-7894(01)80042-3},
url = {https://www.sciencedirect.com/science/article/pii/S0005789401800423},
author = {Richard M. Wenzlaff and Ann R. Eisenberg},
abstract = {Previous research has generally failed to find persistent negative thinking following a depressive episode, suggesting that negative thoughts may simply be by-products of the emotional disturbance. The present research examined the idea that a persistent depressive bias does exist, but it is obscured by thought suppression. Mental control theory suggests that suppressed thoughts can be detected by assessing cognition before the effortful process of distraction is implemented. To test this prediction, formerly dysphoric, chronically dysphoric, and nondysphoric control groups interpreted audio recordings of words—some of which included homophones with emotional alternatives relevant to depression (e.g., weak/week). Participants wrote down each word either immediately or after a 10-sec delay. Although formerly dysphoric individuals did not display a depressive bias in the delayed condition, their immediate responses revealed a depressive bias. As predicted, the emergence of a negative bias was associated with high levels of chronic thought suppression.}
}
@article{LIU2024110207,
title = {Non-negative Tucker decomposition with graph regularization and smooth constraint for clustering},
journal = {Pattern Recognition},
volume = {148},
pages = {110207},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110207},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323009044},
author = {Qilong Liu and Linzhang Lu and Zhen Chen},
keywords = {Non-negative Tucker decomposition, Graph regularization, Randomized algorithm, Clustering},
abstract = {Non-negative Tucker decomposition (NTD) and its graph regularized extensions are the most popular techniques for representing high-dimensional non-negative data, which are typically found in a low-dimensional sub-manifold of ambient space, from a geometric perspective. Therefore, the performance of the graph-based NTD methods relies heavily on the low-dimensional representation of the original data. However, most existing approaches treat the last factor matrix in NTD as a low-dimensional representation of the original data. This treatment leads to the loss of the original data’s multi-linear structure in the low-dimensional subspace. To remedy this defect, we propose a novel graph regularized Lp smooth NTD (GSNTD) method for high-dimensional data representation by incorporating graph regularization and an Lp smoothing constraint into NTD. The new graph regularization term constructed by the product of the core tensor and the last factor matrix in NTD, and it is used to uncover hidden semantics while maintaining the intrinsic multi-linear geometric structure of the data. The addition of the Lp smoothing constraint to NTD may produce a more accurate and smoother solution to the optimization problem. The update rules and the convergence of the GSNTD method are proposed. In addition, a randomized variant of the GSNTD algorithm based on fiber sampling is proposed. Finally, the experimental results on four standard image databases show that the proposed method and its randomized variant have better performance than some other state-of-the-art graph-based regularization methods for image clustering.}
}
@article{FINOTTO201385,
title = {Hybrid fuzzy-genetic system for optimising cabled-truss structures},
journal = {Advances in Engineering Software},
volume = {62-63},
pages = {85-96},
year = {2013},
note = {Special Issue dedicated to Professor Zden ek Bittnar on the occasion of his Seventieth Birthday: Part I},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2013.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S0965997813000513},
author = {V.C. Finotto and W.R.L. {da Silva} and M. Valášek and P. Štemberk},
keywords = {Hybrid system, Structural optimisation, Cabled-truss, Fuzzy logic, Genetic algorithm, Nonlinear finite element analysis},
abstract = {This paper demonstrates an application of a hybrid fuzzy-genetic system in the optimisation of lightweight cabled-truss structures. These structures are described as a system of cables and triangular bar formations jointed at their ends by hinged connections to form a rigid framework. The optimised lightweight structure is determined through a stochastic discrete topology and sizing optimisation procedure that uses ground structure approach, nonlinear finite element analysis, genetic algorithm, and fuzzy logic. The latter is used to include expertise into the evolutionary search with the aim of filtering individuals with low survival possibility, thereby decreasing the total number of evaluations. This is desired because cables, which are inherently nonlinear elements, demand the use of iterative procedures for computing the structural response. Such procedures are computationally costly since the stiffness matrix is evaluated in each iteration until the structure is in equilibrium. Initially, the proposed system is applied to truss benchmarks. Next, the use of cables is investigated and the system’s performance is compared against genetic algorithms. The results indicate that the hybrid system considerably decreased the number of evaluations over genetic algorithms. Also, cabled-trusses showed a significant improvement in structural mass minimisation when compared with trusses.}
}
@article{XIN2023101941,
title = {Training model of practical innovative talents in polytechnic colleges based on fuzzy set and its extended modeling},
journal = {Learning and Motivation},
volume = {84},
pages = {101941},
year = {2023},
issn = {0023-9690},
doi = {https://doi.org/10.1016/j.lmot.2023.101941},
url = {https://www.sciencedirect.com/science/article/pii/S0023969023000723},
author = {Wang Xin and Yan Zhengying and Wang Sheng and Wang Lili},
keywords = {Practical and Innovative Talents, Fuzzy Set, Fuzzy Comprehensive Evaluation, Extended Modeling},
abstract = {Currently, some problems still exist in the cultivation system of innovative and entrepreneurial talents. Therefore, the education department should also continue to carry out practice on the relevant talent training mode, to determine the talent training mode most suitable for the growth of polytechnic colleges. Colleges and universities should take entrepreneurship and innovation ability as the fundamental purpose so that students are not only job seekers, but also creators. The fuzzy set and its extended modelling are crucial for developing students' innovation skills. Therefore, on this basis, this paper proposed a model for cultivating innovative talents with fuzzy sets as the core. The fuzzy comprehensive evaluation (FCE) method is one of the most widely utilized and efficient evaluation methods, which applies to multi-objective evaluation models, thus reflecting the comprehensive and complex nature of the assessment. The ultimate purpose was to derive the decision rules and help the decision-makers make better decisions. The experimental findings of this article showed that there were 3 and 4 students with strong practical ability in Model 1 and Model 2 before the experiment, and there were 8 and 33 students with strong practical ability in Model 1 and Model 2 after the experiment.}
}
@article{YUE2023101929,
title = {A study on the effectiveness of self-assessment learning system of ideological and political education for college students},
journal = {Learning and Motivation},
volume = {84},
pages = {101929},
year = {2023},
issn = {0023-9690},
doi = {https://doi.org/10.1016/j.lmot.2023.101929},
url = {https://www.sciencedirect.com/science/article/pii/S0023969023000607},
author = {Shuaiting Yue and Jie Wei and Haider Aziz and Kwen Liew},
keywords = {Ideological and Political Education, College Student, Self-assessment Learning, Internet Multi-Media, Digital Virtual Technology},
abstract = {Ideology and political education can be strengthened through legal interventions to enhance the judiciary learner approach and instill values and culture in college students' programs. Traditional methods are employed for rapid learning in ideology teaching and politics and social instruction. A self-assessment learning system allows students to evaluate their progress and the significance of changed learning criteria. Democratizing the teaching system and transforming ideological education present challenges, but Internet Multi-Media (IMM) and Digital Virtual Technology (DVT) provides innovative solutions. IMM-DVT enables comparative explorations and better comprehension of ideological and political education. It facilitates understanding of unequal social interactions and promotes the political, economic, and social sciences as essential components of ideology. Collecting knowledge through media streams and enabling interactions in teaching and learning improves educational effectiveness. Innovative ideological and cultural education strategies help to measure political development and college students' ideas resulting from individual experiences. The college's coordination of social practice projects with the examination has led to improved evaluation results for the ideological and political theory course, along with enhanced social practice competence among the students.}
}
@article{BRIZUELA2024101933,
title = {When algebra makes you smile: Playful engagement with early algebraic practices},
journal = {Learning and Instruction},
volume = {92},
pages = {101933},
year = {2024},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2024.101933},
url = {https://www.sciencedirect.com/science/article/pii/S0959475224000604},
author = {Bárbara M. Brizuela and Susanne Strachota},
keywords = {Play, Elementary school, Epistemic affect, Early algebraic practices},
abstract = {Background
The typical competitive and results-driven approach to school mathematics has traditionally been conceived as devoid of play, joy, and positive affect.
Aims
In this paper we address the following questions: What markers of positive affect are observed while students are doing early algebra? Specifically, how are students’ markers of joy related to early algebraic practices? What are the characteristics of playful stances to learning early algebra that are observed when children express positive epistemic affect?
Sample
We analyze three cases in which elementary school students engaged in tasks from an early algebra classroom teaching experiment.
Methods
Drawing from two theoretical frameworks, epistemic affect and early algebra, we conducted microgenetic analyses of lesson transcripts to identify markers of joy and early algebraic practices. We conducted frequency analyses to determine their co-occurrence.
Results
Our results indicate that children expressed joy while engaging in early algebraic practices, evidence of positive epistemic affect. We describe the aspects of each of the cases we present in terms of prior literature on playful stances to learning to further bolster our claims about the relationship between joy and engagement with the early algebraic practices.
Conclusions
We conclude that mathematical learning environments should include open opportunities for students to engage with mathematical content, with multiple entry points and ways to respond. We also conclude that early algebraic practices provide opportunities for playfully engagement and positive epistemic affect.}
}
@article{PANG201667,
title = {A hierarchical alternative updated adaptive Volterra filter with pipelined architecture},
journal = {Digital Signal Processing},
volume = {56},
pages = {67-78},
year = {2016},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2016.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S1051200416000506},
author = {Yanjie Pang and Jiashu Zhang},
keywords = {Nonlinear filter, Hierarchical pipelined structure, Alternative update mechanism, Volterra filter},
abstract = {The pipelined adaptive Volterra filters (PAVFs) with a two-layer structure constitute a class of good low-complexity filters. They can efficiently reduce the computational complexity of the conventional adaptive Volterra filter. Their major drawbacks are low convergence rate and high steady-state error caused by the coupling effect between the two layers. In order to remove the coupling effect and improve the performance of PAVFs, we present a novel hierarchical pipelined adaptive Volterra filter (HPAVF)-based alternative update mechanism. The HPAVFs with hierarchical decoupled normalized least mean square (HDNLMS) algorithms are derived to adaptively update weights of its nonlinear and linear subsections. The computational complexity of HPAVF is also analyzed. Simulations of nonlinear system adaptive identification, nonlinear channel equalization, and speech prediction show that the proposed HPAVF with different independent weight vectors in nonlinear subsection has superior performance to conventional Volterra filters, diagonally truncated Volterra filters, and PAVFs in terms of initial convergence, steady-state error, and computational complexity.}
}
@article{MAGUIRE202222,
title = {Wired for sound: The effect of sound on the epileptic brain},
journal = {Seizure: European Journal of Epilepsy},
volume = {102},
pages = {22-31},
year = {2022},
issn = {1059-1311},
doi = {https://doi.org/10.1016/j.seizure.2022.09.016},
url = {https://www.sciencedirect.com/science/article/pii/S105913112200214X},
author = {Melissa Jane Maguire},
keywords = {Epilepsy, Music, Mozart, Auditory, Ultrasound, Infrasound},
abstract = {Sound waves are all around us resonating at audible and inaudible frequencies. Our ability to hear is crucial in providing information and enabling interaction with our environment. The human brain generates neural oscillations or brainwaves through synchronised electrical impulses. In epilepsy these brainwaves can change and form rhythmic bursts of abnormal activity outwardly appearing as seizures. When two waveforms meet, they can superimpose onto one another forming constructive, destructive or mixed interference. The effects of audible soundwaves on epileptic brainwaves has been largely explored with music. The Mozart Sonata for Two Pianos in D major, K. 448 has been examined in a number of studies where significant clinical and methodological heterogeneity exists. These studies report variable reductions in seizures and interictal epileptiform discharges. Treatment effects of Mozart Piano Sonata in C Major, K.545 and other composer interventions have been examined with some musical exposures, for example Hayden's Symphony No. 94 appearing pro-epileptic. The underlying anti-epileptic mechanism of Mozart music is currently unknown, but interesting research is moving away from dopamine reward system theories to computational analysis of specific auditory parameters. In the last decade several studies have examined inaudible low intensity focused ultrasound as a neuro-modulatory intervention in focal epilepsy. Whilst acute and chronic epilepsy rodent model studies have consistently demonstrated an anti-epileptic treatment effect this is yet to be reported within large scale human trials. Inaudible infrasound is of concern since at present there are no reported studies on the effects of exposure to infrasound on epilepsy. Understanding the impact of infrasound on epilepsy is critical in an era where sustainable energies are likely to increase exposure.}
}
@article{RASHEED201686,
title = {Theoretical accounts to practical models: Grounding phenomenon for abstract words in cognitive robots},
journal = {Cognitive Systems Research},
volume = {40},
pages = {86-98},
year = {2016},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2016.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041715300310},
author = {Nadia Rasheed and Shamsudin H.M. Amin and U. Sultana and Rabia Shakoor and Naila Zareen and Abdul Rauf Bhatti},
keywords = {Grounded cognition, Symbol grounding problem, Cognitive robotics, Connectionist computation},
abstract = {This review concentrates on the issue of acquisition of abstract words in a cognitive robot with the grounding principle, from relevant theories to practical models of agents and robots. Most cognitive robotics models developed for grounding of language take inspiration from the findings of neuroscience and psychology to get the theoretical skeleton of these models. To better understand these modelling approaches, it is indispensable to work from the base (theoretical accounts) to the top (computational models). Therefore in this paper, succinct definition of abstract words is presented first, and then the symbol grounding issue and accounts of grounded cognition for abstract words are given. The next section discusses the computational modelling approaches for abstract words grounding phenomenon. Finally, important cognitive robotics models are reviewed. This paper also points out the strengths and weaknesses of relevant hypotheses and models for the representation of abstract words in the grounded cognition framework and helps the understanding of issues such as where and why modelling efforts stand to address this problem in comparison with theoretical findings.}
}
@article{ESCAMILLA2021102697,
title = {Interaction designers’ perceptions of using motion-based full-body features},
journal = {International Journal of Human-Computer Studies},
volume = {155},
pages = {102697},
year = {2021},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2021.102697},
url = {https://www.sciencedirect.com/science/article/pii/S1071581921001154},
author = {Antonio Escamilla and Javier Melenchón and Carlos Monzo and Jose Antonio Morán},
keywords = {Motion-based feature extraction, Full-body interaction, Interaction designers' perception, Designer-interpretable feature},
abstract = {Movement-based full-body interactions are increasingly being used in the design of interactive spaces, computer-mediated environments, and virtual user experiences due to the development and availability of diverse sensing technologies. In this context, the role of interaction designers is to find systematic and predictable relationships between bodily actions and the corresponding responses from technology. Sensor-based interaction design relies on sensor data analysis and higher-level feature extraction to improve detection capabilities. However, understanding human movement to inform the design of motion-based interactions is not straightforward if the detection capabilities of interaction technologies are unknown. We aim at understanding the problems and opportunities that practitioners—regardless of their technical background—perceive in using different motion-based full-body features. To achieve this, we conducted four separate focus groups with experienced practitioners, with and without technical backgrounds. We used a framework for the analysis of focus group data in information systems research to identify content areas and draw conclusions. Our findings suggest that most interaction designers, regardless of their technical background, consider motion-based feature extraction to be challenging and time-consuming. However, participants acknowledge they might use designer-interpretable features as a potential tool to foster user behavior exploration. Understanding how practitioners link sensor-based interaction design with feature extraction technology is relevant to design computational tools and reduce the technical effort required from designers to characterize the user’s movement.}
}
@article{SU2021166,
title = {Multimodal metaphor detection based on distinguishing concreteness},
journal = {Neurocomputing},
volume = {429},
pages = {166-173},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.11.051},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220318440},
author = {Chang Su and Weijie Chen and Ze Fu and Yijiang Chen},
keywords = {Metaphor, Multiple modalities, Concreteness, Part of speech},
abstract = {Metaphors are a common linguistic phenomenon, and metaphor identification plays an essential role in metaphor processing. Most existing metaphor computing techniques use only texts to gain features, but we acquire additional knowledge from other modalities. At present, the multimodal model in the metaphor field is in the exploratory stage, and the few multimodal models available are still relatively crude. We propose a multimodal metaphor detection method according to the idea that different types of words are suitable for different modality calculations. First, our proposed framework uses a fine-grained concreteness calculation method based on part of speech to distinguish abstract and concrete words. We then choose a different appropriate modal feature and a different metaphor computational method for words with different concreteness. Additionally, we also improve the use of image features in the field of metaphor detection.}
}
@article{TAKAHASHI2023112053,
title = {A fast time-domain boundary element method for three-dimensional electromagnetic scattering problems},
journal = {Journal of Computational Physics},
volume = {482},
pages = {112053},
year = {2023},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2023.112053},
url = {https://www.sciencedirect.com/science/article/pii/S0021999123001481},
author = {Toru Takahashi},
keywords = {Electromagnetic scattering, Combined field integral equation, Marching-on-in-time scheme, Boundary element method, Fast multipole method, Interpolation},
abstract = {This paper proposes a fast time-domain boundary element method (TDBEM) to solve three-dimensional transient electromagnetic scattering problems regarding perfectly electric conductors in the classical marching-on-in-time manner. The algorithm of the fast TDBEM is a time-domain variant of the interpolation-based fast multipole method (IFMM), which is similar to the time-domain IFMM for acoustic scattering problems investigated in the author's previous studies. The principle of the present IFMM is to interpolate the kernel functions of the electric and magnetic field integral equations (EFIE and MFIE, respectively) so that every kernel function is expressed in a form of separation of variables in terms of both the spatial and temporal variables. Such an expression enables to construct a fast method to evaluate the scalar and vector potentials in the EFIE and MFIE with using so-called multipole-moments and local-coefficients associated with a space-time hierarchy. As opposed to O(Ns2Nt) of the conventional TDBEM, the computational complexity of the fast TDBEM is estimated as O(Ns1+δNt), where Ns and Nt stand for the spatial and temporal degrees of freedom, respectively, and δ is typically 1/2 or 1/3. The numerical examples presented the advantages of the proposed fast TDBEM over the conventional TDBEM when solving large-scale problems.}
}
@article{WANG2020223,
title = {Anonymous data collection scheme for cloud-aided mobile edge networks},
journal = {Digital Communications and Networks},
volume = {6},
number = {2},
pages = {223-228},
year = {2020},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2019.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352864819300574},
author = {Anxi Wang and Jian Shen and Chen Wang and Huijie Yang and Dengzhi Liu},
keywords = {Cloud-aided mobile edge networks, Anonymous data collection, Communication model, Path selection},
abstract = {With the rapid spread of smart sensors, data collection is becoming more and more important in Mobile Edge Networks (MENs). The collected data can be used in many applications based on the analysis results of these data by cloud computing. Nowadays, data collection schemes have been widely studied by researchers. However, most of the researches take the amount of collected data into consideration without thinking about the problem of privacy leakage of the collected data. In this paper, we propose an energy-efficient and anonymous data collection scheme for MENs to keep a balance between energy consumption and data privacy, in which the privacy information of senors is hidden during data communication. In addition, the residual energy of nodes is taken into consideration in this scheme in particular when it comes to the selection of the relay node. The security analysis shows that no privacy information of the source node and relay node is leaked to attackers. Moreover, the simulation results demonstrate that the proposed scheme is better than other schemes in aspects of lifetime and energy consumption. At the end of the simulation part, we present a qualitative analysis for the proposed scheme and some conventional protocols. It is noteworthy that the proposed scheme outperforms the existing protocols in terms of the above indicators.}
}
@article{GAUR2023102165,
title = {Artificial intelligence for carbon emissions using system of systems theory},
journal = {Ecological Informatics},
volume = {76},
pages = {102165},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102165},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123001942},
author = {Loveleen Gaur and Anam Afaq and Gursimar Kaur Arora and Nabeel Khan},
keywords = {Artificial intelligence, machine learning, Carbon emission, System of systems theory, Sustainability, Carbon footprint},
abstract = {The impact of artificial intelligence (AI) on the environment is the subject of discourse, with arguments for both positive and negative effects. There is a fine line between AI for good and AI for environmental degradation. Today, companies want to seize the benefits of AI, which distinctively involves reducing the company's carbon footprint. However, AI's carbon emissions differ as per the techniques involved in training it. As the saying goes, a coin always has two sides. Therefore, it cannot be denied that AI can be an effective tool for combating climate change, but its role in contributing to carbon emissions cannot be ignored. Multiple studies indicate that AI could be the game-changer in staving off anthropogenic climatic changes due to the deterioration of the environment and global warming. This double-edged relationship and interdependency of AI and carbon emissions are represented through a system of systems (SoS) approach. SoS states that a plan is created through multiple smaller systems, creating complexity in the design and vice versa. A complex system can be assumed as the world in general, where two individual independent systems AI and carbon emissions, when in interaction, create a complex complementary and contradictory relation, adding to the convolution of the system. This connection is demonstrated by conducting a network analysis and calculating the carbon emissions of six machine learning (ML) algorithms and deep learning (DL) models with different datasets but the same hyperparameters on a carbon emission calculator created through AI algorithms. The primary idea of this study is to encourage the AI society to create efficient AI models that may be used without compromising environmental issues. The focus should be on practicing sustainable AI, that is, sustainability from data collection to model deployment, throughout the lifecycle of AI.}
}
@incollection{BISHT2022277,
title = {Chapter Twelve - Perceiving the level of depression from web text},
editor = {Shikha Jain and Kavita Pandey and Princi Jain and Kah Phooi Seng},
booktitle = {Artificial Intelligence, Machine Learning, and Mental Health in Pandemics},
publisher = {Academic Press},
pages = {277-298},
year = {2022},
isbn = {978-0-323-91196-2},
doi = {https://doi.org/10.1016/B978-0-323-91196-2.00008-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323911962000089},
author = {Sankalp Singh Bisht and Herumb Shandilya and Vaibhav Gupta and Shriyansh Agrawal and Shikha Jain},
keywords = {Depression, Loneliness, Mental disorder, Solitude, Suicide},
abstract = {Depression is one of the deadliest diseases found in today's world, and unfortunately, it is also one of the most ignored problems. Depression is a fact that is very hard to accept for any individual and is always a multistep process. The initial stage of Depression is Loneliness, and thus the information about these emotions can be leveraged and can help in the early detection of Depression, which in turn leads to suicidal thoughts. Tweet data analysis is one of the most popular ways to determine the presence of depression and suicidal thoughts, through the concepts of Machine Learning. Twitter proves to be a very rich source of data, as their user base is potentially large enough, but is also increasing in a fast manner. For the scope of this paper, we predicted from a user's specific tweet, which is categorized for loneliness. These tweets are analyzed to check the level of depression as moderate or severe when people start thinking of suicide. The simulation is carried out using four different models for one level of classification and eight models are used at the second level of classification. It is observed that Gated Recurrent Unit with BERT outperformed all the models and showed the accuracy of 99% and 97%. However, for class-1 recall with XLNet gave the best result with class-1 recall being 0.99. This application can help the individual in early detection of depression without any human intervention and seek medical help. Moreover, it also provides an insight about the feelings of the individual to the medical practitioners, which, in turn, can help them provide better decision-making.}
}
@article{ROBERTS2017225,
title = {Clinical Applications of Stochastic Dynamic Models of the Brain, Part II: A Review},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {2},
number = {3},
pages = {225-234},
year = {2017},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2016.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S2451902217300149},
author = {James A. Roberts and Karl J. Friston and Michael Breakspear},
keywords = {Computational neuroscience, Computational psychiatry, Epilepsy, Mathematical modeling, Melancholia, Stochastic},
abstract = {Brain activity derives from intrinsic dynamics (due to neurophysiology and anatomical connectivity) in concert with stochastic effects that arise from sensory fluctuations, brainstem discharges, and random microscopic states such as thermal noise. The dynamic evolution of systems composed of both dynamic and random fluctuations can be studied with stochastic dynamic models (SDMs). This article, Part II of a two-part series, reviews applications of SDMs to large-scale neural systems in health and disease. Stochastic models have already elucidated a number of pathophysiological phenomena, such as epilepsy and hypoxic ischemic encephalopathy, although their use in biological psychiatry remains rather nascent. Emerging research in this field includes phenomenological models of mood fluctuations in bipolar disorder and biophysical models of functional imaging data in psychotic and affective disorders. Together with deeper theoretical considerations, this work suggests that SDMs will play a unique and influential role in computational psychiatry, unifying empirical observations with models of perception and behavior.}
}
@incollection{SHERIDAN201023,
title = {Chapter 2 - The System Perspective on Human Factors in Aviation},
editor = {Eduardo Salas and Dan Maurino},
booktitle = {Human Factors in Aviation (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {23-63},
year = {2010},
isbn = {978-0-12-374518-7},
doi = {https://doi.org/10.1016/B978-0-12-374518-7.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012374518700002X},
author = {Thomas B. Sheridan},
abstract = {Publisher Summary
This chapter reviews the system perspective in terms of its origins and fundamental quantitative ideas. An appreciation of these basic concepts adds rigor to analysis and synthesis of human-machine systems, and in particular to such systems in aviation. The chapter represents an effort to remind the reader of the meaning of “system,” where it comes from, and what it implies for research, design, construction, operation, and evaluation in aviation, especially with regard to the human role in aviation. Human factors professionals, pilots, and operational personnel in air traffic management and related practitioners who know about “systems” only as a general and often vague term for something complex can benefit from knowing a bit of the history, the people, and the quantitative substance that underlies the terminology. The chapter begins by defining what is meant by a system, then discusses the history of the idea, the major contributors and what they contributed, and what made the systems idea different from previous ideas in technology. It goes on to give examples of systems thinking applied to design, development, and manufacturing of aviation systems in consideration of the people involved. Salient system models such as control, decision, information, and reliability are then explicated.}
}
@article{ZHU2021500,
title = {Low-cost Electric Bus Stability Enhancement Scheme Based on Fuzzy Torque Vectoring Differentials: Design and Hardware-in-the-loop Test},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {10},
pages = {500-507},
year = {2021},
note = {6th IFAC Conference on Engine Powertrain Control, Simulation and Modeling E-COSM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.10.212},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321016141},
author = {Shaopeng Zhu and Xiaoyan Huang and Dezhi Jiang and Zhiqiang Wu},
keywords = {torque vectoring, lateral stability, electric bus, differential control, fuzzy logic;Kalman filte},
abstract = {In this paper, a fuzzy torque vectoring differential controller is proposed to improve the lateral stability for the two-motor-wheel drive electric buses with low sensor cost and computational cost. In order to know the unmeasured sideslip angle, unscented Kalman filter algorithm is used to accurately estimate the sideslip angle based on a nonlinear electric bus model and the measured yaw rate and lateral acceleration signals. Given the advantages of high computational efficiency and human’s heuristic knowledge, fuzzy rules are newly designed to control the torque vectoring differentials, and thereby to provide an extra yaw moment on the electric bus body to compensate the driver steering aiming at stability enhancement. To validate the effectiveness and efficiency of the proposed fuzzy torque vectoring differential controller, hardware-in-the-loop tests were carried out to evaluate the controller performance in real-time under different driving maneuvers and road conditions. The test results indicate that the fuzzy torque vectoring differential controller can significantly improve the handling and path-following accuracy for the two-motor-wheel drive electric buses.}
}
@article{SZUBA2001489,
title = {A formal definition of the phenomenon of collective intelligence and its IQ measure},
journal = {Future Generation Computer Systems},
volume = {17},
number = {4},
pages = {489-500},
year = {2001},
note = {Workshop on Bio-inspired Solutions to Parallel Computing problems},
issn = {0167-739X},
doi = {https://doi.org/10.1016/S0167-739X(99)00136-3},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X99001363},
author = {Tadeusz Szuba},
keywords = {Collective intelligence, Quasi-chaotic model of computations, Synergy, IQ, PROLOG},
abstract = {This paper presents a formalization of collective intelligence (CI). A molecular, quasi-chaotic model of computations allows us to model CI in social structures, and to define its measure (IQS). This methodology works for bacterial colonies and social insects as well as for human social structures. With the CI theory some patterns of human behavior receive formal justification, others can be explained as IQS optimization. The CI formalization assumes that it is a property of a social structure, initializing when individuals interact, and as a result, acquiring the ability to solve new or more complex problems. CI amplifies if the structure improves synergy, which further increases the spectrum and complexity of the problems, which can be solved together. The formalization covers areas where CI results in physical synergy and mental/logical cooperation.}
}
@article{GAO2019146333,
title = {Coactivations of barrel and piriform cortices induce their mutual synapse innervations and recruit associative memory cells},
journal = {Brain Research},
volume = {1721},
pages = {146333},
year = {2019},
issn = {0006-8993},
doi = {https://doi.org/10.1016/j.brainres.2019.146333},
url = {https://www.sciencedirect.com/science/article/pii/S0006899319303877},
author = {Zilong Gao and Ruixiang Wu and Changfeng Chen and Bo Wen and Yahui Liu and Wei Lu and Na Chen and Jing Feng and Ruichen Fan and Dangui Wang and Shan Cui and Jin-Hui Wang},
keywords = {Associative learning, Memory cell, Neural circuit, Barrel cortex, Piriform cortex},
abstract = {After associative learning, a signal induces the recall of its associated signal, or the other way around. This reciprocal retrieval of associated signals is essential for associative thinking and logical reasoning. For the cellular mechanism underlying this associative memory, we hypothesized that the formation of synapse innervations among coactivated sensory cortices and the recruitment of associative memory cells were involved in the integrative storage and reciprocal retrieval of associated signals. Our study indicated that the paired whisker and olfaction stimulations led to an odorant-induced whisker motion and a whisker-induced olfaction response, a reciprocal form of associative memory retrieval. In mice that showed the reciprocal retrieval of associated signals, their barrel and piriform cortical neurons became mutually innervated through their axon projection and new synapse formation. These piriform and barrel cortical neurons gained the ability to encode both whisker and olfaction signals based on synapse innervations from the innate input and the newly formed input. Therefore, the associated activation of sensory cortices by pairing input signals initiates their mutual synapse innervations, and the neurons innervated by new and innate synapses are recruited to be associative memory cells that encode these associated signals. Mutual synapse innervations among sensory cortices to recruit associative memory cells may compose the primary foundation for the integrative storage and reciprocal retrieval of associated signals. Our study also reveals that new synapses onto the neurons enable these neurons to encode memories to new specific signals.}
}
@article{MOGLIA2017173,
title = {A review of Agent-Based Modelling of technology diffusion with special reference to residential energy efficiency},
journal = {Sustainable Cities and Society},
volume = {31},
pages = {173-182},
year = {2017},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2017.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S2210670716306813},
author = {Magnus Moglia and Stephen Cook and James McGregor},
keywords = {Agent-Based Modelling, Diffusion of innovation, HVAC, Lighting, Appliances},
abstract = {Residential energy efficiency is an important strategy for reducing greenhouse gas emissions. There are many technologies that help improve residential energy efficiency, and in fact, increased energy efficiency has already helped reduce global greenhouse gas emissions significantly in the past. However, with greater innovation, further improvements can be made and improving energy efficiency is an ongoing activity. Policymakers around the world are putting strategies in place to speed up the adoption of energy efficient technologies and practices, but ultimately this process is based on choice by residents themselves. Human decision making and choice however is a very complex issue, and complex computational tools are required in order to analyse and/or predict the impact of various policies. Traditionally, equation-based models such as Bass and Choice models have been used to describe the diffusion of technologies in a population, but certain limitations have been identified. This article explores what these limitations are in the context of energy efficient residential technologies and how an alternative computational and empirical paradigm, Agent-Based Modelling (ABM), can help resolve some of these limitations. As such, this is a review article into how ABM can support analysis of strategies to catalyse greater uptake of energy efficiency in the residential sector.}
}
@article{HARTLEY1997169,
title = {Semantic networks: visualizations of knowledge},
journal = {Trends in Cognitive Sciences},
volume = {1},
number = {5},
pages = {169-175},
year = {1997},
issn = {1364-6613},
doi = {https://doi.org/10.1016/S1364-6613(97)01057-7},
url = {https://www.sciencedirect.com/science/article/pii/S1364661397010577},
author = {Roger T. Hartley and John A. Barnden},
abstract = {The history of semantic networks is almost as long as that of their parent discipline, artificial intelligence. They have formed the basis of many fascinating, yet controversial, discussions in conferences and in the literature, ranging from metaphysics through to complexity theory in computer science. Many excellent surveys of the field have been written, and yet it is our belief that none of them has examined the important link between their use as a formal scheme for knowledge representation and their more heuristic use as an informal tool for thinking. In our consideration of semantic networks as computerized tools, we will discuss three levels of abstraction that we believe can help us understand how semantic networks are used. I}
}
@article{HOGAN200855,
title = {Advancing the dialogue between inner and outer empiricism: A comment on O’Nualláin},
journal = {New Ideas in Psychology},
volume = {26},
number = {1},
pages = {55-68},
year = {2008},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2007.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X07000293},
author = {Michael J. Hogan},
keywords = {Consciousness, Inner empiricism, Outer empiricism, Evolution, No-mind, Mythos, Logos},
abstract = {In a recent contribution to New Ideas in Psychology, Seán O’Nualláin draws out a distinction between inner and outer empiricism, and suggests that consciousness research can benefit from analysis in both directions, that is, via the exploration of facts and relations that facilitate a third-person understanding of consciousness (by reference to an analysis of the structures, processes, and functions of the brain) and via the direct exploration of conscious experience itself, both in terms of its computational (content filled) and non-computational (content empty) aspects. In positing a substrate of subjectivity independent of the contents of consciousness (and, more specifically, a state of “nothingness”), Ò’Nualláin follows a long tradition deeply rooted in mythical, religious, and esoteric schools of belief and practice. Although there is considerable debate amongst philosophers, psychologists, and neuroscientists as to whether or not a non-computational view of consciousness is viable, O’Nualláin accepts that such a possibility does exist. Further, he suggests that a dialogue between the inner and outer empiricists will be fruitful. In this comment I, critique Ò’Nualláin's initial thoughts on the subject and draw out a series of useful distinctions that will help to advance the dialogue between inner and outer empiricism. Critical amongst these distinctions is explicit reference to (1) ontological and epistemological interdependencies in consciousness research, and (2) states of consciousness that describe the transition from “mindfulness” through “nothingness” to “no-mind”.}
}
@article{CHIU2024100197,
title = {Future research recommendations for transforming higher education with generative AI},
journal = {Computers and Education: Artificial Intelligence},
volume = {6},
pages = {100197},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2023.100197},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X23000760},
author = {Thomas K.F. Chiu},
keywords = {Generative artificial intelligence, ChatGPT, Learning outcomes, AI literacy, Assessment},
abstract = {Higher education is crucial for producing ethical citizens and professionals globally. The introduction of generative AI (GenAI), such as ChatGPT, has posed opportunities and challenges to the traditional model of education. However, the current conversations primarily focus on policy development and assessment, with limited research on the future of higher education. GenAI's impact on learning outcomes, pedagogy, and assessment is crucial for reforming and advancing the workforce. This qualitative study aims to investigate student perspectives on GenAI's impact on higher education. The study uses an initial conceptual framework driven by a systematic literature review to investigate the opportunities and challenges of AI in education. This framework serves as an initial data collection and analysis framework. A sample of 51 students from three research-intensive universities was selected for this study. Thematic analysis identified three themes and 10 subthemes. The findings suggest that future higher education should be transformed to train students to be future-ready for employment in a society powered by GenAI. They suggest new learning outcomes—skills in learning and teaching with GenAI, AI literacy—and emphasize the significance of interdisciplinarity and maker learning, with assessment focusing on in-class and hands-on activities. They recommend six future research directions – competence for future workforce and its self-assessment measures, AI literacy or competency measures, new literacies and their relationships, interdisciplinary teaching, Innovative pedagogies and their evaluation, new assessment and its acceptance.}
}
@article{NARASIMHAN197879,
title = {Modelling behaviour: the need for a computational approach},
journal = {Journal of Social and Biological Structures},
volume = {1},
number = {1},
pages = {79-94},
year = {1978},
issn = {0140-1750},
doi = {https://doi.org/10.1016/0140-1750(78)90020-9},
url = {https://www.sciencedirect.com/science/article/pii/0140175078900209},
author = {R. Narasimhan},
abstract = {The principal objective of this paper is to argue the thesis that a science concerned with the study of behaviour requires the computational approach in a serious way for its theoretical advancement. It is pointed out that modelling behaviour requires the articulation of explanations at three levels. The methodology of computational simulations is indispensable to articulating explanations at the first level which underlie explanations at the other two levels. The paper contrasts the currently fashionable approaches in artificial intelligence studies to the kind of constraints viable behavioural models must satisfy. Teachability and open-endedness are two of the essential characteristics of organisms that any satisfactory model must have. It is argued that analogy-based computational techniques and paradigmatic learning/teaching techniques are two modelling aspects that require imaginative study.}
}
@article{INCI2025118571,
title = {Surrogate model based optimization of variable stiffness composite wingbox for improved buckling load with manufacturing and failure constraints},
journal = {Composite Structures},
volume = {351},
pages = {118571},
year = {2025},
issn = {0263-8223},
doi = {https://doi.org/10.1016/j.compstruct.2024.118571},
url = {https://www.sciencedirect.com/science/article/pii/S0263822324006998},
author = {Hasan İnci and Altan Kayran},
keywords = {Variable stiffness composite, Curvilinear fiber path, Wingbox, Buckling load, Particle swarm optimization, Whale optimization algorithm, Surrogate model, Radial basis functions},
abstract = {The buckling load of a wingbox is maximized by applying different automatic fiber placement (AFP) strategies on the skins of the wing having NACA 4412 airfoil profile. Specifically, the effect of single, dual and three region application of AFP on the optimal buckling load is studied to provide an outlook on the improvement of the optimal buckling load by increasing the number of skin panels, between rib stations, over which distinct reference fiber path definitions are made. Buckling load optimizations with manufacturing and failure constraints are performed via two different metaheuristic optimization algorithms to ensure that the global optimum reference fiber paths are obtained. For computational efficiency, separate radial basis function based surrogate models are generated for the buckling and the failure analysis. Our results show that the application of AFP separately with distinct reference fiber path definitions in each skin panel between the rib stations, the buckling load can be improved compared to the application of the AFP to a larger section of the wing skin. For the wingbox studied, compared to the wing with quasi-isotropic skins, 94% increase in the local buckling load of the root panel is obtained with the three region AFP application.}
}
@article{BAUCELLS201329,
title = {Guided decisions processes},
journal = {EURO Journal on Decision Processes},
volume = {1},
number = {1},
pages = {29-44},
year = {2013},
issn = {2193-9438},
doi = {https://doi.org/10.1007/s40070-013-0003-8},
url = {https://www.sciencedirect.com/science/article/pii/S2193943821000108},
author = {Manel Baucells and Rakesh K. Sarin},
keywords = {Decision analysis, Behavioral decision making, Narrow bracket, Insurance, Multi-attribute decisions},
abstract = {The heuristics and bias research program has convincingly demonstrated that our judgments and choices are prone to systematic errors. Decision analysis requires coherent judgments about beliefs (probabilities) and tastes (utilities), and a rational procedure to combine them so that choices maximize subjective expected utility. A guided decision process is a middle-of-the-road between decision analysis and intuitive judgments in which the emphasis is on improving decisions through simple decision rules. These rules reduce cost of thinking, or decision effort, for the myriad decisions that one faces in daily life; but at the same time, they are personalized to the individual and produce near optimal choices. We discuss the principles behind the guided decision processes research program, and illustrate the approach using several examples.}
}
@article{LANG2017298,
title = {Mesoscopic Simulation Models for Logistics Planning Tasks in the Automotive Industry},
journal = {Procedia Engineering},
volume = {178},
pages = {298-307},
year = {2017},
note = {RelStat-2016: Proceedings of the 16th International Scientific Conference Reliability and Statistics in Transportation and Communication October 19-22, 2016. Transport and Telecommunication Institute, Riga, Latvia},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.01.118},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817301182},
author = {Sebastian Lang and Tobias Reggelin and Toralf Wunder},
keywords = {automotive industry, logistics planning, production planning, mesoscopic simulation, discrete-rate simulation},
abstract = {The paper evaluates mesoscopic simulation models applied to logistics planning tasks in the automotive industry. In terms of level of detail, mesoscopic simulation models fall between object based discrete-event simulation models and flow based continuous simulation models. Mesoscopic models represent logistics flow processes on an aggregated level through piecewise constant flow rates instead of modeling individual flow objects. The results are not obtained by counting individual objects but by using mathematical formulas to calculate the results as continuous quantities in every modeling time step. This leads to a fast model creation and computation. The authors expect that mesoscopic simulation models can help to support decisions on the operational, tactical and strategic level of planning. The paper describes a mesoscopic simulation model of the goods receiving of an assembly plant and compares the simulation results and computation time with a discrete-event model.}
}
@article{CARVAJALRODRIGUEZ201576,
title = {Incorporación de la programación informática en el currículum de Biología},
journal = {Magister},
volume = {27},
number = {2},
pages = {76-82},
year = {2015},
issn = {0212-6796},
doi = {https://doi.org/10.1016/j.magis.2015.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0212679615000286},
author = {Antonio Carvajal-Rodríguez},
keywords = {Docencia, Bioinformática, Biología computacional, Python, Teaching, Bioinformatics, Computational biology, Python},
abstract = {Resumen
La investigación en biología ha cambiado radicalmente debido al efecto combinado de los avances en biotecnología y ciencias de la computación. En consecuencia, la biología computacional y la bioinformática son tan esenciales para la biología del siglo xxi como la biología molecular lo fue en el anterior. Sin embargo, las competencias correspondientes a razonamiento matemático y computacional en el currículo de Biología apenas han cambiado en los últimos 25 años. La formación del biólogo debería ser tan sofisticada desde el punto de vista computacional como la del físico o la del ingeniero. La incorporación de estos cambios requiere tanto de un mayor esfuerzo de integración de las asignaturas cuantitativas existentes en el ámbito de los problemas biológicos como de la contextualización de las asignaturas propias de la biología desde un punto de vista más formal y de modelización. En este trabajo se revisan algunos de los esfuerzos que en este sentido se están haciendo en el panorama internacional y se presenta también la experiencia del autor en el diseño e impartición de un curso de iniciación a la programación para biólogos usando una metodología de aprendizaje basado en problemas.
The joint effect of biotechnology and computing has changed the research in biology. Consequently, computational biology is as essential for 21st-century biologists as molecular biology was in the 20th. However, Biology curricula have little emphasis in quantitative thinking and computation. The education for biologists should become as sophisticated as the computational education of physicists and engineers. The necessary changes to reach this goal require the connection of mathematics and quantitative subjects with real biological problems and at the same time, teaching some biological subjects from a modeling and computational perspective. In the present work, some of the current international effort in this path is reviewed and additionally, the author's experience when teaching an introduction to programming for biologists is presented.}
}
@article{IGELSTROM201770,
title = {The inferior parietal lobule and temporoparietal junction: A network perspective},
journal = {Neuropsychologia},
volume = {105},
pages = {70-83},
year = {2017},
note = {Special Issue: Concepts, Actions and Objects: Functional and Neural Perspectives},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2017.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0028393217300015},
author = {Kajsa M. Igelström and Michael S.A. Graziano},
keywords = {Angular gyrus, Supramarginal gyrus, Ventral parietal cortex, Posterior superior temporal sulcus, Internal cognition, Frontoparietal executive control network},
abstract = {Information processing in specialized, spatially distributed brain networks underlies the diversity and complexity of our cognitive and behavioral repertoire. Networks converge at a small number of hubs – highly connected regions that are central for multimodal integration and higher-order cognition. We review one major network hub of the human brain: the inferior parietal lobule and the overlapping temporoparietal junction (IPL/TPJ). The IPL is greatly expanded in humans compared to other primates and matures late in human development, consistent with its importance in higher-order functions. Evidence from neuroimaging studies suggests that the IPL/TPJ participates in a broad range of behaviors and functions, from bottom-up perception to cognitive capacities that are uniquely human. The organization of the IPL/TPJ is challenging to study due to the complex anatomy and high inter-individual variability of this cortical region. In this review we aimed to synthesize findings from anatomical and functional studies of the IPL/TPJ that used neuroimaging at rest and during a wide range of tasks. The first half of the review describes subdivisions of the IPL/TPJ identified using cytoarchitectonics, resting-state functional connectivity analysis and structural connectivity methods. The second half of the article reviews IPL/TPJ activations and network participation in bottom-up attention, lower-order self-perception, undirected thinking, episodic memory and social cognition. The central theme of this review is to discuss how network nodes within the IPL/TPJ are organized and how they participate in human perception and cognition.}
}
@article{BENTLEY2000465,
title = {Statistics and archaeology in Israel},
journal = {Computational Statistics & Data Analysis},
volume = {32},
number = {3},
pages = {465-483},
year = {2000},
issn = {0167-9473},
doi = {https://doi.org/10.1016/S0167-9473(99)00094-8},
url = {https://www.sciencedirect.com/science/article/pii/S0167947399000948},
author = {Jim Bentley and Tammi J Schneider},
keywords = {Variability, Density estimation, Kriging, Mapping: Surface survey, Archaeology Archaeometrics},
abstract = {While the field of statistics is fairly young, the field of archaeology is quite old. Modern archaeology prides itself on its ability to glean maximum information about the past from minimal information collected in the present. This paper attempts to show how the application of statistical thinking and techniques can aid the archaeologist in retrieving as much information as possible from artifacts; thus allowing the archaeologists to leave the majority of a site for future generations. In the past few years, archaeologists working in Israel have joined forces with statisticians in an attempt to generate more accurate recordings of archaeological information than is currently the standard in the Middle East. Careful application of statistical methods has reduced collection time and improved the display of archaeological information. An understanding of statistical concepts such as variability and density estimation has already been shown to be of use to archaeologists. Conversely, the use of examples from the field have proven to be of use in motivating humanities students to learn about statistical thinking. Archaeology has also provided a field in which students of statistics may apply their new found knowledge. The combination of statistics and archaeology is clearly of benefit to both disciplines.}
}
@article{DAVID2022132522,
title = {Integrating fourth industrial revolution (4IR) technologies into the water, energy & food nexus for sustainable security: A bibliometric analysis},
journal = {Journal of Cleaner Production},
volume = {363},
pages = {132522},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.132522},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622021230},
author = {Love O. David and Nnamdi I. Nwulu and Clinton O. Aigbavboa and Omoseni O. Adepoju},
keywords = {WEF Nexus, Fourth industrial revolution, Industry 4.0, Cleaner production, Internet of things (IoT)},
abstract = {The technologies of the fourth Industrial Revolution (4IR/Industry 4.0) have been a technological catalyst for all fields of human endeavor, permeating the water, energy, and food (WEF) nexus. However, there is no empirical evidence of the extent of applications and the permeability level in ensuring the three resources’ security. This study explored the relationship of the fourth industrial revolution technologies and the water, energy, and food nexus by evaluating the applications of the various technologies of 4IR on WEF nexus and examined the effect of 4IR on WEF nexus. The objectives were achieved using the qualitative methodology and bibliometric analysis of content analysis. The result showed that most fourth industrial revolution technologies had not been integrated with the WEF nexus. The result showed that only the Internet of Things (IoT) and Big Data analytics had permeated the nexus, which shows that data of the resources will be the foundation of the nexus. The systematic collection, accuracy of data, and empirical analysis of data will determine the level of security of WEF nexus. The qualitative results show that there are applications of the fourth industrial revolution technologies to the individual sectors of the nexus, birthing Water 4.0, Energy 4.0, and Food 4.0. The Bibliometric analysis result shows that the integration of the fourth industrial revolution with the WEF nexus will lead to cleaner production practices relating to the technological processes of water, energy, and food resources. These practices will ensure the environment's safety from WEF wastes and the water, energy, and food security in production processes. The empirical research and bibliometric analysis result, rooted in the concept of cleaner production, shows that the fourth industrial revolution affected the WEF nexus. The effects are; the birth of clean technologies & industrial applications, the catalyst for sustainability security of WEF nexus leveraging on life cycle thinking, enablement of technological transfer, enhancement of economic growth, and urban planning. The study concludes that the fourth industrial revolution technologies affect WEF nexus, ensuring the popularization of cleaner production strategies and processes of the resources during trade-offs and synergies. The study recommends the integration of a cleaner production concept in WEF processing. It should follow the innovation diffusion theory (IDT) and Technology acceptance theory (TAM) when applying 4IR technologies to the nexus of water, energy, and food resources, for their sustainable security.}
}
@article{MACHADO2021103322,
title = {Contributions of modularity to the circular economy: A systematic review of literature},
journal = {Journal of Building Engineering},
volume = {44},
pages = {103322},
year = {2021},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2021.103322},
url = {https://www.sciencedirect.com/science/article/pii/S2352710221011803},
author = {Natália Machado and Sandra Naomi Morioka},
keywords = {Modularity, Circular economy, Product modular design, Module optimization, Product lifecycle},
abstract = {In the creation of practices to foster the thinking of the circular economy (CE), modularity can be a facilitator. There are many studies that address modularity and the circular economy individualized, but the integration between the two is still little addressed. This study conducts a systematic review of the literature and aims to identify how modularity can contribute to the circular economy. The data analysis begins with the identification of the main characteristics of the literature that address modularity and circular economy, bringing the evolution of studies over the years, main journals, co-citation of references, co-occurrence of keywords and shows four research clusters linked to modularity and CE, being: conceptualization of modularization, modular design of products, module optimization and product lifecycle. In addition, identifies fifteen benefits of modularity that can contribute to the implementation of strategies for the circular economy and five barriers from the perspective of the circular economy that can inhibit the contribution process. So, it was possible to create an integrative conceptual framework that shows how modularity can contribute to the circular economy. From the results, future research is identified in order to contribute to the transition from a linear economy to a circular economy.}
}
@article{GUPTA2005267,
title = {Power-law distribution in a learning process: competition, learning and natural selection},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {345},
number = {1},
pages = {267-274},
year = {2005},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2004.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0378437104009860},
author = {Hari M. Gupta and José R. Campanha},
keywords = {Power-law, Learning, Natural selection},
abstract = {In the present work, we propose a model for the statistical distribution of people versus number of steps acquired by them in a learning process, based on competition, learning and natural selection. We consider that learning ability is normally distributed. We found that the number of people versus step acquired by them in a learning process is given through a power law. As competition, learning and selection is also at the core of all economical and social systems, we consider that power-law scaling is a quantitative description of this process in social systems. This gives an alternative thinking in holistic properties of complex systems.}
}
@article{PELOROSSO2020101867,
title = {Modeling and urban planning: A systematic review of performance-based approaches},
journal = {Sustainable Cities and Society},
volume = {52},
pages = {101867},
year = {2020},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2019.101867},
url = {https://www.sciencedirect.com/science/article/pii/S2210670719316968},
author = {Raffaele Pelorosso},
keywords = {Systems thinking, Thermodynamics of open systems, Standards, Spatial planning, Model classification},
abstract = {New planning approaches based on performance measures of the urban system are emerging to face the current challenges to the sustainability of cities. Through modelling, planners can understand the general behavior of the system and, consequently, decide the strategic allocation of land uses and human activities with respect to performances of the considered processes and the socio-ecological and economic uncertainties. Thus, model-based planning approaches present strong similarities with the performance-based planning (PBP) approaches and modelling can represent a valuable tool for the evolution and expansion of PBP. In this paper, a systematic review has explored a) the contribution of modelling within PBP approaches in moving cities towards sustainability; b) the applicability for modeling in PBP in urban contexts. Twelve operational examples of model-based urban planning and PBP have been identified in energy, water infrastructure, land use and ecological planning areas. A scoring system for potential model applicability in urban planning was tested in the sampled case studies. Moreover, several critical elements in the relation between modeling approaches and PBP have been identified. Finally, a discussion on the system performance concept as a new urban planning paradigm has been proposed.}
}
@article{CAI201253,
title = {On fast and accurate block-based motion estimation algorithms using particle swarm optimization},
journal = {Information Sciences},
volume = {197},
pages = {53-64},
year = {2012},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2012.02.014},
url = {https://www.sciencedirect.com/science/article/pii/S002002551200117X},
author = {Jing Cai and W. {David Pan}},
keywords = {Particle swarm optimization, Motion estimation, Fast block-matching methods, Video sequences, Computational complexity},
abstract = {Both fast and accurate block-matching algorithms are critical to efficient compression of video frames using motion estimation and compensation. While the particle swarm optimization approach holds the promise of alleviating the local optima problem suffered typically by existing very fast block matching methods, motion estimation algorithms based on particle swarm optimization in the literature appear to be either much slower than some leading fast block-matching methods for a given accuracy of motion estimation, or less accurate for a given computational complexity. In this paper, we show that the conventional particle swarm optimization approach, which was originally designed to solve general optimization problems where fast convergence of the algorithm might not be a primary concern, could be modified appropriately so that it could provide accurate motion estimation with very low computational cost in the specific context of video motion estimation. To this end, we proposed a new block matching algorithm based on a set of strategies adapted from the standard particle swarm optimization approach. Extensive simulations showed that the proposed method could achieve significant improvements over leading fast block matching methods including the diamond search and the cross-diamond search methods, in terms of both estimation accuracy and computational cost. In particular, the proposed method based on particle swarm optimization is not only much faster, but also remarkably more accurate (about 2dB higher in terms of the Peak Signal-to-Noise-Ratio) than the competing methods on video sequences with large motion.}
}
@article{JANG2020107524,
title = {Hemispheric asymmetries in processing numerical meaning in arithmetic},
journal = {Neuropsychologia},
volume = {146},
pages = {107524},
year = {2020},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2020.107524},
url = {https://www.sciencedirect.com/science/article/pii/S0028393220301974},
author = {Selim Jang and Daniel C. Hyde},
keywords = {Arithmetic, Numerical cognition, Cerebral hemispheres, Late positivity, Distance effect},
abstract = {Hemispheric asymmetries in arithmetic have been hypothesized based on neuropsychological, developmental, and neuroimaging work. However, it has been challenging to separate asymmetries related to arithmetic specifically, from those associated general cognitive or linguistic processes. Here we attempt to experimentally isolate the processing of numerical meaning in arithmetic problems from language and memory retrieval by employing novel non-symbolic addition problems, where participants estimated the sum of two dot arrays and judged whether a probe dot array was the correct sum of the first two arrays. Furthermore, we experimentally manipulated which hemisphere receive the probe array first using a visual half-field paradigm while recording event-related potentials (ERP). We find that neural sensitivity to numerical meaning in arithmetic arises under left but not right visual field presentation during early and middle portions of the late positive complex (LPC, 400-800 ms). Furthermore, we find that subsequent accuracy for judgements of whether the probe is the correct sum is better under right visual field presentation than left, suggesting a left hemisphere advantage for integrating information for categorization or decision making related to arithmetic. Finally, neural signatures of operational momentum, or differential sensitivity to whether the probe was greater or less than the sum, occurred at a later portion of the LPC (800-1000 ms) and regardless of visual field of presentation, suggesting a temporal and functional dissociation between magnitude and ordinal processing in arithmetic. Together these results provide novel evidence for differences in timing and hemispheric lateralization for several cognitive processes involved in arithmetic thinking.}
}
@article{WALLENTIN2017165,
title = {Dynamic hybrid modelling: Switching between AB and SD designs of a predator-prey model},
journal = {Ecological Modelling},
volume = {345},
pages = {165-175},
year = {2017},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2016.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0304380016303714},
author = {Gudrun Wallentin and Christian Neuwirth},
keywords = {Hybrid model, Multi-paradigmatic modelling, Agent-based model, System-dynamics model, Predator-prey system},
abstract = {Entities and processes in complex systems are of diverse nature and operate at various spatial and temporal scales. Hybrid agent-based (AB) and system dynamics (SD) models have been suggested to capture the essence of these systems in a natural and computationally efficient way. However, the integration of the equation-based SD and individual-based AB models is not least challenged by considerable conceptual differences between these models. Examples of tightly integrated and dynamically switching hybrid models are rare. The aim of this paper is to expand on theoretical frameworks of hybrid agent-based and system dynamics models in ecology to support the model design process of dynamically switching hybrid models. We suggested six alternative model designs that switched between the two modelling paradigms. By the example of a fish-plankton lake ecosystem we demonstrated that a well-designed switching hybrid model can be a performant modelling approach that retains relevant spatial and attributive information. Important findings with respect to optimising computational versus predictive performance were (1) the most plausible results were produced by a spatially explicit design based on spatial plankton stocks and fish switching between individual agents and aggregate school-agents, (2) higher levels of aggregation did not necessarily result in higher computational performance, and (3) adaptive, emergence-based triggers for the paradigm switches minimised information loss and could connect hierarchical and spatial scales. In conclusion, we argue to reach beyond efficiency-oriented considerations and use emergent super-individuals as structural elements of dynamically switching hybrid models.}
}
@incollection{BOGLE20031,
title = {Computer aided biochemical process engineering},
editor = {Andrzej Kraslawski and Ilkka Turunen},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {14},
pages = {1-10},
year = {2003},
booktitle = {European Symposium on Computer Aided Process Engineering-13},
issn = {1570-7946},
doi = {https://doi.org/10.1016/S1570-7946(03)80082-2},
url = {https://www.sciencedirect.com/science/article/pii/S1570794603800822},
author = {I.D.L. Bogle},
abstract = {The growth of the biochemical industries is heating up in Europe after not meeting the initial expectations. CAPE tools have made some impact and progress on computer aided synthesis and design of biochemical processes is demonstrated on a process for the production of a hormone. Systems thinking is being recognised by the life science community and to gain genuinely optimal process solutions it is necessary to design right through from product and function to metabolism and manufacturing process. The opportunities for CAPE experts to contribute in the explosion of interest in the Life Sciences is strong if we think of the ‘Process’ in CAPE as any process involving physical or (bio-)chemical change.}
}
@article{SAYLIK2022136915,
title = {Functional neuroanatomical correlates of contingency judgement},
journal = {Neuroscience Letters},
volume = {791},
pages = {136915},
year = {2022},
issn = {0304-3940},
doi = {https://doi.org/10.1016/j.neulet.2022.136915},
url = {https://www.sciencedirect.com/science/article/pii/S0304394022004761},
author = {Rahmi Saylik and Andre J. Szameitat and Adrian L. Williams and Robin A. Murphy},
keywords = {Contingency learning, Associative relations, Shared and diverse areas, Lateral prefrontal lobe, Parietal lobe, fMRI},
abstract = {Contingency judgement is an ability to detect relationships between events and is crucial in the allocation of attentional resources for reasoning, categorization, and decision making to control behaviour in our environment. Research has suggested that the allocation of attention is sensitive to the frequency of contingency information whether it constitutes a negative, zero or positive relationship. The aim of the present study was to explore the functional neuroanatomical correlates of contingency judgement with different frequencies and whether these are distinct from each other or whether they rely on a common mechanism. Using three contingency tasks within a streaming paradigm (one each for negative, zero, and positive contingency frequencies), we assessed brain activity by means of functional magnetic resonance imaging (fMRI) in 20 participants. Contingency frequency was manipulated between blocks which allowed us to determine the neural correlates of each of the three contingency tasks as well as the common areas of activation. The conjunction of task activation showed activity in left parietal cortices (BA 23, 40) and superior temporal gyrus (BA42). Further, the interaction analysis revealed distinct areas that mainly involve lateral (BA 45) and medial (BA 9) prefrontal cortices in the judgment of negative contingencies compared with positive and zero contingencies. We interpret the finding as evidence that the shared regions may be involved in coding, integration, and updating of associative relations and distinct regions may be involved in the investment of attentional resources to varied degrees in the computation of contingencies to make a judgment.}
}
@article{ROY2024131183,
title = {Optimizing the quantity of recharge water into a sedimentary aquifer through infiltration galleries using a surrogate assisted coupled simulation–optimization approach},
journal = {Journal of Hydrology},
volume = {635},
pages = {131183},
year = {2024},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2024.131183},
url = {https://www.sciencedirect.com/science/article/pii/S002216942400578X},
author = {Dilip Kumar Roy and Deborah L. Leslie and Michele L. Reba and Ahmed A. Hashem and Emily Bellis and John Nowlin},
keywords = {Alluvial aquifer, Managed aquifer recharge, Infiltration galleries, Coupled simulation–optimization, Management model},
abstract = {The Mississippi River Valley Alluvial Aquifer (MRVAA) is the main irrigation source for the Lower Mississippi River Basin. Irrigation water abstraction to meet the demands for extensive agricultural practices has contributed to groundwater depletion in this area. A managed aquifer recharge (MAR) approach has been proposed in this geographic location to minimize the impact of pumping on groundwater depletion. However, it is essential to determine the optimal amount of water to be injected through a MAR technique to reduce the decline in groundwater heads. This paper utilizes a coupled simulation–optimization (S-O) approach to estimate the optimal recharge volume into the alluvial aquifer through infiltration galleries. The aquifer processes were simulated using a physically based, three-dimensional finite-difference numerical code, MODFLOW. The MODFLOW model was calibrated and validated using the recharge rates and available groundwater head data for 26 months (27 February 2020 to 27 May 2022). The calibrated and validated models were then deployed within the coupled S-O approach to develop an aquifer recharge management model to estimate optimal groundwater recharge rates to minimize groundwater decline. Computational efficiency of the aquifer recharge management model was achieved using surrogate models that accurately reproduced the groundwater heads calculated by MODFLOW. Our evaluation demonstrates that a planned transient groundwater recharge strategy, obtained as a solution of the surrogate model based coupled S-O approach, is a useful management strategy for optimized water recharge and groundwater depletion control. This study shows the promise of the surrogate model based coupled S-O approach to potentially reduce groundwater depletion in the MRVAA by utilizing optimized recharge rates at the infiltration galleries. This work has potential applications to other aquifers and geographic locations to mitigate groundwater depletion issues due to extensive agricultural practices.}
}
@article{BARRETO20114206,
title = {Lumping the States of a Finite Markov Chain Through Stochastic Factorization},
journal = {IFAC Proceedings Volumes},
volume = {44},
number = {1},
pages = {4206-4211},
year = {2011},
note = {18th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20110828-6-IT-1002.00073},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016442680},
author = {André M.S. Barreto and Marcelo D. Fragoso},
abstract = {Abstract
In this work we show how the lumping of states of a finite Markov chain can be regarded as a special decomposition of its transition matrix called stochastic factorization. The idea is simple: when a transition matrix is factored into the product of two stochastic matrices, one can swap the factors of the multiplication to obtain another model, potentially much smaller than the original one. We prove in the paper that the smaller Markov chain has the same reducibility and the same number of closed sets as the original model. Additionally, the stationary distributions of both chains are related through a linear transformation. By interpreting the lumping of states as a particular case of stochastic factorization, we discuss in which circumstances the lumped transition matrix can be used in place of the original one to compute its stationary distribution. To illustrate our ideas we use the computation of Google's PageRank as an example.}
}
@article{DUAN20211084,
title = {Hierarchical community structure preserving approach for network embedding},
journal = {Information Sciences},
volume = {546},
pages = {1084-1096},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.09.053},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520309622},
author = {Zhen Duan and Xian Sun and Shu Zhao and Jie Chen and Yanping Zhang and Jie Tang},
keywords = {Network embedding, Representation learning, Granulation, Hierarchical community},
abstract = {Network embedding aims to map the topological proximities of all nodes in a network into a low-dimensional representation space. Previous studies mainly focus on preserving the within-layer structure of the network (such as first-order proximities, second-order proximities, and community structure). However, many complex networks present a hierarchical organization, often in the form of a hierarchy community structure. How to effectively preserve the within-layer structure and the hierarchical community structure under multi-granularity is a meaningful and still tough task. Inspired by Granular Computing, which is a problem-solving concept deeply rooted in human thinking ability to perceive the real world under multi-granularity, we propose a unified network embedding framework by preserving both the within-layer structure and the hierarchical community structure of the network under multi-granularity, named as Hierarchical Community structure preserving approach for Network Embedding (HCNE). Firstly, different granular networks from fine to coarse are constructed by network granulation which reveals the hierarchical community structure of the original network. Secondly, from coarse to fine, finer networks inherit the embedding of coarse-grained networks as good initialization embedding in the refinement process so that the embedding preserved both the within-layer structure and the hierarchical community structure of the network under multi-granularity. Finally, the learned embedding of each node fed into downstream tasks, including multi-label classification and network visualization. Experimental results demonstrate that HCNE significantly outperforms other state-of-the-art methods. Meanwhile, we intuitively show the effectiveness of HCNE on network visualization which can preserve both the within-layer structure and the hierarchical community structure of the network under multi-granularity.}
}
@article{DING2024105075,
title = {Serious game-based learning and learning by making games: Types of game-based pedagogies and student gaming hours impact students' science learning outcomes},
journal = {Computers & Education},
volume = {218},
pages = {105075},
year = {2024},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2024.105075},
url = {https://www.sciencedirect.com/science/article/pii/S0360131524000897},
author = {Ai-Chu Elisha Ding and Cheng-Han Yu},
keywords = {Game-based pedagogy, Game-based learning, Student game experiences, Weekly gaming hours, Science education},
abstract = {Game-based learning (GBL) has garnered significant attention as a means to enhance student science learning. However, researchers often broadly categorize GBL without specific attention to teachers' practices or the different game-based pedagogies (GBPs) such as serious game-based learning and learning by making games. Serious game-based learning involves the use of games designed primarily for purposes beyond entertainment, engaging students in learning activities. Learning by making games goes further by enabling students to construct and expand their knowledge through creative activities. Furthermore, the impact of students' weekly gaming hours on their academic performance under different GBPs has not been thoroughly explored. In this study, we analyzed pre- and post-science unit assessment data to investigate the effects of serious game-based learning and learning by making games, along with gaming hours, on the science learning outcomes of 129 sixth-grade students in a Midwestern urban school district in the United States. Employing Bayesian paired t-tests and ANOVA, we examined the effects of two GBPs (serious game learning and game-making) implemented by two science teachers and the interaction between each GBP and students' weekly gaming hours. Our findings indicate that both GBPs led to improvements in students’ learning, with those participating in making games showing an average increase of 3.5 points more on the posttests than those engaged in serious game-based learning. Additionally, data on student-reported weekly gaming hours showed that in both GBP conditions, students with medium gaming hours showed the most improvement compared to those with fewer or more gaming hours. There was also a potential interaction between types of GBP and the effect of gaming hours outcomes in that the learning gain difference between the two GBPs was more pronounced among students reporting fewer gaming hours, although this effect was not significant. This study contributes to the GBL field by providing practical design recommendations for integrating GBL into formal Kindergarten to 12th grade (K-12) settings, taking into account teachers' pedagogical agency and students' gaming experiences.}
}
@article{ABDUSSAMI2023103094,
title = {Provably secured lightweight authenticated key agreement protocol for modern health industry},
journal = {Ad Hoc Networks},
volume = {141},
pages = {103094},
year = {2023},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2023.103094},
url = {https://www.sciencedirect.com/science/article/pii/S1570870523000148},
author = {Mohammad Abdussami and Ruhul Amin and Satyanarayana Vollala},
keywords = {Health care industry, Authentication, Key agreement, Scyther simulation},
abstract = {Internet of Medical Things (IoMT) has facilitated the healthcare industry by providing ease of communication among doctors and patients living in remote areas for accomplishing diagnosis, real-time monitoring, and treatment procedure efficiently. The patient’s health-related data must be secured from various attacks of adversary since the data is sensitive and highly prone to attacks. This paper proposes an architecture that suits both localized and emergency scenarios. This architecture utilizes cloud server and edge computing technology. Provably secured lightweight authenticated key agreement protocol for modern health industry (PSLA2P) provides a lightweight authentication and key agreement protocol that can be deployed in the proposed network architecture. It protects the privacy of the patient’s health-related data by providing anonymity and untraceability. Real-Or-Random (ROR) model is used for the formal analysis of PSLA2P. We have verified the security weaknesses of PSLA2P using the Scyther simulator. Moreover, the informal analysis ensures high-level mitigation against known possible attacks. PSLA2P achieves better performance in terms of computation and communication overhead.}
}
@article{MORCH2023100697,
title = {Makerspace activities in a school setting: Top-down and bottom-up approaches for teachers to leverage pupils' making in science education},
journal = {Learning, Culture and Social Interaction},
volume = {39},
pages = {100697},
year = {2023},
issn = {2210-6561},
doi = {https://doi.org/10.1016/j.lcsi.2023.100697},
url = {https://www.sciencedirect.com/science/article/pii/S2210656123000132},
author = {Anders I. Mørch and Ellen E. Flø and Kristina T. Litherland and Renate Andersen},
keywords = {K-12, Makerspace, Programming, Rising-to-the-concrete, Sociocultural perspective, STEM},
abstract = {This article addresses the opportunities and challenges of turning a science, technology, engineering, and mathematics (STEM) classroom into a makerspace for hands-on experimentation with digital tools and materials in science education. In this qualitative case study, over a period of 16 weeks, video data were collected during making activities in an advanced placement science course with 19 pupils aged 12–16 years, and interviews were conducted. We combined thematic and interaction analyses of empirical data and identified three themes: 1) engagement and spontaneous concepts, 2) programming and making physical objects, and 3) subject integration. Our conceptual framework for the analyses integrated two features of the Vygotskian sociocultural theory of learning: concept development as a dialectical process of scientific and everyday concepts and the “tool and symbol” duality. Our findings show that both top-down and bottom-up approaches to integrating school subjects into a makerspace were effective but underused. We illustrate this by mapping pupils' shared understanding in a sociotechnical space, visualized as a process of “rising to the concrete”, which may require teacher's scaffolding at different levels of abstraction and use of instructional materials in different modalities.}
}
@article{XIE20189,
title = {Detecting leadership in peer-moderated online collaborative learning through text mining and social network analysis},
journal = {The Internet and Higher Education},
volume = {38},
pages = {9-17},
year = {2018},
issn = {1096-7516},
doi = {https://doi.org/10.1016/j.iheduc.2018.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S1096751618300332},
author = {Kui Xie and Gennaro {Di Tosto} and Lin Lu and Young Suk Cho},
keywords = {Leadership, Computer-supported collaborative learning, Text mining, Social network analysis, Learning analytics, Online learning},
abstract = {Structured tasks and peer-moderated discussions are pedagogical models that have shown unique benefits for online collaborative learning. Students appointed with leadership roles are able to positively affect the dynamics in their groups by engaging with participants, raising questions, and advancing problem solving. To help monitoring and controlling the latent social dynamics associated with leadership behavior, we propose a methodological approach that makes use of computational techniques to mine the content of online communications and analyze group structure to identify students who behave as leaders. Through text mining and social network analysis, we systematically process the discussion posts made by students from four sections of an online course in an American university. The results allow us to quantify each individual's contribution and summarize their engagement in the form of a leadership index. The proposed methodology, when compared to judgements made by experts who manually coded samples of the data, is shown to have comparable performances, but, being fully automated, has the potential to be easily replicable. The summary offered by the leadership index is intended as actionable information that can guide just-in-time interventions together with other tools based on learning analytics.}
}
@article{HOBENSACK2024104753,
title = {A rapid review on current and potential uses of large language models in nursing},
journal = {International Journal of Nursing Studies},
volume = {154},
pages = {104753},
year = {2024},
issn = {0020-7489},
doi = {https://doi.org/10.1016/j.ijnurstu.2024.104753},
url = {https://www.sciencedirect.com/science/article/pii/S0020748924000658},
author = {Mollie Hobensack and Hanna {von Gerich} and Pankaj Vyas and Jennifer Withall and Laura-Maria Peltonen and Lorraine J. Block and Shauna Davies and Ryan Chan and Liesbet {Van Bulck} and Hwayoung Cho and Robert Paquin and James Mitchell and Maxim Topaz and Jiyoun Song},
keywords = {Rapid review, Nursing informatics, Large language models, Generative AI, ChatGPT},
abstract = {Background
The application of large language models across commercial and consumer contexts has grown exponentially in recent years. However, a gap exists in the literature on how large language models can support nursing practice, education, and research. This study aimed to synthesize the existing literature on current and potential uses of large language models across the nursing profession.
Methods
A rapid review of the literature, guided by Cochrane rapid review methodology and PRISMA reporting standards, was conducted. An expert health librarian assisted in developing broad inclusion criteria to account for the emerging nature of literature related to large language models. Three electronic databases (i.e., PubMed, CINAHL, and Embase) were searched to identify relevant literature in August 2023. Articles that discussed the development, use, and application of large language models within nursing were included for analysis.
Results
The literature search identified a total of 2028 articles that met the inclusion criteria. After systematically reviewing abstracts, titles, and full texts, 30 articles were included in the final analysis. Nearly all (93 %; n = 28) of the included articles used ChatGPT as an example, and subsequently discussed the use and value of large language models in nursing education (47 %; n = 14), clinical practice (40 %; n = 12), and research (10 %; n = 3). While the most common assessment of large language models was conducted by human evaluation (26.7 %; n = 8), this analysis also identified common limitations of large language models in nursing, including lack of systematic evaluation, as well as other ethical and legal considerations.
Discussion
This is the first review to summarize contemporary literature on current and potential uses of large language models in nursing practice, education, and research. Although there are significant opportunities to apply large language models, the use and adoption of these models within nursing have elicited a series of challenges, such as ethical issues related to bias, misuse, and plagiarism.
Conclusion
Given the relative novelty of large language models, ongoing efforts to develop and implement meaningful assessments, evaluations, standards, and guidelines for applying large language models in nursing are recommended to ensure appropriate, accurate, and safe use. Future research along with clinical and educational partnerships is needed to enhance understanding and application of large language models in nursing and healthcare.}
}
@article{MOHIT20221713,
title = {Approach of artificial intelligence for analysing properties of concrete},
journal = {Materials Today: Proceedings},
volume = {48},
pages = {1713-1717},
year = {2022},
note = {SCPINM-2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.10.028},
url = {https://www.sciencedirect.com/science/article/pii/S221478532106507X},
author = { Mohit and Balwinder Lallotra},
keywords = {Concrete, Artificial intelligence, Artificial neural network, Workability, Compressive strength},
abstract = {Technological progress is often measured by computation power. At the moment we are in the golden age where we are blessed with a perfect trio, machine learning algorithms, huge datasets across disciplines, and processing hardware. The constant desire to understand the human brain has led us to try mimicking it, thus forming the basis of neural networks creating a way for deep learning algorithms. Such algorithms have proven to work on non-linear data sets effectively, generating results that could find patterns just like our brains. In this paper, we explore a recently rising application for Neural Network frameworks; in particular, concrete in basic designing. We design and implement tests to analyze various properties of concrete of different concrete mixes. Customarily, the ability of concrete to perform is influenced by numerous non-straight factors, and testing its quality includes the destructive procedure of concrete samples.}
}
@article{CHEN2024147,
title = {Dynamic response analysis of a typical time-varying mass system: A moving-interface pipe model, the WKB-recursive solution and experimental validation},
journal = {Applied Mathematical Modelling},
volume = {125},
pages = {147-166},
year = {2024},
issn = {0307-904X},
doi = {https://doi.org/10.1016/j.apm.2023.08.044},
url = {https://www.sciencedirect.com/science/article/pii/S0307904X23003979},
author = {Weiting Chen and Guoping Chen and Tengfei Chen and Xing Tan and Hanbo Shao and Huan He},
keywords = {Time-varying system, Moving-interface model, The WKB-recursive method, Experimental validation},
abstract = {This research aims to seek a suitable way for analyzing the time-varying characteristics of some engineering phenomena such as the fuel consumption of flying rockets, the movement process of control rods used to regulate the reaction rate in nuclear reactors, etc. To address this problem, a simulated procedure containing modeling and response solving is proposed. A common variable cross-section pipeline with time-varying mass is designed as a practical time-varying system. A simplified segmented beam model with a moving interface is proposed to simulate the pipe's time-varying behavior. Then the Wentzel-Kramers-Brillouin (WKB)-recursive method is proposed to solve this time-varying problem. A two-segmented beam example is used to verify its computability. The computational efficiency is greatly improved in comparison with the conventional numerical integration methods. To validate this proposed procedure, numerical simulation is carried out and an experiment is specially designed and implemented. In the experiment, many cases of different rates of mass variation and excitation forces are carried out. Overall, the numerical dynamic responses match well with the experimental ones, which indicates that the proposed procedure is suitable for analyzing the system's time-varying characteristics.}
}
@article{MERRICK2019511,
title = {On choosing the resolution of normative models},
journal = {European Journal of Operational Research},
volume = {279},
number = {2},
pages = {511-523},
year = {2019},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2019.06.017},
url = {https://www.sciencedirect.com/science/article/pii/S0377221719304928},
author = {James H. Merrick and John P. Weyant},
keywords = {Problem structuring, Validation of OR computations, Information theory, Strategic planning, OR in environment and climate change},
abstract = {Long time horizon normative models are frequently used for policy analysis, strategic planning, and system analysis. Choosing the granularity of the temporal or spatial resolution of such models is an important modeling decision, often having a first order impact on model results. This type of decision is frequently made by modeler judgment, particularly when the predictive power of alternative choices cannot be tested. In this paper, we show how the implicit tradeoffs modelers make in these formulation decisions, in particular in the tradeoff between the accuracy of representation enabled by the available data and model parsimony, may be addressed with established information theoretic ideas. The paper provides guidance for modelers making these tradeoffs or, in certain cases, enables explicit tests for assessing appropriate levels of resolution. We will mainly focus on optimization based normative models in the discussion here, and draw our examples from the energy and climate domain.}
}
@incollection{2025xxi,
title = {Editor’s biographies},
editor = {Rajesh Kumar Dhanaraj and Prithi Samuel and Malathy Sathyamoorthy and Balamurugan Balusamy and Vinayakumar Ravi},
booktitle = {Self-Powered Sensors},
publisher = {Academic Press},
pages = {xxi-xxiii},
year = {2025},
isbn = {978-0-443-13792-1},
doi = {https://doi.org/10.1016/B978-0-443-13792-1.09989-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443137921099892}
}
@article{WEI2024109991,
title = {Towards self-explainable graph convolutional neural network with frequency adaptive inception},
journal = {Pattern Recognition},
volume = {146},
pages = {109991},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109991},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006891},
author = {Feifei Wei and Kuizhi Mei},
keywords = {Self-explainable neural network, Frequency adaptive filter, Graph convolutional neural networks (GCN)},
abstract = {Graph convolutional neural networks (GCNs) have demonstrated powerful representing ability of irregular data, e.g., skeletal data and graph-structured data, providing the effective mechanism to fuse the neighbor nodes. However, inheriting from the deep learning, GCN also lacks interpretability, which hinders its application to scenarios that have high demand for transparency. Although, there have been many efforts on the interpretability of deep learning, they mainly concentrate on i.i.d data that is hard to be deployed to GCNs, which involve not only the node feature, but also the graph structure. There are few works that attempt to explain it with post-hoc manner, which can be biased, resulting in mis-representation of the true explanation. Therefore, in this paper, we propose a framework, namely ExpFiGCN, that reveals explainability of the GCNs from the perspective of graph structure and mathematical analysis. Specifically, ExpFiGCN can find the most intrinsically relevant node to the central node and obtain the informative and discriminative signals while performing denoising. For the graph structure, we find K-nearest nodes; for the mathematical analysis, every channel of a node and its neighborhoods contribute dynamically to the final channel signal, which can capture the inherent difference of different channels and neighbor nodes. Meanwhile, it can enhance the representation ability of nodes and ameliorate the over-smoothing problem. On the other hand, our model can dynamically adjust the importance of neighborhoods to the central vertex. We empirically validate the effectiveness of the proposed framework ExpFiGCN on various benchmark datasets. Experimental results show that our method achieves substantial improvements and outperforms the state-of-the-art performance strikingly.}
}
@article{PIMDEE2024e29172,
title = {Enhancing Thai student-teacher problem-solving skills and academic achievement through a blended problem-based learning approach in online flipped classrooms},
journal = {Heliyon},
volume = {10},
number = {7},
pages = {e29172},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e29172},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024052034},
author = {Paitoon Pimdee and Aukkapong Sukkamart and Cherisa Nantha and Thiyaporn Kantathanawat and Punnee Leekitchwatana},
keywords = {Academic achievement, Blended learning, Online flipped classroom, Problem-solving skills, Student-teachers, Thailand},
abstract = {The study aimed to develop a learning model that enhances Thai student-teacher problem-solving skills (PSS) and academic achievement (AA) through a blended problem-based learning (PBL) approach in online flipped classrooms. Phase 1 consisted of the design of the Flipped PARSER (problem-attempt-research-solutions-evaluation-reflect) Model (FPM) through the study of documentation and research. Phase 2 involved using nine experts to assist with the model's development and evaluation using in-depth interviews and content analysis. Phase 3 involved the application and use of the FPM by 30 student-teachers, from which their AA, PSS, and satisfaction were evaluated against the control group of 31 participants. The results from the nine experts' input on the FPM design were significant. Instructor activities included selecting content, media, materials, and their design and development. Other components included learning activities, tools, lesson delivery, and evaluation. Also, PBL methods were again confirmed as an instrumental pedagogy in teaching PSS. When combined with online learning and flipped classrooms, the study's results were higher than those using traditional classroom methods. The study contributes to the literature by determining that online teaching models effectively teach PSS and raise AA scores.}
}
@article{GARAVAGLIA2010258,
title = {Modelling industrial dynamics with “History-friendly” simulations},
journal = {Structural Change and Economic Dynamics},
volume = {21},
number = {4},
pages = {258-275},
year = {2010},
issn = {0954-349X},
doi = {https://doi.org/10.1016/j.strueco.2010.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0954349X10000573},
author = {Christian Garavaglia},
keywords = {Simulation, Industrial dynamics, Evolutionary economics, “History-Friendly” models, Complexity},
abstract = {The use of simulation techniques has increased greatly in recent years. In economics the industrial dynamics approach makes use of simulation techniques to understand the complexity of the industrial process of continuous change. Among these models, a new branch of studies known as “History-friendly” models aims at establishing a close link between formal theory, developing stand-alone theoretical simulation models, and empirical evidence. In this paper, we study “History-friendly” analyses and counterfactuals. Some examples of “History-friendly” models are widely examined. Finally, the paper makes a critical contribution to “History-friendly” methodology and defines the role of “History-friendly” models in the debate on the empirical validation of simulations.}
}
@article{VEKSLER20169,
title = {The performance comparison problem: Universal task access for cross-framework evaluation, Turing tests, grand challenges, and cognitive decathlons},
journal = {Biologically Inspired Cognitive Architectures},
volume = {18},
pages = {9-22},
year = {2016},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2016.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X16300810},
author = {Vladislav D. Veksler and Norbou Buchler and Christian Lebiere and Don Morrison and Troy Kelley},
keywords = {Grand challenge, Cognitive decathlon, Turing test, Performance comparison, Simulation, API, Standards},
abstract = {A driver for achieving human-level AI and high-fidelity cognitive architectures is the ability to easily test and compare the performance and behavior of computational agents/models to humans and to one another. One major difficulty in setting up and getting participation in large-scale cognitive decathlon and grand challenge competitions, or even smaller scale cross-framework evaluation and Turing testing, is that there is no standard interface protocol that enables and facilitates human and computational agent “plug-and-play” participation across various tasks. We identify three major issues. First, human-readable task interfaces aren’t often translated into machine-readable form. Second, in the cases where a task interface is made available in a machine-readable protocol, the protocol is often task-specific, and differs from other task protocols. Finally, where both human and machine-readable versions of the task interface exist, the two versions often differ in content. This makes the bar of entry extremely high for comparison of humans and multiple computational frameworks across multiple tasks. This paper proposes a standard approach to task design where all task interactions adhere to a standard API. We provide examples of how this method can be employed to gather human and computational simulation data in text-and-button tasks, visual and animated tasks, and in real-time robotics tasks.}
}
@article{YUAN2023110243,
title = {Improving vessel connectivity in retinal vessel segmentation via adversarial learning},
journal = {Knowledge-Based Systems},
volume = {262},
pages = {110243},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.110243},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122013399},
author = {Yuchen Yuan and Lituan Wang and Lei Zhang},
keywords = {Retinal vessel segmentation, Vessel connectivity, Structural priors, Adversarial learning},
abstract = {Despite having achieved human-level performance in retinal vessel segmentation, deep learning based methods still suffer from poor connectivity of vessels in the generated segmentation maps. Since most methods operate as pixelwise classifiers, the vessel structure is ignored during the optimization of the segmentation network. To address this problem, a novel framework is proposed to enhance the vessel connectivity by incorporating the vessel structure into the segmentation network. First, to obtain the structural priors, the vessel structural priors extraction module (VSPEM) is proposed; VSPEM employs the powerful feature extraction ability of the convolutional autoencoder. After being pretrained, the proposed VSPEM can be used to extract useful latent features from the ground truths, which perform as the structural priors in segmentation. Then, the segmentation network is enforced to generate results that follow the distribution of the learned priors via adversarial learning. We have validated our method on three publicly available datasets, i.e., the DRIVE, CHASE_DB1 and STARE, and the state-of-the-art experimental results achieved on the above datasets demonstrate the efficacy of the proposed framework. Moreover, we show that the proposed framework is independent of segmentation models and can further improve model performance on vessel connectivity without introducing extra memory or a computational burden.}
}
@article{SUCHOW2017522,
title = {Evolution in Mind: Evolutionary Dynamics, Cognitive Processes, and Bayesian Inference},
journal = {Trends in Cognitive Sciences},
volume = {21},
number = {7},
pages = {522-530},
year = {2017},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2017.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S1364661317300773},
author = {Jordan W. Suchow and David D. Bourgin and Thomas L. Griffiths},
keywords = {Bayesian inference, cognitive processes, creativity, evolution, learning, memory},
abstract = {Evolutionary theory describes the dynamics of population change in settings affected by reproduction, selection, mutation, and drift. In the context of human cognition, evolutionary theory is most often invoked to explain the origins of capacities such as language, metacognition, and spatial reasoning, framing them as functional adaptations to an ancestral environment. However, evolutionary theory is useful for understanding the mind in a second way: as a mathematical framework for describing evolving populations of thoughts, ideas, and memories within a single mind. In fact, deep correspondences exist between the mathematics of evolution and of learning, with perhaps the deepest being an equivalence between certain evolutionary dynamics and Bayesian inference. This equivalence permits reinterpretation of evolutionary processes as algorithms for Bayesian inference and has relevance for understanding diverse cognitive capacities, including memory and creativity.}
}
@article{GIERISCH200972,
title = {Factors associated with annual-interval mammography for women in their 40s},
journal = {Cancer Epidemiology},
volume = {33},
number = {1},
pages = {72-78},
year = {2009},
issn = {1877-7821},
doi = {https://doi.org/10.1016/j.cdp.2009.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0361090X0900021X},
author = {Jennifer M. Gierisch and Suzanne C. O’Neill and Barbara K. Rimer and Jessica T. DeFrank and J. Michael Bowling and Celette Sugg Skinner},
keywords = {Breast neoplasms, Guideline adherence, Health behavior, Middle aged, Attitude to health, Patient compliance, Mass screening, Female, Risk factor, Health knowledge},
abstract = {Background: Evidence is mounting that annual mammography for women in their 40s may be the optimal schedule to reduce morbidity and mortality from breast cancer. Few studies have assessed predictors of repeat mammography on an annual interval among these women. Methods: We assessed mammography screening status among 596 insured Black and Non-Hispanic white women ages 43–49. Adherence was defined as having a second mammogram 10–14 months after a previous mammogram. We examined socio-demographic, medical and healthcare-related variables on receipt of annual-interval repeat mammograms. We also assessed barriers associated with screening. Results: 44.8% of the sample were adherent to annual-interval mammography. A history of self-reported abnormal mammograms, family history of breast cancer and never having smoked were associated with adherence. Saying they had not received mammography reminders and reporting barriers to mammography were associated with non-adherence. Four barrier categories were associated with women's non-adherence: lack of knowledge/not thinking mammograms are needed, cost, being too busy, and forgetting to make/keep appointments. Conclusions: Barriers we identified are similar to those found in other studies. Health professionals may need to take extra care in discussing mammography screening risk and benefits due to ambiguity about screening guidelines for women in their 40s, especially for women without family histories of breast cancer or histories of abnormal mammograms. Reminders are important in promoting mammography and should be coupled with other strategies to help women maintain adherence to regular mammography.}
}
@incollection{PINTARIC20162367,
title = {Towards Outcomes-Based Education of Computer-Aided Chemical Engineering},
editor = {Zdravko Kravanja and Miloš Bogataj},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {38},
pages = {2367-2372},
year = {2016},
booktitle = {26th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-63428-3.50399-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780444634283503994},
author = {Zorka Novak Pintarič and Zdravko Kravanja},
keywords = {Computer-Aided, Chemical Engineering, Education, Bologna process, Learning Outcomes},
abstract = {Chemical engineering education is nowadays increasingly supported by the use of various computational tools as the employers’ requirements for computing skills of graduates are growing too. However, students often acquire computational skills in an unsystematic manner due to a lack of defining and applying computer-based outcomes within the syllabuses suitable for the particular level of the Bologna three-cycle system. This paper bridges this gap by providing the review of the essential learning outcomes in the computer-aided chemical engineering education during all three cycles. The identified outcomes gradually progress from application-based competencies up to more advanced process modeling ones based on knowledge synthesis and creation. Accordingly, the educational strategies and curricula can be redesigned in order to integrate courses more efficiently both horizontally and vertically, and upgrade the use of computational tools.}
}
@article{YURKOVICH2017431,
title = {A Padawan Programmer’s Guide to Developing Software Libraries},
journal = {Cell Systems},
volume = {5},
number = {5},
pages = {431-437},
year = {2017},
issn = {2405-4712},
doi = {https://doi.org/10.1016/j.cels.2017.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S2405471217303368},
author = {James T. Yurkovich and Benjamin J. Yurkovich and Andreas Dräger and Bernhard O. Palsson and Zachary A. King},
abstract = {With the rapid adoption of computational tools in the life sciences, scientists are taking on the challenge of developing their own software libraries and releasing them for public use. This trend is being accelerated by popular technologies and platforms, such as GitHub, Jupyter, R/Shiny, that make it easier to develop scientific software and by open-source licenses that make it easier to release software. But how do you build a software library that people will use? And what characteristics do the best libraries have that make them enduringly popular? Here, we provide a reference guide, based on our own experiences, for developing software libraries along with real-world examples to help provide context for scientists who are learning about these concepts for the first time. While we can only scratch the surface of these topics, we hope that this article will act as a guide for scientists who want to write great software that is built to last.}
}
@article{LIU2020102176,
title = {Using a new approach for revealing the spatiotemporal patterns of functional urban polycentricity: A case study in the Tokyo metropolitan area},
journal = {Sustainable Cities and Society},
volume = {59},
pages = {102176},
year = {2020},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2020.102176},
url = {https://www.sciencedirect.com/science/article/pii/S2210670720301633},
author = {Kai Liu and Yuji Murayama and Toshiaki Ichinose},
keywords = {Functional urban area detection, Functional urban polycentricity, Multi-step Decision-making Newman algorithm, Tokyo Master Plan, Tokyo metropolitan area},
abstract = {This research designs a new approach by modifying the Fast-Newman algorithm for better implementing the process of detecting functional urban areas (FUAs) and further revealing the spatiotemporal patterns of functional urban polycentricity through 20 view-windows of each FUA in the Tokyo metropolitan area (TMA) by using geo-tagged big data. Through the 20 view-windows of our GIS microscope, it is possible to uncover patterns of functional connections and daily urban rhythms under the same layout of the functional urban structure in the TMA. Furthermore, our findings can elucidate the double-sided thinking by combining the explanations of functional urban polycentricity with the policy effects of the Tokyo Master Plan (TMP) from the perspective of area-byarea analysis across the entire TMA. Our results imply that the functional urban structure of the TMA is a four-level, annular pattern. The TMP still has room for the improvement toward sustainable urban planning in the TMA. Based on the investigation on the patterns of functional urban polycentricity, this research has obtained many hints and clues for improving the TMP. Rethinking the effectiveness of the TMP can also provide trustworthy academic verification and provide suggestions about concrete amendments that can enlighten future urban planning.}
}
@article{WANG2023105931,
title = {Precision safety management (PSM): A novel and promising approach to safety management in the precision era},
journal = {Safety Science},
volume = {157},
pages = {105931},
year = {2023},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2022.105931},
url = {https://www.sciencedirect.com/science/article/pii/S0925753522002703},
author = {Bing Wang and Miaoting Yun and Qiong Liu and Yuanjie Wang},
keywords = {Precision, Safety management, precision safety management (PSM), Safety information, Safety risk},
abstract = {Precision is the ultimate goal of safety management. Over the past few decades, technological advances and applications (e.g., advances and applications of information technology) in safety management and its research have given safety managers the ability to pursue and realize relatively precise safety management based on sufficient and precise safety information. It can be said that safety management has entered the precision era. Therefore, in the present and future, developing novel approaches to precise prevention and control of safety risks is an inevitable safety management trend. This paper proposed a new and promising approach to safety management called precision safety management (PSM). The main objective of this paper was to answer the following five basic questions regarding PSM from a theoretical perspective: (i) What is PSM? (ii) Why is it necessary to develop PSM? (iii) What are the relationships between PSM and other concepts? (iv) What does PSM do? (v) How does an organization use PSM? Additionally, this paper presented the application of the PSM approach to the precise prevention and control of safety risks in a chemical industrial park as a case study. The main contributions of this paper are a theoretical framework for PSM, and a practical case for PSM. This study can help researchers and practitioners understand PSM and lay the foundation for its future research and practice.}
}
@article{ESPINOSAMIRELESDEVILLAFRANCA2024101042,
title = {Network traffic management via exclusive roads for altruistic vehicles under mixed traffic equilibrium},
journal = {Transportation Research Interdisciplinary Perspectives},
volume = {25},
pages = {101042},
year = {2024},
issn = {2590-1982},
doi = {https://doi.org/10.1016/j.trip.2024.101042},
url = {https://www.sciencedirect.com/science/article/pii/S2590198224000289},
author = {Alonso {Espinosa Mireles de Villafranca} and Claudio Roncoli},
keywords = {Automated vehicles, Traffic management, Altruistic routing, Multiclass equilibrium, Genetic algorithm, Network design},
abstract = {We present a computational study of network ensembles with two types of coexisting vehicle classes: an altruistically routing vehicle (ARV) class – potentially automated vehicles that are routed to reduce total system travel time – and a selfishly routing vehicle (SRV) class, corresponding to human-driven vehicles. We investigate the performance of these networks when some links are reserved for exclusive use by the ARVs. The goal of these interventions is to avoid or mitigate the detrimental effects of the SRVs on the costs of the ARVs. We formulate the problem as a bi-level network design problem, where the upper level deals with optimising the choice of ARV-exclusive links minimising the statistical dispersion of used-route costs, while the lower level finds the corresponding traffic equilibrium under static traffic assignment conditions. We tackle the ARV-exclusive link selection with a genetic algorithm, where the fitness of solutions is based on the dispersion of the costs of routes used by ARVs. The mixed equilibrium is found by solving a multi-class static traffic assignment problem, with constraints on the SRV flows on the ARV-exclusive links. SRVs attempt to minimise their personal travel time, whilst ARVs attempt to drive the flows to system optimal. Our approach is effective in reducing the per-vehicle travel cost of the ARVs to below that of the SRVs, making altruistic routing a more attractive option on average. Our results are consistent across networks with different structures and demand levels.}
}
@article{MCDANIELS200333,
title = {Decision structuring to alleviate embedding in environmental valuation},
journal = {Ecological Economics},
volume = {46},
number = {1},
pages = {33-46},
year = {2003},
issn = {0921-8009},
doi = {https://doi.org/10.1016/S0921-8009(03)00103-4},
url = {https://www.sciencedirect.com/science/article/pii/S0921800903001034},
author = {Timothy L McDaniels and Robin Gregory and Joseph Arvai and Ratana Chuenpagdee},
keywords = {Embedding, Structured decision process, Environmental valuation, Value-focused thinking, Small group process, Fisheries valuation},
abstract = {Embedding is the widely-observed phenomenon that a good is assigned a higher value if evaluated on its own rather than as part of a more inclusive set. Embedding is considered a serious problem affecting the quality of many environmental management and health risk policy judgments. This paper presents the results of an experiment involving of a structured, small-group approach for conducting environmental policy evaluations. It focuses on eliciting problem-specific values and discussion among participants about the pros and cons of multiple project alternatives, in the context of tradeoffs between fisheries production and electricity generation from dams. Study results show a significant reduction in embedding, which is viewed as an improvement in the quality of the preference judgments compared with a standard contingent valuation (CV) approach.}
}