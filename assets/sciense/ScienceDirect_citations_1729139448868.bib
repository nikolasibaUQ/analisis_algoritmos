@article{BEJAR2005117,
title = {Sensor networks and distributed CSP: communication, computation and complexity},
journal = {Artificial Intelligence},
volume = {161},
number = {1},
pages = {117-147},
year = {2005},
note = {Distributed Constraint Satisfaction},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2004.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S000437020400150X},
author = {Ramón Béjar and Carmel Domshlak and Cèsar Fernández and Carla Gomes and Bhaskar Krishnamachari and Bart Selman and Magda Valls},
keywords = {Distributed CSP benchmark, Phase transitions, Randomized combinatorial search, Communication network delays, NP-completeness},
abstract = {We introduce SensorDCSP, a naturally distributed benchmark based on a real-world application that arises in the context of networked distributed systems. In order to study the performance of Distributed CSP (DisCSP) algorithms in a truly distributed setting, we use a discrete-event network simulator, which allows us to model the impact of different network traffic conditions on the performance of the algorithms. We consider two complete DisCSP algorithms: asynchronous backtracking (ABT) and asynchronous weak commitment search (AWC), and perform performance comparison for these algorithms on both satisfiable and unsatisfiable instances of SensorDCSP. We found that random delays (due to network traffic or in some cases actively introduced by the agents) combined with a dynamic decentralized restart strategy can improve the performance of DisCSP algorithms. In addition, we introduce GSensorDCSP, a plain-embedded version of SensorDCSP that is closely related to various real-life dynamic tracking systems. We perform both analytical and empirical study of this benchmark domain. In particular, this benchmark allows us to study the attractiveness of solution repairing for solving a sequence of DisCSPs that represent the dynamic tracking of a set of moving objects.}
}
@incollection{ERICSSON200112256,
title = {Protocol Analysis in Psychology},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {12256-12262},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/01598-9},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767015989},
author = {K.A. Ericsson},
abstract = {Protocol analysis is the name for the methodology for eliciting, transcribing, and encoding verbal reports of thoughts into objective data for evaluating and testing theories of thinking. Philosophers since Aristotle have introspected on their own thinking as a means to analyze the structure of their thought processes. However, introspective analysis of one's thoughts and behavior was found to be reactive. In response to these criticisms a general theoretical framework was developed for how participants could verbalize their thinking without influencing the course of their thinking. Instructions to elicit such immediate reports were developed and shown to uncover thinking without the reactive effects due to explanations and descriptions of their thinking. Rigorous methods for analyzing verbal reports have been developed based on a formal analysis of the tasks. Short segments of verbal reports are coded into formal categories and the resulting data show similar reliability and validity as other forms of data on cognitive processes, such as reaction times and eye fixations. This general framework for collecting and analyzing verbal reports of thinking has been applied to laboratory studies of memory, problem solving, and decision making, and to everyday life in the study of expert performance and text comprehension.}
}
@article{HAMZI2023133853,
title = {Learning dynamical systems from data: A simple cross-validation perspective, part IV: Case with partial observations},
journal = {Physica D: Nonlinear Phenomena},
volume = {454},
pages = {133853},
year = {2023},
issn = {0167-2789},
doi = {https://doi.org/10.1016/j.physd.2023.133853},
url = {https://www.sciencedirect.com/science/article/pii/S0167278923002075},
author = {Boumediene Hamzi and Houman Owhadi and Yannis Kevrekidis},
keywords = {Learning dynamical systems, Kernel flows, Partial observations, Computational graph completion},
abstract = {A simple and interpretable way to learn a dynamical system from data is to interpolate its governing equations with a kernel. In particular, this strategy is highly efficient (both in terms of accuracy and complexity) when the kernel is data-adapted using Kernel Flows (KF) (Owhadi and Yoo, 2019), (which uses gradient-based optimization to learn a kernel based on the premise that a kernel is good if there is no significant loss in accuracy if half of the data is used for interpolation). In this work, we extend previous work on learning dynamical systems using Kernel Flows (Hamzi and Owhadi, 2021; Darcy et al. 2021; Lee et al. 2023; Darcy et al. 2023; Owhadi and Romit Maulik, 2021) to the case of learning vector-valued dynamical systems from time-series observations that are partial/incomplete in the state space. The method combines Kernel Flows with Computational Graph Completion.}
}
@article{CAI2004135,
title = {Why do U.S. and Chinese students think differently in mathematical problem solving?: Impact of early algebra learning and teachers’ beliefs},
journal = {The Journal of Mathematical Behavior},
volume = {23},
number = {2},
pages = {135-167},
year = {2004},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2004.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732312304000124},
author = {Jinfa Cai},
abstract = {This paper reports two studies that examined the impact of early algebra learning and teachers’ beliefs on U.S. and Chinese students’ thinking. The first study examined the extent to which U.S. and Chinese students’ selection of solution strategies and representations is related to their opportunity to learn algebra. The second study examined the impact of teachers’ beliefs on their students’ thinking through analyzing U.S. and Chinese teachers’ scoring of student responses. The results of the first study showed that, for the U.S. sample, students who have formally learned algebraic concepts are as likely to use visual representations as those who have not formally learned algebraic concepts in their problem solving. For the Chinese sample, students rarely used visual representations whether or not they had formally learned algebraic concepts. The findings of the second study clearly showed that U.S. and Chinese teachers view students’ responses involving concrete strategies and visual representations differently. Moreover, although both U.S. and Chinese teachers value responses involving more generalized strategies and symbolic representations equally high, Chinese teachers expect 6th graders to use the generalized strategies to solve problems while U.S. teachers do not. The research reported in this paper contributed to our understanding of the differences between U.S. and Chinese students’ mathematical thinking. This research also established the feasibility of using teachers’ scoring of student responses as an alternative and effective way of examining teachers’ beliefs.}
}
@article{LOTURCO2022104059,
title = {The knowledge and skill content of production complexity},
journal = {Research Policy},
volume = {51},
number = {8},
pages = {104059},
year = {2022},
note = {Special Issue on Economic Complexity},
issn = {0048-7333},
doi = {https://doi.org/10.1016/j.respol.2020.104059},
url = {https://www.sciencedirect.com/science/article/pii/S0048733320301372},
author = {Alessia {Lo Turco} and Daniela Maggioni},
keywords = {Occupational complexity, Services, Regional growth, STEM},
abstract = {In this paper we investigate the labour content of complex products. By exploiting O*NET information on the skill and knowledge required by occupations, we find that the product complexity measure suggested by Hausmann and Hidalgo (2009) is highly intensive in STEM knowledge and in Science, Mathematics and Critical Thinking skill requirements. We then propose a new measure of occupational complexity based on these occupational features. Among other advantages, this indicator has the merit to measure complexity for service industries that, so far, has never been measured. In an empirical model of the growth of USA Metropolitan Areas (MSAs), we find that MSAs whose initial industrial structure embeds a higher level of occupational complexity experience higher real per capita GDP growth over the 2001–2017 period. The occupational complexity measure is a stronger predictor of growth than other metrics of industries’ occupational and task content as well as compared to indicators of local occupational and industrial composition. When we separately compute occupational complexity of service and manufacturing industries and delve into their specific role for long run growth, we find a prominent role of the occupation complexity embedded in local services with respect to the one embedded in local manufacturing. Our baseline evidence is corroborated in the context of the NUTS3 regions of France over the period 2010–2017.}
}
@article{GILL2018733,
title = {Data to Decision and Judgment Making – a Question of Wisdom},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {30},
pages = {733-738},
year = {2018},
note = {18th IFAC Conference on Technology, Culture and International Stability TECIS 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.11.205},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318328702},
author = {Karamjit S Gill},
keywords = {algorithms, artificial intelligence, big data, calculation, decision, judgment, wisdom},
abstract = {The technological waves of super artificial intelligence, big data, algorithms, and machine learning continue to impact our thinking and actions, thereby affecting the ways individuals, professions and institutions make judgments. On the one hand, there is an argument that more data and knowledge together with the cyber physical system of industry4.0 will automatically push society along some track toward a better world for all. On the other hand, we hear worrying voices of the imponderable downsides of powerful new cyber-, bio-, Nano-technologies, and synthetic biology. In the age of uncertainties, big data and the algorithm, how is the decision and judgment making process being affected?}
}
@article{HERNIMAN2021373,
title = {Interrelationships between depressive symptoms and positive and negative symptoms of recent onset schizophrenia spectrum disorders: A network analytical approach},
journal = {Journal of Psychiatric Research},
volume = {140},
pages = {373-380},
year = {2021},
issn = {0022-3956},
doi = {https://doi.org/10.1016/j.jpsychires.2021.05.038},
url = {https://www.sciencedirect.com/science/article/pii/S0022395621003071},
author = {Sarah E. Herniman and Lisa J. Phillips and Stephen J. Wood and Sue M. Cotton and Edith J. Liemburg and Kelly A. Allott},
keywords = {Comorbidity, Co-occurrence, Early psychosis, Depression, Affect},
abstract = {Objective
There is a need to better understand the interrelationships between positive and negative symptoms of recent-onset schizophrenia spectrum disorders (SSD) and co-occurring depressive symptoms. Aims were to determine: (1) whether depressive symptoms are best conceptualised as distinct from, or intrinsic to, positive and negative symptoms; and (2) bridging symptoms.
Methods
Network analysis was applied to data from 198 individuals with depressive and psychotic symptoms in SSD from the Psychosis Recent Onset GRoningen Survey (PROGR-S). Measures were: Montgomery–Åsberg Depression Rating Scale and Positive and Negative Syndrome Scale.
Results
Positive symptoms were just as likely to be associated with depressive and negative symptoms, and had more strong associations with depressive than negative symptoms. Negative symptoms were more likely to be associated with depressive than positive symptoms, and had more strong associations with depressive than positive symptoms. Suspiciousness and stereotyped thinking bridged between positive and depressive symptoms, and apparent sadness and lassitude between negative and depressive symptoms.
Conclusions
Depressive symptoms might be best conceptualised as intrinsic to positive and negative symptoms pertaining to deficits in motivation and interest in the psychotic phase of SSD. Treatments targeting bridges between depressive and positive symptoms, and depressive and such negative symptoms, might prevent or improve co-occurring depressive symptoms, or vice-versa, in the psychotic phase of SSD.}
}
@article{WISTEN199777,
title = {Distributed computation of dynamic traffic equilibria},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {5},
number = {2},
pages = {77-93},
year = {1997},
note = {Parallel Computing in Transport Research},
issn = {0968-090X},
doi = {https://doi.org/10.1016/S0968-090X(97)00003-X},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X9700003X},
author = {M.B. Wisten and M.J. Smith},
abstract = {The dynamic traffic assignment problem is formulated in the space of splitting rates rather than link and route flows. A distributed algorithm for computation of dynamic user-equilibria is specified. The algorithm has been implemented on a Meiko Computing Surface with 32 T800 processors and some numerical results are given. We do not yet have a general proof of convergence for the algorithm but we have been able to demonstrate convergence with all test networks used.}
}
@article{LI2023110701,
title = {Graph neural network architecture search for rotating machinery fault diagnosis based on reinforcement learning},
journal = {Mechanical Systems and Signal Processing},
volume = {202},
pages = {110701},
year = {2023},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2023.110701},
url = {https://www.sciencedirect.com/science/article/pii/S088832702300609X},
author = {Jialin Li and Xuan Cao and Renxiang Chen and Xia Zhang and Xianzhen Huang and Yongzhi Qu},
keywords = {Rotating machinery, Fault diagnosis, Graph neural network, Neural architecture search, Reinforcement learning},
abstract = {In order to improve the accuracy of fault diagnosis, researchers are constantly trying to develop new diagnostic models. However, limited by the inherent thinking of human beings, it has always been difficult to build a pioneering architecture for rotating machinery fault diagnosis. In order to solve this problem, this paper uses reinforcement learning algorithm based on adjacency matrix to carry out network architecture search (NAS) of rotating machinery fault diagnosis model. A reinforcement learning agent for deep deterministic policy gradient (DDPG) is developed based on actor–critic neural networks. The observation state of reinforcement learning is used to develop the graph neural network (GNN) diagnosis model, and the diagnosis accuracy is fed back to the agent as a reward for updating the reinforcement learning parameters. The MFPT bearing fault datasets and the developed gear pitting fault experimental data are used to validate the proposed network architecture search method based on reinforcement learning (RL-NAS). The proposed method is proved to be practical and effective in various aspects such as fault diagnosis ability, search space, search efficiency and multi-working condition performance.}
}
@article{GREGORY198254,
title = {Current design thinking: 24 papers from Design 79, I Chem E Midlands Branch (available from (ChemE Rugby) 336 pp, £15},
journal = {Design Studies},
volume = {3},
number = {1},
pages = {54},
year = {1982},
issn = {0142-694X},
doi = {https://doi.org/10.1016/0142-694X(82)90084-9},
url = {https://www.sciencedirect.com/science/article/pii/0142694X82900849},
author = {Sydney Gregory}
}
@article{HIPOLITO2023103510,
title = {Breaking boundaries: The Bayesian Brain Hypothesis for perception and prediction},
journal = {Consciousness and Cognition},
volume = {111},
pages = {103510},
year = {2023},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2023.103510},
url = {https://www.sciencedirect.com/science/article/pii/S1053810023000478},
author = {Inês Hipólito and Michael Kirchhoff},
keywords = {Bayesian Brain Hypothesis, Modularity of the Mind, Cognitive processes, Informational boundaries},
abstract = {This special issue aims to provide a comprehensive overview of the current state of the Bayesian Brain Hypothesis and its standing across neuroscience, cognitive science and the philosophy of cognitive science. By gathering cutting-edge research from leading experts, this issue seeks to showcase the latest advancements in our understanding of the Bayesian brain, as well as its potential implications for future research in perception, cognition, and motor control. A special focus to achieve this aim is adopted in this special issue, as it seeks to explore the relation between two seemingly incompatible frameworks for the understanding of cognitive structure and function: the Bayesian Brain Hypothesis and the Modularity Theory of the Mind. In assessing the compatibility between these theories, the contributors to this special issue open up new pathways of thinking and advance our understanding of cognitive processes.}
}
@article{CHARPIN2012613,
title = {A computational linear elastic fracture mechanics-based model for alkali–silica reaction},
journal = {Cement and Concrete Research},
volume = {42},
number = {4},
pages = {613-625},
year = {2012},
issn = {0008-8846},
doi = {https://doi.org/10.1016/j.cemconres.2012.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0008884612000051},
author = {Laurent Charpin and Alain Ehrlacher},
keywords = {Alkali-Aggregate Reaction (C), Expansion (C), Microcracking (B), Energy Criterion, Particle Size distribution (B)},
abstract = {A fracture mechanics model for alkali–silica reaction (ASR) is presented that deals with the case of a concrete made up of dense spherical aggregates. Chemistry and diffusion (of ions and gel) are not modelled. The focus is put on the mechanical consequences of the progressive replacement of the aggregates by a less dense gel. A ring-shaped crack then appears in the cement paste depending on the pressure build-up, according to an incremental energy criterion. The stored elastic energy and deformation of each configuration are determined assuming that each aggregate is embedded in an infinite cement paste matrix, through Finite Element Analysis. We note a very different behaviour of aggregates of different sizes. Adding the contributions of different aggregates leads to an estimate of the free expansion of a concrete of given aggregate size distribution. Parameters of the model are identified, providing a good fit to experiments taken from Multon's work.}
}
@article{BRYANSMITH2023105405,
title = {Real-time social media sentiment analysis for rapid impact assessment of floods},
journal = {Computers & Geosciences},
volume = {178},
pages = {105405},
year = {2023},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2023.105405},
url = {https://www.sciencedirect.com/science/article/pii/S0098300423001097},
author = {Lydia Bryan-Smith and Jake Godsall and Franky George and Kelly Egode and Nina Dethlefs and Dan Parsons},
keywords = {Social media, Sentiment analysis, Flooding, Artificial Intelligence},
abstract = {Traditional approaches to flood modelling mostly rely on hydrodynamic physical simulations. While these simulations can be accurate, they are computationally expensive and prohibitively so when thinking about real-time prediction based on dynamic environmental conditions. Alternatively, social media platforms such as Twitter are often used by people to communicate during a flooding event, but discovering which tweets hold useful information is the key challenge in extracting information from posts in real time. In this article, we present a novel model for flood forecasting and monitoring that makes use of a transformer network that assesses the severity of a flooding situation based on sentiment analysis of the multimodal inputs (text and images). We also present an experimental comparison of a range of state-of-the-art deep learning methods for image processing and natural language processing. Finally, we demonstrate that information induced from tweets can be used effectively to visualise fine-grained geographical flood-related information dynamically and in real-time.}
}
@incollection{KUMAR2024147,
title = {Chapter Eight - Machine learning model for teaching and emotional intelligence},
editor = {Muskan Garg and Deepika Koundal},
booktitle = {Emotional AI and Human-AI Interactions in Social Networking},
publisher = {Academic Press},
pages = {147-168},
year = {2024},
isbn = {978-0-443-19096-4},
doi = {https://doi.org/10.1016/B978-0-443-19096-4.00014-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443190964000146},
author = {Mohit Kumar and Syam Machinathu Parambil Gangadharan and Nabanita Choudhury},
keywords = {Cognitive thinking, Design thinking, E-Learning, Emotional intelligence, Intelligent quotient, Social neuroscience},
abstract = {Education that is ongoing and permanent for the purpose of adaptable up-skilling and retraining has been identified as a contributory factor, along with a relentless race against time, fast scientific progress, and unanticipated challenges. Students in postsecondary learning require mechanisms that can enable long-term, dependable knowledge production and storage, and this is particularly true in the age after a pandemic that occurred during the development of new technologies. There has been an explosion of e-learning platforms and methodologies that have been developed to remedy this issue; however, not all of them have been as successful as would be ideal. This type of new knowledge is very difficult to execute properly; it needs complex, careful educational and interface design to perform as well as it does and keep learners interested. This framework was designed and developed in this study. This technology was employed to support a cutting-edge pedagogic study based on neuroscience that was offered to faculty at universities of higher education. This framework was designed and developed in this study. This technology was employed to support a cutting-edge pedagogic study based on neuroscience that was offered to faculty at universities of higher education. Having a high intelligence quotient does not guarantee a successful and happy life. Success also necessitates self-awareness and emotional control. Our idea was to create a computational paradigm that would educate students in both programming and emotional intelligence. This experiment was successful in addressing the problem of excessive screen use by providing a student interface without displays.}
}
@article{SHIRALKAR2023100115,
title = {An intelligent method for supply chain finance selection using supplier segmentation: A payment risk portfolio approach},
journal = {Cleaner Logistics and Supply Chain},
volume = {8},
pages = {100115},
year = {2023},
issn = {2772-3909},
doi = {https://doi.org/10.1016/j.clscn.2023.100115},
url = {https://www.sciencedirect.com/science/article/pii/S2772390923000240},
author = {Kedar Shiralkar and Arunkumar Bongale and Satish Kumar and Anupkumar M. Bongale},
keywords = {Supply chain finance (SCF), Supplier segmentation, Supplier categorization, Risk portfolio model, Supply chain sustainability, Supplier relationship management, Modern portfolio theory, Trade credit, Factoring, Dynamic discounting},
abstract = {The COVID-19 pandemic-driven financial crisis grew significant interest among firms to adopt supply chain finance (SCF) to optimize working capital for the financial stability of the supply chain. However, it is impractical for firms with a diverse and extensive supplier base to strategize the SCF solutions for individual suppliers by assessing their financial risk. Hence, this study conceptualizes an intelligent method to demonstrate how supplier segmentation based on suppliers’ payment risk portfolios helps supply chain practitioners to assess suppliers’ financial risk and strategize manageable supply chain finance solutions for them. This method employs a stochastic optimization model to compute suppliers’ optimum payment risk portfolios and generate a supplier segmentation matrix to offer supply chain practitioners the cognitive ability to select appropriate SCF solutions for their suppliers. The proposed method can be implemented into an AI-driven explainable recommendation system to aid supply chain practitioners in applying smart strategic thinking in supply chain finance decision-making.}
}
@article{PITOWSKY1996161,
title = {Laplace's demon consults an oracle: The computational complexity of prediction},
journal = {Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics},
volume = {27},
number = {2},
pages = {161-180},
year = {1996},
issn = {1355-2198},
doi = {https://doi.org/10.1016/1355-2198(96)85115-X},
url = {https://www.sciencedirect.com/science/article/pii/135521989685115X},
author = {Itamar Pitowsky}
}
@article{KARI2022102843,
title = {The Sabatier principle as a tool for discovery and engineering of industrial enzymes},
journal = {Current Opinion in Biotechnology},
volume = {78},
pages = {102843},
year = {2022},
issn = {0958-1669},
doi = {https://doi.org/10.1016/j.copbio.2022.102843},
url = {https://www.sciencedirect.com/science/article/pii/S095816692200177X},
author = {Jeppe Kari and Kay Schaller and Gustavo A Molina and Kim Borch and Peter Westh},
abstract = {The recent breakthrough in all-atom, protein structure prediction opens new avenues for a range of computational approaches in enzyme design. These new approaches could become instrumental for the development of technical biocatalysts, and hence our transition toward more sustainable industries. Here, we discuss one approach, which is well-known within inorganic catalysis, but essentially unexploited in biotechnology. Specifically, we review examples of linear free-energy relationships (LFERs) for enzyme reactions and discuss how LFERs and the associated Sabatier Principle may be implemented in algorithms that estimate kinetic parameters and enzyme performance based on model structures.}
}
@incollection{KALET2014479,
title = {Chapter 5 - Computational Models and Methods},
editor = {Ira J. Kalet},
booktitle = {Principles of Biomedical Informatics (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {479-578},
year = {2014},
isbn = {978-0-12-416019-4},
doi = {https://doi.org/10.1016/B978-0-12-416019-4.00005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124160194000056},
author = {Ira J. Kalet},
keywords = {Computational models and methods, Computing with genes, Computing with proteins, Computing with cells, Natural language processing, State machines, Dynamic models, Stochastic processes},
abstract = {This chapter introduces additional methods for deriving useful results from data and for creating complex models of biological processes. These methods include: search through data suitably organized, as sequences, or as networks, natural language processing, and modeling with state machines.}
}
@article{OZENCIRA2023101273,
title = {Mapping research on musical creativity: A bibliometric review of the literature from 1990 to 2022},
journal = {Thinking Skills and Creativity},
volume = {48},
pages = {101273},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101273},
url = {https://www.sciencedirect.com/science/article/pii/S1871187123000433},
author = {Gözde Ozenc-Ira},
keywords = {Musical creativity, Creativity, Bibliometric review, Science mapping, VOSviewer},
abstract = {This study aims to map the research literature on musical creativity that was published from 1990 to 2022 by using metadata extracted from 1,177 Web of Science-indexed publications in terms of trends in publications and citations data, leading journals, authors, institutions/organizations, and countries, collaborative networks between authors, institutions, and countries, and trends in keyword frequencies and co-occurrences. The main findings of this study are that (1) research on musical creativity has undergone an incipient phase and has had a growing scientific interest since the mid-2000s, (2) musical creativity is a relatively more specific research field compared to general creativity research that has been represented by more specific sub-fields, e.g., music psychology and ethnomusicology, (3) a small number of scholars – especially from the USA, England, Russia, Spain, Australia, and some countries from South Europe – have made the more impactful contribution as regards musical creativity, (4) there is a small number of research collaborations among scholars, yet the collaborative networks among countries and institutions occur intercontinentally, (5) musical creativity research is growing with cross-disciplinary links with several branches of psychology, neurosciences, cognitive sciences, education, sociology, arts and humanities, and computer sciences, and (6) eight main topical foci have been founded in the literature from 1990 to date – i.e., computational creativity, processes of improvisation, improvisation teaching and learning, interactions/collaboration during improvisation, effects of improvisation practice, innovative music technology, esthetic aspect of everyday creativity, and music therapy. Further research on musical creativity could map the literature by focusing on contextual themes.}
}
@article{SAJID2023103174,
title = {Thermal case classification of solar-powered cars for binary tetra hybridity nanofluid using Cash and Carp method with Hamilton-Crosser model},
journal = {Case Studies in Thermal Engineering},
volume = {49},
pages = {103174},
year = {2023},
issn = {2214-157X},
doi = {https://doi.org/10.1016/j.csite.2023.103174},
url = {https://www.sciencedirect.com/science/article/pii/S2214157X2300480X},
author = {Tanveer Sajid and Wasim Jamshed and Nek Muhammad Katbar and Mohamed R. Eid and Assmaa Abd-Elmonem and Nesreen Sirelkhtam Elmki Abdalla and Sayed M. {El Din} and Gilder Cieza Altamirano},
keywords = {Solar sports car, Solar sheet, Reiner-Philippoff tetrhybrid nanofluid, Thermal radiation, Heat generation},
abstract = {Solar energy is the most important source of thermal energy that comes from the sun. This kind of energy has enormous potential applications in fields of technology such as photovoltaic panels, renewable power, solar light poles, and solar pumps used for water extraction. The era in which we are living is all about the applications of solar energy in industrial sectors most importantly in solar sports car manufacturing. This article presents a new way of thinking about the heat transport analyses of photovoltaic hybrid vehicles, by factoring Casson-Sutterby liquid with the inclusion of various effects like variable thermal conduction, thermal radiation, heat generation, and tetrahybrid nanoparticles. To solve the modelled equations in regards to both momentum and energy, another well-computational approach known as the Cash and Carp method was used. The effects of a wide variety of factors on temperature, shear stress, and velocity fields, as well as the surface drag coefficient and Nusselt number, are briefly described and illustrated in the form of tables and figures. It then found that the thermal radiation, heat production, and thermal conductivity parameters and insertion of agglomerative tetrhybrid nanoparticles in the base fluid amplify heat transfer rate, it has been shown that the performance of the solar car increases in terms of heat transition. In comparison to standard nanofluid, tetrahybrid nanofluid is the most effective medium for the transmission of heat. From the regression analysis, it is observed that the error in terms of Nusselt number is smaller 0.0151 for the case ε=1.5, and increases to 0.0151 in the case of ε=2.5. Relative percentage error is smaller 4.62% in the case of heat generation Q=0.7 but a maximum of 15.8% in the case of thermal radiation Rd=2.}
}
@article{STORAASLI1993349,
title = {Computational mechanics analysis tools for parallel-vector supercomputers},
journal = {Computing Systems in Engineering},
volume = {4},
number = {4},
pages = {349-354},
year = {1993},
note = {Parallel Computational Methods for Large-Scale Structural Analysis and Design},
issn = {0956-0521},
doi = {https://doi.org/10.1016/0956-0521(93)90002-E},
url = {https://www.sciencedirect.com/science/article/pii/095605219390002E},
author = {O.O. Storaasli and D.T. Nguyen and M.A. Baddourah and J. Qin},
abstract = {Computational algorithms for structural analysis on parallel-vector supercomputers are reviewed. These parallel algorithms, developed by the authors, are for the assembly of structural equations, “out-of-core” strategies for linear equation solution, massively distributed-memory equation solution, unsymmetric equation solution, general eigen-solution, geometrically nonlinear finite element analysis, design sensitivity analysis for structural dynamics, optimization algorithm and domain decomposition. The source code for many of these algorithms is available from NASA Langley.}
}
@article{MIASNIKOVA202126,
title = {Cross-frequency phase coupling of brain oscillations and relevance attribution as saliency detection in abstract reasoning},
journal = {Neuroscience Research},
volume = {166},
pages = {26-33},
year = {2021},
issn = {0168-0102},
doi = {https://doi.org/10.1016/j.neures.2020.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0168010219305772},
author = {Aleksandra Miasnikova and Gleb Perevoznyuk and Olga Martynova and Mikhail Baklushev},
keywords = {Abstract reasoning, Salience, Phase synchronization, Cross-frequency coupling, Phase-to-phase coupling, EEG},
abstract = {Abstract reasoning is associated with the ability to detect relations among objects, ideas, events. It underlies the understanding of other individuals’ thoughts and intentions. In natural settings, individuals have to infer relevant associations that have proven to be reliable or precise predictors. Salience theory suggests that the attribution of meaning to stimulus depends on their contingency, saliency, and relevance to adaptation. So far, subjective estimates of relevance have mostly been explored in motivation and implicit learning. Mechanisms underlying formation of associations in abstract thinking with regard to their subjective relevance, or salience, are not clear. Applying novel computational methods, we investigated relevance detection in categorization tasks in 17 healthy individuals. Two models of relevance detection were developed: a conventional one with nouns from the same semantic category, an aberrant one based on an insignificant common feature. Control condition introduced non-related words. The participants were to detect either a relevant principle or an insignificant feature to group presented words. In control condition they inferred that the stimuli were irrelevant to any grouping idea. Cross-frequency phase coupling analysis revealed statistically distinct patterns of synchronization representing search and decision in the models of normal and aberrant relevance detection. Significantly distinct frontotemporal functional networks with central and parietal components in the theta and alpha frequency bands may reflect differences in relevance detection.}
}
@article{AIGBAVBOA20173003,
title = {Sustainable Construction Practices: “A Lazy View” of Construction Professionals in the South Africa Construction Industry},
journal = {Energy Procedia},
volume = {105},
pages = {3003-3010},
year = {2017},
note = {8th International Conference on Applied Energy, ICAE2016, 8-11 October 2016, Beijing, China},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2017.03.743},
url = {https://www.sciencedirect.com/science/article/pii/S1876610217308068},
author = {Clinton Aigbavboa and Ifije Ohiomah and Thulisile Zwane},
keywords = {Climate change, sustainable thinking, sustainable construction practices, South Africa},
abstract = {The construction industry has been found to cause damaging effects to the environment by means of waste generation, energy and water depletion and several other forms of damage to the environment. This damage has led to experts and environmentalist calling for a sustainable way of carrying out construction activities. Thus, this study addresses the challenges hindering the adoption of sustainable construction practices in the South Africa construction industry. The data used in this research were sourced from both primary and secondary sources. The primary data was collected through a questionnaire aimed at practicing construction professional in the South African construction industry. Indicative Findings from the questionnaire survey revealed that the foremost challenges faced by South African construction industry towards the adoption of sustainable construction practices is the assumption (a lazy view) of additional cost to building projects, followed by limited understanding of the benefits of sustainable construction amongst others. The study contributes to sustainability thinking in the South African construction industry; and it is recommended that strategies and actions should be pursued actively to speed up the process in creating a sustainable-oriented construction industry, which is paramount towards building a sustainable future.}
}
@article{CHILMON2020106870,
title = {Modelling and simulation considerations for an end-to-end supply chain system},
journal = {Computers & Industrial Engineering},
volume = {150},
pages = {106870},
year = {2020},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2020.106870},
url = {https://www.sciencedirect.com/science/article/pii/S0360835220305659},
author = {Barbara Chilmon and Nicoleta S. Tipi},
keywords = {Simulation, End-to-end supply chain, Systematic literature review},
abstract = {The efforts of this review paper are twofold: to provide an insightful examination of various contributions to knowledge surrounding simulation methods within an end-to-end supply chain and to guide research agenda by indicating generic elements required to model such systems using simulation. The authors examined 255 publications from 21 peer-reviewed journals in the field of an end-to-end supply chain and simulation using a systematic literature review approach. Each publication was thoroughly reviewed to capture best practices and key characteristics relative to simulation modelling techniques used in the context of complex end-to-end supply chain systems. This allowed for identification of generic elements required to model such systems, which were grouped into Structural, Computational and System Organization pillars. This research contributes to the body of knowledge by defining generic aspects of simulation modelling techniques used to study properties and attributes of complex end-to-end supply chains. The paper advances the theoretical understanding of the simulation methods used and applicability of simulation methodology in modelling end-to-end supply chain systems. The research presents the key findings from the use of simulation in modelling end-to-end supply chains and the main ways in which this modelling technique has informed research and practise.}
}
@article{MANDAVE2023100276,
title = {Bio-inspired computing algorithms in dementia diagnosis – a application-oriented review},
journal = {Results in Control and Optimization},
volume = {12},
pages = {100276},
year = {2023},
issn = {2666-7207},
doi = {https://doi.org/10.1016/j.rico.2023.100276},
url = {https://www.sciencedirect.com/science/article/pii/S2666720723000784},
author = {Deepa D. Mandave and Lalit V. Patil},
keywords = {Dementia, Biomotivated algorithms, Image segmentation, Meta-heuristic, Alzheimer, Optimization, Feature selection},
abstract = {Dementia is a major neurocognitive disease which affects memory, thinking skills, attitudes, and social behavior, extremely causing disturbances in daily routine activities and social activities. Alzheimer is the most general form of dementia in the elderly. Recently, biomotivated techniques have become famous in the domain of healthcare and have obtained appreciable success. This review shows that these techniques are mostly utilized to resolve various problems such as image segmentation, feature selection, classification, and optimization in the detection of various disorders like cancer, anemia, Alzheimer, kidney and skin diseases. It is observed that the dementia diagnosis was performed using classical approaches which led to reduced performance (accuracy, precision). This performance parameter can be enhanced by using biomotivated techniques. This paper presents a comprehensive analysis of the different role of biomotivated metaheuristics in the domain of dementia diagnosis with a detailed analysis of published work. The results showed that a biomotivated technique plays an important role in dementia diagnosis.}
}
@incollection{DORFLER202057,
title = {Artificial Intelligence},
editor = {Steven Pritzker and Mark Runco},
booktitle = {Encyclopedia of Creativity (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Oxford},
pages = {57-64},
year = {2020},
isbn = {978-0-12-815615-5},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.23863-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093245238637},
author = {Viktor Dörfler},
keywords = {AI, AI paradigms, Artificial intelligence, Artificial neural networks, Cognition, Creativity, Intuition, Learning, Machine learning, Mind, Mind and machine, Narrow AI, Thinking, Thinking machines, Wide AI},
abstract = {ARTIFICIAL INTELLIGENCE is a label coined to describe machines that can perform something humans would perform through thinking. In this chapter, I am looking at artificial intelligence (AI) specifically in the context of creativity. My view is inevitably a personal one, as for the time being, the answers to the tough questions on AI entail working with beliefs more so than with facts, opinions more so than hard evidence. What matters most in “AI Creativity” is how we define creativity, as this definition will determine whether we can consider AI to be creative, now or in the future. It is also important to explore what kind of impact AI has or may have on human creativity.}
}
@article{WORBOYS199885,
title = {Computation with imprecise geospatial data},
journal = {Computers, Environment and Urban Systems},
volume = {22},
number = {2},
pages = {85-106},
year = {1998},
issn = {0198-9715},
doi = {https://doi.org/10.1016/S0198-9715(98)00023-4},
url = {https://www.sciencedirect.com/science/article/pii/S0198971598000234},
author = {Michael Worboys},
abstract = {Imprecision in spatial data arises from the granularity or resolution at which observations of phenomena are made, and from the limitations imposed by computational representations, processing and presentational media. Precision is an important component of spatial data quality, and a key to appropriate integration of collections of data sets. Previous work of the author provides a theoretical foundation for imprecision of spatial data resulting from finite granularities, and gives the beginnings of an approach to reasoning with such data using methods similar to rough set theory. This paper develops the theory further, and extends the work to a model that includes both spatial and semantic components. Notions such as observation, schema, frame of discernment and vagueness are examined and formalised.}
}
@article{XU2024102430,
title = {A temporal approach to online discussion during disasters: Applying SIR infectious disease model to predict topic growth and examining effects of temporal distance},
journal = {Public Relations Review},
volume = {50},
number = {2},
pages = {102430},
year = {2024},
issn = {0363-8111},
doi = {https://doi.org/10.1016/j.pubrev.2024.102430},
url = {https://www.sciencedirect.com/science/article/pii/S0363811124000092},
author = {Sifan Xu and Xinyan Zhao and Jie Chen},
keywords = {Disaster, SIR, Computational modeling, SIR model, Twitter big data, Climate change, Topic growth, Construal level},
abstract = {Discussions on social media during major disasters are robust and often have multiple frames of reference. Temporal perspectives, however, are still lacking in current understandings of social-mediated discussions during disasters and crises, but incorporating temporal perspectives can significantly enhance environmental scanning efforts as prescribed in the issues management framework. The purpose of the current research is twofold: to apply and validate the SIR (Susceptible-Infectious-Recovered) model to examine topics’ growth over time on social media and to understand how future orientation of social media users (an indicator of temporal distance) affects their construal of a disaster through supervised machine learning. We based our analysis on Twitter discussions during the Texas winter storm in 2021. Results of the study show great fit of the SIR model for topic growth, and that temporal distance affects users’ construal of the event in line with core predictions of construal level theory. Theoretical, methodological, and practical implications on social-mediated discussions related to climate change-induced and -intensified disasters and issues management are discussed.}
}
@article{BEECH2023105401,
title = {Consequences of phonological variation for algorithmic word segmentation},
journal = {Cognition},
volume = {235},
pages = {105401},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2023.105401},
url = {https://www.sciencedirect.com/science/article/pii/S0010027723000355},
author = {Caroline Beech and Daniel Swingley},
keywords = {Language acquisition, Computational modeling, Word segmentation, Phonological variation},
abstract = {Over the first year, infants begin to learn the words of their language. Previous work suggests that certain statistical regularities in speech could help infants segment the speech stream into words, thereby forming a proto-lexicon that could support learning of the eventual vocabulary. However, computational models of word segmentation have typically been tested using language input that is much less variable than actual speech is. We show that using actual, transcribed pronunciations rather than dictionary pronunciations of the same speech leads to worse segmentation performance across models. We also find that phonologically variable input poses serious problems for lexicon building, because even correctly segmented word forms exhibit a complex, many-to-many relationship with speakers' intended words. Many phonologically distinct word forms were actually the same intended word, and many identical transcriptions came from different intended words. The fact that previous models appear to have substantially overestimated the utility of simple statistical heuristics suggests a need to consider the formation of the lexicon in infancy differently.}
}
@article{KASNECI2023102274,
title = {ChatGPT for good? On opportunities and challenges of large language models for education},
journal = {Learning and Individual Differences},
volume = {103},
pages = {102274},
year = {2023},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2023.102274},
url = {https://www.sciencedirect.com/science/article/pii/S1041608023000195},
author = {Enkelejda Kasneci and Kathrin Sessler and Stefan Küchemann and Maria Bannert and Daryna Dementieva and Frank Fischer and Urs Gasser and Georg Groh and Stephan Günnemann and Eyke Hüllermeier and Stephan Krusche and Gitta Kutyniok and Tilman Michaeli and Claudia Nerdel and Jürgen Pfeffer and Oleksandra Poquet and Michael Sailer and Albrecht Schmidt and Tina Seidel and Matthias Stadler and Jochen Weller and Jochen Kuhn and Gjergji Kasneci},
keywords = {Large language models, Artificial intelligence, Education, Educational technologies},
abstract = {Large language models represent a significant advancement in the field of AI. The underlying technology is key to further innovations and, despite critical views and even bans within communities and regions, large language models are here to stay. This commentary presents the potential benefits and challenges of educational applications of large language models, from student and teacher perspectives. We briefly discuss the current state of large language models and their applications. We then highlight how these models can be used to create educational content, improve student engagement and interaction, and personalize learning experiences. With regard to challenges, we argue that large language models in education require teachers and learners to develop sets of competencies and literacies necessary to both understand the technology as well as their limitations and unexpected brittleness of such systems. In addition, a clear strategy within educational systems and a clear pedagogical approach with a strong focus on critical thinking and strategies for fact checking are required to integrate and take full advantage of large language models in learning settings and teaching curricula. Other challenges such as the potential bias in the output, the need for continuous human oversight, and the potential for misuse are not unique to the application of AI in education. But we believe that, if handled sensibly, these challenges can offer insights and opportunities in education scenarios to acquaint students early on with potential societal biases, criticalities, and risks of AI applications. We conclude with recommendations for how to address these challenges and ensure that such models are used in a responsible and ethical manner in education.}
}
@article{BEDNORZ2024101169,
title = {Effects of domain-specific linguistic factors on the difficulty of mathematics tasks},
journal = {The Journal of Mathematical Behavior},
volume = {75},
pages = {101169},
year = {2024},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2024.101169},
url = {https://www.sciencedirect.com/science/article/pii/S0732312324000464},
author = {David Bednorz and Michael Kleine and Rudolf {vom Hofe}},
keywords = {Mathematical tasks, Task features, linguistic complexity, Task difficulty},
abstract = {Linguistic features as a task-related feature influence the difficulty of mathematical tasks. To reduce this influence (e.g., in testing situations), studies on linguistic simplification focus on modifying linguistic features. These studies show little or no effect on increasing test performance. An open question is whether a quantitative–exploratory approach with texts from a specific domain can be an additional model for reducing the linguistic influence on mathematical tasks. To answer this question, generalized linear mixed models were used to determine the effects of linguistic factors, the requirements of the items, and the effects of linguistic factors when differentiating the requirements of the items, while controlling for further person- and item-related effects. The results show that linguistic factors can have either a negative or positive influence on test performance. The findings indicate that for mathematics assessments and teaching, it might be essential to consider the influence of language factors and task requirements.}
}
@article{VANSANTEN19902001,
title = {Computational advances in catalyst modelling.},
journal = {Chemical Engineering Science},
volume = {45},
number = {8},
pages = {2001-2011},
year = {1990},
issn = {0009-2509},
doi = {https://doi.org/10.1016/0009-2509(90)80073-N},
url = {https://www.sciencedirect.com/science/article/pii/000925099080073N},
author = {R.A. {van Santen}},
keywords = {Molecular Catalysis, Theoretical Chemistry, Catalyst Modelling, Zeolite Stability, Theoretical Kinitics.},
abstract = {Fruitful theoretical approaches to predict catalyst stability, to simulate transition states or assist catalyst characterization become available due to the computational possibilities generated by supercomputers. Advances in theoretical chemistry and catalysis provide the conceptual framework that enables application in catalyst modelling. Especially in zeolite catalysis computational techniques are increasingly applied. Because of their well-defined structures they are very suitable for the application of graphics approaches. Techniques have been developed to determine interaction-potentials on the basis of quantumchemical cluster-calculations and to verify them by comparison with experimental and spectroscopic data. Stimulated by quantum chemical studies in chemisorption as well as organometallic chemistry, computational studies of reaction intermediates in homogeneous as well as heterogeneous catalytic reactions have been undertaken. The development of potential energy surface parametrization schemes is of importance to enable the application of molecular dynamics studies to catalyst stability and reactivity}
}
@article{HILLERT2021103158,
title = {How did language evolve in the lineage of higher primates?},
journal = {Lingua},
volume = {264},
pages = {103158},
year = {2021},
issn = {0024-3841},
doi = {https://doi.org/10.1016/j.lingua.2021.103158},
url = {https://www.sciencedirect.com/science/article/pii/S0024384121001303},
author = {Dieter Hillert},
keywords = {Broca’s area, Comparative studies, Homo erectus, Language capacity, Neural circuits, Prehistoric artefacts},
abstract = {Speech components emerged in the hominin lineage before the rise of modern human behavior and were already in place in monkey species. Evidence from genetics to archaeological records points to an accumulative increase of those computational properties required for modern language. At about 2.4 mya, the polytypical species Homo erectus sensu lato (s.l.) appeared with significant cortical growth indicated by neural migration factors and fossil skulls. The evidence suggests that early Homo erectus s.l. was equipped with a computational capacity for premodern language. The same species developed Acheulean toolmaking and showed signs of a symbolic and aesthetic mind at about half a mya. We conclude that the modern language capacity evolved at around 1 mya in the merging species late Homo erectus s.l. and pre-archaic Homo sapiens.}
}
@article{YECKEL1998206,
title = {Three-dimensional computations of solution hydrodynamics during the growth of potassium dihydrogen phosphate: II. Spin down},
journal = {Journal of Crystal Growth},
volume = {191},
number = {1},
pages = {206-224},
year = {1998},
issn = {0022-0248},
doi = {https://doi.org/10.1016/S0022-0248(98)00102-X},
url = {https://www.sciencedirect.com/science/article/pii/S002202489800102X},
author = {Andrew Yeckel and Yuming Zhou and Michael Dennis and Jeffrey J. Derby},
keywords = {Fluid flow, Solution growth, Finite element model},
abstract = {Three-dimensional, time-dependent flows that occur in the Lawrence Livermore National Laboratory system for rapid growth of potassium dihydrogen phosphate (KDP) crystals from solution are studied using massively parallel finite element computations. The simulation reveals that excellent global mixing occurs during the spin-down phase of a time-dependent stirring cycle. The large scale fluid motions in the radial and axial directions that promote mixing are caused primarily by effects of platform geometry, but are augmented to some degree by the intrinsic tendency of a decelerating rotational flow to reverse direction within Ekman layers that form at the boundaries. Along with Part I of this work [Y. Zhou and J.J. Derby, J. Crystal Growth 180 (1997) 497], which emphasized spin up and steady rotation, significant advances have been made in our understanding of hydrodynamic phenomena in this system.}
}
@incollection{COXON2019179,
title = {Chapter 7 - Transforming Future Mobility},
editor = {Selby Coxon and Robbie Napper and Mark Richardson},
booktitle = {Urban Mobility Design},
publisher = {Elsevier},
pages = {179-214},
year = {2019},
isbn = {978-0-12-815038-2},
doi = {https://doi.org/10.1016/B978-0-12-815038-2.00007-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128150382000074},
author = {Selby Coxon and Robbie Napper and Mark Richardson},
keywords = {Innovation methodology, Design thinking, Future mobility},
abstract = {The book having built a picture of a driverless, accessible, positive experience and inventively built mobility landscape, now leverages the techniques of design thinking to demonstrate the tools of a future economy and how they might be applied to a range of future mobility speculations. This chapter demonstrates that a combination of technology developments and design thinking skills can generate inventive compelling solutions to mobility problems. The chapter is illustrated with examples of these research speculations.}
}
@article{KLIMUSOVA2016652,
title = {Psychometric Properties of the Learning Potential Test},
journal = {Procedia - Social and Behavioral Sciences},
volume = {217},
pages = {652-656},
year = {2016},
note = {Future Academy Multidisciplinary Conference “ICEEPSY & CPSYC & icPSIRS & BE-ci” 13–17 October 2015 Istanbul},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2016.02.089},
url = {https://www.sciencedirect.com/science/article/pii/S1877042816001142},
author = {Helena Klimusová and Petr Květon},
keywords = {psychometrics, admission test},
abstract = {The use of cognitive ability tests to help select highly performing students is becoming a standard in most major Czech universities. Such tests need to show good psychometric properties. To highlight the importance of these properties to the process of selection, this study explores the psychometric properties of the Learning Potential test, which is used as a selection criterion within the admission procedure at a major Czech university. This study's objective is to assess the psychometric properties of the Learning Potential Test and provide an insight into its structure. The Cronbach's alpha were computed to assess the internal consistency of the test. The structure of the items was explored by the factor analysis methods. Factor analysis indicated the anticipated structure of the test with two major factors - critical thinking/verbal reasoning abilities and numerical/spatial/analytical abilities. Since the role of admission tests in the process of selection new university students is crucial, it is essential to periodically reassess its psychometric characteristics to ensure that our test remain relevant and applicable.}
}
@article{CARLISLE1996248,
title = {Software Caching and Computation Migration in Olden},
journal = {Journal of Parallel and Distributed Computing},
volume = {38},
number = {2},
pages = {248-255},
year = {1996},
issn = {0743-7315},
doi = {https://doi.org/10.1006/jpdc.1996.0145},
url = {https://www.sciencedirect.com/science/article/pii/S0743731596901458},
author = {Martin C. Carlisle and Anne Rogers},
abstract = {Software caching and computation migration are mechanisms that satisfy remote references by either bringing a copy of the data to the computation or moving the computation to the data. We evaluate these mechanisms usingOlden, a system that, with minimal programmer annotations, provides parallelism for C programs that use recursively defined structures, such as trees, lists, and DAGs. We demonstrate that providing both software caching and computation migration can improve the performance of these programs, and provide a compile-time heuristic that selects between them for each pointer dereference. We have implemented the heuristic in Olden on the Thinking Machines CM-5. We describe our implementation and report on experiments with eleven benchmarks.}
}
@article{ZHOU1997497,
title = {Three-dimensional computations of solution hydrodynamics during the growth of potassium dihydrogen phosphate I. Spin up and steady rotation},
journal = {Journal of Crystal Growth},
volume = {180},
number = {3},
pages = {497-509},
year = {1997},
note = {Modelling in Crystal Growth},
issn = {0022-0248},
doi = {https://doi.org/10.1016/S0022-0248(97)00251-0},
url = {https://www.sciencedirect.com/science/article/pii/S0022024897002510},
author = {Yuming Zhou and Jeffrey J. Derby},
keywords = {Solution growth, Three-dimensional modeling, Fluid flow},
abstract = {A novel, massively parallel implementation of the Galerkin finite element method is used to study three-dimensional, time-dependent flows which occur during the rapid growth of potassium dihydrogen phosphate crystals from solution in a system employed by researchers at Lawrence Livermore National Laboratory. Computations for the hydrodynamics of system spin up and steady rotation indicate the importance of time-dependent flow phenomena and emphasize the significant role played by the support and crystal geometry in forming the complicated flows in this system. Predicted flow structures correlate well with experimental observations of inclusion formation.}
}
@article{MOLINARO20231150,
title = {A goal-centric outlook on learning},
journal = {Trends in Cognitive Sciences},
volume = {27},
number = {12},
pages = {1150-1164},
year = {2023},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2023.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S1364661323002073},
author = {Gaia Molinaro and Anne G.E. Collins},
keywords = {goals, learning, decision-making, reinforcement learning, rewards, abstraction, motivation, computational modeling},
abstract = {Goals play a central role in human cognition. However, computational theories of learning and decision-making often take goals as given. Here, we review key empirical findings showing that goals shape the representations of inputs, responses, and outcomes, such that setting a goal crucially influences the central aspects of any learning process: states, actions, and rewards. We thus argue that studying goal selection is essential to advance our understanding of learning. By following existing literature in framing goal selection within a hierarchy of decision-making problems, we synthesize important findings on the principles underlying goal value attribution and exploration strategies. Ultimately, we propose that a goal-centric perspective will help develop more complete accounts of learning in both biological and artificial agents.}
}
@article{KING1980313,
title = {Thinking: Readings in cognitive science: P.N. Johnson-Laird and P.C. Wason Cambridge University Press, 1977},
journal = {Artificial Intelligence},
volume = {13},
number = {3},
pages = {313-322},
year = {1980},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(80)90005-3},
url = {https://www.sciencedirect.com/science/article/pii/0004370280900053},
author = {Margaret King}
}
@article{MAHOWALD2024517,
title = {Dissociating language and thought in large language models},
journal = {Trends in Cognitive Sciences},
volume = {28},
number = {6},
pages = {517-540},
year = {2024},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2024.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S1364661324000275},
author = {Kyle Mahowald and Anna A. Ivanova and Idan A. Blank and Nancy Kanwisher and Joshua B. Tenenbaum and Evelina Fedorenko},
keywords = {large language models, language and thought, cognitive neuroscience, linguistic competence, computational modeling},
abstract = {Large language models (LLMs) have come closest among all models to date to mastering human language, yet opinions about their linguistic and cognitive capabilities remain split. Here, we evaluate LLMs using a distinction between formal linguistic competence (knowledge of linguistic rules and patterns) and functional linguistic competence (understanding and using language in the world). We ground this distinction in human neuroscience, which has shown that formal and functional competence rely on different neural mechanisms. Although LLMs are surprisingly good at formal competence, their performance on functional competence tasks remains spotty and often requires specialized fine-tuning and/or coupling with external modules. We posit that models that use language in human-like ways would need to master both of these competence types, which, in turn, could require the emergence of separate mechanisms specialized for formal versus functional linguistic competence.}
}
@article{SHEFFIELD2024100333,
title = {Understanding Cognitive Behavioral Therapy for Psychosis Through the Predictive Coding Framework},
journal = {Biological Psychiatry Global Open Science},
volume = {4},
number = {4},
pages = {100333},
year = {2024},
issn = {2667-1743},
doi = {https://doi.org/10.1016/j.bpsgos.2024.100333},
url = {https://www.sciencedirect.com/science/article/pii/S2667174324000466},
author = {Julia M. Sheffield and Aaron P. Brinen and Brandee Feola and Stephan Heckers and Philip R. Corlett},
keywords = {Belief updating, CBTp, Persecutory delusions, Predictive coding, Psychotherapy, Volatility},
abstract = {Psychological treatments for persecutory delusions, particularly cognitive behavioral therapy for psychosis, are efficacious; however, mechanistic theories explaining why they work rarely bridge to the level of cognitive neuroscience. Predictive coding, a general brain processing theory rooted in cognitive and computational neuroscience, has increasing experimental support for explaining symptoms of psychosis, including the formation and maintenance of delusions. Here, we describe recent advances in cognitive behavioral therapy for psychosis–based psychotherapy for persecutory delusions, which targets specific psychological processes at the computational level of information processing. We outline how Bayesian learning models employed in predictive coding are superior to simple associative learning models for understanding the impact of cognitive behavioral interventions at the algorithmic level. We review hierarchical predictive coding as an account of belief updating rooted in prediction error signaling. We examine how this process is abnormal in psychotic disorders, garnering noisy sensory data that is made sense of through the development of overly strong delusional priors. We argue that effective cognitive behavioral therapy for psychosis systematically targets the way sensory data are selected, experienced, and interpreted, thus allowing for the strengthening of alternative beliefs. Finally, future directions based on these arguments are discussed.}
}
@article{BARILE2022467,
title = {Platform-based innovation ecosystems: Entering new markets through holographic strategies},
journal = {Industrial Marketing Management},
volume = {105},
pages = {467-477},
year = {2022},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2022.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0019850122001614},
author = {Sergio Barile and Cristina Simone and Francesca Iandolo and Antonio Laudando},
keywords = {Platforms, Innovation ecosystems, Platform-based innovation ecosystems, Holographic strategies, Digital algorithms, Platform envelopment},
abstract = {The platformization seems to be a demiurgic force, increasingly (re)shaping this millennium and its socio-economic, technological and physical structures, institutions, and human lives. Innovation ecosystems are experiencing this platformization, leading to the rise of platform-based innovation ecosystems. However, the industrial and managerial literature still lacks a shared definition, a consistent theoretical and strategic framework to explain how platform-based innovation ecosystems emerge and replicate from market to market. This conceptual work attempts to fill those gaps by integrating the extant literature on innovation ecosystems in two ways. First, moving from the literature on innovation ecosystems and industry platforms, using systems thinking framing, it explains the platformization of innovation ecosystems through the double lens structure-system. Second, it identifies the holographic strategy as one of the typical patterns featuring platform-based innovation ecosystem envelopment beyond extant market boundaries. These conceptualizations have insightful theoretical, managerial, and policy implications. In particular, the work discusses the ecosystem as a valid unit of analysis for understanding such an unprecedented shaped-by-platform landscape. Then, it describes the growth strategies of the platform-based innovation ecosystem supporting the platform sponsor in mastering multipoint competition. Eventually, the study pinpoints crucial issues for policymakers in regulating the impact that platformization is having on society.}
}
@article{KELTNER2021216,
title = {A taxonomy of positive emotions},
journal = {Current Opinion in Behavioral Sciences},
volume = {39},
pages = {216-221},
year = {2021},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2021.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S2352154621000991},
author = {Dacher Keltner and Alan Cowen},
abstract = {Within social functionalist theory (SFT), emotions structure attachment relations, cooperative alliances, hierarchies, and collectives. Within this line of thinking, a rich array of positive emotions enable the formation and negotiation of these relationships. Guided by these arguments, we synthesize how top-down confirmatory studies and data-driven, computational studies converge on evidence for 11 positive emotions with distinct experience, expression, and physiology. This taxonomy includes amusement, awe, compassion, contentment, desire, love, joy, interest, pride, relief, and triumph. We conclude by considering how recent taxonomic efforts will advance emotion science in mapping the distinct forms and functions of the positive emotions.}
}
@article{TARMIZI2010384,
title = {Effects of Problem-based Learning Approach in Learning of Statistics among University Students},
journal = {Procedia - Social and Behavioral Sciences},
volume = {8},
pages = {384-392},
year = {2010},
note = {International Conference on Mathematics Education Research 2010 (ICMER 2010)},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2010.12.054},
url = {https://www.sciencedirect.com/science/article/pii/S1877042810021592},
author = {Rohani Ahmad Tarmizi and Sahar Bayat},
keywords = {Problem-based learning, E-learning, Statistic learning performance, Mental load},
abstract = {Current Mathematics Curriculum concerns are focused on students’ needs to think mathematically rather than just mathematical computation. Students should be able to develop more complex, abstract, and powerful mathematical structures. This can dramatically enable them to solve a broad variety of meaningful problems. Furthermore, students ought to become autonomous and self-motivated in their mathematical activities such as acquiring mathematical concepts, skills and problem solving; meta-cognitively aware of their mathematical thinking; highly motivated in mathematics learning and develop positive attitudes towards mathematical task. To achieve this learning goal, an investigation into efficient learning mode, the problem-based learning (PBL) was undertaken. PBL has been successfully applied in medical, engineering, economics, and accounting field but lack of evidence in mathematics field. This study examined possible outcomes of PBL among postgraduate students who were taking Educational Statistic course. Three statistic tests were employed to assess the students’ performance in statistic learning. The Meta-cognitive Awareness Inventory (MAI), which comprises of 52 items was used to assess the students’ meta-cognitive strategy in solving Educational Statistics problems. Students’ motivation towards the PBL learning was measured by Keller's Motivational Design Questionnaire with 36 items. Comparison of students’ performance based on three tests showed that there is significant diffrence between mean performance (F [2,28]=5.571, p<0.05). In addition, results indicated that there is significant positive effects on students meta-cognitive awareness (t [30]=3.358, p<0.05) and on students motivation level (t [30]=2.484, p<0.05) after undergoing PBL intervention.}
}
@article{BAI2011364,
title = {Prediction of human voluntary movement before it occurs},
journal = {Clinical Neurophysiology},
volume = {122},
number = {2},
pages = {364-372},
year = {2011},
issn = {1388-2457},
doi = {https://doi.org/10.1016/j.clinph.2010.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S1388245710005699},
author = {Ou Bai and Varun Rathi and Peter Lin and Dandan Huang and Harsha Battapady and Ding-Yu Fei and Logan Schneider and Elise Houdayer and Xuedong Chen and Mark Hallett},
keywords = {Human intention, Voluntary movement, Prediction, Movement-related cortical potentials (MRCP), Event-related desynchronization (ERD), Electroencephalography (EEG), Brain–computer interface (BCI), Consciousness},
abstract = {Objective
Human voluntary movement is associated with two changes in electroencephalography (EEG) that can be observed as early as 1.5s prior to movement: slow DC potentials and frequency power shifts in the alpha and beta bands. Our goal was to determine whether and when we can reliably predict human natural movement BEFORE it occurs from EEG signals ONLINE IN REAL-TIME.
Methods
We developed a computational algorithm to support online prediction. Seven healthy volunteers participated in this study and performed wrist extensions at their own pace.
Results
The average online prediction time was 0.62±0.25s before actual movement monitored by EMG signals. There were also predictions that occurred without subsequent actual movements, where subjects often reported that they were thinking about making a movement.
Conclusion
Human voluntary movement can be predicted before movement occurs.
Significance
The successful prediction of human movement intention will provide further insight into how the brain prepares for movement, as well as the potential for direct cortical control of a device which may be faster than normal physical control.}
}
@article{CHAKRAVARTY2010606,
title = {The creative brain – Revisiting concepts},
journal = {Medical Hypotheses},
volume = {74},
number = {3},
pages = {606-612},
year = {2010},
issn = {0306-9877},
doi = {https://doi.org/10.1016/j.mehy.2009.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S0306987709006926},
author = {Ambar Chakravarty},
abstract = {Summary
Creativity is a complex neuro-psycho-philosophical phenomenon which is difficult to define literally. Fundamentally it involves the ability to understand and express novel orderly relationships. The creative process involves four stages – preparation, incubation, illumination and verification. A high level of general intelligence, domain specific knowledge and special skills are necessary pre-requisites. It is possible that in addition, some creative people might have architectural alternations of specific portions of the posterior neocortex. Associated with such pre-requisites, the process of creative innovation (incubation and illumination stages) necessitates the need for an ability of divergent thinking, a novelty seeking behavior, some degree of suppression of latent inhibition and a subtle degree of frontal dysfunction. The author hypothesizes that these features are often inter-linked and subtle frontally disinhibited behavior is conducive towards creativity by allowing uninterrupted flow of creative thought possessing and opening up new avenues towards problem solving. Perhaps the most essential feature of the creative brain is its degree of connectivity – both inter-hemispheric and intra-hemispheric. Connectivity correlates or binds together functions of apparently structurally isolated domains on brain modules sub-serving different functions. It is felt that creative cognition is a self rewarding process where divergent thinking would promote connectivity through development of new synapses. In addition, the phenomenon of synaesthesia has often been observed in creative visual artists. Creative innovation often occurs during low arousal states and creative people often manifests features of affective disorders. This suggests a role of neurotransmitters in creative innovation. Dopaminergic pathways are involved in the novelty seeking attitude of creative people while norepinephrine levels are depressed during discovery of novel orderly relationships. The relationship between mood and catecholamines and that of creative cognition is often in an inverted U-shaped form. It is hypothesized that that subtle frontal dysfunction is a pre-requisite for creative cognition but here again the relationship is also in an inverted U-form.}
}
@article{BOYDDAVIS2018185,
title = {‘A dialogue between the real-world and the operational model’ – The realities of design in Bruce Archer’s 1968 doctoral thesis},
journal = {Design Studies},
volume = {56},
pages = {185-204},
year = {2018},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2017.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X17300893},
author = {Stephen {Boyd Davis} and Simone Gristwood},
keywords = {design history, philosophy of design, science of design, design research, systematic method},
abstract = {The article centres on a single document, the 1968 doctoral thesis of L. Bruce Archer. It traces Archer’s earlier publications and the sources that informed and inspired his thinking as a way of understanding his influential work at the Royal College of Art from 1962. Analysis suggests that Archer’s ambition for a rigorous ‘science of design’ inspired by linear algorithmic approaches was increasingly threatened with disruption by his experience of large, complex design projects. Reflecting on Archer's engagement with other models of designing, the article ends with Archer’s retrospective view and an account of his significantly altered opinions. Archer is located as both a theorist and someone fascinated by the commercial and practical aspects of designing.}
}
@article{KUAI2020101103,
title = {The extensible Data-Brain model: Architecture, applications and directions},
journal = {Journal of Computational Science},
volume = {46},
pages = {101103},
year = {2020},
note = {20 years of computational science},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2020.101103},
url = {https://www.sciencedirect.com/science/article/pii/S1877750320300752},
author = {Hongzhi Kuai and Ning Zhong},
keywords = {Artificial intelligence (AI), Brain informatics, Brain computing, Data-Brain, Brain big data, Web intelligence (WI), Intelligence systems},
abstract = {One of the key ideas in realizing human-like intelligence is to understand information-processing mechanisms in the human brain. Brain Informatics is a rapidly expanding interdisciplinary field to systematically utilize brain-related data, information and knowledge coming from the entire research process for in-depth brain investigation. In the past few years, a data-centric conceptual brain model, namely Data-Brain, has been proposed, providing the foundation for the systematic Brain Informatics methodology. The Data-Brain model constitutes a conceptual framework and detailed guideline for managing and analyzing brain big data. The development of Data-Brain model also demands the support from advanced technologies. This paper presents an extensible version of the Data-Brain with advanced computing techniques in the connected world. It provides a global understanding of how multidisciplinary techniques work together to tackle brain computing challenges. Particularly, the integrated K-I-D (Knowledge-Information-Data) loop is proposed, constructing a cycle as the thinking space to help pursue the systematic brain investigation, by which the extensible Data-Brain model continuously iterates and evolves through the never-ending learning. Such synergistic evolvement will power future progress for building intelligence systems and applications connected with the study of complex human brain.}
}
@article{ZHANG2022104545,
title = {Watching a hands-on activity improves students’ understanding of randomness},
journal = {Computers & Education},
volume = {186},
pages = {104545},
year = {2022},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2022.104545},
url = {https://www.sciencedirect.com/science/article/pii/S0360131522001166},
author = {Icy (Yunyi) Zhang and Mary C. Tucker and James W. Stigler},
keywords = {Hands-on demonstration, Computer simulation, Statistics education, Multimedia learning, Online instruction, Instructional sequence, Embodied cognition},
abstract = {Introductory statistics students struggle to understand randomness as a data generating process, and especially its application to the practice of data analysis. Although modern computational techniques for data analysis such as simulation, randomization, and bootstrapping have the potential to make the idea of randomness more concrete, representing such random processes with R code is not as easy for students to understand as is something like a coin-flip, which is both concrete and embodied. In this study, in the context of multimedia learning, we designed and tested the efficacy of an instructional sequence that preceded computational simulations with embodied demonstrations. We investigated the role that embodied hands-on movement might play in facilitating students’ understanding of the shuffle function in R. Our findings showed that students who watched a video of hands shuffling data written on pieces of paper learned more from a subsequent live-coding demonstration of randomization using R than did students only introduced to the concept using R. Although others have found an advantage of students themselves engaging in hands-on activities, this study showed that merely watching someone else engage can benefit learning. Implications for online and remote instruction are discussed.}
}
@article{LIU2024101910,
title = {Understanding ceiling temperature as a predictive design parameter for circular polymers},
journal = {Cell Reports Physical Science},
volume = {5},
number = {4},
pages = {101910},
year = {2024},
issn = {2666-3864},
doi = {https://doi.org/10.1016/j.xcrp.2024.101910},
url = {https://www.sciencedirect.com/science/article/pii/S2666386424001462},
author = {Xiaoyang Liu and Shivani Kozarekar and Alexander Shaw and Tie-Qi Xu and Eugene Y.-X. Chen and Linda J. Broadbelt},
keywords = {circular polymers, ceiling temperature, thermodynamic parameters, uncertainty propagation, density functional theory},
abstract = {Summary
The rise of polymeric materials marks a notable achievement of the past century, yet challenges in recycling have led to their accumulation in various environments. Efforts to address this include advancements in mechanical recycling, degradation processes, and chemical recycling techniques, particularly chemical recycling to monomer, which offers a path toward a circular economy for plastics. In this perspective, we discuss how ceiling temperature (Tc) can be used as a design parameter for circular (closed-loop recyclable) polymers and provide an overview of typical experimental approaches for deriving Tc, focusing on ΔHp and ΔSp as the key parameters for prediction. The concept of Tc is heavily embedded in the polymer literature and provides a simple but still useful way of quickly ranking different polymers in terms of their relative thermodynamic stability of polymer versus monomer states. While Tc in the bulk state as an intrinsic value is a desirable quantity, it is infeasible in many cases to measure equilibrium states in the bulk; thus, many researchers have focused on investigating Tc in solution, where there may be dependencies of Tc on the solvent, concentration, or other factors, resulting in a family of apparent Tc values at each set of conditions. We thus explore computational studies as a complement to experimental measurements of Tc. To this end, we focus here on the advantages, obstacles, and outlook of the establishment of predictive computational approaches to calculate key thermodynamic parameters related to polymer circularity, namely ΔHp, ΔSp, ΔGp, and Tc values.}
}
@article{CHOI199617,
title = {Computation and semiotic practice as compositional process},
journal = {Computers & Mathematics with Applications},
volume = {32},
number = {1},
pages = {17-35},
year = {1996},
issn = {0898-1221},
doi = {https://doi.org/10.1016/0898-1221(96)00084-3},
url = {https://www.sciencedirect.com/science/article/pii/0898122196000843},
author = {I. Choi},
keywords = {Dynamical systems, Semiotics, Computer music, Synergetics, Cognitive systems, Music composition, Chaos, Music synthesis},
abstract = {In sound computation, computational processes are brought into the acoustic domain by a set of formalized instructions for controlling parameters in synthesis engines and compositional algorithms. From the acoustic events, listeners often extract patterns or “musical objects” in their perception to the extent that certain associations are made external to the computational process. Perceived, musical objects rapidly become immutable, and that immutability may be considered a compositional problem. The problem is how to approach a compositional project for bringing new insights into play while, on one side, using the existing representational system such as symbolic language in computation, and on the other side, facing listeners' perceptual tendency to make external associations. For composers, the problem requires technical solutions as well as an ability to articulate the philosophical issues. This problem also exists in semiotics, a general study of signs, when one has to borrow language from existing linguistic systems in order to express a new thought without being trapped within the immutability of given linguistic sources. Semiotic practice is a discipline which emphasizes the function of semiotics to generate necessary discourse to examine the linguistic system in use and its logocentric tendency—the tendency towards known signs. We define semiotic practice as a signifying process in which meaning may be generated during that particular process under study; thus, meaning in semiotic practice is temporal context-dependent as a function of signifiers. The compositional problems involving sound computation for generating cases to support semiotic practice inquires about two tasks: 1.(1) how to design software which enables acoustic events to be observed as processes rather than observing sounds only as familiar objects or transformations featuring the recognition of objects, and2.(2) how to compose a piece of music so mutability of signs can be observed. To meet these problems, this paper examines perspectives on systems and cognition from multiple views such as semiotics, computation theories, and synergetics. We also discuss software designed for composition in terms of semiotic practice.}
}
@article{ELAZZAOUI2023119595,
title = {A digital twin-based edge intelligence framework for decentralized decision in IoV system},
journal = {Information Sciences},
volume = {649},
pages = {119595},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.119595},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523011805},
author = {Abir {El Azzaoui} and Sekione Reward Jeremiah and Neal N. Xiong and Jong Hyuk Park},
keywords = {IoV, Digital twin, Smart contracts, Smart transportation},
abstract = {The Internet of Vehicle (IoV) is an emerging technology for the development of future smart cities. With the fast and exponential growing rate of Internet of Things (IoT), the smart transportation field is ushering in a revolutionary advancement. Smart transportation systems facilitate better informed, more coordinated, and smarter use of transport networks, with the use of advanced information and communication technologies applied to vehicles to help improve traffic management, minimize congestion, improve safety, and ultimately provide a more intelligent use of transport networks. Smart transportation is an integral part of modern-day smart city projects. However, the world climate institution has reported that carbon emissions from the overall transportation system accounts for one-fifth of global carbon dioxide with a sum of around 24% of energy. Electric vehicles (EV) represent a solution for this issue, yet, it is not sustainable. The communication between EVs and a roadside unit (RSU), and the continuous computational power required to support an IoV system also requires a reliance on energy harvesting. With this in mind, in this paper, we propose a decentralized trust management solution for IoV systems to reduce both carbon footprint and offload the computation power required. Our solution resides in developing the digital twin of vehicles on an intelligent edge environment to simulate the physical vehicle and handle the required data processing. Also, we implement a smart contract model for fast, secure, and sustainable on-road battery recharging between EVs.}
}
@article{HULME2017345,
title = {From control to causation: Validating a ‘complex systems model’ of running-related injury development and prevention},
journal = {Applied Ergonomics},
volume = {65},
pages = {345-354},
year = {2017},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2017.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S000368701730159X},
author = {A. Hulme and P.M. Salmon and R.O. Nielsen and G.J.M. Read and C.F. Finch},
keywords = {Systems ergonomics, STAMP, Sports injury prevention, Running injury},
abstract = {Introduction
There is a need for an ecological and complex systems approach for better understanding the development and prevention of running-related injury (RRI). In a previous article, we proposed a prototype model of the Australian recreational distance running system which was based on the Systems Theoretic Accident Mapping and Processes (STAMP) method. That model included the influence of political, organisational, managerial, and sociocultural determinants alongside individual-level factors in relation to RRI development. The purpose of this study was to validate that prototype model by drawing on the expertise of both systems thinking and distance running experts.
Materials and methods
This study used a modified Delphi technique involving a series of online surveys (December 2016- March 2017). The initial survey was divided into four sections containing a total of seven questions pertaining to different features associated with the prototype model. Consensus in opinion about the validity of the prototype model was reached when the number of experts who agreed or disagreed with survey statement was ≥75% of the total number of respondents.
Results
A total of two Delphi rounds was needed to validate the prototype model. Out of a total of 51 experts who were initially contacted, 50.9% (n = 26) completed the first round of the Delphi, and 92.3% (n = 24) of those in the first round participated in the second. Most of the 24 full participants considered themselves to be a running expert (66.7%), and approximately a third indicated their expertise as a systems thinker (33.3%). After the second round, 91.7% of the experts agreed that the prototype model was a valid description of the Australian distance running system.
Conclusion
This is the first study to formally examine the development and prevention of RRI from an ecological and complex systems perspective. The validated model of the Australian distance running system facilitates theoretical advancement in terms of identifying practical system-wide opportunities for the implementation of sustainable RRI prevention interventions. This ‘big picture’ perspective represents the first step required when thinking about the range of contributory causal factors that affect other system elements, as well as runners' behaviours in relation to RRI risk.}
}
@article{JONES2001325,
title = {NMR quantum computation},
journal = {Progress in Nuclear Magnetic Resonance Spectroscopy},
volume = {38},
number = {4},
pages = {325-360},
year = {2001},
issn = {0079-6565},
doi = {https://doi.org/10.1016/S0079-6565(00)00033-9},
url = {https://www.sciencedirect.com/science/article/pii/S0079656500000339},
author = {J.A. Jones}
}
@incollection{1998361,
title = {Appendix B - Suggestions for further reading in computational biology},
editor = {Steven L. Salzberg and David B. Searls and Simon Kasif},
series = {New Comprehensive Biochemistry},
publisher = {Elsevier},
volume = {32},
pages = {361-365},
year = {1998},
booktitle = {Computational Methods in Molecular Biology},
issn = {0167-7306},
doi = {https://doi.org/10.1016/S0167-7306(08)60474-3},
url = {https://www.sciencedirect.com/science/article/pii/S0167730608604743}
}
@article{BELLA2023100509,
title = {Circular dichroism simulations of chiral buckybowls by means curvature analyses},
journal = {FlatChem},
volume = {40},
pages = {100509},
year = {2023},
issn = {2452-2627},
doi = {https://doi.org/10.1016/j.flatc.2023.100509},
url = {https://www.sciencedirect.com/science/article/pii/S2452262723000417},
author = {Giovanni Bella and Giuseppe Bruno and Antonio Santoro},
keywords = {Buckybowl, Chirality, Curvature, TD-DFT, Circular dichroism},
abstract = {A detailed understanding and interpretation of chiral properties of molecular systems, especially in condensed phase, often requires computational models that allow their structural and electronic features to be connected to the observed experimental spectra. The present paper is focused on modelling the circular dichroism spectra of chiral buckybowls, combining topological aspects and the density functional theory. For the first time Ball Pivoting Algorithm was proposed to hook up the chemical topology to the DFT through the surface reconstruction. Particularly, the gaussian curvature of a constructed probe set of corannulene and sumanene derivatives was used as discriminant parameter to benchmark a list of 10 functionals (B3LYP, B97D, M06-2X, HSEH1PBE, wB97XD, CAM-B3LYP, LC-wPBE, TPSSTPSS, mPW1PW91 and APFD). The latter provide to be noticeably accurate to reproduce the curvature effect of the considered molecules. A TD-DFT/BOMD mixed approach provided a comprehensive overview of the spectral chiral pattern prediction trends when multiple DFT functionals are scanned. The preliminary topological analysis efforts were then recompensed with the very precise computed CD spectra, again APFD confirmed as the leader functional, this time for TD-DFT vertical transition calculations. Therefore, we strongly recommend the use of the of dispersion embedded APFD functional coupled with the 6–311++G(2d,2p) basis set for the computation of the functionalized chiral buckybowls ECD spectra. © 2017 Elsevier Inc. All rights reserved.}
}
@article{YOON2021100865,
title = {United States and South Korean citizens’ interpretation and assessment of COVID-19 quantitative data},
journal = {The Journal of Mathematical Behavior},
volume = {62},
pages = {100865},
year = {2021},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2021.100865},
url = {https://www.sciencedirect.com/science/article/pii/S0732312321000262},
author = {Hyunkyoung Yoon and Cameron O’Neill Byerley and Surani Joshua and Kevin Moore and Min Sook Park and Stacy Musgrave and Laura Valaas and James Drimalla},
keywords = {COVID-19, Graphs, Representations of quantitative data, Rate of change, Exponential growth},
abstract = {We investigate United States and South Korean citizens’ mathematical schemes and how these schemes supported or hindered their attempts to assess the severity of COVID-19. We selected web and media-based COVID-19 data representations that we hypothesized citizens would interpret differently depending on their mathematical schemes. We included items that we conjectured would be easier or more difficult to interpret with schemes that prior research had reported were more or less productive, respectively. We used the representations during clinical interviews with 25 United States and seven South Korean citizens. We illustrate that citizens’ mathematical schemes (as well as their beliefs) impacted how they assessed the severity of COVID-19. We present vignettes of citizens’ schemes that inhibited interpreting representations of COVID-19 in ways compatible with the displayed quantitative data, schemes that aided them in assessing the severity of COVID-19, and beliefs about the reliability of scientific data that overrode their mathematical conclusions.}
}
@article{SAWLEY1994363,
title = {A comparative study of the use of the data-parallel approach for compressible flow calculations},
journal = {Parallel Computing},
volume = {20},
number = {3},
pages = {363-373},
year = {1994},
issn = {0167-8191},
doi = {https://doi.org/10.1016/S0167-8191(06)80019-0},
url = {https://www.sciencedirect.com/science/article/pii/S0167819106800190},
author = {M.L. Sawley and C.M. Bergman},
keywords = {Computational fluid dynamics, Euler equations, Data-parallel programming, Portability, Performance results},
abstract = {The results are presented of an investigation into the use of the data-parallel programming approach on four different massively-parallel computers: the MasPar MP-1 and MP-2 and the Thinking Machines CM-200 and CM-5. A code to calculate inviscid compressible flow, originally written in FORTRAN 77 for a traditional vector computer, has been re-written entirely in Fortran 90 to take advantage of the compilers available on the massively-parallel computers. It is shown that the discretization of the governing equations on a regular mesh is well adapted to data parallelism. For a typical test problem of supersonic flow through a ramped duct, computational speeds have been achieved using these massively-parallel computers that are superior to those obtained using a single processor of a Cray Y-MP. In addition, this study has enabled the question of code portability between the different computers to be assessed.}
}
@incollection{CAPPAI2024804,
title = {Molecular Dynamics Simulations of Thermal Transport in Solid State Systems},
editor = {Manuel Yáñez and Russell J. Boyd},
booktitle = {Comprehensive Computational Chemistry (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {804-820},
year = {2024},
isbn = {978-0-12-823256-9},
doi = {https://doi.org/10.1016/B978-0-12-821978-2.00095-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128219782000957},
author = {Antonio Cappai and Claudio Melis and Luciano Colombo and Riccardo Dettori},
keywords = {Atomistic simulations, Classical molecular dynamics, Computational methods, Harmonic and anharmonic vibrational properties, Lattice thermal conductivity, Nanoscale thermal transport, Non-equilibrium thermodynamics, Thermal management, Thermal sciences, Thermoelectricity},
abstract = {In this chapter, we provide a synoptic review of the theoretical/computational approaches currently used to characterize thermal transport at the nanoscale, a topic of paramount importance for several applications and technological thermal management requirements. We focus in particular on the description of the atomistic techniques based on equilibrium (EMD), non-equilibrium (NEMD), and approach to equilibrium (AEMD) molecular dynamics (MD), which allow to efficiently describe relatively large and structurally complex systems with a reduced computational cost as compared to fully "ab-initio" techniques. We describe the theoretical background for each simulation strategy, as well as their implementation in state-of-the-art MD codes by underlying their intrinsic limitations and providing strategies to control some of them. We finally perform a series of benchmark calculations on bulk crystalline silicon by showing that the estimated thermal conductivity is weakly dependent on the specific strategy actually employed, while the overall computational cost is largely dependent on it.}
}
@article{JIANG2024122157,
title = {Explicit potential function and fast algorithm for computing potentials in α×β conic surface resistor network},
journal = {Expert Systems with Applications},
volume = {238},
pages = {122157},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122157},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423026593},
author = {Xiaoyu Jiang and Gaojun Zhang and Yanpeng Zheng and Zhaolin Jiang},
keywords = {Resistor network, Chebyshev polynomials, DST-IV, Fast algorithm},
abstract = {Resistor network research is of great importance, yet many resistor networks and their large-scale fast computations have not received sufficient attention. This paper proposes a new resistor network with idiosyncratic shape, i.e., a α×β conic surface (CS) resistor network that resembles the upper part of a three-dimensional Dirac function. Utilizing the Recursion Transform (RT-V) method of Tan, a recursive matrix equation model is constructed based on Kirchhoff’s law and nodal voltages, which contains the modified tridiagonal Toeplitz matrix. By using the orthogonal matrix transformation, the eigenvalues and eigenvectors of the modified tridiagonal Toeplitz are obtained. The discrete sine transform of the fourth type (DST−IV) is utilized to solve node voltages, while the explicit potential function is represented by the Chebyshev polynomials of the second kind. In addition, explicit potential functions for some special cases are provided, and the potential distribution is illustrated using dynamic three-dimensional graph. To achieve a rapid calculation of the potential, a fast algorithm based on the multiplication of DST-IV with a vector is proposed. In the end, analysis of computational efficiency for the explicit potential function and the fast algorithm are shown.}
}
@article{COOPER2022100755,
title = {Balboa security v. M&M systems: Forensic accounting for determining commercial damages},
journal = {Journal of Accounting Education},
volume = {58},
pages = {100755},
year = {2022},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2021.100755},
url = {https://www.sciencedirect.com/science/article/pii/S0748575121000427},
author = {John R. Cooper and Brett S. Kawada},
keywords = {Forensic accounting, Commercial damages, Litigation, Supplier-customer relationship},
abstract = {The ability of accounting students to apply skills beyond traditional accounting in a thoughtful and analytical way is becoming increasingly important, especially in fraud detection and forensic accounting. This case provides an opportunity for students to use critical thinking and problem-solving skills in applying accounting knowledge to a supplier-customer commercial damages litigation matter. Students are provided with a fact pattern of a supplier-customer relationship where they analyze issues related to commercial damages stemming from sources common in real world forensic accounting cases. Students evaluate the facts, which include not only financial data but also interviews with key personnel of parties to the legal action, and demonstrate an understanding of the issues involved in the case through responses of questions regarding overriding forensic accounting and professional practice issues. Students will also prepare a written commercial damages report demonstrating the ability to effectively communicate their analyses.}
}
@article{DELEERSNYDER2024105602,
title = {A multidimensional AI-trained correction to the 1D approximate model for Airborne TDEM sensing},
journal = {Computers & Geosciences},
volume = {188},
pages = {105602},
year = {2024},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2024.105602},
url = {https://www.sciencedirect.com/science/article/pii/S0098300424000852},
author = {Wouter Deleersnyder and David Dudal and Thomas Hermans},
keywords = {Forward modelling, Machine Learning, Surrogate modelling, Electromagnetics, Airborne},
abstract = {The computational resources required to solve the full 3D inversion of time-domain electromagnetic data are immense. To overcome the time-consuming 3D simulations, we construct a surrogate model, more precisely, a data-driven statistical model to replace the 3D simulations. It is trained on 3D data and predicts the approximate output much faster. We construct a surrogate model that predicts the discrepancy between a 1D subsurface model and a deviation of the 1D assumption. The latter response is fastly computable with a semi-analytical 1D forward model. We exemplify the approach on a two-layered case. The results are encouraging even with few training samples. Given the computational cost related to the 3D simulations, there are limitations in the number of training samples that can be generated. In addition, certain applications require a wide range of parameters to be sampled, such as the electrical conductivity parameters in a saltwater intrusion case. The challenge of this work is achieving the best possible accuracy with only a few thousand samples. We propose to view the performance in terms of learning gain, representing the gain from the surrogate model whilst still acknowledging a residual discrepancy. Our works open new avenues for effectively simulating 3D TDEM data.}
}
@article{INDLEKOFER20021035,
title = {Number theory—probabilistic, heuristic, and computational approaches},
journal = {Computers & Mathematics with Applications},
volume = {43},
number = {8},
pages = {1035-1061},
year = {2002},
issn = {0898-1221},
doi = {https://doi.org/10.1016/S0898-1221(02)80012-8},
url = {https://www.sciencedirect.com/science/article/pii/S0898122102800128},
author = {K.-H Indlekofer},
keywords = {Probabilistic number theory, Asymptotic results on arithmetic function, Computational number theory, Stone-Cech compactification, Measure and integration on },
abstract = {After the description of the models of Kubilius, Novoselov and Schwarz, and Spilker, respectively, a probability theory for finitely additive probability measures is developed by use of the Stone-Cech compactification of N. The new model is applied to the result of Erdős and Wintner about the limit distribution of additive functions and to the famous result of Szemerédi in combinatorial number theory. Further, it is explained how conjectures on prime values of irreducible polynomials are used in the search for large prime twins and Sophie Germain primes.}
}
@incollection{NIEVERGELT1993167,
title = {Experiments in Computational Heuristics and Their Lessons for Software and Knowledge Engineering},
editor = {Marshall C. Yovits},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {37},
pages = {167-205},
year = {1993},
issn = {0065-2458},
doi = {https://doi.org/10.1016/S0065-2458(08)60405-2},
url = {https://www.sciencedirect.com/science/article/pii/S0065245808604052},
author = {Jurg Nievergelt},
abstract = {Publisher Summary
This chapter presents examples that illustrate a type of programming project increasingly prominent in the field of knowledge engineering. The examples are chosen on grounds of familiarity, without any claim to represent the field of heuristic programming at large. The chapter begins by describing some software projects in computational heuristics. These projects are presented as case studies of the interaction between software engineering and knowledge engineering that illustrate the decisive importance of a powerful software environment. The chapter presents the case of the smart game board and describes the main software tool for rapid prototyping of game implementations, needed to conduct experiments. It also describes a project involving heuristics and knowledge engineering that has been evolving without interruption for the past five years. It presents paradigms of software development favored by system designers and implementers of exploratory development projects, and explains why these are often diametrically opposed to the conventional software engineering lore.}
}
@article{HALLOWELL2023100240,
title = {Democratising or disrupting diagnosis? Ethical issues raised by the use of AI tools for rare disease diagnosis},
journal = {SSM - Qualitative Research in Health},
volume = {3},
pages = {100240},
year = {2023},
issn = {2667-3215},
doi = {https://doi.org/10.1016/j.ssmqr.2023.100240},
url = {https://www.sciencedirect.com/science/article/pii/S2667321523000240},
author = {Nina Hallowell and Shirlene Badger and Francis McKay and Angeliki Kerasidou and Christoffer Nellåker},
keywords = {Computational phenotyping, Rare disease, Diagnosis, AI, Qualitative interviews},
abstract = {Computational phenotyping (CP) technology uses facial recognition algorithms to classify and potentially diagnose rare genetic disorders on the basis of digitised facial images. This AI technology has a number of research as well as clinical applications, such as supporting diagnostic decision-making. Using the example of CP, we examine stakeholders’ views of the benefits and costs of using AI as a diagnostic tool within the clinic. Through a series of in-depth interviews (n ​= ​20) with: clinicians, clinical researchers, data scientists, industry and support group representatives, we report stakeholder views regarding the adoption of this technology in a clinical setting. While most interviewees were supportive of employing CP as a diagnostic tool in some capacity we observed ambivalence around the potential for artificial intelligence to overcome diagnostic uncertainty in a clinical context. Thus, while there was widespread agreement amongst interviewees concerning the public benefits of AI assisted diagnosis, namely, its potential to increase diagnostic yield and enable faster more objective and accurate diagnoses by up skilling non specialists and thereby enabling access to diagnosis that is potentially lacking, interviewees also raised concerns about ensuring algorithmic reliability, expunging algorithmic bias and that the use of AI could result in deskilling the specialist clinical workforce. We conclude that, prior to widespread clinical implementation, on-going reflection is needed regarding the trade-offs required to determine acceptable levels of bias and conclude that diagnostic AI tools should only be employed as an assistive technology within the dysmorphology clinic.}
}
@incollection{TONDEUR202424,
title = {Chapter 3 - Quality improvement movements},
editor = {Yves Tondeur},
booktitle = {Sustainable Quality Improvements for Isotope Dilution in Molecular Ultratrace Analyses},
publisher = {Elsevier},
pages = {24-70},
year = {2024},
isbn = {978-0-443-29034-3},
doi = {https://doi.org/10.1016/B978-0-443-29034-3.00022-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443290343000223},
author = {Yves Tondeur},
keywords = {Accreditation & technology-in-use, Commitment-based approach, Empirical vs. rational methods, Isomer selectivity, Isotope dilution, Known & documented quality, Legislating competition, Methods innovation rule, Performance assessment, Precision & trueness, Purpose of quality control samples, Quick fixes vs. fundamental solution, Structural conflicts},
abstract = {Emerging over recent years is the notion that quality improvements are hard to come by, when in fact, countless opportunities to integrate new developments are overlooked primarily because of the restrictive ways in which analytical protocols have been written and enforced, or the misconceptions about their function. A point of instability was reached. The manifestation of a quality malaise can be seen through the efforts by many to enhance quality by attempting to transfer the responsibility for quality back to those doing the work and to those who need the work products. The testing industry is now forced into abandoning old formal structures, mental models, and behaviors. Realigning our thinking, discovering the limits, and identifying the structural conflicts are essential if one wants to improve quality. This chapter makes clear that doing the same thing than before better is not enough, or to solely implement quick fixes can be wasteful; somehow, we need to ensure the application of the method coevolves with its environment. As chemists, what can we do?}
}
@article{GUO2024324,
title = {Optimization of robot manipulator configuration calibration by using Zhang neural network for repetitive motion},
journal = {Applied Mathematical Modelling},
volume = {134},
pages = {324-348},
year = {2024},
issn = {0307-904X},
doi = {https://doi.org/10.1016/j.apm.2024.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0307904X24002853},
author = {Pengfei Guo and Yunong Zhang and Shuai Li and Ning Tan},
keywords = {Temporally dependent quadratic programming, Filtered reciprocal-kind Zhang neural network, Lyapunov stability, Robot manipulator configuration calibration},
abstract = {High precision and low complexity control algorithm plays an important role in the developing of the end-effector instrumentation of different robot manipulators. In order to reduce the kinetic energy and the high-speed drift phenomenon of the repetitive motion tracking task, the robot manipulator needs to calibrate its configuration. In this paper, we formulate the configuration calibration of the robot manipulator for the repetitive motion task as a future quadratic programming optimization problem constrained with equality constraints, which is also regarded as a fundamental problem in artificial intelligence and modern control engineering. Zhang neural network, which is a canonical method, can be adopted to deal with the continuous form of the future optimization problem, named as temporally dependent quadratic programming problem with equality constraints. In order to overcome the issue of temporally dependent inverse computing, a novel Zhang neural network model and its uncertain disturbance tolerant model, which are termed as filtered reciprocal-kind Zhang neural network model and uncertain disturbance tolerant filtered reciprocal-kind Zhang neural network model, respectively, are proposed by integrating the energy-type cost function and Zhang neural network design formula for solving the temporally dependent quadratic programming problem with equality constraints in this paper. Based on the Euler discrete formula and the models, the discrete filtered reciprocal-kind Zhang neural network and the discrete uncertain disturbance tolerant filtered reciprocal-kind Zhang neural network algorithms are proposed for solving the future quadratic programming problem with equality constraints and the robot manipulator configuration calibration problem of repetitive motion. The convergence properties of the reciprocal-kind Zhang neural network model and its corresponding uncertain disturbance tolerant model are obtained by Lyapunov stability theory of nonlinear system and its corresponding perturbed system, while the convergence property of the filtered reciprocal-kind Zhang neural network model is analyzed by the limit thinking. For the repetitive motion task, three experiments for solving the configuration calibration problem of PUMA560, Kinova Jaco2, and Franka Emika Panda robot manipulators are performed to illustrate the effectiveness, robustness and superiority of our proposed discrete filtered reciprocal-kind Zhang neural network algorithms.}
}
@article{MANCHO200655,
title = {A tutorial on dynamical systems concepts applied to Lagrangian transport in oceanic flows defined as finite time data sets: Theoretical and computational issues},
journal = {Physics Reports},
volume = {437},
number = {3},
pages = {55-124},
year = {2006},
issn = {0370-1573},
doi = {https://doi.org/10.1016/j.physrep.2006.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0370157306003401},
author = {Ana M. Mancho and Des Small and Stephen Wiggins},
keywords = {Lagrangian transport, Geophysical fluid flows, Finite time hyperbolicity, Finite time Lyapunov exponents, Stable and unstable manifolds, Transport barriers},
abstract = {In the past 15 years the framework and ideas from dynamical systems theory have been applied to a variety of transport and mixing problems in oceanic flows. The motivation for this approach comes directly from advances in observational capabilities in oceanography (e.g., drifter deployments, remote sensing capabilities, satellite imagery, etc.) which reveal space–time structures that are highly suggestive of the structures one visualizes in the global, geometrical study of dynamical systems theory. In this tutorial, we motivate this approach by showing the relationship between fluid transport in two-dimensional time-periodic incompressible flows and the geometrical structures that exist for two-dimensional area-preserving maps, such as hyperbolic periodic orbits, their stable and unstable manifolds and KAM (Kolmogorov–Arnold–Moser) tori. This serves to set the stage for the attempt to “transfer” this approach to more realistic flows modelling the ocean. However, in order to accomplish this several difficulties must be overcome. The first difficulty that confronts us that any attempt to carry out a dynamical systems approach to transport requires us to obtain the appropriate “dynamical system”, which is the velocity field describing the fluid flow. In general, adequate model velocity fields are obtained by numerical solution of appropriate partial differential equations describing the dynamical evolution of the velocity field. Numerical solution of the partial differential equations can only be done for a finite time interval, and since the ocean is generally not time-periodic, this leads to a new type of dynamical system: a finite-time, aperiodically time-dependent velocity field defined as a data set on a space–time grid. The global, geometrical analysis of transport in such dynamical systems requires both new concepts and new analytical and computational tools, as well as the necessity to discard some of the standard ideas and results from dynamical systems theory. The purpose of this tutorial is to describe these new concepts and analytical tools first using simple dynamical systems where quantities can be computed exactly. We then discuss their computational implications and implementation in the context of a model geophysical flow: a turbulent wind-driven double-gyre in the quasigeostrophic approximation.}
}
@article{HROBARIK20066,
title = {Computational study of bonding trends in the metalloactinyl series EThM and MThM′ (E=N−, O, F+; M, M′=Ir−, Pt, Au+)},
journal = {Chemical Physics Letters},
volume = {431},
number = {1},
pages = {6-12},
year = {2006},
issn = {0009-2614},
doi = {https://doi.org/10.1016/j.cplett.2006.08.144},
url = {https://www.sciencedirect.com/science/article/pii/S0009261406013741},
author = {Peter Hrobárik and Michal Straka and Pekka Pyykkö},
abstract = {The title systems, including EThE′, are treated at DFT level using a B3LYP functional and small-core quasirelativistic pseudopotentials. Most of the studied systems are bent, like their isoelectronic ThO2 analogue, except for some anionic systems containing Ir. The bond lengths vary considerably and can lie above or below the sum of triple-bond covalent radii. Among the studied systems, the iridium-containing species show the strongest back-donation to Th. The bonding can be simply understood and could theoretically go up to a ‘24-electron principle’ limit at the actinide.}
}
@article{WHITACRE2020100816,
title = {The roles of tools and models in a prospective elementary teachers’ developing understanding of multidigit multiplication},
journal = {The Journal of Mathematical Behavior},
volume = {60},
pages = {100816},
year = {2020},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2020.100816},
url = {https://www.sciencedirect.com/science/article/pii/S0732312320300808},
author = {Ian Whitacre and Chepina Rumsey},
keywords = {Prospective teachers, Mental computation, Multiplication, Tools, Models},
abstract = {It is important for prospective elementary teachers to understand multidigit multiplication deeply; however, the development of such understanding presents challenges. We document the development of a prospective elementary teacher’s reasoning about multidigit multiplication during a Number and Operations course. We present evidence of profound progress in Valerie’s understanding of multidigit multiplication, and we highlight the roles of particular tools and models in her developing reasoning. In this way, we contribute an illuminating case study that can inform the work of mathematics teacher educators. We discuss specific instructional implications that derive from this case.}
}
@article{WOODSIDE2011153,
title = {Responding to the severe limitations of cross-sectional surveys: Commenting on Rong and Wilkinson’s perspectives},
journal = {Australasian Marketing Journal (AMJ)},
volume = {19},
number = {3},
pages = {153-156},
year = {2011},
note = {Special Section: Marketing and Public Policy},
issn = {1441-3582},
doi = {https://doi.org/10.1016/j.ausmj.2011.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S1441358211000231},
author = {Arch G. Woodside},
keywords = {Direct research, Executives, Folk theory-of-mind, fs/QCA.com, Sensemaking, Surveys, Thinking},
abstract = {While a meta-analysis is necessary to test the claim that the logic dominates the majority of studies, most studies by academic scholars on thinking and actions by executives appear to rely on cross-sectional surveys that use self-reports by executives via scaled (e.g., strongly disagree to strongly agree) instruments whereby one executive per firm completes the instrument and data are collected for 50–500 firms. Useable response rates in these studies are almost always below 30% of the distributions of the surveys. While these studies are sometimes worthwhile for learning how respondents assess concepts and relationships among concepts, Rong and Wilkinson’s perspective on the severe limits to the value of such studies rings true: such surveys reveal more about executives’ sensemaking processes than the actual processes. The limitations of using one-shot, one-person-per-firm, self-reports as valid indicators of causal relationships of actual processes are so severe that academics should do more than think twice before using such surveys as the main method for collecting data – if scholars seek to understand and describe actual processes additional methods are necessary for data collection. The relevant literature includes several gems of exceptionally high quality, validity, and usefulness in the study of actual processes; identifying these studies is a useful step toward reducing the reliance on one-shot self-report surveys.}
}
@article{EGOROV2007293,
title = {Neural logic molecular, counter-intuitive},
journal = {Biomolecular Engineering},
volume = {24},
number = {3},
pages = {293-299},
year = {2007},
note = {6th Atlantic Symposium on Computational Biology},
issn = {1389-0344},
doi = {https://doi.org/10.1016/j.bioeng.2007.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S1389034407000342},
author = {Igor K. Egorov},
keywords = {Creative thinking, Boolean logic gates, Molecular mechanism, Transcription regulation, Somatic hypermutation, Neuron},
abstract = {A hypothesis is proposed that multiple “LOGIC” genes control Boolean logic in a neuron. Each hypothetical LOGIC gene encodes a transcription factor that regulates another LOGIC gene(s). Through transcription regulation, LOGIC genes connect into a complex circuit, such as a XOR logic gate or a two-input flip–flop logic circuit capable of retaining information. LOGIC gene duplication, mutation and recombination may result in the diversification of Boolean logic gates. Creative thinking may sometimes require counter-intuitive reasoning, rather than common sense. Such reasoning is likely to engage novel logic circuits produced by LOGIC somatic mutations. An individual's logic maturates by a mechanism of somatic hypermutation, gene conversion and recombination of LOGIC genes in precursor cells followed by selection of neurons in the brain for functional competence. In this model, a single neuron among billions in the brain may contain a unique logic circuit being the key to a hard intellectual problem. The output of a logic neuron is likely to be a neurotransmitter. This neuron is connected to other neurons in the spiking neural network. The LOGIC gene hypothesis is testable by molecular techniques. Understanding mechanisms of authentic human ingenuity may help to invent digital systems capable of creative thinking.}
}
@incollection{FROEHLICH2023685,
title = {Mixed methods and social network analysis},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {685-692},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.11059-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305110590},
author = {Dominik E. Froehlich},
keywords = {Data collection, Education research, Ethics, Mixed methods social network analysis, Mixed methods, Relational methods, Research design, Social network analysis, Structure},
abstract = {In this chapter, we discuss the application of mixed methods thinking to social network analysis, a methodological approach that focuses on social relationships and structures. For that purpose, we first define mixed methods and social network analysis and their intersection, which we call Mixed Methods Social Network Analysis (MMSNA). We then summarize the historical developments in social network analysis, which also explain the reason for the increasing application of MMSNA in educational research. The majority of the chapter then focuses on how MMSNA is applied in educational research and what the main topics of the current academic debates are.}
}
@incollection{MURMAN2012323,
title = {15 - Innovation in aeronautics through Lean Engineering},
editor = {Trevor M. Young and Mike Hirst},
booktitle = {Innovation in Aeronautics},
publisher = {Woodhead Publishing},
pages = {323-360},
year = {2012},
isbn = {978-1-84569-550-7},
doi = {https://doi.org/10.1533/9780857096098.3.323},
url = {https://www.sciencedirect.com/science/article/pii/B978184569550750015X},
author = {E.M. Murman},
keywords = {Lean Engineering, Lean Product Development},
abstract = {Abstract:
The dynamics of innovation theory indicate that, for products as mature as aircraft, process innovation is an important contributor to product success and innovation. Many aerospace companies have adopted Lean Thinking as an enterprise-wide continuous improvement strategy. This chapter extends Lean Thinking to the engineering domain with a Lean Engineering framework based upon observational findings from a decade of research in the aerospace domain, published works on Toyota and Southwest Airlines, and practitioner input. Examples illustrate how the framework maybe be applied. Lean Engineering is not totally new to aerospace, and it continues to evolve. Future challenges are briefly summarized.}
}
@article{BUSBY20161029,
title = {Agent-based computational modelling of social risk responses},
journal = {European Journal of Operational Research},
volume = {251},
number = {3},
pages = {1029-1042},
year = {2016},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2015.12.034},
url = {https://www.sciencedirect.com/science/article/pii/S037722171501173X},
author = {J.S. Busby and B.S.S. Onggo and Y. Liu},
keywords = {OR in societal problem analysis, Multiagent systems, Risk management},
abstract = {A characteristic aspect of risks in a complex, modern society is the nature and degree of the public response – sometimes significantly at variance with objective assessments of risk. A large part of the risk management task involves anticipating, explaining and reacting to this response. One of the main approaches we have for analysing the emergent public response, the social amplification of risk framework, has been the subject of little modelling. The purpose of this paper is to explore how social risk amplification can be represented and simulated. The importance of heterogeneity among risk perceivers, and the role of their social networks in shaping risk perceptions, makes it natural to take an agent-based approach. We look in particular at how to model some central aspects of many risk events: the way actors come to observe other actors more than external events in forming their risk perceptions; the way in which behaviour both follows risk perception and shapes it; and the way risk communications are fashioned in the light of responses to previous communications. We show how such aspects can be represented by availability cascades, but also how this creates further problems of how to represent the contrasting effects of informational and reputational elements, and the differentiation of private and public risk beliefs. Simulation of the resulting model shows how certain qualitative aspects of risk response time series found empirically – such as endogenously-produced peaks in risk concern – can be explained by this model.}
}
@article{REN20231643,
title = {An Edge Computing Algorithm Based on Multi-Level Star Sensor Cloud},
journal = {CMES - Computer Modeling in Engineering and Sciences},
volume = {136},
number = {2},
pages = {1643-1659},
year = {2023},
issn = {1526-1492},
doi = {https://doi.org/10.32604/cmes.2023.025248},
url = {https://www.sciencedirect.com/science/article/pii/S1526149223002813},
author = {Siyu Ren and Shi Qiu and Keyang Cheng},
keywords = {Star-sensing, sensor cloud, fuzzy set, edge computing, mapping},
abstract = {Star sensors are an important means of autonomous navigation and access to space information for satellites. They have been widely deployed in the aerospace field. To satisfy the requirements for high resolution, timeliness, and confidentiality of star images, we propose an edge computing algorithm based on the star sensor cloud. Multiple sensors cooperate with each other to form a sensor cloud, which in turn extends the performance of a single sensor. The research on the data obtained by the star sensor has very important research and application values. First, a star point extraction model is proposed based on the fuzzy set model by analyzing the star image composition, which can reduce the amount of data computation. Then, a mapping model between content and space is constructed to achieve low-rank image representation and efficient computation. Finally, the data collected by the wireless sensor is delivered to the edge server, and a different method is used to achieve privacy protection. Only a small amount of core data is stored in edge servers and local servers, and other data is transmitted to the cloud. Experiments show that the proposed algorithm can effectively reduce the cost of communication and storage, and has strong privacy.}
}
@article{SUDDENDORF201826,
title = {Prospection and natural selection},
journal = {Current Opinion in Behavioral Sciences},
volume = {24},
pages = {26-31},
year = {2018},
note = {Survival circuits},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2018.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S2352154617302449},
author = {T Suddendorf and A Bulley and B Miloyan},
abstract = {Prospection refers to thinking about the future, a capacity that has become the subject of increasing research in recent years. Here we first distinguish basic prospection, such as associative learning, from more complex prospection commonly observed in humans, such as episodic foresight, the ability to imagine diverse future situations and organize current actions accordingly. We review recent studies on complex prospection in various contexts, such as decision-making, planning, deliberate practice, information gathering, and social coordination. Prospection appears to play many important roles in human survival and reproduction. Foreseeing threats and opportunities before they arise, for instance, drives attempts at avoiding future harm and obtaining future benefits, and recognizing the future utility of a solution turns it into an innovation, motivating refinement and dissemination. Although we do not know about the original contexts in which complex prospection evolved, it is increasingly clear through research on the emergence of these capacities in childhood and on related disorders in various clinical conditions, that limitations in prospection can have profound functional consequences.}
}
@article{RODRIGUEZ2022104446,
title = {Using scaffolded feedforward and peer feedback to improve problem-based learning in large classes},
journal = {Computers & Education},
volume = {182},
pages = {104446},
year = {2022},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2022.104446},
url = {https://www.sciencedirect.com/science/article/pii/S0360131522000173},
author = {María Fernanda Rodríguez and Miguel Nussbaum and Leyla Yunis and Tomás Reyes and Danilo Alvares and Jean Joublan and Patricio Navarrete},
abstract = {The growing demand for access to higher education has seen institutions turn increasingly towards large classes. Implementing active, problem-based learning in this context can be difficult as it requires the lecturer to attend to every student's individual needs. Given the lack of tools for providing personalized feedback, this represents a significant challenge. The aim of this study is to see how best to support lecturers in giving timely feedback to students in a large class during problem-based learning. To meet this goal, we propose a model that combines feedforward, scaffolded using an automated summarization tool, with peer feedback. In this sense, the lecturer first provides feedforward through a series of general comments before an anonymous peer gives personalized feedback. The results show that, despite not giving personalized feedback, the lecturer is able to provide enriched formative feedforward thanks to the summary generated by the automated system. Furthermore, in more qualitative terms, the students show that they appreciate the opportunity to both give and receive feedback. Finally, the students' critical thinking skills are also shown to improve progressively from one activity to the next. Given the research gap regarding how lecturers use the reports generated by automated summarization tools, our study contributes to the literature by proposing a strategy for lecturers to use such reports to provide feedforward. Additionally, this study also contributes to the literature by proposing a model that can be fully integrated in both synchronous and asynchronous online learning.}
}
@article{DEVOE2012466,
title = {Time, money, and happiness: How does putting a price on time affect our ability to smell the roses?},
journal = {Journal of Experimental Social Psychology},
volume = {48},
number = {2},
pages = {466-474},
year = {2012},
issn = {0022-1031},
doi = {https://doi.org/10.1016/j.jesp.2011.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0022103111002897},
author = {Sanford E. DeVoe and Julian House},
keywords = {Time, Money, Impatience, Happiness},
abstract = {In this paper, we investigate how the impatience that results from placing a price on time impairs individuals' ability to derive happiness from pleasurable experiences. Experiment 1 demonstrated that thinking about one's income as an hourly wage reduced the happiness that participants derived from leisure time on the internet. Experiment 2 revealed that a similar manipulation decreased participants' state of happiness after listening to a pleasant song and that this effect was fully mediated by the degree of impatience experienced during the music. Finally, Experiment 3 showed that the deleterious effect on happiness caused by impatience was attenuated by offering participants monetary compensation in exchange for time spent listening to music, suggesting that a sensation of unprofitably wasted time underlay the induced impatience. Together these experiments establish that thinking about time in terms of money can influence how people experience pleasurable events by instigating greater impatience during unpaid time.}
}
@article{EGIDI2020155,
title = {Desertification risk, economic resilience and social issues: From theory to practice},
journal = {Chinese Journal of Population, Resources and Environment},
volume = {18},
number = {2},
pages = {155-163},
year = {2020},
issn = {2325-4262},
doi = {https://doi.org/10.1016/j.cjpre.2021.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S2325426221000310},
author = {Gianluca Egidi and Luca Salvati},
keywords = {Population dynamics, Ecosystem functioning, Socio-ecological resilience, Complex adaptive systems, Interpretative framework},
abstract = {Land degradation and early forms of desertification in both advanced economies and emerging countries reflect complex socio-environmental processes driven by multiple interactions between biophysical and socioeconomic forces across different spatial scales. The present study investigates desertification risk, land degradation, and socio-demographic dynamics through the lens of “resilience,” adopting complex adaptive systems (CAS) thinking. The resilience of socio-environmental systems exposed to land degradation is defined as the capacity of a regional economy to respond to crises and reorganize by making changes to preserve functions, structure, and feedback, and to promote future development options. By reviewing the socioeconomic resilience of local socio-ecological systems exposed to land degradation, this study achieves a better comprehension of the multifaceted processes that lead to a higher risk of desertification and the intimate relationship with underlying population trends and demographic dynamics. A comprehensive approach based on resilience thinking was formulated to review both environmental and socio-demographic issues at the landscape scale, and provide a suitable foundation for sustainability science and regional development policies.}
}
@article{HARTWIGSEN20212075,
title = {How does hemispheric specialization contribute to human-defining cognition?},
journal = {Neuron},
volume = {109},
number = {13},
pages = {2075-2090},
year = {2021},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2021.04.024},
url = {https://www.sciencedirect.com/science/article/pii/S0896627321002907},
author = {Gesa Hartwigsen and Yoshua Bengio and Danilo Bzdok},
keywords = {human intelligence, artificial general intelligence, computational design principles, deep learning, language, global workspace theory},
abstract = {Summary
Uniquely human cognitive faculties arise from flexible interplay between specific local neural modules, with hemispheric asymmetries in functional specialization. Here, we discuss how these computational design principles provide a scaffold that enables some of the most advanced cognitive operations, such as semantic understanding of world structure, logical reasoning, and communication via language. We draw parallels to dual-processing theories of cognition by placing a focus on Kahneman’s System 1 and System 2. We propose integration of these ideas with the global workspace theory to explain dynamic relay of information products between both systems. Deepening the current understanding of how neurocognitive asymmetry makes humans special can ignite the next wave of neuroscience-inspired artificial intelligence.}
}
@article{WELLEK1961715,
title = {The contribution of the perception-typological approaches to the typology of character, and the role of sensation, imagination, and thinking in the organizational concept of personality},
journal = {Acta Psychologica},
volume = {19},
pages = {715-723},
year = {1961},
issn = {0001-6918},
doi = {https://doi.org/10.1016/S0001-6918(61)80321-1},
url = {https://www.sciencedirect.com/science/article/pii/S0001691861803211},
author = {Albert Wellek}
}
@article{LIU202257,
title = {Hierarchical neighborhood entropy based multi-granularity attribute reduction with application to gene prioritization},
journal = {International Journal of Approximate Reasoning},
volume = {148},
pages = {57-67},
year = {2022},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2022.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X22000809},
author = {Keyu Liu and Tianrui Li and Xibei Yang and Hengrong Ju and Xin Yang and Dun Liu},
keywords = {Gene selection, Granular computing, Multi-granularity attribute reduction, Neighborhood rough set, Trilevel thinking},
abstract = {As a prominent model of granular computing, neighborhood rough set provides clear granularity organization and expression in terms of inherent parameter (neighborhood radius). Such characteristic is widely captured in a plenitude of attribute reduction procedures, while igniting a tricky issue of tuning parameters. In this study, we therefore propose a parameter-free multi-granularity attribute reduction scheme. Fundamentally, our scheme applies three-way decision as thinking in threes. First, data-aware multi-granularity structure is automatically induced from self-contained distance space instead of manually edited or appointed granularities. Second, a novel multi-granularity feature evaluation criterion named hierarchical neighborhood entropy is defined to measure the feature significance. Finally, a sequential forward searching algorithm is designed to find the optimal reduct. With application to gene prioritization, our method performed on microarray data is experimentally demonstrated to be more effective and efficient in differentially expressed genes discovery as compared with other well-established attribute reduction algorithms.}
}
@article{KORN2023102578,
title = {Navigating large chemical spaces in early-phase drug discovery},
journal = {Current Opinion in Structural Biology},
volume = {80},
pages = {102578},
year = {2023},
issn = {0959-440X},
doi = {https://doi.org/10.1016/j.sbi.2023.102578},
url = {https://www.sciencedirect.com/science/article/pii/S0959440X23000520},
author = {Malte Korn and Christiane Ehrt and Fiorella Ruggiu and Marcus Gastreich and Matthias Rarey},
abstract = {The size of actionable chemical spaces is surging, owing to a variety of novel techniques, both computational and experimental. As a consequence, novel molecular matter is now at our fingertips that cannot and should not be neglected in early-phase drug discovery. Huge, combinatorial, make-on-demand chemical spaces with high probability of synthetic success rise exponentially in content, generative machine learning models go hand in hand with synthesis prediction, and DNA-encoded libraries offer new ways of hit structure discovery. These technologies enable to search for new chemical matter in a much broader and deeper manner with less effort and fewer financial resources. These transformational developments require new cheminformatics approaches to make huge chemical spaces searchable and analyzable with low resources, and with as little energy consumption as possible. Substantial progress has been made in the past years with respect to computation as well as organic synthesis. First examples of bioactive compounds resulting from the successful use of these novel technologies demonstrate their power to contribute to tomorrow's drug discovery programs. This article gives a compact overview of the state-of-the-art.}
}
@article{NDUNGO2020,
title = {mSphere of Influence: Learning from Nature—Antibody Profiles Important for Protection of Young Infants},
journal = {mSphere},
volume = {5},
number = {5},
year = {2020},
issn = {2379-5042},
doi = {https://doi.org/10.1128/msphere.01021-20},
url = {https://www.sciencedirect.com/science/article/pii/S2379504220001356},
author = {Esther Ndungo},
keywords = {antibody profiles, enteric pathogens, maternal-infant immunity, systems serology},
abstract = {Esther Ndungo works in the field of maternal-infant immunity against enteric pathogens. In this mSphere of Influence article, she reflects on how the paper “Fc glycan-mediated regulation of placental antibody transfer” by Jennewein et al. (M. F. Jennewein, I. Goldfarb, S. Dolatshahi, C. Cosgrove, et al., Cell 178:202–215.e14, 2019, https://doi.org/10.1016/j.cell.2019.05.044) impressed upon her the value of thinking “outside the box” and looking to nature to guide her research.
ABSTRACT
Esther Ndungo works in the field of maternal-infant immunity against enteric pathogens. In this mSphere of Influence article, she reflects on how the paper “Fc glycan-mediated regulation of placental antibody transfer” by Jennewein et al. (M. F. Jennewein, I. Goldfarb, S. Dolatshahi, C. Cosgrove, et al., Cell 178:202–215.e14, 2019, https://doi.org/10.1016/j.cell.2019.05.044) impressed upon her the value of thinking “outside the box” and looking to nature to guide her research.}
}
@article{FARAJ2021100337,
title = {Unto the breach: What the COVID-19 pandemic exposes about digitalization},
journal = {Information and Organization},
volume = {31},
number = {1},
pages = {100337},
year = {2021},
issn = {1471-7727},
doi = {https://doi.org/10.1016/j.infoandorg.2021.100337},
url = {https://www.sciencedirect.com/science/article/pii/S1471772721000038},
author = {Samer Faraj and Wadih Renno and Anand Bhardwaj},
keywords = {COVID, Digitalization, Technology, Organizing, Breaching experiment},
abstract = {Much recent scholarly investigation has been focused on the promise of digitalization and the new ways of working and organizing it makes possible. In this paper, we analyze how the COVID-19 pandemic has acted as a natural breaching experiment that has challenged taken-for-granted expectations about digitalization and revealed four important issues: uneven access to digital infrastructures, the persistence of the analog in digitalization, the brittleness of unchecked digitalization, and panoptical surveillance. The sudden shift to digital work has exposed taken-for-granted assumptions about the universality of digital access. The crisis has also revealed that many highly digitalized processes still rely on analog elements. The pandemic has also exposed that many algorithms used in digitalized inter-organizational processes are brittle due to overreliance on historic patterns. Finally, the pandemic has breached fundamental expectations of privacy when organizational surveillance was extended into private and public spaces. Thus, the pandemic has laid bare fundamental challenges in digitalization and has exposed the limits of rose‑tinted thinking about the relation between technology and organizing.}
}
@incollection{OLIVEIRA200793,
title = {3 - Fundamentals of Quantum Computation and Quantum Information},
editor = {Ivan S. Oliveira and Tito J. Bonagamba and Roberto S. Sarthour and Jair C.C. Freitas and Eduardo R. deAzevedo},
booktitle = {NMR Quantum Information Processing},
publisher = {Elsevier Science B.V.},
address = {Amsterdam},
pages = {93-136},
year = {2007},
isbn = {978-0-444-52782-0},
doi = {https://doi.org/10.1016/B978-044452782-0/50005-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780444527820500051},
author = {Ivan S. Oliveira and Tito J. Bonagamba and Roberto S. Sarthour and Jair C.C. Freitas and Eduardo R. deAzevedo}
}
@article{YANG2018182,
title = {A Geodesign Method of Human-Energy-Water Interactive Systems for Urban Infrastructure Design: 10KM2 Near-Zero District Project in Shanghai},
journal = {Engineering},
volume = {4},
number = {2},
pages = {182-189},
year = {2018},
note = {Sustainable Infrastructure},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2018.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S2095809918301978},
author = {Perry Pei-Ju Yang and Cheryl Shu-Fang Chi and Yihan Wu and Steven Jige Quan},
keywords = {Geodesign, Urban design, Urban infrastructure, Energy performance, Iterative process, Multi-objective optimization},
abstract = {The grand challenges of climate change demand a new paradigm of urban design that takes the performance of urban systems into account, such as energy and water efficiency. Traditional urban design methods focus on the form-making process and lack performance dimensions. Geodesign is an emerging approach that emphasizes the links between systems thinking, digital technology, and geographic context. This paper presents the research results of the first phase of a larger research collaboration and proposes an extended geodesign method for a district-scale urban design to integrate systems of renewable energy production, energy consumption, and storm water management, as well as a measurement of human experiences in cities. The method incorporates geographic information system (GIS), parametric modeling techniques, and multidisciplinary design optimization (MDO) tools that enable collaborative design decision-making. The method is tested and refined in a test case with the objective of designing a near-zero-energy urban district. Our final method has three characteristics. ① Integrated geodesign and parametric design: It uses a parametric design approach to generate focal-scale district prototypes by means of a custom procedural algorithm, and applies geodesign to evaluate the performances of design proposals. ② A focus on design flow: It elaborates how to define problems, what information is selected, and what criteria are used in making design decisions. ③ Multi-objective optimization: The test case produces indicators from performance modeling and derives principles through a multi-objective computational experiment to inform how the design can be improved. This paper concludes with issues and next steps in modeling urban design and infrastructure systems based on MDO tools.}
}
@article{DAVIS20111046,
title = {Homogeneous steady deformation: A review of computational techniques},
journal = {Journal of Structural Geology},
volume = {33},
number = {6},
pages = {1046-1062},
year = {2011},
issn = {0191-8141},
doi = {https://doi.org/10.1016/j.jsg.2011.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0191814111000447},
author = {Joshua R. Davis and Sarah J. Titus},
keywords = {Kinematic model, Homogeneous deformation, Velocity gradient, Transpression, Vorticity},
abstract = {Homogeneous steady models are frequently used in the structural geology community to describe rock deformation. We review the literature on these models in a streamlined, coordinate-free framework based on matrix exponentials and logarithms. These mathematical tools allow us to compute progressive and simultaneous deformations easily. As an application, we develop transpression with triclinic symmetry in two ways. The tools let us integrate field data related to position and velocity in computing best-fit models with many degrees of freedom. As an application, we reanalyze a published study to demonstrate the extent to which kinematic vorticity is sensitive to modeling assumptions. The tools also open the door to an increased role for the mathematics of Lie groups (spaces of deformations) in structural geology. We suggest two topics for further study: numerical methods for non-steady deformations, and statistics of deformation tensors.}
}
@article{NOVOTOTSKYVLASOV1995S114,
title = {PS-12-13 Event-related brain activity analysis by mean wave halfperiod duration computation method},
journal = {Electroencephalography and Clinical Neurophysiology/Electromyography and Motor Control},
volume = {97},
number = {4},
pages = {S114},
year = {1995},
issn = {0924-980X},
doi = {https://doi.org/10.1016/0924-980X(95)92838-D},
url = {https://www.sciencedirect.com/science/article/pii/0924980X9592838D},
author = {V.Y. Novototsky-Vlasov}
}
@article{NA2023105139,
title = {Towards a neurocomputational account of social controllability: From models to mental health},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {148},
pages = {105139},
year = {2023},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2023.105139},
url = {https://www.sciencedirect.com/science/article/pii/S0149763423001082},
author = {Soojung Na and Shawn A. Rhoads and Alessandra N.C. Yu and Vincenzo G. Fiore and Xiaosi Gu},
keywords = {Social controllability, Computational psychiatry, Reinforcement learning, Model-based learning, Model-free learning, Cognitive map},
abstract = {Controllability, or the influence one has over their surroundings, is crucial for decision-making and mental health. Traditionally, controllability is operationalized in sensorimotor terms as one’s ability to exercise their actions to achieve an intended outcome (also termed “agency”). However, recent social neuroscience research suggests that humans also assess if and how they can exert influence over other people (i.e., their actions, outcomes, beliefs) to achieve desired outcomes ("social controllability”). In this review, we will synthesize empirical findings and neurocomputational frameworks related to social controllability. We first introduce the concepts of contextual and perceived controllability and their respective relevance for decision-making. Then, we outline neurocomputational frameworks that can be used to model social controllability, with a focus on behavioral economic paradigms and reinforcement learning approaches. Finally, we discuss the implications of social controllability for computational psychiatry research, using delusion and obsession-compulsion as examples. Taken together, we propose that social controllability could be a key area of investigation in future social neuroscience and computational psychiatry research.}
}
@article{WEISSMAN2011516,
title = {A computational framework for authoring and searching product design specifications},
journal = {Advanced Engineering Informatics},
volume = {25},
number = {3},
pages = {516-534},
year = {2011},
note = {Special Section: Engineering informatics in port operations and logistics},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2011.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1474034611000061},
author = {Alexander Weissman and Martin Petrov and Satyandra K. Gupta},
keywords = {Product design specifications, Engineering design, Requirements engineering},
abstract = {The development of product design specifications (PDS) is an important part of the product development process. Incompleteness, ambiguity, or inconsistency in the PDS can lead to problems during the design process and may require unnecessary design iterations. This generally results in increased design time and cost. Currently, in many organizations, PDS are written using word processors. Since documents written by different authors can be inconsistent in style and word choice, it is difficult to automatically search for specific requirements. Moreover, this approach does not allow the possibility of automated design verification and validation against the design requirements and specifications. In this paper, we present a computational framework and a software tool based on this framework for writing, annotating, and searching computer-interpretable PDS. Our approach allows authors to write requirement statements in natural language to be consistent with the existing authoring practice. However, using mathematical expressions, keywords from predefined taxonomies, and other metadata the author of PDS can then annotate different parts of the requirement statements. This approach provides unambiguous meaning to the information contained in PDS, and helps to eliminate mistakes later in the process when designers must interpret requirements. Our approach also enables users to construct a new PDS document from the results of the search for requirements of similar devices and in similar contexts. This capability speeds up the process of creating PDS and helps authors write more detailed documents by utilizing previous, well written PDS documents. Our approach also enables checking for internal inconsistencies in the requirement statements.}
}
@article{ZHOU2023126996,
title = {Coal consumption prediction in thermal power units: A feature construction and selection method},
journal = {Energy},
volume = {273},
pages = {126996},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2023.126996},
url = {https://www.sciencedirect.com/science/article/pii/S0360544223003900},
author = {Jian Zhou and Wei Zhang},
keywords = {Thermal power units, Coal consumption prediction, Regression analysis, K-means algorithm, Genetic algorithm},
abstract = {Digitization and related facilities have enabled the thermal power generation enterprises to record real-time data of thermal power units. There are many data-driven applications based on real-time monitoring and operational data in power units, while limited studies lay on the operational improvements, especially on coal consumption prediction under all working conditions. We build an intelligent prediction model of coal consumption based on key features selection, working condition clustering, and regression analysis. We combine feature construction and feature selection methods to cope with the problem caused by directly specifying feature subset for model building of traditional prediction method, which may fall into the thinking pattern and miss potentially better feature subset. Besides, to cope with the different coal consumption under different working conditions, we apply cluster analysis to construct a sub-coal consumption prediction model for each cluster category. Numerical results show that compared with other methods, it has the advantages of lower regression error and moderate model complexity, which can provide efficient decision support for operational improvement in thermal power generation.}
}
@article{ZHANG2024103588,
title = {Anonymous data sharing scheme for resource-constrained internet of things environments},
journal = {Ad Hoc Networks},
volume = {163},
pages = {103588},
year = {2024},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2024.103588},
url = {https://www.sciencedirect.com/science/article/pii/S1570870524001999},
author = {Zetian Zhang and Jingyu Wang and Lixin Liu and Yongfeng Li and Yun Hao and Hanqing Yang},
keywords = {Anonymity, Resource-constrained, Integrity verification, Data sharing, Accountability, Revocation},
abstract = {With the rapid development of Internet of Things (IoT) technology in industrial, agricultural, medical and other fields, IoT terminal devices face security and privacy challenges when sharing data. Among them, ensuring data confidentiality, achieving dual-side privacy protection, and performing reliable data integrity verification are basic requirements. Especially in resource-constrained environments, limitations in the storage, computing, and communication capabilities of devices increase the difficulty of implementing these security safeguards. To address this problem, this paper proposes a resource-constrained anonymous data-sharing scheme (ADS-RC) for the IoT. In ADS-RC, we use elliptic curve operations to replace computation-intensive bilinear pairing operations, thereby reducing the computational and communication burden on end devices. We combine an anonymous verifiable algorithm and an attribute encryption algorithm to ensure double anonymity and data confidentiality during the data-sharing process. To deal with potential dishonest behavior, this solution supports the revocation of malicious user permissions. In addition, we designed a batch data integrity verification algorithm and stored verification evidence on the blockchain to ensure the security and traceability of data during transmission and storage. Through experimental verification, the ADS-RC scheme achieves reasonable efficiency in correctness, security and efficiency, providing a new solution for data sharing in resource-constrained IoT environments.}
}
@article{ANDERSON1998214,
title = {Stereovision: beyond disparity computations},
journal = {Trends in Cognitive Sciences},
volume = {2},
number = {6},
pages = {214-222},
year = {1998},
issn = {1364-6613},
doi = {https://doi.org/10.1016/S1364-6613(98)01180-2},
url = {https://www.sciencedirect.com/science/article/pii/S1364661398011802},
author = {Barton L Anderson},
keywords = {sterovision, disparity, 3-D sterograms, perceptual grouping, occlusion},
abstract = {One of the most powerful sources of information about three-dimensional (3-D) structure is provided by stereovision (or stereopsis). For over a century, theoretical and empirical investigations into this ability have focused on the role of binocular disparity in generating percepts of 3-D structure. Recent work in image segmentation demonstrates that stereovision can cause large changes in perceptual organization that cannot be understood on the basis of binocular disparity alone. It is argued that these phenomena reveal the need for theoretical tools beyond those that have dominated the study of visual perception over the past three decades.}
}
@article{SATTARI2021104981,
title = {Application of Bayesian network and artificial intelligence to reduce accident/incident rates in oil & gas companies},
journal = {Safety Science},
volume = {133},
pages = {104981},
year = {2021},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2020.104981},
url = {https://www.sciencedirect.com/science/article/pii/S0925753520303787},
author = {Fereshteh Sattari and Renato Macciotta and Daniel Kurian and Lianne Lefsrud},
keywords = {Artificial intelligence, Bayesian network, Machine learning, Keyword analysis, Incident data, Process safety management, Latent causes},
abstract = {Process safety management (PSM) is a framework that demonstrates a company’s commitment to process safety, a better understanding of hazards and risks, a comprehensive assessment and management of risks, and enhanced learning from experience to improve overall safety and operational performance. Companies often use an incident data reporting system to execute PSM. While companies keep incident data in thousands of reports, rarely do they glean full value in learning from these to prevent and reduce future incidents. To overcome this challenge, this research applied machine learning and keyword analysis to label and classify 8199 incident reports from an oil and gas company into nine groups identified in the latest version of PSM guidelines published by the Center for Chemical Process Safety (CCPS). To converge on an optimal solution, two different Bayesian network techniques (Tabu and hill climbing) were applied. Both methods resulted in the same map, showing that the Total Number of Incidents has the maximum dependency (50%) on Asset Integrity & Reliability; this means focusing resources on this aspect could reduce the total number of incidents by half. Cross correlation analysis (CCA) was also applied, which validated and confirmed this result. This analysis identifies which measures enhance the company’s safety management strategy to reduce these latent causes, but also supports critical thinking, enhanced communication, and learning culture to improve organizational safety.}
}
@article{STONE2022419,
title = {On second thoughts: changes of mind in decision-making},
journal = {Trends in Cognitive Sciences},
volume = {26},
number = {5},
pages = {419-431},
year = {2022},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2022.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S1364661322000407},
author = {Caleb Stone and Jason B. Mattingley and Dragan Rangelov},
keywords = {decision-making, change of mind, sequential sampling, metacognition},
abstract = {The ability to change initial decisions in the face of new or potentially conflicting information is fundamental to adaptive behavior. From perceptual tasks to multiple-choice tests, research has shown that changes of mind often improve task performance by correcting initial errors. Decision makers must, however, strike a balance between improvements that might arise from changes of mind and potential energetic, temporal, and psychological costs. In this review, we provide an overview of the change-of-mind literature, focusing on key behavioral findings, computational mechanisms, and neural correlates. We propose a conceptual framework that comprises two core decision dimensions – time and evidence source – which link changes of mind across decision contexts, as a first step toward an integrated psychological account of changes of mind.}
}
@article{REZAPOUR2024108003,
title = {Learning experience assessment through players chat content in multiplayer online games},
journal = {Computers in Human Behavior},
volume = {151},
pages = {108003},
year = {2024},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.108003},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223003540},
author = {Mohammad Mahdi Rezapour and Afsaneh Fatemi and Mohammad Ali Nematbakhsh},
keywords = {Learning experience assessment, Multiplayer online game, Natural language processing, BERT, Stealth assessment, Game-based learning},
abstract = {Assessing players’ learning experiences in a proper manner is a fundamental aspect of successful game-based learning programs. One notable characteristic of these programs is stealth assessment, which involves integrating formative assessment into the learning environment without disrupting the learning process. In multiplayer online games (MOGs), the in-game online chat system is a commonly used tool that enables players to communicate through text or voice messages during gameplay. However, there is a lack of specific research on incorporating players’ in-game chat content for computational learning experience assessment, which could enhance the validity of stealth assessment. This study proposes a stealth assessment method based on natural language processing to highlight the significance of players’ in-game chat data in estimating learners’ skills in MOGs. A natural language processing model is developed using a distilled version of the Google BERT pre-trained model. The evaluations demonstrate that the proposed method accurately estimates a player’s skill level by analyzing a few chat messages from the player. This method has the potential to make a profound impact on the field of game-based learning by enabling more precise assessment and supporting the design of tailored interventions and adaptive learning systems. This study pioneers computational skill assessment through chats in MOGs, opening up new opportunities for future investigations in skill assessment and having the potential to transform the field of game-based learning.}
}
@article{FADLALLA1995987,
title = {Improving the performance of enumerative search methods—part II: Computational experiments},
journal = {Computers & Operations Research},
volume = {22},
number = {10},
pages = {987-994},
year = {1995},
issn = {0305-0548},
doi = {https://doi.org/10.1016/0305-0548(95)00016-F},
url = {https://www.sciencedirect.com/science/article/pii/030505489500016F},
author = {Adam Fadlalla and James R. Evans and Martin S. Levy},
abstract = {Generally, branch and bound algorithms typically use mechanistic search strategies and generally do not fully exploit “local” information inherent in problem structures; that is, specific problem-domain knowledge. Some exceptions are found in [2–5]. Incorporatiing intelligence in branch and bound algorithms has been suggested by Glover [1], but not studied in a rigorous experimental framework. We use the mean tardiness job sequencing problem to explore these issues. This paper is divided into two Parts. In Part I [9], we provided the intuitive motivation for this investigation and an experimental framework. In Part II, we present detailed computational results and statistical analysis. The results indicate that branch and bound algorithms can be enhanced significantly by exploiting local knowledge of problem structure and more judicious search strategies.}
}