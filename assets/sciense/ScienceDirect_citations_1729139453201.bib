@article{IANDOLI2014298,
title = {Socially augmented argumentation tools: Rationale, design and evaluation of a debate dashboard},
journal = {International Journal of Human-Computer Studies},
volume = {72},
number = {3},
pages = {298-319},
year = {2014},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2013.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S1071581913001043},
author = {Luca Iandoli and Ivana Quinto and Anna {De Liddo} and Simon {Buckingham Shum}},
keywords = {Computer-supported argument visualization, Grounding process, Common ground, Debate dashboard, Collective deliberation, Visual feedback},
abstract = {Collaborative Computer-Supported Argument Visualization (CCSAV) is a technical methodology that offers support for online collective deliberation over complex dilemmas. As compared with more traditional conversational technologies, like wikis and forums, CCSAV is designed to promote more critical thinking and evidence-based reasoning, by using representations that highlight conceptual relationships between contributions, and through computational analytics that assess the structural integrity of the network. However, to date, CCSAV tools have achieved adoption primarily in small-scale educational contexts, and only to a limited degree in real world applications. We hypothesise that by reifying conversations as logical maps to address the shortcomings of chronological streams, CCSAV tools underestimate the importance of participation and interaction in enhancing collaborative knowledge-building. We argue, therefore, that CCSAV platforms should be socially augmented in order to improve their mediation capability. Drawing on Clark and Brennan influential Common Ground theory, we designed a Debate Dashboard, which augmented a CCSAV tool with a set of widgets that deliver meta-information about participants and the interaction process. An empirical study simulating a moderately sized collective deliberation scenario provides evidence that this experimental version outperformed the control version on a range of indicators, including usability, mutual understanding, quality of perceived collaboration, and accuracy of individual decisions. No evidence was found that the addition of the Debate Dashboard impeded the quality of the argumentation or the richness of content.}
}
@incollection{WANG2001297,
title = {Computational Intelligence in Agile Manufacturing Engineering},
editor = {A. Gunasekaran},
booktitle = {Agile Manufacturing: The 21st Century Competitive Strategy},
publisher = {Elsevier Science Ltd},
address = {Oxford},
pages = {297-315},
year = {2001},
isbn = {978-0-08-043567-1},
doi = {https://doi.org/10.1016/B978-008043567-1/50016-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780080435671500164},
author = {Kesheng Wang}
}
@article{TURKSON2020110464,
title = {Sustainability assessment of energy production: A critical review of methods, measures and issues},
journal = {Journal of Environmental Management},
volume = {264},
pages = {110464},
year = {2020},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2020.110464},
url = {https://www.sciencedirect.com/science/article/pii/S0301479720303984},
author = {Charles Turkson and Adolf Acquaye and Wenbin Liu and Thanos Papadopoulos},
keywords = {Sustainability, Energy production, Systematic review, Systems thinking, Energy policy, Sustainability assessment},
abstract = {Sustainable operations of energy production systems have become an increasingly important policy agenda globally because of the massive pressure placed on energy resources needed to support economic development and population growth. Due to the increasing research interest in examining the operational impacts of energy production systems on the society and the environment, this paper critically reviews the academic literature on the clean, affordable and secure supply of energy focussing on methods of assessments, measures of sustainability and emerging issues in the literature. While there have been some surveys on the sustainability of energy production systems they have either tended to focus on one assessment approach or one type of energy generation technology. This study builds on previous studies by providing a broader and comprehensive examination of the literature across generation technologies and assessment methods. A systematic review of 128 scholarly articles covering a 20-year period, ending 2018, and gathered from ProQuest, Scopus, and manual search is conducted. Synthesis and critical evaluation of the reviewed papers highlight a number of research gaps that exist within the sustainable energy production systems research domain. In addition, using mapping and cluster analyses, the paper visually highlights the network of dominant research issues, which emerged from the review.}
}
@article{KESBERG2021110458,
title = {Personal values as motivational basis of psychological essentialism: An exploration of the value profile underlying essentialist beliefs},
journal = {Personality and Individual Differences},
volume = {171},
pages = {110458},
year = {2021},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2020.110458},
url = {https://www.sciencedirect.com/science/article/pii/S0191886920306498},
author = {Rebekka Kesberg and Johannes Keller},
keywords = {Essentialist beliefs, Human values, Psychological essentialism},
abstract = {Essentialist lay-theories can reflect a belief in genetic, social, and metaphysical determinism. These three types of essentialist beliefs are similar as they can be linked to a set of motives and each of those beliefs is related to stereotyping and prejudice. Nevertheless, the available evidence indicates that the three types of essentialist thinking are largely unrelated and it is unclear why some individuals endorse one type of essentialism and reject another. Examining the association between the endorsement of essentialist beliefs and personal values, our results based on N = 348 respondents indicate that specific value profiles build the motivational basis for specific essentialist beliefs. Specifically, conservation values are associated with belief in genetic and metaphysical determinism, while self-transcendence and self-enhancement are associated with belief in social determinism.}
}
@article{MARTINS20183890,
title = {2MBio, a novel tool to encourage creative participatory conceptual design of bioenergy systems – The case of wood fuel energy systems in south Mozambique},
journal = {Journal of Cleaner Production},
volume = {172},
pages = {3890-3906},
year = {2018},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2017.05.062},
url = {https://www.sciencedirect.com/science/article/pii/S0959652617309873},
author = {Ricardo Martins and Judith A. Cherni and Nuno Videira},
keywords = {Design thinking, Systems thinking, Mozambique, Participatory design tools, Wood fuel energy systems, Bioenergy},
abstract = {This paper proposes a new conceptual design tool for bioenergy systems, the 2MBio, and its implementation on the case of wood fuel energy systems (WES) in South Mozambique. Dependence on wood fuel characterises most Sub-Saharan countries and WES are complex socio-ecological systems dynamically linked to crucial development issues, e.g., deforestation and poverty. In Mozambique WES supply over 70% of the national energy needs through an informal business network worth around one million euros each year. In contrast with the 2MBio, currently available tools often aim at supporting decision-making on WES with off-the-shelf expert solutions and optimisation of WES efficiency, supply chains and resource management. While relevant and useful, such approaches are frequently unsuitable to engage the knowledge and creativity of a wide range of crucial actors. The 2MBio addresses this gap providing a simple, visual platform on paper that supports from illiterate to professional users, to stimulate creative ideas and apply current knowledge while designing their own WES. The results of implementation in real settings in South Mozambique produced relevant design breakthroughs. Compared with the absence of any other support tool, and faced with same design challenges, the 2MBio participatory design workshops in south Mozambique resulted in comprehensive analysis of wood fuel energy systems, and innovative integrated WES solutions design. The proposed approach raised participants’ awareness about opportunities and constrains linked to their WES while also facilitating information sharing new learning dynamics and enhance creativity.}
}
@article{STROMMER2022134322,
title = {Forward-looking impact assessment – An interdisciplinary systematic review and research agenda},
journal = {Journal of Cleaner Production},
volume = {377},
pages = {134322},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.134322},
url = {https://www.sciencedirect.com/science/article/pii/S095965262203894X},
author = {Kiia Strömmer and Jarrod Ormiston},
keywords = {Impact assessment, Forward-looking, Temporality, Futures thinking},
abstract = {New and established ventures are under increasing pressure to consider how their current actions impact our future world. Whilst many practitioners are paying greater attention to their future impact, most impact assessment research focuses on the retrospective measurement of impact. Limited studies have explored how impact assessment is used as a tool to forecast or predict the intended impact of organisational action. This study aims to overcome this gap by exploring forward-looking approaches to impact assessment. An interdisciplinary systematic review of the impact assessment literature was conducted to answer the question: “How and why do organisations utilise forward-looking, future-oriented approaches to impact assessment?“. The findings elaborate on the common research themes, challenges, and gaps in understanding forward-looking impact assessment. An integrated process model is developed to show the relationships between various antecedents, methods, and effects of forward-looking impact assessment. Based on the review, the paper puts forward a research agenda to provoke further inquiry on forward-looking, future-oriented approaches to impact assessments related to four research themes: uncertainty, values and assumptions, stakeholder cooperation, and learning. The study contributes to the impact assessment literature by providing an overview of how the current literature comprehends forward-looking approaches and insights into how a more holistic view of temporality in impact assessment can be developed.}
}
@article{ZHAO2023106750,
title = {A cooperative population-based iterated greedy algorithm for distributed permutation flowshop group scheduling problem},
journal = {Engineering Applications of Artificial Intelligence},
volume = {125},
pages = {106750},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106750},
url = {https://www.sciencedirect.com/science/article/pii/S095219762300934X},
author = {Hui Zhao and Quan-Ke Pan and Kai-Zhou Gao},
keywords = {Distributed permutation flowshop, Group scheduling, Total flowtime, Iterated greedy algorithm, Co-evolutionary},
abstract = {This paper studies the distributed permutation flowshop group scheduling problem (DPFGSP) with the consideration of minimizing total flowtime (TF), which has important applications in the modern manufacturing process. Based on the characteristics of the problem, a cooperative population-based iterated greedy (CPIG) algorithm is proposed by combining the advantages of the divide-and-rule policy, population-based evolution and iterated greedy algorithm. The CPIG divides the DPFGSP into two coupled sub-problems of group scheduling sub-problem and job scheduling sub-problem, and starts with a single population for simplicity. Unlike in the traditional cooperative co-evolutionary algorithms, the two-coupled sub-problems are addressed with a certain probability that can be determined in favor of solving the whole scheduling problem. Some advanced technologies are used, including the constructive heuristics based initialization, the critical factories based destruction and construction, the new best solution based population updating mechanism. The comprehensive experimental evaluation of 810 instances shows that the CPIG algorithm performs much better than the five state-of-the-art metaheuristics in the literature which are closely related to the considered scheduling problem.}
}
@article{BIRO2015876,
title = {Measuring the Level of Algorithmic Skills at the End of Secondary Education in Hungary},
journal = {Procedia - Social and Behavioral Sciences},
volume = {176},
pages = {876-883},
year = {2015},
note = {International Educational Technology Conference, IETC 2014, 3-5 September 2014, Chicago, IL, USA},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.01.553},
url = {https://www.sciencedirect.com/science/article/pii/S187704281500590X},
author = {Piroska Biró and Mária Csernoch and János Máth and Kálmán Abari},
keywords = {level of digital thinking, algorithmic skills, school leaving exams in Informatics and Mathematics},
abstract = {Students starting their tertiary studies in Informatics are found to have a low level of algorithmic skills and understanding of programming, which leads to the high number of drop out students and failed semesters during their studies. The students’ low level of programming skills contrasts with their excellent results in the school leaving exams. To find out the reasons for this we have launched the TAaAS project (Testing Algorithmic and Application Skills), which focuses on the students’ algorithmic skills and programming ability in traditional and non-traditional programming environments. Our analyses proved that school leaving exams are not able to measure these abilities of the students, and beyond that, are not able to distinguish between the different levels of the students. Students are accepted into the universities and start their studies based on the misleading results of the school leaving exams.}
}
@article{LAVALLEY2024108825,
title = {Transdiagnostic failure to adapt interoceptive precision estimates across affective, substance use, and eating disorders: A replication and extension of previous results},
journal = {Biological Psychology},
volume = {191},
pages = {108825},
year = {2024},
issn = {0301-0511},
doi = {https://doi.org/10.1016/j.biopsycho.2024.108825},
url = {https://www.sciencedirect.com/science/article/pii/S030105112400084X},
author = {Claire A. Lavalley and Navid Hakimi and Samuel Taylor and Rayus Kuplicki and Katherine L. Forthman and Jennifer L. Stewart and Martin P. Paulus and Sahib S. Khalsa and Ryan Smith},
keywords = {Interoception, Depression, Anxiety, Substance use, Eating disorders, Precision, Priors, Bayesian perception, Computational modeling},
abstract = {Recent Bayesian theories of interoception suggest that perception of bodily states rests upon a precision-weighted integration of afferent signals and prior beliefs. In a previous study, we fit a computational model of perception to behavior on a heartbeat tapping task to test whether aberrant precision-weighting could explain misestimation of cardiac states in psychopathology. We found that, during an interoceptive perturbation designed to amplify afferent signal precision (inspiratory breath-holding), healthy individuals increased the precision-weighting assigned to ascending cardiac signals (relative to resting conditions), while individuals with anxiety, depression, substance use disorders, and/or eating disorders did not. In this pre-registered study, we aimed to replicate and extend our prior findings in a new transdiagnostic patient sample (N = 285) similar to the one in the original study. As expected, patients in this new sample were also unable to adjust beliefs about the precision of cardiac signals – preventing the ability to accurately perceive changes in their cardiac state. Follow-up analyses combining samples from the previous and current study (N = 719) also afforded power to identify group differences between narrower diagnostic categories, and to examine predictive accuracy when logistic regression models were trained on one sample and tested on the other. With this confirmatory evidence in place, future studies should examine the utility of interoceptive precision measures in predicting treatment outcomes and test whether these computational mechanisms might represent novel therapeutic targets.}
}
@article{NKONGOLO2022182,
title = {Using Deep Packet Inspection Data to Examine Subscribers on the Network},
journal = {Procedia Computer Science},
volume = {215},
pages = {182-191},
year = {2022},
note = {4th International Conference on Innovative Data Communication Technology and Application},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.021},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922020920},
author = {Mike Nkongolo and Jacobus Phillipus {van Deventer} and Sydney Mambwe Kasongo},
keywords = {Deep packet inspection, machine learning, UGRansome, telecommunication, data science},
abstract = {This article proposes the creation of the deep packet inspection (DPI) dataset to study subscribers’ behavior on the network, applying ensemble learning to this dataset, and comparing it with the UGRansome dataset. The subscriber can be thought of as a person or a group of users using a network service or connectivity. The DPI features represent the subscriber network usage, and the ensemble learning approach is implemented on the DPI dataset to predict the subscriber's service category on the network. The classification and prediction problem addressed on the DPI dataset reached a precision of 100%. The paper predicts that the web and streaming categories with Netflix, Facebook, and YouTube services will be the most utilized in the next few years. This study will lead to a better understanding of the idiosyncratic behavior of active subscribers on the network, exposing novel network anomalies and facilitating the development of novel DPI systems.}
}
@article{KOK2016342,
title = {Crowd behavior analysis: A review where physics meets biology},
journal = {Neurocomputing},
volume = {177},
pages = {342-362},
year = {2016},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2015.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S0925231215017403},
author = {Ven Jyn Kok and Mei Kuan Lim and Chee Seng Chan},
keywords = {Crowd behavior analysis, Biologically inspired, Physics-inspired, Computer vision, Survey},
abstract = {Although the traits emerged in a mass gathering are often non-deliberative, the act of mass impulse may lead to irrevocable crowd disasters. The two-fold increase of carnage in crowd since the past two decades has spurred significant advances in the field of computer vision, towards effective and proactive crowd surveillance. Computer vision studies related to crowd are observed to resonate with the understanding of the emergent behavior in physics (complex systems) and biology (animal swarm). These studies, which are inspired by biology and physics, share surprisingly common insights, and interesting contradictions. However, this aspect of discussion has not been fully explored. Therefore, this survey provides the readers with a review of the state-of-the-art methods in crowd behavior analysis from the physics and biologically inspired perspectives. We provide insights and comprehensive discussions for a broader understanding of the underlying prospect of blending physics and biology studies in computer vision.}
}
@article{CHEN20171,
title = {Heterogeneity in generalized reinforcement learning and its relation to cognitive ability},
journal = {Cognitive Systems Research},
volume = {42},
pages = {1-22},
year = {2017},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2016.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041716300559},
author = {Shu-Heng Chen and Ye-Rong Du},
keywords = {Generalized reinforcement learning, Experience-weighted attraction learning, Cognitive ability, Granularity},
abstract = {In this paper, we study the connections between working memory capacity (WMC) and learning in the context of economic guessing games. We apply a generalized version of reinforcement learning, popularly known as the experience-weighted attraction (EWA) learning model, which has a connection to specific cognitive constructs, such as memory decay, the depreciation of past experience, counterfactual thinking, and choice intensity. Through the estimates of the model, we examine behavioral differences among individuals due to different levels of WMC. In accordance with ‘Miller’s magic number’, which is the constraint of working memory capacity, we consider two different sizes (granularities) of strategy space: one is larger (finer) and one is smaller (coarser). We find that constraining the EWA models by using levels (granules) within the limits of working memory allows for a better characterization of the data based on individual differences in WMC. Using this level-reinforcement version of EWA learning, also referred to as the EWA rule learning model, we find that working memory capacity can significantly affect learning behavior. Our likelihood ratio test rejects the null that subjects with high WMC and subjects with low WMC follow the same EWA learning model. In addition, the parameter corresponding to ‘counterfactual thinking ability’ is found to be reduced when working memory capacity is low.}
}
@article{BINKOWSKA201435,
title = {Computational and experimental study of charge distribution in the α-disulfonyl carbanions},
journal = {Journal of Molecular Structure},
volume = {1062},
pages = {35-43},
year = {2014},
issn = {0022-2860},
doi = {https://doi.org/10.1016/j.molstruc.2014.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0022286014000258},
author = {Iwona Binkowska and Jacek Koput and Arnold Jarczewski},
keywords = {Proton transfer, Carbon acids, Charge distribution,  computation},
abstract = {The electron densities of the disulfonyl carbanions were determined using experimental 13C chemical shifts. The 13C NMR spectra and electron densities for the disulfonyl, nitro, and cyano carbon acids were calculated at the MP2/cc-pVDZ level of theory. The calculated chemical shifts for disulfonyl carbanions show satisfying correlation with our own experimental data. The calculated π electron densities at the Cα atom correspond roughly to the “experimental” π electron densities estimated from the 13C chemical shifts. The natural charges at Cα in disulfonyl stabilized carbanions are significantly more negative than with other types of carbanions, partly because of the significant negative natural charge of the α carbon in parent carbon acids. The calculated increase of the negative charge caused by ionization is larger for sulfonyl carbon acids than for cyano- and nitroalkanes. The 13C chemical shifts δ of Cα in disulfonyl stabilized carbanions decrease with more negative calculated negative natural charge at Cα, with a slope of 220ppm/electron. The influence of phenyl ring para-substitution on the charge distribution in carbanions and relationship between the 13C chemical shifts and charge density have been discussed. It appears that the π electron density in these planar or nearly planar carbanions has a decisive impact on the chemical shifts.}
}
@article{DEALMEIDA2022478,
title = {Assisting in the choice to fill a vacancy to compose the PROANTAR team: Applying VFT and the CRITIC-GRA-3N methodology},
journal = {Procedia Computer Science},
volume = {214},
pages = {478-486},
year = {2022},
note = {9th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.202},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922019123},
author = {Isaque David Pereira {de Almeida} and Lucas Ramon dos Santos Hermogenes and Igor Pinheiro de Araújo Costa and Miguel Ângelo Lellis Moreira and Carlos Francisco Simões Gomes and Marcos {dos Santos} and David de Oliveira Costa and Ian José Agra Gomes},
keywords = {CRITIC-GRA-3N method, Brazilian Navy, COVID-19},
abstract = {Antarctica is the southernmost continent of our planet, and it has been verified as the coldest region on earth. The Brazilian Antarctic Program (PROANTAR) has as its main objective the promotion of high-quality scientific research in the Antarctic region, seeking to understand the events that occur there. PROANTAR, coordinated by the Navy Commander, has some sectors that are based in Brazil and others that are located in the Antarctic continent. The military that volunteers to occupy any vacancy that is allocated to that continent needs, besides passing through several pre-established criteria, to pass the selection process. The purpose of this article is to help the Naval Administration in the selection of volunteer officers to occupy a vacancy in the Antarctic continent. To obtain the alternatives, the officers that best fit the established vacancy, and the criteria to be evaluated, Value-Focused Thinking (VFT) was applied. Next, with all the necessary data, the CRITIC-GRA-3N method was used as a Multicriteria Decision Support (MDS) technique, the CRITIC-GRA-3N method, the CRITIC Importance Through Intercriteria Correlation (CRITIC) method to obtain the criteria weights and the Grey Relational Analysis (GRA) method, with three normalizations, to order the alternatives. At the end of the application of the methods, the article can generate five ordinations of the volunteer officers to occupy the vacancy offered in PROANTAR.}
}
@article{HABTEMARIAM1990653,
title = {Research in computational epidemiology},
journal = {Mathematical and Computer Modelling},
volume = {14},
pages = {653-658},
year = {1990},
issn = {0895-7177},
doi = {https://doi.org/10.1016/0895-7177(90)90263-M},
url = {https://www.sciencedirect.com/science/article/pii/089571779090263M},
author = {T. Habtemariam and D. Oryang and F. Gabreab and V. Robnett and G. Trammell},
abstract = {The emerging new area referred to as computational science or science done on a computer adds a third dimension to the traditional methods of theoretical and experimental approaches. Counterparts to computational science such as computational linguistice, computational engineering and others arc beginning to take roots. Naturally, new research paths and opportunities in computational epidemiology must also be explored. One of the major challenges in epidemiologic research is the issue of how to realistically and effectively handle complex bioepidemiologic dynamics involving interactions between humans or animals, etiological agents and the multiple array of environmental and socioeconomic determinants which affect these populations. To understand the behavior of such complex biological systems, it is useful to devise computer based simulation models. Computational epidemiologic approaches now provide alternative avenues to classical laboratory and/or field experimental methods. Systems which may be impractical because they are too large, or, not feasible because the cost is too prohibitive can now be simulated realistically. In the past obtaining solutions to biomathematical equations with any degree of complexity was impossible. However, the availability of powerful computers now makes the quantitative analysis of such systems feasible and indeed practical. With this in mind our research at Tuskegee University has focused on: a) Epidemiologic modelling and expert systems, and, b) Hypertext/hypermedia based epidemiologic knowledge management. The case studies for our research involve the bioepidemiologic dynamics of two complex host-parasite systems of trypanosoma and schistosoma. The ultimate goal is to develop resources and methodologies based on computational technology to advance epidemiologic research. The paper will address the methodological issues and findings as well as questions related to configuring an appropriate research workstation for computational epidemiology.}
}
@article{FU2022107,
title = {Everyday Creativity is Associated with Increased Frontal Electroencephalography Alpha Activity During Creative Ideation},
journal = {Neuroscience},
volume = {503},
pages = {107-117},
year = {2022},
issn = {0306-4522},
doi = {https://doi.org/10.1016/j.neuroscience.2022.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0306452222004596},
author = {Lei Fu and Jia Zhao and Jiangzhou Sun and Yuchi Yan and Mujie Ma and Qunlin Chen and Jiang Qiu and Wenjing Yang},
keywords = {Everyday creativity, Alpha power, Alpha coherence, Creative ideation, Frontal cortex},
abstract = {Everyday creativity is the basic ability of human survival and penetrates every aspect of life. Nevertheless, the neural mechanisms underlying everyday creativity was largely unexplored. In this study, seventy-five participants completed the creative behaviour inventory, a tool for assessing creative behaviour in daily life. The participants also completed the alternate uses task (AUT) during an electroencephalography (EEG) assessment to evaluate creative thinking. Alpha power was used to quantify neural oscillations during the creative process, while alpha coherence was used to quantify information communication between frontal regions and other sites during creative ideation. Moreover, these two task-related quantitative measures were combined to investigate the relationship between individual differences in everyday creativity and EEG alpha activity during creative idea generation. Compared with the reference period, increased alpha power was observed in the frontal cortex of the right hemisphere and increased functional coupling was observed between frontal and parietal/temporal regions during the activation period. Interestingly, individual differences in everyday creativity were associated with distinct patterns of EEG alpha activity. Specifically, individuals with higher everyday creativity had increased alpha power in the frontal cortex, and increased changes in coherence in frontal-temporal regions of the right hemisphere while performing the AUT. It might indicate that individuals with higher everyday creativity had an enhanced ability to focus on internal information processing and control bottom-up stimuli, as well as better selection of novel semantic information when performing creative ideation tasks.}
}
@article{RASANAN2024857,
title = {Beyond discrete-choice options},
journal = {Trends in Cognitive Sciences},
volume = {28},
number = {9},
pages = {857-870},
year = {2024},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2024.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S136466132400175X},
author = {Amir Hosein Hadian Rasanan and Nathan J. Evans and Laura Fontanesi and Catherine Manning and Cynthia Huang-Pollock and Dora Matzke and Andrew Heathcote and Jörg Rieskamp and Maarten Speekenbrink and Michael J. Frank and Stefano Palminteri and Christopher G. Lucas and Jerome R. Busemeyer and Roger Ratcliff and Jamal Amani Rad},
abstract = {While decision theories have evolved over the past five decades, their focus has largely been on choices among a limited number of discrete options, even though many real-world situations have a continuous-option space. Recently, theories have attempted to address decisions with continuous-option spaces, and several computational models have been proposed within the sequential sampling framework to explain how we make a decision in continuous-option space. This article aims to review the main attempts to understand decisions on continuous-option spaces, give an overview of applications of these types of decisions, and present puzzles to be addressed by future developments.}
}
@article{WISE2024144,
title = {Naturalistic reinforcement learning},
journal = {Trends in Cognitive Sciences},
volume = {28},
number = {2},
pages = {144-158},
year = {2024},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2023.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S1364661323002127},
author = {Toby Wise and Kara Emery and Angela Radulescu},
keywords = {reinforcement learning, decision-making, naturalistic, computational modeling},
abstract = {Humans possess a remarkable ability to make decisions within real-world environments that are expansive, complex, and multidimensional. Human cognitive computational neuroscience has sought to exploit reinforcement learning (RL) as a framework within which to explain human decision-making, often focusing on constrained, artificial experimental tasks. In this article, we review recent efforts that use naturalistic approaches to determine how humans make decisions in complex environments that better approximate the real world, providing a clearer picture of how humans navigate the challenges posed by real-world decisions. These studies purposely embed elements of naturalistic complexity within experimental paradigms, rather than focusing on simplification, generating insights into the processes that likely underpin humans’ ability to navigate complex, multidimensional real-world environments so successfully.}
}
@article{KIREEV1994143,
title = {Approximate molecular electrostatic potential computations: applications to quantitative structure-activity relationships},
journal = {Journal of Molecular Structure: THEOCHEM},
volume = {304},
number = {2},
pages = {143-150},
year = {1994},
issn = {0166-1280},
doi = {https://doi.org/10.1016/S0166-1280(96)80006-6},
url = {https://www.sciencedirect.com/science/article/pii/S0166128096800066},
author = {Dmitry B. Kireev and Valery I. Fetisov and Nikolai S. Zefirov},
abstract = {Two new methods for calculating molecular electrostatic potentials are considered, taking into account QSAR requirements. The first of these is based on quantum chemical approximations; the other uses the topology of molecules. A program for displaying potential contour maps generated by various methods is presented. Examples of the successful use of these methods are given.}
}
@article{GRAGERT199711,
title = {Differential geometric computations and computer algebra},
journal = {Mathematical and Computer Modelling},
volume = {25},
number = {8},
pages = {11-24},
year = {1997},
issn = {0895-7177},
doi = {https://doi.org/10.1016/S0895-7177(97)00055-1},
url = {https://www.sciencedirect.com/science/article/pii/S0895717797000551},
author = {P.K.H Gragert and P.H.M Kersten},
keywords = {Computer algebra, Differential geometry, Literate programming, Supersymmetry},
abstract = {The use of computer algebra in the field of differential geometry and its applications to geometric structures of partial differential equations is discussed. The differential geometric setting is shortly described; a number of programs are slightly touched, some examples given, and an application to the construction of supersymmetric extensions of the Korteweg-de Vries equation is demonstrated.}
}
@article{ZHANG201499,
title = {Profiles of psychiatric symptoms among amphetamine type stimulant and ketamine using inpatients in Wuhan, China},
journal = {Journal of Psychiatric Research},
volume = {53},
pages = {99-102},
year = {2014},
issn = {0022-3956},
doi = {https://doi.org/10.1016/j.jpsychires.2014.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S0022395614000508},
author = {Yao Zhang and Zaifeng Xu and Sheng Zhang and Alethea Desrosiers and Richard S. Schottenfeld and Marek C. Chawarski},
keywords = {Amphetamine type stimulants (ATS), Ketamine, Psychiatric symptoms},
abstract = {Amphetamine type stimulants (ATS) and ketamine have emerged as major drug problems in China, and chronic extensive exposure to these substances frequently co-occurs with psychiatric symptoms. This study compares the psychiatric symptoms of patients reporting ATS use only, ATS and ketamine use, or ketamine use only who were admitted to an inpatient psychiatry ward in Wuhan, China between 2010 and 2011. Data on 375 study participants collected during their ward admission and extracted from their clinical records included their socio-demographics, scores on the Brief Psychiatric Rating Scale (BPRS), and urine toxicology screens.
Results
The ketamine-only group had significantly lower total BPRS scores and significantly lower scores on Thinking Disorder, Activity, and Hostility-Suspicion BPRS subscales than the ATS-only and ATS + ketamine groups (p < 0.001 for all comparisons). The ketamine-only group also had significantly higher scores on the subscales of Anxiety-Depression and Anergia. The ATS-only group had significantly higher scores on subscales of Thinking Disorder, Activity, and Hostility-Suspicion and significantly lower scores on Anxiety-Depression and Anergia subscales than the ketamine-only and ATS + ketamine groups (p < 0.001 for all comparisons). A K-means cluster method identified three distinct clusters of patients based on the similarities of their BPRS subscale profiles, and the identified clusters differed markedly on the proportions of participants reporting different primary drugs of abuse. The study findings suggest that ketamine and ATS users present with different profiles of psychiatric symptoms at admission to inpatient treatment.}
}
@article{GARLING1994355,
title = {Computational-process modelling of household activity scheduling},
journal = {Transportation Research Part B: Methodological},
volume = {28},
number = {5},
pages = {355-364},
year = {1994},
issn = {0191-2615},
doi = {https://doi.org/10.1016/0191-2615(94)90034-5},
url = {https://www.sciencedirect.com/science/article/pii/0191261594900345},
author = {Tommy Gärling and Mei-Po Kwan and Reginald G. Golledge},
abstract = {Models of households' travel choices are an important focus of research. For some time, it has been known that such models need to incorporate how travel depends on activity choices. It is argued that production system models constitute an alternative or necessary complementary approach if the goal is to develop models of interdependent activity and travel choices, or activity scheduling, which are based on behavioral science theories of higher cognitive processes. Several computational-process models (CPMs) which implement production systems as computer programs are reviewed. Currently, no encompassing CPM exists but some may be possible to integrate in a descriptive model of activity scheduling.}
}
@incollection{ZHANG2021440,
title = {The Use and Value of Geographic Information Systems in Transportation Modeling},
editor = {Roger Vickerman},
booktitle = {International Encyclopedia of Transportation},
publisher = {Elsevier},
address = {Oxford},
pages = {440-447},
year = {2021},
isbn = {978-0-08-102672-4},
doi = {https://doi.org/10.1016/B978-0-08-102671-7.10364-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780081026717103641},
author = {Ming Zhang},
keywords = {Accessibility modeling, Big-data, Four-step models, Geographic information systems (GIS), Land use-transportation integrated (LUTI) models, New mobility, Relational database, Topological data structure, Traffic simulation, Transportation modeling, Visualization},
abstract = {Geographic information systems (GIS) and transportation modeling have been developing in their respective fields separately. However, the geographic focus of GIS and the geographic nature of transportation make the use of GIS in transportation modeling both a logical choice and a motivation for modeling enhancement. This chapter first explains how the topological data structure underlying the design of vector GIS fits well the needs of data handling for transportation modeling. It then discusses the use and value of GIS in transportation modeling in three areas: (1) GIS for the four-step transportation models and for specific transportation modeling interests such as accessibility modeling and land use-transportation integrated (LUTI) modeling; (2) GIS visualization capabilities for presenting transportation modeling output and for facilitating visual thinking and knowledge construction; and (3) the potential of and challenges to GIS in transportation modeling amid rapid development of big-data technologies and new mobility.}
}
@article{PIERONI2016412,
title = {Transforming a Traditional Product Offer into PSS: A Practical Application},
journal = {Procedia CIRP},
volume = {47},
pages = {412-417},
year = {2016},
note = {Product-Service Systems across Life Cycle},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2016.03.036},
url = {https://www.sciencedirect.com/science/article/pii/S2212827116300051},
author = {Marina Pieroni and Caio Marques and Carina Campese and Daniel Guzzo and Glauco Mendes and Janaína Costa and Maiara Rosa and Maicon Gouveia de Oliveira and Victor Macul and Henrique Rozenfeld},
keywords = {product-service system, servitization, business model, design thinking, practical application, action research},
abstract = {In the last decades, companies have shifted from traditional business models based on selling products to product-service systems (PSS). Despite this tendency, there is a paucity of complete methodologies and tools to guide companies on how the transition should occur. To address this issue, the goal of this research is to present a complete framework to support manufacturing companies in the servitization journey. This novel proposal involves the application of design thinking to define the value proposition integrated with a PSS oriented business model creation, that goes beyond generic methods normally applied; and the specification of business process architecture to support PSS implementation. This research followed a prescriptive approach by means of action research technique. Key findings of the framework application are presented.}
}
@article{BLISS19921,
title = {Reasoning supported by computational tools},
journal = {Computers & Education},
volume = {18},
number = {1},
pages = {1-9},
year = {1992},
issn = {0360-1315},
doi = {https://doi.org/10.1016/0360-1315(92)90030-9},
url = {https://www.sciencedirect.com/science/article/pii/0360131592900309},
author = {Joan Bliss and Jon Ogborn and Richard Boohan and Jonathan Briggs and Tim Brosnan and Derek Brough and Harvey Mellar and Rob Miller and Caroline Nash and Cathy Rodgers and Babis Sakonidis},
abstract = {This paper sets out the work of the Tools for Exploratory Learning Programme within the ESRC Initiative Information Technology in Education. The research examines young secondary children's reasoning with computational tools. We distinguish between exploratory and expressive modes of learning, that is, interaction with another's model and creation of one's own model, respectively. The research focuses on reasoning, rather than learning, along three dimensions: quantitative, qualitative, and semi-quantitative. It provides a 3 × 2 classification of tasks according to modes of learning and types of reasoning. Modelling tools were developed for the study and descriptions of these are given. The research examined children's reasoning with tools in all three dimensions looking more exhaustively at the semi-quantitative. Pupils worked either in an exploratory mode or an expressive mode on one of the following topics: Traffic, Health and Diet, and Shops and Profits. They spent 3–4 h individually with a researcher over 2 weeks, carrying out four different activities: reasoning without the computer; learning to manipulate first the computer then later the tool and finally carrying out a task with the modelling tool. Pupils were between 12 and 14 yr. Research questions both about children's reasoning when working with or creating models and about the nature of the tools used are discussed. Finally an analytic scheme is set out which describes the nature of the causal and non-causal reasoning observed together with some tentative results.}
}
@article{WASKAN2003259,
title = {Intrinsic cognitive models},
journal = {Cognitive Science},
volume = {27},
number = {2},
pages = {259-283},
year = {2003},
issn = {0364-0213},
doi = {https://doi.org/10.1016/S0364-0213(02)00119-2},
url = {https://www.sciencedirect.com/science/article/pii/S0364021302001192},
author = {Jonathan A Waskan},
keywords = {Philosophy, Artificial intelligence, Psychology, Representation, Philosophy of mind, Philosophy of computation, Causal reasoning, Knowledge representation, Computer simulation},
abstract = {Theories concerning the structure, or format, of mental representation should (1) be formulated in mechanistic, rather than metaphorical terms; (2) do justice to several philosophical intuitions about mental representation; and (3) explain the human capacity to predict the consequences of worldly alterations (i.e., to think before we act). The hypothesis that thinking involves the application of syntax-sensitive inference rules to syntactically structured mental representations has been said to satisfy all three conditions. An alternative hypothesis is that thinking requires the construction and manipulation of the cognitive equivalent of scale models. A reading of this hypothesis is provided that satisfies condition (1) and which, even though it may not fully satisfy condition (2), turns out (in light of the frame problem) to be the only known way to satisfy condition (3).}
}
@article{MANCINI2022102697,
title = {Out of sight, out of mind? The importance of local context and trust in understanding the social acceptance of biogas projects: A global scale review},
journal = {Energy Research & Social Science},
volume = {91},
pages = {102697},
year = {2022},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2022.102697},
url = {https://www.sciencedirect.com/science/article/pii/S2214629622002018},
author = {Eliana Mancini and Andrea Raggi},
keywords = {Social acceptance, Bioenergy, Non-technical barriers, Biogas, Socio-cultural factors, Life Cycle Thinking},
abstract = {Social acceptance is considered the main non-technical barrier to the development of bioenergy projects. This paper presents the results of a systematic literature review aimed to cover a lack in state-of-the-art literature about socio-cultural factors affecting the acceptance of biogas projects at a global scale. Moreover, this study is aimed at identifying which methods are used for studying this phenomenon, with a focus on the Life Cycle Thinking-oriented ones. Journal articles and conference proceedings were considered. At the end of the screening phases, 54 documents were selected and reviewed. The results showed that acceptance concerns two main issues: biogas plants and its presence in a given location and digestate application on fields. This review showed different results between high-income and low-middle-income countries. As regards the former, trust was the most mentioned socio-cultural factor. Education, as well as women's living conditions were considered important in the latter. However, a contextualisation of every outcome based on local peculiarities is needed in order to understand in a better way the accepting/refuting phenomena of the projects. As regards the second objective of this study, Life Cycle Analysis resulted the most widespread Life Cycle Thinking methodology. In conclusion, the outcomes of this work may be useful to identify the non-technical factors and the most suitable approach that should be considered for a successful implementation of site-specific biogas projects.}
}
@article{YANG2024109519,
title = {Global optimization strategy of prosumer data center system operation based on multi-agent deep reinforcement learning},
journal = {Journal of Building Engineering},
volume = {91},
pages = {109519},
year = {2024},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2024.109519},
url = {https://www.sciencedirect.com/science/article/pii/S2352710224010878},
author = {Dongfang Yang and Xiaoyuan Wang and Rendong Shen and Yang Li and Lei Gu and Ruifan Zheng and Jun Zhao and Xue Tian},
keywords = {Data center system, Global cooperative optimization, D3QN, VDN},
abstract = {The escalating issues of high energy consumption and carbon emissions in data centers (DCs) necessitate the optimization of system operations. However, early optimization strategies were overly simplistic and lacked automated updating and iterative capabilities. With the evolution of artificial intelligence (AI), researchers have applied deep reinforcement learning (DRL) algorithms to system operations. However, the optimization focus has been limited to the internal systems, lacking global optimization. In this paper, a global optimization control strategy based on the Dueling double-deep Q network (D3QN) and value decomposition network (VDN) algorithms is proposed to make the DCs system operate more closely with the upstream, midstream, and downstream. By adjusting battery charging/discharging capacity, computational workload, and waste heat utilization heating temperature global synergistic optimization is achieved. Compared with without optimization, renewable energy waste, operation cost, total electricity consumption, and grid electricity consumption are reduced by 18.37%, 9.78%, 4.01%, and 29.74%, respectively. Additionally, a detailed comparison between non-algorithmic optimization and algorithmic optimization is provided, offering valuable insights for substantial energy savings and emissions reduction in DCs. The results demonstrate the importance of fully exploring the interactive potential between upstream energy supply, midstream computational workload, and downstream waste heat recovery to achieve synergistic global optimization of “computing power", “thermal energy" and “electrical energy" for the sustainable and green development of DCs or other prosumer buildings.}
}
@article{ERKELENS19982999,
title = {A computational model of depth perception based on headcentric disparity},
journal = {Vision Research},
volume = {38},
number = {19},
pages = {2999-3018},
year = {1998},
issn = {0042-6989},
doi = {https://doi.org/10.1016/S0042-6989(98)00084-4},
url = {https://www.sciencedirect.com/science/article/pii/S0042698998000844},
author = {Casper J. Erkelens and Raymond {van Ee}},
keywords = {Binocular vision, Stereopsis, Disparity, Binocular saccades},
abstract = {It is now well established that depth is coded by local horizontal disparity and global vertical disparity. We present a computational model which explains how depth is extracted from these two types of disparities. The model uses the two (one for each eye) headcentric directions of binocular targets, derived from retinal signals and oculomotor signals. Headcentric disparity is defined as the difference between headcentric directions of corresponding features in the left and right eye’s images. Using Helmholtz’s coordinate systems we decompose headcentric disparity into azimuthal and elevational disparity. Elevational disparities of real objects are zero if the signals which contribute to headcentric disparity do not contain any errors. Azimuthal headcentric disparity is a 1D quantity from which an exact equation relating distance and disparity can be derived. The equation is valid for all headcentric directions and for all binocular fixation positions. Such an equation does not exist if disparity is expressed in retinal coordinates. Possible types of errors in oculomotor signals (six) produce global elevational disparity fields which are characterised by different gradients in the azimuthal and elevational directions. Computations show that the elevational disparity fields uniquely characterise both the type and size of the errors in oculomotor signals. Our model uses a measure of the global elevational disparity field together with local azimuthal disparity to accurately derive headcentric distance throughout the visual field. The model explains existing data on whole-field disparity transformations as well as hitherto unexplained aspects of stereoscopic depth perception.}
}
@article{FURLAN2022163,
title = {The earth vibrates with analogies: The Dirac sea and the geology of the vacuum},
journal = {Studies in History and Philosophy of Science},
volume = {93},
pages = {163-174},
year = {2022},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2022.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0039368122000590},
author = {Stefano Furlan and Rocco Gaudenzi},
keywords = {Analogies, Analogical thinking, Heuristics, History of quantum physics, Vacuum, Spontaneous symmetry breaking},
abstract = {The debate around analogy in modern physics that focuses on its role as a logical inference often correspondingly overlooks its historical dimension and the other equally important functions and aspects that are intertwined with this dimension. Inspired by a close investigation of the primary sources and archival material of a few historical actors, this paper lays out a framework on analogy-making which preserves as much as possible its historical complexity. While not losing sight of the logical role, our framework puts a special emphasis on the heuristic process, and aims at offering to the historian and philosopher of science as well as the physicist some tools to capture the subtle functions of analogical reasoning involved in such a process. After having traced it out theoretically, we make use of this framework to interpret the growth of the ideas of two remarkable physicists dealing with the multifaceted notion of vacuum in 20th century physics. We first consider the trajectory followed by John A. Wheeler, between the 1960s and 1970s, towards (in his own words) a “geology of the vacuum”; and then examine, starting from the hitherto neglected Japanese reception of the idea of Dirac sea in the early 1930s, the pathway that led Yoichiro Nambu to the discovery of spontaneous symmetry breaking.}
}
@article{DUBEY2020118,
title = {Understanding exploration in humans and machines by formalizing the function of curiosity},
journal = {Current Opinion in Behavioral Sciences},
volume = {35},
pages = {118-124},
year = {2020},
note = {Curiosity (Explore vs Exploit)},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2020.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S2352154620301108},
author = {Rachit Dubey and Thomas L Griffiths},
abstract = {Recent work in machine learning has demonstrated the benefits of providing artificial agents with a sense of curiosity—a form of intrinsic reward that supports exploration. Two strategies have emerged for defining these rewards: favoring novelty and pursuing prediction errors. Psychological theories of curiosity have also emphasized these two factors. We show how these two literatures can be connected by understanding the function of curiosity, which requires thinking about the abstract computational problem that both humans and machines face as they explore their world.}
}
@article{YANG2023414,
title = {A review of sequential three-way decision and multi-granularity learning},
journal = {International Journal of Approximate Reasoning},
volume = {152},
pages = {414-433},
year = {2023},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2022.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X2200192X},
author = {Xin Yang and Yanhua Li and Tianrui Li},
keywords = {Three-way decision, Granular computing, Sequential three-way decision, Three-way multi-granularity learning},
abstract = {The concept of three-way decision, interpreted and described as thinking, problem solving, and information processing in “threes”, has been widely studied and applied in machine learning and data engineering in recent years. In open-world environment, the connection and interaction of dynamic and uncertainty by multi-granularity learning gives more vitality to three-way decision. In this paper, we investigate and summarize the initial and development models of three-way decision. Then we revisit the historical line of sequential three-way decision from rough set to granular computing. Besides, we focus on exploring a unified framework of three-way multi-granularity learning with four crucial problems on mining uncertain region continually. Finally, we give some proposals on three-way decision associated with open-continual learning.}
}
@article{HUANG202233634,
title = {Transition from synaptic simulation to nonvolatile resistive switching behavior based on an Ag/Ag:ZnO/Pt memristor},
journal = {RSC Advances},
volume = {12},
number = {52},
pages = {33634-33640},
year = {2022},
issn = {2046-2069},
doi = {https://doi.org/10.1039/d2ra05483c},
url = {https://www.sciencedirect.com/science/article/pii/S2046206922032296},
author = {Yong Huang and Jiahao Yu and Yu Kong and Xiaoqiu Wang},
abstract = {ABSTRACT
The advent of memristors and the continuing research and development in the field of brain-inspired computing could allow realization of a veritable “thinking machine”. In this study, ZnO-based memristors were fabricated using a radio frequency magnetron sputtering method. The ZnO oxide layer was prepared by incorporating silver nanocrystals (NCs). Several synaptic functions, i.e. nonlinear transmission characteristics, short-term potentiation, long-term potentiation/depression, and pair-pulse facilitation, were imitated in the memristor successfully. Furthermore, the transition from synaptic behaviors to bipolar resistive switching behaviors of the device was also observed under repeated stimulus. It is speculated that the switching mechanism is due to the formation and rupture of the conductive Ag filaments and the corresponding electrochemical metallization. The experimental results demonstrate that the Ag/Ag:ZnO/Pt memristor with resistive switching and several synaptic behaviors has a potential application in neuromorphic computing and data storage systems.}
}
@article{DELEON2003507,
title = {On the computation of the Lichnerowicz–Jacobi cohomology},
journal = {Journal of Geometry and Physics},
volume = {44},
number = {4},
pages = {507-522},
year = {2003},
issn = {0393-0440},
doi = {https://doi.org/10.1016/S0393-0440(02)00056-6},
url = {https://www.sciencedirect.com/science/article/pii/S0393044002000566},
author = {Manuel {de León} and Belén López and Juan C. Marrero and Edith Padrón},
keywords = {Jacobi manifolds, Poisson manifolds, Lie algebroids, Lichnerowicz–Jacobi cohomology, Contact manifolds, Locally conformal symplectic manifolds},
abstract = {Lichnerowicz–Jacobi cohomology of Jacobi manifolds is reviewed. The use of the associated Lie algebroid allows to prove that the Lichnerowicz–Jacobi cohomology is invariant under conformal changes of the Jacobi structure. We also compute the Lichnerowicz–Jacobi cohomology for a large variety of examples.}
}
@article{SCHACTER1999403,
title = {Computer-based performance assessments: a solution to the narrow measurement and reporting of problem-solving☆☆The findings and opinions expressed in this report do not reflect the position or policies of ISX, Advanced Research Projects Agency, the Department of the Navy, or the Department of Defense; nor do they reflect the positions or policies of the National Institute on Student Achievement, Curriculum, and Assessment, the Office of Educational Research and Improvement, or the US Department of Education.},
journal = {Computers in Human Behavior},
volume = {15},
number = {3},
pages = {403-418},
year = {1999},
issn = {0747-5632},
doi = {https://doi.org/10.1016/S0747-5632(99)00029-1},
url = {https://www.sciencedirect.com/science/article/pii/S0747563299000291},
author = {J. Schacter and H.E. Herl and G.K.W.K. Chung and R.A. Dennis and H.F. O'Neil},
keywords = {Assessment, Problem solving, Computers, Internet, Technology, Education},
abstract = {Although performance assessments test for higher order thinking and problem solving, they rarely report students' thinking process data back to teachers, students, or the public. Web-based database-backed performance assessments provide a viable means for concurrently reporting both performance and thinking process data. In the research conducted here, we report our findings from a study that assessed student problem solving using networked computers. Both performance and process data could be reported back to teachers and students such that they could diagnose and understand how they performed and what problem-solving processes contributed to or detracted from their performance.}
}
@article{THIBODEAU2017852,
title = {How Linguistic Metaphor Scaffolds Reasoning},
journal = {Trends in Cognitive Sciences},
volume = {21},
number = {11},
pages = {852-863},
year = {2017},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2017.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1364661317301535},
author = {Paul H. Thibodeau and Rose K. Hendricks and Lera Boroditsky},
keywords = {analogy, decision making, framing, language and thought, metaphor, reasoning},
abstract = {Language helps people communicate and think. Precise and accurate language would seem best suited to achieve these goals. But a close look at the way people actually talk reveals an abundance of apparent imprecision in the form of metaphor: ideas are ‘light bulbs’, crime is a ‘virus’, and cancer is an ‘enemy’ in a ‘war’. In this article, we review recent evidence that metaphoric language can facilitate communication and shape thinking even though it is literally false. We first discuss recent experiments showing that linguistic metaphor can guide thought and behavior. Then we explore the conditions under which metaphors are most influential. Throughout, we highlight theoretical and practical implications, as well as key challenges and opportunities for future research.}
}
@article{RASMUSSEN2007195,
title = {Reinventing solutions to systems of linear differential equations: A case of emergent models involving analytic expressions},
journal = {The Journal of Mathematical Behavior},
volume = {26},
number = {3},
pages = {195-210},
year = {2007},
note = {An Inquiry Oriented Approach to Differential Equations},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2007.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732312307000338},
author = {Chris Rasmussen and Howard Blumenfeld},
keywords = {Modeling, Undergraduate mathematics, Realistic mathematics education, Student thinking, Proportional reasoning},
abstract = {An enduring challenge in mathematics education is to create learning environments in which students generate, refine, and extend their intuitive and informal ways of reasoning to more sophisticated and formal ways of reasoning. Pressing concerns for research, therefore, are to detail students’ progressively sophisticated ways of reasoning and instructional design heuristics that can facilitate this process. In this article we analyze the case of student reasoning with analytic expressions as they reinvent solutions to systems of two differential equations. The significance of this work is twofold: it includes an elaboration of the Realistic Mathematics Education instructional design heuristic of emergent models to the undergraduate setting in which symbolic expressions play a prominent role, and it offers teachers insight into student thinking by highlighting qualitatively different ways that students reason proportionally in relation to this instructional design heuristic.}
}
@article{LEMOEL2020110,
title = {Towards a multi-level understanding in insect navigation},
journal = {Current Opinion in Insect Science},
volume = {42},
pages = {110-117},
year = {2020},
note = {Neuroscience * Biomechanics of Insect Flight and Bio-inspired engineering},
issn = {2214-5745},
doi = {https://doi.org/10.1016/j.cois.2020.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S2214574520301310},
author = {Florent {Le Moël} and Antoine Wystrach},
abstract = {To understand the brain is to understand behaviour. However, understanding behaviour itself requires consideration of sensory information, body movements and the animal’s ecology. Therefore, understanding the link between neurons and behaviour is a multi-level problem, which can be achieved when considering Marr’s three levels of understanding: behaviour, computation, and neural implementation. Rather than establishing direct links between neurons and behaviour, the matter boils down to understanding two transitions: the link between neurons and brain computation on one hand, and the link between brain computations and behaviour on the other hand. The field of insect navigation illustrates well the power of such two-sided endeavour. We provide here examples revealing that each transition requires its own approach with its own intrinsic difficulties, and show how modelling can help us reach the desired multi-level understanding.}
}
@article{POWELL2016147,
title = {Deconstructing intellectual curiosity},
journal = {Personality and Individual Differences},
volume = {95},
pages = {147-151},
year = {2016},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2016.02.037},
url = {https://www.sciencedirect.com/science/article/pii/S0191886916300927},
author = {Christopher Powell and Ted Nettelbeck and Nicholas R. Burns},
keywords = {Curiosity, Intellectual curiosity, Epistemic Curiosity, Need for Cognition, Typical Intellectual Engagement, Intellect},
abstract = {Scales of Need for Cognition (NFC), Typical Intellectual Engagement (TIE), and Epistemic Curiosity (EC) measure intellectual curiosity (IC). These scales correlate strongly and have been factor-analyzed individually but not together. Here N=396 (143 males) undergraduates completed measures of NFC, TIE, and EC. Six factors, labeled Intellectual Avoidance, Deprivation, Problem Solving, Abstract Thinking, Reading, and Wide Interest, were identified. TIE is the broadest scale, measuring all factors except Deprivation; NFC measures Intellectual Avoidance and Problem Solving, plus Abstract Thinking and Deprivation to a lesser degree; and EC largely measures Deprivation. Moreover, Reading may not fit in the IC domain; higher-order factor analysis indicated that, whereas items measuring Reading loaded more strongly on their first-order factor, items measuring the other factors strongly loaded on a general factor of IC. These results are significant for understanding the contents of these scales, and for future scale development.}
}
@article{WANG2022e09982,
title = {Applying the post-digital strategy of anexact architecture to non-standard design practices within the challenging construction contexts},
journal = {Heliyon},
volume = {8},
number = {8},
pages = {e09982},
year = {2022},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2022.e09982},
url = {https://www.sciencedirect.com/science/article/pii/S2405844022012701},
author = {Sining Wang and Dandan Lin},
keywords = {Design practice strategy, Post-digital architecture, Parametric design, Developing region, Non-standard architecture},
abstract = {New architectural forms offered by digital design approaches often appear incompatible with the prescribed precision and control in construction, especially in developing regions where advanced implementation means are limited. In response, this paper suggests working with design practice indeterminacy. Named ‘anexact architecture’, the post-digital design practice strategy presents a convergent diagram of seeking the feasible design solution space. It relies on the procedural parametric modelling to constantly integrate computation and humanisation, so that a rigorous built outcome is capable of accommodating project-specific idiosyncrasies and constraints. The demonstrator projects are discussed based on the combination of the Participatory Action Research method and the idea of anexact architecture. This paper aims to illustrate the peculiarity of anexact architecture and its ideology of treating design delivery uncertainties as essentials rather than negatives when practicing in a volatile construction context.}
}
@article{FAHIMI2024,
title = {Improving the Efficiency of Inferences From Hybrid Samples for Effective Health Surveillance Surveys: Comprehensive Review of Quantitative Methods},
journal = {JMIR Public Health and Surveillance},
volume = {10},
year = {2024},
issn = {2369-2960},
doi = {https://doi.org/10.2196/48186},
url = {https://www.sciencedirect.com/science/article/pii/S2369296024000188},
author = {Mansour Fahimi and Elizabeth C Hair and Elizabeth K Do and Jennifer M Kreslake and Xiaolu Yan and Elisa Chan and Frances M Barlas and Abigail Giles and Larry Osborn},
keywords = {hybrid samples, composite estimation, optimal composition factor, unequal weighting effect, composite weighting, weighting, surveillance, sample survey, data collection, risk factor},
abstract = {Background
Increasingly, survey researchers rely on hybrid samples to improve coverage and increase the number of respondents by combining independent samples. For instance, it is possible to combine 2 probability samples with one relying on telephone and another on mail. More commonly, however, researchers are now supplementing probability samples with those from online panels that are less costly. Setting aside ad hoc approaches that are void of rigor, traditionally, the method of composite estimation has been used to blend results from different sample surveys. This means individual point estimates from different surveys are pooled together, 1 estimate at a time. Given that for a typical study many estimates must be produced, this piecemeal approach is computationally burdensome and subject to the inferential limitations of the individual surveys that are used in this process.
Objective
In this paper, we will provide a comprehensive review of the traditional method of composite estimation. Subsequently, the method of composite weighting is introduced, which is significantly more efficient, both computationally and inferentially when pooling data from multiple surveys. With the growing interest in hybrid sampling alternatives, we hope to offer an accessible methodology for improving the efficiency of inferences from such sample surveys without sacrificing rigor.
Methods
Specifically, we will illustrate why the many ad hoc procedures for blending survey data from multiple surveys are void of scientific integrity and subject to misleading inferences. Moreover, we will demonstrate how the traditional approach of composite estimation fails to offer a pragmatic and scalable solution in practice. By relying on theoretical and empirical justifications, in contrast, we will show how our proposed methodology of composite weighting is both scientifically sound and inferentially and computationally superior to the old method of composite estimation.
Results
Using data from 3 large surveys that have relied on hybrid samples composed of probability-based and supplemental sample components from online panels, we illustrate that our proposed method of composite weighting is superior to the traditional method of composite estimation in 2 distinct ways. Computationally, it is vastly less demanding and hence more accessible for practitioners. Inferentially, it produces more efficient estimates with higher levels of external validity when pooling data from multiple surveys.
Conclusions
The new realities of the digital age have brought about a number of resilient challenges for survey researchers, which in turn have exposed some of the inefficiencies associated with the traditional methods this community has relied upon for decades. The resilience of such challenges suggests that piecemeal approaches that may have limited applicability or restricted accessibility will prove to be inadequate and transient. It is from this perspective that our proposed method of composite weighting has aimed to introduce a durable and accessible solution for hybrid sample surveys.}
}
@article{GREENSPAN1990490,
title = {A counterexample of the use of energy as a measure of computational accuracy},
journal = {Journal of Computational Physics},
volume = {91},
number = {2},
pages = {490-494},
year = {1990},
issn = {0021-9991},
doi = {https://doi.org/10.1016/0021-9991(90)90051-2},
url = {https://www.sciencedirect.com/science/article/pii/0021999190900512},
author = {Donald Greenspan}
}
@article{CHAUDHARI2024100953,
title = {PSOGSA: A parallel implementation model for data clustering using new hybrid swarm intelligence and improved machine learning technique},
journal = {Sustainable Computing: Informatics and Systems},
volume = {41},
pages = {100953},
year = {2024},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2023.100953},
url = {https://www.sciencedirect.com/science/article/pii/S2210537923001087},
author = {Shruti Chaudhari and Anuradha Thakare and Ahmed M. Anter},
keywords = {Clustering, Swarm intelligence, PSO, Gravitational search algorithm, Neural network, GPU},
abstract = {With the digitization of the entire world and huge requirements of understanding unknown patterns from the data, clustering becomes an important research area. The quick and accurate division of large datasets with a range of properties or features becomes challenging. The parallel implementation of clustering algorithms must satisfy stringent computational requirements to handle large amounts of data. This can be achieved by designing a GPU based optimal computational model with a heuristic approach. Swarm Intelligence (SI), a family of bio-inspired algorithms, that has been effectively applied to a number of real-world clustering problems. The Gravitational Search Algorithm (GSA) is a heuristic search optimization approach based on Newton's Law of Gravitation and mass interactions. Although it has a slow searching rate in the last iterations, this strategy has been proved to be capable of discovering the global optimum. This paper presents GPU based hybrid parallel algorithms for data clustering. A newly developed, hybrid Particle Swarm Optimization (PSO) and Gravitational Search Algorithm (GSA) i.e., PSOGSA achieves the global optima. PSOGSA utilizes novel training methods for enhanced Neural Networks (NN) in order to examine the efficiency of algorithms and resolves the challenges of trapping in local minima. This also shows the sluggish convergence rate of standard evolutionary learning algorithms. The Nearest Neighbour Partition (Partitioning of the Neighbourhood) algorithm can be used to improve the performance of NN. A parallel version of Hybrid PSOGSA with NN is implemented to achieve optimal results with better computational time. Compared to the CPU-based regular PSO, the suggested Hybrid PSOGSA with NN achieved optimal clustering with 71% improved computational time.}
}
@article{BOVE20031040,
title = {Computational fluid dynamics in the evaluation of hemodynamic performance of cavopulmonary connections after the norwood procedure for hypoplastic left heart syndrome},
journal = {The Journal of Thoracic and Cardiovascular Surgery},
volume = {126},
number = {4},
pages = {1040-1047},
year = {2003},
issn = {0022-5223},
doi = {https://doi.org/10.1016/S0022-5223(03)00698-6},
url = {https://www.sciencedirect.com/science/article/pii/S0022522303006986},
author = {Edward L. Bove and Marc R. {de Leval} and Francesco Migliavacca and Gualtiero Guadagni and Gabriele Dubini},
keywords = {17, 21},
abstract = {Objective
Computational fluid dynamics have been used to study the hemodynamic performance of surgical operations, resulting in improved design. Efficient designs with minimal energy losses are especially important for cavopulmonary connections. The purpose of this study was to compare hydraulic performance between the hemi-Fontan and bidirectional Glenn procedures, as well as the various types of completion Fontan operations.
Methods
Three-dimensional models were constructed of typical hemi-Fontan and bidirectional Glenn operations according to anatomic data derived from magnetic resonance scans, angiocardiograms, and echocardiograms. Boundary conditions were imposed, and fluid dynamics were calculated from a mathematic code. Power losses, flow distribution to each lung, and pressures were measured at three predetermined levels of pulmonary arteriolar resistance. Models of the lateral tunnel, total cavopulmonary connection, and extracardiac conduit completion Fontan operations were constructed, and power losses, total flow distribution, vena caval and pulmonary arterial pressures, and flow distribution of inferior vena caval return were calculated.
Results
The hemi-Fontan and bidirectional Glenn procedures performed nearly identically, with similar power losses and nearly equal flow distributions to each lung at all levels of pulmonary arteriolar resistance. However, the lateral tunnel Fontan procedure as performed after the hemi-Fontan operation had lower power losses (6.9 mW, pulmonary arteriolar resistance 3 units) than the total cavopulmonary connection (40.5 mW) or the extracardiac conduit (42.9 mW), although the inclusion of an enlargement patch toward the right in the total cavopulmonary connection was effective in reducing the difference (10.0 mW). Inferior vena caval flow to the right lung was 52% for the lateral tunnel, compared with 19%, 30%, 19%, and 15% for the total cavopulmonary connection, total cavopulmonary connection with right-sided enlargement patch, extracardiac conduit, and extracardiac conduit with a bevel to the left lung, respectively.
Conclusions
According to these methods, the hemi-Fontan and bidirectional Glenn procedures performed equally well, but important differences in energy losses and flow distribution were found after the completion Fontan procedures. The superior hydraulic performance of the lateral tunnel Fontan operation after the hemi-Fontan procedure relative to any other method may be due to closer to optimal caval offset achieved in the surgical reconstruction.}
}
@article{HAYES201739,
title = {Regression-based statistical mediation and moderation analysis in clinical research: Observations, recommendations, and implementation},
journal = {Behaviour Research and Therapy},
volume = {98},
pages = {39-57},
year = {2017},
note = {Best Practice Guidelines for Modern Statistical Methods in Applied Clinical Research},
issn = {0005-7967},
doi = {https://doi.org/10.1016/j.brat.2016.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0005796716301887},
author = {Andrew F. Hayes and Nicholas J. Rockwood},
keywords = {Mediation analysis, Moderation, Interaction, Regression analysis, Mechanisms},
abstract = {There have been numerous treatments in the clinical research literature about various design, analysis, and interpretation considerations when testing hypotheses about mechanisms and contingencies of effects, popularly known as mediation and moderation analysis. In this paper we address the practice of mediation and moderation analysis using linear regression in the pages of Behaviour Research and Therapy and offer some observations and recommendations, debunk some popular myths, describe some new advances, and provide an example of mediation, moderation, and their integration as conditional process analysis using the PROCESS macro for SPSS and SAS. Our goal is to nudge clinical researchers away from historically significant but increasingly old school approaches toward modifications, revisions, and extensions that characterize more modern thinking about the analysis of the mechanisms and contingencies of effects.}
}
@article{MAHONY2020104668,
title = {New ideas for non-animal approaches to predict repeated-dose systemic toxicity: Report from an EPAA Blue Sky Workshop},
journal = {Regulatory Toxicology and Pharmacology},
volume = {114},
pages = {104668},
year = {2020},
issn = {0273-2300},
doi = {https://doi.org/10.1016/j.yrtph.2020.104668},
url = {https://www.sciencedirect.com/science/article/pii/S0273230020300945},
author = {Catherine Mahony and Randolph S. Ashton and Barbara Birk and Alan R. Boobis and Tom Cull and George P. Daston and Lorna Ewart and Thomas B. Knudsen and Irene Manou and Sebastian Maurer-Stroh and Luigi Margiotta-Casaluci and Boris P. Müller and Pär Nordlund and Ruth A. Roberts and Thomas Steger-Hartmann and Evita Vandenbossche and Mark R. Viant and Mathieu Vinken and Maurice Whelan and Zvonar Zvonimir and Mark T.D. Cronin},
keywords = {Repeated dose toxicity testing, Alternatives, Safety assessment, Chemical legislation, , , Read-across, },
abstract = {The European Partnership for Alternative Approaches to Animal Testing (EPAA) convened a ‘Blue Sky Workshop’ on new ideas for non-animal approaches to predict repeated-dose systemic toxicity. The aim of the Workshop was to formulate strategic ideas to improve and increase the applicability, implementation and acceptance of modern non-animal methods to determine systemic toxicity. The Workshop concluded that good progress is being made to assess repeated dose toxicity without animals taking advantage of existing knowledge in toxicology, thresholds of toxicological concern, adverse outcome pathways and read-across workflows. These approaches can be supported by New Approach Methodologies (NAMs) utilising modern molecular technologies and computational methods. Recommendations from the Workshop were based around the needs for better chemical safety assessment: how to strengthen the evidence base for decision making; to develop, standardise and harmonise NAMs for human toxicity; and the improvement in the applicability and acceptance of novel techniques. “Disruptive thinking” is required to reconsider chemical legislation, validation of NAMs and the opportunities to move away from reliance on animal tests. Case study practices and data sharing, ensuring reproducibility of NAMs, were viewed as crucial to the improvement of non-animal test approaches for systemic toxicity.}
}
@article{KAVLOCK2005265,
title = {Computational Toxicology: Framework, Partnerships, and Program Development: September 29–30, 2003, Research Triangle Park, North Carolina},
journal = {Reproductive Toxicology},
volume = {19},
number = {3},
pages = {265-280},
year = {2005},
note = {Systems Biology/Computational Toxicology},
issn = {0890-6238},
doi = {https://doi.org/10.1016/j.reprotox.2004.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S0890623804000747},
author = {Robert Kavlock and Gerald T. Ankley and Tim Collette and Elaine Francis and Karen Hammerstrom and Jack Fowle and Hugh Tilson and Greg Toth and Patricia Schmieder and Gilman D. Veith and Eric Weber and Douglas C. Wolf and Doug Young}
}
@article{RAIKOV2018492,
title = {Cognitive Modelling Quality Rising by Applying Quantum and Optical Semantic Approaches},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {30},
pages = {492-497},
year = {2018},
note = {18th IFAC Conference on Technology, Culture and International Stability TECIS 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.11.309},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318329823},
author = {A. Raikov},
keywords = {big data, quantum semantic, cognitive modelling, deep learning, decision-making, optical computing},
abstract = {Advanced decision support systems require significant acceleration of decision-making under conditions when factors describing the situation are ill-defined and non-metric. As a rule, such conditions arise in the field of politics, culture and in the social sphere. Cognitive modelling technology is applied in these cases. The cognitive models are created by people, experts from different subjects’ fields. The modelling processes take a great deal of time. Furthermore, the result of the modelling has to be verified when the model’s creators cannot get complete information and have to understand the problem very quickly. The factors and their mutual relationships in cognitive models could be verified with Big Data analysis technology. But this approach takes into account only denotational semantics that are based on the mapping of the model on formalised logical constructions, words, objects, schemes. This paper addresses the issue of creating cognitive semantics that take into consideration thinking, feeling and transcendental factors. It is shown that the classical computer or quantum computer cannot ensure cognitive semantics because they are based on discrete representation of data. An optical computing and Optical Semantic approach could be applied. The architecture of the special optical processor is represented.}
}
@article{SELVAKKUMARAN2020111053,
title = {Review of the use of system dynamics (SD) in scrutinizing local energy transitions},
journal = {Journal of Environmental Management},
volume = {272},
pages = {111053},
year = {2020},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2020.111053},
url = {https://www.sciencedirect.com/science/article/pii/S0301479720309816},
author = {Sujeetha Selvakkumaran and Erik O. Ahlgren},
keywords = {System dynamics, Modelling, Local, Energy transitions, Multi-level perspective},
abstract = {Local energy transition processes are complex socio-technical transitions requiring careful study. The use of System Dynamics (SD) in modelling and analyzing local energy transitions is especially suitable given the characteristics of SD. Our aim is to systematically categorize the different ways SD is used and useful to scrutinize local energy transitions, and to see if we can discern any common themes that can be useful to researchers looking to scrutinize local energy transitions, using SD. The study is exploratory in nature, with peer-reviewed journal and conference articles analyzed using content analysis. The six categories on which the articles are analyzed are: the sector the article studies; the transition that is studied in the article; the modelling depth in the article; the objective of the article; the justification for using SD provided in the article and the levels of interaction with ‘local’. Our findings show most of the local energy transitions have been studied using simulatable Stock and Flow Diagrams in SD methodology. The important sectors in the energy field are represented in terms of SD modelling of local energy transitions, including electricity, transport, district heating etc. Most of the local energy transitions scrutinized by SD in the articles have descriptive objectives, with some prescriptive, and just one evaluative objective. In terms of justification for using SD provided by the articles analyzed in this study, we found four major themes along which the justifications that were provided. They are dynamics, feedbacks, delays and complexity, systematic thinking, bridging disciplines and actor interactions and behaviour. The ‘dynamics, feedbacks, delays and complexity’ theme is the most cited justification for the use of SD in scrutinizing local energy transitions, followed by systematic thinking.}
}
@incollection{SALIMI201883,
title = {Chapter 2 - Fundamentals of Systemic Approach},
editor = {Fabienne Salimi and Frederic Salimi},
booktitle = {A Systems Approach to Managing the Complexities of Process Industries},
publisher = {Elsevier},
pages = {83-180},
year = {2018},
isbn = {978-0-12-804213-7},
doi = {https://doi.org/10.1016/B978-0-12-804213-7.00002-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128042137000025},
author = {Fabienne Salimi and Frederic Salimi},
keywords = {Systems engineering, systems thinking, critical thinking, Safety Critical Element (SCE), Project Management, Complexity, Emergence, SE Competency, Type of Systems, IIoT, Big Data},
abstract = {System thinking, system engineering, and complexity management are the back bone of any operational excellence and process safety management system. This chapter aims to give a solid but concise background for the fundamentals of system engineering, system thinking, and complexity management for process industry. Different type of processes, requirement engineering and management, safety critical systems, critical thinking, and SE competency framework are discussed. It also addresses issues that pertain to human judgment and how people employ rules of thumb and heuristics to problem-solving situations. Various modes of engineering are discussed along with the complexities and concerns within each: cognitive systems engineering, control engineering, software engineering, industrial engineering, performance engineering, and several others. A distinction is also made between technical performance measures and key performance parameters. A list of leading indicators, insights, and requirements are then delineated among the various aspects of system engineering. Finally, an overall analysis of systems thinking, which concerns the process of understanding how various systems are implemented, is provided.}
}
@article{ALTAY20111111,
title = {Fuzzy cognitive mapping in factor elimination: A case study for innovative power and risks},
journal = {Procedia Computer Science},
volume = {3},
pages = {1111-1119},
year = {2011},
note = {World Conference on Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2010.12.181},
url = {https://www.sciencedirect.com/science/article/pii/S1877050910005569},
author = {Ayca Altay and Gülgün Kayakutlu},
keywords = {Fuzzy cognitive maps, Innovation, Factor prioritization, Factor elimination},
abstract = {Factor or criteria prioritization is essential for decision making and planning. In most areas in decision making, integrating the related literature yields an exuberance of criteria which leads a robust decision. Yet, an excess number of criteria may handicap decision making or evaluations in terms of computational time and complexity. In these circumstances, decreasing the number of factors in exchange for a negligible amount of knowledge can emancipate the decision maker yet does not affect the quality of the decision. This elimination can be conducted through qualitative methods such as interviews or quantitative methods. However, quantitative methods are more trustworthy since qualitative methods can be deceptive due to the perceptions of the interviewee. Furthermore, working with larger groups is more prone to neutrality in terms of group thinking. On the subject of innovative power and risks, the literature offers 48 criteria depending on the industry, size or demographics of related companies. Prioritizing and working with these criteria for their decision making applications becomes computationally expensive, especially when embedded in more complex algorithms. In this study, 48 criteria will be reduced using Fuzzy Cognitive Maps and it is believed to provide a sufficient number of criteria with a negligible loss of information and comparisons will be conducted.}
}
@article{MCGILL2021113697,
title = {Evaluation of public health interventions from a complex systems perspective: A research methods review},
journal = {Social Science & Medicine},
volume = {272},
pages = {113697},
year = {2021},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2021.113697},
url = {https://www.sciencedirect.com/science/article/pii/S0277953621000290},
author = {Elizabeth McGill and Vanessa Er and Tarra Penney and Matt Egan and Martin White and Petra Meier and Margaret Whitehead and Karen Lock and Rachel {Anderson de Cuevas} and Richard Smith and Natalie Savona and Harry Rutter and Dalya Marks and Frank {de Vocht} and Steven Cummins and Jennie Popay and Mark Petticrew},
keywords = {Systems thinking, Complexity science, Evaluation methodologies, Public health, Practice},
abstract = {Introduction
Applying a complex systems perspective to public health evaluation may increase the relevance and strength of evidence to improve health and reduce health inequalities. In this review of methods, we aimed to: (i) classify and describe different complex systems methods in evaluation applied to public health; and (ii) examine the kinds of evaluative evidence generated by these different methods.
Methods
We adapted critical review methods to identify evaluations of public health interventions that used systems methods. We conducted expert consultation, searched electronic databases (Scopus, MEDLINE, Web of Science), and followed citations of relevant systematic reviews. Evaluations were included if they self-identified as using systems- or complexity-informed methods and if they evaluated existing or hypothetical public health interventions. Case studies were selected to illustrate different types of complex systems evaluation.
Findings
Seventy-four unique studies met our inclusion criteria. A framework was developed to map the included studies onto different stages of the evaluation process, which parallels the planning, delivery, assessment, and further delivery phases of the interventions they seek to inform; these stages include: 1) theorising; 2) prediction (simulation); 3) process evaluation; 4) impact evaluation; and 5) further prediction (simulation). Within this framework, we broadly categorised methodological approaches as mapping, modelling, network analysis and ‘system framing’ (the application of a complex systems perspective to a range of study designs). Studies frequently applied more than one type of systems method.
Conclusions
A range of complex systems methods can be utilised, adapted, or combined to produce different types of evaluative evidence. Further methodological innovation in systems evaluation may generate stronger evidence to improve health and reduce health inequalities in our complex world.}
}
@article{HAAS2024110900,
title = {Models vetted against prediction error and parameter sensitivity standards can credibly evaluate ecosystem management options},
journal = {Ecological Modelling},
volume = {498},
pages = {110900},
year = {2024},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2024.110900},
url = {https://www.sciencedirect.com/science/article/pii/S0304380024002886},
author = {Timothy C. Haas},
keywords = {Model vetting, Model credibility, Ecosystem management, Parameter sensitivity, Robust statistical estimators, High performance computing},
abstract = {A new standard for assessing model credibility is developed. This standard consists of parameter estimation, prediction error assessment, and a parameter sensitivity analysis that is driven by outside individuals who are skeptical of the model’s credibility (hereafter, skeptics). Ecological/environmental models that have a one-step-ahead prediction error rate that is better than naive forecasting — and are not excessively sensitive to small changes in their parameter values are said here to be vetted. A procedure is described that can perform this assessment on any model being evaluated for possible participation in an ecosystem management decision. Uncertainty surrounding the model’s ability to predict future values of its output variables and in the estimates of all of its parameters should be part of any effort to vett a model. The vetting procedure described herein, Prediction Error Rate-Deterministic Sensitivity Analysis (PER-DSA), incorporates these two aspects of model uncertainty. DSA in particular, requires participation by skeptics and is the reason why a successful DSA gives a model sufficient credibility to have a voice in ecosystem management decision making. But these models need to be stochastic and represent the mechanistic processes of the system being modeled. For such models, performing a PER-DSA can be computationally expensive. A cluster computing algorithm to speed-up these computations is described as one way to answer this challenge. This new standard is illustrated through a PER-DSA of a population dynamics model of South African rhinoceros (Ceratotherium simum simum).}
}
@article{SENANAYAKE2024104705,
title = {Agent-based simulation for pedestrian evacuation: A systematic literature review},
journal = {International Journal of Disaster Risk Reduction},
volume = {111},
pages = {104705},
year = {2024},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2024.104705},
url = {https://www.sciencedirect.com/science/article/pii/S2212420924004679},
author = {Gayani P.D.P. Senanayake and Minh Kieu and Yang Zou and Kim Dirks},
keywords = {Pedestrian behaviour modelling, Agent-based modelling, Behavioural decision-making, Emergency evacuation},
abstract = {Agent-based models (ABMs) offer promise for realistically simulating human behaviours and interactions during emergency evacuations. This review aims to systematically assess the state of the art in ABM-based evacuation modelling with respect to methodologies, validation practices, and the associated challenges over the past decade. The review critically examines 134 studies from 2013 to 2023 that have applied ABMs for pedestrian evacuation simulation to synthesise current capabilities, limitations, and advancement pathways. Findings identify persistent challenges related to modeller bias, computational complexity, data scarcity for calibration and validation, and the predominance of simplistic rule-based decision-making models, while promise exists with the adoption of flexible behavioural frameworks, high-performance computing architectures, machine learning techniques for adaptive agent behaviours and surrogate modelling, and evolutionary computation methods for transparent rule generation. The findings underscore the importance of interdisciplinary collaboration among behavioural scientists, modellers, and emergency planners to enhance the realism and reliability of ABMs. By providing a critical synthesis of the state-of-the-art and proposing future research directions, this review aims to accelerate the development and application of ABMs that can meaningfully enhance the safety and resilience of communities facing emergencies.}
}
@article{SIMON1993431,
title = {Experience in using SIMD and MIMD parallelism for computational fluid dynamics},
journal = {Applied Numerical Mathematics},
volume = {12},
number = {5},
pages = {431-442},
year = {1993},
issn = {0168-9274},
doi = {https://doi.org/10.1016/0168-9274(93)90103-X},
url = {https://www.sciencedirect.com/science/article/pii/016892749390103X},
author = {Horst D. Simon and Leonardo Dagum},
keywords = {Parallel architectures, MIMD, SIMD, computational fluid dynamics.},
abstract = {One of the key objectives of the Applied Research Branch in the Numerical Aerodynamic Simulation (NAS) Systems Division at NASA Ames Research Center is the accelerated introduction of highly parallel machines into a fully operational environment. In this report we summarize some of the experiences with the parallel testbed machines at the NAS Applied Research Branch. We discuss the performance results obtained from the implementation of two computational fluid dynamics (CFD) applications, an unstructured grid solver and a particle simulation, on the Connection Machine CM-2 and the Intel iPSC/860.}
}
@article{RANGEL2012970,
title = {Value normalization in decision making: theory and evidence},
journal = {Current Opinion in Neurobiology},
volume = {22},
number = {6},
pages = {970-981},
year = {2012},
note = {Decision making},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2012.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0959438812001201},
author = {Antonio Rangel and John A Clithero},
abstract = {A sizable body of evidence has shown that the brain computes several types of value-related signals to guide decision making, such as stimulus values, outcome values, and prediction errors. A critical question for understanding decision-making mechanisms is whether these value signals are computed using an absolute or a normalized code. Under an absolute code, the neural response used to represent the value of a given stimulus does not depend on what other values might have been encountered. By contrast, under a normalized code, the neural response associated with a given value depends on its relative position in the distribution of values. This review provides a simple framework for thinking about value normalization, and uses it to evaluate the existing experimental evidence.}
}
@article{TAKANO201922,
title = {Difficulty in updating positive beliefs about negative cognition is associated with increased depressed mood},
journal = {Journal of Behavior Therapy and Experimental Psychiatry},
volume = {64},
pages = {22-30},
year = {2019},
issn = {0005-7916},
doi = {https://doi.org/10.1016/j.jbtep.2019.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0005791617302926},
author = {Keisuke Takano and Julie {Van Grieken} and Filip Raes},
keywords = {depression, Rumination, Memory, Reinforcement learning, Q-learning},
abstract = {Background and objectives
Depressed people hold positive beliefs about negative cognition (e.g., rumination is useful to find a solution), which may motivate those individuals to engage in sustained negative thinking. However, in reality, rumination often leads to unfavorable outcomes. Thus, such beliefs create a large discrepancy between one's expectations and the actual outcome. Therefore, we hypothesized that this prediction error would be associated with increased depressed mood.
Methods
We observed how people update their positive beliefs about negative cognition within a volatile environment, in which negative cognition does not always result in a beneficial outcome. Forty-six participants were offered two response options (retrieving a negative or positive personal memory) and subsequently provided either an economic reward or punishment. Retrieving a negative (rather than positive) memory was initially reinforced, although this action-outcome contingency was reversed during the task. In the control condition, positive memory retrieval was initially reinforced, although a contingency reversal was employed to encourage negative memory retrieval.
Results
Model-based computational modeling revealed that participants who showed a delay in switching from negative to positive (but not from positive to negative) responses experienced increased levels of depressed mood. This delay in switching was also found to be associated with depressive symptoms and trait rumination.
Limitations
The non-clinical nature of the sample may limit the clinical implications of the results.
Conclusions
Difficulty in updating positive beliefs (or outcome predictions) for negative cognition may play an important role in depressive symptomatology.}
}
@article{HAMALAINEN199619,
title = {Accelerating genetic algorithm computation in tree shaped parallel computer},
journal = {Journal of Systems Architecture},
volume = {42},
number = {1},
pages = {19-36},
year = {1996},
issn = {1383-7621},
doi = {https://doi.org/10.1016/1383-7621(96)00009-4},
url = {https://www.sciencedirect.com/science/article/pii/1383762196000094},
author = {Timo Hämäläinen and Harri Klapuri and Jukka Saarinen and Pekka Ojala and Kimmo Kaski},
keywords = {Parallel genetic algorithms, Parallel implementation, Parallel computing, Tree shape architecture},
abstract = {Realizations of genetic algorithms (GAs) in a tree shape parallel computer architecture are presented using different levels of parallelism. In addition, basic models for parallel GAs are considered. The tree shape parallel computer system, GAPA (Genetic Algorithm Parallel Accelerator) with special hardware for GA computation, is described in detail. Also mappings for centralized and distributed GA models are given and their performance has been measured for different population sizes.}
}
@article{CAO2024101244,
title = {Explanatory models in neuroscience, Part 1: Taking mechanistic abstraction seriously},
journal = {Cognitive Systems Research},
volume = {87},
pages = {101244},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101244},
url = {https://www.sciencedirect.com/science/article/pii/S138904172400038X},
author = {Rosa Cao and Daniel Yamins},
keywords = {Mechanism, Models, Explanation, Constraints, Similarity, Mapping, Abstraction, Functional abstraction, Neural networks, Computation, Philosophy, Vision, Constraint, Prediction, Transform, Levels of explanation, Mechanistic explanation, Neuroscience, Understanding},
abstract = {Despite the recent success of neural network models in mimicking animal performance on various tasks, critics worry that these models fail to illuminate brain function. We take it that a central approach to explanation in systems neuroscience is that of mechanistic modeling, where understanding the system requires us to characterize its parts, organization, and activities, and how those give rise to behaviors of interest. However, it remains controversial what it takes for a model to be mechanistic, and whether computational models such as neural networks qualify as explanatory on this approach. We argue that certain kinds of neural network models are actually good examples of mechanistic models, when an appropriate notion of mechanistic mapping is deployed. Building on existing work on model-to-mechanism mapping (3M), we describe criteria delineating such a notion, which we call 3M++. These criteria require us, first, to identify an abstract level of description that is still detailed enough to be “runnable”, and then, to construct model-to-brain mappings using the same principles as those employed for brain-to-brain mapping across individuals. Perhaps surprisingly, the abstractions required are just those already in use in experimental neuroscience and deployed in the construction of more familiar computational models — just as the principles of inter-brain mappings are very much in the spirit of those already employed in the collection and analysis of data across animals. In a companion paper, we address the relationship between optimization and intelligibility, in the context of functional evolutionary explanations. Taken together, mechanistic interpretations of computational models and the dependencies between form and function illuminated by optimization processes can help us to understand why brain systems are built they way they are.}
}
@article{ZHENG20034147,
title = {A novel approach of three-dimensional hybrid grid methodology: Part 1. Grid generation},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {192},
number = {37},
pages = {4147-4171},
year = {2003},
issn = {0045-7825},
doi = {https://doi.org/10.1016/S0045-7825(03)00385-2},
url = {https://www.sciencedirect.com/science/article/pii/S0045782503003852},
author = {Yao Zheng and Meng-Sing Liou},
keywords = {Computational fluid dynamics, Grid generation, Hybrid grid},
abstract = {We propose a novel approach of three-dimensional hybrid grid methodology, the DRAGON grid method in the three-dimensional space. The DRAGON grid is created by means of a Direct Replacement of Arbitrary Grid Overlapping by Nonstructured grid, and is structured-grid dominated with unstructured grids in small regions. The DRAGON grid scheme is an adaptation to the Chimera thinking. It is capable of preserving the advantageous features of both the structured and unstructured grids, and eliminates/minimizes their shortcomings. In the present paper, we describe essential and programming aspects, and challenges of the three-dimensional DRAGON grid method, with respect to grid generation. We demonstrate the capability of generating computational grids for multi-components complex configurations.}
}
@article{BARROUILLET2011151,
title = {Dual-process theories of reasoning: The test of development},
journal = {Developmental Review},
volume = {31},
number = {2},
pages = {151-179},
year = {2011},
note = {Special Issue: Dual-Process Theories of Cognitive Development},
issn = {0273-2297},
doi = {https://doi.org/10.1016/j.dr.2011.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0273229711000177},
author = {Pierre Barrouillet},
keywords = {Dual-process theories, Cognitive development, Conditional reasoning},
abstract = {Dual-process theories have become increasingly influential in the psychology of reasoning. Though the distinction they introduced between intuitive and reflective thinking should have strong developmental implications, the developmental approach has rarely been used to refine or test these theories. In this article, I review several contemporary dual-process accounts of conditional reasoning that theorize the distinction between the two systems of reasoning as a contrast between heuristic and analytic processes, probabilistic and mental model reasoning, or emphasize the role of metacognitive processes in reflective reasoning. These theories are evaluated in the light of the main developmental findings. It is argued that a proper account of developmental phenomena requires the integration of the main strengths of these three approaches. I propose such an integrative theory of conditional understanding and argue that the modern dual-process framework could benefit from earlier contributions that made the same distinction between intuition and reflective thinking, such as Piaget’s theory.}
}
@article{TRAYVICK2024116109,
title = {Speech and language patterns in autism: Towards natural language processing as a research and clinical tool},
journal = {Psychiatry Research},
volume = {340},
pages = {116109},
year = {2024},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2024.116109},
url = {https://www.sciencedirect.com/science/article/pii/S0165178124003949},
author = {Jadyn Trayvick and Sarah B. Barkley and Alessia McGowan and Agrima Srivastava and Arabella W. Peters and Guillermo A. Cecchi and Jennifer H. Foss-Feig and Cheryl M. Corcoran},
keywords = {Autism, Speech, Language, Natural language processing, Automated speech analysis, Acoustics, Computational phenotyping},
abstract = {Speech and language differences have long been described as important characteristics of autism spectrum disorder (ASD). Linguistic abnormalities range from prosodic differences in pitch, intensity, and rate of speech, to language idiosyncrasies and difficulties with pragmatics and reciprocal conversation. Heterogeneity of findings and a reliance on qualitative, subjective ratings, however, limit a full understanding of linguistic phenotypes in autism. This review summarizes evidence of both speech and language differences in ASD. We also describe recent advances in linguistic research, aided by automated methods and software like natural language processing (NLP) and speech analytic software. Such approaches allow for objective, quantitative measurement of speech and language patterns that may be more tractable and unbiased. Future research integrating both speech and language features and capturing “natural language” samples may yield a more comprehensive understanding of language differences in autism, offering potential implications for diagnosis, intervention, and research.}
}
@article{DURSO2015336,
title = {The Threat-Strategy Interview},
journal = {Applied Ergonomics},
volume = {47},
pages = {336-344},
year = {2015},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2014.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0003687014001409},
author = {Francis T. Durso and Sadaf Kazi and Ashley N. Ferguson},
keywords = {Strategies, Knowledge elicitation, Threat and error management},
abstract = {Operators in dynamic work environments use strategies to manage threats in order to achieve task goals. We introduce a structured interview method, the Threat-Strategy Interview (TSI), and an accompanying qualitative analysis to induce operator-level threats, strategies, and the cues that give rise to them. The TSI can be used to elicit knowledge from operators who are on the front line of managing threats to provide an understanding of strategic thinking, which in turn can be applied toward a variety of problems.}
}
@article{RUTTEN20211,
title = {50 Years of Russian Literature: Mapping, Mixing, and Queering Slavic Literary Studies},
journal = {Russian Literature},
volume = {125-126},
pages = {1-8},
year = {2021},
note = {50 Years of Russian Literature & Teffi’s Theatrical & Cinematic Work},
issn = {0304-3479},
doi = {https://doi.org/10.1016/j.ruslit.2021.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0304347921000673},
author = {Ellen Rutten},
keywords = {Russian Literature, Editorial, Transdisciplinarity, Slavic Literary Studies, Transnational Academic Communication},
abstract = {Russian Literature turned fifty this year. In this editorial contribution, editor-in-chief Ellen Rutten reflects on the journal’s past, its current profile, and future editorial plans. As Rutten argues, Russian Literature has three distinguishing features. First, the journal has always generously invited other disciplines on board – and its transdisciplinary inclusivity has increased in recent years – while maintaining a steady gaze on Slavic literary studies. Second, the journal acts as a transnational and transcontinental scholarly contact zone – a status that cannot be isolated from our choice to publish both Anglophone and Russophone analyses. And third, Russian Literature brings together a range of scholarly voices and genres that is unusually broad for a scholarly periodical, through a strategy of active editorial outreach to young talents and leading experts in the field. Rutten concludes with a few words on upcoming volumes and plans, including new archival publications and volumes-in-the-making inspired by recent shifts in thinking about geopolitics, gender, and health and environment.}
}
@article{GUPTA19971,
title = {Future Challenges for Fuzzy-Neural Computing Systems},
journal = {IFAC Proceedings Volumes},
volume = {30},
number = {25},
pages = {1-6},
year = {1997},
note = {IFAC Symposium on Artificial Intelligence in Real Time Control (AIRTC'97), Kuala Lumpur, Malaysia, 22-25 September 1997},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)41292-4},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017412924},
author = {Madan M. Gupta},
keywords = {Neural Systems, Fuzzy Systems, Fuzzy Logic, Neural Fuzzy Computing},
abstract = {Recently, several significant advances have been made in two distinct theoretical areas. These theoretical advances have created an innovative field of theoretical and applied interest: fuzzy neural systems. Researchers have provided a theoretical basis in the field while industry has used this theoretical basis to create a new class of machines using the innovative technology of fuzzy neural networks. The theory of fuzzy logic provides a mathematical framework for capturing the uncertainties associated with human cognitive processes, such as thinking and reasoning. It also provides a mathematical morphology for emulating certain perceptual and linguistic attributes associated with human cognition. On the other hand, computational neural network paradigms have evolved in the process of understanding the incredible learning and adaptive features of neuronal mechanisms inherent in certain biological species. The integration of these two fields, fuzzy logic and neural networks, has the potential for combining the benefits of these two fascinating fields into a single capsule. The intent of this paper is to describe the basic notions of biological and computational neuronal morphologies, and to describe the principles and architectures of fuzzy neural networks.}
}
@incollection{WANDELL2025360,
title = {Visual processing},
editor = {Jordan Henry Grafman},
booktitle = {Encyclopedia of the Human Brain (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {360-381},
year = {2025},
isbn = {978-0-12-820481-8},
doi = {https://doi.org/10.1016/B978-0-12-820480-1.00116-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204801001169},
author = {Brian A. Wandell and Jonathan Winawer},
keywords = {Physiological optics, Retinal circuits, Eye movements, Lateral geniculate nucleus, V1, Visual cortex, Functional specialization, Neural signaling, Visual field maps, Retinotopy, Receptive fields, Sparse representations, Asynchronous representation, Redundancy, Bayesian inference},
abstract = {The human visual system is a network of neural components that combine to create our perception of the world and guide our behavior. Deciphering the computational principles of this system is an important scientific challenge. We review measurements of these components, from the retinal encoding to cortical circuitry, and from molecules to circuits, focusing on measurements that are relevant to visual processing. We then delve into principles proposed to explain how this diverse collection of visual components enables us to interpret our surroundings.}
}
@article{PENG202484,
title = {Multi-perspective thought navigation for source-free entity linking},
journal = {Pattern Recognition Letters},
volume = {178},
pages = {84-90},
year = {2024},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2023.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167865523003677},
author = {Bohua Peng and Wei He and Bin Chen and Aline Villavicencio and Chengfu Wu},
keywords = {Information retrieval, Question generation, Entity linking, Chain-of-thought reasoning},
abstract = {Neural entity-linking models excel at bridging the lexical gap of multiple facets of facts, such as entity-related claims or evidence documents. Despite advancements in self-supervised learning and pretrained language models, challenges persist in entity linking, particularly in interpretability and transferability. Moreover, these models need many aligned documents to adapt to emerging entities, which may not be available due to data scarcity. In this work, we propose a novel Demonstrative Self-TrAining fRamework (D-STAR) that leverages multi-perspective thought navigation. D-STAR iteratively optimizes a question generator and an entity retriever by navigating thoughts on a dynamic graph reasoning across multiple perspectives for question generation. The generated question–answer pairs, along with hard negatives shared in the graph, enable adaptation with minimal computational overhead. Additionally, we introduce a new task, source-free entity linking, focusing on unsupervised transfer learning without direct access to original domain data. To demonstrate the feasibility of this task, we provide a generated question–answering dataset, FandomWiki, for novel entities. Our experiments show that D-STAR significantly improves baselines on SciFact, Zeshel, and FandomWiki.}
}
@article{SUN201859,
title = {An ecosystemic framework for business sustainability},
journal = {Business Horizons},
volume = {61},
number = {1},
pages = {59-72},
year = {2018},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2017.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0007681317301271},
author = {Jiazhe Sun and Shunan Wu and Kaizhong Yang},
keywords = {Business sustainability, Systems theory, Ecosystemic theory, Complexity science, Adaptive management, Corporate sustainability},
abstract = {This article introduces an ecosystemic framework to foster innovation for business sustainability. We emphasize the idea of systemic thinking in which the business operates as a system similar to a living organism. In this framework, businesses impact the environment in which they operate in a fluid, dynamic, and interdependent way. This approach contrasts with the linear approach commonly used in business and other disciplines, which tries to explain what might cause an action or reaction but ignores any feedback effect between the subsequent action and its cause. This article offers practical solutions and guidance for business leaders to incorporate complexity science into creating sustainable businesses.}
}
@article{THOMPSON2011107,
title = {Intuition, reason, and metacognition},
journal = {Cognitive Psychology},
volume = {63},
number = {3},
pages = {107-140},
year = {2011},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2011.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0010028511000454},
author = {Valerie A. Thompson and Jamie A. {Prowse Turner} and Gordon Pennycook},
keywords = {Metacognition, Reasoning, Dual Process Theories, Intuition, Analytic thinking, Retrospective confidence},
abstract = {Dual Process Theories (DPT) of reasoning posit that judgments are mediated by both fast, automatic processes and more deliberate, analytic ones. A critical, but unanswered question concerns the issue of monitoring and control: When do reasoners rely on the first, intuitive output and when do they engage more effortful thinking? We hypothesised that initial, intuitive answers are accompanied by a metacognitive experience, called the Feeling of Rightness (FOR), which can signal when additional analysis is needed. In separate experiments, reasoners completed one of four tasks: conditional reasoning (N=60), a three-term variant of conditional reasoning (N=48), problems used to measure base rate neglect (N=128), or a syllogistic reasoning task (N=64). For each task, participants were instructed to provide an initial, intuitive response to the problem along with an assessment of the rightness of that answer (FOR). They were then allowed as much time as needed to reconsider their initial answer and provide a final answer. In each experiment, we observed a robust relationship between the FOR and two measures of analytic thinking: low FOR was associated with longer rethinking times and an increased probability of answer change. In turn, FOR judgments were consistently predicted by the fluency with which the initial answer was produced, providing a link to the wider literature on metamemory. These data support a model in which a metacognitive judgment about a first, initial model determines the extent of analytic engagement.}
}
@article{JING2020644,
title = {A Learner Model Integrating Cognitive and Metacognitive And Its Application on Scratch Programming Projects},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {5},
pages = {644-649},
year = {2020},
note = {3rd IFAC Workshop on Cyber-Physical & Human Systems CPHS 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.04.154},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321002913},
author = {Sifeng Jing and Ying Tang and Xiwei Liu and Xiaoyan Gong},
keywords = {learner model, cognitive state, metacognitive ability, individualized teaching},
abstract = {learner’s cognitive and metacognitive are key personal profile for individualized teaching. To evaluate learner’s comprehensive characteristics, existing learner model were reviewed. Two challenges of constructing an accurate and comprehensive learner model integrating cognitive and metacognitive were summarized. A plan of constructing a comprehensive learner model was made based on analysis of existing massive online learning environment, sensor information technology and educational data-mining. As a case study, a method of how to map learning data onto learners’ cognitive and metacognitive was proposed based on an analysis of a number of pupils’ Scratch projects. Three mapping table were established. Pupil’s cognitive skill could be evaluated from technology shown from Scratch project, namely, data structure, algorithm, computational practices and overall evaluation. Content shown from Scratch project were used to infer pupil’s cognitive style. Meta-cognitive ability can be measured from computational practices and behavior in programming process.}
}
@article{DAVID2019646,
title = {Development of Escape Room Game using VR Technology},
journal = {Procedia Computer Science},
volume = {157},
pages = {646-652},
year = {2019},
note = {The 4th International Conference on Computer Science and Computational Intelligence (ICCSCI 2019) : Enabling Collaboration to Escalate Impact of Research Results for Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.08.223},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919311421},
author = {David David and  Edwin and Edward Arman and  Hikari and Natalia Chandra and Nadia Nadia},
keywords = {Virtual Reality, Presence, Prototype, Unity, Samsung Gear VR},
abstract = {Escape room is one of the media games that can improve the logic of thinking. Puzzles in the escape room traditionally have disadvantages because the type of puzzle that is made requires a lot of material. The purpose of this research is to produce a game with Escape Room as the basic theme with Virtual Reality technology. Virtual Reality technology is used to develop presence in users, attendance is about the intimacy of users with the gaming world. By using Virtual Reality, the puzzle elements that are created can be replaced regularly without the need to change the building’s skeleton. The development method used is a prototype model using Unity game machines. The research method was carried out using a questionnaire for user analysis. The application generated from this research is the Escape Room VR game that can be played on an Android smartphone that is compatible with Samsung Gear VR. The application can be used as an additional means for traditional Escape Room games.}
}
@article{SCHULTZ2022104766,
title = {Animacy and the prediction of behaviour},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {140},
pages = {104766},
year = {2022},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2022.104766},
url = {https://www.sciencedirect.com/science/article/pii/S014976342200255X},
author = {Johannes Schultz and Chris D. Frith},
keywords = {Animacy, Action prediction, Goal-directed action, Mentalizing, Theory-of-Mind, Intentions, Economic games, Social cognition},
abstract = {To survive, all animals need to predict what other agents are going to do next. We review neural mechanisms involved in the steps required for this ability. The first step is to determine whether an object is an agent, and if so, how sophisticated it is. This involves brain regions carrying representations of animate agents. The movements of the agent can then be anticipated in the short term based solely on physical constraints. In the longer term, taking into account the agent’s goals and intentions is useful. Observing goal directed behaviour activates the neural action observation network, and predicting future goal directed behaviour is helped by the observer’s own action generating mechanisms. Intentions are critically important in determining actions when interacting with other agents, as several intentions can lie behind an action. Here, interpretation is helped by prior beliefs about the agent and the brain’s mentalising system is engaged. Biologically-constrained computational models of action recognition exist, but equivalent models for understanding intentional agents remain to be developed.}
}
@incollection{MONLEZUN2023159,
title = {Chapter 6 - AI+patient safety: adaptive, embedded, intelligent},
editor = {Dominique J. Monlezun},
booktitle = {The Thinking Healthcare System},
publisher = {Academic Press},
pages = {159-182},
year = {2023},
isbn = {978-0-443-18906-7},
doi = {https://doi.org/10.1016/B978-0-443-18906-7.00007-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443189067000076},
author = {Dominique J. Monlezun},
keywords = {Bias, Blockchain, Clinical alarms, Clinical reports, Command center intelligence, Data privacy, Data security, Design thinking, Drug safety, Explainability, Patient safety, Reproducibility},
abstract = {This chapter discusses the AI transformation of patient safety within modern healthcare systems, particularly those digitally extended with telehealth. It carefully details the definitions and debates in patient safety (including the major actors and researchers in this community who share a general critique about the slow to absent sustained, substantive, and equally shared improvements in healthcare systems' safe delivery of patient care). The chapter provides the standard (including WHO) and innovative though still practical conceptualizations of patient safety and then progresses to recent more promising recent advances in human-centered, standardized, and AI-enabled patient safety (including safety as design thinking and system strategy). The chapter illustrates these developments with concrete use cases (in AI-enabled drug safety, clinical reports, and alarms) and augmentation with automation (including embedded, ambient, and command center safety intelligence). The chapter concludes by considering new and growing challenges in AI-driven patient safety (including data security, privacy, bias, and inconsistency) and emerging solutions (including blockchain, bias reduction, reproducibility, explainability, effectiveness, and safety in embedded design).}
}
@article{MARINI201828,
title = {Life cycle perspective in RC building integrated renovation},
journal = {Procedia Structural Integrity},
volume = {11},
pages = {28-35},
year = {2018},
note = {XIV INTERNATIONAL CONFERENCE ON BUILDING PATHOLOGY AND CONSTRUCTIONS REPAIR, FLORENCE, ITALY, JUNE 20-22, 2018},
issn = {2452-3216},
doi = {https://doi.org/10.1016/j.prostr.2018.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S2452321618301069},
author = {A. Marini and C. Passoni and A. Belleri},
keywords = {Life Cycle thinking, Deep renovation, Integrated retrofit, Resilience, Sustainability},
abstract = {Enormous resources are invested in Europe for the transition into a sustainable, low carbon, and resilient society. In the construction sector, these concepts are slowly being applied to the renovation of the existing building stock by enforcing their deep and holistic renovation targeting sustainability, safety and resilience. Effectiveness of such an approach to the renovation with respect to traditional retrofit actions emerges when broadening the time frame of the analyses, shifting from the construction time to a life cycle perspective. In this case, the potential of the holistic approach becomes clear in reducing costs, impacts on the inhabitants and impacts on the environment over the building life cycle. Within such a new perspective, new technology options are needed to innovatively combine structural retrofit, architectural restyling and energy efficiency measures. Furthermore, a new design approach conjugating the principles of sustainability, safety and resilience over the building life cycle is required. In such a transition, synergistic and cooperative work of researchers, design professionals, and all the stakeholders in the construction sector is required. In this paper, the basic features of an expanded Life Cycle Thinking (eLCT) approach will be presented, which not only entails the use of recyclable/reusable materials, but also encourages interventions carried out from the outside the buildings to reduce building downtime and avoid inhabitant relocation. In addition, such an expanded LCT fosters the adoption of reparable, easy maintainable, adaptable and fully demountable solutions, such as those featuring dry, demountable and pre-fabricated components. Finally, it addresses the need to account for the End of Life scenario from the initial design stages to guarantee selective dismantling and reuse or recycle to reduce construction waste. Finally, a discussion on the main barriers and challenges in the transition towards this new approach to the renovation of existing building stock is briefly presented.}
}
@article{ADENIJI2023,
title = {Draft genome sequence of active gold mine isolate Pseudomonas iranensis strain ABS_30},
journal = {Microbiology Resource Announcements},
volume = {12},
number = {12},
year = {2023},
issn = {2576-098X},
doi = {https://doi.org/10.1128/MRA.00849-23},
url = {https://www.sciencedirect.com/science/article/pii/S2576098X23009234},
author = {Adetomiwa A. Adeniji and Ayansina S. Ayangbenro and Olubukola O. Babalola and Julie C. {Dunning Hotopp}},
keywords = {bioremediation, biosynthetic clusters, genome sequence, gold mine, , secondary metabolites},
abstract = {ABSTRACT
Pseudomonas iranensis ABS_30, isolated from gold mining soil, exhibits metal-resistant properties valuable for heavy metal removal. We report the draft genome sequencing of the P. iranensis ABS_30 strain, which is 5.9 Mb in size.}
}
@article{LU2022100056,
title = {Nonlinear EEG signatures of mind wandering during breath focus meditation},
journal = {Current Research in Neurobiology},
volume = {3},
pages = {100056},
year = {2022},
issn = {2665-945X},
doi = {https://doi.org/10.1016/j.crneur.2022.100056},
url = {https://www.sciencedirect.com/science/article/pii/S2665945X22000298},
author = {Yiqing Lu and Julio Rodriguez-Larios},
keywords = {EEG, Mind wandering, Meditation, Complexity, Nonlinear analysis},
abstract = {In meditation practices that involve focused attention to a specific object, novice practitioners often experience moments of distraction (i.e., mind wandering). Previous studies have investigated the neural correlates of mind wandering during meditation practice through Electroencephalography (EEG) using linear metrics (e.g., oscillatory power). However, their results are not fully consistent. Since the brain is known to be a chaotic/nonlinear system, it is possible that linear metrics cannot fully capture complex dynamics present in the EEG signal. In this study, we assess whether nonlinear EEG signatures can be used to characterize mind wandering during breath focus meditation in novice practitioners. For that purpose, we adopted an experience sampling paradigm in which 25 participants were iteratively interrupted during meditation practice to report whether they were focusing on the breath or thinking about something else. We compared the complexity of EEG signals during mind wandering and breath focus states using three different algorithms: Higuchi's fractal dimension (HFD), Lempel-Ziv complexity (LZC), and Sample entropy (SampEn). Our results showed that EEG complexity was generally reduced during mind wandering relative to breath focus states. We conclude that EEG complexity metrics are appropriate to disentangle mind wandering from breath focus states in novice meditation practitioners, and therefore, they could be used in future EEG neurofeedback protocols to facilitate meditation practice.}
}
@article{KRYSSANOV2001329,
title = {Understanding design fundamentals: how synthesis and analysis drive creativity, resulting in emergence},
journal = {Artificial Intelligence in Engineering},
volume = {15},
number = {4},
pages = {329-342},
year = {2001},
note = {Methodology of Emergent Sythesis},
issn = {0954-1810},
doi = {https://doi.org/10.1016/S0954-1810(01)00023-1},
url = {https://www.sciencedirect.com/science/article/pii/S0954181001000231},
author = {V.V Kryssanov and H Tamaki and S Kitamura},
keywords = {Engineering design, Creativity, Semiotics, Emergence},
abstract = {This paper presents results of an ongoing interdisciplinary study to develop a computational theory of creativity for engineering design. Human design activities are surveyed, and popular computer-aided design methodologies are examined. It is argued that semiotics has the potential to merge and unite various design approaches into one fundamental theory that is naturally interpretable and so comprehensible in terms of computer use. Reviewing related work in philosophy, psychology, and cognitive science provides a general and encompassing vision of the creativity phenomenon. Basic notions of algebraic semiotics are given and explained in terms of design. This is to define a model of the design creative process, which is seen as a process of semiosis, where concepts and their attributes represented as signs organized into systems are evolved, blended, and analyzed, resulting in the development of new concepts. The model allows us to formally describe and investigate essential properties of the design process, namely its dynamics and non-determinism inherent in creative thinking. A stable pattern of creative thought — analogical and metaphorical reasoning — is specified to demonstrate the expressive power of the modeling approach; illustrative examples are given. The developed theory is applied to clarify the nature of emergence in design: it is shown that while emergent properties of a product may influence its creative value, emergence can simply be seen as a by-product of the creative process. Concluding remarks summarize the research, point to some unresolved issues, and outline directions for future work.}
}
@incollection{ANGELETOS2023613,
title = {Chapter 20 - Dampening general equilibrium: incomplete information and bounded rationality☆☆This chapter subsumes an older paper of ours, entitled “Dampening General Equilibrium: From Micro to Macro” (Angeletos and Lian, 2017). We have benefited from the comments of various colleagues, especially those of the editors, Rüdiger Bachmann, Wilbert van der Klaauw, and Giorgio Topa. Angeletos acknowledges the support of the National Science Foundation under Grant Number SES-1757198.},
editor = {Rüdiger Bachmann and Giorgio Topa and Wilbert {van der Klaauw}},
booktitle = {Handbook of Economic Expectations},
publisher = {Academic Press},
pages = {613-645},
year = {2023},
isbn = {978-0-12-822927-9},
doi = {https://doi.org/10.1016/B978-0-12-822927-9.00028-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128229279000288},
author = {George-Marios Angeletos and Chen Lian},
keywords = {General equilibrium, incomplete information, higher-order beliefs, Level- Thinking, reflective equilibrium, coordination, beauty contests},
abstract = {We review how realistic frictions in information and/or rationality arrest general equilibrium (GE) feedbacks. In one specification, we maintain rational expectations but remove common knowledge of aggregate shocks. In another, we replace rational expectations with Level-k Thinking or a smooth variant thereof. Two other approaches, heterogeneous priors and cognitive discounting, capture the same essence while offering a gain in tractability. Relative to the full-information rational-expectation (FIRE) benchmark, all these modifications amount to attenuation of GE effects, especially in the short run. This in turn translates to either under- or overreaction in aggregate outcomes, depending on whether GE feedbacks are positive or negative in the first place. We review a few applications, with emphasis on monetary and fiscal policy. We finally discuss how the available evidence on expectations, along with other considerations, can help guide the choice among the various alternatives, as well as between them and FIRE.}
}
@article{BRESSANELLI2024142512,
title = {Are digital servitization-based Circular Economy business models sustainable? A systemic what-if simulation model},
journal = {Journal of Cleaner Production},
volume = {458},
pages = {142512},
year = {2024},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2024.142512},
url = {https://www.sciencedirect.com/science/article/pii/S0959652624019607},
author = {Gianmarco Bressanelli and Nicola Saccani and Marco Perona},
keywords = {Circular economy, Digital servitization, Sustainability impact assessment, Electrical and electronics equipment, Life cycle thinking, Systemic perspective},
abstract = {Manufacturing companies are struggling with the implementation of Circular Economy, especially due to the uncertainty regarding its potential sustainability benefits. In particular, and despite digital servitization is advocated by several studies as a way to achieve environmental gains, circular business models based on digital servitization are not always sustainable due to burden shifting and unexpected consequences which are difficult to assess before implementation. This is particularly relevant for the Electrical and Electronics Equipment industry, which suffers structural weaknesses such as the dependance on critical raw materials and an increasing waste generation. However, literature lacks models and tools able to address the complexity inherent in the systemic micro-macro perspective envisioned by Circular Economy, while studies that quantitatively assess the sustainability impacts and trade-offs of digital servitization-based circular scenarios are limited. This article aims to develop a better understanding of how the sustainability impacts of circular and servitized scenarios can be assessed and quantified at the economic, environmental, and social level, adopting a systemic perspective through the development of a what-if simulation model. The model is implemented in a spreadsheet tool and applied to a digital servitization-based Circular Economy scenario inspired by the case of a company offering long-lasting, high-efficient washing machines as-a-service. Results show that digital servitization can actually lead to a win-win-win situation with net positive effects to the environment, the society, and the economy. This result is based on the joint application of product design for digitalization and life extension, pay-per-use business models, and product reuse. These results are robust within a significant range of key parameters values. Practitioners and policymakers may use the model to support the evaluation of different circular and servitized scenarios before implementation.}
}
@article{THANKACHAN2024101283,
title = {A mathematical formulation of learner cognition for personalised learning experiences},
journal = {Cognitive Systems Research},
volume = {88},
pages = {101283},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101283},
url = {https://www.sciencedirect.com/science/article/pii/S1389041724000779},
author = {Jeena A. Thankachan and Bama Srinivasan},
keywords = {Virtual Learning Environment (VLE), Cognitive Evaluation Metrics (CEM), Multimode evaluation, Cognitive abilities, Learning tasks, Reinforcement learning},
abstract = {The paper focuses on the assessment of cognitive skills within Virtual Learning Environments (VLEs). In response to the global shift to remote learning amid the COVID-19 pandemic, VLEs, which include learning management systems (LMS) and online collaboration platforms, gained prominence. The proposed work leverages an established Cattell–Horn–Carroll (CHC) theory to propose eight metrics, which collectively form a part of Cognitive Evaluation Metrics (CEM). The proposed metrics introduce a novel computational approach for multimode evaluation of learners’ cognitive abilities for each learning task within a learning environment. The paper details the formalism for the evaluation of the metrics and makes a contribution towards the potential of the proposed methodology to evaluate cognitive abilities. Additionally, the work implements CEM integration into the learner module of a Game-Based Learning (GBL) environment. Analysis of simulations in the GBL environment, along with statistical analysis, provides insights into the normal distribution of cognitive metrics. This reveals diverse ranges in various abilities such as long or short term memory, working memory, reasoning, attention, and processing speed. The paper also explores the impact of virtual assistants, which highlights their limited relevance to enhance cognitive abilities but serve as valuable on-demand support resources.}
}
@article{SUN2024103771,
title = {Supply chain planning with free trade zone and uncertain demand},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {192},
pages = {103771},
year = {2024},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2024.103771},
url = {https://www.sciencedirect.com/science/article/pii/S1366554524003624},
author = {Haoying Sun and Manoj Vanajakumari and Chelliah Sriskandarajah and Subodha Kumar},
keywords = {Supply chain management, Robust optimization, Dynamic lot sizing},
abstract = {Our research is inspired by the subcontracting problem at a major oil field services company in North America. The company’s supply chain consists of suppliers bringing raw materials to a Free Trade Zone (FTZ). The FTZ receives raw materials in full containers from various suppliers, and then the company ships them to various plants (e.g. oil excavation sites) frequently via subcontractors. This allows the company to focus on managing only the inbound transportation and inventory at the FTZ. The demand for each raw material is stochastic. We derive an algorithm running at polynomial time for the stochastic programming formulation and perform μ− regret Robust Optimization to handle the demand uncertainty. We also use a Sample Average Approximation method to alleviate the high computational requirement of the robust optimization model. The modeling approach demonstrated by this paper not only meets the needs of this specific company and industry but also can be applied to other industries with similar supply chain structures.}
}
@article{GINOSAR20231858,
title = {Are grid cells used for navigation? On local metrics, subjective spaces, and black holes},
journal = {Neuron},
volume = {111},
number = {12},
pages = {1858-1875},
year = {2023},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2023.03.027},
url = {https://www.sciencedirect.com/science/article/pii/S0896627323002234},
author = {Gily Ginosar and Johnatan Aljadeff and Liora Las and Dori Derdikman and Nachum Ulanovsky},
abstract = {Summary
The symmetric, lattice-like spatial pattern of grid-cell activity is thought to provide a neuronal global metric for space. This view is compatible with grid cells recorded in empty boxes but inconsistent with data from more naturalistic settings. We review evidence arguing against the global-metric notion, including the distortion and disintegration of the grid pattern in complex and three-dimensional environments. We argue that deviations from lattice symmetry are key for understanding grid-cell function. We propose three possible functions for grid cells, which treat real-world grid distortions as a feature rather than a bug. First, grid cells may constitute a local metric for proximal space rather than a global metric for all space. Second, grid cells could form a metric for subjective action-relevant space rather than physical space. Third, distortions may represent salient locations. Finally, we discuss mechanisms that can underlie these functions. These ideas may transform our thinking about grid cells.}
}
@article{ADENIJI2023,
title = {Draft genome sequence of Priestia megaterium AB-S79 strain isolated from active gold mine},
journal = {Microbiology Resource Announcements},
volume = {13},
number = {2},
year = {2023},
issn = {2576-098X},
doi = {https://doi.org/10.1128/mra.01055-23},
url = {https://www.sciencedirect.com/science/article/pii/S2576098X23010629},
author = {Adetomiwa A. Adeniji and Ayansina S. Ayangbenro and Olubukola O. Babalola},
keywords = {bioremediation, biosynthetic traits, genome analysis, , secondary metabolites, genomics},
abstract = {ABSTRACT
We screened and isolated Priestia megaterium strain AB-S79 from active gold mine soil, then sequenced its genome to unravel its biosynthetic traits. The isolate with a 5.7-Mb genome can be utilized as a reference in genome-guided strain selection for metabolic engineering and other biotechnological operations.}
}
@article{LIMONGELLI199289,
title = {Abstract specification of structures and methods in symbolic mathematical computation},
journal = {Theoretical Computer Science},
volume = {104},
number = {1},
pages = {89-107},
year = {1992},
issn = {0304-3975},
doi = {https://doi.org/10.1016/0304-3975(92)90167-E},
url = {https://www.sciencedirect.com/science/article/pii/030439759290167E},
author = {C. Limongelli and M. Temperini},
abstract = {This paper describes a methodology based on the object-oriented programming paradigm, to support the design and implementation of a symbolic computation system. The requirements of the system are related to the specification and treatment of mathematical structures. This treatment is considered from both the numerical and the symbolic points of view. The resulting programming system should be able to support the formal definition of mathematical data structures and methods at their highest level of abstraction, to perform computations on instances created from such definitions, and to handle abstract data structures through the manipulation of their logical properties. Particular consideration is given to the correctness aspects. Some examples of convenient application of the proposed design methodology are presented.}
}
@article{FARHAT199361,
title = {Two-dimensional viscous flow computations on the Connecti on Machine: Unstructured meshes, upwind schemes and massively parallel computations},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {102},
number = {1},
pages = {61-88},
year = {1993},
issn = {0045-7825},
doi = {https://doi.org/10.1016/0045-7825(93)90141-J},
url = {https://www.sciencedirect.com/science/article/pii/004578259390141J},
author = {Charbel Farhat and Loula Fezoui and Stéphane Lanteri},
abstract = {Here we report on our effort in simulating two-dimensional viscous flows on the Connection Machine, using a second-order accurate monotomic upwind scheme for conservation laws (MUSCL) on fully unstructured grids. The spatial approximation combines an upwind finite volume method for the discretization of the convective fluxes with a classical Galerkin finite element method for the discretization of the diffusive fluxes. The resulting semi-discrete equations are time integrated with a second-order low-storage explicit Runge-Kutta method. A communication efficient strategy for mapping thousands of processors onto an arbitrary mesh is presented and proposed as an alternative to the fast north-east-west-south (NEWS) communication mechanism, which is restricted to structured grids. Measured performance results for the simulation of low Reynolds number chaotic flows indicate that an 8K CM-2 (8192 processors) with single precision floating point arithmetic is at least as fast as one CRAY-2 processor.}
}
@incollection{LEE2016135,
title = {Chapter 7 - Identifying and Tracking Emotional and Cognitive Mathematical Processes of Middle School Students in an Online Discussion Group},
editor = {Sharon Y. Tettegah and Michael P. McCreery},
booktitle = {Emotions, Technology, and Learning},
publisher = {Academic Press},
address = {San Diego},
pages = {135-153},
year = {2016},
series = {Emotions and Technology},
isbn = {978-0-12-800649-8},
doi = {https://doi.org/10.1016/B978-0-12-800649-8.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012800649800002X},
author = {Amos Lee and Sharon Tettegah},
keywords = {Online discourse, Math discussions, Math learning, Systemic functional linguistics, Identification analysis},
abstract = {Math discussions are important when learning math. Explaining one’s thinking, listening to other’s thoughts, and reflecting are but a few of the benefits derived from discussions held in class. However, with the growth of online courses, how do math discussions change when in an online setting? While much research exists about math discussions in classrooms, there is not much research on math discussions held online. Due to the important role of discussions in learning math, along with the growing trend of online classes, this study begins to take a look at how students make sense of and keep track of each other’s comments in an online discussion. In these online discussions, turn taking is not as intuitive as face-to-face interactions. Making sense of the discussion sequence and theme can also be challenging. In this study, I found that students used terms that represented mathematical operations to better explain their thought processes and also kept track of how their peers used these terms as well. These findings suggest that, for these students, when in an online discussion, the terms used were of importance when trying to make their thinking clear to their classmates. Also, in these groups, the mathematical terms were commonly used and re-used by more than one individual in trying to gain a consensus in their group thinking. These findings are important when thinking about how to best foster math discussion and learning in an online environment and for designing online classes that institutions use to supplement or support students.}
}
@article{STREVENS202192,
title = {Permissible idealizations for the purpose of prediction},
journal = {Studies in History and Philosophy of Science Part A},
volume = {85},
pages = {92-100},
year = {2021},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2020.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0039368120301813},
author = {Michael Strevens},
keywords = {Prediction, Idealization, Modeling, Difference-making, Causal relevance},
abstract = {Every model leaves out or distorts some factors that are causally connected to its target phenomenon—the phenomenon that it seeks to predict or explain. If we want to make predictions, and we want to base decisions on those predictions, what is it safe to omit or to simplify, and what ought a causal model to describe fully and correctly? A schematic answer: the factors that matter are those that make a difference to the target phenomenon. There are several ways to understand differencemaking. This paper advances a view as to which is the most relevant to the forecaster and the decision-maker. It turns out that the right notion of differencemaking for thinking about idealization in prediction is also the right notion for thinking about idealization in explanation; this suggests a carefully circumscribed version of Hempel’s famous thesis that there is a symmetry between explanation and prediction.}
}
@article{DIETRICH200722,
title = {Who’s afraid of a cognitive neuroscience of creativity?},
journal = {Methods},
volume = {42},
number = {1},
pages = {22-27},
year = {2007},
note = {Neurocognitive Mechanisms of Creativity: A Toolkit},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2006.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S1046202306003100},
author = {Arne Dietrich},
keywords = {Consciousness, Insight, Prefrontal cortex, Right brain, Divergent thinking, Neuroimaging, Attention},
abstract = {This article has two goals. First, the ideas outlined here can be seen as a sustained and disciplined demolition project aimed at sanitizing our bad habits of thinking about creativity. Apart from the enormous amount of fluff out there, the study of creativity is, quite unfortunately, still dominated by a number of rather dated ideas that are either so simplistic that nothing good can possibly come out of them or, given what we know about the brain, factually mistaken. As cognitive neuroscience is making more serious contact with the knowledge base of creativity, we must, from the outset, clear the ground of these pernicious fossil traces from a bygone era. The best neuroimaging techniques help little if we don’t know what to look for. Second, as an antidote to these theoretical duds, the article offers fresh ideas on possible mechanisms of creativity. Given that they are grounded in current understanding of cognitive and neural processes, it is hoped that these ideas represent steps broadly pointing in the right direction. In the end, the fundamental question we must ask ourselves is what, exactly, are the mental processes—or their critical elements—that yield creative thoughts.}
}
@article{RAHARINIRINA2021100332,
title = {Inferring gene regulatory networks from single-cell RNA-seq temporal snapshot data requires higher-order moments},
journal = {Patterns},
volume = {2},
number = {9},
pages = {100332},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100332},
url = {https://www.sciencedirect.com/science/article/pii/S266638992100180X},
author = {N. Alexia Raharinirina and Felix Peppert and Max {von Kleist} and Christof Schütte and Vikram Sunkara},
keywords = {single cell, RNA sequencing, time-course snapshots, Markov chains, chemical master equation, moment equations},
abstract = {Summary
Single-cell RNA sequencing (scRNA-seq) has become ubiquitous in biology. Recently, there has been a push for using scRNA-seq snapshot data to infer the underlying gene regulatory networks (GRNs) steering cellular function. To date, this aspiration remains unrealized due to technical and computational challenges. In this work we focus on the latter, which is under-represented in the literature. We took a systemic approach by subdividing the GRN inference into three fundamental components: data pre-processing, feature extraction, and inference. We observed that the regulatory signature is captured in the statistical moments of scRNA-seq data and requires computationally intensive minimization solvers to extract it. Furthermore, current data pre-processing might not conserve these statistical moments. Although our moment-based approach is a didactic tool for understanding the different compartments of GRN inference, this line of thinking—finding computationally feasible multi-dimensional statistics of data—is imperative for designing GRN inference methods.}
}
@article{CHATRABHUJ2024101045,
title = {Design of an iterative method for environmental-sustainable development: Integrating bioinspired computing techniques},
journal = {Environmental Development},
volume = {51},
pages = {101045},
year = {2024},
issn = {2211-4645},
doi = {https://doi.org/10.1016/j.envdev.2024.101045},
url = {https://www.sciencedirect.com/science/article/pii/S2211464524000836},
author = { Chatrabhuj and Kundan Meshram},
keywords = {Sustainable development, Bioinspired computing, Hybrid algorithms, Agent-based modelling, High-performance computing},
abstract = {The need for sustainable development has grown in response to global environmental, social, and economic challenges. Conventional computational methods frequently struggle to address the complex nature of the Sustainable Development Goals (SDGs), lacking the ability to balance global search with local optimization and failing to prioritize goals related to sustainability. To address these restrictions, this work introduces the Integrated Bioinspired Computing Model for Sustainable Development (IBCMSD). By combining Genetic Algorithms (GAs), Artificial Neural Networks (ANNs), and Ant Colony Optimization (ACO), a cohesive hybrid model is developed that improves exploration and exploitation, balance for increased efficiency, and solution quality. It is implemented on High-Performance Computing (HPC) clusters to ensure scalability and resilience when dealing with complicated optimization challenges. Furthermore, using a multidisciplinary co-design method completes the model with multiple views, increasing its relevance and applicability in real-world circumstances. IBCMSD makes a significant contribution to computational sustainability by leveraging bioinspired computing, potentially enabling informed decision-making and SDG accomplishment across multiple domains.}
}
@article{STEPHEN2021103085,
title = {Automated essay scoring (AES) of constructed responses in nursing examinations: An evaluation},
journal = {Nurse Education in Practice},
volume = {54},
pages = {103085},
year = {2021},
issn = {1471-5953},
doi = {https://doi.org/10.1016/j.nepr.2021.103085},
url = {https://www.sciencedirect.com/science/article/pii/S1471595321001219},
author = {Tracey C. Stephen and Mark C. Gierl and Sharla King},
keywords = {Automated essay scoring, Constructed-response examinations, Nursing education assessment, Reliability measures},
abstract = {Nursing students’ higher-level thinking skills are ideally assessed through constructed-response items. At the baccalaureate level in North America, however, this exam format has largely fallen into disuse owing to the labor-intensive process of scoring written exam papers. The authors sought to determine if automated essay scoring (AES) would be an efficient and reliable alternative to human scoring. Four constructed-response exam items were administered to an initial cohort of 359 undergraduate nursing students in 2016 and to a second cohort of 40 students in 2018. The items were graded by two human raters (HR1 & HR2) and an AES software platform. AES approximated or surpassed agreement and reliability measures achieved by the HR1 and HR2 with each other, and AES surpassed both human raters in efficiency. A list of answer keywords was created to increase the efficiency and reliability of AES. Low agreement between human raters may be explained by rater drift and fatigue, and shortcomings in the development of Item 1 may have reduced its overall agreement and reliability measures. It can be concluded that AES is a reliable and cost-effective means of scoring constructed-response nursing examinations, but further studies employing greater sample sizes are needed to establish this definitively.}
}
@incollection{SUGHRUE2024151,
title = {Chapter 6 - Reimagining neurocognitive functions as emergent phenomena: What resting state is really showing us},
editor = {Michael E. Sughrue and Jacky T. Yeung and Nicholas B. Dadario},
booktitle = {Connectomic Medicine},
publisher = {Academic Press},
pages = {151-157},
year = {2024},
isbn = {978-0-443-19089-6},
doi = {https://doi.org/10.1016/B978-0-443-19089-6.00008-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443190896000082},
author = {Michael E. Sughrue and Jacky T. Yeung and Nicholas B. Dadario},
keywords = {Brain hub, Brain landscape, Network control theory, Neurocognitive function, Resting-state fMRI, Structural connectome},
abstract = {In this chapter, we introduce a new way of thinking about neurocognitive functioning and related dysfunction. We discuss how structural wiring patterns, global rhythms in deep structures, and electrochemical gain from neurotransmitters play a key role in the internal dynamics of what the brain is doing. Importantly, together, these elements dictate how the brain can or cannot obtain different brain states. Simultaneously, disruption in intrinsic structures and internal dynamics alters the energetic landscape causing some brain states to become more favorable or less favorable. Importantly, we go on to describe how landscapes arise from structural connectomes, and how these connections can dictate spontaneous behavioral patterns and tendencies in normal as well as pathologic states, such as a depressed patient being stuck in a self-ruminating and negative state. Resting-state fMRI also provides a keyhole into these processes as the entire set of the structural connectome creates the patterns of functional connectivity seen in resting-state brain activity.}
}
@incollection{FROEMER2025234,
title = {Belief updates, learning and adaptive decision making},
editor = {Jordan Henry Grafman},
booktitle = {Encyclopedia of the Human Brain (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {234-251},
year = {2025},
isbn = {978-0-12-820481-8},
doi = {https://doi.org/10.1016/B978-0-12-820480-1.00059-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204801000590},
author = {Romy Froemer and Matthew R. Nassar},
keywords = {Reinforcement learning, Reward, Value, Action, Dopamine, Belief updating, Sequential sampling, Attention, Confidence, Context, Experience, Goal-directed behavior, Cost-benefit decision-making},
abstract = {People make decisions every day and the outcomes of those decisions often lead them to change their beliefs and in some cases shape their future behavior. How does the brain decide which meal to order at a restaurant, and how does it learn from the experience of eating that meal? Here we review work from neuroscience, psychology and economics that shapes our understanding of how the brain makes decisions and learns through experience. We focus on computational mechanisms that can explain core phenomena in learning and decision making as well as how such mechanisms are implemented in the brain. Our review highlights both the considerable progress made in the last decades elucidating mechanisms of learning and decision making as well as the vast territory of open questions that remain to be answered.}
}
@article{LEUNG2020345,
title = {Limited cognitive ability and selective information processing},
journal = {Games and Economic Behavior},
volume = {120},
pages = {345-369},
year = {2020},
issn = {0899-8256},
doi = {https://doi.org/10.1016/j.geb.2020.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0899825620300063},
author = {Benson Tsz Kin Leung},
keywords = {Limited ability, Information overload, Information avoidance, Confirmation bias, Wishful thinking, Polarization},
abstract = {This paper studies the information processing behavior of a decision maker (DM) who can only process a subset of all information he receives: before taking an action, the DM receives sequentially a number of signals and decides whether to process or ignore each of them as it is received. The model generates an information processing behavior consistent with that documented in the psychological literature: first, the DM chooses to process signals that are strong; second, his processing strategy exhibits confirmation bias if he has a strong prior belief; third, he tends to process signals that suggest favorable outcomes (wishful thinking). As an application I analyze how the Internet and the induced change in information availability affects the processing behavior of the DM. I show that providing more/better information to the DM could strengthen his confirming bias.}
}
@article{ORJI2022100626,
title = {Assessing the pre-conditions for the pedagogical use of digital tools in the Nigerian higher education sector},
journal = {The International Journal of Management Education},
volume = {20},
number = {2},
pages = {100626},
year = {2022},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2022.100626},
url = {https://www.sciencedirect.com/science/article/pii/S1472811722000283},
author = {Ifeyinwa Juliet Orji and Frank Ojadi and Ukoha Kalu Okwara},
keywords = {Digitalization, Higher education, TOE theory, Social media, Learning outcomes, Nigeria},
abstract = {Currently, there is a burgeoning interest in digitalization as evidenced in extant literature. Nevertheless, the effect, based on teachers’ own perspectives, of the pedagogical use of digital technologies on learning outcomes in the higher education sector has been under-investigated. Thus, this paper aims to investigate the pre-conditions for the effective adoption of social media tools in the Nigerian higher education sector and to assess the impact of the adoption on specific learning outcomes. A multi-criteria decision-making (MCDM) methodology was proposed for study analysis, aided by views of experts with sufficient teaching experience in Nigerian business school programs. The results indicate that adequate budgetary allocations, technical competence, a sufficient level of privacy, and an effective government regulatory framework are the most important of the investigated pre-conditions. Additionally, the pedagogical use of social media in business school programs is more strongly associated with learning outcomes such as professionalism and strategic thinking, emotional intelligence, and social maturity. Hence, the article offers guidance to decision-makers in the higher education sector on how to actualize the successful adoption of social media for pedagogical use and build effective business strategies at various levels of the digitalization process.}
}
@article{SHUBBAR2024382,
title = {Bridging Qatar's food demand and self-sufficiency: A system dynamics simulation of the energy–water–food nexus},
journal = {Sustainable Production and Consumption},
volume = {46},
pages = {382-399},
year = {2024},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2024.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S2352550924000423},
author = {Haya Talib Shubbar and Furqan Tahir and Tareq Al-Ansari},
keywords = {Carbon emissions, Energy-water-food nexus, Food self-sufficiency, Food security, Qatar, System dynamics},
abstract = {The food sector in Qatar is confronted with formidable challenges due to its harsh environmental conditions. Striving for total food self-sufficiency in such an environment would inevitably exert pressure on the energy and water sectors. This heightened demand for energy and water translates into increased costs and escalates environmental impacts. Consequently, this study embarks on an in-depth analysis of food production within the context of Qatar's energy-water-food nexus, aiming to demonstrate how varying degrees of food self-sufficiency may impact the demand on Qatar's water and energy sectors, as well as on greenhouse gas (GHG) emissions. Moreover, this study demonstrates to what extent specific subsystems within the nexus can be modified to enhance sustainability. An energy-water-food nexus is meticulously crafted within the proposed framework to elucidate the intricate interdependencies among these sectors, incorporating pertinent external variables. These interconnections are then transmuted into a system dynamics model (SDM), facilitating a nuanced exploration of potential transformations and their ripple effects. Furthermore, a life-cycle thinking approach explicitly tailored to Qatar was implemented to estimate GHG emissions accurately. Four distinct scenarios are rigorously examined using the SDM, spanning from a status quo perspective to ambitious transitions toward full food self-sufficiency. The findings of the scenarios indicate that scenario 4, which partially provides the country with its food demands locally using desalinated water, treated wastewater, and groundwater and satisfies 20 % of its energy demand from solar energy, is the most ideal with an annual 5.36 × 1010 kWh/year energy consumption, 1.73 × 1012 l/year water demand, and 3.26 × 1010 kg CO2 eq./year emissions. The outcomes underscore the imperative for prioritizing less energy-intensive resources to mitigate overall energy consumption. Additionally, achieving an optimal national scenario necessitates a judicious equilibrium between food imports and domestic production.}
}
@article{CHANG20114075,
title = {Dynamic multi-criteria evaluation of co-evolution strategies for solving stock trading problems},
journal = {Applied Mathematics and Computation},
volume = {218},
number = {8},
pages = {4075-4089},
year = {2011},
issn = {0096-3003},
doi = {https://doi.org/10.1016/j.amc.2011.09.032},
url = {https://www.sciencedirect.com/science/article/pii/S0096300311012033},
author = {Ying-Hua Chang and Tz-Ting Wu},
keywords = {Co-evolutionary model, Evolution strategies, Artificial neural network, Dynamic stock trading decision making, Optimization},
abstract = {Risk and return are interdependent in a stock portfolio. To achieve the anticipated return, comparative risk should be considered simultaneously. However, complex investment environments and dynamic change in decision making criteria complicate forecasts of risk and return for various investment objects. Additionally, investors often fail to maximize their profits because of improper capital allocation. Although stock investment involves multi-criteria decision making (MCDM), traditional MCDM theory has two shortfalls: first, it is inappropriate for decisions that evolve with a changing environment; second, weight assignments for various criteria are often oversimplified and inconsistent with actual human thinking processes. In 1965, Rechenberg proposed evolution strategies for solving optimization problems involving real number parameters and addressed several flaws in traditional algorithms, such as their use of point search only and their high probability of falling into optimal solution area. In 1992, Hillis introduced the co-evolutionary concept that the evolution of living creatures is interactive with their environments (multi-criteria) and constantly improves the survivability of their genes, which then expedites evolutionary computation. Therefore, this research aimed to solve multi-criteria decision making problems of stock trading investment by integrating evolutionary strategies into the co-evolutionary criteria evaluation model. Since co-evolution strategies are self-calibrating, criteria evaluation can be based on changes in time and environment. Such changes not only correspond with human decision making patterns (i.e., evaluation of dynamic changes in criteria), but also address the weaknesses of multi-criteria decision making (i.e., simplified assignment of weights for various criteria). Co-evolutionary evolution strategies can identify the optimal capital portfolio and can help investors maximize their returns by optimizing the preoperational allocation of limited capital. This experimental study compared general evolution strategies with artificial neural forecast model, and found that co-evolutionary evolution strategies outperform general evolution strategies and substantially outperform artificial neural forecast models. The co-evolutionary criteria evaluation model avoids the problem of oversimplified adaptive functions adopted by general algorithms and the problem of favoring weights but failing to adaptively adjust to environmental change, which is a major limitation of traditional multi-criteria decision making. Doing so allows adaptation of various criteria in response to changes in various capital allocation chromosomes. Capital allocation chromosomes in the proposed model also adapt to various criteria and evolve in ways that resemble thinking patterns.}
}
@article{LEONIDOV2022112279,
title = {Strategic stiffening/cooling in the Ising game},
journal = {Chaos, Solitons & Fractals},
volume = {160},
pages = {112279},
year = {2022},
issn = {0960-0779},
doi = {https://doi.org/10.1016/j.chaos.2022.112279},
url = {https://www.sciencedirect.com/science/article/pii/S0960077922004891},
author = {Andrey Leonidov and Ekaterina Vasilyeva},
keywords = {Binary choice game, Ising game, Graph, Forward-looking, Myopic, Noise},
abstract = {The dynamic noisy binary choice (Ising) game of forward-looking agents on a complete graph is analysed. It is shown that strategic considerations lead to effective interaction strengthening (noise reduction) as compared to the myopic game. We show that strategic agents are able to come to consensus in the wider range of noise values than myopic ones. Effective population dynamics with time-dependent probabilities reflecting this strategic stiffening/cooling effect is described.}
}
@article{AWD2023107403,
title = {A review on the enhancement of failure mechanisms modeling in additively manufactured structures by machine learning},
journal = {Engineering Failure Analysis},
volume = {151},
pages = {107403},
year = {2023},
issn = {1350-6307},
doi = {https://doi.org/10.1016/j.engfailanal.2023.107403},
url = {https://www.sciencedirect.com/science/article/pii/S1350630723003576},
author = {Mustafa Awd and Lobna Saeed and Frank Walther},
keywords = {Modeling, Simulation, Finite Element Analysis, Analytical Models, Multi-Physics, Data-Driven Modeling, Additive Manufacturing, Failure Mechanisms},
abstract = {This review discusses the feasibility of using microstructure- and defect-sensitive models to predict the fatigue behavior of additively generated materials through a non-exclusive qualitative assessment of the current literature on the structural integrity of additively manufactured structures. The time it takes to implement additively manufactured structures is reduced if a computational model can predict and enhance their mechanical performance. Computational modeling techniques can express nonlinear, multimodal functions in failure analysis of engineering materials, reducing environmental waste and providing sustainable technology. Machine learning is used in manufacturing and industrial sectors to optimize process parameters and model data-driven correlations between processes, structures, and properties. Machine learning and artificial intelligence can be combined to enable atomistic-scale damage tolerance design, which can be customized using computer programming. The main advantage of machine learning is that it can be well integrated with finite element, analytical, or empirical modeling with a significant increase in the yield of the model being used.}
}
@article{PACINI200969,
title = {Synergy: A Framework for Leadership Development and Transformation},
journal = {Perioperative Nursing Clinics},
volume = {4},
number = {1},
pages = {69-74},
year = {2009},
note = {Leadership},
issn = {1556-7931},
doi = {https://doi.org/10.1016/j.cpen.2008.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S1556793108001022},
author = {Christine M. Pacini},
keywords = {Synergy, Leadership development, Orientation, Professional development, Staff development, Clinical education},
abstract = {Given the current demands of the health care environment, the need for nurses minimally competent in clinical judgment, caring practice, advocacy and moral agency, collaboration, responsiveness to diversity, systems thinking, inquiry, and facilitation of learning is critical in light of ever-increasing contextual complexity and variability of patient needs. The Synergy Model provides an exemplary and relevant framework for clinical practice with the ultimate aim of improving patient outcomes. Tenets of accountability and professionalism are central to the model and, in its entirety, it provides a practical and useful approach for thinking about and redesigning educational products and processes in clinical settings.}
}