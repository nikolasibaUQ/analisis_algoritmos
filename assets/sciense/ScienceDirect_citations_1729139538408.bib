@article{GOMES2019411,
title = {State-of-the-art of transmission expansion planning: A survey from restructuring to renewable and distributed electricity markets},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {111},
pages = {411-424},
year = {2019},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2019.04.035},
url = {https://www.sciencedirect.com/science/article/pii/S014206151831888X},
author = {Phillipe Vilaça Gomes and João Tomé Saraiva},
keywords = {Heuristics, Optimization, Mathematical programming, Metaheuristic, Transmission expansion planning},
abstract = {Transmission Expansion Planning (TEP) problem aims at identifying when and where new equipment as transmission lines, cables and transformers should be inserted on the grid. The transmission upgrade capacity is motivated by several factors as meeting the increasing electricity demand, increasing the reliability of the system and providing non-discriminatory access to cheap generation for consumers. However, TEP problems have been changing over the years as the electrical system evolves. In this way, this paper provides a detailed historical analysis of the evolution of the TEP over the years and the prospects for this challenging task. Furthermore, this study presents an outline review of more than 140 recent articles about TEP problems, literature insights and identified gaps as a critical thinking in how new tools and approaches on TEP can contribute for the new era of renewable and distributed electricity markets.}
}
@article{SHAN201032,
title = {Study on large time-delay constant temperature control system based on TEC},
journal = {The Journal of China Universities of Posts and Telecommunications},
volume = {17},
pages = {32-35},
year = {2010},
issn = {1005-8885},
doi = {https://doi.org/10.1016/S1005-8885(09)60586-0},
url = {https://www.sciencedirect.com/science/article/pii/S1005888509605860},
author = {Jiang-dong SHAN and Ge WU and Xiao-jian TIAN},
keywords = {thermoelectric cooler, large time-delay control system, proportion integration differentiation (PID) control, constant temperature control},
abstract = {This paper designes a diminutive constant temperature control system based on thermoelectric cooler (TEC). Considering that the system is a large time-delay control system, the paper proposes a new method to determine the transfer function of the controlled system which gets the transfer function by doing nonlinear fitting of the step response of the controlled system. The characteristics of system model which is established by the method are basically same as the actual constant temperature control system. This method provides a new way of thinking to the design of large time-delay control system.}
}
@article{ELMAIZI2019126,
title = {A novel information gain based approach for classification and dimensionality reduction of hyperspectral images},
journal = {Procedia Computer Science},
volume = {148},
pages = {126-134},
year = {2019},
note = {THE SECOND INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S187705091930016X},
author = {Asma Elmaizi and Hasna Nhaila and Elkebir Sarhrouni and Ahmed Hammouch and Chafik Nacir},
keywords = {Hyperspectral images, dimentionality reduction, information gain, classification accuracy},
abstract = {Recently, the hyperspectral sensors has improved our ability to monitor the earth surface with high spectral resolution. However, the high dimensionality of spectral data brings challenges for the image processing. Consequently, the dimensionality reduction is a necessary step in order to reduce the computational complexity and increase the classification accuracy. In this paper, we propose a new filter approach based on information gain for dimensionality reduction and classification of hyperspectral images. A special strategy based on hyperspectral bands selection is adopted to pick the most informative bands and discard the irrelevant and noisy ones. The algorithm evaluates the relevancy of the bands based on the information gain function with the support vector machine classifier. The proposed method is compared using two benchmark hyperspectral datasets (Indiana, Pavia) with three competing methods. The comparison results showed that the information gain filter approach outperforms the other methods on the tested datasets and could significantly reduce the computation cost while improving the classification accuracy.}
}
@incollection{ESCHE2014699,
title = {Systematic Modeling for Optimization},
editor = {Mario R. Eden and John D. Siirola and Gavin P. Towler},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {34},
pages = {699-704},
year = {2014},
booktitle = {Proceedings of the 8th International Conference on Foundations of Computer-Aided Process Design},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-63433-7.50101-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780444634337501012},
author = {Erik Esche and David Müller and Günter Wozny},
keywords = {Multiple-Scale Modeling, Optimization, Convexification, Linearization},
abstract = {Optimization usually requires models, which are computationally speaking less expensive than models commonly used for simulations. At the same time, process optimization and model predictive control etc. require dependable accuracies in addition to the fastness. To demystify the art of preparing process models for optimization, a workflow is presented in this contribution, which systematically deduces models based on simplification of existing models and experiment based deduction of computationally inexpensive correlations.}
}
@article{STEPHAN2024111077,
title = {EPiC grasshopper: A bottom-up parametric tool to quantify life cycle embodied environmental flows of buildings and infrastructure assets},
journal = {Building and Environment},
volume = {248},
pages = {111077},
year = {2024},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2023.111077},
url = {https://www.sciencedirect.com/science/article/pii/S0360132323011046},
author = {André Stephan and Fabian Prideaux and Robert H. Crawford},
keywords = {Hybrid life cycle assessment, Embodied energy, Embodied carbon, Grasshopper, Buildings, Infrastructure, LCA, Python, Rhinoceros, Design},
abstract = {Reducing the embodied environmental flows of built assets is becoming increasingly important and is a key priority for actors in the built environment to improve life cycle environmental performance. Policies and related targets for embodied environmental flow reductions are emerging. Despite this, tools for quantifying the life cycle embodied environmental flows of built assets are limited in variety and scope. Parametric life cycle assessment (LCA) tools have emerged to address some of these limitations. These tools can enhance decision making, be embedded directly into CAD programs, and offer real-time LCA calculations across multiple design variations. Yet, existing parametric tools for LCA rely on process-based material environmental flow data, limited geometries, limited real-time data visualisation capacity, and often require specialised technical expertise to use. These gaps limit their ability to provide transparent, robust, and rapid assessments. This paper introduces EPiC Grasshopper, an open-source, open-access, bottom-up, parametric tool that enables the quantification of life cycle embodied environmental flows at the early stages of built asset design, bridging the aforementioned gaps. The key characteristics and functionalities of the tool are described, followed by verification (checking that calculations are correct), validation (checking that results are representative of reality), and demonstration of its application to two built asset case studies, i.e. parametrically-defined Australian house and road. The paper shows how the tool can be used to generate designs to meet specific embodied environmental flow targets as well as streamline and increase the uptake of embodied environmental flow assessment and considerations in built asset design workflows.}
}
@article{MILLI2021104881,
title = {A rational reinterpretation of dual-process theories},
journal = {Cognition},
volume = {217},
pages = {104881},
year = {2021},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2021.104881},
url = {https://www.sciencedirect.com/science/article/pii/S0010027721003024},
author = {Smitha Milli and Falk Lieder and Thomas L. Griffiths},
keywords = {Bounded rationality, Dual-process theories, Meta-decision making, Bounded optimality, Metareasoning, Resource-rationality},
abstract = {Highly influential “dual-process” accounts of human cognition postulate the coexistence of a slow accurate system with a fast error-prone system. But why would there be just two systems rather than, say, one or 93? Here, we argue that a dual-process architecture might reflect a rational tradeoff between the cognitive flexibility afforded by multiple systems and the time and effort required to choose between them. We investigate what the optimal set and number of cognitive systems would be depending on the structure of the environment. We find that the optimal number of systems depends on the variability of the environment and the difficulty of deciding when which system should be used. Furthermore, we find that there is a plausible range of conditions under which it is optimal to be equipped with a fast system that performs no deliberation (“System 1”) and a slow system that achieves a higher expected accuracy through deliberation (“System 2”). Our findings thereby suggest a rational reinterpretation of dual-process theories.}
}
@article{MAO2022109671,
title = {A decision support engine for infill drilling attractiveness evaluation using rule-based cognitive computing under expert uncertainties},
journal = {Journal of Petroleum Science and Engineering},
volume = {208},
pages = {109671},
year = {2022},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2021.109671},
url = {https://www.sciencedirect.com/science/article/pii/S0920410521013000},
author = {Qiangqiang Mao and Xiaohua Ma and Yuhe Wang},
keywords = {Cognitive computing, Fuzzy inference, Infill well placement, Drilling attractiveness evaluation, Expert uncertainties quantification},
abstract = {Optimally drilling new wells in a developed reservoir is an essential strategy to potentially tap remaining oil for a complete life circle of oilfield development. Further, the determination of optimal infill drilling targets is a challenging issue which involves the integration of data, experts' knowledge and human decisions. The decision process can be essentially regarded as a systematic evaluation of drilling attractiveness. To automate drilling attractiveness evaluation, we develop a decision support engine using rule-based cognitive computing to rank and recommend drilling candidates. Such drilling candidates are chosen by the quantification of regional drilling attractiveness. Then we use two cases with different settings to show its general applicability and human-like reasoning abilities. The reasoning process considers expertise and human-involved uncertainties. The expertise is characterized by certain representation of fuzzy rules sets. Our results highlight the potential of our recommendation engine in pinpointing the most productive drilling location. And our method avoids the expensive reservoir simulation runs. Moreover, fuzzy drilling attractiveness evaluation can serve as an alternative initialization method of model-based infill well optimization, which avoids local optimum problem and greatly saves iteration time. Our approach extends human's reasoning capability and accelerates human's decision-making process with very low computational cost.}
}
@incollection{HASIJA2023247,
title = {Chapter 11 - Bioinformatics workflow management systems},
editor = {Yasha Hasija},
booktitle = {All About Bioinformatics},
publisher = {Academic Press},
pages = {247-265},
year = {2023},
isbn = {978-0-443-15250-4},
doi = {https://doi.org/10.1016/B978-0-443-15250-4.00006-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044315250400006X},
author = {Yasha Hasija},
keywords = {Galaxy, GenePattern, Image analysis, KNIME, LINCS tools, NextFlow},
abstract = {In the discipline of bioinformatics, a flow of work, or a sequence of computational or analytical tasks, is managed by a bioinformatics workflow management system, which is a subtype of a workflow automation system. This type of system is used to construct and manage the flow of work. There are numerous different work process situations available at this time. Some of them have been developed with the intention that scholars in a variety of subjects, such as cosmology and geology, will be able to make use of them as frameworks for logical work processes. Workflow frameworks such as Galaxy, GenePattern, KNIME, LINCS Tools, image analysis, and NextFlow are discussed in this chapter.}
}
@article{GERO2001283,
title = {The differences between retrospective and concurrent protocols in revealing the process-oriented aspects of the design process},
journal = {Design Studies},
volume = {22},
number = {3},
pages = {283-295},
year = {2001},
issn = {0142-694X},
doi = {https://doi.org/10.1016/S0142-694X(00)00030-2},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X00000302},
author = {John S. Gero and Hsien-Hui Tang},
keywords = {design cognition, protocol studies, case study},
abstract = {This paper presents the results of studying a single designer using protocol analyses and examines the implications of the results on studies of design thinking. It contrasts two types of protocols: concurrent protocols and retrospective protocols. The results indicate that concurrent and retrospective protocols both produce very similar outcomes in terms of exploring the process-oriented aspects of designing. As a result, it is argued there is no associated interference with the ongoing design process when using concurrent protocols.}
}
@incollection{ZELINSKY2005395,
title = {CHAPTER 65 - Specifying the Components of Attention in a Visual Search Task},
editor = {Laurent Itti and Geraint Rees and John K. Tsotsos},
booktitle = {Neurobiology of Attention},
publisher = {Academic Press},
address = {Burlington},
pages = {395-400},
year = {2005},
isbn = {978-0-12-375731-9},
doi = {https://doi.org/10.1016/B978-012375731-9/50069-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780123757319500690},
author = {Gregory J. Zelinsky},
abstract = {ABSTRACT
Although commonly treated as a unitary process, attention is more likely a collection of task-related but separable operations. Three components of attention (set, selection, and movement) are identified and defined within the context of a computationally explicit model of eye movements during visual search. The model compares filter-based representations of the target and search displays to derive a salience map indicating likely target candidates in a scene. Eye position is defined as the centroid of activity on this saliency map. As this map is thresholded over time, the changing centroid produces a sequence of movements that eventually cause simulated gaze to become aligned with the target. By adopting a more computational language and making explicit the underlying operations of the task, visual search, a behavior that has long been hobbled to the concept of attention, can be well described without appeal to an abstracted attention theory.}
}
@article{ROBINSON2009310,
title = {Children's understanding of the inverse relation between multiplication and division},
journal = {Cognitive Development},
volume = {24},
number = {3},
pages = {310-321},
year = {2009},
issn = {0885-2014},
doi = {https://doi.org/10.1016/j.cogdev.2008.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0885201408000889},
author = {Katherine M. Robinson and Adam K. Dubé},
keywords = {Arithmetic, Inversion, Conceptual knowledge, Procedural knowledge, Factual knowledge, Multiplication, Division},
abstract = {Children's understanding of the inversion concept in multiplication and division problems (i.e., that on problems of the form d * e/e no calculations are required) was investigated. Children in Grades 6, 7, and 8 completed an inversion problem-solving task, an assessment of procedures task, and a factual knowledge task of simple multiplication and division. Application of the inversion concept in the problem-solving task was low and constant across grades. Most participants approved of the inversion-based shortcut but only a slight majority preferred it. Three clusters of children were identified based on their performance on the three tasks. The inversion cluster used and approved of the inversion shortcut the most and had high factual knowledge. The negation cluster used the negation strategy, had lower approval of the inversion shortcut, and had medium factual knowledge. The computation cluster used computation and had the lowest approval and the weakest factual knowledge. The findings highlight the importance of addressing the multiplication and division inversion concept in theories of children's mathematical competence.}
}
@article{BORIS2013113,
title = {Flux-Corrected Transport looks at forty},
journal = {Computers & Fluids},
volume = {84},
pages = {113-126},
year = {2013},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2013.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0045793013001874},
author = {Jay Paul Boris},
keywords = {Flux-Corrected Transport (FCT), Monotonicity, Positivity, Computational Fluid Dynamics (CFDs), Large Eddy Simulation (LES), Monotone Integrated Large Eddy Simulation (MILES), Implicit Large Eddy Simulation (ILES)},
abstract = {This year, 2013, marks the 40th anniversary of the journal article “Flux-Corrected Transport I. SHASTA, A Fluid Transport Algorithm That Works” by Jay Boris and David Book [1]. Flux-Corrected Transport (FCT) removed a serious roadblock to advances in Computational Fluid Dynamics (CFD) by enabling the accurate treatment of strong, time-dependent shock problems in blast, reactive-flow, and combustion physics, and in aerodynamics and astrophysics. Steep gradients in conserved fluid variables could now be convected across a computational grid without the appearance of spurious oscillations and physically impossible negative values. The nonlinear “flux-correction” algorithm introduced in FCT imposes the physical properties of conservation, locality, causality, and monotonicity on the numerical solutions for convection without adding a great deal of numerical diffusion. This article shows that implementing these physical properties in solving the continuity equation through high-resolution FCT also results in a serviceable Large-Eddy Simulation treatment of turbulent flows without need for additional “subgrid turbulence models.” We have named this simplified approach Monotone Integrated Large Eddy Simulation (MILES).}
}
@article{KARUNATHILAKE2019558,
title = {Renewable energy selection for net-zero energy communities: Life cycle based decision making under uncertainty},
journal = {Renewable Energy},
volume = {130},
pages = {558-573},
year = {2019},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2018.06.086},
url = {https://www.sciencedirect.com/science/article/pii/S0960148118307389},
author = {Hirushie Karunathilake and Kasun Hewage and Walter Mérida and Rehan Sadiq},
keywords = {Multi-criteria decision making, Life cycle thinking, Fuzzy techniques, Renewable energy, Community energy system planning},
abstract = {Developing net-zero energy communities powered by renewable energy (RE) resources has become a popular concept. To make the best choices for community-level net-zero energy systems, it is necessary to identify the best energy technologies at local level. Evaluation of RE technologies has to be extended from technical and economic aspects to include environmental and social wellbeing. It is possible to identify the true costs and benefits of energy use by taking a cradle-to-grave life cycle perspective. In this study, a RE screening and multi-stage energy selection framework was developed. A fuzzy multi-criteria decision making approach was used in ranking the technologies to incorporate the conflicting requirements, stakeholder priorities, and uncertainties. Different scenarios were investigated to reflect different decision maker priorities. Under a pro-environment scenario, small hydro, onshore wind, and biomass combustion technologies perform best. Under a pro-economic decision scenario, biomass combustion, small hydro, and landfill gas have the best rankings. Triple bottom line sustainability was combined with technical feasibility through a ruled-based approach to avoid the theoretical pitfalls inherent in energy-related decision making. This assessment is geared towards providing decision makers with flexible tools, and is expected to aid in the pre-project planning stage of RE projects.}
}
@article{CERKA2015376,
title = {Liability for damages caused by artificial intelligence},
journal = {Computer Law & Security Review},
volume = {31},
number = {3},
pages = {376-389},
year = {2015},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2015.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S026736491500062X},
author = {Paulius Čerka and Jurgita Grigienė and Gintarė Sirbikytė},
keywords = {Artificial intelligence, Liability for damages, Legal regulation, AI-as-Tool, Risks by AI, Respondeat (respondent) superior, Vicarious liability, Strict liability},
abstract = {The emerging discipline of Artificial Intelligence (AI) has changed attitudes towards the intellect, which was long considered to be a feature exclusively belonging to biological beings, i.e. homo sapiens. In 1956, when the concept of Artificial Intelligence emerged, discussions began about whether the intellect may be more than an inherent feature of a biological being, i.e. whether it can be artificially created. AI can be defined on the basis of the factor of a thinking human being and in terms of a rational behavior: (i) systems that think and act like a human being; (ii) systems that think and act rationally. These factors demonstrate that AI is different from conventional computer algorithms. These are systems that are able to train themselves (store their personal experience). This unique feature enables AI to act differently in the same situations, depending on the actions previously performed. The ability to accumulate experience and learn from it, as well as the ability to act independently and make individual decisions, creates preconditions for damage. Factors leading to the occurrence of damage identified in the article confirm that the operation of AI is based on the pursuit of goals. This means that with its actions AI may cause damage for one reason or another; and thus issues of compensation will have to be addressed in accordance with the existing legal provisions. The main issue is that neither national nor international law recognizes AI as a subject of law, which means that AI cannot be held personally liable for the damage it causes. In view of the foregoing, a question naturally arises: who is responsible for the damage caused by the actions of Artificial Intelligence? In the absence of direct legal regulation of AI, we can apply article 12 of United Nations Convention on the Use of Electronic Communications in International Contracts, which states that a person (whether a natural person or a legal entity) on whose behalf a computer was programmed should ultimately be responsible for any message generated by the machine. Such an interpretation complies with a general rule that the principal of a tool is responsible for the results obtained by the use of that tool since the tool has no independent volition of its own. So the concept of AI-as-Tool arises in the context of AI liability issues, which means that in some cases vicarious and strict liability is applicable for AI actions.}
}
@incollection{MURRAY202119,
title = {Chapter Two - The neurocognitive mechanisms of responsibility: A framework for normatively relevant neuroscience},
editor = {Martín Hevia},
series = {Developments in Neuroethics and Bioethics},
publisher = {Academic Press},
volume = {4},
pages = {19-40},
year = {2021},
booktitle = {Regulating Neuroscience: Transnational Legal Challenges},
issn = {2589-2959},
doi = {https://doi.org/10.1016/bs.dnb.2021.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S2589295921000023},
author = {Samuel Murray and Felipe {De Brigard}},
keywords = {Moral responsibility, Autonomy of ethics, Moral neuroscience, Decision-making, Practical reasoning, Moral agency},
abstract = {We argue that research in cognitive neuroscience can contribute meaningfully to some normative theorizing. To make our case, we develop one instance where ethical inquiry progressed through empirical research into the computational basis of decision-making. From this, we draw some general considerations about the kinds of normative inquiry where research in cognitive neuroscience might be relevant.}
}
@article{CAI20051145,
title = {BioSim—a biomedical character-based problem solving environment},
journal = {Future Generation Computer Systems},
volume = {21},
number = {7},
pages = {1145-1156},
year = {2005},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2004.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X04000469},
author = {Yang Cai and Ingo Snel and Betty Cheng and B. {Suman Bharathi} and Clementine Klein and Judith Klein-Seetharaman},
keywords = {Scientific visualization, Biological discovery, Game design, Problem solving, Artificial life, Education},
abstract = {Understanding and solving biomedical problems requires insight into the complex interactions between the components of biomedical systems by domain and non-domain experts. This is challenging because of the enormous amount of data and knowledge in this domain. Therefore, non-traditional educational tools have been developed such as a biological storytelling system, animations of biomedical processes and concepts, and interactive virtual laboratories. The next-generation problem solving tools need to be more interactive to include users with any background, while remaining sufficiently flexible to target open research problems at any level of abstraction, from the conformational changes of a protein to the interaction of the various biochemical pathways in our body. Here, we present an interactive and visual problem solving environment for the biomedical domain. We designed a biological world model, in which users can explore biological interactions by role-playing “characters” such as cells and molecules or as an observer in a “shielded vessel”, both with the option of networked collaboration between simultaneous users. The system architecture of these “characters” contains four main components: (1) bio-behavior is modeled using cellular automata; (2) bio-morphing uses vision-based shape tracking techniques to learn from recordings of real biological dynamics; (3) bio-sensing is based on molecular principles of recognition to identify objects, environmental conditions and progression in a process; (4) bio-dynamics implements mathematical models of cell growth and fluid-dynamic properties of biological solutions. The principles are implemented in a simple world model of the human vascular system and a biomedical problem that involves an infection by Neisseria meningitides where the biological characters are white and red blood cells and Neisseria cells. Our case studies show that the problem solving environment can inspire user's strategic, creative and innovative thinking.}
}
@article{FALL2010140,
title = {Artificial states? On the enduring geographical myth of natural borders},
journal = {Political Geography},
volume = {29},
number = {3},
pages = {140-147},
year = {2010},
issn = {0962-6298},
doi = {https://doi.org/10.1016/j.polgeo.2010.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0962629810000533},
author = {Juliet J. Fall},
keywords = {Artificial states, Boundaries, Ethnic homogeneity, Failed states, Nationalism, Natural boundaries, Territorial trap},
abstract = {Alberto Alesina, William Easterly and Janina Matuszeski's paper Artificial States, published as a National Bureau of Economic Research Working Paper in June 2006, suggests a theory linking the nature of country borders to the economic success of countries (Alesina, Easterly, & Matuszeski, 2006). This paper critically examines this suggestion that natural boundaries and ethnic homogeneity are desirable for economic reasons. It takes issue with the understanding of artificial and natural boundaries that they develop, arguing that this ignores two centuries of critical and quantitative geographical scholarship that has mapped, documented and critiqued the obsession of a link between topography and the appropriate shape of states and boundaries. It explores how their argument is linked to a defence of ethnically homogeneous states. The focus is on their teleological and paradoxically ahistorical vision that naturalizes politics by appealing to spatial myths of homogeneity and geometric destiny, grounded in a reactionary understanding of space as container. In so doing, I am mindful of the strong links between such proposals and calls for post-conflict partition, and the corresponding discourses of ethnic and cultural homogenization on which they rely. Instead of thinking of boundaries as geometric objects, squiggly or not, I consider boundaries through the simultaneous processes of reification, naturalization, and fetishization.}
}
@article{CASTRO2023105510,
title = {An experimental and simulation study of the impact of emotional information on analogical reasoning},
journal = {Cognition},
volume = {238},
pages = {105510},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2023.105510},
url = {https://www.sciencedirect.com/science/article/pii/S0010027723001440},
author = {Ariana A. Castro and John E. Hummel and Howard Berenbaum},
keywords = {Reasoning, Emotion, Computational models, Attention, Analogies},
abstract = {We investigated whether and how emotional information would affect analogical reasoning. We hypothesized that task-irrelevant emotional information would impair performance whereas task-relevant emotional information would enhance it. In Study 1, 233 undergraduates completed a novel version of the People Pieces Task (Emotional Faces People Task), an analogical reasoning task in which the task characters displayed emotional or neutral facial expressions (within-participants). The emotional faces were relevant or irrelevant to the task (between-participants). We simulated the behavioral results using the Learning and Inference with Schemas and Analogies (LISA) model of relational reasoning. LISA is a neurally plausible, symbolic-connectionist computational model of analogical reasoning. In comparison to neutral trials, participants were slower but more accurate on emotion-relevant trials, and were faster but less accurate on emotion-irrelevant trials. Simulations using the LISA model demonstrated that it is possible to account for the effects of emotional information on reasoning in terms of how emotional stimuli attract attention during a reasoning task. In Study 2, 255 undergraduates completed the Emotional Faces People Task at either a high- or low-working memory load. The high working memory load condition of Study 2 replicated the findings of Study 1, showing that participants were more accurate on emotion-relevant trials than on emotion-irrelevant trials; in Study 2, this increased accuracy could not be accounted for by a speed-accuracy tradeoff. The working memory manipulation influenced the manner in which the congruence (with the correct answer) of emotion-irrelevant emotion influenced performance. Simulations using the LISA model showed that manipulating the salience of emotion, the error penalty, as well as vigilance (which determines the likelihood that LISA will notice it has attended to an irrelevant relation), could reasonably reproduce the behavioral results of both low and high working memory load conditions of Study 2.}
}
@incollection{OGDEN2021457,
title = {Geohydrology: Hydrological Modeling},
editor = {David Alderton and Scott A. Elias},
booktitle = {Encyclopedia of Geology (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {457-476},
year = {2021},
isbn = {978-0-08-102909-1},
doi = {https://doi.org/10.1016/B978-0-08-102908-4.00115-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780081029084001156},
author = {Fred L. Ogden},
keywords = {Conceptual, Data driven, Discretization, Heterogeneity, Machine learning, Perceptual, Physical, Process-based, Stochastic, Uncertainty},
abstract = {Hydrologic models simulate one or more components of the hydrological cycle, the global water cycle on Earth. This article discusses the features, constraints, and limitations of different broad classes of hydrologic models, along with challenges associated with their application. Heterogeneity and uncertainties in material properties dominate most hydrologic settings in nature. These factors make dominant flow paths and residence times highly uncertain in most settings. For this reason hydrologic models often begin from a perceptual model that the modeler believes to represent the important system behaviors. Next, a set of equations are coded into a computational model and tested. The most common computational hydrologic model types are: analytical, conceptual, data-driven, and process-based. All hydrologic models require use of a discretization, and are lumped at some scale. Purely physics-based models are possible only in rare special situations with low uncertainty in media properties and reduced heterogeneity.}
}
@article{FRY2010218,
title = {On the nature of tetraalkylammonium ions in common electrochemical solvents: General and specific solvation – Quantitative aspects},
journal = {Journal of Electroanalytical Chemistry},
volume = {638},
number = {2},
pages = {218-224},
year = {2010},
issn = {1572-6657},
doi = {https://doi.org/10.1016/j.jelechem.2009.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S0022072809004288},
author = {Albert J. Fry and L. Kraig Steffen},
keywords = {Computational electrochemistry, Tetraalkylammonium ions, Specific solvation, Inner sphere solvation, General solvation},
abstract = {The free energies of each of 80 tetraalkylammonium ion/solvent complexes [R4N+/(solv)n], with R ranging from methyl through butyl and n ranging from 1 through 4, were computed by density functional theory (DFT) in five common electrochemical solvents: dimethylformamide (DMF), dimethylsulfoxide (DMSO), acetonitrile (AN), dichloromethane (DCM), and methanol (MeOH). The energies of the complexes were computed both with and without their solvation energies. Additional computations of the energies of the individual components, both solvated and unsolvated, were also carried out. The resulting data permit construction of a thermodynamic cycle for each R4N+/solvent pair that in turn allows the determination of the extent of general and specific solvation energies for that pair. An additional series of computations for pentane as solvent were carried out. Since this solvent should not coordinate with tetraalkylammonium ions, these computations provide a test of the validity of the computational method. This work represents a useful new general protocol for assessing the relative importance of general and specific solvation in chemical systems.}
}
@article{MILLIGAN2020106458,
title = {Effects of human land use and temperature on community dynamics in European forests},
journal = {Quaternary Science Reviews},
volume = {247},
pages = {106458},
year = {2020},
issn = {0277-3791},
doi = {https://doi.org/10.1016/j.quascirev.2020.106458},
url = {https://www.sciencedirect.com/science/article/pii/S0277379120304200},
author = {G. Milligan and R.H.W. Bradshaw and D. Clancy and K. Żychaluk and M. Spencer},
keywords = {Holocene, Europe, Vegetation dynamics, Paleoclimatology, Compositional data analysis, Stochastic differential equations, Land use},
abstract = {Climate change and human land use are thought to play a dominant role in the dynamics of European central-latitude forests in the Holocene. A wide range of mathematical and statistical models have been used to study the effects of these variables on forest dynamics, including physiologically-based simulations and phenomenological community models. However, for statistical analysis of pollen count data, compositional data analysis is particularly well suited, because pollen counts give only relative information. We studied the effects of changes in human land use and temperature on European central-latitude forest dynamics at 7 sites over most of the last 10ka, using a stochastic model for compositional dynamics of pollen count data. Our approach has a natural ecological interpretation in terms of relative proportional population growth rates, and does not require information on pollen production, dispersal, or deposition. We showed that the relative proportional population growth rates of Fagus and Picea were positively affected by intensified human land use, and that those of Tilia and Ulmus were negatively affected. Also, the relative proportional population growth rate of Fagus was negatively affected by increases in temperature above about 18∘C. Overall, the effects of temperature on the rate of change of forest composition were more important than those of human land use. Although there were aspects of dynamics, such as short-term oscillations, that our model did not capture, our approach is broadly applicable and founded on ecological principles, and gave results consistent with current thinking.}
}
@article{HOME200255,
title = {Fluids and forces in eighteenth-century electricity},
journal = {Endeavour},
volume = {26},
number = {2},
pages = {55-59},
year = {2002},
issn = {0160-9327},
doi = {https://doi.org/10.1016/S0160-9327(02)01411-4},
url = {https://www.sciencedirect.com/science/article/pii/S0160932702014114},
author = {Roderick W. Home},
abstract = {Our understanding of the history of electricity in the eighteenth century has changed significantly since the early 1960s, when Thomas Kuhn presented it as a leading example to support his general view of the history of science. In particular, while the ideas of Benjamin Franklin are still seen as important, they are no longer seen as constituting a revolution in the theory of electricity. They appear instead as merely one stage in a long drawn-out process of evolution in electrical thinking.}
}
@incollection{SOUSA2021290,
title = {Material Design-for-eXcellence Framework – Application to Composites},
editor = {Dermot Brabazon},
booktitle = {Encyclopedia of Materials: Composites},
publisher = {Elsevier},
address = {Oxford},
pages = {290-301},
year = {2021},
isbn = {978-0-12-819731-8},
doi = {https://doi.org/10.1016/B978-0-12-819724-0.00105-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128197240001051},
author = {S.P.B. Sousa and A.J. Baptista and A.T. Marques},
keywords = {Advanced materials, CFRP, Composites, Eco-efficiency, GFRP, Material Design-for-eXcellence, Materials Life Cycle, Materials Performance, M-DfX Scorecards, Sustainability},
abstract = {Despite a good number of material selection methods and tools, there is a lack of straightforward material performance methods that allow an easy and multi-dimensional assessment of the material properties, relating to the inner structure of the material in a multi-scale proposition, supported by a Life-Cycle Assessment mindset. Material Design-for-eXcellence (M-DfX) is a novel approach to support the assessment of material performance in a systematic and visual way, through the evaluation of material properties (“X” dimensions) and characteristics in a normalized form. It manages the material composition complexity in different scales, adopting a modular configuration analogy. The integrated analysis of Material Performance is attained via an effectiveness assessment of the properties’ characteristics, cross-evaluated with efficiency/eco-efficiency aspects within a Life Cycle approach, resulting in new original scorecards and quadrants graphical tools. It has adopted a Lean Thinking approach for the use of visual management and waste identification in relation to production and resource efficiency. A demonstration example of M-DfX is given for the framework testing in the Composite Materials field, comparing a CRFP composite versus GRFP composite real use case application for the body of an airport bus vehicle.}
}
@article{BUI2017115,
title = {Envisioning the future of ‘big data’ biomedicine},
journal = {Journal of Biomedical Informatics},
volume = {69},
pages = {115-117},
year = {2017},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2017.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S1532046417300709},
author = {Alex A.T. Bui and John Darrell {Van Horn}},
keywords = {Biomedicine, Data science, Software, Computing, Training},
abstract = {Through the increasing availability of more efficient data collection procedures, biomedical scientists are now confronting ever larger sets of data, often finding themselves struggling to process and interpret what they have gathered. This, while still more data continues to accumulate. This torrent of biomedical information necessitates creative thinking about how the data are being generated, how they might be best managed, analyzed, and eventually how they can be transformed into further scientific understanding for improving patient care. Recognizing this as a major challenge, the National Institutes of Health (NIH) has spearheaded the “Big Data to Knowledge” (BD2K) program – the agency’s most ambitious biomedical informatics effort ever undertaken to date. In this commentary, we describe how the NIH has taken on “big data” science head-on, how a consortium of leading research centers are developing the means for handling large-scale data, and how such activities are being marshalled for the training of a new generation of biomedical data scientists. All in all, the NIH BD2K program seeks to position data science at the heart of 21st Century biomedical research.}
}
@article{LORIMER2009152,
title = {Empathic accuracy in coach–athlete dyads who participate in team and individual sports},
journal = {Psychology of Sport and Exercise},
volume = {10},
number = {1},
pages = {152-158},
year = {2009},
issn = {1469-0292},
doi = {https://doi.org/10.1016/j.psychsport.2008.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1469029208000526},
author = {Ross Lorimer and Sophia Jowett},
keywords = {Empathy, Understanding, Interaction, Coach–athlete dyads},
abstract = {Objective
The purpose of the present study was to investigate the empathic accuracy of coach–athlete dyads participating in team and individual sports.
Method
An adaptation of Ickes's [2001. Measuring empathic accuracy. In J. A. Hall & F. J. Bernieri (Eds.), Interpersonal sensitivity (pp. 219–242). Mahwah, NJ: Lawrence Erlbaum Associates] unstructured dyadic interaction paradigm was used to assess the empathic accuracy of 40 coach–athlete dyads. Accordingly, each dyad was filmed during a training session. The dyad members viewed selected video footage that displayed discrete interactions that had naturally occurred during that session. Dyad members reported what they remembered thinking/feeling while making inferences about what their partner's thought/felt at each point. Empathic accuracy was estimated by comparing self-reports and inferences.
Results
The results indicted that accuracy for coaches in individual sports was higher than coaches in team sports. Shared cognitive focus also differed between team and individual sports, and fully mediated the effect of sport-type on coach empathic accuracy. Moreover, coaches whose training sessions were longer demonstrated increased empathic accuracy. No differences were found for athletes.
Conclusions
The results suggest that the dynamics of the interaction between a coach and an athlete play a key role in how accurately they perceive each other.}
}
@incollection{SNYDER200089,
title = {Chapter 5 - Hope as a Common Factor across Psychotherapy Approaches: A Lesson from the Dodo's Verdict},
editor = {C.R. Snyder},
booktitle = {Handbook of Hope},
publisher = {Academic Press},
address = {San Diego},
pages = {89-108},
year = {2000},
isbn = {978-0-12-654050-5},
doi = {https://doi.org/10.1016/B978-012654050-5/50007-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780126540505500075},
author = {C.R. Snyder and Julia D. Taylor},
abstract = {Publisher Summary
Despite providing different explanations and targeting disparate symptoms, various psychological approaches for producing change appear to be equally effective. The chapter compares this phenomenon to the Dodo's verdict in “Alice in Wonderland,” in the end race. It explores the question—specifically, what mechanism (or mechanisms) underlie the equal, high efficacy produced by differing types of psychological interventions. Agency reflects people's thoughts about their capacity to use the pathways they have selected to reach their goals. Agency is crucial for the psychotherapy process because it provides mental energy so that a client can undertake various therapy-related activities. This type of goal-directed motivation is reflected in self-affirming mental statements. Operating across differing samples and methodologies, agentic thinking both initiates and helps sustain clients' improvements in psychotherapy. Furthermore, enlisting the literature on placebo effects to illustrate the impact of agentic thinking. The chapter demonstrates how agency alone propels clinical improvement. Agentic thought is the motivational force or engine in hope theory. All the mental energy imaginable, however, cannot guarantee successful goal attainment in psychotherapy. Perceptions that one can produce the routes to those goals is a second necessary component. The chapter also explores pathways thinking in the context of varying psychotherapies.}
}
@article{SCHMID2019178,
title = {Representing stuff in the human brain},
journal = {Current Opinion in Behavioral Sciences},
volume = {30},
pages = {178-185},
year = {2019},
note = {Visual perception},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2019.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S2352154619300816},
author = {Alexandra C Schmid and Katja Doerschner},
abstract = {Our experience of materials does not merely comprise judgments of single properties such as glossiness or roughness but is rather made up of a multitude of simultaneous impressions of qualities. To understand the neural mechanisms yielding such complex impressions, we suggest that it is necessary to extend existing experimental approaches to those that view material perception as a distributed and dynamic process. A distributed representations framework not only fits better with our perceptual experience of material qualities, it is commensurate with recent psychophysics and neuroimaging results.}
}
@article{PASMAN201880,
title = {How can we improve process hazard identification? What can accident investigation methods contribute and what other recent developments? A brief historical survey and a sketch of how to advance},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {55},
pages = {80-106},
year = {2018},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2018.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S0950423018300329},
author = {Hans J. Pasman and William J. Rogers and M. Sam Mannan},
keywords = {Accident-incident investigation, Hazard identification, Causation, System approach},
abstract = {Risk assessment is essential for various purposes such as facility siting, safeguarding, and licensing. Hazard identification (HAZID), which suffers greatly from incompleteness, is still the weakest link in risk assessment. Of course, this recognition is not new and many efforts have been spent to improve the situation, of which some have been rather successful. To find out what can go wrong, creative divergent thinking is required. Hazard identification should result in scenario definition. In that respect, applying the present tools as HAZOP and FMEA there is still a great emphasis on the material and equipment aspects. In contrast, underlying management and leadership failure in its many forms reflecting in organizational and human failure, due to complexity, attracts much less attention. Unlike in HAZID, in accident investigation the occurrence of an event with nasty consequences is no doubt a fact, so there must be one or more causes and the traces will lead to them. Over the years, methods for accident and incident investigation have gone through a significant evolution. From the early-on simplistic domino stone model and the human operator always at fault, via models of latent failure due to failing management involvement and via extensive root cause analysis (RCA) to a system approach. Hence, in accident investigation, management failure appearing in the many possible forms of human and organizational factors, obtained already 30 years ago with the RCA technique much attention, while it nowadays culminates in the socio-technical system approach. So, the question arises whether for improved HAZID we can learn from the accident investigation experience. In addition, safer design and advances from static risk assessment towards more accurate predictive operational dynamic risk assessment and management, will also be enabled by possibilities offered by big data and analytics. Digitization, automation and simulation, hence computerization, will be of great help in improving the identification of hazards and tracing the corresponding scenarios. The paper reviews the developmental history of both accident investigation and hazard identification methodology; incidentally it will identify commonality and differences. On the basis of the comparison and of recent advances in computerization, the paper will investigate to what extent beneficial modifications and additions can be made to obtain a higher degree of completeness in HAZID.}
}
@article{ZHANG2024,
title = {Utilizing neuroimaging visualization technology to enhance standardized neurosurgical training for Traditional Chinese Medicine residents: A neuroanatomical education study},
journal = {Brain Hemorrhages},
year = {2024},
issn = {2589-238X},
doi = {https://doi.org/10.1016/j.hest.2024.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S2589238X24000640},
author = {Rongjun Zhang and Zhigang Gong and Wenbing Jiang and Zhaofeng Su},
keywords = {Neuroimaging visualization, Traditional Chinese Medicine, Neurosurgical training, Neuroanatomy, DSI Studio, Clinical Education, Meridians, Neural fiber tracts},
abstract = {Objective
This study aims to address the difficulties encountered by Traditional Chinese Medicine (TCM) students in learning neuroanatomy during clinical training by utilizing neuroimaging visualization technology.
Methods
81 students were divided into a control group (40 students) and an observation group (41 students). The control group followed traditional teaching methods as prescribed by the curriculum, while the observation group received additional training with the neuroimaging visualization software DSI Studio. This included whole-brain neural fiber reconstruction and cortical spinal tract evaluation in the context of stroke. Upon completion of the training, both groups were assessed on neuroanatomical theory, case analysis, neurological examination, and clinical skills. The teaching effectiveness was compared based on assessment results and feedback from questionnaires administered to the observation group.
Results
The observation group significantly outperformed the control group in theoretical knowledge, case analysis, and physical examination (P < 0.05). Over 90 % of students in the observation group reported via questionnaire that the integration of neuroimaging visualization technology significantly enhanced their understanding of neuroanatomy and clinical reasoning skills.
Conclusion
The clinical teaching approach augmented with neuroimaging visualization technology significantly improves the standardized training outcomes for TCM neurosurgical residents.}
}
@article{ARAGONES20141,
title = {Rhetoric and analogies},
journal = {Research in Economics},
volume = {68},
number = {1},
pages = {1-10},
year = {2014},
issn = {1090-9443},
doi = {https://doi.org/10.1016/j.rie.2013.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S1090944313000410},
author = {Enriqueta Aragones and Itzhak Gilboa and Andrew Postlewaite and David Schmeidler},
keywords = {Rhetoric, Analogies, Complexity},
abstract = {The art of rhetoric may be defined as changing other people's minds (opinions, beliefs) without providing them new information. One technique heavily used by rhetoric employs analogies. Using analogies, one may draw the listener's attention to similarities between cases and to re-organize existing information in a way that highlights certain regularities. In this paper we offer two models of analogies, discuss their theoretical equivalence, and show that finding good analogies is a computationally hard problem.}
}
@article{ALAGEEL20152003,
title = {Human Factors in the Design and Evaluation of Bioinformatics Tools},
journal = {Procedia Manufacturing},
volume = {3},
pages = {2003-2010},
year = {2015},
note = {6th International Conference on Applied Human Factors and Ergonomics (AHFE 2015) and the Affiliated Conferences, AHFE 2015},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2015.07.247},
url = {https://www.sciencedirect.com/science/article/pii/S2351978915002486},
author = {Naelah Al-Ageel and Areej Al-Wabil and Ghada Badr and Noura AlOmar},
keywords = {Bioinformatics tools, Human factors, Usability metrics, Heuristics evaluation},
abstract = {Human factors contribute significantly to the information visualization design considerations and usability evaluation process, and have been shown to play an important role in the design, development and quality assurance of bioinformatics tools. Despite the technological advances in bioinformatics computational methods, humans are an indispensable part of the data mining and decision making process. The complexity of biology data visualization can make perception and analysis a complex cognitive activity for professionals in the bioinformatics domain. Information Visualization (InfoVis) can provide valuable assistance for data analysis in bioinformatics by visually depicting sequences, genomes, alignments, and macromolecular structures. InfoVis coupled with interaction modalities of bioinformatics tools also impact the efficiency and effectiveness of decision-making tasks in applied bioinformatics computing. However, the way people perceive and interact with bioinformatics tools can strongly influence their understanding of the complex data as well as the perceived usability and accessibility of these systems. In this paper, we present a synthesis of research studies and initiatives that have recently examined human factors in interaction and visualization for bioinformatics tools, particularly in perception-based design. Although bioinformatics’ visualization and interaction design research that involves human factors is considered in its infancy, a plethora of potentially promising areas have yet to be explored. The aims of this paper are to review current human factors research in interaction, usability and visualization within bioinformatics tools to provide a basis for future investigations in systems and software engineering of bioinformatics tools, and to identify promising areas for future research directions in interaction design of bioinformatics tools.}
}
@incollection{PIGGOTT2022176,
title = {8.10 - Optimization of Marine Renewable Energy Systems},
editor = {Trevor M. Letcher},
booktitle = {Comprehensive Renewable Energy (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {176-220},
year = {2022},
isbn = {978-0-12-819734-9},
doi = {https://doi.org/10.1016/B978-0-12-819727-1.00179-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128197271001795},
author = {Matthew D. Piggott and Stephan C. Kramer and Simon W. Funke and David M. Culley and Athanasios Angeloudis},
keywords = {Tidal stream, Tidal range, Optimization, Modelling},
abstract = {Optimizing marine renewable energy systems to maximize performance is key to their success. However, a range of physical, environmental, engineering, economic as well as computational challenges means that this is not straightforward. This article considers this topic, focusing on those systems whose performance is coupled to the hydrodynamics providing the resource; tidal power represents a clear example of this. In such cases system design must be optimal in relation to the resource׳s magnitude as well as its spatial and temporal variation, which are all dependent on the system׳s configuration and operation and so cannot be assumed to be known at the design stage. Designing based on the ambient resource could lead to under-performance. Coupling between the design and the resource has implications for the complexity of the optimization problem and potential hydrodynamical and environmental impacts. This coupling distinguishes many marine energy systems from other renewables which do not impact in any significant manner on the resource. The optimal design of marine energy systems thus represents a challenging and somewhat unique problem. However, feedback also opens up a number of possibilities where the resource can be ‘controlled’, to maximize the cumulative power obtained from multiple devices or plants, or to achieve some other complementary goal. Design optimization is thus critical, with many issues to consider. Due to the complexity of the problem a computational based solution is a necessity in all but the simplest scenarios. However, the coupled feedback requires that an iterative solution approach be used, which combined while the vast range of spatial and temporal scales means that methodological compromises need to be made. These compromises need to be understood, with the correct computational tool used at the appropriate point in the design process. This article reviews these challenges as well as the progress that has been made in addressing them.}
}
@article{KWON2024488,
title = {On knowing a gene: A distributional hypothesis of gene function},
journal = {Cell Systems},
volume = {15},
number = {6},
pages = {488-496},
year = {2024},
issn = {2405-4712},
doi = {https://doi.org/10.1016/j.cels.2024.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S2405471224001236},
author = {Jason J. Kwon and Joshua Pan and Guadalupe Gonzalez and William C. Hahn and Marinka Zitnik},
keywords = {lexical semantics, gene function, machine learning, artificial intelligence, distributed representations, word embeddings, large language models, transformers},
abstract = {Summary
As words can have multiple meanings that depend on sentence context, genes can have various functions that depend on the surrounding biological system. This pleiotropic nature of gene function is limited by ontologies, which annotate gene functions without considering biological contexts. We contend that the gene function problem in genetics may be informed by recent technological leaps in natural language processing, in which representations of word semantics can be automatically learned from diverse language contexts. In contrast to efforts to model semantics as “is-a” relationships in the 1990s, modern distributional semantics represents words as vectors in a learned semantic space and fuels current advances in transformer-based models such as large language models and generative pre-trained transformers. A similar shift in thinking of gene functions as distributions over cellular contexts may enable a similar breakthrough in data-driven learning from large biological datasets to inform gene function.}
}
@article{JIN20241386,
title = {Meet the authors: Xueqin Jin, Jian Huang, Huan Wang, Kan Wang, and Nieng Yan},
journal = {Cell Chemical Biology},
volume = {31},
number = {8},
pages = {1386-1387},
year = {2024},
note = {Special issue: Bridging chemistry and biology},
issn = {2451-9456},
doi = {https://doi.org/10.1016/j.chembiol.2024.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S2451945624003118},
author = {Xueqin Jin and Jian Huang and Huan Wang and Kan Wang and Nieng Yan},
abstract = {In an interview with Dr. Samantha Nelson, a scientific editor of Cell Chemical Biology, the authors of the perspective entitled “A versatile residue numbering scheme for Nav and Cav channels” share their thoughts on life as scientists.}
}
@article{TALAAT2021164,
title = {The validity of an artificial intelligence application for assessment of orthodontic treatment need from clinical images},
journal = {Seminars in Orthodontics},
volume = {27},
number = {2},
pages = {164-171},
year = {2021},
note = {Artificial Intelligence applications in Orthodontics -An update},
issn = {1073-8746},
doi = {https://doi.org/10.1053/j.sodo.2021.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S1073874621000359},
author = {Sameh Talaat and Ahmed Kaboudan and Wael Talaat and Budi Kusnoto and Flavio Sanchez and Mohammed H. Elnagar and Christoph Bourauel and Ahmed Ghoneima},
abstract = {Aim: To assess the validity of a Convolutional Neural Network (CNN) digital model to detect and localize orthodontic malocclusions from intraoral clinical images. Materials and methods: The sample of this study consisted of the intraoral images of 700 Subjects. All images were intraoral clinical images, in one of the following views: Left Occlusion, Right Occlusion, Front Occlusion, Upper Occlusal, and Lower Occlusal. The following malocclusion conditions were localized: crowding, spacing, increased overjet, cross bite, open bite, deep bite. The images annotations were repeated by the same investigator (S.T) with a one week interval (ICC ≥ 0.9). The CNN model used for this research study was the “You Only Look Once” model. This model can detect and localize multiple objects or multiple instances of the same object in each image. It is a fully convolutional deep neural network; 24 convolutional layers followed by 2 fully connected layers. This model was implemented using the TensorFlow framework freely available from Google. Results: The created CNN model was able to detect and localize the malocclusions with an accuracy of 99.99%, precision of 99.79%, and a recall of 100%. Conclusions: The use of computational deep convolutional neural networks to identify and localize orthodontic problems from clinical images proved valid. The built AI engine accurately detected and localized malocclusion from different views of intra-oral clinical images.}
}
@article{READ201952,
title = {Using neural networks as models of personality process: A tutorial},
journal = {Personality and Individual Differences},
volume = {136},
pages = {52-67},
year = {2019},
note = {Dynamic Personality Psychology},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2017.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S0191886917306724},
author = {Stephen J. Read and Vita Droutman and Benjamin J. Smith and Lynn C. Miller},
keywords = {Neural networks, Computational modeling, Within-subjects variability, Connectionist modeling, Personality dynamics},
abstract = {This paper presents a tutorial for creating neural network models of personality processes. Such models enable researchers to create explicit models of both personality structure and personality dynamics, and to address issues of recent concern in personality, such as, “If personality is stable, then how is it possible that within subject variability in personality states can be as large as or larger than between subject variability in personality?” or “Is it possible to understand personality dynamics and personality structure within a common framework?” We discuss why one should want to use neural networks, review what a neural network model is, review a previous model we have constructed, discuss how to conceptualize issues in such a way that they can be computationally modeled, show how that conceptualization can be translated into a model, and discuss the utility of such models for understanding personality structure and personality dynamics. To build our model we use a neural network modeling package called emergent that is freely available, and a specific architecture called Leabra to build a runnable model that addresses one of the questions posed above: How can within subject variability in personality related states be as large as between subject variability in personality?}
}
@article{OUYANG2024e29176,
title = {Unmasking the challenges in ideological and political education in China: A thematic review},
journal = {Heliyon},
volume = {10},
number = {8},
pages = {e29176},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e29176},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024052071},
author = {Sha Ouyang and Wei Zhang and Jian Xu and Abdullah {Mat Rashid} and Shwu Pyng How and Aminuddin {Bin Hassan}},
keywords = {Ideological and political education, China, Thematic review},
abstract = {China's distinctive educational approach, particularly its emphasis on ideological and political education, has garnered considerable academic attention for its impact on shaping individual values, fostering citizenship, and maintaining social stability. Despite the Chinese government's prioritization of ideological and political education, academic research in this field appears constrained, with existing studies predominantly focusing on normative and descriptive aspects. Normative research delineates how ideological and political education should be executed, while descriptive research illustrates its practical implementation. The effectiveness of these approaches is significantly diminished if they are not adequately interconnected—when only the current reality is explained without providing tools for improvement or when prescribed steps for improvement lack a basis in specific contexts. This paper conducts a comprehensive review of research on ideological and political education using ATLAS. ti 9 for thematic analysis. The review aims to unveil the intricate landscape of current research in China and address key questions: What are the primary trends in the literature on ideological and political education between 2021 and July 2023? What challenges does ideological and political education face? Through a direct exploration of these issues, this paper seeks to optimize the ideological and political education system, elevate its adaptability and effectiveness, and open avenues for research, fostering a more dynamic, inclusive, and resilient development of ideological and political education.}
}
@article{LUCKRING2023100950,
title = {Model validation hierarchies for connecting system design to modeling and simulation capabilities},
journal = {Progress in Aerospace Sciences},
volume = {142},
pages = {100950},
year = {2023},
issn = {0376-0421},
doi = {https://doi.org/10.1016/j.paerosci.2023.100950},
url = {https://www.sciencedirect.com/science/article/pii/S0376042123000660},
author = {James M. Luckring and Scott Shaw and William L. Oberkampf and Rick E. Graves},
keywords = {Modeling and simulation, Validation hierarchy, Systems architecture, Physics taxonomy, Surface-to-air missile defense system},
abstract = {Hierarchical structures provide a means to systematically deconstruct an engineering system of arbitrary complexity into its subsystems, components, and physical processes. Model validation hierarchies can aid in understanding the coupling and interaction of subsystems and components, as well as improve the understanding of how simulation models are used to design and optimize the engineering system of interest. The upper tiers of the hierarchy address systems and subsystems architecture decompositions, while the lower tiers address physical processes that are both coupled and uncoupled. Recent work connects these two general sections of the hierarchy through a transition tier, which blends the focus of system functionality and physics modeling activities. This work also includes a general methodology for how a model validation hierarchy can be constructed for any type of engineering system in any operating environment, e.g., land, air, sea, or space. We review previous work on the construction and use of model validation hierarchies in not only the field of aerospace systems, but also from commercial nuclear power plant systems. Then an example of a detailed model validation hierarchy is constructed and discussed for a surface-to-air missile defense system with an emphasis on the missile subsystems.}
}
@article{AMMAR2018116,
title = {MPEG-4 AVC stream-based saliency detection. Application to robust watermarking},
journal = {Signal Processing: Image Communication},
volume = {60},
pages = {116-130},
year = {2018},
issn = {0923-5965},
doi = {https://doi.org/10.1016/j.image.2017.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0923596517301674},
author = {Marwa Ammar and Mihai Mitrea and Marwen Hasnaoui and Patrick {Le Callet}},
keywords = {Saliency map, MPEG-4 AVC stream, Density fixation map, Saccade locations, Robust watermarking},
abstract = {By bridging uncompressed-domain saliency detection and MPEG-4 AVC compression principles, the present paper advances a methodological framework for extracting the saliency maps directly from the stream syntax elements. In this respect, inside each GOP, the intensity, color, orientation and motion elementary saliency maps are related to the energy of the luma coefficients, to the energy of chroma coefficients, to the gradient of the prediction modes and to the amplitude of the motion vectors, respectively. The three spatial saliency maps are pooled according to an average formula, while the static-temporal fusion is achieved by six different formulas. The experiments consider both ground-truth and applicative evaluations. The ground-truth benchmarking investigates the relation between the predicted MPEG-4 AVC saliency map and the actual human saliency, captured by eye-tracking devices. It is based on two corpora (representing density fixation maps and saccade locations), two objective criteria (related to the closeness between the predicted and the real saliency maps and to the difference between the behavior of the predicted saliency map in fixation and random locations), two objective measures (KLD – the Kullback Leibler Divergence and AUC – the Area Under the ROC Curve) and 5 state-of-the-art saliency models (3 acting in spatial domain and 2 acting in compressed domain). The applicative validation is carried out by integrating the MPEG-4 AVC saliency map into a robust watermarking application. As an overall conclusion, the paper demonstrates that although the MPEG-4 AVC standard does not explicitly relies on any visual saliency principle, its stream syntax elements preserve this property. Four main benefits for the MPEG-4 AVC based saliency extraction are thus brought to light: (1) it outperforms (or, at least, is as good as) state-of-the-art uncompressed domain methods, (2) it allows significant gains to be obtained in watermarking transparency (for prescribed data payload and robustness), (3) it is less sensitive to the randomness in the processed visual content, and (4) it has a linear computational complexity. For instance, the ground truth results exhibit absolute relative gains between 60% and 164% in KLD, between 17% and 21% in AUC, and relative gains in KLD sensitivity between 1.18 and 6.12 and in AUC sensitivity between 1.06 and 33.7; the applicative validation brings to light transparency gains up to 10 dB in PSNR.}
}
@incollection{GROSSBERG19873,
title = {The Qijantized Geometry of Visual Space: The Coherent Computation of Depth, Form and Lightness},
editor = {Stephen Grossberg},
series = {Advances in Psychology},
publisher = {North-Holland},
volume = {43},
pages = {3-79},
year = {1987},
booktitle = {The Adaptive Brain II},
issn = {0166-4115},
doi = {https://doi.org/10.1016/S0166-4115(08)61756-2},
url = {https://www.sciencedirect.com/science/article/pii/S0166411508617562},
author = {Stephen Grossberg},
keywords = {binocular vision, brightness perception, figure-ground, feature extraction, form perception, neural network, nonlinear resonance, receptive field, short-term memory, spatial scales, visual completion},
abstract = {A theory is presented of how global visual interactions between depth, length, lightness, and form percepts can occur. The theory suggests how quantized activity patterns which reflect these visual properties can coherently fill-in, or complete, visually ambiguous regions starting with visually informative data features. Phenomena such as the Cornsweet and Craik-O'Brien effects, phantoms and subjective contours, binocular brightness summation, the equidistance tendency, Emmert's law, allelotropia, multiple spatial frequency scaling and edge detection, figure-ground completion, coexistence of depth and binocular rivalry, reflectance rivalry, Fechner's paradox, decrease of threshold contrast with increased number of cycles in a grating pattern, hysteresis, adaptation level tuning, Weber law modulation, shift of sensitivity with background luminance, and the finite capacity of visual short term memory are discussed in terms of a small set of concepts and mechanisms. Limitations of alternative visual theories which depend upon Fourier analysis, Laplacians, zero-crossings, and cooperative depth planes are described. Relationships between monocular and binocular processing of the same visual patterns are noted, and a shift in emphasis from edge and disparity computations toward the characterization of resonant activity-scaling correlations across multiple spatial scales is recommended. This recommendation follows from the theory's distinction between the concept of a structural spatial scale, which is determined by local receptive field properties, and a functional spatial scale, which is defined by the interaction between global properties of a visual scene and the network as a whole. Functional spatial scales, but not structural spatial scales, embody the quantization of network activity that reflects a scene's global visual representation. A functional scale is generated by a filling-in resonant exchange, or FIRE, which can be ignited by an exchange of feedback signals among the binocular cells where monocular patterns are binocularly matched.}
}
@article{BREIGER2018104,
title = {Capturing distinctions while mining text data: Toward low-tech formalization for text analysis},
journal = {Poetics},
volume = {68},
pages = {104-119},
year = {2018},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2018.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X17301584},
author = {Ronald L. Breiger and Robin Wagner-Pacifici and John W. Mohr},
keywords = {Text mining, Hermeneutics, National security, Computational sociology, Big data, Close reading},
abstract = {In this article we consider some low-tech approaches to text mining. Our goal is to articulate a RiCH (Reader in Control of Hermeneutics) style of text analysis that takes advantage of the digital affordances of modern reading practices and easily deployable computational tools while also preserving the primacy of the interpretive lens of the human reader. In the article we offer three analytical interventions that are suitable to the low-tech formalizations we propose: the first and most developed intervention tracks the (normally computationally ignored) “stop” words; the second identifies the use of strategic anxiety terms in the texts; and the third (less developed in this article) introduces the grammatical features of modality (including modalization statements of probability and usuality, and modulation statements regarding degrees of obligation and inclination). All three analytical interventions provide a productive tracking of various modes and degrees of strategic decisiveness, contradiction, uncertainty and indeterminacy in a corpus of recent U.S. National Security Strategy reports.}
}
@incollection{GRANGER1986137,
title = {The Computation Of Contingency In Classical Conditioning},
editor = {Gordon H. Bower},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {20},
pages = {137-192},
year = {1986},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(08)60018-3},
url = {https://www.sciencedirect.com/science/article/pii/S0079742108600183},
author = {Richard H. Granger and Jeffrey C. Schlimmer},
abstract = {Publisher Summary
This chapter discusses a unified framework, which encompasses the computations, algorithms, and neurobiological implementations underlying classical conditioning. It presents an extensive mathematical analysis of the constraints on classical conditioning—that is, the precise contingency conditions under which mammals may and may not learn a particular association between two events in a classical conditioning situation. In classical conditioning, an unconditional stimulus (US)—that is, a cue, which is inherently biologically salient to an animal (such as an electric shock), is repeatedly paired with a conditional stimulus (CS), a cue that initially has no special significance to the animal over repeated trials, the animal can learn that the CS is predictive of or associated with the US. This phenomenon of associative learning is subject to laws and constraints: An association is learned to some extent in some conditions and to a lesser extent in others.}
}
@article{LABELLA2021107141,
title = {An optimal Best-Worst prioritization method under a 2-tuple linguistic environment in decision making},
journal = {Computers & Industrial Engineering},
volume = {155},
pages = {107141},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107141},
url = {https://www.sciencedirect.com/science/article/pii/S0360835221000450},
author = {Álvaro Labella and Bapi Dutta and Luis Martínez},
keywords = {Best-Worst method, 2-tuple linguistic model, Multi-criteria group decision making},
abstract = {Multi-criteria group decision making (MCGDM) deals with decision makers who evaluate alternatives over several criteria. MCGDM problems evolve in tandem with the progress of our society. Such progress has given rise to the large-scale group decision making (LS-GDM) problems in which hundreds of decision makers may participate in the decision process and new challenges to face such as groups’ formation and polarization opinions. Most real world MCGDM problems present changing contexts with uncertainty that cannot be modeled by numerical values. Under these circumstances, the use of linguistic variables and computing with words (CW) processes have provided successfully results. Concretely, the 2-tuple linguistic computational model stands out because its precise linguistic computations and high interpretability. On the other hand, pairwise comparison is a widely used elicitation technique in MCGDM, but a large number of comparisons might lead inconsistent decision makers’ preferences. The Best-Worst method (BWM) reduces the number of pairwise comparisons and the inconsistency in decision makers’ opinions. Several BWM approaches have been proposed to manage linguistic information but none of them take advantage of the 2-tuple linguistic computational process based on the CW approach, which would allow to obtain precise and understandable results. This paper aims to present an extended 2-tuple BWM to reduce the number of pairwise comparisons in MCGDM problems and model the uncertainty associated with them to accomplish accuracy computations and obtaining interpretable results. Moreover, we apply our proposal to LS-GDM scenarios in which polarization opinions and sub-groups identification, ignored from any of BWM proposals, are considered. Finally, the new model is applied to several illustrative MCGDM problems.}
}
@article{WU2024103772,
title = {Fuser: An enhanced multimodal fusion framework with congruent reinforced perceptron for hateful memes detection},
journal = {Information Processing & Management},
volume = {61},
number = {4},
pages = {103772},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103772},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324001328},
author = {Fan Wu and Bin Gao and Xiaoou Pan and Linlin Li and Yujiao Ma and Shutian Liu and Zhengjun Liu},
keywords = {Hateful memes detection, Multimodal fusion, Congruent reinforced perceptron, Main semantic, Auxiliary context},
abstract = {As a multimodal form of hate speech on social media, hateful memes are more aggressive and cryptic threats to the real life of humans. Automatic detection of hateful memes is crucial, but the images and texts in most memes are only weakly consistent or even irrelevant. Although existing works have achieved the initial goal of detecting hateful memes with pre-trained models, they are limited to monolithic inference methods while ignoring the semantic differences between multimodal representations. To strengthen the comprehension and reasoning of the hidden meaning behind the memes by combining real-world knowledge, we propose an enhanced multimodal fusion framework with congruent reinforced perceptron for hateful memes detection. Inspired by the human cognitive mechanism, we first divide the extracted multisource representations into main semantics and auxiliary contexts based on their strength and relevance, and then precode them into lightly correlated embeddings with unified spatial dimensions via a novel prefix uniform layer, respectively. To jointly learn the intrinsic correlation between primary and secondary semantics, a congruent reinforced perceptron with brain-like perceptual integration is designed to seamlessly fuse multimodal representations in a shared latent space while maintaining the feature integrity in the sub-fusion space, thereby implicitly reasoning about the subtle metaphors behind the memes. Extensive experiments on four benchmark datasets fully demonstrate the effectiveness and superiority of our architecture compared with previous state-of-the-art methods.}
}
@article{JETTER201445,
title = {Fuzzy Cognitive Maps for futures studies—A methodological assessment of concepts and methods},
journal = {Futures},
volume = {61},
pages = {45-57},
year = {2014},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2014.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0016328714000809},
author = {Antonie J. Jetter and Kasper Kok},
keywords = {Fuzzy Cognitive Maps, Future studies, Scenarios, Mental model, System thinking},
abstract = {Fuzzy Cognitive Map (FCM) modelling is highly suitable for the demands of future studies: it uses a mix of qualitative and quantitative approaches, it enables the inclusion of multiple and diverse sources to overcome the limitations of expert opinions, it considers multivariate interactions that lead to nonlinearities, and it aims to make implicit assumptions (or mental models) explicit. Despite these properties, the field of future studies is slow to adopt FCM and to apply the increasingly solid theoretical foundations and rigorous practices for FCM applications that are evolving in other fields. This paper therefore discusses theoretical and practical aspects of constructing and applying FCMs within the context of future studies: based on an extensive literature review and the authors’ experience with FCM projects, it provides an introduction of fundamental concepts of FCM modelling, a step-wise description and discussion of practical methods and their pitfalls, and an overview over future research directions for FCM in future studies.}
}
@article{BRIAS2016151,
title = {Computing the reliability kernel of a time-variant system: Application to a corroded beam},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {12},
pages = {151-155},
year = {2016},
note = {8th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.07.566},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316308242},
author = {A. Brias and J-D. Mathias and G. Deffuant},
keywords = {Discrete systems, Dynamic systems, Initial states, Reliability analysis, Reliability kernel, System failures, Transition matrix, Reliable design},
abstract = {Time-variant reliability analysis aims at assessing the probability of failure of a time-variant system within a given time horizon. We illustrate in this paper the computation of the reliability kernel which is the set of initial states for which the probability of failure remains under a threshold within the considered time horizon. This paper supposes that the time-variant system is discrete in time and space with given probabilities of transition between space states. We use a recursive relation for computing the cumulative probability of failure of the system, linking the probability of failure at time t with the probability of being at a given state x (for all possible states) at time t — 1. Applying this relation, it is possible to compute the probability of failure at any starting point in the state space and hence to derive the reliability kernel. The computation of this kernel gives informations about the system which can be further helpful in reliable design. The approach is illustrated on an example of a steel beam under corrosion.}
}
@incollection{SCHILLER2018246,
title = {Some Thermodynamics and Electrostatics With a View to Electrochemistry},
editor = {Klaus Wandelt},
booktitle = {Encyclopedia of Interfacial Chemistry},
publisher = {Elsevier},
address = {Oxford},
pages = {246-257},
year = {2018},
isbn = {978-0-12-809894-3},
doi = {https://doi.org/10.1016/B978-0-12-409547-2.13605-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095472136054},
author = {R. Schiller},
keywords = {Activity coefficient, Chemical potential, Cole–Cole plot, Conservation laws, Dielectric relaxation time, Dipole moment, Electric dipole, Entropy of mixing, Gauss law of electrostatics, Kramers–Kronig relations, Osmotic coefficient, Poisson equation, Polarization, Relative permittivity, Solvation energy},
abstract = {This article tries to offer an overview of some basic laws of thermodynamics and electrostatics which are considered to be part of the foundations of electrochemical thinking. Equilibrium thermodynamics is introduced in terms of conservation laws paying particular attention to the notion of chemical potential. After discussing the forces, potentials, and energetics of charges in vacuum the same problems are dealt with in continuous dielectric media. Here polarization, formation and role of dipole moments, their relation to relative permittivity (dielectric constant) are discussed both in macroscopic and atomic/molecular terms. Finally the kinetics of the response of relative permittivity to the variation of polarizing fields and charges are described. Solvation processes are referred to in connection with both thermodynamic and electrostatic considerations.}
}
@article{ASH1984412,
title = {Computations of cuspidal cohomology of congruence subgroups of SL(3, Z)},
journal = {Journal of Number Theory},
volume = {19},
number = {3},
pages = {412-436},
year = {1984},
issn = {0022-314X},
doi = {https://doi.org/10.1016/0022-314X(84)90081-7},
url = {https://www.sciencedirect.com/science/article/pii/0022314X84900817},
author = {Avner Ash and Daniel Grayson and Philip Green},
abstract = {Algorithms are presented which find a basis of the vector space of cuspidal cohomology of certain congruence subgroups of SL(3, Z) and which determine the action of the Hecke operators on this space. These algorithms were implemented on a computer. Four pairs of cuspidal classes were found with prime level less than 100. Tables are given of the eigenvalues of the first few Hecke operators on these classes.}
}
@article{ANDERSON2015283,
title = {Sparse factors for the positive and negative syndrome scale: Which symptoms and stage of illness?},
journal = {Psychiatry Research},
volume = {225},
number = {3},
pages = {283-290},
year = {2015},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2014.12.025},
url = {https://www.sciencedirect.com/science/article/pii/S016517811401018X},
author = {Ariana Anderson and Marsha Wilcox and Adam Savitz and Hearee Chung and Qingqin Li and Giacomo Salvadore and Dai Wang and Isaac Nuamah and Steven P. Riese and Robert M. Bilder},
keywords = {PANSS, Confirmatory factor analysis, Exploratory factor analysis, Schizophrenia, RDoC, Dimensional Measures},
abstract = {The Positive and Negative Syndrome Scale (PANSS) is frequently described with five latent factors, yet published factor models consistently fail to replicate across samples and related disorders. We hypothesize that (1) a subset of the PANSS, instead of the entire PANSS scale, would produce the most replicable five-factor models across samples, and that (2) the PANSS factor structure may be different depending on the treatment phase, influenced by the responsiveness of the positive symptoms to treatment. Using exploratory factor analysis, confirmatory factor analysis and cross validation on baseline and post-treatment observations from 3647 schizophrenia patients, we show that five-factor models fit best across samples when substantial subsets of the PANSS items are removed. The optimal model at baseline (five factors) omits 12 items: Motor Retardation, Grandiosity, Somatic Concern, Lack of Judgment and Insight, Difficulty in Abstract Thinking, Mannerisms and Posturing, Disturbance of Volition, Preoccupation, Disorientation, Excitement, Guilt Feelings and Depression. The PANSS factor models fit differently before and after patients have been treated. Patients with larger treatment response in positive symptoms have larger variations in factor structure across treatment stage than the less responsive patients. Negative symptom scores better predict the positive symptoms scores after treatment than before treatment. We conclude that sparse factor models replicate better on new samples, and the underlying disease structure of Schizophrenia changes upon treatment.}
}
@article{ALIJAH2007193,
title = {On the N3O2- paradigm},
journal = {Journal of Molecular Structure},
volume = {844-845},
pages = {193-199},
year = {2007},
note = {STUDIES IN HYDROGEN-BONDED SYSTEMS – A collection of Invited Papers in honour of Professor Lucjan Sobcyk, on the occasion of his 80th Birthday},
issn = {0022-2860},
doi = {https://doi.org/10.1016/j.molstruc.2007.04.024},
url = {https://www.sciencedirect.com/science/article/pii/S0022286007003316},
author = {Alexander Alijah and Eugene S. Kryachko},
keywords = {, Theoretical calculations, Isomers, Electron detachment},
abstract = {A survey of the existing experimental and theoretical data on the trinitrogen dioxide anion N3O2- that manifests a controversy as to the number of isomers and their chemical structures is presented. To resolve the controversy, new computational studies are performed at the MP2/aug-cc-pVTZ computational level. Two hitherto unknown isomers are predicted, one with singlet and one with triplet spin multiplicity. The singlet isomer, structurally characterized as N2·[ONO]−, is the most stable among all known isomers and accounts for fragmentation patterns observed in the recent dissociative photodetachment experiments.}
}
@article{CHOGA202491,
title = {Rapid dynamic changes of FL.2 variant: A case report of COVID-19 breakthrough infection},
journal = {International Journal of Infectious Diseases},
volume = {138},
pages = {91-96},
year = {2024},
issn = {1201-9712},
doi = {https://doi.org/10.1016/j.ijid.2023.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S1201971223007725},
author = {Wonderful T. Choga and Gobuiwang Khilly {Kurusa (Gasenna)} and James Emmanuel San and Tidimalo Ookame and Irene Gobe and Mohammed Chand and Badisa Phafane and Kedumetse Seru and Patience Matshosi and Boitumelo Zuze and Nokuthula Ndlovu and Teko Matsuru and Dorcas Maruapula and Ontlametse T. Bareng and Kutlo Macheke and Lesego Kuate-Lere and Labapotswe Tlale and Onalethata Lesetedi and Modiri Tau and Mpaphi B. Mbulawa and Pamela Smith-Lawrence and Mogomotsi Matshaba and Roger Shapiro and Joseph Makhema and Darren P. Martin and Tulio {de Oliveira} and Richard J. Lessells and Shahin Lockman and Simani Gaseitsiwe and Sikhulile Moyo},
keywords = {SARS-CoV-2, Evolution, FL.2, Immunocompromised, Botswana},
abstract = {We investigated intra-host genetic evolution using two SARS-CoV-2 isolates from a fully vaccinated (primary schedule x2 doses of AstraZeneca plus a booster of Pfizer), >70-year-old woman with a history of lymphoma and hypertension who presented a SARS-CoV-2 infection for 3 weeks prior to death due to COVID-19. Two full genome sequences were determined from samples taken 13 days apart with both belonging to Pango lineage FL.2: the first detection of this Omicron sub-variant in Botswana. FL.2 is a sub-lineage of XBB.1.9.1. The repertoire of mutations and minority variants in the Spike protein differed between the two time points. Notably, we also observed deletions within the ORF1a and Membrane proteins; both regions are associated with high T-cell epitope density. The internal milieu of immune-suppressed individuals may accelerate SARS-CoV-2 evolution; hence, close monitoring is warranted.}
}
@article{WANG2025100880,
title = {Application of gamification based virtual robots in urban landscape Design: Interaction and entertainment experience in the design process},
journal = {Entertainment Computing},
volume = {52},
pages = {100880},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100880},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124002489},
author = {Wenling Wang},
keywords = {Gamification elements, Virtual robot, Urban landscape design, Design interaction, Entertainment experience},
abstract = {The traditional design process lacks fun and participation, so new elements need to be introduced to enhance the attraction and creativity of the design. The goal of the research is to design an urban landscape design method based on gamified elements and virtual robots to increase the interactivity and entertainment experience in the design process, and to explore its impact on the design results. This paper proposes an urban landscape design framework based on gamified elements and virtual robots, which includes multiple stages in the design process, each of which introduces different gamified tasks and interactions of virtual robots. Designers can gain new design ideas and inspiration by completing tasks and interacting with virtual robots. The study evaluated the effects of design methods using gamification elements and virtual robots on the design process and design results. The results show that this approach effectively increases the engagement and enjoyment of the design process, while also promoting innovation and sustainability of the design results. The urban landscape design method based on gamification elements and virtual robots has broad application prospects, which can bring more creativity and fun to urban landscape design.}
}
@article{GOLDSCHMIDT2006549,
title = {Variances in the impact of visual stimuli on design problem solving performance},
journal = {Design Studies},
volume = {27},
number = {5},
pages = {549-569},
year = {2006},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2006.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X06000172},
author = {Gabriela Goldschmidt and Maria Smolkov},
keywords = {creativity, design problems, problem solving, visual stimuli},
abstract = {Research in cognitive psychology and in design thinking has shown that the generation of inner representations in imagery and external representations via sketching are instrumental in design problem solving. In this paper we focus on another facet of visual representation in design: the ‘consumption’ of external visual representations, regarded as stimuli, when those are present in the designer's work environment. An empirical study revealed that the presence of visual stimuli of different kinds can affect performance, measured in terms of practicality, originality and creativity scores attained by designs developed by subjects under different conditions. The findings suggest that the effect of stimuli is contingent on the type of the design problem that is being solved.}
}
@article{ZU2023107200,
title = {Random walk numerical scheme for the steady-state of stochastic differential equations},
journal = {Communications in Nonlinear Science and Numerical Simulation},
volume = {121},
pages = {107200},
year = {2023},
issn = {1007-5704},
doi = {https://doi.org/10.1016/j.cnsns.2023.107200},
url = {https://www.sciencedirect.com/science/article/pii/S1007570423001181},
author = {Jian Zu},
keywords = {Continuous-time random walk, Stochastic differential equation, Steady state, Invariant distribution},
abstract = {The continuous-time random walk (CTRW) scheme is a time-continuous and space-discretization method to obtain the numerical solution of stochastic differential equations (SDEs). Compared with the traditional time-discretization scheme, it has the advantages of numerical stability and can alleviate the curse of dimensionality. This paper proposes an improved version of the CTRW scheme for the numerical solution of SDEs. By compensating the artificial diffusion caused by the Poisson approximation of the drift term of the SDE, the improved CTRW scheme has significantly better performance in the weak noise case, especially in approximating the invariant probability measure. Numerical studies show that the improved CTRW scheme has more accuracy than the existing one but takes less computation time. In addition, it has better accuracy of the mean holding time. We also modify the hybrid Fokker–Planck solver proposed for the CTRW scheme to compute the invariant probability measure.}
}
@article{WANG2024121777,
title = {Deep learning-based flatness prediction via multivariate industrial data for steel strip during tandem cold rolling},
journal = {Expert Systems with Applications},
volume = {237},
pages = {121777},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121777},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423022790},
author = {Qinglong Wang and Jie Sun and Yunjian Hu and Wenqiang Jiang and Xinchun Zhang and Zhangqi Wang},
keywords = {Strip flatness, Tandem cold rolling, Deep learning, Multi-variate prediction, Industrial data},
abstract = {Flatness deviations in the tandem cold-rolling process of steel strips have a direct impact on product quality and shape, leading to strip breakage, reduced working speed, and equipment damage. However, conventional physics-based numerical models are inadequate for accurately predicting flatness in the complex operating conditions and variables of tandem rolling environments. To address this challenge, a novel approach is proposed that utilizes deep convolutional neural networks (DCNNs) based on real industrial data from tandem cold rolling. The multi-input and multi-output architecture of our DCNNs enables them to solve the multi-level nonlinear problem associated with flatness prediction in the tandem cold-rolling process. The flatness profiles are effectively predicted using the proposed method, incorporating multiple variables without requiring additional data pre-processing methods. Additionally, the effects of network width, depth, and topology on flatness prediction performance are thoroughly investigated. The developed Inception-ResNet demonstrates remarkable predictive performance while using fewer model parameters and exhibiting lower computational complexity compared to other network architectures. Specifically, the proposed Inception-ResNet-39 model, consisting of 39 layers of learnable parameters, achieves state-of-the-art predictive performance. Our deep learning-based approach accurately predicts flatness in tandem cold-rolling through end-to-end modeling and provides complete pipelines for model transfer construction to ensure efficient implementation.}
}
@article{YUAN202317,
title = {MFGAD: Multi-fuzzy granules anomaly detection},
journal = {Information Fusion},
volume = {95},
pages = {17-25},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523000490},
author = {Zhong Yuan and Hongmei Chen and Chuan Luo and Dezhong Peng},
keywords = {Granular computing, Fuzzy rough set theory, Unsupervised anomaly detection, Multi-granularity, Hybrid data},
abstract = {Unsupervised anomaly detection is an important research direction in the process of unsupervised knowledge acquisition. It has been successfully applied in many fields, such as online fraud identification, loan approval, and medical diagnosis. Multi-granularity thinking is an effective information fusion method for solving problems in a multi-granular environment, which allows people to understand and analyze problems from multiple perspectives. However, there are few studies on building anomaly detection models using the idea of multi-fuzzy granules. To this end, this paper constructs a multi-fuzzy granules anomaly detection method by using a fuzzy rough computing model. In this method, a hybrid metric is first used to calculate the fuzzy relations. Then, two ranking sequences are constructed based on the significance of attributes. Furthermore, forward and reverse multi-fuzzy granules are constructed to define anomaly scores based on the ranking sequences. Finally, a multi-fuzzy granules-based anomaly detection algorithm is designed to detect anomalies. The experimental results compared with existing algorithms show the effectiveness of the proposed algorithm.}
}
@article{FIELDS2022104714,
title = {Neurons as hierarchies of quantum reference frames},
journal = {Biosystems},
volume = {219},
pages = {104714},
year = {2022},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2022.104714},
url = {https://www.sciencedirect.com/science/article/pii/S0303264722000983},
author = {Chris Fields and James F. Glazebrook and Michael Levin},
keywords = {Activity-dependent remodeling, Bayesian inference, Bioelectricity, Computation, Learning, Memory},
abstract = {Conceptual and mathematical models of neurons have lagged behind empirical understanding for decades. Here we extend previous work in modeling biological systems with fully scale-independent quantum information-theoretic tools to develop a uniform, scalable representation of synapses, dendritic and axonal processes, neurons, and local networks of neurons. In this representation, hierarchies of quantum reference frames act as hierarchical active-inference systems. The resulting model enables specific predictions of correlations between synaptic activity, dendritic remodeling, and trophic reward. We summarize how the model may be generalized to nonneural cells and tissues in developmental and regenerative contexts.}
}
@article{HANOCH20021,
title = {“Neither an angel nor an ant”: Emotion as an aid to bounded rationality},
journal = {Journal of Economic Psychology},
volume = {23},
number = {1},
pages = {1-25},
year = {2002},
issn = {0167-4870},
doi = {https://doi.org/10.1016/S0167-4870(01)00065-4},
url = {https://www.sciencedirect.com/science/article/pii/S0167487001000654},
author = {Yaniv Hanoch},
keywords = {Bounded rationality, Emotion},
abstract = {The role of emotion as a source of bounded rationality has been largely ignored. Following Herbert Simon, economists as well as psychologists have mainly focused on cognitive constraints while neglecting to integrate the growing body of research on emotion which indicates that reason and emotion are interconnected. Accordingly, the present paper aims to bridge the existing gap. By establishing a link between the two domains of research, emotion and bounded rationality, it will be suggested that emotions work together with rational thinking in two distinct ways, and thereby function as an additional source of bounded rationality. The aim, therefore, is not to offer an alternative to bounded rationality; rather, the purpose is to elaborate and supplement themes emerging out of bounded rationality.}
}
@article{TRELEAVEN198859,
title = {Parallel architecture overview},
journal = {Parallel Computing},
volume = {8},
number = {1},
pages = {59-70},
year = {1988},
note = {Proceedings of the International Conference on Vector and Parallel Processors in Computational Science III},
issn = {0167-8191},
doi = {https://doi.org/10.1016/0167-8191(88)90109-3},
url = {https://www.sciencedirect.com/science/article/pii/0167819188901093},
author = {Philip C. Treleaven},
keywords = {Computer architecture, parallel computing},
abstract = {An increasing number of parallel computer products are appearing in the market place. Their design motivations and market areas cover a broad spectrum: (i) Transaction Processing Systems, such as Parallel UNIX systems (e.g. SEQUENT Balance), for data processing applications; (ii) Numeric Supercomputers, such as Hypercube systems (e.g. INTEL iPSC), for scientific and engineering applications; (iii) VLSI Architectures, such as parallel microcomputers (e.g. INMOS Transputer), for exploiting very large scales of integration; (iv) High-Level Language Computers, such as Logic machines (e.g. FUJITSU Kabu-Wake), for symbolic computation; and (v) Neurocomputers, such as Connectionist computers (e.g. THINKING MACHINES Connection Machine), for general-purpose pattern matching applications. This survey paper gives an overview of these novel parallel computers and discusses the likely commercial impact of parallel computers.}
}
@article{LI2022111937,
title = {Detection method of timber defects based on target detection algorithm},
journal = {Measurement},
volume = {203},
pages = {111937},
year = {2022},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2022.111937},
url = {https://www.sciencedirect.com/science/article/pii/S0263224122011332},
author = {Dongjie Li and Zilei Zhang and Baogang Wang and Chunmei Yang and Liwei Deng},
keywords = {Wood defect detection, YOLOX, Target detection, Feature fusion},
abstract = {Deep learning has achieved certain results in the field of wood surface defect detection. To address the problems of low accuracy of the detection results of surface defects on boards, slow detection speed and large number of model parameters, this article take advantage of computer vision to improve the feature fusion module of YOLOX target detection algorithm, by adding efficient channel attention (ECA) mechanism, adaptive spatial feature fusion mechanism (ASFF) and improve the confidence loss and localization loss functions as Focal loss and Efficient Intersection over Union (EIoU) loss, to enhance the feature extraction ability and detection accuracy of the algorithm. Considering the depth and width of the model, the depth-separable convolution and optional multi-version algorithm are used to reduce the model parameters and computational effort to seek the optimal model. Experiments show that the improved model detects four types of defects in rubber timber with a considerable improvement and has significant advantages over other target detection algorithms.}
}
@article{LIM2024127512,
title = {Progressive expansion: Cost-efficient medical image analysis model with reversed once-for-all network training paradigm},
journal = {Neurocomputing},
volume = {581},
pages = {127512},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.127512},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224002832},
author = {Shin Wei Lim and Chee Seng Chan and Erma Rahayu {Mohd Faizal} and Kok Howg Ewe},
keywords = {Medical image analysis, Machine learning, Model optimization, Cost-effective model},
abstract = {Low computational cost artificial intelligence (AI) models are vital in promoting the accessibility of real-time medical services in underdeveloped areas. The recent Once-For-All (OFA) network (without retraining) can directly produce a set of sub-network designs with Progressive Shrinking (PS) algorithm; however, the training resource and time inefficiency downfalls are apparent in this method. In this paper, we propose a new OFA training algorithm, namely the Progressive Expansion (ProX) to train the medical image analysis model. It is a reversed paradigm to PS, where technically we train the OFA network from the minimum configuration and gradually expand the training to support larger configurations. Empirical results showed that the proposed paradigm could reduce training time up to 68%; while still being able to produce sub-networks that have either similar or better accuracy compared to those trained with OFA-PS on ROCT (classification), BRATS and Hippocampus (3D-segmentation) public medical datasets. The code implementation for this paper is accessible at: https://github.com/shin-wl/ProX-OFA.}
}
@article{ASAI1991323,
title = {Discipline Pascal with descriptive environment; precise writing to learn programming and to avoid errors},
journal = {Computers & Education},
volume = {16},
number = {4},
pages = {323-335},
year = {1991},
issn = {0360-1315},
doi = {https://doi.org/10.1016/0360-1315(91)90006-D},
url = {https://www.sciencedirect.com/science/article/pii/036013159190006D},
author = {Hitohisa Asai},
abstract = {Dijkstra's article “On the cruelty of really teaching computing science” [l] has encouraged me to review my thoughts and experience. In the teaching environments of an introductory programming course (CS1, CS2 and others), I have discovered a mental gap in the minds of some students, which has often led to difficulties in class. A Pascal statement contains highly condensed information. In order to deal with this type of information, the students must apply a certain thinking level. In a Pascal statement, much information is concealed behind the written words. For example, consider a variable in a program. The data type and locality of the variable are not explicitly expressed in a statement. In other words, this condensed information is hiding, e.g. invisible. Hence a degree of the student's ability to focus on it may fade away in his mind. If programming code demands writing precise information in a statement then the student's difficulty may be alleviated because he can see it. This would be an attempt to narrow the gap by writing Discipline Pascal code closer to the precise thinking level. I believe that this proposal would also bring benefits to experienced programmers.}
}
@article{SINCLAIR2004169,
title = {Improving computer-assisted instruction in teaching higher-order skills},
journal = {Computers & Education},
volume = {42},
number = {2},
pages = {169-180},
year = {2004},
issn = {0360-1315},
doi = {https://doi.org/10.1016/S0360-1315(03)00070-8},
url = {https://www.sciencedirect.com/science/article/pii/S0360131503000708},
author = {Kelsey J Sinclair and Carl E Renshaw and Holly A Taylor},
keywords = {Evaluation methodologies, Multimedia/hypermedia systems, Secondary education, Applications in subject areas},
abstract = {Computer-assisted instruction (CAI) has been shown to enhance rote memory skills and improve higher order critical thinking skills. The challenge now is to identify what aspects of CAI improve which specific higher-order skills. This study focuses on the effectiveness of using CAI to teach logarithmic graphing and dimensional analysis. Two groups of ninth graders participated in a one-class period laboratory. Experiment 1 compared a fully automated computer laboratory to an equivalent paper-and-pencil exercise. Experiment 2 compared the same automated computer laboratory in Experiment 1 with a revised, less automated computer version. Both the paper-and-pencil exercise and the less automated computer exercise required students to perform basic mathematical calculations. The results from a post-test revealed that very few students were able to master the complex task of dimensional analysis, but students who took the paper-based and revised, less automated version scored higher overall. These results imply that students required to perform basic calculations had a better understanding of the lab as a whole. These results suggest that until students master basic skills, they do not have the cognitive resources to concentrate on higher-order concepts. This is supported by cognitive load theory.}
}
@article{NAIK2017509,
title = {Metastability in Senescence},
journal = {Trends in Cognitive Sciences},
volume = {21},
number = {7},
pages = {509-521},
year = {2017},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2017.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364661317300797},
author = {Shruti Naik and Arpan Banerjee and Raju S. Bapi and Gustavo Deco and Dipanjan Roy},
keywords = {healthy aging, whole-brain computational modeling, metastability},
abstract = {The brain during healthy aging exhibits gradual deterioration of structure but maintains a high level of cognitive ability. These structural changes are often accompanied by reorganization of functional brain networks. Existing neurocognitive theories of aging have argued that such changes are either beneficial or detrimental. Despite numerous empirical investigations, the field lacks a coherent account of the dynamic processes that occur over our lifespan. Taking advantage of the recent developments in whole-brain computational modeling approaches, we hypothesize that the continuous process of aging can be explained by the concepts of metastability − a theoretical framework that gives a systematic account of the variability of the brain. This hypothesis can bridge the gap between existing theories and the empirical findings on age-related changes.}
}
@article{PAN2024102334,
title = {Novel blockchain deep learning framework to ensure video security and lightweight storage for construction safety management},
journal = {Advanced Engineering Informatics},
volume = {59},
pages = {102334},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102334},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623004627},
author = {Xing Pan and Luoxin Shen and Botao Zhong and Da Sheng and Fang Huang and Luhan Yang},
keywords = {Construction safety management, Video security storage, Blockchain, Deep learning, Video summarization, Pre-defined rule},
abstract = {In construction management, video data tampering behavior like manual forging and deletion can negatively impact on-site safety and accident accountability. Blockchain technology holds the potential to address this issue by leveraging distributed ledger characteristics. However, blockchain's limited storage capacity and block size make it difficult to upload large-sized construction data such as daily monitoring video. Furthermore, it is unnecessary to store all construction data in any case. Therefore, this study proposes a blockchain deep learning framework that focuses on how to efficiently extract and securely store key information (i.e., video summarization that involves worker’s unsafe behavior) on-blockchain for data traceability. The framework involves a novel data-driven and rule-based keyframe extraction (DRKE) model to lightweight large-sized construction video in the nascent field of deep learning and blockchain combination. To define parameters for the DRKE model, specific construction rules (e.g., people’s unsafe behavior-based and people-based rules) have been pre-defined. This framework has been evaluated, and the results demonstrate its capability for effective video security storage, facilitating practical needs in construction management. The study extends existing research and provides a practical solution for large-sized construction video storage with security and lightweight considerations. The proposed video security storage and data lightweight process offers substantial benefits to construction management, such as streamlined accident investigation and accountability and improved on-site work efficiency, contributing to the smooth progress of construction projects.}
}
@article{RINGE2023101268,
title = {Cation effects on electrocatalytic reduction processes at the example of the hydrogen evolution reaction},
journal = {Current Opinion in Electrochemistry},
volume = {39},
pages = {101268},
year = {2023},
issn = {2451-9103},
doi = {https://doi.org/10.1016/j.coelec.2023.101268},
url = {https://www.sciencedirect.com/science/article/pii/S2451910323000613},
author = {Stefan Ringe},
keywords = {Cation effects, Hydrogen evolution reaction, Hydrogen underpotential deposition, CO reduction, Electric double layer, Solid-liquid interface},
abstract = {Cation effects provide invaluable insights into electrochemistry. In this review, I discuss them with a main focus on the hydrogen evolution reaction and a summary of recent in situ spectroscopic and electrochemical measurements as well as advanced computational simulation results conducted at varying cation identities, concentrations, and pH. According to these works, the interfacial cation concentration is the main descriptor to explain cation and pH effects. The detailed mechanism (such as e.g. water polarization, water structure changes, field-stabilization of intermediates) depends strongly on potential, pH, oxophilicity of the electrode, or the nature of the rate-limiting step and proton donor. With growing convergence in this field, cation effects remain a highly challenging and promising topic for research.}
}
@article{RAHAMAN2021114633,
title = {An efficient multilevel thresholding based satellite image segmentation approach using a new adaptive cuckoo search algorithm},
journal = {Expert Systems with Applications},
volume = {174},
pages = {114633},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114633},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421000749},
author = {Jarjish Rahaman and Mihir Sing},
keywords = {Thresholding, Segmentation, Otsu’s inter-class variance, Tsallis entropy, Cuckoo search algorithm, McCulloch’s method, A new adaptive cuckoo search algorithm},
abstract = {It is the most challenging and difficult task to segment a satellite image because of its complete randomness, multiple regions of interest, weak correlation with pixels, and regions of ambiguity. There are several Nature-inspired algorithms available, which are used to overcome these difficulties and those are more efficient to generate the best threshold value for the segmentation of satellite images. Though various modern methodologies opt for better results but methods have some drawbacks too like techniques are computationally expensive and time-consuming. In this paper, we have proposed a more effective satellite image segmentation approach using a new adaptive cuckoo search (ACS) algorithm. The result obtained from the projected technique is compared with CSMcCulloch incorporating McCulloch’s method for levy flight generation in Cuckoo Search (CS) algorithm by using two different objective functions namely Otsu’s method and Tsallis entropy function. The measurement techniques such as PSNR, MSE, FSIM, SSIM, UIQI, and computational time in term of CPU running time have been considered for validating and evaluating the proposed method. This proposed algorithm technique has resulted in improve segmentation quality of satellite images and reduced computational time. The analysis of the convergence rate proves that ACS is superior to the CSMcCulloch algorithm for reaching the global convergence rate. These experimental outcomes help to encourage researchers in different domains such as computer vision, application of medical image analysis, machine learning as well as deep learning.}
}
@article{MARZANO20231028,
title = {Manufacturing Ergonomics Improvements in Distillery Industry Using Digital Tools},
journal = {Procedia CIRP},
volume = {118},
pages = {1028-1032},
year = {2023},
note = {16th CIRP Conference on Intelligent Computation in Manufacturing Engineering},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.06.176},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123004031},
author = {Adelaide Marzano},
keywords = {Digital manufacturing system, ergonomics, design},
abstract = {This paper presents the steps taken by distilleries to uphold years old traditions and how new design tools can streamlined the current manufacturing processes. Different methods for bung removal are explored and how they are used today within warehouses and distilleries worldwide. The aim is to test new designs to replace the current tools used in distillery process to perform heavily manual tasks. Models of the current and new design are produced, and both are tested in a digital environment for ergonomics and time efficiency purposes.}
}
@article{MONACHESI2021101531,
title = {Building the sustainable city through Twitter: Creative skilled migrants and innovative technology use},
journal = {Telematics and Informatics},
volume = {58},
pages = {101531},
year = {2021},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2020.101531},
url = {https://www.sciencedirect.com/science/article/pii/S0736585320301908},
author = {Paola Monachesi and Saskia Witteborn},
keywords = {Creative migrants, Smart city, Social media, Sustainability, Technology, Twitter},
abstract = {We investigate the role of creative skilled migrants in broadcasting an alternative use of technology in support of a sustainable smart city. We do so by analyzing the themes they produced on Twitter. We focus on Amsterdam as a case, and urban planners and designers as examples of creative migrants. Computational methodology allowed for a selection of naturally occurring data in social media. We show that the creative migrants actively contribute to shaping the smart-sustainable city through the themes of top-down technological solutions and bottom-up participation by highlighting innovative uses of technology in support of the environment and citizens’ needs. However, the migrants do not question received historical and geopolitical power constellations. Moreover, they propose the Western city as a role model for solving pressing urban problems.}
}
@article{QIAN2024117679,
title = {A novel dataset and feature selection for data-driven conceptual design of offshore jacket substructures},
journal = {Ocean Engineering},
volume = {303},
pages = {117679},
year = {2024},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2024.117679},
url = {https://www.sciencedirect.com/science/article/pii/S0029801824010163},
author = {Han Qian and Emmanouil Panagiotou and Mengyan Peng and Eirini Ntoutsi and Chongjie Kang and Steffen Marx},
keywords = {Offshore jacket substructure, Conceptual design, Data-driven method, Machine learning, Dataset, Feature selection},
abstract = {Conceptual design is crucial for designing offshore jacket substructures because it sets the direction for the entire design process. Nevertheless, conventional simulation-based optimization methods for jacket conceptual design face challenges, such as high computational costs and restricted optimization objectives. This paper proposes a data-driven method for offshore jacket conceptual design using machine learning (ML). First, a novel dataset of completed and under-construction jackets worldwide was established as the cornerstone of ML. The dataset comprised “in-action” data capturing key structural parameters of jackets and information on design boundary conditions. Subsequently, different features were comprehensively selected to identify and visualize their correlations for an interpretable data-driven design, ensuring the effectiveness of the dataset for training the ML models. Finally, random forest and eXtreme gradient boosting models were trained on the data from the selected feature subsets and then employed to predict individual jacket structural parameters. The predictive performance of the models indicates that the dataset and feature selection can capture the fundamental and shared characteristics of well-designed jackets, thereby improving the accuracy and efficiency of the conceptual design process. This study suggests the potential of a data-driven conceptual design for offshore jacket substructures.}
}
@article{JIA2011445,
title = {Evolutionary level set method for structural topology optimization},
journal = {Computers & Structures},
volume = {89},
number = {5},
pages = {445-454},
year = {2011},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2010.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0045794910002567},
author = {Haipeng Jia and H.G. Beom and Yuxin Wang and Song Lin and Bo Liu},
keywords = {Evolutionary structure optimization, Structure topology optimization, Intelligent computation, Level set method},
abstract = {This paper proposes an evolutionary accelerated computational level set algorithm for structure topology optimization. It integrates the merits of evolutionary structure optimization (ESO) and level set method (LSM). Traditional LSM algorithm is largely dependent on the initial guess topology. The proposed method combines the merits of ESO techniques with those of LSM algorithm, while allowing new holes to be automatically generated in low strain energy within the nodal neighboring region during optimization. The validity and robustness of the new algorithm are supported by some widely used benchmark examples in topology optimization. Numerical computations show that optimization convergence is accelerated effectively.}
}
@article{RENDONCASTRILLON2023104,
title = {Training strategies from the undergraduate degree in chemical engineering focused on bioprocesses using PBL in the last decade},
journal = {Education for Chemical Engineers},
volume = {44},
pages = {104-116},
year = {2023},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2023.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S1749772823000258},
author = {Leidy Rendón-Castrillón and Margarita Ramírez-Carmona and Carlos Ocampo-López},
keywords = {Research hotbed, Biotechnology, Green chemistry, Circular economy, Sustainability, ABET, Engineering education},
abstract = {Global engineering education addresses the development of professional competencies in undergraduates to prepare professionals capable of solving complex technical problems under social, environmental, and economic challenges. In this work, training was carried out to incorporate the bioprocess research of the chemical engineering students at Universidad Pontificia Bolivariana in Medellin, Colombia, using a project-based learning methodology (PBL). An open call was made to the students, and they were challenged to build a prototype which they had to support together with a written report as evidence for their admission to the research hotbed and assign them research projects in bioprocesses. In the last decade, 276 students participated in the hotbed generating 21 conference presentations, four software, 14 research articles, and 16 academic awards. In parallel, a survey was conducted to analyze the perception of graduates participating in the hotbed according to a list of 17 competency criteria relevant to the chemical engineering program. It was found that the average perception is at the highest levels (4−5), which indicates that most of the graduates value the significant contribution made by the CIBIOT hotbed to the development of a professional in experimentation, communication, and acquisition of new knowledge.}
}
@article{HINZEN2024110952,
title = {The ‘L-factor’: Language as a transdiagnostic dimension in psychopathology},
journal = {Progress in Neuro-Psychopharmacology and Biological Psychiatry},
volume = {131},
pages = {110952},
year = {2024},
issn = {0278-5846},
doi = {https://doi.org/10.1016/j.pnpbp.2024.110952},
url = {https://www.sciencedirect.com/science/article/pii/S0278584624000204},
author = {Wolfram Hinzen and Lena Palaniyappan},
keywords = {Thought, Psychosis, Neurocognition, Psychopathology, Brain networks},
abstract = {Thoughts and moods constituting our mental life incessantly change. When the steady flow of this dynamics diverges in clinical directions, the possible pathways involved are captured through discrete diagnostic labels. Yet a single vulnerable neurocognitive system may be causally involved in psychopathological deviations transdiagnostically. We argue that language viewed as integrating cortical functions is the best current candidate, whose forms of breakdown along its different dimensions are then manifest as symptoms – from prosodic abnormalities and rumination in depression to distortions of speech perception in verbal hallucinations, distortions of meaning and content in delusions, or disorganized speech in formal thought disorder. Spontaneous connected speech provides continuous objective readouts generating a highly accessible bio-behavioral marker with the potential of revolutionizing neuropsychological measurement. This argument turns language into a transdiagnostic ‘L-factor’ providing an analytical and mechanistic substrate for previously proposed latent general factors of psychopathology (‘p-factor’) and cognitive functioning (‘c-factor’). Together with immense practical opportunities afforded by rapidly advancing natural language processing (NLP) technologies and abundantly available data, this suggests a new era of translational clinical psychiatry, in which both psychopathology and language may be rethought together.}
}
@article{SURESHBABU2006277,
title = {Modeling and simulation in signal transduction pathways: a systems biology approach},
journal = {Biochimie},
volume = {88},
number = {3},
pages = {277-283},
year = {2006},
issn = {0300-9084},
doi = {https://doi.org/10.1016/j.biochi.2005.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0300908405001999},
author = {C.V. {Suresh Babu} and Eun {Joo Song} and Young Sook Yoo},
keywords = {Biological systems, Signal transduction, Systems biology, Modeling and simulation},
abstract = {Modeling, the heart of systems biology, of complex processes (example: signal transduction) is a wide scientific discipline where many approaches from different areas are confronted with the aim of better understanding, identifying and modeling of complex data coming from various sources. The purpose of this paper is to introduce the basic steps of systems biology view towards signaling pathways, which mainly deals with the computational tools. The paper emphasizes the modeling and simulation approach in the signal transduction pathways using the topologies of the biochemical reactions with an overview of the different types of software platforms. Finally, we demonstrated the epidermal growth factor receptor signaling pathway model as an example to study the growth factor mediated signaling system with biological experiments. This paper will enables new comers to underline the strengths of the computational approaches towards signal transduction, as well as to highlight the systems biology research directions.}
}
@article{WANG201715,
title = {An overview on the roles of fuzzy set techniques in big data processing: Trends, challenges and opportunities},
journal = {Knowledge-Based Systems},
volume = {118},
pages = {15-30},
year = {2017},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2016.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950705116304452},
author = {Hai Wang and Zeshui Xu and Witold Pedrycz},
keywords = {Big data, Data-intensive science, Fuzzy sets, Fuzzy logic, Granular computing},
abstract = {In the era of big data, we are facing with an immense volume and high velocity of data with complex structures. Data can be produced by online and offline transactions, social networks, sensors and through our daily life activities. A proper processing of big data can result in informative, intelligent and relevant decision making completed in various areas, such as medical and healthcare, business, management and government. To handle big data more efficiently, new research paradigm has been engaged but the ways of thinking about big data call for further long-term innovative pursuits. Fuzzy sets have been employed for big data processing due to their abilities to represent and quantify aspects of uncertainty. Several innovative approaches within the framework of Granular Computing have been proposed. To summarize the current contributions and present an outlook of further developments, this overview addresses three aspects: (1) We review the recent studies from two distinct views. The first point of view focuses on what types of fuzzy set techniques have been adopted. It identifies clear trends as to the usage of fuzzy sets in big data processing. Another viewpoint focuses on the explanation of the benefits of fuzzy sets in big data problems. We analyze when and why fuzzy sets work in these problems. (2) We present a critical review of the existing problems and discuss the current challenges of big data, which could be potentially and partially solved in the framework of fuzzy sets. (3) Based on some principles, we infer the possible trends of using fuzzy sets in big data processing. We stress that some more sophisticated augmentations of fuzzy sets and their integrations with other tools could offer a novel promising processing environment.}
}
@article{CIMBUROVA2023127839,
title = {Making trees visible: A GIS method and tool for modelling visibility in the valuation of urban trees},
journal = {Urban Forestry & Urban Greening},
volume = {81},
pages = {127839},
year = {2023},
issn = {1618-8667},
doi = {https://doi.org/10.1016/j.ufug.2023.127839},
url = {https://www.sciencedirect.com/science/article/pii/S1618866723000109},
author = {Zofie Cimburova and Stefan Blumentrath and David N. Barton},
keywords = {Cultural ecosystem services, GIS, Tree valuation, Urban trees, Visibility analysis},
abstract = {Tree visibility is a key determinant of cultural ecosystem services of urban trees. This paper develops a flexible, efficient and easy-to-use GIS method for modelling individual tree visibility to support tree valuation. The method is implemented as a GRASS GIS AddOn tool called v.viewshed.impact, making it available to a broad spectrum of users and purposes. Thanks to empirically validated underlying algorithms and parallel processing, the method is accurate and fast in analysing high-resolution datasets and large numbers of trees. We demonstrate the method in two use cases in Oslo, Norway, showing that it provides an alternative to field-based assessment of visibility indicators in tree valuation methods and facilitates the inclusion of complex visibility indicators not possible to assess in the field. We argue that the method could also be used for tree management and planning, urban ecosystem accounting and neighbour conflict resolution related to trees.}
}
@incollection{WOOLLISCROFT2020153,
title = {Chapter 12 - Precision medicine},
editor = {James O. Woolliscroft},
booktitle = {Implementing Biomedical Innovations into Health, Education, and Practice},
publisher = {Academic Press},
pages = {153-167},
year = {2020},
isbn = {978-0-12-819620-5},
doi = {https://doi.org/10.1016/B978-0-12-819620-5.00012-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128196205000126},
author = {James O. Woolliscroft},
keywords = {Precision medicine, Environment, Behavior, Microbiome, Genome, Pharmacogenomics},
abstract = {The convergence of computational, technologic and biomedical advances has enabled the development of precision medicine. Growing out of an understanding that there is a need for a new taxonomy of disease, the vision for precision medicine is to better understand the complex relationships in health and disease through the assemblage of massive databases that include individuals’ genomes, microbiomes, exposomes (a subsection of the environment), epigenomes, physiologic data, signs and symptoms, and other relevant information. Through the development of a holistic picture of genomic, microbiota, environmental and behavioral factors leading to disease, the intent is to intervene before disease becomes manifest to maintain or restore to health. Precision medicine will drive not only disruptive changes in the practice of clinical medicine, but also changes in our very conceptualization of health and disease.}
}
@article{CLARKE2009460,
title = {The mediating effects of coping strategies in the relationship between automatic negative thoughts and depression in a clinical sample of diabetes patients},
journal = {Personality and Individual Differences},
volume = {46},
number = {4},
pages = {460-464},
year = {2009},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2008.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S0191886908004285},
author = {Dave Clarke and Tanya Goosen},
keywords = {Automatic thoughts, Cognitive behaviour therapy, Coping, Depression, Diabetes},
abstract = {High levels of depression have been found among diabetes patients, but few studies have examined the influence of coping strategies on the relationship between diabetics’ negative thoughts and their depression. The purpose of the study was to investigate the effects of coping strategies as mediators in the path from automatic negative thoughts to depression. A questionnaire containing the Automatic Thoughts Questionnaire, the Ways of Coping Checklist, a depression inventory and demographic questions was completed by 57 male and 57 female New Zealand diabetic patients, aged 28–88 years (median=60.5, mean=59.3, SD=14.6). Automatic negative thoughts, emotion-focused coping and depression, but not problem-focused coping, were significantly correlated, after controlling for relevant demographic and diabetes variables. Hierarchical linear regression analysis of data showed that emotion-focused coping functioned as a partial mediator between negative thoughts and depression. Cognitive therapy was suggested to control both automatic negative thoughts and emotion-focused coping behaviours of self-blame, wishful thinking and avoidance.}
}
@incollection{MORGERA1986389,
title = {COMPUTATIONAL COMPLEXITY AND VLSI IMPLEMENTATION OF AN OPTIMAL FEATURE SELECTION STRATEGY††Work supported by Canada NSERC Grant AO912.},
editor = {Edzard S. GELSEMA and Laveen N. KANAL},
booktitle = {Pattern Recognition in Practice},
publisher = {Elsevier},
address = {Amsterdam},
pages = {389-400},
year = {1986},
isbn = {978-0-444-87877-9},
doi = {https://doi.org/10.1016/B978-0-444-87877-9.50036-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780444878779500364},
author = {Salvatore D. Morgera}
}
@article{JIANG2020556,
title = {Energy aware edge computing: A survey},
journal = {Computer Communications},
volume = {151},
pages = {556-580},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S014036641930831X},
author = {Congfeng Jiang and Tiantian Fan and Honghao Gao and Weisong Shi and Liangkai Liu and Christophe Cérin and Jian Wan},
keywords = {Edge computing, Energy efficiency, Computing offloading, Benchmarking, Computation partitioning},
abstract = {Edge computing is an emerging paradigm for the increasing computing and networking demands from end devices to smart things. Edge computing allows the computation to be offloaded from the cloud data centers to the network edge and edge nodes for lower latency, security and privacy preservation. Although energy efficiency in cloud data centers has been broadly investigated, energy efficiency in edge computing is largely left uninvestigated due to the complicated interactions between edge devices, edge servers, and cloud data centers. In order to achieve energy efficiency in edge computing, a systematic review on energy efficiency of edge devices, edge servers, and cloud data centers is required. In this paper, we survey the state-of-the-art research work on energy-aware edge computing, and identify related research challenges and directions, including architecture, operating system, middleware, applications services, and computation offloading.}
}
@incollection{DU202457,
title = {Chapter 3 - Data, machine learning, first-principles, and hybrid models in the petrochemical industry},
editor = {Masoud Soroush and Richard {D Braatz}},
booktitle = {Artificial Intelligence in Manufacturing},
publisher = {Academic Press},
pages = {57-96},
year = {2024},
isbn = {978-0-323-99135-3},
doi = {https://doi.org/10.1016/B978-0-323-99135-3.00011-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323991353000117},
author = {Di Du and Johannes Pieter Schmal},
keywords = {Data, Data types, Data-driven models, First-principles models, Hybrid models, Machine learning},
abstract = {With the increase in computational power, memory, data storage, and data availability models have become more abundant and powerful, leading to many process improvements and benefits in the petrochemical industry. In this chapter, we will first discuss the data types based on dimensions. We then discuss different machine-learning approaches and other data-driven approaches with applications in the petrochemical industry. First-principles and hybrid modeling approaches are also discussed and compared with machine-learning approaches.}
}
@article{LOPEZ2015289,
title = {DAMQT 2.0: A new version of the DAMQT package for the analysis of electron density in molecules},
journal = {Computer Physics Communications},
volume = {192},
pages = {289-294},
year = {2015},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2015.02.027},
url = {https://www.sciencedirect.com/science/article/pii/S0010465515000855},
author = {Rafael López and Jaime Fernández Rico and Guillermo Ramírez and Ignacio Ema and David Zorrilla},
keywords = {Electron density, Electrostatic potential, Electric field, Hellmann–Feynman forces, Density deformations},
abstract = {DAMQT 2.0 is a new version of the DAMQT package for the analysis of electron density in molecules and the fast computation of the density, density deformations, electrostatic potential and field, and Hellmann–Feynman forces. Algorithms for the partition of the electron density and the computation of related properties like density deformations, electrostatic potential and field and Hellmann–Feynman forces have been improved and their codes, fully rewritten. MPI versions of the most computational demanding modules are now included in the package for parallel computation. The Graphical User Interface has been also enhanced, with new features including a 2D plotter and significant improvements in the 3D viewer.
Program summary
Program title: DAMQT 2.0 Catalogue identifier: AEDL_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEDL_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: GPLv3 No. of lines in distributed program, including test data, etc.: 317,270 No. of bytes in distributed program, including test data, etc.: 40,193,220 Distribution format: tar.gz Programming language: Fortran90 and C++. Computer: Any. Operating system: Linux, Windows (7, 8). RAM: 200 Mbytes Classification: 16.1. Catalogue identifier of previous version: AEDL_v1_0 Journal reference of previous version: Comput. Phys. Comm. 180(2009)1654 External routines: Qt (4.8 or higher), OpenGL (3.x or higher), freeGLUT 2.8.x Nature of problem: Analysis of the molecular electron density and density deformations, including fast evaluation of electrostatic potential, electric field and Hellmann–Feynman forces on nuclei. Solution method: The method of Deformed Atoms in Molecules, reported elsewhere [1], is used for partitioning the molecular electron density into atomic fragments, which are further expanded in spherical harmonics times radial factors. The partition is used for defining molecular density deformations and for the fast calculation of several properties associated to density. Restrictions: Density must come from an LCAO calculation (any level) with spherical (not Cartesian) Slater or Gaussian functions. Unusual features: The program contains an OPEN statement to binary files (stream) in several files. This statement has not a standard syntax in Fortran 90. Two possibilities are considered in conditional compilation: Intel’s ifort and Fortran2003 standard. The latter is applied to compilers other than ifort (gfortran uses this one, for instance). Additional comments: Quick-start guide and User’s manual in PDF format included in the package. User’s manual is also accessible from the Graphical User Interface. The distribution file for this program is over 40 Mbytes and therefore is not delivered directly when downloaded or Email is requested. Instead an html file giving details of how the program can be obtained is sent. Running time: Largely dependent on the system size and the module run (from fractions of a second to hours). References:[1]J. Fernández Rico, R. López, I. Ema and G. Ramírez, J. Mol. Struct. Theochem 727 (2005) 115.}
}
@article{ZHANG20231815,
title = {An intention inference method for the space non-cooperative target based on BiGRU-Self Attention},
journal = {Advances in Space Research},
volume = {72},
number = {5},
pages = {1815-1828},
year = {2023},
issn = {0273-1177},
doi = {https://doi.org/10.1016/j.asr.2023.04.032},
url = {https://www.sciencedirect.com/science/article/pii/S0273117723003101},
author = {Honglin Zhang and Jianjun Luo and Yuan Gao and Weihua Ma},
keywords = {Space non-cooperative target, Intention inference, Time series, BiGRU, Self-attention mechanism},
abstract = {Intention inference for space non-cooperative targets is the key to space situational awareness and assistant decision for collision avoidance. Given that the problem of target intention inference is essential to learn the dynamically changing time-series characteristics of space non-cooperative target intentions and infer their relative motion patterns for threat warning, this paper adopts a deep learning-based approach, introduces a bidirectional propagation mechanism and self-attention mechanism based on Gated Recurrent Unit (GRU) and proposes a bidirectional Gated Recurrent Unit (BiGRU)-Self Attention-based space non-cooperative target intention inference model. BiGRU is used to learn deep information in time-series characteristics of the space non-cooperative target, and self-attention mechanism is used to adaptively extract and assign weights to key characteristics to capture the internal correlations in time-series information, thus improving model performance. The line-of-sight measurements are used as the characteristics of target intention inference, and the typical target motion intentions are defined. Subsequently, the proposed model is trained and tested on the test set, with the accuracy reaching 97.1%. Besides, the effectiveness and advantages of the proposed model are verified by the simulation of a case study and comparison evaluations. The results demonstrate that our proposed model could significantly improve the accuracy, computational efficiency, and noise resistance for the space non-cooperative target intention inference compared with the existing intention inference models.}
}
@article{ZHAO201568,
title = {Approximate methods for optimal replacement, maintenance, and inspection policies},
journal = {Reliability Engineering & System Safety},
volume = {144},
pages = {68-73},
year = {2015},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2015.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0951832015001957},
author = {Xufeng Zhao and Khalifa N. Al-Khalifa and Toshio Nakagawa},
keywords = {Hazard function, Mean time to failure, Age replacement, Imperfect maintenance, Approximate inspection},
abstract = {It might be difficult sometimes to derive theoretical and numerical solutions for analytical maintenance modelings due to the computational complexity. This paper takes up several approximate models in maintenance theory, by using the cumulative hazard function H(t) and the newly proposed asymptotic MTTF (Mean Time to Failure) skilfully. We firstly denote by tx the time when the expected number of failures is x. Using H(tx)=x, we estimate failure times, model age and periodic replacements, and sequential imperfect maintenance. Motivated by the asymptotic method of computation of MTTF, we secondly model the expected cost rate for a parallel system when replacement is made at system failure, and give approximate computations for the sequential inspection policy. Optimizations of each model are obtained approximately in an easier way. When failure times have a Weibull distribution, it is shown from numerical examples that the obtained approximate optimal solutions have good approximations of the exact ones.}
}
@article{KADERAVEK201527,
title = {SCIIENCE: The creation and pilot implementation of an NGSS-based instrument to evaluate early childhood science teaching},
journal = {Studies in Educational Evaluation},
volume = {45},
pages = {27-36},
year = {2015},
issn = {0191-491X},
doi = {https://doi.org/10.1016/j.stueduc.2015.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0191491X15000218},
author = {Joan N. Kaderavek and Tamala North and Regina Rotshtein and Hoangha Dao and Nicholas Liber and Geoff Milewski and Scott C. Molitor and Charlene M. Czerniak},
keywords = {Discourse analysis, Teacher assessment, Language of science in classrooms, Validity/reliability},
abstract = {This paper describes the development, testing and implementation of the Systematic Characterization of Inquiry Instruction in Early LearNing Classroom Environments (SCIIENCE). The SCIIENCE instrument was designed to capture best practices outlined in the National Research Council's Framework for K-12 Science Education as they occur within a science lesson. The goals of the SCIIENCE instrument are to (a) assess the quality of science instruction in PK-3 classrooms, (b) capture teacher behaviors and instructional practices that engage students in the lesson, promote scientific studies, encourage higher-level thinking, and (c) provide a feedback mechanism for guiding professional development of PK-3 teachers. Science educators can apply this instrument to teacher behaviors and use the data to improve classroom inquiry instructional methodology.}
}
@article{PROKOPENKO2019134,
title = {Self-referential basis of undecidable dynamics: From the Liar paradox and the halting problem to the edge of chaos},
journal = {Physics of Life Reviews},
volume = {31},
pages = {134-156},
year = {2019},
note = {Physics of Mind},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2018.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S1571064519300077},
author = {Mikhail Prokopenko and Michael Harré and Joseph Lizier and Fabio Boschetti and Pavlos Peppas and Stuart Kauffman},
keywords = {Self-reference, Diagonalization, Undecidability, Incomputability, Program-data duality, Complexity},
abstract = {In this paper we explore several fundamental relations between formal systems, algorithms, and dynamical systems, focussing on the roles of undecidability, universality, diagonalization, and self-reference in each of these computational frameworks. Some of these interconnections are well-known, while some are clarified in this study as a result of a fine-grained comparison between recursive formal systems, Turing machines, and Cellular Automata (CAs). In particular, we elaborate on the diagonalization argument applied to distributed computation carried out by CAs, illustrating the key elements of Gödel's proof for CAs. The comparative analysis emphasizes three factors which underlie the capacity to generate undecidable dynamics within the examined computational frameworks: (i) the program-data duality; (ii) the potential to access an infinite computational medium; and (iii) the ability to implement negation. The considered adaptations of Gödel's proof distinguish between computational universality and undecidability, and show how the diagonalization argument exploits, on several levels, the self-referential basis of undecidability.}
}
@article{JIANG2024100078,
title = {Human-AI interaction research agenda: A user-centered perspective},
journal = {Data and Information Management},
pages = {100078},
year = {2024},
issn = {2543-9251},
doi = {https://doi.org/10.1016/j.dim.2024.100078},
url = {https://www.sciencedirect.com/science/article/pii/S2543925124000147},
author = {Tingting Jiang and Zhumo Sun and Shiting Fu and Yan Lv},
keywords = {Human-AI interaction, Human-AI collaboration, Human-AI competition, Human-AI conflict, Human-AI symbiosis},
abstract = {The rapid growth of artificial intelligence (AI) has given rise to the field of Human-AI Interaction (HAII). This study meticulously reviewed the research themes, theoretical foundations, and methodological frameworks of the HAII field, aiming to construct a comprehensive overview of this field and provide robust support for future investigations. HAII research themes include human-AI collaboration, competition, conflict, and symbiosis. Theories drawn from communication, psychology, and sociology support these studies, while the employed methods include both self-reporting and observational approaches commonly utilized in user studies. It is suggested that future research should broaden its focus to encompass diverse user groups, AI roles, and tasks. Moreover, it is necessary to develop multi-disciplinary theories and integrate multi-level research methods to support the sustained development of the field. This study not only furnishes indispensable theoretical and practical insights for forthcoming research endeavors but also catalyzes the realization of a future distinguished by seamless interaction between humans and AI.}
}
@incollection{SADEGHI2024457,
title = {Chapter Thirteen - Dynamic framework for large-scale modeling of membranes and peripheral proteins},
editor = {Markus Deserno and Tobias Baumgart},
series = {Methods in Enzymology},
publisher = {Academic Press},
volume = {701},
pages = {457-514},
year = {2024},
booktitle = {Biophysical Approaches for the Study of Membrane Structure—Part B: Theory and Simulations},
issn = {0076-6879},
doi = {https://doi.org/10.1016/bs.mie.2024.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S0076687924001137},
author = {Mohsen Sadeghi and David Rosenberger},
keywords = {Membranes, Mesoscopic modeling, Particle-based modeling, Hydrodynamics, Membrane-protein interaction},
abstract = {In this chapter, we present a novel computational framework to study the dynamic behavior of extensive membrane systems, potentially in interaction with peripheral proteins, as an alternative to conventional simulation methods. The framework effectively describes the complex dynamics in protein-membrane systems in a mesoscopic particle-based setup. Furthermore, leveraging the hydrodynamic coupling between the membrane and its surrounding solvent, the coarse-grained model grounds its dynamics in macroscopic kinetic properties such as viscosity and diffusion coefficients, marrying the advantages of continuum- and particle-based approaches. We introduce the theoretical background and the parameter-space optimization method in a step-by-step fashion, present the hydrodynamic coupling method in detail, and demonstrate the application of the model at each stage through illuminating examples. We believe this modeling framework to hold great potential for simulating membrane and protein systems at biological spatiotemporal scales, and offer substantial flexibility for further development and parametrization.}
}
@article{MCGEE201440,
title = {The pragmatics of paragraphing English argumentative text},
journal = {Journal of Pragmatics},
volume = {68},
pages = {40-72},
year = {2014},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2014.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0378216614000770},
author = {Iain McGee},
keywords = {Paragraphing, Lexical cohesion, Argumentative text, Textual colligation, Foregrounding, Discourse signaling, Rhetorical devices, Computational Linguistics},
abstract = {Computational linguistic work into the paragraph and paragraphing has highlighted the significant role that intra-paragraph lexical cohesion plays in ‘marking off’ one paragraph unit from another. The goal of the research reported on in this paper is to consider, in some detail, the relationship that exists between the lexical repetition patterns in an argumentative text (as identified by a computational procedure), the genre moves within it, the actual paragraphing of the texts, and the textual colligation features of the paragraphs. The Link Set Median procedure (Berber-Sardinha, 1997, Berber-Sardinha, 2001, Berber-Sardinha, 2002) is used to document exact, inflectional and derivational lexical repetition usage across 10 short English argumentative texts, and to predict where segmentations originally occurred in the texts. The resulting data are then analyzed in the light of diverse research interests into the paragraph, and classified accordingly. A comparison of these results is made with data where there is either a marginal or no difference in the link set medians of adjacent sentences across paragraph junctures within the same texts. It is suggested that this novel approach of analyzing computational data from multiple paragraph-specific research interests results in a clearer picture of paragraphing practice emerging.}
}
@article{KNUUTTILA201476,
title = {Varieties of noise: Analogical reasoning in synthetic biology},
journal = {Studies in History and Philosophy of Science Part A},
volume = {48},
pages = {76-88},
year = {2014},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2014.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0039368114000612},
author = {Tarja Knuuttila and Andrea Loettgers},
keywords = {Synthetic biology, Interdisciplinarity, Analogical reasoning, Engineering sciences, Complex systems, Noise},
abstract = {The picture of synthetic biology as a kind of engineering science has largely created the public understanding of this novel field, covering both its promises and risks. In this paper, we will argue that the actual situation is more nuanced and complex. Synthetic biology is a highly interdisciplinary field of research located at the interface of physics, chemistry, biology, and computational science. All of these fields provide concepts, metaphors, mathematical tools, and models, which are typically utilized by synthetic biologists by drawing analogies between the different fields of inquiry. We will study analogical reasoning in synthetic biology through the emergence of the functional meaning of noise, which marks an important shift in how engineering concepts are employed in this field. The notion of noise serves also to highlight the differences between the two branches of synthetic biology: the basic science-oriented branch and the engineering-oriented branch, which differ from each other in the way they draw analogies to various other fields of study. Moreover, we show that fixing the mapping between a source domain and the target domain seems not to be the goal of analogical reasoning in actual scientific practice.}
}
@incollection{IRISH2024,
title = {Interactions between episodic and semantic memory},
booktitle = {Reference Module in Neuroscience and Biobehavioral Psychology},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-12-809324-5},
doi = {https://doi.org/10.1016/B978-0-443-15754-7.00009-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443157547000092},
author = {Muireann Irish and Matthew D. Grilli},
keywords = {Aging, Alzheimer's disease, Amnesia, Autobiographical memory, Episodic memory, Gradients, Hippocampus, Prospection, Semantic dementia, Semantic memory},
abstract = {In the present chapter, we challenge the idea that episodic and semantic memory are distinct memory systems supported by dissociable brain networks. Drawing on converging findings from cognitive neuroscience and neuropsychology, we show how these forms of declarative memory share remarkably similar neural networks and interact to support an array of cognitive endeavors. We contend that these points of overlap and apparent dissociations can be reconciled by situating the representational content of declarative memory along an episodic-semantic gradient or continuum. This continuum perspective can account for several bodies of research showing that episodic and semantic memory are highly interdependent in natural states and functional contexts, from the way memories are reorganized over time to how humans mentally construct future scenarios. We discuss these points of synergy and highlight unresolved questions, with a view to orienting the field towards a more integrated perspective.}
}
@article{STRACHANREGAN2024e28340,
title = {The impact of room shape on affective states, heartrate, and creative output},
journal = {Heliyon},
volume = {10},
number = {6},
pages = {e28340},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e28340},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024043718},
author = {K. Strachan-Regan and O. Baumann},
keywords = {Built environment, Neuroarchitecture, Environmental psychology, Emotion, Creativity},
abstract = {The architectural design of space can deeply impact an individuals' mood, physiology, and mental health. While previous research has predominantly focused on elements like nature and lighting within architectural spaces, there is a growing literature base that also investigates the psychological and neurophysiological impacts of geometrical properties of architectural spaces. Employing virtual reality technology, the study sought to investigate the effects of curved and rectangular architectural spaces on affective states, heart rate, and creativity. A total of 35 participants were exposed to two distinct virtual environments: a curved room and a rectangular room. Participants' self-reported mood was assessed using the Positive and Negative Affect Schedule (PANAS-Long Form). Heart rate was monitored using a pulse oximeter, and creative output was evaluated using the Guilford Alternative Uses Task (GAUT). Statistical comparisons between the two room types indicated that participants experienced higher positive affect and lower negative affect in the curved room condition compared to the rectangular room condition. Furthermore, heart rate measurements revealed lower physiological arousal in the curved room. Additionally, participants exhibited higher creative output in the curved room as opposed to the rectangular room. These findings align with previous literature on the influence of geometric factors on affective responses. The implications of this study are significant as they pertain to individuals' daily environments and their impact on health and well-being. The positive influence of curved room geometry on mood, arousal, and creativity emphasises the importance of considering room layout and design in various settings, such as workplaces and educational environments. Architects and designers can utilise these findings to inform their decisions and promote neuroarchitecture that enhances positive emotional experiences and productivity.}
}
@article{NI2011100,
title = {Influence of curriculum reform: An analysis of student mathematics achievement in Mainland China},
journal = {International Journal of Educational Research},
volume = {50},
number = {2},
pages = {100-116},
year = {2011},
note = {Curricular effect on the teaching and learning of mathematics: Findings from two longitudinal studies in China and the United States},
issn = {0883-0355},
doi = {https://doi.org/10.1016/j.ijer.2011.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0883035511000413},
author = {Yujing Ni and Qiong Li and Xiaoqing Li and Zhong-Hua Zhang},
keywords = {Curriculum reform, Primary mathematics, Curriculum evaluation, Student mathematics achievement, Cognitive, Affective},
abstract = {This study investigated curriculum influences on student mathematics achievement by following two groups of students from fifth to sixth grade that were taught either the reformed curriculum or the conventional curriculum. Analyses with three-level modeling were conducted to examine learning outcomes of the students who were assessed three times over a period of 18 months. Achievement was measured with regard to computation, routine problem solving, and complex problem solving. Affective aspects included self-reported interest in learning mathematics, classroom participation, views of the nature of mathematics, and views of learning mathematics. The results showed overall improved performance among all the students over the time on computation, routine problem solving, and complex problem solving but not on the affective measures. There were differentiated patterns of performance between the groups. On the initial assessment, the reform group performed better than the non-reform group on calculation, complex problem solving, and indicated higher interest in learning mathematics. The two groups did not differ on the other achievement and affective measures at the first time of assessment. There was no significant difference in growth rate between the groups on the cognitive and affective measures except that the non-reform group progressed at a faster pace on calculation. Therefore, the non-reform group outperformed the reform group on computation at the third (last) assessment. These results are discussed with respect to the possible influence of the curriculum on student learning.}
}
@article{TERZOPOULOU2024104133,
title = {Iterative voting with partial preferences},
journal = {Artificial Intelligence},
volume = {332},
pages = {104133},
year = {2024},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2024.104133},
url = {https://www.sciencedirect.com/science/article/pii/S0004370224000699},
author = {Zoi Terzopoulou and Panagiotis Terzopoulos and Ulle Endriss},
keywords = {Social choice theory, Iterative voting, Partial preferences},
abstract = {Voting platforms can offer participants the option to sequentially modify their preferences, whenever they have a reason to do so. But such iterative voting may never converge, meaning that a state where all agents are happy with their submitted preferences may never be reached. This problem has received increasing attention within the area of computational social choice. Yet, the relevant literature hinges on the rather stringent assumption that the agents are able to rank all alternatives they are presented with, i.e., that they hold preferences that are linear orders. We relax this assumption and investigate iterative voting under partial preferences. To that end, we define and study two families of rules that extend the well-known k-approval rules in the standard voting framework. Although we show that for none of these rules convergence is guaranteed in general, we also are able to identify natural conditions under which such guarantees can be given. Finally, we conduct simulation experiments to test the practical implications of our results.}
}
@article{SCOTNEY2020102981,
title = {The form of a ‘half-baked’ creative idea: Empirical explorations into the structure of ill-defined mental representations},
journal = {Acta Psychologica},
volume = {203},
pages = {102981},
year = {2020},
issn = {0001-6918},
doi = {https://doi.org/10.1016/j.actpsy.2019.102981},
url = {https://www.sciencedirect.com/science/article/pii/S0001691819303129},
author = {Victoria S. Scotney and Jasmine Schwartz and Nicole Carbert and Adam Saab and Liane Gabora},
keywords = {Analogy, Art, Creative process, Honing, Mental representation, Structure mapping},
abstract = {Creative thought is conventionally believed to involve searching memory and generating multiple independent candidate ideas followed by selection and refinement of the most promising. Honing theory, which grew out of the quantum approach to describing how concepts interact, posits that what appears to be discrete, separate ideas are actually different projections of the same underlying mental representation, which can be described as a superposition state, and which may take different outward forms when reflected upon from different perspectives. As creative thought proceeds, this representation loses potentiality to be viewed from different perspectives and manifest as different outcomes. Honing theory yields different predictions from conventional theories about the mental representation of an idea midway through the creative process. These predictions were pitted against one another in two studies: one closed-ended and one open-ended. In the first study, participants were interrupted midway through solving an analogy problem and wrote down what they were thinking in terms of a solution. In the second, participants were instructed to create a painting that expressed their true essence and describe how they conceived of the painting. For both studies, naïve judges categorized these responses as supportive of either the conventional view or the honing theory view. The results of both studies were significantly more consistent with the predictions of honing theory. Some implications for creative cognition, and cognition in general, are discussed.}
}
@article{SAZHIN2024110832,
title = {A behavioral dataset of predictive decisions given trends in information across adulthood},
journal = {Data in Brief},
volume = {56},
pages = {110832},
year = {2024},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2024.110832},
url = {https://www.sciencedirect.com/science/article/pii/S2352340924007960},
author = {Daniel Sazhin and Vishnu Murty and Chelsea Helion and David V. Smith},
keywords = {Strategic, Dynamic, Trends, Function learning, Cognition},
abstract = {Making early and good predictions is a critical feature of decision making in domains such as investing and predicting the spread of diseases. Past literature indicates that people use recent and longer-term trends to extrapolate future outcomes. Nonetheless, less is known about what differentiates the strategies people use to make better predictions than others. Furthermore, factors underlying predictive judgments could be an important behavioral component in psychosocial research investigating manic-depression, anxiety, and age effects. Additionally, predictive judgments may be moderated based on the experience of living in areas with greater income inequality. To address these issues, we used investment tasks where participants had to predict future outcomes of their investments based on a trend in information. In the task, participants predicted how many tokens a gold mine would produce on the twelfth turn. On each turn, participants could ask for more information at a cost, or make a prediction about whether the gold mine would produce more or less than 100 tokens by the 12th turn. The trend was determined by function type (exponential and inverse exponential functions), whether the function was more linear or curved (growth factors), and good or bad outcomes (final values). This paradigm could help disentangle to what degree people use recent or longer-term information to inform their predictive judgments. We used Qualtrics to conduct this study. We also collected questionnaire data quantifying anxiety, impulsivity, risk attitudes, manic-depressive symptoms, and other psychosocial characteristics. The study was administered to adults with age ranges across the lifespan (N = 360; 225 male, 132 female; 3 nonbinary; mean age: 44.3 years; SD: 15.4 years, min: 18 years, max: 78 years). Additionally, we sampled across areas with high- and low-income inequality, thereby allowing researchers to investigate if value-based decisions are associated with participants’ local communities. We outline potential ways to use and reuse this data, including exploring how individual differences are associated with predictive judgments.}
}
@incollection{SANCHEZSILVA2013437,
title = {17 - Risk assessment and management of civil infrastructure networks: a systems approach},
editor = {S. Tesfamariam and K. Goda},
booktitle = {Handbook of Seismic Risk Analysis and Management of Civil Infrastructure Systems},
publisher = {Woodhead Publishing},
pages = {437-464},
year = {2013},
series = {Woodhead Publishing Series in Civil and Structural Engineering},
isbn = {978-0-85709-268-7},
doi = {https://doi.org/10.1533/9780857098986.4.437},
url = {https://www.sciencedirect.com/science/article/pii/B9780857092687500177},
author = {M. Sánchez-Silva and C. Gómez},
keywords = {infrastructure, transportation networks, systems thinking, risk assessment, decision-making, optimization},
abstract = {Abstract:
Infrastructure networks are complex systems due to the large number of components that interact in a nonlinear way. Detecting and understanding the properties of such systems is of paramount importance to make effective decisions about risk management and sustainable development. This chapter presents a systems approach to risk management and risk-based decision making in infrastructure networks. In the proposed approach, the internal structure of a network is detected via pattern recognition (clustering), and structured information is used to enhance conceptual and computational analyses of reliability, vulnerability, damage propagation, and resource allocation. The approach can be applied to network analysis of complex infrastructure systems subjected to extreme events, such as earthquakes.}
}
@article{TRAUSANMATU20231052,
title = {Identification of creativity in collaborative conversations based on the polyphonic model},
journal = {Procedia Computer Science},
volume = {221},
pages = {1052-1057},
year = {2023},
note = {Tenth International Conference on Information Technology and Quantitative Management (ITQM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.08.087},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923008451},
author = {Stefan Trausan-Matu},
keywords = {polyphonic model, creativity, brainstorming, collaboration, computer-supported collaborative learning, natural language processing, deep learning},
abstract = {The paper presents a theoretical approach and a set of experiments that operationalize it for the identification of creative moments in conversations. State-of-the-art artificial intelligence technology is used for the operationalization: natural language processing, machine learning, and deep neural networks The approach is based on the polyphonic model introduced by Trausan-Matu, which starts from Mikhail Bakhtin's analogy of discourse building in texts with polyphonic music. The divergent and convergent steps of creativity are related to the inter-animation of voices through dissonances and consonances in polyphonic, contrapuntal music.}
}
@article{DEVISSCHER2021105317,
title = {Time for change: Learning from community forests to enhance the resilience of multi-value forestry in British Columbia, Canada},
journal = {Land Use Policy},
volume = {103},
pages = {105317},
year = {2021},
issn = {0264-8377},
doi = {https://doi.org/10.1016/j.landusepol.2021.105317},
url = {https://www.sciencedirect.com/science/article/pii/S0264837721000405},
author = {Tahia Devisscher and Jillian Spies and Verena C. Griess},
keywords = {Climate change, Community forestry, Resilience, Wildfire, Social license, Land tenure},
abstract = {Forests around the world are experiencing the cumulative effects of rapid social and environmental change. Building resilience in the forestry sector has thus become of major importance in many countries, including Canada. While British Columbia (BC) generates the highest revenue from the forestry sector in Canada, the planning and management of forests in this province face several limitations that hinder the application of resilience thinking in a fully integrated way that accounts not only for ecosystem processes but also the close interconnection between forests and people. Community forestry in BC provides experience gained over 20 years that can form the basis for a more holistic, long-term approach to enhance the resilience of forested landscapes. Based on interviews with managers of 5 case study community forests (CFs), and a survey of all CFs in BC over three consecutive years, we present pilot practices to manage forests for resilience at the stand- and landscape-levels. Findings show that these practices mainly focus on (1) age and species diversification, (2) introduction of more drought-tolerant species, (3) systematic long-term monitoring of productivity and forest health, (4) wildfire risk management, and (5) introduction of enhanced silviculture such as thinning, rehabilitation and fertilization. Between 2016 and 2018, 38 CFs in BC invested more than CAD 4.5 million in enhanced silvicultural practices using their own funds. The area-based tenure of CFs motivated not only long-term planning and investment, but also shifted the mindset among residents towards a more multi-functional and dynamic view of the forest. Building adaptive capacity and social license, CFs foster a future where forest health and community well-being are compatible. These lessons can be scaled to BC and other forested landscapes in Canada and around the world. Scaling mechanisms include: (1) facilitating knowledge exchange; (2) increasing multi-stakeholder collaboration; (3) replication and mainstreaming of effective practices; (4) rethinking the forest tenure system; and (5) systematic research and monitoring to learn from pilot studies that could inform strategic interventions with landscape-scale impact. Multi-functional forests which are increasingly affected by climate change and novel disturbances could particularly benefit from the insights shared in this paper to build social-ecological resilience.}
}
@article{ANTONIETTI20082172,
title = {Undergraduates’ metacognitive knowledge about the psychological effects of different kinds of computer-supported instructional tools},
journal = {Computers in Human Behavior},
volume = {24},
number = {5},
pages = {2172-2198},
year = {2008},
note = {Including the Special Issue: Internet Empowerment},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2007.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0747563207001598},
author = {Alessandro Antonietti and Barbara Colombo and Yuri Lozotsev},
keywords = {Metacognition, Belief, Learning, Education, Computer},
abstract = {Literature about metacognition suggests that learners develop personal beliefs about the educational technologies that they are asked to employ and that such beliefs can influence learning outcomes. In this perspective, opinions about the psychological effects of computer-supported instructional tools were analysed by means of a questionnaire which included items about the motivational and emotional aspects of learning, the behaviour to have during the learning process, the mental abilities and the style of thinking required, and the cognitive benefits. Items were presented five times: each time they made reference to a different kind of tool (online courses, hypertexts, Web forums, multimedia presentations, and virtual simulations). The questionnaire was filled out by 99 undergraduates attending engineering courses. Results showed that students ranked the psychological effects of the computer-supported tools in a relative different order according to the kind of tool and attributed distinctive effects to each tool. Gender and expertise played a minor role in modulating undergraduates’ beliefs. Implications for instruction were discussed.}
}