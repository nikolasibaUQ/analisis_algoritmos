@article{KOMPALLI2016534,
title = {Clusters of Genetic-based Attributes Selection of Cancer Data},
journal = {Procedia Computer Science},
volume = {89},
pages = {534-539},
year = {2016},
note = {Twelfth International Conference on Communication Networks, ICCN 2016, August 19– 21, 2016, Bangalore, India Twelfth International Conference on Data Mining and Warehousing, ICDMW 2016, August 19-21, 2016, Bangalore, India Twelfth International Conference on Image and Signal Processing, ICISP 2016, August 19-21, 2016, Bangalore, India},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.06.098},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916311632},
author = {Vijaya Sri Kompalli and K. Usha Rani},
keywords = {Cluster, Coupling, Cohesion, Genetic Algorithm, Fuzzy C-Means.},
abstract = {Clustering of data simplifies the task of data analysis and results in better disease diagnosis. Well-existing K-Means clustering hard computes clusters. Due to which the data may be centered to a specific cluster having less concentration on the effect of the coupling of clusters. Soft Computing methods are widely used in medical field as it contains fuzzy natured data. A Soft Computing approach of clustering called Fuzzy C-Means (FCM) deals with coupling. FCM clustering soft computes the clusters to determine the clusters based on the probability of having memberships in each of the clusters. The probability function used, determines the extent of coupling among the clusters. In order to achieve the computational efficiency and binding of features genetic evaluation is introduced. Genetic-based features are identified having more cohesion based on the fitness function values and then the coupling of the clusters is done using K-Means clustering in one trial and FCM in another trial. Analysis of coupling and cohesion is performed on Wisconsin Breast Cancer Dataset. Nature of clusters formations are observed with respect to coupling and cohesion.}
}
@article{PONRANI2024110203,
title = {Brain-computer interfaces inspired spiking neural network model for depression stage identification},
journal = {Journal of Neuroscience Methods},
volume = {409},
pages = {110203},
year = {2024},
issn = {0165-0270},
doi = {https://doi.org/10.1016/j.jneumeth.2024.110203},
url = {https://www.sciencedirect.com/science/article/pii/S0165027024001481},
author = {M. Angelin Ponrani and Monika Anand and Mahmood Alsaadi and Ashit Kumar Dutta and Roma Fayaz and Sojomon Mathew and Mousmi Ajay Chaurasia and  Sunila and Manisha Bhende},
keywords = {Brain-Computer Interface, EEG Signals, Deep Learning, Depression, Next Generation Neuro-Technologies, Pulse Neural Network},
abstract = {Background
Depression is a global mental disorder, and traditional diagnostic methods mainly rely on scales and subjective evaluations by doctors, which cannot effectively identify symptoms and even carry the risk of misdiagnosis. Brain-Computer Interfaces inspired deep learning-assisted diagnosis based on physiological signals holds promise for improving traditional methods lacking physiological basis and leads next generation neuro-technologies. However, traditional deep learning methods rely on immense computational power and mostly involve end-to-end network learning. These learning methods also lack physiological interpretability, limiting their clinical application in assisted diagnosis.
Methodology
A brain-like learning model for diagnosing depression using electroencephalogram (EEG) is proposed. The study collects EEG data using 128-channel electrodes, producing a 128×128 brain adjacency matrix. Given the assumption of undirected connectivity, the upper half of the 128×128 matrix is chosen in order to minimise the input parameter size, producing 8,128-dimensional data. After eliminating 28 components derived from irrelevant or reference electrodes, a 90×90 matrix is produced, which can be used as an input for a single-channel brain-computer interface image.
Result
At the functional level, a spiking neural network is constructed to classify individuals with depression and healthy individuals, achieving an accuracy exceeding 97.5 %.
Comparison with existing methods
Compared to deep convolutional methods, the spiking method reduces energy consumption.
Conclusion
At the structural level, complex networks are utilized to establish spatial topology of brain connections and analyse their graph features, identifying potential abnormal brain functional connections in individuals with depression.}
}
@article{MAJID2004108,
title = {Can language restructure cognition? The case for space},
journal = {Trends in Cognitive Sciences},
volume = {8},
number = {3},
pages = {108-114},
year = {2004},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2004.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S1364661304000208},
author = {Asifa Majid and Melissa Bowerman and Sotaro Kita and Daniel B.M. Haun and Stephen C. Levinson},
abstract = {Frames of reference are coordinate systems used to compute and specify the location of objects with respect to other objects. These have long been thought of as innate concepts, built into our neurocognition. However, recent work shows that the use of such frames in language, cognition and gesture varies cross-culturally, and that children can acquire different systems with comparable ease. We argue that language can play a significant role in structuring, or restructuring, a domain as fundamental as spatial cognition. This suggests we need to rethink the relation between the neurocognitive underpinnings of spatial cognition and the concepts we use in everyday thinking, and, more generally, to work out how to account for cross-cultural cognitive diversity in core cognitive domains.}
}
@article{PANETSOS2011314,
title = {Physical measurement of brain perception abilities. Foundations of a working methodology for the design of “intelligent” beings},
journal = {Procedia Computer Science},
volume = {7},
pages = {314-316},
year = {2011},
note = {Proceedings of the 2nd European Future Technologies Conference and Exhibition 2011 (FET 11)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2011.09.052},
url = {https://www.sciencedirect.com/science/article/pii/S1877050911006120},
author = {F. Panetsos and S.L. Andino Gonzalez and P.C. Marijuan and C. Herrera-Rincon},
keywords = {Emergent properties, complexity, artificial brain, synthetic approach},
abstract = {Most of the important properties of the brain (thinking, consciousness, music, etc.) are severely ill-defined. They are not the direct output of biological sensors or their combinations but emerge from complex computations at the network level and are not necessarily represented in the sensory input or the activity of individual cells. They are emergent properties arising from dynamic interactions between neurons in the different relay stations of the sensory pathways where recognition of basic physical properties of incoming stimuli take place. Emergent properties and interactions between them range from physical properties of stimuli to cognitive operations as emotions or consciousness and gradually involve interactions between sensory pathways, associative cortexes, hippocampus, or the amygdala. Here we propose to build neural tissues from embryonic stem cells in “in vitro” controlled environments to determine the way physical inputs are transformed into what humans perceive and measure. We will start with “low complexity” tissues able to perform low level recognition of physical properties, to gradually increase the complexity of the tissue to investigate how the physical characteristics of the incoming stimuli correspond at higher levels to the emergent properties of the system. Mathematical methods based on networks theory, nonlinear dynamics, fractal theory and chaos among other will be used to determine and measure the emergent properties of the nervous tissue at different complexity levels. We expected to provide criteria and methodologies to measure human-like perception variables and use them for the design of future living artifacts (autonomous robots, intelligent sensors, hybrid systems, etc.).}
}
@incollection{ESFELD2001859,
title = {Atomism and Holism: Philosophical Aspects},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {859-864},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/01005-6},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767010056},
author = {M. Esfeld},
abstract = {Social atomism is the thesis that an individual considered in isolation can have thoughts with a determinate conceptual content. Social holism, by contrast, is the thesis that social relations are essential for a human being in order to be a ‘thinking’ being. The discussion on atomism vs. holism extends to aspects of thoughts as well. Semantic atomism is the thesis that each thought has a meaning independently of other thoughts. Semantic holism, in reverse, is the thesis that the meaning of a thought consists in its inferential relations to other thoughts in a system of thoughts. Confirmation atomism is the thesis that thoughts can be empirically confirmed one by one. Confirmation holism, by contrast, is the thesis that only a whole system of thoughts or a whole theory can be confirmed by experience. Social atomism in modern philosophy goes back to Hobbes. Social holism comes up in romanticism and its predecessors; it is worked out by Hegel. In today's discussion, the rule-following considerations that are developed by Kripke on behalf of the later Wittgenstein are the main argument for social holism. Social atomists counter this argument by a naturalistic account of rule following in terms of certain dispositions to behavior.}
}
@incollection{ULLMAN1988548,
title = {Visual routines**This report describes research done at the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. Support for the Laboratory's artificial intelligence research is provided in part by the Advanced Research Projects Agency of the Department of Defense under Office of Naval Research contract N00014-80-C-0505 and in part by National Science Foundation Grant 79-23110MCS. Reprint requests should be sent to Shimon Ullman Department of Psychology and Artificial Intelligence Laboratory, M.I.T., Cambridge, MA 02139. U.S.A.},
editor = {Allan Collins and Edward E. Smith},
booktitle = {Readings in Cognitive Science},
publisher = {Morgan Kaufmann},
pages = {548-579},
year = {1988},
isbn = {978-1-4832-1446-7},
doi = {https://doi.org/10.1016/B978-1-4832-1446-7.50047-9},
url = {https://www.sciencedirect.com/science/article/pii/B9781483214467500479},
author = {SHIMON ULLMAN},
abstract = {This paper examines the processing of visual information beyond the creation of the early representations. A fundamental requirement at this level is the capacity to establish visually abstract shape properties and spatial relations. This capacity plays a major role in object recognition, visually guided manipulation, and more abstract visual thinking. For the human visual system, the perception of spatial properties and relations that are complex from a computational standpoint nevertheless often appears deceivingly immediate and effortless. The proficiency of the human system in analyzing spatial information far surpasses the capacities of current artificial systems. The study of the computations that underlie this competence may therefore lead to the development of new more efficient methods for the spatial analysis of visual information. The perception of abstract shape properties and spatial relations raises fundamental difficulties with major implications for the overall processing of visual information. It will be argued that the computation of spatial relations divides the analysis of visual information into two main stages. The first is the bottom-up creation of certain representations of the visible environment. The second stage involves the application of processes called ‘visual routines’ to the representations constructed in the first stage. These routines can establish properties and relations that cannot be represented explicitly in the initial representations. Visual routines are composed of sequences of elemental operations. Routines for different properties and relations share elemental operations. Using a fixed set of basic operations, the visual system can assemble different routines to extract an unbounded variety of shape properties and spatial relations.}
}
@article{WEI2024115144,
title = {Shared and distinctive brain networks underlying trait and state rumination},
journal = {Behavioural Brain Research},
volume = {472},
pages = {115144},
year = {2024},
issn = {0166-4328},
doi = {https://doi.org/10.1016/j.bbr.2024.115144},
url = {https://www.sciencedirect.com/science/article/pii/S0166432824003000},
author = {Luqing Wei and Hui Dong and Fanxi Ding and Can Luo and Chanyu Wang and Chris Baeken and Guo-Rong Wu},
keywords = {Trait rumination, State rumination, Connectome-based predictive modeling, Functional connectivity},
abstract = {Although trait and state rumination play a central role in the exacerbation of negative affect, evidence suggests that they are weakly correlated and exert distinct influences on emotional reactivity to stressors. Whether trait and state rumination share a common or exhibit distinct neural substrate remains unclear. In this study, we utilized functional near-infrared spectroscopy (fNIRS) combined with connectome-based predictive modeling (CPM) to identify neural fingerprints associated with trait and state rumination. CPM identified distinctive functional connectivity (FC) profiles that contribute to the prediction of trait rumination, primarily involving FC within the default mode network (DMN) and the dorsal attention network (DAN) as well as FC between the DMN, control network (CN), DAN, and salience network (SN). Conversely, state rumination was predominantly associated with FC between the DMN and CN. Furthermore, the predictive features of trait rumination can be robustly generalized to predict state rumination, and vice versa. In conclusion, this study illuminates the importance of both DMN and non-DMN systems in the emergence and persistence of rumination. While trait rumination was associated with stronger and broader FC than state rumination, the generalizability of the predictive features underscores the presence of shared neural mechanisms between the two forms of rumination. These identified connectivity fingerprints may hold promise as targets for innovative therapeutic interventions aimed at mitigating rumination-related negative affect.}
}
@article{ESSEX2018554,
title = {Model falsifiability and climate slow modes},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {502},
pages = {554-562},
year = {2018},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2018.02.090},
url = {https://www.sciencedirect.com/science/article/pii/S0378437118301766},
author = {Christopher Essex and Anastasios A. Tsonis},
keywords = {Climate complexity, Computer errors, Computational over-stabilization, Dynamical and thermodynamical sensitivity, Slow climate modes},
abstract = {The most advanced climate models are actually modified meteorological models attempting to capture climate in meteorological terms. This seems a straightforward matter of raw computing power applied to large enough sources of current data. Some believe that models have succeeded in capturing climate in this manner. But have they? This paper outlines difficulties with this picture that derive from the finite representation of our computers, and the fundamental unavailability of future data instead. It suggests that alternative windows onto the multi-decadal timescales are necessary in order to overcome the issues raised for practical problems of prediction.}
}
@article{VAIS2013718,
title = {Laplacians on flat line bundles over 3-manifolds},
journal = {Computers & Graphics},
volume = {37},
number = {6},
pages = {718-729},
year = {2013},
note = {Shape Modeling International (SMI) Conference 2013},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2013.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0097849313000873},
author = {Alexander Vais and Daniel Brandes and Hannes Thielhelm and Franz-Erich Wolter},
keywords = {Spectral geometry, Vector bundles, Computational topology, Laplace operator, Knots, Seifert surfaces, FEM},
abstract = {The well-known Laplace–Beltrami operator, established as a basic tool in shape processing, builds on a long history of mathematical investigations that have induced several numerical models for computational purposes. However, the Laplace–Beltrami operator is only one special case of many possible generalizations that have been researched theoretically. Thereby it is natural to supplement some of those extensions with concrete computational frameworks. In this work we study a particularly interesting class of extended Laplacians acting on sections of flat line bundles over compact Riemannian manifolds. Numerical computations for these operators have recently been accomplished on two-dimensional surfaces. Using the notions of line bundles and differential forms, we follow up on that work giving a more general theoretical and computational account of the underlying ideas and their relationships. Building on this we describe how the modified Laplacians and the corresponding computations can be extended to three-dimensional Riemannian manifolds, yielding a method that is able to deal robustly with volumetric objects of intricate shape and topology. We investigate and visualize the two-dimensional zero sets of the first eigenfunctions of the modified Laplacians, yielding an approach for constructing characteristic well-behaving, particularly robust homology generators invariant under isometric deformation. The latter include nicely embedded Seifert surfaces and their non-orientable counterparts for knot complements.}
}
@article{WEN201811,
title = {Fast ranking nodes importance in complex networks based on LS-SVM method},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {506},
pages = {11-23},
year = {2018},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2018.03.076},
url = {https://www.sciencedirect.com/science/article/pii/S0378437118303947},
author = {Xiangxi Wen and Congliang Tu and Minggong Wu and Xurui Jiang},
keywords = {Complex network, Node importance, AHP, LS-SVM},
abstract = {Achieving high accuracy and comprehensiveness in node importance evaluation of complex networks is time-consuming. To solve this problem, a method based on Least Square Support Vector Machine (LS-SVM) was proposed. Firstly, four complicated importance indicators which reflect the node importance globally and comprehensively were selected. Then analytic hierarchy process (AHP) method was applied to obtain the node’s importance evaluation. On this basis, three simple indicators with low computational complexity were proposed, and LS-SVM was adopted to find the mapping rules between simple indicators and AHP evaluation. The experiments on artificial network and actual network show the validity of proposed method: the evaluation based on complicated indicators is consistent with reality and reflects node importance accurately; simple indicators evaluation by LS-SVM saved a lot of computational time and improved the evaluating efficiency. Our method can provide guidance on influential node identification in large scale complex networks.}
}
@article{VELAVELUPILLAI201436,
title = {Constructive and computable Hahn–Banach theorems for the (second) fundamental theorem of welfare economics},
journal = {Journal of Mathematical Economics},
volume = {54},
pages = {36-39},
year = {2014},
issn = {0304-4068},
doi = {https://doi.org/10.1016/j.jmateco.2014.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0304406814001062},
author = {K. {Vela Velupillai}},
keywords = {Fundamental theorems of welfare economics, Hahn–Banach theorem, Constructive analysis, Computable analysis},
abstract = {The Hahn–Banach Theorem plays a crucial role in the second fundamental theorem of welfare economics. To date, all mathematical economics and advanced general equilibrium textbooks concentrate on using non-constructive or incomputable versions of this celebrated theorem. In this paper we argue for the introduction of constructive or computable Hahn–Banach theorems in mathematical economics and advanced general equilibrium theory. The suggested modification would make applied and policy-oriented economics intrinsically computational.}
}
@article{DIAZBERRIOS2022100953,
title = {High school student understanding of exponential and logarithmic functions},
journal = {The Journal of Mathematical Behavior},
volume = {66},
pages = {100953},
year = {2022},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2022.100953},
url = {https://www.sciencedirect.com/science/article/pii/S0732312322000219},
author = {Tomás Díaz-Berrios and Rafael Martínez-Planell},
keywords = {APOS theory, Exponentiation, Logarithm, Rational exponents, Exponential and logarithmic functions},
abstract = {We use Action-Process-Object-Schema theory (APOS) to study high school student understanding of exponentiation and their construction of exponential and logarithmic functions. We extend didactic materials similar to those of Ferrari-Escolá et al. (2016) to include exponentials on the rational numbers and to help students construct logarithms as numbers. Qualitative data from the problem-solving activities of two groups of eight students each during a series of teaching episodes suggests that some students can use these materials successfully. The data analysis enabled us to give specific suggestions on how to help other students do some of the constructions needed to understand these functions. Research shows these constructions are difficult for students. The findings of our study led to contributing a new and detailed genetic decomposition that can be tested and improved in future research cycles.}
}
@article{MAO2024101988,
title = {A survey on semantic processing techniques},
journal = {Information Fusion},
volume = {101},
pages = {101988},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.101988},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523003044},
author = {Rui Mao and Kai He and Xulang Zhang and Guanyi Chen and Jinjie Ni and Zonglin Yang and Erik Cambria},
keywords = {Semantic processing, Word sense disambiguation, Anaphora resolution, Named entity recognition, Concept extraction, Subjectivity detection},
abstract = {Semantic processing is a fundamental research domain in computational linguistics. In the era of powerful pre-trained language models and large language models, the advancement of research in this domain appears to be decelerating. However, the study of semantics is multi-dimensional in linguistics. The research depth and breadth of computational semantic processing can be largely improved with new technologies. In this survey, we analyzed five semantic processing tasks, e.g., word sense disambiguation, anaphora resolution, named entity recognition, concept extraction, and subjectivity detection. We study relevant theoretical research in these fields, advanced methods, and downstream applications. We connect the surveyed tasks with downstream applications because this may inspire future scholars to fuse these low-level semantic processing tasks with high-level natural language processing tasks. The review of theoretical research may also inspire new tasks and technologies in the semantic processing domain. Finally, we compare the different semantic processing techniques and summarize their technical trends, application trends, and future directions.}
}
@article{KAMATH2020100944,
title = {Making grammars for material and tectonic complexity: An example of a thin-tile vault},
journal = {Design Studies},
volume = {69},
pages = {100944},
year = {2020},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2020.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X20300326},
author = {Ayodh Vasant Kamath},
keywords = {affordance, architectural design, creativity, reflective practice, making grammars},
abstract = {Shape grammars are a framework to view design as non-deterministic, creative, visual computation, and making as the deterministic execution of a design in the material world. Making grammars conceive of making as creative, multi-sensory, material computation. However, examples in the literature on making grammar are insufficiently complex to demonstrate the creativity of non-visual senses in making. This paper develops a making grammar for thin-tile vault construction as a sensory ethnography to ‘show making’ to designers as being a creative practice involving visual and non-visual senses. To do so, the role of drawing in shape grammar and making grammar is differentiated, and environmental psychology is used to develop a framework for the use of drawing to depict multi-sensory processes in making grammar.}
}
@article{PREM2024100075,
title = {Principles of digital humanism: A critical post-humanist view},
journal = {Journal of Responsible Technology},
volume = {17},
pages = {100075},
year = {2024},
issn = {2666-6596},
doi = {https://doi.org/10.1016/j.jrt.2024.100075},
url = {https://www.sciencedirect.com/science/article/pii/S2666659624000015},
author = {Erich Prem},
keywords = {Digital humanism, Principles, Computer ethics, AI ethics, Digital sovereignty, Humanism},
abstract = {Digital humanism emerges from serious concerns about the way in which digitisation develops, its impact on society and on humans. While its motivation is clear and broadly accepted, it is still an emerging field that does not yet have a universally accepted definition. Also, it is not always clear how to differentiate digital humanism from other similar endeavours. In this article, we critically investigate the notion of digital humanism and present its main principles as shared by its key proponents. These principles include the quest for human dignity and the ideal of a better society based on core values of the Enlightenment. The paper concludes that digital humanism is to be treated as a technical endeavour to shape digital technologies and use them for digital innovation, a political endeavour investigating power shifts triggered by digital technology, and, at the same time, as a philosophical endeavour including the quest to delineate its scope and to draw boundaries for the digital. Methodologically, digital humanism is an interdisciplinary effort to debate a broad range of digitisation shortfalls in their totality, from privacy infringements to power shifts, from human alienation to disownment. While it overlaps with a range of established fields and other movements, digital humanism reflects a new academic, engineering, and societal awareness of the challenges of digital technologies.}
}
@article{BATTLEDAY20151865,
title = {Modafinil for cognitive neuroenhancement in healthy non-sleep-deprived subjects: A systematic review},
journal = {European Neuropsychopharmacology},
volume = {25},
number = {11},
pages = {1865-1881},
year = {2015},
issn = {0924-977X},
doi = {https://doi.org/10.1016/j.euroneuro.2015.07.028},
url = {https://www.sciencedirect.com/science/article/pii/S0924977X15002497},
author = {R.M. Battleday and A.-K. Brem},
keywords = {Neuroenhancement, Modafinil, Cognitive, Psychometric, Enhancement, Nootropic},
abstract = {Modafinil is an FDA-approved eugeroic that directly increases cortical catecholamine levels, indirectly upregulates cerebral serotonin, glutamate, orexin, and histamine levels, and indirectly decreases cerebral gamma-amino-butrytic acid levels. In addition to its approved use treating excessive somnolence, modafinil is thought to be used widely off-prescription for cognitive enhancement. However, despite this popularity, there has been little consensus on the extent and nature of the cognitive effects of modafinil in healthy, non-sleep-deprived humans. This problem is compounded by methodological discrepancies within the literature, and reliance on psychometric tests designed to detect cognitive effects in ill rather than healthy populations. In order to provide an up-to-date systematic evaluation that addresses these concerns, we searched MEDLINE with the terms “modafinil” and “cognitive”, and reviewed all resultant primary studies in English from January 1990 until December 2014 investigating the cognitive actions of modafinil in healthy non-sleep-deprived humans. We found that whilst most studies employing basic testing paradigms show that modafinil intake enhances executive function, only half show improvements in attention and learning and memory, and a few even report impairments in divergent creative thinking. In contrast, when more complex assessments are used, modafinil appears to consistently engender enhancement of attention, executive functions, and learning. Importantly, we did not observe any preponderances for side effects or mood changes. Finally, in light of the methodological discrepancies encountered within this literature, we conclude with a series of recommendations on how to optimally detect valid, robust, and consistent effects in healthy populations that should aid future assessment of neuroenhancement.}
}
@article{SANGAIAH2020347,
title = {Cognitive IoT system with intelligence techniques in sustainable computing environment},
journal = {Computer Communications},
volume = {154},
pages = {347-360},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.02.049},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419314616},
author = {Arun Kumar Sangaiah and Jerline Sheebha Anni Dhanaraj and Prabu Mohandas and Aniello Castiglione},
keywords = {Computational intelligence, Cognition, Multi-sensor, Data fusion, IoT},
abstract = {Forest border crossing animals creates major societal related issues, in addition to endangering their own lives. This is the objective focused in this paper targeting the species “The Elephant”, incorporating with technical methodologies namely, multi-sensor data fusion, cognition theories and computational intelligence techniques. Multi-sensor data fusion provides three level detection of target, along with its related outputs, which improves performance metrics. Cognition theory resulted in obtaining other interesting features about the target. Computational intelligence techniques integrate and conclude the presence of the target in the pseudo-boundary. The technical combination enhances the novelty of the research work, resulting in achieving remarkable accuracy and minimized false alert. An IoT kit was designed and deployed in the real time wild environment in Hosur forest region for collecting the data of Elephant. Further, the notification is sent to the registered mobile of the forest authority, as an early warning for chasing the pachyderm back to the forest.}
}
@article{MADORE2022707,
title = {Readiness to remember: predicting variability in episodic memory},
journal = {Trends in Cognitive Sciences},
volume = {26},
number = {8},
pages = {707-723},
year = {2022},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2022.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S1364661322001127},
author = {Kevin P. Madore and Anthony D. Wagner},
keywords = {episodic retrieval, attention lapsing, goal processing, arousal, locus coeruleus, posterior alpha},
abstract = {Learning and remembering are fundamental to our lives, so what causes us to forget? Answers often highlight preparatory processes that precede learning, as well as mnemonic processes during the act of encoding or retrieval. Importantly, evidence now indicates that preparatory processes that precede retrieval attempts also have powerful influences on memory success or failure. Here, we review recent work from neuroimaging, electroencephalography, pupillometry, and behavioral science to propose an integrative framework of retrieval-period dynamics that explains variance in remembering in the moment and across individuals as a function of interactions among preparatory attention, goal coding, and mnemonic processes. Extending this approach, we consider how a ‘readiness to remember’ (R2R) framework explains variance in high-level functions of memory and mnemonic disruptions in aging.}
}
@article{ZADEH20082751,
title = {Is there a need for fuzzy logic?},
journal = {Information Sciences},
volume = {178},
number = {13},
pages = {2751-2779},
year = {2008},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2008.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S0020025508000716},
author = {Lotfi A. Zadeh},
keywords = {Fuzzy logic, Fuzzy sets, Approximate reasoning, Computing with words, Computing with perceptions, Generalized theory of uncertainty},
abstract = {“Is there a need for fuzzy logic?” is an issue which is associated with a long history of spirited discussions and debate. There are many misconceptions about fuzzy logic. Fuzzy logic is not fuzzy. Basically, fuzzy logic is a precise logic of imprecision and approximate reasoning. More specifically, fuzzy logic may be viewed as an attempt at formalization/mechanization of two remarkable human capabilities. First, the capability to converse, reason and make rational decisions in an environment of imprecision, uncertainty, incompleteness of information, conflicting information, partiality of truth and partiality of possibility – in short, in an environment of imperfect information. And second, the capability to perform a wide variety of physical and mental tasks without any measurements and any computations [L.A. Zadeh, From computing with numbers to computing with words – from manipulation of measurements to manipulation of perceptions, IEEE Transactions on Circuits and Systems 45 (1999) 105–119; L.A. Zadeh, A new direction in AI – toward a computational theory of perceptions, AI Magazine 22 (1) (2001) 73–84]. In fact, one of the principal contributions of fuzzy logic – a contribution which is widely unrecognized – is its high power of precisiation. Fuzzy logic is much more than a logical system. It has many facets. The principal facets are: logical, fuzzy-set-theoretic, epistemic and relational. Most of the practical applications of fuzzy logic are associated with its relational facet. In this paper, fuzzy logic is viewed in a nonstandard perspective. In this perspective, the cornerstones of fuzzy logic – and its principal distinguishing features – are: graduation, granulation, precisiation and the concept of a generalized constraint. A concept which has a position of centrality in the nontraditional view of fuzzy logic is that of precisiation. Informally, precisiation is an operation which transforms an object, p, into an object, p∗, which in some specified sense is defined more precisely than p. The object of precisiation and the result of precisiation are referred to as precisiend and precisiand, respectively. In fuzzy logic, a differentiation is made between two meanings of precision – precision of value, v-precision, and precision of meaning, m-precision. Furthermore, in the case of m-precisiation a differentiation is made between mh-precisiation, which is human-oriented (nonmathematical), and mm-precisiation, which is machine-oriented (mathematical). A dictionary definition is a form of mh-precisiation, with the definiens and definiendum playing the roles of precisiend and precisiand, respectively. Cointension is a qualitative measure of the proximity of meanings of the precisiend and precisiand. A precisiand is cointensive if its meaning is close to the meaning of the precisiend. A concept which plays a key role in the nontraditional view of fuzzy logic is that of a generalized constraint. If X is a variable then a generalized constraint on X, GC(X), is expressed as X isr R, where R is the constraining relation and r is an indexical variable which defines the modality of the constraint, that is, its semantics. The primary constraints are: possibilistic, (r=blank), probabilistic (r=p) and veristic (r=v). The standard constraints are: bivalent possibilistic, probabilistic and bivalent veristic. In large measure, science is based on standard constraints. Generalized constraints may be combined, qualified, projected, propagated and counterpropagated. The set of all generalized constraints, together with the rules which govern generation of generalized constraints, is referred to as the generalized constraint language, GCL. The standard constraint language, SCL, is a subset of GCL. In fuzzy logic, propositions, predicates and other semantic entities are precisiated through translation into GCL. Equivalently, a semantic entity, p, may be precisiated by representing its meaning as a generalized constraint. By construction, fuzzy logic has a much higher level of generality than bivalent logic. It is the generality of fuzzy logic that underlies much of what fuzzy logic has to offer. Among the important contributions of fuzzy logic are the following: 1.FL-generalization. Any bivalent-logic-based theory, T, may be FL-generalized, and hence upgraded, through addition to T of concepts and techniques drawn from fuzzy logic. Examples: fuzzy control, fuzzy linear programming, fuzzy probability theory and fuzzy topology.2.Linguistic variables and fuzzy if–then rules. The formalism of linguistic variables and fuzzy if–then rules is, in effect, a powerful modeling language which is widely used in applications of fuzzy logic. Basically, the formalism serves as a means of summarization and information compression through the use of granulation.3.Cointensive precisiation. Fuzzy logic has a high power of cointensive precisiation. This power is needed for a formulation of cointensive definitions of scientific concepts and cointensive formalization of human-centric fields such as economics, linguistics, law, conflict resolution, psychology and medicine.4.NL-Computation (computing with words). Fuzzy logic serves as a basis for NL-Computation, that is, computation with information described in natural language. NL-Computation is of direct relevance to mechanization of natural language understanding and computation with imprecise probabilities. More generally, NL-Computation is needed for dealing with second-order uncertainty, that is, uncertainty about uncertainty, or uncertainty2 for short. In summary, progression from bivalent logic to fuzzy logic is a significant positive step in the evolution of science. In large measure, the real-world is a fuzzy world. To deal with fuzzy reality what is needed is fuzzy logic. In coming years, fuzzy logic is likely to grow in visibility, importance and acceptance.}
}
@article{LEPORE2024e35863,
title = {A holistic framework to model student's cognitive process in mathematics education through fuzzy cognitive maps},
journal = {Heliyon},
volume = {10},
number = {16},
pages = {e35863},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e35863},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024118944},
author = {Mario Lepore},
keywords = {Student's cognitive model, Fuzzy cognitive map, Mathematics education, Quantitative research, Qualitative research},
abstract = {This study introduces a pioneering framework for modeling students' cognitive processes in mathematics education through Fuzzy Cognitive Maps (FCMs). By integrating key educational theories—Duval's Semiotic Representation Theory, Niss's Mathematical Competencies, Marton's Variation Theory, and the broad Engagement, Motivation, and Participation framework— the model offers a comprehensive and holistic understanding of students' cognitive landscapes. This research underscores the necessity of a multidimensional approach to capturing the intricate interplay of cognitive, affective, and behavioral factors in students' mathematical learning experiences. The novelty lies in its methodological innovation, employing FCMs to transcend traditional qualitative analyzes and facilitate quantitative insights into students' cognitive processes. This approach is particularly relevant in the current era dominated by digital learning environments and artificial intelligence, where real-time, automated analysis of student interactions is increasingly vital. The proposed FCM has been developed over the years with a data-driven approach; the concepts and relationships in it have been derived from the literature and refined by the author's experience in the field. Illustrated through case studies, the framework's utility is demonstrated in diverse contexts, highlighting how the quantitative data obtained are confirmed by qualitative approach: analyzing the impact of remote learning during the Covid-19 pandemic on student engagement and exploring Augmented Reality's role in enhancing mathematical conceptualization. These applications show the framework's adaptability and its potential to integrate new technologies in educational practices. However, the transition from qualitative to quantitative methodologies poses a challenge, given the prevalent use of qualitative approaches in mathematics education research. Additionally, the technological implementation of the FCM model in educational software presents practical hurdles, necessitating further development to ensure ease of integration and use in real-time educational settings. Future work will focus on bridging these methodological gaps and overcoming technological challenges to broaden the FCM model's applicability and enhance its contribution to advancing mathematics education.}
}
@article{MCLOUGHLIN2022173,
title = {Midfrontal Theta Activity in Psychiatric Illness: An Index of Cognitive Vulnerabilities Across Disorders},
journal = {Biological Psychiatry},
volume = {91},
number = {2},
pages = {173-182},
year = {2022},
note = {Biomarkers of Psychosis},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2021.08.020},
url = {https://www.sciencedirect.com/science/article/pii/S0006322321015651},
author = {Gráinne McLoughlin and Máté Gyurkovics and Jason Palmer and Scott Makeig},
keywords = {Biomarker, Cognitive control, EEG, ERP, Oscillations, Theta},
abstract = {There is an urgent need to identify the mechanisms that contribute to atypical thinking and behavior associated with psychiatric illness. Behavioral and brain measures of cognitive control are associated with a variety of psychiatric disorders and conditions as well as daily life functioning. Recognition of the importance of cognitive control in human behavior has led to intensive research into behavioral and neurobiological correlates. Oscillations in the theta band (4–8 Hz) over medial frontal recording sites are becoming increasingly established as a direct neural index of certain aspects of cognitive control. In this review, we point toward evidence that theta acts to coordinate multiple neural processes in disparate brain regions during task processing to optimize behavior. Theta-related signals in human electroencephalography include the N2, the error-related negativity, and measures of theta power in the (time-)frequency domain. We investigate how these theta signals are affected in a wide range of psychiatric conditions with known deficiencies in cognitive control: anxiety, obsessive-compulsive disorder, attention-deficit/hyperactivity disorder, and substance abuse. Theta-related control signals and their temporal consistency were found to differ in most patient groups compared with healthy control subjects, suggesting fundamental deficits in reactive and proactive control. Notably, however, clinical studies directly investigating the role of theta in the coordination of goal-directed processes across different brain regions are uncommon and are encouraged in future research. A finer-grained analysis of flexible, subsecond-scale functional networks in psychiatric disorders could contribute to a dimensional understanding of psychopathology.}
}
@incollection{COLLECCHIA20243,
title = {Chapter 1 - What does artificial intelligence mean?},
editor = {Giampaolo Collecchia and Riccardo {De Gobbi}},
booktitle = {AI in Clinical Practice},
publisher = {Academic Press},
pages = {3-5},
year = {2024},
isbn = {978-0-443-14054-9},
doi = {https://doi.org/10.1016/B978-0-443-14054-9.00019-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443140549000193},
author = {Giampaolo Collecchia and Riccardo {De Gobbi}},
keywords = {Artificial intelligence, digital world, Gestalt perception, technological advance, machines, algorithms},
abstract = {We all have daily interactions with artificial intelligence (AI), which, since Alan Turing laid the foundations in 1936, has undergone a radical evolution in meaning and applications. It has become a contemporary tool that supports us every day in numerous activities (telephone assistants, web search engines, social networks, photo sharing, listening to music, spam filters, and commercial profiling). In fact, all these technologies are based on AI.}
}
@article{ZHANG2023205,
title = {A survey for solving mixed integer programming via machine learning},
journal = {Neurocomputing},
volume = {519},
pages = {205-217},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222014035},
author = {Jiayi Zhang and Chang Liu and Xijun Li and Hui-Ling Zhen and Mingxuan Yuan and Yawen Li and Junchi Yan},
keywords = {Mixed integer programming, Machine learning, Combinatorial optimization},
abstract = {Machine learning (ML) has been recently introduced to solving optimization problems, especially for combinatorial optimization (CO) tasks. In this paper, we survey the trend of leveraging ML to solve the mixed-integer programming problem (MIP). Theoretically, MIP is an NP-hard problem, and most CO problems can be formulated as MIP. Like other CO problems, the human-designed heuristic algorithms for MIP rely on good initial solutions and cost a lot of computational resources. Therefore, researchers consider applying machine learning methods to solve MIP since ML-enhanced approaches can provide the solution based on the typical patterns from the training data. Specifically, we first introduce the formulation and preliminaries of MIP and representative traditional solvers. Then, we show the integration of machine learning and MIP with detailed discussions on related learning-based methods, which can be further classified into exact and heuristic algorithms. Finally, we propose the outlook for learning-based MIP solvers, the direction toward more combinatorial optimization problems beyond MIP, and the mutual embrace of traditional solvers and ML components. We maintain a list of papers that utilize machine learning technologies to solve combinatorial optimization problems, which is available at https://github.com/Thinklab-SJTU/awesome-ml4co.}
}
@article{PAZZANI1991401,
title = {A computational theory of learning causal relationships},
journal = {Cognitive Science},
volume = {15},
number = {3},
pages = {401-424},
year = {1991},
issn = {0364-0213},
doi = {https://doi.org/10.1016/0364-0213(91)80003-N},
url = {https://www.sciencedirect.com/science/article/pii/036402139180003N},
author = {Michael Pazzani},
abstract = {I present a cognitive model of the human ability to acquire causal relationships. I report on experimental evidence demonstrating that human learners acquire accurate causal relationships more rapidly when training examples are consistent with a general theory of causality. This article describes a learning process that uses a general theory of causality as background knowledge. The learning process, which I call theory-driven learning (TDL), hypothesizes causal relationships consistent both with observed data and the general theory of causality. TDL accounts for data on both the rate at which human learners acquire causal relationships, and the types of causal relationships they acquire. Experiments with TDL demonstrate the advantage of TDL for acquiring causal relationships over similarity-based approaches to learning: Fewer examples are required to learn an accurate relationship.}
}
@incollection{BERGER20234,
title = {3.02 - Electronic structure of oxide and halide perovskites},
editor = {Jan Reedijk and Kenneth R. Poeppelmeier},
booktitle = {Comprehensive Inorganic Chemistry III (Third Edition)},
publisher = {Elsevier},
edition = {Third Edition},
address = {Oxford},
pages = {4-25},
year = {2023},
isbn = {978-0-12-823153-1},
doi = {https://doi.org/10.1016/B978-0-12-823144-9.00102-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128231449001023},
author = {Robert F. Berger},
keywords = {Band structure, Density functional theory, Perovskite, Photocatalyst, Photovoltaic},
abstract = {Compounds crystallizing in the ABX3 perovskite structure are studied for a remarkable variety of technologies. Particularly for applications such as photovoltaics and photocatalysis, it is crucial to understand the key features of perovskite electronic structure and how they can be tuned by modifying the composition and crystal structure. This chapter begins with an overview of the compositional and structural diversity of perovskites. Then, density functional theory-based computational methods that have been used to study perovskite compounds are described. Next, the electronic band structures of an undistorted oxide (SrTiO3) and halide (CsPbI3) perovskite are explained in detail, merging the viewpoints of crystal wavefunctions as both linear combinations of atomic orbitals and perturbed plane waves. Finally, routes toward the tunability of perovskite electronic structure and properties are reviewed for various modifications: changes in elemental composition, various modes of geometric distortion, the application of high pressure or strain, and the formation of superstructures with reduced dimensionality. While the concepts and discussion herein are relevant to all perovskite compounds, the examples described in this chapter are mainly d0 oxide perovskite photocatalysts and halide perovskite photovoltaics.}
}
@article{NUMRICH201169,
title = {Self-similarity of parallel machines},
journal = {Parallel Computing},
volume = {37},
number = {2},
pages = {69-84},
year = {2011},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2010.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167819110001444},
author = {Robert W. Numrich and Michael A. Heroux},
keywords = {Parallel algorithms, Benchmark analysis, Computational intensity, Computational force, Dimensional analysis, Equivalence class, Self-similarity, Scaling, Mixing coefficient},
abstract = {Self-similarity is a property of physical systems that describes how to scale parameters such that dissimilar systems appear to be similar. Computer systems are self-similar if certain ratios of computational forces, also known as computational intensities, are equal. Two machines with different computational power, different network bandwidth and different inter-processor latency behave the same way if they have the same ratios of forces. For the parallel conjugate gradient algorithm studied in this paper, two machines are self-similar if and only if the ratio of one force describing latency effects to another force describing bandwidth effects is the same for both machines. For the two machines studied in this paper, this ratio, which we call the mixing coefficient, is invariant as problem size and processor count change. The two machines have the same mixing coefficient and belong to the same equivalence class.}
}
@article{BROCAS2022331,
title = {Adverse selection and contingent reasoning in preadolescents and teenagers},
journal = {Games and Economic Behavior},
volume = {133},
pages = {331-351},
year = {2022},
issn = {0899-8256},
doi = {https://doi.org/10.1016/j.geb.2022.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0899825622000616},
author = {Isabelle Brocas and Juan D. Carrillo},
keywords = {Developmental decision-making, Lab-in-the-field experiment, Contingent reasoning, Winner's curse},
abstract = {We study from a developmental viewpoint the ability to perform contingent reasoning and the cognitive abilities that facilitate optimal behavior. Individuals from 11 to 17 years old participate in a simplified version of the two-value, deterministic “acquire-a-company” adverse selection game (Charness and Levin, 2009; Martínez-Marquina et al., 2019). We find that even our youngest subjects understand well the basic principles of contingent reasoning (offer the reservation price of one of the sellers), although they do not necessarily choose the optimal price. Performance improves steadily and significantly over the developmental window but it is not facilitated by repeated exposure or feedback. High cognitive ability–measured by a high performance in a working memory task–is necessary to behave optimally in the simplest settings but it is not sufficient to solve the most complex situations.}
}
@article{KANAAN2024143,
title = {An Introduction to Approaching Architecture in the Muslim World: Novel Paths of Investigations},
journal = {Journal of Material Cultures in the Muslim World},
volume = {4},
number = {2},
pages = {143-152},
year = {2024},
issn = {2666-6278},
doi = {https://doi.org/10.1163/26666286-12340043},
url = {https://www.sciencedirect.com/science/article/pii/S2666627824000082},
author = {Ruba Kana‘an and Avinoam Shalem}
}
@incollection{WANG2017259,
title = {Chapter 14 - Reason and Emotion in Xunzi’s Moral Psychology},
editor = {T.-W. Hung and T.J. Lane},
booktitle = {Rationality},
publisher = {Academic Press},
address = {San Diego},
pages = {259-276},
year = {2017},
isbn = {978-0-12-804600-5},
doi = {https://doi.org/10.1016/B978-0-12-804600-5.00014-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128046005000143},
author = {E.H. Wang},
keywords = {Xunzi, moral rationalism, emotion},
abstract = {In this paper I explore the extent to which Xunzi may, or may not, be a moral rationalist by investigating the roles reason and emotion play in Xunzi’s moral psychology. To this end, I address Soek’s and Slingerland’s recent work on this subject. Seok (2013) recently characterized two contrasting models of moral psychology: “reason based” and “emotion based”; the former takes the reflective and conscious reasoning ability to be the essence of one’s moral judgment and action, while emotions and affective mechanisms play only minor roles (if any); the latter takes emotional states to be essential or at least necessary. Soek understands Confucian ethics in general to operate with the emotion-based model, but his argument mainly concerns Mencius’ work. Slingerland (2010), on the other hand, categorizes Xunzi’s moral psychology as a theory that presumes what he calls the “high reason model,” which significantly resembles the reason-based model in Soek’s account. Slingerland understands that, on Xunzi’s account, rational faculties and emotional faculties are competitive in the reasoning process. Moreover, he takes Xunzi to prioritize the rational faculties, thinking that they can and should monitor emotional responses, and override them when needed. Slingerland also cites recent empirical studies to criticize this model. I argue that Xunzi’s moral psychology cannot be captured by either of the two models Soek characterizes, but presents to us a third alternative: it gives us a good example of a hybrid model of these two. Indeed, Xunzi’s emphasis on ritual practices in the cultivation of xin and qing toward sagehood sheds light on a possible interplay between reason and emotion in ideal moral judgment/decision. This discussion inspires further consideration of what a moral rationalist may be, and the extent to which Xunzi may, or may not, be a moral rationalist.}
}
@article{RAN2024102578,
title = {Spatiotemporal characteristics and influencing factors of airport service quality in China},
journal = {Journal of Air Transport Management},
volume = {117},
pages = {102578},
year = {2024},
issn = {0969-6997},
doi = {https://doi.org/10.1016/j.jairtraman.2024.102578},
url = {https://www.sciencedirect.com/science/article/pii/S0969699724000437},
author = {Xinyue Ran and Lingling Li and Ruiling Han},
keywords = {Airport, Aviation complaint, Service quality, Influencing factors, Spatiotemporal differentiation characteristic},
abstract = {Airport service quality (ASQ) is essential for determining the quality of ground civil aviation services. In this study, ASQ was assessed using the monthly airport aviation complaint data from 2015 to 2019 of 196 airports in mainland China (except for airports in Hong Kong, Macao, and Taiwan, which are not included in the statistics). First, we constructed a seasonal index of aviation complaints to evaluate and compare the overall temporal characteristics of ASQ in China. Second, the spatial distribution pattern of ASQ in China was determined using the aviation complaint concentration index and hot spot analysis model. Finally, the major influencing factors and categories of ASQ in China were analyzed considering spatiotemporal dimensions using the correspondence analysis method. The results revealed that there were clear seasonal differences among ASQ in China, with a high–low–low–high distribution during all four seasons. The regional agglomeration trend of airport aviation complaints was obvious, and the spatial difference in ASQ was large. Northern and western China performed better than southern China. Spatiotemporal and influencing factors of ASQ were the most strongly correlated factors in each quarter and region. The predominant source of aviation complaints across all types of airports is related to fundamental services, with check-in services identified as the most impactful category affecting ASQ. This study, based on 60 months of statistical data, offers a comprehensive evaluation of ASQ throughout the entire airport network in mainland China, from the perspective of aviation complaints. Additionally, a systematic ASQ evaluation method and research system encompassing time, space, and elements were established. This framework not only stimulates the improvement and enhancement of ASQ but also provides a theoretical foundation for differentially enhancing ASQ in regional airports. Overall, our results contribute to breaking through the qualitative research thinking system in ASQ research from a theoretical perspective, paving the way for exploring broader research in enhancing the quality of ground civil aviation services.}
}
@incollection{TAMIR2024263,
title = {Chapter Five - Predicting other people shapes the social mind},
editor = {Bertram Gawronski},
series = {Advances in Experimental Social Psychology},
publisher = {Academic Press},
volume = {69},
pages = {263-315},
year = {2024},
booktitle = {Advances in Experimental Social Psychology},
issn = {0065-2601},
doi = {https://doi.org/10.1016/bs.aesp.2023.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S006526012300028X},
author = {Diana I. Tamir and Mark A. Thornton},
keywords = {Social cognition, Emotion, Mentalizing, Prediction, Social neuroscience, Mental states, Actions, Traits, Dynamics},
abstract = {People have a rich understanding of the social world within which they are embedded. How do they organize this social knowledge? And to what end? We suggest that these two questions are intimately linked. We review a burgeoning literature which shows how the social mind organizes different layers of social knowledge—including knowledge of actions, mental states, personality traits, situations, and relationships—into parsimonious low-dimensional maps. By distilling much of the complexity of the social world down to coordinates on a few key psychological dimensions, people construct a highly efficient representation of the social world. We go on to review recent research showing that these maps facilitate accurate, automatic prediction of real-world social dynamics. Specifically, the placement of stimuli within these maps implicitly encodes predictions about the social future, both within the same layer of social knowledge, and across different layers. Moreover, the ability of these maps to predict the social future is no coincidence: increasing evidence suggests that the goal of prediction actively shapes the way people organize social knowledge. We conclude by discussing challenges and future directions for studying the predictive social mind.}
}
@article{EVANS1989499,
title = {A review and synthesis of OR/MS and creative problem solving (Parts 1 and 2)},
journal = {Omega},
volume = {17},
number = {6},
pages = {499-524},
year = {1989},
issn = {0305-0483},
doi = {https://doi.org/10.1016/0305-0483(89)90055-8},
url = {https://www.sciencedirect.com/science/article/pii/0305048389900558},
author = {JR Evans},
keywords = {creativity, problem-solving, OR/MS methodology},
abstract = {Problem solving in operations research and management science is both a science and an art. While much has been written about the science of OR/MS, relatively little has been written about the art. Art, by its very nature, is a creative discipline. This implies that creative thinking should be an important component of OR/MS methodology. A rich literature on creative thinking exists, mostly in the domains of psychology and design. Creativity has been indirectly discussed in OR/MS research and practice, but seldom as a central theme. The purpose of this paper is to review the literature on creative thinking and problem solving that has special relevance to traditional OR/MS methodology. In this part we focus on problem solving, the need for creative thinking, and fundamental concepts of creativity. In Part 2 we synthesize the OR/MS literature that relates to creative thinking, and provide a framework for integrating structured creative thinking processes with OR/MS methodology. Finally, we discuss implications for education, research, and practice.}
}
@article{PHILLIPS2009597,
title = {Advances in evolution and genetics: Implications for technology strategy},
journal = {Technological Forecasting and Social Change},
volume = {76},
number = {5},
pages = {597-607},
year = {2009},
note = {Two Special Sections: Advances in Evolution and Genetics: Implications for Technology Strategy The Digital Economy in Asia},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2008.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0040162508001522},
author = {Fred Phillips and Yu-Shan Su},
keywords = {Evolution, Selection, Genetics, Technology strategy, Technology forecasting},
abstract = {Genetic and evolutionary principles are of great importance to technology strategists, both directly (as in the forecasting of genetic engineering technologies) and as a source of metaphor and perspective on socio-technical change. Recent rapid progress in the molecular sciences have revealed new genetic mechanisms of evolution, and introduced new controversies of interpretation. How do these recent developments affect technology forecasting and our view of technological evolution? This paper provides a quick primer for TFSC readers on several new developments in evolution and genetics, comments upon a number of common misconceptions and pitfalls in evolutionary thinking, and critically describes some controversies and open questions, introducing key readings and sources. It relates genetic and evolutionary knowledge, analogies and metaphors to areas of interest to researchers in technology forecasting and assessment, noting possible future directions. The paper concludes with an overview of the other papers in this special section.}
}
@article{PATON1997245,
title = {The organisations of hereditary information},
journal = {Biosystems},
volume = {40},
number = {3},
pages = {245-255},
year = {1997},
issn = {0303-2647},
doi = {https://doi.org/10.1016/S0303-2647(96)01652-8},
url = {https://www.sciencedirect.com/science/article/pii/S0303264796016528},
author = {Ray Paton},
keywords = {Gene, Syntax/semantics, Hierarchy, Epigenetic system, Talkback},
abstract = {The meaning of hereditary information is not simple. It includes not only what a system receives and transmits but particularly what it makes. The syntactic basis to hereditary information is also not straightforward. For example, is DNA instructions, or data, or both? The answer to this question requires an appreciation of the meaning of the information yet there are a number of possible semantic systems for describing hereditary information including proteins and development. The descriptive boundaries of hereditary information are examined by locating some general organising themes including hierarchy, ecology, regulation, epigenetic systems and talkback. Though metaphors have limits in terms of their explanatory power, a number have influenced the development of biological thinking and biosystems have variously been represented as chemical laboratories, computers, electromechanical machines and societies. In this article a further metaphor is discussed, that of life-as-a-play or dance in which the trio of script (genome), cast (metabolism) and stage (cellular structure) co-exist and pre-exist the phenotypic life history which inherits them. A fuller examination of this trio provides an important perspective on the study of the organisations of information processing in hereditary systems.}
}
@article{WILKINS2007635,
title = {Inexpensive fusion methods for enhancing feature detection},
journal = {Signal Processing: Image Communication},
volume = {22},
number = {7},
pages = {635-650},
year = {2007},
note = {"Special Issue on Content-Based Multimedia Indexing and Retrieval"},
issn = {0923-5965},
doi = {https://doi.org/10.1016/j.image.2007.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0923596507000732},
author = {Peter Wilkins and Tomasz Adamek and Noel E. O’Connor and Alan F. Smeaton},
keywords = {Feature detection, Data fusion, TRECVID},
abstract = {Recent successful approaches to high-level feature detection in image and video data have treated the problem as a pattern classification task. These typically leverage the techniques learned from statistical machine learning, coupled with ensemble architectures that create multiple feature detection models. Once created, co-occurrence between learned features can be captured to further boost performance. At multiple stages throughout these frameworks, various pieces of evidence can be fused together in order to boost performance. These approaches whilst very successful are computationally expensive, and depending on the task, require the use of significant computational resources. In this paper we propose two fusion methods that aim to combine the output of an initial basic statistical machine learning approach with a lower-quality information source, in order to gain diversity in the classified results whilst requiring only modest computing resources. Our approaches, validated experimentally on TRECVid data, are designed to be complementary to existing frameworks and can be regarded as possible replacements for the more computationally expensive combination strategies used elsewhere.}
}
@article{WILLMANN201616,
title = {Robotic timber construction — Expanding additive fabrication to new dimensions},
journal = {Automation in Construction},
volume = {61},
pages = {16-23},
year = {2016},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2015.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0926580515002046},
author = {Jan Willmann and Michael Knauss and Tobias Bonwetsch and Anna Aleksandra Apolinarska and Fabio Gramazio and Matthias Kohler},
keywords = {Non-standard timber structures, Automated assembly, Computational design, Industrial full scale implementation, Additive digital fabrication, Robotic Timber Construction},
abstract = {This paper presents a novel approach to non-standard timber assembly – Robotic Timber Construction (RTC) – where robotic fabrication is used to expand additive digital fabrication techniques towards industrial full scale dimensions. Featuring robotic systems that grasp, manipulate, and finally position building components according to a precise digital blueprint, RTC combines robotic assembly procedures and advanced digital design of non-standard timber structures. The resulting architectural morphologies allow for a convergence of aesthetic and functional concerns, enabling structural optimisation through the locally differentiated aggregation of material. Initiated by the group of Gramazio Kohler Research at ETH Zurich, this approach offers a new perspective on automated timber construction, where the focus is shifted from the processing of single parts towards the assembly of generic members in space. As such, RTC promotes unique advantages over conventional approaches to timber construction, such as, for example, CNC joinery and cutting: through the automated placement of material exactly where it is needed, RTC combines additive and largely waste-free construction with economic assembly procedures, it does not require additional external building reference, and it offers digital control across the entire building process, even when the design and assembly information are highly complex. This paper considers 1) research parameters for the individual components of RTC (such as computational design processes, construction methods and fabrication strategies), and 2) the architectural implications of integrating these components into a systemic, unifying process at the earliest stages of design. Overall, RTC leads to profound changes in the design, performance and expressive language of architecture and thus fosters the creation of architecture that profoundly reinvents its constructive repertoire.}
}
@article{BENDETOWICZ2017216,
title = {Brain morphometry predicts individual creative potential and the ability to combine remote ideas},
journal = {Cortex},
volume = {86},
pages = {216-229},
year = {2017},
note = {Is a "single" brain model sufficient?},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2016.10.021},
url = {https://www.sciencedirect.com/science/article/pii/S0010945216303161},
author = {David Bendetowicz and Marika Urbanski and Clarisse Aichelburg and Richard Levy and Emmanuelle Volle},
keywords = {Creativity, Semantic associations, Rostral prefrontal, Frontal pole, Morphometry},
abstract = {For complex mental functions such as creative thinking, inter-individual variability is useful to better understand the underlying cognitive components and brain anatomy. Associative theories propose that creative individuals have flexible semantic associations, which allows remote elements to be formed into new combinations. However, the structural brain variability associated with the ability to combine remote associates has not been explored. To address this question, we performed a voxel-based morphometry (VBM) study and explored the anatomical connectivity of significant regions. We developed a Remote Combination Association Task adapted from Mednick's test, in which subjects had to find a solution word related to three cue words presented to them. In our adaptation of the task, we used free association norms to quantify the associative distance between the cue words and solution words, and we varied this distance. The tendency to solve the task with insight and the ability to evaluate the appropriateness of a proposed solution were also analysed. Fifty-four healthy volunteers performed this task and underwent a structural MRI. Structure–function relationships were analysed using regression models between grey matter (GM) volume and task performance. Significant clusters were mapped onto an atlas of white matter (WM) tracts. The ability to solve the task, which depended on the associative distance of the solution word, was associated with structural variation in the left rostrolateral prefrontal and posterior parietal regions; the left rostral prefrontal region was connected to distant regions through long-range pathways. By using a creative combination task in which the semantic distance between words varied, we revealed a brain network centred on the left frontal pole that appears to support the ability to combine information in new ways by bridging the semantic distance between pieces of information.}
}
@article{ALCANTUD2022118276,
title = {Ranked hesitant fuzzy sets for multi-criteria multi-agent decisions},
journal = {Expert Systems with Applications},
volume = {209},
pages = {118276},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118276},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422014142},
author = {José Carlos R. Alcantud},
keywords = {Hesitant fuzzy set, Aggregation operator, Score, Ranking, Decision making},
abstract = {This paper introduces and investigates ranked hesitant fuzzy sets, a novel extension of hesitant fuzzy sets that is less demanding than both probabilistic and proportional hesitant fuzzy sets. This new extension incorporates hierarchical knowledge about the various evaluations submitted for each alternative. These evaluations are ranked (for example by their plausibility, acceptability, or credibility), but their position does not necessarily derive from supplementary numerical information (as in probabilistic and proportional hesitant fuzzy sets). In particular, strictly ranked hesitant fuzzy sets arise when no ties exist, i.e., when for any fixed alternative, each submitted evaluation is either strictly more plausible or strictly less plausible than any other submitted evaluation. A detailed comparison with similar models from the literature is performed. Then in order to produce a natural strategy for multi-criteria multi-agent decisions with ranked hesitant fuzzy sets, canonical representations, scores and aggregation operators are designed in the framework of ranked hesitant fuzzy sets. In order to help implementation of this model, Mathematica code is provided for the computation of both scores and aggregators. The decision-making technique that is prescribed is tested with a comparative analysis with four methodologies based on probabilistic hesitant fuzzy information. A conclusion of this numerical exercise is that this methodology is reliable, applicable and robust. All these evidences show that ranked hesitant fuzzy sets are an intuitive extension of the hesitant fuzzy set model designed by V. Torra, that can be implemented in practice with the aid of computationally assisted algorithms.}
}
@article{ROLISON2022105401,
title = {Developmental differences in description-based versus experience-based decision making under risk in children},
journal = {Journal of Experimental Child Psychology},
volume = {219},
pages = {105401},
year = {2022},
issn = {0022-0965},
doi = {https://doi.org/10.1016/j.jecp.2022.105401},
url = {https://www.sciencedirect.com/science/article/pii/S0022096522000303},
author = {Jonathan J. Rolison and Thorsten Pachur and Teresa McCormack and Aidan Feeney},
keywords = {Decision making under risk, Children, Computational modeling, Description-based decision making, Experience-based decision making, Risk taking},
abstract = {The willingness to take a risk is shaped by temperaments and cognitive abilities, both of which develop rapidly during childhood. In the adult developmental literature, a distinction is drawn between description-based tasks, which provide explicit choice–reward information, and experience-based tasks, which require decisions from past experience, each emphasizing different cognitive demands. Although developmental trends have been investigated for both types of decisions, few studies have compared description-based and experience-based decision making in the same sample of children. In the current study, children (N = 112; 5–9 years of age) completed both description-based and experience-based decision tasks tailored for use with young children. Child temperament was reported by the children’s primary teacher. Behavioral measures suggested that the willingness to take a risk in a description-based task increased with age, whereas it decreased in an experience-based task. However, computational modeling alongside further inspection of the behavioral data suggested that these opposite developmental trends across the two types of tasks both were associated with related capacities: older (vs. younger) children’s higher sensitivity to experienced losses and higher outcome sensitivity to described rewards and losses. From the temperamental characteristics, higher attentional focusing was linked with a higher learning rate on the experience-based task and a bias to accept gambles in the gain domain on the description-based task. Our findings demonstrate the importance of comparing children’s behavior across qualitatively different tasks rather than studying a single behavior in isolation.}
}
@incollection{SCHAUB2022555,
title = {Chapter 22 - Conclusions},
editor = {Michael Schaub and Marc Kéry},
booktitle = {Integrated Population Models},
publisher = {Academic Press},
pages = {555-563},
year = {2022},
isbn = {978-0-323-90810-8},
doi = {https://doi.org/10.1016/B978-0-12-820564-8.24002-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128205648240025},
author = {Michael Schaub and Marc Kéry},
keywords = {Continuous time scale, Full annual cycle integrated population model, Future developments, Individual heterogeneity, Long-term ecological research, Multispecies integrated population model, Sampling design, Spatial integrated population model, Spatial scale},
abstract = {In this final chapter, we first look back and briefly summarize what we have learned in this book. We then look forward and sketch out possible avenues of future research into integrated population models (IPMs) and where it may or should go. We especially foresee likely future developments in three areas. The first is in developing alternative formulations of the population, or process, model in an IPM, which currently is mostly a classical matrix population model. In the future, we expect to see refinements along some or all of the spatial, temporal, and individual axes of fundamental demographic information—that is, a general shift away from discrete to more continuous scales along these dimensions of the description of population dynamics. In particular, we think a more widespread “spatialization” of IPMs is imminent. We also think that IPMs for two or more species with explicit links among them will increasingly be developed because they allow the study of interactions among species at a very basic mechanistic level. The second area of likely future progress in IPMs deals with the observation model, especially the Gaussian error model in the state-space model for population counts. In a sense, this model is a misspecification that cannot explicitly account for the false-positives and false-negatives that now are so commonly included in the capture-recapture class of models. The third area where we envision future progress in IPMs is with more fundamental statistical and computational work. We expect further progress in algorithm fitting, goodness-of-fit testing, models that account for dependence among the components of joint likelihood, and the study and development of more effective sampling designs. Finally, we are excited to see many more applications of existing and future IPMs to improve our scientific conclusions and conservation and wildlife management decisions.}
}
@article{SAEED2022122012,
title = {A simple approach for short-term wind speed interval prediction based on independently recurrent neural networks and error probability distribution},
journal = {Energy},
volume = {238},
pages = {122012},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.122012},
url = {https://www.sciencedirect.com/science/article/pii/S036054422102260X},
author = {Adnan Saeed and Chaoshun Li and Zhenhao Gan and Yuying Xie and Fangjie Liu},
keywords = {Wind speed interval prediction, Independently recurrent neural networks, Quantile regression, Error prediction, Distribution estimation},
abstract = {Improving the quality of Wind Speed Interval prediction is important to maximize the usage of integrated wind energy as well as to reduce the adverse effects of the uncertainties, introduced by the random fluctuations of wind, to the power systems. This paper utilizes independently recurrent neural network to propose two new interval prediction frameworks. This network possesses the ability to retain memory at different lengths, which is helpful in capturing temporal features, especially for multi-horizon forecasts where the local dynamics get quite involved. In the first approach, we integrated a quantile regression loss function into this network to generate the intervals. This framework however, require to train different regressors to generate the conditional quantiles. Removing this limitation, a new simple and intuitive approach, is proposed which estimates the prediction intervals using a Gaussian function centered on the prediction and estimated error by a point prediction model and an error prediction model respectively. In our computational experiments, which involve two different wind fields contributing to eight different cases, an improvement of 43% and 12%, in average coverage width criterion index, over traditional models and LSTM based model respectively is remarkable. Thus, the proposed framework is able to produce high quality PIs while simultaneously reducing the computational cost.}
}
@article{LEBUDA202466,
title = {A meta-perspective on the creative metacognition framework. Reply to comments on “A systematic framework of creative metacognition”},
journal = {Physics of Life Reviews},
volume = {50},
pages = {66-71},
year = {2024},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2024.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S1571064524000794},
author = {Izabela Lebuda and Mathias Benedek}
}
@article{HEDE2015522,
title = {TRIZ and the Paradigms of Social Sustainability in Product Development Endeavors},
journal = {Procedia Engineering},
volume = {131},
pages = {522-538},
year = {2015},
note = {TRIZ and Knowledge-Based Innovation in Science and Industry},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2015.12.447},
url = {https://www.sciencedirect.com/science/article/pii/S1877705815043398},
author = {Shantesh Hede and Paula Verandas Ferreira and Manuel Nunes Lopes and Luis Alexandre Rocha},
keywords = {Sustainability, Decision Modeling, TRIZ, Product Development ;},
abstract = {The Business practices of an industrialized civilization are responsible for intensifying the dynamics of the interdependent environmental, social and economic domains of our ecosystem. The worldwide objective to accomplish Sustainability is invariably addressed by Policy makers and Institutions by means of moderately disparate co-relations between Environmental and Social considerations. The dimension of Social Sustainability has a direct co-relation towards the extended continuation of a globalized Enterprise. The stated co-relation is an interconnected and interdependent network comprising of growth in Innovation and Sustainability at the Environmental and Economic frontiers. From the standpoint of Innovation, the 20th century has been dominated by both TRIZ with OTSM and Kurzweil's Law of Accelerating Returns to steer the future of revolutionary innovations. Moreover, TRIZ and its evolved counterpart OTSM have been extensively utilized for macro-scale problem solving scenarios, while Kurzweil's Law has reached up to quantum scale whereby matter as we know exhibits an entire range of unique properties with a potential to dramatically transform our human civilization. Accordingly, the perceived limitations and vague applicability of TRIZ in sub-macro scale innovations has been discussed. The contemporary tools for project evaluation (e.g.: cost benefit analysis) and product development (e.g.: linear stage-gate process) quintessential for commercializing innovations are identified to be limited, both in scope and accuracy for delivering a long term ‘sustainable’ competitive advantage to an Enterprise. Consequently, the proposed conceptual Multifaceted Framework addresses the issue of social sustainability in Product Development. The underpinnings of Systems Thinking, TRIZ and OTSM, Complex Adaptive Systems, Socio-Economics & Human Behavior forms the fundamental basis of the proposed Multifaceted Framework. The novel perspective offered by the proposed Framework enables product development teams to overcome the inherent myopia and other limitations associated with the contemporary Environmental Life Cycle Analysis and Sustainability related Decision Models. An Expert opinion based evaluation technique in conjugation with a Multilayered Decision Modeling Method have been incorporated as a salient features in the proposed framework. The evaluation technique is utilized for assigning numerical values to the pertinent sustainability related criteria of the Multilayered Decision Model. The proposed Framework plays a crucial role in product development and decision modeling across the Idea Screening Phase (Stage 2) up to the Feasibility Analysis Phase (Stage 4). In addition, a modified version Taguchi Loss Function is included to exemplify a tangible relation between Product Quality parameters and Sustainability. The objective of the proposed framework is to provide an efficient, yet comprehensive evaluation as well as an effective product development strategy with a distinct and a holistic outlook on Social Sustainability.}
}
@article{AHMADI2022101232,
title = {Energy efficiency improvement and emission reduction potential of domestic gas burners through re-orientating the angle and position of burner holes: Experimental and numerical study},
journal = {Thermal Science and Engineering Progress},
volume = {32},
pages = {101232},
year = {2022},
issn = {2451-9049},
doi = {https://doi.org/10.1016/j.tsep.2022.101232},
url = {https://www.sciencedirect.com/science/article/pii/S2451904922000397},
author = {Ali Akbar Ahmadi and Alireza Rahbari and Mostafa Mohamadi},
keywords = {Experimental and numerical study, Domestic burners, Burner geometry, Turbulent combustion, Thermal efficiency},
abstract = {The trend towards enhancing the thermal performance of domestic cooking burners necessitates developing a new design for such devices. With this picture in mind, this paper numerically and experimentally investigates the effect of burner head design configurations on the energy efficiency and CO emission of domestic gas burners. The results of a three-dimensional steady-state computational fluid dynamics (CFD) model is validated with the experimental data according to Volunteers in Technical Assistance (VITA) standard in cold start, hot start, and Simmer condition for two types of burners. Having the model validated, a step-by-step approach has been undertaken to improve the design of these reference cases, resulted in a total number of nine burner configurations analysed in this research. This is followed by determining the influence of introduced geometries on the thermal efficiency of burners. Based on the insights from the numerical model, the most efficient burner exhibits 3.3–22.2% higher thermal efficiency and 20.2–32.6% lower CO emission—depending on the gas flow rate—relative to the conventional burners. The optimised design can be implemented into existing burners with relatively little need for reconstruction.}
}
@article{GARIRA2023122,
title = {The transmission mechanism theory of disease dynamics: Its aims, assumptions and limitations},
journal = {Infectious Disease Modelling},
volume = {8},
number = {1},
pages = {122-144},
year = {2023},
issn = {2468-0427},
doi = {https://doi.org/10.1016/j.idm.2022.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S2468042722001075},
author = {Winston Garira and Bothwell Maregere},
keywords = {Single scale modelling of infectious disease dynamics, Multiscale modelling of infectious disease dynamics, Scales of organization of infectious disease system, Transmission mechanism theory of disease dynamics, Levels of organization of infectious disease system, The replication-transmission relativity theory of disease dynamics},
abstract = {Most of the progress in the development of single scale mathematical and computational models for the study of infectious disease dynamics which now span over a century is build on a body of knowledge that has been developed to address particular single scale descriptions of infectious disease dynamics based on understanding disease transmission process. Although this single scale understanding of infectious disease dynamics is now founded on a body of knowledge with a long history, dating back to over a century now, that knowledge has not yet been formalized into a scientific theory. In this article, we formalize this accumulated body of knowledge into a scientific theory called the transmission mechanism theory of disease dynamics which states that at every scale of organization of an infectious disease system, disease dynamics is determined by transmission as the main dynamic disease process. Therefore, the transmission mechanism theory of disease dynamics can be seen as formalizing knowledge that has been inherent in the study of infectious disease dynamics using single scale mathematical and computational models for over a century now. The objective of this article is to summarize this existing knowledge about single scale modelling of infectious dynamics by means of a scientific theory called the transmission mechanism theory of disease dynamics and highlight its aims, assumptions and limitations.}
}
@article{ABDELHAMID2023101986,
title = {Discovering epistasis interactions in Alzheimer’s disease using integrated framework of ensemble learning and multifactor dimensionality reduction (MDR)},
journal = {Ain Shams Engineering Journal},
volume = {14},
number = {7},
pages = {101986},
year = {2023},
issn = {2090-4479},
doi = {https://doi.org/10.1016/j.asej.2022.101986},
url = {https://www.sciencedirect.com/science/article/pii/S2090447922002970},
author = {Marwa M. {Abd El Hamid} and Mohamed Shaheen and Yasser M.K. Omar and Mai S. Mabrouk},
keywords = {Epistasis Interactions, Alzheimer's disease, Personalized Medicine, Ensemble learning techniques},
abstract = {Alzheimer's disease (AD) is a complex disorder with strong genetic factors. The proposed framework is applied to Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. We present a novel framework integrating ensemble learning and MDR constructive induction algorithm to discover epistasis interactions associated with AD in a computationally efficient method. Discovering epistasis interactions is a big challenge and significantly impacts personalized medicine (PM). The applied ensemble learning algorithms are random forests (RF) with Gini index and permutation importance, Extreme Gradient Boosting (XGBoost), and classification and regression trees (CART). The classification accuracy of 5-way models varied between (0.8674–0.8758), whereas the accuracy of 2-way, 3-way, and 4-way models varied between (0.6515–0.6649), (0.7071–0.7170), and (0.7811–0.7878) respectively. The promising results of this proposed framework show high-ranked risk genes and up to 5-way epistasis models that contribute to the disease risk efficiently and at higher accuracy.}
}
@article{JANG2022103225,
title = {Generative Design by Reinforcement Learning: Enhancing the Diversity of Topology Optimization Designs},
journal = {Computer-Aided Design},
volume = {146},
pages = {103225},
year = {2022},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2022.103225},
url = {https://www.sciencedirect.com/science/article/pii/S0010448522000239},
author = {Seowoo Jang and Soyoung Yoo and Namwoo Kang},
keywords = {Generative design, Topology optimization, Deep learning, Reinforcement learning, Design diversity},
abstract = {Generative design refers to computational design methods that can automatically conduct design exploration under constraints defined by designers. Among many approaches, topology optimization-based generative designs aim to explore diverse topology designs, which cannot be represented by conventional parametric design approaches. Recently, data-driven topology optimization research has started to exploit artificial intelligence, such as deep learning or machine learning, to improve the capability of design exploration. This study proposes a reinforcement learning (RL) based generative design process, with reward functions maximizing the diversity of topology designs. We formulate generative design as a sequential problem of finding optimal design parameter combinations in accordance with a given reference design. Proximal Policy Optimization is used as the learning framework, which is demonstrated in the case study of an automotive wheel design problem. To reduce the heavy computational burden of the wheel topology optimization process required by our RL formulation, we approximate the optimization process with neural networks. With efficient data preprocessing/augmentation and neural architecture, the neural networks achieve a generalized performance and symmetricity-reserving characteristics. We show that RL-based generative design produces a large number of diverse designs within a short inference time by exploiting GPU in a fully automated manner. It is different from the previous approach using CPU which takes much more processing time and involving human intervention.}
}
@article{LI2023106560,
title = {High energy capacity or high power rating: Which is the more important performance metric for battery energy storage systems at different penetrations of variable renewables?},
journal = {Journal of Energy Storage},
volume = {59},
pages = {106560},
year = {2023},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2022.106560},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X2202549X},
author = {Mingquan Li and Rui Shan and Ahmed Abdulla and Jialin Tian and Shuo Gao},
keywords = {Energy storage, Energy-to-power ratio (EPR), Decarbonization, Carbon emissions, Renewable integration, Low-carbon transition},
abstract = {Studies exploring the role and value of energy storage in deep decarbonization often overlook the balance between the energy capacity and the power rating of storage systems—a key performance parameter that can affect every part of storage operation. Here, we quantitatively evaluate the system-wide impacts of battery storage systems with various energy-to-power ratios (EPRs) and at different levels of renewable penetration. We take Jiangsu province in China as our case study, due to its high electricity consumption and aggressive renewable energy targets. Our results show the evolving role of storage: as renewable penetration increases, higher EPRs are favored, as they lead to system-wide cost reductions, lower GHG emissions, and higher power system reliability. Whereas existing studies make exogenous assumptions about the lifetime of storage, we show that lifetimes across EPRs and renewable scenarios span 10 to 20 years. Existing research can thus send false signals to investors and grid planners, delaying the deployment of storage and retarding the energy transition. By showing how different EPRs yield different benefits at different stages of the energy transition, our results help investors, policy makers, and system planners design forward-thinking and dynamic policies that encourage prudent storage uptake.}
}
@article{TEMIZER2011114,
title = {Thermomechanical contact homogenization with random rough surfaces and microscopic contact resistance},
journal = {Tribology International},
volume = {44},
number = {2},
pages = {114-124},
year = {2011},
issn = {0301-679X},
doi = {https://doi.org/10.1016/j.triboint.2010.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0301679X10002318},
author = {İ. Temizer},
keywords = {Contact mechanics, Homogenization, Thermal contact resistance, Randomness},
abstract = {We extend an earlier computational thermomechanical contact homogenization framework [Temizer İ, Wriggers P. International Journal for Numerical Methods in Engineering 2010; 83:27–58] to random rough surfaces generated through the random-field model based on the concepts of ensemble averaging and sample enlargement towards the effective limit. Additionally, the homogenization theory is revisited in order to incorporate thermal dissipation at the microscopic contact interface within a thermodynamically consistent approach that preserves dissipation across the scales. Large-scale three-dimensional computations were performed to demonstrate the effectiveness and feasibility of the computational framework for an accurate characterization of the macroscopic thermomechanical response of rough surfaces in contact.}
}
@article{ZHOU2024122849,
title = {Elevating urban sustainability: An intelligent framework for optimizing water-energy-food nexus synergies in metabolic landscapes},
journal = {Applied Energy},
volume = {360},
pages = {122849},
year = {2024},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2024.122849},
url = {https://www.sciencedirect.com/science/article/pii/S0306261924002320},
author = {Yanlai Zhou and Fi-John Chang and Li-Chiu Chang and Edwin Herricks},
keywords = {Nexus optimization, Water-energy-food (WEF), Renewable energy, Urban metabolism, Artificial intelligence (AI)},
abstract = {As global urbanization accelerates, harmonizing water, energy, and food (WEF) resources within urban contexts is pivotal for sustainable development. This study introduces the Intelligent Urban Metabolism Framework (IUMF) for synergizing WEF dynamics, with a focus on socio-technological linkages and environmental concerns arising from climate change. Through a pioneering fusion of system dynamics simulation, machine learning surrogate, metaheuristic optimization, and multi-criteria decision making techniques, IUMF offers a transformative approach to resource management under climate uncertainty. Leveraging comprehensive data sourced from Taipei, Taiwan, this study demonstrates noteworthy enhancements in WEF nexus synergies, including a 9% boost in water supply, an 8% rise in energy benefits, and a significant 13.8% increase in food production. The cases corresponding to the best solutions under the scenario depicting a wet year and high solar radiation intensity would attain the largest benefits: 873 million m3 of water supply (water sector), 90.3 million USD of power benefits (energy sector), and 79 million kg of food production (food sector). These advancements are achieved while reducing computational runtime from 20 h to 30 min. By fostering a user-friendly interface and embracing an intelligent framework, IUMF catalyzes urban sustainability efforts. Our study highlights the potential of intelligent frameworks in addressing complex urban challenges and guiding the evolution of resource-efficient systems and offers a blueprint for a more resilient and sustainable urban future.}
}
@article{WANG2022101643,
title = {Model for deep learning-based skill transfer in an assembly process},
journal = {Advanced Engineering Informatics},
volume = {52},
pages = {101643},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101643},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622001070},
author = {Kung-Jeng Wang and Luh {Juni Asrini} and Lucy Sanjaya and Hong-Phuc Nguyen},
keywords = {Convolutional neural network, Deep learning, Faster region-based convolutional neural network, Human machine interaction, Skill transfer},
abstract = {As the variety of products and manufacturing processes increases, the expansion of flexible training approaches is crucial to support the development of human skills. This study presents a model for skill transfer support that extracts experts’ relevant skills as actions and objects relevant to the action into a computational model for transferring skills. This model engages two modes of deep learning as the groundwork, namely, convolutional neural network (CNN) for action recognition and faster region-based convolutional neural network (R-CNN) for object detection. To evaluate the performance of the proposed model, a case study of the final assembly of a GPU card is conducted. The accuracy of CNN and faster R-CNN are 95.4% and 96.8%, respectively. The goal of this model is to guide junior operators during the assembly by providing step-by-step instructions in performing complex tasks. The present study facilitates flexible training in terms of adapting new skills from skilled operators to naïve operators by deep learning.}
}
@article{SAAVEDRA20091324,
title = {Experimental transition state for the Corey–Bakshi–Shibata reduction},
journal = {Tetrahedron Letters},
volume = {50},
number = {12},
pages = {1324-1327},
year = {2009},
issn = {0040-4039},
doi = {https://doi.org/10.1016/j.tetlet.2009.01.033},
url = {https://www.sciencedirect.com/science/article/pii/S0040403909000793},
author = {Jaime Saavedra and Sean E. Stafford and Matthew P. Meyer},
abstract = {Asymmetric reductions of prochiral ketones are important transformations in the syntheses of natural products, pharmaceuticals, and fine chemicals. The Corey–Bakshi–Shibata reduction is unique among hydride transfer reductions in its tremendous substrate range and catalytic nature. Here, a coordinated computational and experimental approach is taken toward understanding the origins of the high selectivity and broad substrate range, which are hallmarks of this reduction.}
}
@article{MORAVEC2023147,
title = {Global trends in disruptive technological change: social and policy implications for education},
journal = {On the Horizon},
volume = {31},
number = {34},
pages = {147-173},
year = {2023},
issn = {1074-8121},
doi = {https://doi.org/10.1108/OTH-02-2023-0007},
url = {https://www.sciencedirect.com/science/article/pii/S1074812123000325},
author = {John W. Moravec and María Cristina Martínez-Bravo},
keywords = {Literature review, Meta-analysis, Technology, Education policy, Disruptive technology, Global trends},
abstract = {Purpose
The purpose of this study is to identify global trends in disruptive technological change and map the social and policy implications, particularly as they relate to the educational ecosystem and main stakeholders across all levels of education.
Design/methodology/approach
The authors conducted a two-stage meta-analysis of 1,155 scholarly, peer-reviewed articles. The investigation involves a systematized literature review for data identification and collation adhering to defined selection criteria, and a network analysis to scrutinize data, consolidate information and unveil correlations and patterns from the literature review to produce a set of recommendations.
Findings
The study unveiled educational trends related to disruptive technologies and delineated four principal clusters representing how these technologies are transforming the education ecosystem. Additionally, a series of transversal aspects that reveal a societal vulnerability toward future prospects in the realms of ethics, sustainability, resilience, security, and policy were identified.
Practical implications
The findings spotlight an enlarging chasm between industry (and society at large) and conventional education, where many transformations triggered by disruptive technologies remain absent from teaching and learning systems. The study further offers recommendations and envisions potential scenarios, urging stakeholders to respond based on their positions concerning disruptive technologies.
Originality/value
Expanding from the meta-analysis of pertinent literature, this paper offers four collections of curated resources, four mini case studies and four scenarios for policymakers and local communities to consider, enabling them to plot courses for their optimal futures.}
}
@article{RICCARDO20211011,
title = {Increasing supply chain resilience through efficient redundancy allocation: a risk-averse mathematical model},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {1},
pages = {1011-1016},
year = {2021},
note = {17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.120},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321008752},
author = {Aldrighetti Riccardo and Battini Daria and Ivanov Dmitry},
keywords = {disruption risk, risk-averse mathematical model, supply chain network design, resilient supply chain, efficient redundancy allocation, COVID-19},
abstract = {The COVID-19 pandemic has created significant uncertainty in all areas of life, including supply chains (SCs). This paper presents a new risk-averse mixed-integer nonlinear problem mathematical model for the design and planning of a two-echelon resilient SC network. Disruption events, which can partially or completely reduce the available capacity, are included in the model. The model’s objective is to minimise the total costs by determining the optimal facility location and capacity, allocation flows and resilience actions for hedging against disruption risk. A solution procedure is tested through computational experiments, and managerial insights were formed based on a numerical example for several disruption configurations, with a specific case of long-term crises similar to the COVID-19 pandemic. The results showed that recovery activities are the most efficient actions to take for a short-term disruption event. Besides, proactive resilience investment in a protection system and flexibility enhancement allows the SC to handle the disruption period with a limited increase in network building costs and overcapacity.}
}
@article{CERULLI2024111976,
title = {Optimal initial donor selection for the synthetic control method},
journal = {Economics Letters},
volume = {244},
pages = {111976},
year = {2024},
issn = {0165-1765},
doi = {https://doi.org/10.1016/j.econlet.2024.111976},
url = {https://www.sciencedirect.com/science/article/pii/S0165176524004609},
author = {Giovanni Cerulli},
keywords = {Synthetic control method, Machine learning, Program evaluation},
abstract = {This paper argues that the performance of the Synthetic Control Method (SCM) can depend on the initial set of control units (donors) due to a bias–variance trade-off. A forward stepwise selection algorithm, validated via cross-validation, may reduce prediction error by suggesting to use fewer donors.}
}
@article{MAMAT201311,
title = {MAR: Maximum Attribute Relative of soft set for clustering attribute selection},
journal = {Knowledge-Based Systems},
volume = {52},
pages = {11-20},
year = {2013},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2013.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0950705113001706},
author = {Rabiei Mamat and Tutut Herawan and Mustafa Mat Deris},
keywords = {Data mining, Soft set theory, Clustering attributes, Attribute relative, Complexity},
abstract = {Clustering, which is a set of categorical data into a homogenous class, is a fundamental operation in data mining. One of the techniques of data clustering was performed by introducing a clustering attribute. A number of algorithms have been proposed to address the problem of clustering attribute selection. However, the performance of these algorithms is still an issue due to high computational complexity. This paper proposes a new algorithm called Maximum Attribute Relative (MAR) for clustering attribute selection. It is based on a soft set theory by introducing the concept of the attribute relative in information systems. Based on the experiment on fourteen UCI datasets and a supplier dataset, the proposed algorithm achieved a lower computational time than the three rough set-based algorithms, i.e. TR, MMR, and MDA up to 62%, 64%, and 40% respectively and compared to a soft set-based algorithm, i.e. NSS up to 33%. Furthermore, MAR has a good scalability, i.e. the executing time of the algorithm tends to increase linearly as the number of instances and attributes are increased respectively.}
}
@article{MOHAN2020771,
title = {Spread Spectrum Hop Count analyzing technique based code-division multiple access for data frequencies examining in wireless network},
journal = {Computer Communications},
volume = {150},
pages = {771-776},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2019.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419304980},
author = {N. Mohan},
keywords = {Data transfer, Quality improvement, Spread Spectrum, Hop Count, CDMA},
abstract = {Code-division multiple access (CDMA) is a bandwidth access technique used by different radio waves and signal advancements. CDMA is a way of providing multiple access, where transmitters can send data at the same time, where a single clock channel can be completed. It enables us to share data frequencies with a few systems (refer to the data transfer and capacity). From the multiple backward spaces allow this CDMA uses a wide range of novelty innovation and exceptional coding scheme (where each transmitter is allocated code). In this work, the different application of the Spread Spectrum Hop Count Analyzing Technique (SSHCA–CDMA) is presented which organizes information testing techniques to create accessible assessment data, with the ultimate goal of providing the most efficient techniques for execution improvement thinking. The underlying area of eligibility testing and the evaluation of metadata inquiry are the expectation space, the data that select the most effective regulatory function. Similarly, in this work, master-based techniques have been demonstrated to validate and analyze SSHCA cells. Long, most recent developments have retained a perspective and the nature of customer correspondence management.}
}
@article{KWIATKOWSKA201335,
title = {Fuzzy logic and semiotic methods in modeling of medical concepts},
journal = {Fuzzy Sets and Systems},
volume = {214},
pages = {35-50},
year = {2013},
note = {Soft Computing in the Humanities and Social Sciences},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2012.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S0165011412001376},
author = {Mila Kwiatkowska and Krzysztof Kielan},
keywords = {Fuzzy system models, Medicine, Cognitive sciences, Decision support systems, Depression},
abstract = {The field of medicine is a quickly growing area of application for computer-based systems. However, the use of computerized methods in this knowledge-intensive and expert-based discipline brings multiple challenges. The major problem is the modeling, representing, and interpreting of diverse medical concepts. For example, some symptoms and their etiologies are described in terms of molecular biology and genetics, physiological processes are defined using models from chemistry and physics; yet mental disorders are defined in more subjective terms of feelings, behaviours, habits, and life events. Thus, the representation of medical concepts must be sufficiently expressive to model concepts which are inherently complex, context-dependent, evolving, and often imprecise. Furthermore, the representation must be formal or, at least, sufficiently rigorous in order to be processed by computers and at the same time, the representation must be human-readable in order to be validated by humans. In this paper, we describe the modeling process of medical concepts as a mapping from the real-world medical concepts into their computational models, and further into their physical implementation. First, we define the notion of a concept as a fundamental unit of knowledge and specify the fundamental principles of the computational representation of a concept. Second, we describe the characteristics of medical concepts, specifically their historical and cultural changeability, their social and cultural ambiguity, and their varied levels of precision. Third, we present a meta-modeling framework for computational representation of medical concepts. Our framework is based on fuzzy logic and semiotic methods which allow us to explicitly model two important characteristics of medical concepts: imprecision and context-dependency. We present the framework using an example of a mental disorder, specifically, the concept of clinical depression. To exemplify the changeable and evolutionary character of medical concepts, we discuss the development of the diagnostic criteria for depression. Finally, we use the example of the assessment of depression to describe the computational representation for polythetic and multi-dimensional concepts and for categorical and non-categorical concepts. We demonstrate how the proposed modeling framework utilizes (1) a fuzzy-logic approach to represent the non-categorical (continuous) nature of the symptoms and (2) a semiotic approach to represent the polythetic (contextual interpretation) and dimensional nature of the symptoms.}
}
@article{KHAN2023110525,
title = {AAD-Net: Advanced end-to-end signal processing system for human emotion detection & recognition using attention-based deep echo state network},
journal = {Knowledge-Based Systems},
volume = {270},
pages = {110525},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110525},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123002757},
author = {Mustaqeem Khan and Abdulmotaleb {El Saddik} and Fahd Saleh Alotaibi and Nhat Truong Pham},
keywords = {Affective computing, Attention mechanism, Convolution neural network, Echo state networks, Emotion recognition, Human–computer interaction, Audio speech signals},
abstract = {Speech signals are the most convenient way of communication between human beings and the eventual method of Human–Computer Interaction (HCI) to exchange emotions and information. Recognizing emotions from speech signals is a challenging task due to the sparse nature of emotional data and features. In this article, we proposed a Deep Echo-State-Network (DeepESN) system for emotion recognition with a dilated convolution neural network and multi-headed attention mechanism. To reduce the model complexity, we incorporate a DeepESN that combines reservoir computing for higher-dimensional mapping. We also used fine-tuned Sparse Random Projection (SRP) to reduce dimensionality and adopted an early fusion strategy to fuse the extracted cues and passed the joint feature vector via a classification layer to recognize emotions. Our proposed model is evaluated on two public speech corpora, EMO-DB and RAVDESS, and tested for subject/speaker-dependent/independent performance. The results show that our proposed system achieves a high recognition rate, 91.14, 85.57 for EMO-DB, and 82.01, 77.02 for RAVDESS, using speaker-dependent and independent experiments, respectively. Our proposed system outperforms the State-of-The-Art (SOTA) while requiring less computational time.}
}
@article{BAUM1997195,
title = {A Bayesian approach to relevance in game playing},
journal = {Artificial Intelligence},
volume = {97},
number = {1},
pages = {195-242},
year = {1997},
note = {Relevance},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00059-3},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000593},
author = {Eric B. Baum and Warren D. Smith},
keywords = {Relevance, Game tree search, Game theory, Computer game playing, Directed search, Utility guided search, Metareasoning, Computer chess, Computer Othello, Game trees, Graphical model, Decision theory, Utility, Bayesian model, Evaluation function, Rational search},
abstract = {The point of game tree search is to insulate oneself from errors in the evaluation function. The standard approach is to grow a full width tree as deep as time allows, and then value the tree as if the leaf evaluations were exact. The alpha-beta algorithm implements this with great computational efficiency. This approach has been effective in many games. Our approach is to form a Bayesian model of our uncertainty. We adopt an evaluation function that returns a probability distribution estimating the probability of various errors in valuing each position. These estimates are obtained by training from data. We thus use additional information at each leaf not available to the standard approach. We utilize this information in three ways: to evaluate which move is best after we are done expanding, to allocate additional thinking time to moves where additional time is most relevant to game outcome, and, perhaps most importantly, to expand the tree along the most relevant lines. Our measure of the relevance of expanding a given leaf provably approximates a measure of the impact of expanding the leaf on expected payoff, including the impact of the outcome of the leaf expansion on later expansion decisions. Our algorithms run (under reasonable assumptions) in time linear in the size of the final tree and hence except for a small constant factor, are as time efficient as alpha-beta. Our algorithm focuses on relevant lines, on which it can in principle grow a tree several times as deep as alpha-beta in a given amount of time. We have tested our approach on a variety of games, including Othello, Kalah, Warri, and others. Our probability independence approximations are seen to be significantly violated, but nonetheless our tree valuation scheme was found to play significantly better than minimax or the Probability Product rule when both competitors search the same tree. Our full search algorithm was found to outplay a highly ranked, directly comparable alpha-beta Othello program even when the alpha-beta program was given sizeable time odds, and also performed well against the three top Othello programs on the Internet Othello Server.}
}
@article{KS2024137917,
title = {Quantum computing basics, applications and future perspectives},
journal = {Journal of Molecular Structure},
volume = {1308},
pages = {137917},
year = {2024},
issn = {0022-2860},
doi = {https://doi.org/10.1016/j.molstruc.2024.137917},
url = {https://www.sciencedirect.com/science/article/pii/S002228602400440X},
author = {Balamurugan {K S} and Sivakami A and Mathankumar M and Yalla Jnan Devi {Satya prasad} and Irfan Ahmad},
keywords = {Qubits, Quantum computing, Quantum cryptography, Superposition},
abstract = {Quantum Computing observed a significant rise to public and technologies in past three decades, the reason behind for the development of quantum computing is to solve various problems which are so complex that traditional (classical) computers were not able to solve. New technologies, hardware components and software advancements are being discovered all around the world in order to use this powerful tool. But in addition to the development of technologies and the attempt to scale up the quantum computers, new challenges and problems too came in light which makes it tough for further progress in the quest to unlock the true development of quantum computers. Various methods has been identified for Quantum Information Processing (QIP), but the error rates were more than what we would expect often resulting in inappropriate computations which eventually gives inaccurate conclusions.In this work, we discuss about the prominent hardware and software methods to build the quantum computers with low error rates and better accuracy, we will look onto the topics related to qubits and its principles which are incorporated in Quantum Processing Units (QPUs) which govern the working of quantum computers, the topics of quantum algorithms and its methodology are also been discussed to provide a clear understanding of the manipulation of qubits according to the purpose needed. In addition to that we will talk about the applications like quantum teleportation and cryptography which utilizes the quantum computers, and discuss about the future enhancements which can be done using this technology.}
}
@article{MARSHALL2023131,
title = {The role of quantum mechanics in cognition-based evolution},
journal = {Progress in Biophysics and Molecular Biology},
volume = {180-181},
pages = {131-139},
year = {2023},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2023.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S007961072300041X},
author = {Perry Marshall},
abstract = {In 2021 I noted that in all information-based systems we understand, Cognition creates Code, which controls Chemical reactions. Known agents write software which controls hardware, and not the other way around. I proposed the same is true in all of biology. Though the textbook description of cause and effect in biology proposes the reverse, that Chemical reactions produce Code from which Cognition emerges, there are no examples in the literature demonstrating either step. A mathematical proof for the first step, cognition generating code, is based on Turing's halting problem. The second step, code controlling chemical reactions, is the role of the genetic code. Thus a central question in biology: What is the nature and source of cognition? In this paper I propose a relationship between biology and Quantum Mechanics (QM), hypothesizing that the same principle that enables an observer to collapse a wave function also grants biology its agency: the organism's ability to act on the world instead of merely being a passive recipient. Just as all living cells are cognitive (Shapiro 2021, 2007; McClintock 1984; Lyon 2015; Levin 2019; Pascal and Pross, 2022), I propose humans are quantum observers because we are made of cells and all cells are observers. This supports the century-old view that in QM, the observer does not merely record the event but plays a fundamental role in its outcome.The classical world is driven by laws, which are deductive; the quantum world is driven by choices, which are inductive. When the two are combined, they form the master feedback loop of perception and action for all biology. In this paper I apply basic definitions of induction, deduction and computation to known properties of QM to show that the organism altering itself (and its environment) is a whole shaping its parts. It is not merely parts comprising a whole. I propose that an observer collapsing the wave function is the physical mechanism for producing negentropy. The way forward in solving the information problem in biology is understanding the relationship between cognition and QM.}
}
@article{MEMARIAN2023100022,
title = {ChatGPT in education: Methods, potentials, and limitations},
journal = {Computers in Human Behavior: Artificial Humans},
volume = {1},
number = {2},
pages = {100022},
year = {2023},
issn = {2949-8821},
doi = {https://doi.org/10.1016/j.chbah.2023.100022},
url = {https://www.sciencedirect.com/science/article/pii/S2949882123000221},
author = {Bahar Memarian and Tenzin Doleck},
keywords = {ChatGPT, Large language models, Education, Artificial intelligence, Machine learning, Data science, Pedagogy},
abstract = {ChatGPT has been under the scrutiny of public opinion including in education. Yet, less work has been done to analyze studies conducted on ChatGPT in educational contexts. This review paper examines where ChatGPT is employed in educational literature and areas of potential, challenges, and future work. A total of 63 publications were included in this review using the general framework of open and axial coding. We coded and summarized the methods, and reported potentials, limitations, and future work of each study. Thematic analysis of reviewed studies revealed that most extant studies in the education literature explore ChatGPT through a commentary and non-empirical lens. The potentials of ChatGPT include but are not limited to the development of personalized and complex learning, specific teaching and learning activities, assessments, asynchronous communication, feedback, accuracy in research, personas, and task delegation and cognitive offload. Several areas of challenge that ChatGPT is or will be facing in education are also shared. Examples include but are not limited to plagiarism deception, misuse or lack of learning, accountability, and privacy. There are both concerns and optimism about the use of ChatGPT in education, yet the most pressing need is to ensure student learning and academic integrity are not sacrificed. Our review provides a summary of studies conducted on ChatGPT in education literature. We further provide a comprehensive and unique discussion on future considerations for ChatGPT in education.}
}
@article{YANG2022849,
title = {Creative problem solving in knowledge-rich contexts},
journal = {Trends in Cognitive Sciences},
volume = {26},
number = {10},
pages = {849-859},
year = {2022},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2022.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S1364661322001565},
author = {Wenjing Yang and Adam E. Green and Qunlin Chen and Yoed N. Kenett and Jiangzhou Sun and Dongtao Wei and Jiang Qiu},
keywords = {creativity, creative problem solving, knowledge, analogy, transfer},
abstract = {Creative problem solving (CPS) in real-world contexts often relies on reorganization of existing knowledge to serve new, problem-relevant functions. However, classic creativity paradigms that minimize knowledge content are generally used to investigate creativity, including CPS. We argue that CPS research should expand consideration of knowledge-rich problem contexts, both in novices and experts within specific domains. In particular, paradigms focusing on creative analogical transfer of knowledge may reflect CPS skills that are applicable to real-world problem solving. Such paradigms have begun to provide process-level insights into cognitive and neural characteristics of knowledge-rich CPS and point to multiple avenues for fruitfully expanding inquiry into the role of crystalized knowledge in creativity.}
}
@article{RAJPAL2022119624,
title = {Psychedelics and schizophrenia: Distinct alterations to Bayesian inference},
journal = {NeuroImage},
volume = {263},
pages = {119624},
year = {2022},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2022.119624},
url = {https://www.sciencedirect.com/science/article/pii/S105381192200739X},
author = {Hardik Rajpal and Pedro A.M. Mediano and Fernando E. Rosas and Christopher B. Timmermann and Stefan Brugger and Suresh Muthukumaraswamy and Anil K. Seth and Daniel Bor and Robin L. Carhart-Harris and Henrik J. Jensen},
keywords = {Psychedelics, Schizophrenia, Information theory, Predictive processing},
abstract = {Schizophrenia and states induced by certain psychotomimetic drugs may share some physiological and phenomenological properties, but they differ in fundamental ways: one is a crippling chronic mental disease, while the others are temporary, pharmacologically-induced states presently being explored as treatments for mental illnesses. Building towards a deeper understanding of these different alterations of normal consciousness, here we compare the changes in neural dynamics induced by LSD and ketamine (in healthy volunteers) against those associated with schizophrenia, as observed in resting-state M/EEG recordings. While both conditions exhibit increased neural signal diversity, our findings reveal that this is accompanied by an increased transfer entropy from the front to the back of the brain in schizophrenia, versus an overall reduction under the two drugs. Furthermore, we show that these effects can be reproduced via different alterations of standard Bayesian inference applied on a computational model based on the predictive processing framework. In particular, the effects observed under the drugs are modelled as a reduction of the precision of the priors, while the effects of schizophrenia correspond to an increased precision of sensory information. These findings shed new light on the similarities and differences between schizophrenia and two psychotomimetic drug states, and have potential implications for the study of consciousness and future mental health treatments.}
}
@article{SELKER2005410,
title = {Fostering motivation and creativity for computer users},
journal = {International Journal of Human-Computer Studies},
volume = {63},
number = {4},
pages = {410-421},
year = {2005},
note = {Computer support for creativity},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2005.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S1071581905000443},
author = {Ted Selker},
keywords = {Communicate, Communication, Community, Computers, Creative, Creativity, Creativity-enhancing, Design, Engineering, Filter, Graphics, Human–computer, Idea, Interaction, Motivation, Product, Programming, Social, Support, Text, User interface},
abstract = {Creativity might be viewed as any process which results in a novel and useful product. People use computers for creative tasks; they flesh out ideas for text, graphics, engineering solutions, etc. Computer programming is an especially creative activity, but few tools for programming aid creativity. Computers can be designed to foster creativity as well. As a start, all computer programs should help users enumerate ideas, remember alternatives and support various ways to compare them. More sophisticated thinking aids could implement other successful techniques as well. Most computers are used in solitude; however, people depend on social supports for creativity. User scenarios can provide the important social support and gracious cues normally offered by collaborators that keep people motivated and help them consider alternatives. People also use computers to build community and to communicate. Computers should also support and filter these potentially creativity-enhancing communication acts. User-interface designers are so busy exposing features and fighting bugs that they might ignore their users’ needs for motivation and creativity support. This paper develops the notion that creativity and motivation enhancement can easily be aligned with the design of high-quality human–computer interaction. User interface toolkits and evaluations should include support for motivation and creativity-enhancing approaches.}
}
@article{MADUABUCHI2023125889,
title = {Deep neural networks for quick and precise geometry optimization of segmented thermoelectric generators},
journal = {Energy},
volume = {263},
pages = {125889},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.125889},
url = {https://www.sciencedirect.com/science/article/pii/S036054422202775X},
author = {Chika Maduabuchi and Chibuoke Eneh and Abdulrahman Abdullah Alrobaian and Mohammad Alkhedher},
keywords = {Segmented thermoelectric generator, Solar energy, Deep neural networks, Geometry optimization, Thermo-mechanical analysis, Finite element method},
abstract = {To solve the problems of the current optimization methods for solar segmented thermoelectric generator performance based on numerical methods, this paper applied deep neural networks to optimize the device geometry for improved thermo-mechanical performance. The motivation for using the deep neural network is to overcome the lengthy computational time and very high computational energy required by the traditional numerical method in optimizing the segmented thermoelectric generator performance. The numerical model is built using ANSYS software and the effects of temperature dependency in the 4 thermoelectric materials are considered to ensure result accuracy. Furthermore, 16 possible geometry parameters which were previously not considered, encompassing the individual and combined segment's heights and cross-sectional areas are optimized to find which set of parameters are the best in maximizing the device performance. The deep neural network is a regressive multilayer perceptron with network hyperparameters comprising 2 hidden layers with 5 neurons per layer. The training process is governed by the Levenberg-Marquardt standard backpropagation algorithm to minimize the mean squared error and maximize the regression correlation between the neural network forecasted outputs and the numerical-generated dataset. The most significant contribution of the proposed deep neural network is that it was able to quickly and accurately forecast the device performance in just 10 s, which was 2880 times faster than the conventional numerical-based optimization approach. Additionally, the optimized device had a maximum efficiency of 18%, which was 78% higher than that of the unoptimized device. Also, the thermal stress of the optimized device was 73% less than that of the unoptimized device design, indicating an extension in the device mechanical reliability and service lifetime. The results reported in this paper will accelerate the ease at which efficient, long-lasting segmented thermoelectric generators are manufactured by harnessing the power of artificial intelligence.}
}
@article{VARAS2023101289,
title = {Teachers’ strategies and challenges in teaching 21st century skills: Little common understanding},
journal = {Thinking Skills and Creativity},
volume = {48},
pages = {101289},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101289},
url = {https://www.sciencedirect.com/science/article/pii/S1871187123000597},
author = {Diego Varas and Macarena Santana and Miguel Nussbaum and Susana Claro and Patricia Imbarack},
keywords = {21st century skills, In-service teacher perceptions, Teacher education, Teaching practice, 6 Cs, 4 Cs},
abstract = {Faced with a world of accelerating change and rapidly-evolving technology, education systems must provide students with the skills they need to succeed in the 21st century. However, many countries have failed to incorporate the teaching of these skills within their schools. Our study therefore looks to portray teachers' understanding, strategies and obstacles in teaching these skills across Latin American classrooms. To do so, we analyzed the responses to an online survey from 1391 active teachers across 20 countries in the region. This revealed varying understandings of 21st century skills, with little common understanding. Most teachers failed to mention the skills included in the most popular framework (the 4 Cs); those who did reported using the same strategies, regardless of the skill being taught. These strategies included project-based learning, oracy activities, literacy strategies, and teamwork. We conclude that there is little or no common understanding around these skills, nor the best strategies for developing them. Our study helps understand the potential causes preventing the teaching of these skills in the classroom, a problem that extends beyond Latin America.}
}
@incollection{2024xiii,
title = {Preface},
editor = {Kok Siew Ng and Elias Martinez Hernandez and Aki Yamaguchi},
booktitle = {A New Systems Thinking Approach to Sustainable Resource Management},
publisher = {Elsevier},
pages = {xiii-xv},
year = {2024},
isbn = {978-0-323-99869-7},
doi = {https://doi.org/10.1016/B978-0-323-99869-7.00011-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323998697000115}
}
@article{WARNIER201715,
title = {Distributed monitoring for the prevention of cascading failures in operational power grids},
journal = {International Journal of Critical Infrastructure Protection},
volume = {17},
pages = {15-27},
year = {2017},
issn = {1874-5482},
doi = {https://doi.org/10.1016/j.ijcip.2017.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S1874548216300427},
author = {Martijn Warnier and Stefan Dulman and Yakup Koç and Eric Pauwels},
keywords = {Power Grids, Cascading Failures, Robustness, Real-Time Monitoring, Distributed Computation},
abstract = {Electrical power grids are vulnerable to cascading failures that can lead to large blackouts. The detection and prevention of cascading failures in power grids are important problems. Currently, grid operators mainly monitor the states (loading levels) of individual components in a power grid. The complex architecture of a power grid, with its many interdependencies, makes it difficult to aggregate the data provided by local components in a meaningful and timely manner. Indeed, monitoring the resilience of an operational power grid to cascading failures is a major challenge. This paper attempts to address this challenge. It presents a robustness metric based on the topology and operative state of a power grid to quantify the robustness of the grid. Also, it presents a distributed computation method with self-stabilizing properties that can be used for near real-time monitoring of grid robustness. The research thus provides insights into the resilience of a dynamic operational power grid to cascading failures during real-time in a manner that is both scalable and robust. Computations are pushed to the power grid network, making the results available at each node and enabling automated distributed control mechanisms to be implemented.}
}
@article{CUI2024102334,
title = {Improved matrix model of sequence grid partition based on vector space sampling},
journal = {Physical Communication},
volume = {64},
pages = {102334},
year = {2024},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2024.102334},
url = {https://www.sciencedirect.com/science/article/pii/S1874490724000521},
author = {Lina Cui},
keywords = {Inference prediction, Data evaluation, Data matrix, Noise interference, Bayesian algorithm, Sparse grid},
abstract = {In information technology and data science predictive analysis work, the goal is to infer guess values as accurately as possible. A more densely spaced grid of points should be set as a tool for operating measurement models. It has a wide range of applications in psychology, education and other social science research fields. Although the grid seems to meet the requirements of further improving the accuracy of reasoning and guessing from a certain point of view, it also means complex calculations. When multiple false values fall between another grid point, the traditional Data Oriented Architecture fails. To guarantee the accuracy and efficiency of the calculation, it is necessary to solve the problem of excessive noise in the prediction of samples in meta-learning to solve the uncertainty caused by the noise database data or the assumption of the matrix model. This paper proposes a grid partition movable inference least squares method, that is, the sparse Bayesian least squares method based on the grid minimum matrix. The method is used for solving the problem that the prediction noise of the sample in the meta-learning is too large, to solve the uncertainty brought by the generated noise database data or the array matrix model assumption. According to the Symplectic Bayesian learning of the sequence matrix model, the basic parameters for processing the input sample database data are determined. The sparse Bayesian algorithm solution is used on the grid, combined with the off-grid inference guess of the sequence matrix model and the grid division. We set a coarse grid of points around the grid of false values. The rough point grid division is set, and the grid division around the false value is divided in detail. Then select more appropriate meta-learning parameters to clean up data. In the experiment analysis, we take the small sample study as the scene and carry on the concrete analysis to the experiment. The test proof in the fitting degree of the algorithm adopted in the paper surpasses the Support Vector Machine (SVM) and the k-Nearest Neighbor (KNN) algorithm by 3.5% and 6.4% respectively. The convergence effect surpasses the contrast plan above 10 and has a bigger superiority in the inference factor thrust. It proves that the optimization method in this paper has a strong promotion effect for the application of data forecasting systems in various industries, and has theoretical value as well as practical significance.}
}
@article{MARGINEANU2014131,
title = {Systems biology, complexity, and the impact on antiepileptic drug discovery},
journal = {Epilepsy & Behavior},
volume = {38},
pages = {131-142},
year = {2014},
note = {SI: NEWroscience 2013},
issn = {1525-5050},
doi = {https://doi.org/10.1016/j.yebeh.2013.08.029},
url = {https://www.sciencedirect.com/science/article/pii/S1525505013004344},
author = {Doru Georg Margineanu},
keywords = {Systems biology, Systems/network pharmacology, Drug resistance in epilepsy, Antiepileptic drug, Polypharmacology, Multitarget drug, Phenotypic screening, Modeling, Drug discovery},
abstract = {The number of available anticonvulsant drugs increased in the period spanning over more than a century, amounting to the current panoply of nearly two dozen so-called antiepileptic drugs (AEDs). However, none of them actually prevents/reduces the post-brain insult development of epilepsy in man, and in no less than a third of patients with epilepsy, the seizures are not drug-controlled. Plausibly, the enduring limitation of AEDs' efficacy derives from the insufficient understanding of epileptic pathology. This review pinpoints the unbalanced reductionism of the analytic approaches that overlook the intrinsic complexity of epilepsy and of the drug resistance in epilepsy as the core conceptual flaw hampering the discovery of truly antiepileptogenic drugs. A rising awareness of the complexity of epileptic pathology is, however, brought about by the emergence of nonreductionist systems biology (SB) that considers the networks of interactions underlying the normal organismic functions and of SB-based systems (network) pharmacology that aims to restore pathological networks. By now, the systems pharmacology approaches of AED discovery are fairly meager, but their forthcoming development is both a necessity and a realistic prospect, explored in this review. This article is part of a Special Issue entitled “NEWroscience 2013”.}
}
@article{BANIK2022106232,
title = {Geometric systems of unbiased representatives},
journal = {Information Processing Letters},
volume = {176},
pages = {106232},
year = {2022},
issn = {0020-0190},
doi = {https://doi.org/10.1016/j.ipl.2021.106232},
url = {https://www.sciencedirect.com/science/article/pii/S0020019021001472},
author = {Aritra Banik and Bhaswar B. Bhattacharya and Sujoy Bhore and Leonardo Martínez-Sandoval},
keywords = {Computational geometry, Systems of unbiased representatives, Bicolorings, Np-Hard problems, Geometric ranges},
abstract = {Let P be a finite point set in Rd, B be a bicoloring of P and O be a family of geometric objects (that is, intervals, boxes, balls, etc). An object from O is called balanced with respect to B if it contains the same number of points from each color of B. For a collection B of bicolorings of P, a geometric system of unbiased representatives (G-SUR) is a subset O′⊆O such that for any bicoloring B of B there is an object in O′ that is balanced with respect to B. We pose and study problems on finding G-SURs. We obtain general bounds on the size of G-SURs consisting of intervals, size-restricted intervals, axis-parallel boxes and Euclidean balls. We show that the G-SUR problem is NP-Hard even in the simple case of points on a line and interval ranges. Furthermore, we study a related problem on determining the size of the largest and smallest balanced intervals for points on the real line with a random distribution and coloring. Our results are a natural extension to a geometric context of the work initiated by Balachandran et al. (Discrete Mathematics, 2018) on arbitrary systems of unbiased representatives.}
}
@incollection{SYMMONDS20123,
title = {Chapter 1 - The Neurobiology of Preferences},
editor = {Raymond Dolan and Tali Sharot},
booktitle = {Neuroscience of Preference and Choice},
publisher = {Academic Press},
address = {San Diego},
pages = {3-31},
year = {2012},
isbn = {978-0-12-381431-9},
doi = {https://doi.org/10.1016/B978-0-12-381431-9.00001-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780123814319000012},
author = {Mkael Symmonds and Raymond J. Dolan},
keywords = {Preference, choice, neuroscience, neuroeconomics, decision-making, action, value},
abstract = {Publisher Summary
The neuroscience of choice and preference dates back to the nineteenth century, with the emergence of the idea of functional specialization as a fundamental organizational principle of the brain. The development of neuroimaging techniques—in particular, functional magnetic resonance imaging (fMRI)—has meant that questions related to choice and preference can now be addressed non-invasively in humans. There are important examples where choices do not accord with internal wants. An addict may perform an action in the present despite expressing a desire to avoid doing this very action on a prior occasion. A major conundrum when thinking about neurobiological mechanisms in decision-making is the fact that choices are often noisy or stochastic. A different network of regions in precuneus, left prefrontal, and temproparietal cortex reflected endogenous inequity aversion across subjects, illustrating that even within the context of a specific task, preferences for the same stimulus feature can be expressed in different regions and modulated in a distinct manner.}
}
@article{ZHIHUI2024108706,
title = {Temperature measurement at turbine outlet achieved by a sensing net and infrared thermometry method},
journal = {International Journal of Thermal Sciences},
volume = {196},
pages = {108706},
year = {2024},
issn = {1290-0729},
doi = {https://doi.org/10.1016/j.ijthermalsci.2023.108706},
url = {https://www.sciencedirect.com/science/article/pii/S1290072923005677},
author = {Wang Zhihui and Ma Chaochen and Ji Nian},
keywords = {infrared thermometry, Temperature sensing net (TSN), Turbine outlet temperature, Conjugate heat transfer (CHT), Turbine adiabatic efficiency},
abstract = {The measurement of turbine outlet temperature is challenging because of an intense swirl and high speed at this position. However, accurate measurement of the turbine outlet temperature is fundamental for characterizing the turbine performance. The paper proposed an infrared thermometry method based on the temperature sensing net (TSN) to measure the temperature distribution at the turbine outlet. First, this article describes the design and operation of the measurement procedure through infrared technology to accomplish this difficult task. Then, the temperature and velocity distribution at the turbine outlet and the adiabatic efficiency of the turbine are obtained using the CFD (Computational Fluid Dynamics) method to verify the feasibility of the proposed scheme. And the CHT (Conjugate Heat Transfer) simulation results for the TSN show that the incoming flow mass rate has a great influence on TSN temperature. In contrast, the influence of the incoming flow temperature gradient on it is almost negligible. Moreover, the fluid flow behavior and static temperature distribution around the temperature-sensing wire (TSW) at different Mach numbers are analyzed, and the heat transfer mechanism between the TSW and the fluid is revealed. The results show that the temperature of the TSN is lower than that of the incoming flow, but the distribution law is similar. The main factor affecting the temperature difference between the TSW and the fluid is the incoming flow velocity.}
}
@article{STABLER1984155,
title = {Berwick and Weinberg on linguistics and computational psychology},
journal = {Cognition},
volume = {17},
number = {2},
pages = {155-179},
year = {1984},
issn = {0010-0277},
doi = {https://doi.org/10.1016/0010-0277(84)90017-9},
url = {https://www.sciencedirect.com/science/article/pii/0010027784900179},
author = {Edward P. Stabler}
}
@article{BARLOW1983107,
title = {Vision: A computational investigation into the human representation and processing of visual information: David Marr. San Francisco: W. H. Freeman, 1982. pp. xvi + 397},
journal = {Journal of Mathematical Psychology},
volume = {27},
number = {1},
pages = {107-110},
year = {1983},
issn = {0022-2496},
doi = {https://doi.org/10.1016/0022-2496(83)90030-5},
url = {https://www.sciencedirect.com/science/article/pii/0022249683900305},
author = {H.B. Barlow}
}
@article{LIU2024104393,
title = {Exploring the influence of students' perceptions of face-to-face collaboration on cognitive engagement and learning outcomes in collaborative programming},
journal = {Acta Psychologica},
volume = {248},
pages = {104393},
year = {2024},
issn = {0001-6918},
doi = {https://doi.org/10.1016/j.actpsy.2024.104393},
url = {https://www.sciencedirect.com/science/article/pii/S0001691824002701},
author = {Zhi Liu and Ya Gao and Yuqin Yang and Xi Kong and Liang Zhao},
keywords = {Students' perceptions of collaboration, Cognitive engagement, Epistemic network analysis, Learning outcomes, Collaborative programming},
abstract = {Collaborative programming is being increasingly used to overcome the difficulties of the individual programming process. In this study, we investigated the effect of collaborative perception on cognitive engagement and learning outcomes in collaborative programming. We used a quasi-experimental research to determine the differences in cognitive engagement and learning outcomes of three groups with different levels of collaborative perception. The findings highlight several important conclusions. First, there were significant differences in cognitive engagement and learning outcomes across collaborative perception groups. Students with high levels of collaborative perception demonstrate more comprehensive and diverse cognitive engagement, resulting in higher learning outcomes compared to those with lower perception. Second, students in the low collaborative perception group had more Clarification-Elaboration cognitive connections, and students in the high collaborative perception group had stronger Clarification-Positioning and Clarification-Verification cognitive connections. Third, collaborative perception positively moderated the relationship between cognitive engagement and learning outcomes. In particular, three cognitive engagement, Clarification, Elaboration, and Positioning, had a greater impact on performance when moderated by collaborative perceptions. These findings have practical implications for educators and course designers, emphasizing the importance of considering students' collaborative perception when forming groups and promoting effective collaborative programming.}
}
@article{COVINGTON2016869,
title = {Expanding the Language Network: Direct Contributions from the Hippocampus},
journal = {Trends in Cognitive Sciences},
volume = {20},
number = {12},
pages = {869-870},
year = {2016},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2016.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S1364661316301759},
author = {Natalie V. Covington and Melissa C. Duff},
keywords = {hippocampus, language, memory, online processing, theta oscillations},
abstract = {New research suggests that the same hippocampal computations used in support of memory are also used for language processing, providing direct neurophysiological evidence of a shared neural mechanism for memory and language. This work expands classic memory and language models and represents a new opportunity for studying the memory–language interface.}
}
@article{YANOVA201682,
title = {Relativistic Psychometrics in Subjective Scaling},
journal = {Procedia Computer Science},
volume = {102},
pages = {82-89},
year = {2016},
note = {12th International Conference on Application of Fuzzy Systems and Soft Computing, ICAFS 2016, 29-30 August 2016, Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.09.373},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916325534},
author = {Natalia Yanova},
keywords = {mental representation, relativistic psychometrics, computational theory of perceptions, image understanding, significance, expert test system, psychosemiotics},
abstract = {The article announces the possibilities of semantic modeling in the development of feedback tools in social sciences. A new approach to the computational theory of perceptions (CTP) for analysis of mental object is proposed. The article demonstrates the implementation of relativistic psychometrics for the study of mental response (opinions, expectations and attitudes). The problem of image understanding and its significance is considered in combination of soft and hard computing. It is shown that the modeling of object (its coding and decoding in ‘mental map’) obeys the semiotic and mathematical logic. Computing with perceptions for the rules of mental representation proves their identity to the laws of conservation. The article demonstrates the versatility of the semiotic description of objects in Minkowski space. It also confirms by mathematical solution C. S. Peirce's metaphor, according to which the semiology of language is a truly universal algebra of relations.}
}
@article{WOLTORNIST20212406,
title = {Aggregation and Solvation of Sodium Hexamethyldisilazide: Across the Solvent Spectrum},
journal = {The Journal of Organic Chemistry},
volume = {86},
number = {3},
pages = {2406-2422},
year = {2021},
issn = {0022-3263},
doi = {https://doi.org/10.1021/acs.joc.0c02546},
url = {https://www.sciencedirect.com/science/article/pii/S002232632101077X},
author = {Ryan A. Woltornist and David B. Collum},
abstract = {ABSTRACT
We report solution structures of sodium hexamethyldisilazide (NaHMDS) solvated by >30 standard solvents (ligands). These include: toluene, benzene, and styrene; triethylamine and related trialkylamines; pyrrolidine as a representative dialkylamine; dialkylethers including THF, tert-butylmethyl ether, and diethyl ether; dipolar ligands such as DMF, HMPA, DMSO, and DMPU; a bifunctional dipolar ligand nonamethylimidodiphosphoramide (NIPA); polyamines N,N,N′,N′-tetramethylenediamine (TMEDA), N,N,N′,N″,N″-pentamethyldiethylenetriamine (PMDTA), N,N,N′,N′-tetramethylcyclohexanediamine (TMCDA), and 2,2′-bipyridine; polyethers 12-crown-4, 15-crown-5, 18-crown-6, and diglyme; 4,7,13,16,21,24-hexaoxa-1,10-diazabicyclo[8.8.8]­hexacosane ([2.2.2] cryptand); and tris­[2-(2-methoxyethoxy)­ethyl]­amine (TDA-1). Combinations of 1H, 13C, 15N, and 29Si NMR spectroscopies, the method of continuous variations, X-ray crystallography, and density functional theory (DFT) computations reveal ligand-modulated aggregation to give mixtures of dimers, monomers, triple ions, and ion pairs. 15N–29Si coupling constants distinguish dimers and monomers. Solvation numbers are determined by a combination of solvent titrations, observed free and bound solvent in the slow exchange limit, and DFT computations. The relative abilities of solvents to compete in binary mixtures often match that predicted by conventional wisdom but with some exceptions and evidence of both competitive and cooperative (mixed) solvation. Crystal structures of a NaHMDS cryptate ion pair and a 15-crown-5-solvated monomer are included. Results are compared with those for lithium hexamethyldisilazide, lithium diisopropylamide, and sodium diisopropylamide.}
}
@article{BAI2024120687,
title = {Numerical simulation of wave-number effects on the performance of traveling wave pump-turbine in turbine mode},
journal = {Renewable Energy},
volume = {229},
pages = {120687},
year = {2024},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2024.120687},
url = {https://www.sciencedirect.com/science/article/pii/S0960148124007559},
author = {Yang Bai and Qianming Zhu and Diangui Huang},
keywords = {Traveling wave turbine, Numerical simulation, Performance characteristics, Water head, Wave number},
abstract = {Pumped storage represents an economically viable, highly efficient, and extensive energy storage solution, receiving substantial research attention worldwide. This study utilized the energy dissipation of fish swimming in the water, and the traveling wave plate was arranged in a vertical rectangular flow channel to obtain energy through reverse thinking. Numerical simulation was employed to investigate the performance characteristics of the traveling wave turbine under varying wave number across high and low water head. The results reveal that the efficiency at low water head starts to increase and then decreases with the increase of wave number. There is an optimal wave number N = 4 to maximize turbine efficiency, and the maximum efficiency under low water head condition is 89.35 %. The efficiency of the high head condition shows a continuous upward trend, with a maximum value of 92.27 % in the range of wave number studied. With the increase of wave number, the instantaneous mass flow coefficient and power coefficient become more stable. The advantage of increasing the wave number is that more traveling wave plates can participate in energy capture, which is conducive to reducing the force required by each traveling wave plate and making the pressure distribution in each working cavity more regular.}
}
@incollection{ZHANG2022363,
title = {Chapter 18 - KPF: A retrospective view on urban planning AI for 2020},
editor = {Imdat As and Prithwish Basu and Pratap Talwar},
booktitle = {Artificial Intelligence in Urban Planning and Design},
publisher = {Elsevier},
pages = {363-380},
year = {2022},
isbn = {978-0-12-823941-4},
doi = {https://doi.org/10.1016/B978-0-12-823941-4.00004-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128239414000044},
author = {Snoweria Zhang and Kate Ringo and Richard Chou and Brandon Pachuca and Eric Pietraszkiewicz and Luc Wilson},
keywords = {Computational design, Digital twin, Urban design, Future history, City planning},
abstract = {Architectural historians have been fascinated by the year 1000, as the expectation of an impending apocalypse drove the sharp contrast between a dearth of construction before and a booming market after. One thousand years later, residents of 2020 found themselves at the crossroads again with the effects of climate change looming as a global threat. We constructed this chapter as a piece of a future, speculative, and historical document that examines the use of AI in urban planning and design in 2020. As historians from 2120, we study the evolution of tools at this critical junction with the backdrop of a confluence of crises. From explorative visual interfaces, open data initiatives, and computational design to AI that augments and collaborates with humans in the design and development of the city, we present case studies of both the technology and the projects that demonstrate some of the first applications of AI in negotiating the threat of climate change. Through these first examples, we trace the development of tools and corresponding trends in urban AI to the present year of 2120. The speculative narrative frame allows for an explication of the current urban design workflow using AI alongside an opportunity to conjecture where we believe AI development in design and planning ought to be. City makers in 2020 were not involved in the development of AI technologies. This work can act to inspire technologists who are envisioning the future of the city.}
}
@article{YAMAKAWA2021478,
title = {The whole brain architecture approach: Accelerating the development of artificial general intelligence by referring to the brain},
journal = {Neural Networks},
volume = {144},
pages = {478-495},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021003543},
author = {Hiroshi Yamakawa},
keywords = {Brain reference architecture, Structure-constrained interface decomposition method, Brain information flow, Hypothetical component diagram, Brain-inspired artificial general intelligence, Whole-brain architecture},
abstract = {The vastness of the design space that is created by the combination of numerous computational mechanisms, including machine learning, is an obstacle to creating artificial general intelligence (AGI). Brain-inspired AGI development; that is, the reduction of the design space to resemble a biological brain more closely, is a promising approach for solving this problem. However, it is difficult for an individual to design a software program that corresponds to the entire brain as the neuroscientific data that are required to understand the architecture of the brain are extensive and complicated. The whole-brain architecture approach divides the brain-inspired AGI development process into the task of designing the brain reference architecture (BRA), which provides the flow of information and a diagram of the corresponding components, and the task of developing each component using the BRA. This is known as BRA-driven development. Another difficulty lies in the extraction of the operating principles that are necessary for reproducing the cognitive–behavioral function of the brain from neuroscience data. Therefore, this study proposes structure-constrained interface decomposition (SCID), which is a hypothesis-building method for creating a hypothetical component diagram that is consistent with neuroscientific findings. The application of this approach has been initiated for constructing various regions of the brain. In the future, we will examine methods for evaluating the biological plausibility of brain-inspired software. This evaluation will also be used to prioritize different computational mechanisms, which should be integrated and associated with the same regions of the brain.}
}
@article{UWIZEYE2016647,
title = {A comprehensive framework to assess the sustainability of nutrient use in global livestock supply chains},
journal = {Journal of Cleaner Production},
volume = {129},
pages = {647-658},
year = {2016},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2016.03.108},
url = {https://www.sciencedirect.com/science/article/pii/S0959652616301792},
author = {Aimable Uwizeye and Pierre J. Gerber and Rogier P.O. Schulte and Imke J.M. {de Boer}},
keywords = {Nitrogen, Phosphorus, Nutrient use efficiency, Life-cycle thinking, Livestock supply chain, Soil nutrient stock change},
abstract = {The assessment of the performance of nutrient use along livestock supply chains can help to identify targeted nutrient management interventions, with a goal to benchmark and to monitor the improvement of production practices. It is necessary, therefore, to develop indicators that are capable to describe all nutrient dynamics and management along the chain. This paper proposed a comprehensive framework, based on life-cycle thinking, to assess the sustainability of nitrogen and phosphorus use. The proposed framework represents nutrient flows in typical livestock supply chain from the “cradle-to-primary-processing-gate”, including crop/pasture production, animal production, and primary processing stage as well as the transportation of feed materials, live-animals or animal products. In addition, three indicators, including the life-cycle nutrient use efficiency (life-cycle-NUE), life-cycle net nutrient balance (life-cycle-NNB) and nutrient hotspot index (NHI) were proposed and tested in a case study of mixed dairy supply chains in Europe. Proposed indicators were found to be suitable to describe different aspects of nitrogen and phosphorus dynamics and, therefore, were all needed. Moreover, the disaggregation of life-cycle-NUE and life-cycle-NNB has been investigated and the uncertainties related to the choice of the method used to estimate changes in nutrient soil stock have been discussed. Given these uncertainties, the choice of method to compute the proposed indicators is determined by data availability and by the goal and scope of the exercise.}
}
@article{LEAHY2019102422,
title = {The digital frontier: Envisioning future technologies impact on the classroom},
journal = {Futures},
volume = {113},
pages = {102422},
year = {2019},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2019.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0016328718304166},
author = {Sean M. Leahy and Charlotte Holland and Francis Ward},
keywords = {Artificial intelligence, Augmented reality, Smart materials, Educational technology, Education futures, Education},
abstract = {Global advances in technology and information are purportedly propelling transformations and disruptions across many sectors, including education. However, historical reviews of technology integration in education mainly reveal weak or ineffectual impacts on learning, and only minor reforms to date within the education system. This study adopted a futures studies methodological approach to explore how K-12 educational spaces and experiences might be shaped by emerging and emergent technologies. In this regard, a series of vignettes are presented which critically examine the potential of augmented reality technologies, artificial intelligence, and smart materials technologies to transform future learning experiences and learning environments across K-12 education contexts, while also challenging assumptions about, and considering influences on, these futures. The focus of the study was not to predict a single or desired future for education, but rather to critically consider a range of possible education futures informed by the articulation of these three vignettes. The paper concludes with discourse on an emergent pedagogic approach that has the potential to prepare teachers and learners to interact and flourish within radically re-configured learning spaces that lean on the aforementioned technologies to support transitions within and beyond the school and its connected communities.}
}
@article{PANG2024121485,
title = {A concept lattice-based expert opinion aggregation method for multi-attribute group decision-making with linguistic information},
journal = {Expert Systems with Applications},
volume = {237},
pages = {121485},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121485},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423019875},
author = {Kuo Pang and Luis Martínez and Nan Li and Jun Liu and Li Zou and Mingyu Lu},
keywords = {Concept lattice, Linguistic truth-valued lattice implication algebra, Linguistic information processing, Multi-attribute group decision-making},
abstract = {During the multi-attribute group decision-making (MAGDM) processing, the individuals often hold different opinions about the alternatives. It is necessary to aggregate the different individual opinions into a unified group opinion. In the real world, experts sometimes use linguistic expressions to evaluate attributes in uncertain environments. To address the problem of reducing the information loss of expert opinion aggregation in MAGDM, this paper proposes a MAGDM approach based on linguistic concept lattices in the context of uncertain linguistic expression. A linguistic concept lattice for multi-expert linguistic formal context is first constructed based on linguistic truth-valued lattice implication algebra, which can express both comparable and incomparable linguistic information in the decision-making process. Different expert opinions are aggregated via the extent of fuzzy linguistic concepts, which can reduce information loss in the aggregation process. Second, meet-irreducible elements in the linguistic concept lattice are introduced to reduce the computational complexity of obtaining all fuzzy linguistic concepts in the decision-making process. the distance between the intents of different fuzzy linguistic concepts is considered to enhance the rationality of linguistic decision results. In addition, the expert’s decision-making process for each alternative is visualized via linguistic concept lattices. Finally, the case study and comparative analysis illustrate the validity and rationality of the proposed approach in MAGDM with linguistic information.}
}
@article{GIOVANNINI20143280,
title = {Approach for the rationalisation of product lines variety},
journal = {IFAC Proceedings Volumes},
volume = {47},
number = {3},
pages = {3280-3291},
year = {2014},
note = {19th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20140824-6-ZA-1003.02226},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016421130},
author = {A. Giovannini and A. Aubry and H. Panetto and H. El Haouzi and L. Pierrel and M. Dassisti},
keywords = {Mass customization, Product variety, Knowledge representation, Knowledge-based system},
abstract = {The product variety management is a key process to deal with the flexibility requested by the mass customisation. In this paper we show that current variety-modelling methods miss a customer representation: without a proper assessment of the customers is not possible to define the product variety that has to be developed to meet the requirements of a customer segment. Here we present an innovative approach to rationalise the product variety, i.e. to link each product variant to the customer profile who needs it. The aim is to optimise the product variety avoiding excesses (variants not related to a customer), lacks (customers not related to a variant) or redundancies (two or more variants proposed to a customer). An overview of customer modelling approaches in the classic product design (non-customisable) is presented. The innovative approach is here developed using system-thinking concepts. A knowledge-based system that uses this approach is designed. Finally the approach is explained using a real industrial case of a quasi-real coil design process.}
}
@article{BLACKSCHAFFER20162374289516665393,
title = {Training Pathology Residents to Practice 21st Century Medicine: A Proposal},
journal = {Academic Pathology},
volume = {3},
pages = {2374289516665393},
year = {2016},
issn = {2374-2895},
doi = {https://doi.org/10.1177/2374289516665393},
url = {https://www.sciencedirect.com/science/article/pii/S2374289521002189},
author = {W. Stephen Black-Schaffer and Jon S. Morrow and Michael B. Prystowsky and Jacob J. Steinberg},
keywords = {competency, progressive responsibility, residency training},
abstract = {Scientific advances, open information access, and evolving health-care economics are disrupting extant models of health-care delivery. Physicians increasingly practice as team members, accountable to payers and patients, with improved efficiency, value, and quality. This change along with a greater focus on population health affects how systems of care are structured and delivered. Pathologists are not immune to these disruptors and, in fact, may be one of the most affected medical specialties. In the coming decades, it is likely that the number of practicing pathologists will decline, requiring each pathologist to serve more and often sicker patients. The demand for increasingly sophisticated yet broader diagnostic skills will continue to grow. This will require pathologists to acquire appropriate professional training and interpersonal skills. Today’s pathology training programs are ill designed to prepare such practitioners. The time to practice for most pathology trainees is typically 5 to 6 years. Yet, trainees often lack sufficient experience to practice independently and effectively. Many studies have recognized these challenges suggesting that more effective training for this new century can be implemented. Building on the strengths of existing programs, we propose a redesign of pathology residency training that will meet (and encourage) a continuing evolution of American Board of Pathology and Accreditation Council for Graduate Medical Education requirements, reduce the time to readiness for practice, and produce more effective, interactive, and adaptable pathologists. The essence of this new model is clear definition and acquisition of core knowledge and practice skills that span the anatomic and clinical pathology continuum during the first 2 years, assessed by competency-based metrics with emphasis on critical thinking and skill acquisition, followed by individualized modular training with intensively progressive responsibility during the final years of training. We anticipate that implementing some or all aspects of this model will enable residents to attain a higher level of competency within the current time-based constraints of residency training.}
}
@article{SMOLENTSEV2020111671,
title = {On the role of integrated computer modelling in fusion technology},
journal = {Fusion Engineering and Design},
volume = {157},
pages = {111671},
year = {2020},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2020.111671},
url = {https://www.sciencedirect.com/science/article/pii/S0920379620302192},
author = {Sergey Smolentsev and Gandolfo Alessandro Spagnuolo and Arkady Serikov and Jens Juul Rasmussen and Anders H. Nielsen and Volker Naulin and Jaime Marian and Matti Coleman and Lorenzo Malerba},
keywords = {Fusion technology, Computer modelling, Neutronics, Materials, Plasma, MHD thermofluids, Model integration},
abstract = {Computer modelling is expected to play an increasingly important role in fusion design and technology, where the complexity of the physical processes involved (plasma, materials, engineering), and the highly interconnected nature of systems and components (“system of systems” design), call for support from sophisticated and integrated computer simulation tools. In this paper, we review the contribution of coupled computer modelling to the design of the reactor, breeding blanket and integrated first wall in terms of neutronics, materials behaviour (including plasma-materials interaction, radiation effects and compatibility with fluids), magnetohydrodynamics thermofluid issues and thermo-hydraulic aspects, as well as simulations of plasma transport out of the confinement region to determine heat and particle loads on plasma facing components. The current capabilities and levels of maturity of existing simulation tools are critically analysed, having in mind the possibility of integrating several tools in a single computational suite in the future and highlighting the perspectives and difficulties of such an endeavour.}
}
@article{DELATORRE2014653,
title = {Monte Carlo advances and concentrated solar applications},
journal = {Solar Energy},
volume = {103},
pages = {653-681},
year = {2014},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2013.02.035},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X13001448},
author = {J. Delatorre and G. Baud and J.J. Bézian and S. Blanco and C. Caliot and J.F. Cornet and C. Coustet and J. Dauchet and M. {El Hafi} and V. Eymet and R. Fournier and J. Gautrais and O. Gourmel and D. Joseph and N. Meilhac and A. Pajot and M. Paulin and P. Perez and B. Piaud and M. Roger and J. Rolland and F. Veynandt and S. Weitz},
keywords = {Monte Carlo algorithm, Concentrated solar energy, Solar energy flux density distribution, Solar concentrators design optimization, Sensitivity computation},
abstract = {The Monte Carlo method is partially reviewed with the objective of illustrating how some of the most recent methodological advances can benefit to concentrated solar research. This review puts forward the practical consequences of writing down and handling the integral formulation associated to each Monte Carlo algorithm. Starting with simple examples and up to the most complex multiple reflection, multiple scattering configurations, we try to argue that these formulations are very much accessible to the non specialist and that they allow a straightforward entry to sensitivity computations (for assistance in design optimization processes) and to convergence enhancement techniques involving subtle concepts such as control variate and zero variance. All illustration examples makePROMES - UPR CNRS 8521 - 7, rue du Four Solaire, 66120 Font Romeu Odeillo, France use of the public domain development environment EDStar (including advanced parallelized computer graphics libraries) and are meant to serve as start basis either for the upgrading of existing Monte Carlo codes, or for fast implementation of ad hoc codes when specific needs cannot be answered with standard concentrated solar codes (in particular as far as the new generation of solar receivers is concerned).}
}
@article{JIN201559,
title = {Significance and Challenges of Big Data Research},
journal = {Big Data Research},
volume = {2},
number = {2},
pages = {59-64},
year = {2015},
note = {Visions on Big Data},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2015.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S2214579615000076},
author = {Xiaolong Jin and Benjamin W. Wah and Xueqi Cheng and Yuanzhuo Wang},
keywords = {Big data, Data complexity, Computational complexity, System complexity},
abstract = {In recent years, the rapid development of Internet, Internet of Things, and Cloud Computing have led to the explosive growth of data in almost every industry and business area. Big data has rapidly developed into a hot topic that attracts extensive attention from academia, industry, and governments around the world. In this position paper, we first briefly introduce the concept of big data, including its definition, features, and value. We then identify from different perspectives the significance and opportunities that big data brings to us. Next, we present representative big data initiatives all over the world. We describe the grand challenges (namely, data complexity, computational complexity, and system complexity), as well as possible solutions to address these challenges. Finally, we conclude the paper by presenting several suggestions on carrying out big data projects.}
}
@article{BARON2022113861,
title = {Might pain be experienced in the brainstem rather than in the cerebral cortex?},
journal = {Behavioural Brain Research},
volume = {427},
pages = {113861},
year = {2022},
issn = {0166-4328},
doi = {https://doi.org/10.1016/j.bbr.2022.113861},
url = {https://www.sciencedirect.com/science/article/pii/S0166432822001292},
author = {Mark Baron and Marshall Devor},
keywords = {Anesthesia, Brain evolution, Consciousness, Coma, Mesopontine tegmentum, MPTA},
abstract = {It is nearly axiomatic that pain, among other examples of conscious experience, is an outcome of still-uncertain forms of neural processing that occur in the cerebral cortex, and specifically within thalamo-cortical networks. This belief rests largely on the dramatic relative expansion of the cortex in the course of primate evolution, in humans in particular, and on the fact that direct activation of sensory representations in the cortex evokes a corresponding conscious percept. Here we assemble evidence, drawn from a number of sources, suggesting that pain experience is unlike the other senses and may not, in fact, be an expression of cortical processing. These include the virtual inability to evoke pain by cortical stimulation, the rarity of painful auras in epileptic patients and outcomes of cortical lesions. And yet, pain perception is clearly a function of a conscious brain. Indeed, it is perhaps the most archetypical example of conscious experience. This draws us to conclude that conscious experience, at least as realized in the pain system, is seated subcortically, perhaps even in the “primitive” brainstem. Our conjecture is that the massive expansion of the cortex over the course of evolution was not driven by the adaptive value of implementing consciousness. Rather, the cortex evolved because of the adaptive value of providing an already existing subcortical generator of consciousness with a feed of critical information that requires the computationally intensive capability of the cerebral cortex.}
}
@article{JARRAHI2018577,
title = {Artificial intelligence and the future of work: Human-AI symbiosis in organizational decision making},
journal = {Business Horizons},
volume = {61},
number = {4},
pages = {577-586},
year = {2018},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2018.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0007681318300387},
author = {Mohammad Hossein Jarrahi},
keywords = {Artificial intelligence, Organizational decision making, Human-machine symbiosis, Human augmentation, Analytical and intuitive decision making},
abstract = {Artificial intelligence (AI) has penetrated many organizational processes, resulting in a growing fear that smart machines will soon replace many humans in decision making. To provide a more proactive and pragmatic perspective, this article highlights the complementarity of humans and AI and examines how each can bring their own strength in organizational decision-making processes typically characterized by uncertainty, complexity, and equivocality. With a greater computational information processing capacity and an analytical approach, AI can extend humans’ cognition when addressing complexity, whereas humans can still offer a more holistic, intuitive approach in dealing with uncertainty and equivocality in organizational decision making. This premise mirrors the idea of intelligence augmentation, which states that AI systems should be designed with the intention of augmenting, not replacing, human contributions.}
}
@article{FARHADINIA2016135,
title = {Multiple criteria decision-making methods with completely unknown weights in hesitant fuzzy linguistic term setting},
journal = {Knowledge-Based Systems},
volume = {93},
pages = {135-144},
year = {2016},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2015.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950705115004359},
author = {B. Farhadinia},
keywords = {Multi-criteria decision making, Hesitant fuzzy linguistic term set, Entropy measure, Similarity measure, Distance measure},
abstract = {As for multi-criteria decision making problems with hesitant fuzzy linguistic information, it is common that the criteria involved in the problems are associated with the predetermined weights, whereas the information about criteria weights is generally incomplete. This is because of the complexity and the inherent subjective nature of human thinking. In this circumstance, the weights of criteria can be derived by means of information entropy from the evaluation values of criteria for alternatives. To the best of our knowledge, up to now, there is no work having introduced the concept of entropy measure for hesitant fuzzy linguistic term sets (HFLTSs). Hence, in this paper, we are going to fill in this gap by developing information about how entropy measures of HFLTSs can be designed.}
}
@article{MALINVERNI2021100305,
title = {Educational Robotics as a boundary object: Towards a research agenda},
journal = {International Journal of Child-Computer Interaction},
volume = {29},
pages = {100305},
year = {2021},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2021.100305},
url = {https://www.sciencedirect.com/science/article/pii/S2212868921000349},
author = {Laura Malinverni and Cristina Valero and Marie Monique Schaper and Isabel Garcia {de la Cruz}},
keywords = {Educational robotics, Children, Robots, Boundary object, Intelligent technologies},
abstract = {Educational robotics has become each time more present in the educational experiences of children and young people. Nonetheless, often, the way in which robotics is introduced in educational settings has been considered as unnecessarily narrow. The paper aims at widening the scope of Educational Robotics and expanding the pedagogical possibilities of this field. To this end, the paper draws on the outcomes of two case studies carried out with primary and secondary school children aimed at investigating their views about robots. These studies allow framing and identifying five themes we believe are particularly relevant to rethink the pedagogy of Educational Robotics. Using these themes as cornerstones for reflection, we delineate a set of dimensions and paths to move Educational Robotics beyond the focus on technical skills but instead explore its potential as a boundary object to involve children in reflective processes around the ethical, social and cultural implications of emerging intelligent technologies.}
}
@article{AARSMAN2024101704,
title = {Enhancing inferences and conclusions in body image focused non-experimental research via a causal modelling approach: A tutorial},
journal = {Body Image},
volume = {49},
pages = {101704},
year = {2024},
issn = {1740-1445},
doi = {https://doi.org/10.1016/j.bodyim.2024.101704},
url = {https://www.sciencedirect.com/science/article/pii/S1740144524000263},
author = {Stephanie R. Aarsman and Christopher J. Greenwood and Jake Linardon and Rachel F. Rodgers and Mariel Messer and Hannah K. Jarman and Matthew Fuller-Tyszkiewicz},
keywords = {Causal inference, Non-experimental data, Target trial, Causal diagram},
abstract = {Causal inference is often the goal of psychological research. However, most researchers refrain from drawing causal conclusions based on non-experimental evidence. Despite the challenges associated with producing causal evidence from non-experimental data, it is crucial to address causal questions directly rather than avoiding them. Here we provide a clear, non-technical overview of the fundamental concepts (including the counterfactual framework and related assumptions) and tools that permit causal inference in non-experimental data, intended as a starting point for readers unfamiliar with the literature. Certain tools, such as the target trial framework and causal diagrams, have been developed to assist with the identification and reduction of potential biases in study design and analysis and the interpretation of findings. We apply these concepts and tools to a motivating example from the body image field. We assert that more precise and detailed elucidation of the barriers to causal inference within one’s study is arguably a key first step in the enhancement of non-experimental research and future intervention development and evaluation.}
}
@article{LIU2020107095,
title = {A comparative study of optimization models for the gas detector placement in process facilities},
journal = {Computers & Chemical Engineering},
volume = {143},
pages = {107095},
year = {2020},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2020.107095},
url = {https://www.sciencedirect.com/science/article/pii/S0098135420306189},
author = {Yue Liu and Bo Zhang and Chao Mu},
keywords = {Optimization, Gas detector, Stochastic programming, Computational fluid dynamics},
abstract = {Gas detector network is an important layer of protection in process facilities for prevention gas leakage accidents. But traditional standards just provide basic principles for the installation of detectors. In this study, three stochastic programming (SP) models are developed and contrasted, namely minimal detection time P-Median model (MDTP), minimal leakage concentration P-Median model (MLCP), and minimal individual risk P-median model (MIRP). Meanwhile all possible leak scenarios are identified based on the combination of wind field set and leakage sources. And clustering analysis is used to filter similar scenarios and select representative leak scenarios. The leak consequences are predicted by computational fluid dynamics (CFD) method and the results are served as the input data of these SP models. A case study is carried out in a diesel hydrogenation unit.}
}
@article{YOUNG1987309,
title = {The metaphor machine: A database method for creativity support},
journal = {Decision Support Systems},
volume = {3},
number = {4},
pages = {309-317},
year = {1987},
issn = {0167-9236},
doi = {https://doi.org/10.1016/0167-9236(87)90102-3},
url = {https://www.sciencedirect.com/science/article/pii/0167923687901023},
author = {Lawrence F Young},
keywords = {Creativity Support Systems, Methaphor Generation by Computer, Database Methods for Metaphor Generation, Idea Processing Support, Computer Support of Metaphorical Thinking, Support of Creative Thinking, Qualitative Support Systems, A Relational Calculus for Metaphor Generation, Relational Database Methods for Metaphor Generation, Interactive Support Systems for Creativity, Right-Brained Support Systems, Relational Algebra for Metaphor Generation, Database Structures for Metaphor Generation, Computer Support of Divergent Thinking},
abstract = {This paper shows how a data base method can be applied to the automatic generation of metaphors. The utility of automatic metaphor generation is based on providing interactive support to creative human thinking processes. Such interactive support systems have been called Idea Processing systems, and are seen as special qualitative types of Decision Support Systems (DSS). They include functions to support metaphorical thinking as well as other modes of creative idea development. The paper presents brief backgrounds references on creativity and the relevance of metaphors, as well as to previous work in Idea Processing. It then presents a relational data base method for automatic metaphor generation. The method is described and illustrated, as well as shown in relational algebra and relational calculus notation. In conclusion, the paper indicates how the relational data base method presented can be operationalized through using existing data base software or by integration with a specialized interface for the particular application of metaphor generation.}
}
@article{ROWBOTTOM2013161,
title = {Kuhn vs. Popper on criticism and dogmatism in science, part II: How to strike the balance},
journal = {Studies in History and Philosophy of Science Part A},
volume = {44},
number = {2},
pages = {161-168},
year = {2013},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2012.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S0039368112001161},
author = {Darrell P. Rowbottom},
abstract = {This paper is a supplement to, and provides a proof of principle of, Kuhn vs. Popper on Criticism and Dogmatism in Science: A Resolution at the Group Level. It illustrates how calculations may be performed in order to determine how the balance between different functions in science—such as imaginative, critical, and dogmatic—should be struck, with respect to confirmation (or corroboration) functions and rules of scientific method.}
}