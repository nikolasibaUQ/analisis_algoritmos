@article{MENG2016114,
title = {Water quality permitting: From end-of-pipe to operational strategies},
journal = {Water Research},
volume = {101},
pages = {114-126},
year = {2016},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2016.05.078},
url = {https://www.sciencedirect.com/science/article/pii/S0043135416304043},
author = {Fanlin Meng and Guangtao Fu and David Butler},
keywords = {Integrated modelling, Multi-objective optimisation, Stakeholder engagement, Urban wastewater system, Water quality permitting},
abstract = {End-of-pipe permitting is a widely practised approach to control effluent discharges from wastewater treatment plants. However, the effectiveness of the traditional regulation paradigm is being challenged by increasingly complex environmental issues, ever growing public expectations on water quality and pressures to reduce operational costs and greenhouse gas emissions. To minimise overall environmental impacts from urban wastewater treatment, an operational strategy-based permitting approach is proposed and a four-step decision framework is established: 1) define performance indicators to represent stakeholders’ interests, 2) optimise operational strategies of urban wastewater systems in accordance to the indicators, 3) screen high performance solutions, and 4) derive permits of operational strategies of the wastewater treatment plant. Results from a case study show that operational cost, variability of wastewater treatment efficiency and environmental risk can be simultaneously reduced by at least 7%, 70% and 78% respectively using an optimal integrated operational strategy compared to the baseline scenario. However, trade-offs exist between the objectives thus highlighting the need of expansion of the prevailing wastewater management paradigm beyond the narrow focus on effluent water quality of wastewater treatment plants. Rather, systems thinking should be embraced by integrated control of all forms of urban wastewater discharges and coordinated regulation of environmental risk and treatment cost effectiveness. It is also demonstrated through the case study that permitting operational strategies could yield more environmentally protective solutions without entailing more cost than the conventional end-of-pipe permitting approach. The proposed four-step permitting framework builds on the latest computational techniques (e.g. integrated modelling, multi-objective optimisation, visual analytics) to efficiently optimise and interactively identify high performance solutions. It could facilitate transparent decision making on water quality management as stakeholders are involved in the entire process and their interests are explicitly evaluated using quantitative metrics and trade-offs considered in the decision making process. We conclude that the operational strategy-based permitting shows promising for regulators and water service providers alike.}
}
@article{SHARMA2024110932,
title = {Demand response assisted energy and reserve procurement in renewable integrated dynamic energy market},
journal = {Electric Power Systems Research},
volume = {236},
pages = {110932},
year = {2024},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2024.110932},
url = {https://www.sciencedirect.com/science/article/pii/S0378779624008186},
author = {Akanksha Sharma and Sumedha Sharma},
keywords = {Ancillary services, Demand response, Dynamic optimal power flow, Market clearing, Spinning reserve},
abstract = {As wind power integration increases, the need for heightened reserve procurement becomes imperative for maintaining system reliability. In the dynamic and deregulated multi-objective environment, this study investigates the impact of varying degrees of uncertainty in wind conditions on the acquisition of energy and spinning reserve ancillary services. The investigation focuses on obtaining reserve capacity through thermal generators and demand response (DR) offers, while addressing wind generation variability. To achieve this, the study employs the multi-objective particle swarm and electric field hybrid optimization algorithm to co-optimize total cost and emissions in the simultaneous market clearing of energy and SRAS procurement. The model’s assessment involves a comprehensive evaluation over a one-day period, encompassing 24 hourly intervals on both IEEE 30-bus and IEEE 118-bus test systems. The performance evaluation of the developed algorithm includes a comparative analysis with other heuristic methods, assessing objectives and statistical performance metrics like convergence and spread, complemented by corresponding box plots. The findings highlight how the presented model successfully tackles the challenges posed by fluctuating wind conditions and dynamic DR offers, providing valuable insights for optimizing energy procurement and reserve capacity in wind-integrated power systems.}
}
@incollection{MOINI2023153,
title = {Chapter 9 - Review of the central nervous system},
editor = {Jahangir Moini and Anthony Logalbo and Jennifer G Schnellmann},
booktitle = {Neuropsychopharmacology},
publisher = {Academic Press},
pages = {153-179},
year = {2023},
isbn = {978-0-323-95974-2},
doi = {https://doi.org/10.1016/B978-0-323-95974-2.00023-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323959742000232},
author = {Jahangir Moini and Anthony Logalbo and Jennifer G Schnellmann},
keywords = {Action potential, Amygdala, Astrocytes, Axolemma, Broca's area, Cingulate gyrus, Corpus callosum, Dynein, Insula, Soma},
abstract = {Absract
The brain and spinal cord make up the central nervous system. The brain controls most bodily functions, including awareness, movements, sensations, speech, thoughts, and memory. The spinal cord is connected to the brainstem and runs through the spinal canal. Some reflex movements can occur via spinal cord pathways without the brain structures being involved. In the brain, there are four types of neuroglia (microglia, astrocytes, oligodendrocytes, and ependymal cells). In the spinal cord, there are two types of neuroglia – the Schwann cells and satellite cells. About 90% of the nervous system consists of neuroglia. The neuron is the basic unit of the nervous system, made up of a cell body, dendrites, and axons. Structural classifications of neurons include multipolar, bipolar, and unipolar. Functional classifications of neurons include sensory neurons, motor neurons, and interneurons. Neurotransmitters are chemical substances that aid in the transfer or impulses, with major examples including acetylcholine, biogenic amines, and amino acids. The brain is divided into the cerebral hemispheres, basal nuclei, diencephalon, brainstem, and cerebellum. Chambers of the brain are known as ventricles. Filtering of information is handled by the thalamus, while the hypothalamus is the primary visceral control center, and regulates homeostasis in the body. The brainstem controls breathing, blood pressure regulation, heart rate, and movement. The cerebellum is also involved in regulating movement, as well as emotions, language, and thinking. The spinal cord provides two directions of impulse conduction to and from the brain, and is made up of 31 separate segments.}
}
@article{BURGESS2011427,
title = {On system rollback and totalized fields: An algebraic approach to system change},
journal = {The Journal of Logic and Algebraic Programming},
volume = {80},
number = {8},
pages = {427-443},
year = {2011},
issn = {1567-8326},
doi = {https://doi.org/10.1016/j.jlap.2011.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1567832611000488},
author = {Mark Burgess and Alva Couch},
abstract = {In system operations the term rollback is often used to imply that arbitrary changes can be reversed i.e. ‘rolled back’ from an erroneous state to a previously known acceptable state. We show that this assumption is flawed and discuss error-correction schemes based on absolute rather than relative change. Insight may be gained by relating change management to the theory of computation. To this end, we reformulate previously-defined ‘convergent change operators’ of Burgess into the language of groups and rings. We show that, in this form, the problem of rollback from a convergent operation becomes equivalent to that of ‘division by zero’ in computation. Hence, we discuss how recent work by Bergstra and Tucker on zero-totalized fields helps to clear up long-standing confusion about the options for ‘rollback’ in change management.}
}
@incollection{LACA2019994,
title = {2.68 - Life Cycle Assessment in Biotechnology☆},
editor = {Murray Moo-Young},
booktitle = {Comprehensive Biotechnology (Third Edition)},
publisher = {Pergamon},
edition = {Third Edition},
address = {Oxford},
pages = {994-1006},
year = {2019},
isbn = {978-0-444-64047-5},
doi = {https://doi.org/10.1016/B978-0-444-64046-8.00109-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780444640468001099},
author = {Adriana Laca and Amanda Laca and Mónica Herrero and Mario Díaz},
keywords = {Biofuels, Biopolymers, Biotechnology, Environmental impacts, LCA, Life cycle assessment, Sustainability},
abstract = {The life cycle assessment (LCA) is a technique for assessing environmental impacts in order to identify the key points of a process/product/service as well as for suggesting alternatives to improve its environmental performance. The LCA methodology allows a "cradle to grave" perspective, considering that all the stages involved in the life cycle of a product or activity have a responsibility on the environmental consequences of it. Related terms as Life Cycle Engineering, Social Life Cycle and Life Cycle Thinking are emerging with a wide perspective. In this article, an overview of the main aspects of the used methodology is provided. Applications of LCA in food biotechnology, pharmaceuticals, biopolymers, biofuels and waste management of the biodegradable fractions are reviewed. Not only the utility of LCA to the biotechnological processes but also its main limitations are presented. Current tendencies in the LCA development highlight the need to update tools applicable to different areas with increasing demand of more accurate environmental information. Main footprints commonly used as LCA indicators have been recently combined with the final goal of obtaining a single indicator useful for decision-making. The interest of LCA for product design and the interactions of LCA with other environmental tools are also commented.}
}
@article{XU2023107615,
title = {Selection of optimal seismic intensity measures using fuzzy-probabilistic seismic demand analysis and fuzzy multi-criteria decision approach},
journal = {Soil Dynamics and Earthquake Engineering},
volume = {164},
pages = {107615},
year = {2023},
issn = {0267-7261},
doi = {https://doi.org/10.1016/j.soildyn.2022.107615},
url = {https://www.sciencedirect.com/science/article/pii/S0267726122004602},
author = {Ming-Yang Xu and Da-Gang Lu and Xiao-Hui Yu and Ming-Ming Jia},
keywords = {Optimal intensity measures, Fuzzy-probabilistic seismic demand analysis (FPSDA), Multi-criteria decision-making (MCDM), Fuzzy analytical hierarchical process (FAHP), Fuzzy technique for order preference by similarity to ideal solution (FTOPSIS)},
abstract = {A fuzzy decision-theoretic computational approach for selecting optimal intensity measures (IMs) under the conditions of randomness, fuzziness and uncertainty is proposed in this paper. The main objectives of this study are twofold: (a) to quantify the contribution of multiple evaluation criteria, and (b) to improve the credibility of decision-making results by selecting the appropriate multi-criteria decision making (MCDM) method. To achieve these, three components of the methodology for this approach are utilized: (1) Fuzzy-probabilistic seismic demand analysis (FPSDA) is employed to consider the aleatory and epistemic uncertainties in the seismic demand analysis. (2) Fuzzy analytical hierarchical process (FAHP) is used to determine the importance weights of multiple evaluation criteria. (3) A combination of FAHP and fuzzy technique for order preference by similarity to ideal solution (FTOPSIS) is utilized to determine the optimal IM alternatives. First, the FPSDA method in which the interval PSDA is applied to evaluate the IM performance is developed. The interval results are transformed into triangular fuzzy numbers to establish a fuzzy decision-making matrix. Subsequently, the FAHP is employed to estimate the importance of each evaluation criterion. Then, FAHP–FTOPSIS is utilized to rank the orders of the IM alternatives and select the optimal IM. An illustrative application of the proposed methodology to a five-story reinforced concrete frame structure is presented. The decision results demonstrate that by considering the maximum inter-story displacement angle (θmax), top displacement (Dt), maximum floor velocity (Vmax) and maximum floor acceleration (Amax) as engineering demand parameters (EDPs), the average spectral displacement (Sdam), Sdam, average spectral velocity (Svam), and Vamvatsikos spectral acceleration (VSa2) are the most optimal IMs, respectively. The decision-making results for multiple EDPs indicate that Svam is the optimal IM. Furthermore, the Cordova spectral acceleration (CSa) considering period prolongation has the highest closeness coefficient among the acceleration-related IMs. To verify the effectiveness of the new framework and methodology, the performance of the proposed approach is compared with that of the traditional MCDM method while ignoring fuzziness. The proposed methodology can be applied to other engineering structures and provide a reference for solving similar problems.}
}
@article{KEESTRA2009531,
title = {Foundationalism and neuroscience; silence and language},
journal = {Language Sciences},
volume = {31},
number = {4},
pages = {531-552},
year = {2009},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2007.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0388000107001040},
author = {Machiel Keestra and Stephen J. Cowley},
keywords = {Distributed cognition, Foundationalism and coherentism, Language and cognition, Mereology and dualism, Neuroscience and philosophy},
abstract = {Neuroscience offers more than new empirical evidence about the details of cognitive functions such as language, perception and action. Since it also shows many functions to be highly distributed, interconnected and dependent on mechanisms at different levels of processing, it challenges concepts that are traditionally used to describe these functions. The question is how to accommodate these concepts to the recent evidence. A recent proposal, made in Philosophical Foundations of Neuroscience (2003) by Bennett and Hacker, is that concepts play a foundational role in neuroscience, that empirical research needs to presuppose them and that changing concepts is a philosophical task. In defending this perspective, PFN shows much neuroscientific writing to be dualistic in nature due to our poor grasp of its foundations. In our review article we take a different approach. Instead of foundationalism we plead for a mild coherentism, which allows for a gradual and continuous alteration of concepts in light of new evidence. Following this approach it is also easier to deal with some neurological conditions (like blindsight, synaesthesia) that pose difficulties for our concepts. Finally, although words and concepts seem to seduce us to thinking that many skills and tasks function separately, it is language skill that – as neuroscientific evidence shows – co-emerges with action/perception cycles and thus seems to require revision of some of our central concepts.}
}
@article{KROGER200886,
title = {Distinct neural substrates for deductive and mathematical processing},
journal = {Brain Research},
volume = {1243},
pages = {86-103},
year = {2008},
issn = {0006-8993},
doi = {https://doi.org/10.1016/j.brainres.2008.07.128},
url = {https://www.sciencedirect.com/science/article/pii/S000689930801929X},
author = {James K. Kroger and Leigh E. Nystrom and Jonathan D. Cohen and Philip N. Johnson-Laird},
keywords = {Logic, Reasoning, Math, Cortex, fMRI, Frontal pole},
abstract = {In an effort to clarify how deductive reasoning is accomplished, an fMRI study was performed to observe the neural substrates of logical reasoning and mathematical calculation. Participants viewed a problem statement and three premises, and then either a conclusion or a mathematical formula. They had to indicate whether the conclusion followed from the premises, or to solve the mathematical formula. Language areas of the brain (Broca's and Wernicke's area) responded as the premises and the conclusion were read, but solution of the problems was then carried out by non-language areas. Regions in right prefrontal cortex and inferior parietal lobe were more active for reasoning than for calculation, whereas regions in left prefrontal cortex and superior parietal lobe were more active for calculation than for reasoning. In reasoning, only those problems calling for a search for counterexamples to conclusions recruited right frontal pole. These results have important implications for understanding how higher cognition, including deduction, is implemented in the brain. Different sorts of thinking recruit separate neural substrates, and logical reasoning goes beyond linguistic regions of the brain.}
}
@article{HIEBERT1992439,
title = {Chapter 3 Reflection and communication: Cognitive considerations in school mathematics reform},
journal = {International Journal of Educational Research},
volume = {17},
number = {5},
pages = {439-456},
year = {1992},
issn = {0883-0355},
doi = {https://doi.org/10.1016/S0883-0355(05)80004-7},
url = {https://www.sciencedirect.com/science/article/pii/S0883035505800047},
author = {James Hiebert},
abstract = {The mathematics education reform efforts in the United States are shaped partially by our understanding of how students learn mathematics. Two traditions in psychology influence our current thinking most forcefully — cognitive psychology with its emphasis on individual mental operations and social cognition with its emphasis on context and group interaction. Reflection and communication, as cognitive processes and as representatives of these respective traditions, are used to establish the cognitive-based rationale for the reform and to analyze the nature of recommended changes. Issues addressed include the interdependence of reflection and communication and the way in which these processes can be used to analyze aspects of the school mathematics program, such as the way textbooks ordinarily treat written symbols. Although the theoretical arguments for reflection and communication are being increasingly well-articulated, the empirical data that address the claims are comparatively sparse. Future research efforts should aim to test theoretical claims for reflection and communication and to increase our understanding of the relationships between these cognitive processes and learning mathematics.}
}
@article{GUO2024102426,
title = {Concept-cognitive learning survey: Mining and fusing knowledge from data},
journal = {Information Fusion},
volume = {109},
pages = {102426},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102426},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524002045},
author = {Doudou Guo and Weihua Xu and Weiping Ding and Yiyu Yao and Xizhao Wang and Witold Pedrycz and Yuhua Qian},
keywords = {Concept-cognitive learning, Data mining, Granular computing, Information fusion, Machine learning},
abstract = {Concept-cognitive learning (CCL), an emerging intelligence learning paradigm, has recently become a popular research subject in artificial intelligence and cognitive computing. A central notion of CCL is cognitive and learning things via concepts. In this process, concepts play a fundamental role when mining and fusing knowledge from data to wisdom. With the in-depth research and expansion of CCL in scopes, goals, and methodologies, some difficulties have gradually emerged, including some vague terminology, ambiguous views, and scattered research. Hence, a systematic and comprehensive review of the development process and advanced research about CCL is particularly necessary at the moment. This paper summarizes the theoretical significance, application value, and future development potential of CCL. More importantly, by synthesizing the reviewed related research, we can acquire some interesting results and answer three essential questions: (1) why examine a cognitive and learning framework based on concept? (2) what is the concept-cognitive learning? (3) how to make concept-cognitive learning? The findings of this work could act as a valuable guide for related studies in quest of a clear understanding of the closely related research issues around concept-cognitive learning.}
}
@article{MOSTOFI20203999,
title = {Spatiotemporal Content of Saccade Transients},
journal = {Current Biology},
volume = {30},
number = {20},
pages = {3999-4008.e2},
year = {2020},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2020.07.085},
url = {https://www.sciencedirect.com/science/article/pii/S0960982220311441},
author = {Naghmeh Mostofi and Zhetuo Zhao and Janis Intoy and Marco Boi and Jonathan D. Victor and Michele Rucci},
keywords = {saccade, microsaccade, eye movements, ocular drift, retina, saccadic suppression, post-saccadic enhancement, efficient encoding, active vision, sensory encoding},
abstract = {Summary
Humans use rapid gaze shifts, known as saccades, to explore visual scenes. These movements yield abrupt luminance changes on the retina, which elicit robust neural discharges at fixation onsets. Yet little is known about the spatial content of saccade transients. Here, we show that saccades redistribute spatial information within the temporal range of retinal sensitivity following two distinct regimes: saccade modulations counterbalance (whiten) the spectral density of natural scenes at low spatial frequencies and follow the external power distribution at higher frequencies. This redistribution is a consequence of saccade dynamics, particularly the speed/amplitude/duration relation known as the main sequence. It resembles the redistribution resulting from inter-saccadic eye drifts, revealing a continuum in the modulations given by different eye movements, with oculomotor transitions primarily acting by regulating the bandwidth of whitening. Our findings suggest important computational roles for saccade transients in the establishment of spatial representations and lead to testable predictions about their consequences for visual functions and encoding mechanisms.
Video Abstract
}
}
@incollection{DANISH20231,
title = {What is learning, for whom, and to what end? An overview},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {1-11},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.14001-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305140011},
author = {Joshua A. Danish and Jasmine Y. Ma},
keywords = {Learning theory, Sociocultural, Cognitive critical, Equity, Diversity, Learning sciences},
abstract = {In this overview chapter, we challenge readers to think critically about the key theories of learning studied in the field. We discuss the importance of recognizing the contexts in which these theories have developed and are used. We encourage readers to ask of each theory “for whom?” and “to what end?” We present overviews of both sociocultural and cognitive theories in an order that runs contrary to the usual overview chapter, and end with discussions of what our theories of learning do not yet address, in the hope of pushing us all to think more deeply about what theories of learning can and should do in the future.}
}
@article{SCHAFER2020100360,
title = {Lenstool-HPC: A High Performance Computing based mass modelling tool for cluster-scale gravitational lenses},
journal = {Astronomy and Computing},
volume = {30},
pages = {100360},
year = {2020},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2019.100360},
url = {https://www.sciencedirect.com/science/article/pii/S2213133719301349},
author = {C. Schäfer and G. Fourestey and J.-P. Kneib},
keywords = {Gravitational lensing software, High performance computing algorithms, Applied computing: astronomy, Galaxies: clusters, Galaxies: halos, Lenstool},
abstract = {With the upcoming generation of telescopes, cluster scale strong gravitational lenses will act as an increasingly relevant probe of cosmology and dark matter. The better resolved data produced by current and future facilities requires faster and more efficient lens modelling software. Consequently, we present Lenstool-HPC, a strong gravitational lens modelling and map generation tool based on High Performance Computing (HPC) techniques and the renowned Lenstool software. We also showcase the HPC concepts needed for astronomers to increase computation speed through massively parallel execution on supercomputers. Lenstool-HPC was developed using lens modelling algorithms with high amounts of parallelism. Each algorithm was implemented as a highly optimised CPU, GPU and Hybrid CPU–GPU version. The software was deployed and tested on the Piz Daint cluster of the Swiss National Supercomputing Centre (CSCS). Lenstool-HPC perfectly parallel lens map generation and derivative computation achieves a factor 30 speed-up using only 1 GPU compared to Lenstool. Lenstool-HPC hybrid Lens-model fit generation tested at Hubble Space Telescope precision is scalable up to 200 CPU–GPU nodes and is faster than Lenstool using only 4 CPU–GPU nodes.}
}
@article{URKEN2012553,
title = {Designing evolvable systems in a framework of robust, resilient and sustainable engineering analysis},
journal = {Advanced Engineering Informatics},
volume = {26},
number = {3},
pages = {553-562},
year = {2012},
note = {Evolvability of Complex Systems},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2012.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S1474034612000535},
author = {Arnold B. Urken and Arthur {“Buck” Nimz} and Tod M. Schuck},
keywords = {Error Resilient Data Fusion (ERDF), Artificial systems, Cognitive radio, Electrical grids, Reflexive behaviors, System dynamics},
abstract = {“Evolvability” is a concept normally associated with biology or ecology, but recent work on control of interdependent critical infrastructures reveals that network informatics systems can be designed to enable artificial, human systems to “evolve”. To explicate this finding, we draw on an analogy between disruptive behavior and stable variation in the history of science and the adaptive patterns of robustness and resilience in engineered systems. We present a definition of an evolvable system in the context of a model of robust, resilient and sustainable systems. Our review of this context and standard definitions indicates that many analysts in engineering (as well as in biology and ecology) do not differentiate Resilience from Robustness. Neither do they differentiate overall dependable system adaptability from a multi-phase process that includes graceful degradation and time-constrained recovery, restabilization, and prevention of catastrophic failure. We analyze how systemic Robustness, Resilience, and Sustainability are related to Evolvability. Our analysis emphasizes the importance of Resilience as an adaptive capability that integrates Sustainability and Robustness to achieve Evolvability. This conceptual framework is used to discuss nine engineering principles that should frame systems thinking about developing evolvable systems. These principles are derived from Kevin Kelly’s book: Out of Control, which describes living and artificial self-sustaining systems. Kelly’s last chapter, “The Nine Laws of God,” distills nine principles that govern all life-like systems. We discuss how these principles could be applied to engineering evolvability in artificial systems. This discussion is motivated by a wide range of practical problems in engineered artificial systems. Our goal is to analyze a few examples of system designs across engineering disciplines to explicate a common framework for designing and testing artificial systems. This framework highlights managing increasing complexity, intentional evolution, and resistance to disruptive events. From this perspective, we envision a more imaginative and time-sensitive appreciation of the evolution and operation of “reliable” artificial systems. We conclude with a short discussion of two hypothetical examples of engineering evolvable systems in network-centric communications using Error Resilient Data Fusion (ERDF) and cognitive radio.}
}
@article{LI2024110836,
title = {Progress of independent component analysis and its recent application in spectroscopy quantitative analysis},
journal = {Microchemical Journal},
volume = {202},
pages = {110836},
year = {2024},
issn = {0026-265X},
doi = {https://doi.org/10.1016/j.microc.2024.110836},
url = {https://www.sciencedirect.com/science/article/pii/S0026265X24009482},
author = {Yankun Li and Mengsha Zhang and Xihui Bian and Lu Tian and Chen Tang},
keywords = {Independent component analysis, Spectroscopy, Quantitative analysis, Chemometrics},
abstract = {Independent component analysis (ICA) is a blind source signal processing technique that separates the source signal from the mixed signal. ICA makes great progress in researches and applications in the field of spectroscopy. This review summed up the principles and theoretical foundations of ICA algorithm in spectral analysis, the commonly used and advanced ICA algorithms, and the strategies for determining the optimal number of independent components. Then the progress of researches and applications of ICA combined with other chemometric method for quantitative resolution in spectroscopy technique recently, including infrared, Raman, fluorescence, UV–visible and nuclear magnetic resonance spectroscopy were reviewed. The possible problems concerning ICA in the analysis of practical complex system were discussed and corresponding solutions were proposed, and the research trends and development futures of ICA in the field of quantitative analysis were also prospected. The review can provide a basis and reference for the selection and use of quantitative analytical strategies in the complex mixtures.}
}
@article{MARKOTA202477,
title = {Clinical heterogeneity and ECT in patients with clozapine resistant schizophrenia},
journal = {Schizophrenia Research},
volume = {272},
pages = {77-78},
year = {2024},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2024.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0920996424003736},
author = {Matej Markota and Paul E. Croarkin and Brandon J. Coombes and Melanie T. Gentry and Jonathan G. Leung}
}
@article{SHWARTZASHER2022100341,
title = {Teaching and assessing active learning in online academic courses},
journal = {Social Sciences & Humanities Open},
volume = {6},
number = {1},
pages = {100341},
year = {2022},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2022.100341},
url = {https://www.sciencedirect.com/science/article/pii/S259029112200095X},
author = {Daphna Shwartz-Asher and Anat Raviv and Ronit Herscu-Kluska},
keywords = {21 century skills, Student perceptions, Active learning, Online course, Student learning experience, Student involvement},
abstract = {The current study examines the teaching of 21st century skills during an online academic course. Drawing on the established community of inquiry framework, we assumed that student learning experiences would be correlated with their level of involvement, their perceptions of the course quality and ethicality, and the resulting skills improvement. One hundred and thirty-four (134) college students who enrolled in an online course answered our questionnaire. This course setting was developed in order to create an active learning atmosphere with diverse online activities that were intended to facilitate the learning of 21st century skills. A partial correlation was found between student involvement and their learning experiences. Significant correlations were found between student perceptions of their 21st century skills improvement, the course's quality, the course's ethicality, and their learning experience. We concluded that the course we created produced a meaningful learning experience which empowered students.}
}
@article{SIMEONOV2017193,
title = {Some resonances between Eastern thought and Integral Biomathics in the framework of the WLIMES formalism for modeling living systems},
journal = {Progress in Biophysics and Molecular Biology},
volume = {131},
pages = {193-212},
year = {2017},
note = {Integral Biomathics 2017: The Necessary Conjunction of Western and Eastern Thought Traditions for Exploring the Nature of Mind and Life},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2017.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S0079610717301141},
author = {Plamen L. Simeonov and Andrée C. Ehresmann},
keywords = {Integral Biomathics, Artificial/synthetic and natural life, Phenomenology, Eastern philosophy, Higher-order logic, Wandering Logic Intelligence, Memory Evolutive Systems},
abstract = {Forty-two years ago, Capra published “The Tao of Physics” (Capra, 1975). In this book (page 17) he writes: “The exploration of the atomic and subatomic world in the twentieth century has …. necessitated a radical revision of many of our basic concepts” and that, unlike ‘classical’ physics, the sub-atomic and quantum “modern physics” shows resonances with Eastern thoughts and “leads us to a view of the world which is very similar to the views held by mystics of all ages and traditions.“ This article stresses an analogous situation in biology with respect to a new theoretical approach for studying living systems, Integral Biomathics (IB), which also exhibits some resonances with Eastern thought. Stepping on earlier research in cybernetics1 and theoretical biology,2 IB has been developed since 2011 by over 100 scientists from a number of disciplines who have been exploring a substantial set of theoretical frameworks. From that effort, the need for a robust core model utilizing advanced mathematics and computation adequate for understanding the behavior of organisms as dynamic wholes was identified. At this end, the authors of this article have proposed WLIMES (Ehresmann and Simeonov, 2012), a formal theory for modeling living systems integrating both the Memory Evolutive Systems (Ehresmann and Vanbremeersch, 2007) and the Wandering Logic Intelligence (Simeonov, 2002b). Its principles will be recalled here with respect to their resonances to Eastern thought.}
}
@article{FARAHI2021100326,
title = {A simulation–optimization approach for measuring emergency department resilience in times of crisis},
journal = {Operations Research for Health Care},
volume = {31},
pages = {100326},
year = {2021},
issn = {2211-6923},
doi = {https://doi.org/10.1016/j.orhc.2021.100326},
url = {https://www.sciencedirect.com/science/article/pii/S2211692321000424},
author = {Sorour Farahi and Khodakaram Salimifard},
keywords = {Crisis, Healthcare responsiveness, Resilience, Simulation–optimization},
abstract = {Crisis occurrence in the healthcare context is, for different reasons, a phenomenon that happens abundantly. The priority of the healthcare system during a crisis is to provide quality care and superior services to the injured people. However, given the usually extreme severity of the crisis that results in a significant number of injured people, proper and timely responsiveness of healthcare systems is a challenging issue This study proposes a novel framework using a hybrid simulation–optimization approach to measure the healthcare responsiveness in crisis to address this real-world problem. This paper closely connects operations research techniques to critical systems thinking notions to evaluate the behavior of a system in the face of crisis. Since all arriving casualties to the hospital are first taken to the emergency department (ED), the ED in a case study is used to illustrate the performance of the presented approach. We designed seven crisis scenarios and one scenario of the ED system in a normal situation and modeled them using discrete-event simulation (DES). Patients’ interarrival times act as the driver of workload experienced in ED during crisis scenarios of varying severity. For crisis simulation scenarios that are unable to cope with the severity of the crisis, we developed an optimization model in an optimization tool to determine the optimal configuration of resources. The optimal configuration can improve healthcare resilience. The results show that an interarrival time of 13.8 min is the maximum threshold, below which feasible solutions could not be found, and the ED system is likely to collapse.}
}
@article{FURIA2007164,
title = {Automated compositional proofs for real-time systems},
journal = {Theoretical Computer Science},
volume = {376},
number = {3},
pages = {164-184},
year = {2007},
note = {Fundamental Aspects of Software Engineering},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2007.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0304397507000643},
author = {Carlo A. Furia and Matteo Rossi and Dino Mandrioli and Angelo Morzenti},
keywords = {Formal verification, Modular systems, Real-time, Compositionality, Rely/guarantee},
abstract = {We present a framework for formally proving that the composition of the behaviors of the different parts of a complex, real-time system ensures a desired global specification of the overall system. The framework is based on a simple compositional rely/guarantee circular inference rule, plus a methodology concerning the integration of the different parts into a whole system. The reference specification language is the TRIO metric linear temporal logic. The novelty of our approach with respect to existing compositional frameworks–most of which do not deal explicitly with real-time requirements–consists mainly in its generality and abstraction from any assumptions about the underlying computational model and from any semantic characterizations of the temporal logic language used in the specification. Moreover, the framework deals equally well with continuous and discrete time. It is supported by a tool, implemented on top of the proof-checker PVS, to perform deduction-based verification through theorem-proving of modular real-time axiom systems. As an example of application, we show the verification of a real-time version of the old-fashioned but still relevant “benchmark” of the dining philosophers problem.}
}
@incollection{CONSONNI2023303,
title = {Chapter 12 - Chemical space and molecular descriptors for QSAR studies},
editor = {Kunal Roy},
booktitle = {Cheminformatics, QSAR and Machine Learning Applications for Novel Drug Development},
publisher = {Academic Press},
pages = {303-327},
year = {2023},
isbn = {978-0-443-18638-7},
doi = {https://doi.org/10.1016/B978-0-443-18638-7.00022-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443186387000220},
author = {Viviana Consonni and Davide Ballabio and Roberto Todeschini},
keywords = {Molecular descriptors, Chemical space, Molecular similarity, Mathematical chemistry, QSAR, In silico modeling, Cheminformatics},
abstract = {With the continuous growth of the real and virtual chemical space, efficient computer-assisted methods are required to discover new substances with desired properties and/or predict properties of interest for untested molecules. These methods rely on the principle that the physicochemical and biological properties of compounds are the effects of their structural characteristics. Therefore, the starting point of any chemo- and bioinformatics application is the conversion of a symbolic representation of the molecular structure into numerical information through the calculation of molecular descriptors. Molecular descriptors encode a wide variety of specific molecular features with a different effect on experimental properties and impact on the perceived chemical similarity between molecules. The choice of molecular descriptors is crucial in determining the chemical space representation and computational modeling outcomes. After introducing the fundamental concepts of molecular descriptors in the current epistemological framework, this chapter reviews some of the well-known classical molecular descriptors and fingerprints.}
}
@article{NERSESSIAN2009178,
title = {Hybrid analogies in conceptual innovation in science},
journal = {Cognitive Systems Research},
volume = {10},
number = {3},
pages = {178-188},
year = {2009},
note = {Special Issue on Analogies - Integrating Cognitive Abilities},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2008.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S1389041709000035},
author = {Nancy J. Nersessian and Sanjay Chandrasekharan},
keywords = {Conceptual innovation, Hybrid analogies, Simulation, Visual reasoning, Engineering sciences},
abstract = {Analogies are ubiquitous in science, both in theory and experiments. Based on an ethnographic study of a research lab in neural engineering, we focus on a case of conceptual innovation where the cross-breeding of two types of analogies led to a breakthrough. In vivo phenomena were recreated in two analogical forms: one, as an in vitro physical model, and the other, as a computational model of the first physical model. The computational model also embodied constraints drawn from the neuroscience and engineering literature. Cross connections and linkages were then made between these two analogical models, over time, to solve problems. We describe how the development of the intermediary, hybrid computational model led to a conceptual innovation, and subsequent engineering innovations. Using this case study, we highlight some of the peculiar features of such hybrid analogies that are now used widely in the sciences and engineering sciences, and the significant questions they raise for current theories of analogy.}
}
@article{RIEGER1977315,
title = {Spontaneous computation in cognitive models},
journal = {Cognitive Science},
volume = {1},
number = {3},
pages = {315-354},
year = {1977},
issn = {0364-0213},
doi = {https://doi.org/10.1016/S0364-0213(77)80022-0},
url = {https://www.sciencedirect.com/science/article/pii/S0364021377800220},
author = {Chuck Rieger},
abstract = {The engineering and theory of a style of computation in which code runs spontaneously (as opposed to on demand) are developed. The notion of a spontaneous computation (SC) is defined, briefly surveyed, and compared to other styles of computation. In the first half of the paper, a LISP-based system which carries out a general theory of SC is described. This includes: complex trigger patterns, organization of SC trigger patterns into associative “trigger trees,” and the structure of an SC itself. Higher level organization and control of SC are then discussed, introducing the notion of a “channel.” In the second half of the paper, some theoretical ideas about how to use SC in cognitive models, particularly those modeling language comprehension and problem solving, are presented and discussed. The discussion includes: SC as a model of nonalgorithmic inference, SCs as “character followers” in a story comprehension system, SCs as subgoal protectors and plan optimizers in a problem solver, and the relationships among SC, context, and frames. In particular, ideas related to partially triggered SCs, and their theoretical applications as context-focusers and motivation-generators are explored. The paper represents one aspect of a larger project called the Commonsense Algorithm Project.}
}
@article{TSAO2024100077,
title = {AI for Technoscientific Discovery: A Human-Inspired Architecture},
journal = {Journal of Creativity},
volume = {34},
number = {2},
pages = {100077},
year = {2024},
issn = {2713-3745},
doi = {https://doi.org/10.1016/j.yjoc.2024.100077},
url = {https://www.sciencedirect.com/science/article/pii/S2713374524000037},
author = {J.Y. Tsao and R.G. Abbott and D.C. Crowder and S. Desai and R.P.M. Dingreville and J.E. Fowler and A. Garland and P.P. Iyer and J. Murdock and S.T. Steinmetz and K.A. Yarritu and C.M. Johnson and D.J. Stracuzzi},
keywords = {Artificial intelligence, creativity, extended neurosymbolic knowledge, technoscientific method},
abstract = {We present a high-level architecture for how artificial intelligences might advance and accumulate scientific and technological knowledge, inspired by emerging perspectives on how human intelligences advance and accumulate such knowledge. Agents advance knowledge by exercising a technoscientific method—an interacting combination of scientific and engineering methods. The technoscientific method maximizes a quantity we call “useful learning” via more-creative implausible utility (including the “aha!” moments of discovery), as well as via less-creative plausible utility. Society accumulates the knowledge advanced by agents so that other agents can incorporate and build on to make further advances. The proposed architecture is challenging but potentially complete: its execution might in principle enable artificial intelligences to advance and accumulate an equivalent of the full range of human scientific and technological knowledge.}
}
@article{SOEMERS2023277,
title = {Extracting tactics learned from self-play in general games},
journal = {Information Sciences},
volume = {624},
pages = {277-298},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.12.080},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522015754},
author = {Dennis J.N.J. Soemers and Spyridon Samothrakis and Éric Piette and Matthew Stephenson},
keywords = {Games, Feature selection, Decision trees, Explainable AI},
abstract = {Local, spatial state-action features can be used to effectively train linear policies from self-play in a wide variety of board games. Such policies can play games directly, or be used to bias tree search agents. However, the resulting feature sets can be large, with a significant amount of overlap and redundancies between features. This is a problem for two reasons. Firstly, large feature sets can be computationally expensive, which reduces the playing strength of agents based on them. Secondly, redundancies and correlations between features impair the ability for humans to analyse, interpret, or understand tactics learned by the policies. We look towards decision trees for their ability to perform feature selection, and serve as interpretable models. Previous work on distilling policies into decision trees uses states as inputs, and distributions over the complete action space as outputs. In contrast, we propose and evaluate a variety of decision tree types, which take state-action pairs as inputs, and provide various different types of outputs on a per-action basis. An empirical evaluation over 43 different board games is presented, and two of those games are used as case studies where we attempt to interpret the discovered features.}
}
@article{KOHANOVA2024103942,
title = {Nursing students' perceptions of unfinished nursing care: A cross-sectional study},
journal = {Nurse Education in Practice},
volume = {76},
pages = {103942},
year = {2024},
issn = {1471-5953},
doi = {https://doi.org/10.1016/j.nepr.2024.103942},
url = {https://www.sciencedirect.com/science/article/pii/S1471595324000714},
author = {Dominika Kohanová and Elena Gurková and Marcia Kirwan and Katarína Žiaková and Radka Kurucová},
keywords = {Clinical placement, Clinical practice, Nurse educators, Nursing, Nursing students, Unfinished nursing care},
abstract = {Aim
To investigate the prevalence, patterns and reasons for unfinished nursing care as perceived by nursing students.
Background
Unfinished nursing care (UNC) is a frequently observed phenomenon in the acute care setting. To date, studies have focused primarily on the perspective of nurses or patients, but another important perspective is that of nursing students who provide nursing care in all healthcare settings.
Design
A descriptive cross-sectional study.
Methods
The study included 738 undergraduate nursing students from nine Slovak universities. Data were collected between September 2022 and February 2023 using the Slovak version of the Unfinished Nursing Care Survey tool (UNCS). Data were analyzed using descriptive and inferential statistics.
Results
The mean composite score of UNCS was 2.48 (SD=0.68). In general, 100% of nursing students reported that nurses missed at least one or more nursing care activities during their last clinical placement. The average number of missed nursing care activities was 11.2 per nurse as perceived by nursing students during their last clinical placement. Nursing students reported that the most frequently omitted nursing care activity was spending time with patients and their caregivers (3.15 ± 1.11; 92.9%). The most frequently reported reason for UNC was an inadequate number of nurses on the ward (4.31 ± 1.01; 98.1%). In the study, reported UNC could be predicted by previous experience in healthcare, previous clinical rotation, number of patients per shift, perceived staff adequacy and outcome expectations (p <0.05).
Conclusions
The findings reveal that UNC is a widespread phenomenon and all nursing students report this phenomenon during their clinical placements. Spending time with patients and their caregivers emerged as the most frequently omitted nursing care activity, highlighting the importance of patient-centered care. The primary reason cited for UNC was an inadequate number of nurses, highlighting staffing issues as a significant contributing factor. These findings emphasize the need for targeted interventions to address staff shortages and improve nursing education to prepare students to address UNC in their future practice.}
}
@article{LI2021107300,
title = {Iteration-based parameter identification and its applications about distributed parameter systems},
journal = {Applied Soft Computing},
volume = {105},
pages = {107300},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107300},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621002234},
author = {Rui-Guo Li and Huai-Ning Wu},
keywords = {Parameter identification, Distributed parameter systems (DPSs), Iterative approach, Quantum-leading-following-based optimization, Mutation strategy, Secure communication},
abstract = {This paper addresses a general problem to parameter identification in distributed parameter systems (DPSs), as well as handles a secure communication issue by the extended identification method. First, an iterative approach is put forward based on the approximator and historical information. Meanwhile, an execution scheme for the iterative approach is provided by an optimization policy. Then, combined with the reference model and measurement data, an extended identification method is applied in secure communication. Subsequently, a quantum-leading-following-based optimization with mutation strategy (MQLFBO) algorithm is proposed as the optimization policy. Next, the global convergence and computational complexity are respectively discussed for MQLFBO algorithm in theory. Finally, simulation experiments are performed on parameter identification for DPSs and its application in secure communication, which verify the fast convergence and high precision of the developed method.}
}
@article{SUZUKI2008511,
title = {Research and development of fusion grid infrastructure based on atomic energy grid infrastructure (AEGIS)},
journal = {Fusion Engineering and Design},
volume = {83},
number = {2},
pages = {511-515},
year = {2008},
note = {Proceedings of the 6th IAEA Technical Meeting on Control, Data Acquisition, and Remote Participation for Fusion Research},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2007.09.017},
url = {https://www.sciencedirect.com/science/article/pii/S092037960700498X},
author = {Y. Suzuki and K. Nakajima and N. Kushida and C. Kino and T. Aoyagi and N. Nakajima and K. Iba and N. Hayashi and T. Ozeki and T. Totsuka and H. Nakanishi and Y. Nagayama},
keywords = {Grid, AEGIS, JT-60, LHD, LABCOM},
abstract = {In collaboration with the Naka Fusion Institute of Japan Atomic Energy Agency (NFI/JAEA) and the National Institute for Fusion Science of National Institute of Natural Science (NIFS/NINS), Center for Computational Science and E-systems of Japan Atomic Energy Agency (CCSE/JAEA) aims at establishing an integrated framework for experiments and analyses in nuclear fusion research based on the atomic energy grid infrastructure (AEGIS). AEGIS has been being developed by CCSE/JAEA aiming at providing the infrastructure that enables atomic energy researchers in remote locations to carry out R&D efficiently and collaboratively through the Internet. Toward establishing the integrated framework, we have been applying AEGIS to pre-existing three systems: experiment system, remote data acquisition system, and integrated analysis system. For the experiment system, the secure remote experiment system with JT-60 has been successfully accomplished. For the remote data acquisition system, it will be possible to equivalently operate experimental data obtained from LHD data acquisition and management system (LABCOM system) and JT-60 Data System. The integrated analysis system has been extended to the system executable in heterogeneous computers among institutes.}
}
@article{ZHAN2023106376,
title = {Dual-objective building retrofit optimization under competing priorities using Artificial Neural Network},
journal = {Journal of Building Engineering},
volume = {70},
pages = {106376},
year = {2023},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2023.106376},
url = {https://www.sciencedirect.com/science/article/pii/S2352710223005557},
author = {Jin Zhan and Wenjing He and Jianxiang Huang},
keywords = {Building retrofit, Building energy, Occupant thermal comfort, Artificial neural network, Hyperparameters},
abstract = {Building retrofit has received renewed interests in recent years, driven by energy-savings and indoor environmental quality goals. Digital technologies such as building performance simulation and optimization algorithms have been used to identify optimal retrofit schemes, yet the existing approaches are limited by the slow running speed of physics-based models and sub-optimal results. This study describes a novel framework, the Building Performance Optimization using Artificial Neural Network (BPO-ANN), which can automatically identify optimal building retrofit schemes. A robust Artificial Neural Network model was developed and validated as a surrogate to rapidly assess building performances, which was then connected to a genetic algorithm in search of Pareto optimal. The impact of key design attributes on building performances have been assessed using sensitivity analysis. The BPO-ANN framework has been tested in a high-performing campus building in Northern China under two competing objectives: building energy demand and occupant thermal comfort. It can automatically identify optimal design schemes, which were expected to achieve an energy-savings of 4% and reduce the annual thermal discomfort percentage by 4%. Sensitivity analysis suggested that window-to-wall ratio and HVAC setpoint have contributed the most to the performances of the campus building, followed by the roof U-value and wall U-value. The study has contributed methodologically to simulation-based optimization method, with novelties in the use of neural network algorithms to accelerate the otherwise time-consuming physics-based simulation models. It has also contributed a robust procedure in the tuning of hyperparameters in neural network models, with marked improvements in model prediction and computational efficiency.}
}
@article{CAMPBELL2022108635,
title = {In search of experimental evidence on Scratch programming and students’ achievements in the first-year college computing class? Consider these datasets},
journal = {Data in Brief},
volume = {45},
pages = {108635},
year = {2022},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2022.108635},
url = {https://www.sciencedirect.com/science/article/pii/S2352340922008411},
author = {Oladele O. Campbell and Harrison I. Atagana},
keywords = {CS1, Novice programming, Constructionism, Block-based programming, Experimental data, Coarsened Exact Matching, Quasi-experiment},
abstract = {This article presents datasets representing the demographics and achievements of computer science students in their first programming courses (CS1). They were collected from a research project comparing the effects of a constructionist Scratch programming and the conventional instructions on the achievements of CS1 students from selected Nigerian public colleges. The project consisted of two consecutive quasi-experiments. In both cases, we adopted a non-equivalent pretest-posttest control group design and multistage sampling. Institutions were selected following purposive sampling, and those selected were randomly assigned to the Scratch programming class (experimental) and the conventional (comparison) class. A questionnaire and pre- and post-introductory programming achievement tests were used to collect data. To strengthen the research design, we used the Coarsened Exact Matching (CEM) algorithm to create matched samples from the unmatched data obtained from both experiments. Future studies can use these data to identify the factors influencing CS1 students' performance, investigate how programming pedagogies or tools affect CS1 students' achievements in higher education, identify important trends using machine learning techniques, and address additional research ideas.}
}
@article{JONES2022100996,
title = {Characterizing whole class discussions about data and statistics with conversation profile analysis},
journal = {The Journal of Mathematical Behavior},
volume = {67},
pages = {100996},
year = {2022},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2022.100996},
url = {https://www.sciencedirect.com/science/article/pii/S0732312322000645},
author = {Ryan Seth Jones and Joshua M. Rosenberg},
keywords = {Latent Class Modeling, Data Modeling, Whole class discussion},
abstract = {Whole class discussions (WCDs) are an important pedagogical tool for mathematics classes but are challenging to characterize across large numbers of observations because of their dynamic and complex nature. In this paper, we report on an exploratory method to characterize WCDs in mathematics classes across large numbers of observations that we refer to as Conversation Profile Analysis (CPA). CPA uses Latent Class Modeling (LCM) with live observation data to generate profiles of WCDs in middle-grade mathematics classes. We report on our exploratory use of CPA to analyze observation data from 259 WCDs about data and statistics in middle school classes making use of an innovative approach to instruction called Data Modeling. We identified 4 profiles of WCDs and found that these profiles varied in likelihood across time and were associated with different ways students talked about key mathematical ideas. We also discuss broader implications of the CPA approach to studying WCDs in math classes.}
}
@incollection{CHRISTAKOS2004661,
title = {The cognitive basis of physical modelling},
editor = {Cass T. Miller and Matthew W. Farthing and William G. Gray and George F. Pinder},
series = {Developments in Water Science},
publisher = {Elsevier},
volume = {55},
pages = {661-669},
year = {2004},
booktitle = {Computational Methods in Water Resources: Volume 1},
issn = {0167-5648},
doi = {https://doi.org/10.1016/S0167-5648(04)80089-3},
url = {https://www.sciencedirect.com/science/article/pii/S0167564804800893},
author = {G. Christakos},
abstract = {We revisit the meaning of the term “solution” with regards to a physical model representing a natural system. We suggest that a (non-conventional) epistemic cognition solution (assuming that the model describes incomplete knowledge about nature, and focusing on conceptual mechanisms of scientific thinking) can lead to more realistic results than a (conventional) ontologic solution (assuming that the model describes nature as is, and focusing on form manipulations).}
}
@article{JI2021365,
title = {Blog text quality assessment using a 3D CNN-based statistical framework},
journal = {Future Generation Computer Systems},
volume = {116},
pages = {365-370},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.10.025},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20330028},
author = {Fang Ji and Heqing Zhang and Zijiang Zhu and Weihuang Dai},
keywords = {Blog data, VQA, 3D CNN, Video-related text, Bi-LSTM, Text quality evaluation},
abstract = {Aiming at the problem that blog texts are the streaming data captured by different acquisition modality, each kind of which has its particular quality evaluation mode, this paper proposes a text quality evaluation (TQA) model based on 3D CNN correlated with blog text data. In order to achieve accurate TQA value, the model adopted a Bi-LSTM-based architecture to process video-related blog text as auxiliary part to provide additional information for our TQA architecture. First, the auxiliary part constructs feature vector for each video-related text by the model originating from Bi-LSTM and Seq2Seq. Then, the feature vector was feed to a well-trained decoder to reconstruct the original input data. Then, the feature vector complied with the blog textual data are inputted into end-to-end TQA modal based on the 3D CNN straightly. Comprehensive experimental results on the blog text/video dataset from the well-known truism website “http://www.mafengwo.cn/” have shown that the proposed model reflects the subjective quality of online texts more accurately, and has better overall blog TQA assessment performance than the other state-of-the-art non-reference methods.}
}
@article{KELLEY2011228,
title = {Theoretical explorations of cognitive robotics using developmental psychology},
journal = {New Ideas in Psychology},
volume = {29},
number = {3},
pages = {228-234},
year = {2011},
note = {Special Issue: Cognitive Robotics and Reevaluation of Piaget Concept of Egocentrism},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2009.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X0900035X},
author = {Troy D. Kelley and Daniel N. Cassenti},
keywords = {Development, Robotics, Cognition, Cognitive modeling},
abstract = {How can cognitive robotics inform developmental psychology researchers and what can developmental psychology tell us about creating robots? More importantly, how can cognitive robotics and developmental psychology nourish each other to become a symbiotic relationship for future research? We address the theoretical underpinnings of developmental change using a cognitive architecture implemented on a robotic system and how our theories of knowledge representation relate to critical periods of infant development. Next, we will show how descriptive theories of cognitive development, specifically Zelazo's Levels of Consciousness (LOC; Zelazo, 2000, Zelazo, 2004, Zelazo and Jacques, 1996), can be mapped onto a computational cognitive architecture (ACT-R; Anderson & Lebiere, 1998). Following our discussion of Zelazo's theory, we will apply the ACT-R architecture specifically to the problem of object permanence. Finally, we will address how cognitive robotics can serve as a computational proving ground of developmental psychology for future research.}
}
@incollection{COHEN2024279,
title = {Chapter 14 - Beyond artificial intelligence ethics: exploring empathetic ethical outcomes for artificial intelligence},
editor = {Santi Caballé and Joan Casas-Roma and Jordi Conesa},
booktitle = {Ethics in Online AI-based Systems},
publisher = {Academic Press},
pages = {279-295},
year = {2024},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-443-18851-0},
doi = {https://doi.org/10.1016/B978-0-443-18851-0.00017-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443188510000172},
author = {Hart Cohen and Linda Aulbach},
keywords = {Artificial intelligence, empathy, emotion, ethics, machine learning},
abstract = {This chapter will focus on the area of human emotion as the field of emotional AI or empathic AI is emerging and creating new possibilities—and ethical issues. Artificial intelligence (AI) has evolved into multiple spheres of everyday life and is quickly proving capable of affecting how we live, work and play in the world—with sometimes severe differences in usage, acceptance, legalities, and ethical practices both between countries and within countries. These discrepancies and ethical debates are only going to become more complex. Intelligence is no longer unique to the human species, therefore challenging the definition of what it means to be human. Our chapter uses three cases studies to illustrate how empathy is germane to intimacy, planetary survival, and recognition. Apart from the need to find the right amount and approach of empathy within AI systems to mitigate detrimental emotional effects on humans, there should also be an approach to bring empathy across all AI technologies, irrespective of having Emotional AI (EAI) as a focal point. In addition, there is reason to believe that adding empathy as an extra layer of ethics into official guidelines is not only necessary but desirable, ensuring that the increase of standardization and generalization in the AI field does not invalidate individuality. A lack of empathy could be considered as unethical in itself, so the ongoing developments in the field of empathy and AI and the link between both is to be welcomed.}
}
@article{OWOYELE2022101236,
title = {Masked-Piper: Masking personal identities in visual recordings while preserving multimodal information},
journal = {SoftwareX},
volume = {20},
pages = {101236},
year = {2022},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2022.101236},
url = {https://www.sciencedirect.com/science/article/pii/S2352711022001546},
author = {Babajide Owoyele and James Trujillo and Gerard {de Melo} and Wim Pouw},
keywords = {Multimodal communication, Kinematic research, Data privacy, Open science, Masking, Research reproducibility},
abstract = {In this increasingly data-rich world, visual recordings of human behavior are often unable to be shared due to concerns about privacy. Consequently, data sharing in fields such as behavioral science, multimodal communication, and human movement research is often limited. In addition, in legal and other non-scientific contexts, privacy-related concerns may preclude the sharing of video recordings and thus remove the rich multimodal context that humans recruit to communicate. Minimizing the risk of identity exposure while preserving critical behavioral information would maximize utility of public resources (e.g., research grants) and time invested in audio–visual​ research. Here we present an open-source computer vision tool that masks the identities of humans while maintaining rich information about communicative body movements. Furthermore, this masking tool can be easily applied to many videos, leveraging computational tools to augment the reproducibility and accessibility of behavioral research. The tool is designed for researchers and practitioners engaged in kinematic and affective research. Application areas include teaching/education, communication and human movement research, CCTV, and legal contexts.}
}
@article{HARRIS2023101038,
title = {Spatial visualization and measurement of area: A case study in spatialized mathematics instruction},
journal = {The Journal of Mathematical Behavior},
volume = {70},
pages = {101038},
year = {2023},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2023.101038},
url = {https://www.sciencedirect.com/science/article/pii/S0732312323000081},
author = {Danielle Harris and Tracy Logan and Tom Lowrie},
keywords = {Spatial visualization, Measurement, Area, Learning pathways},
abstract = {The purpose of this study was to explore the influence of spatial visualization skills when students solve area tasks. Spatial visualization is closely related to mathematics achievement, but little is known about how these skills link to task success. We examined middle school students’ representations and solutions to area problems (both non-metric and metric) through qualitative and quantitative task analysis. Task solutions were analyzed as a function of spatial visualization skills and links were made between student solutions on tasks with different goals (i.e., non-metric and metric). Findings suggest that strong spatial visualizers solved the tasks with relative ease, with evidence for conceptual and procedural understanding. By contrast, Low and Average Spatial students more frequently produced errors due to failure to correctly determine linear measurements or apply appropriate formula, despite adequate procedural knowledge. A novel finding was the facilitating role of spatial skills in the link between metric task representation and success in determining a solution. From a teaching and learning perspective, these results highlight the need to connect emergent spatial skills with mathematical content and support students to develop conceptual understanding in parallel with procedural competence.}
}
@article{HOPPENSTEADT201599,
title = {Spin torque oscillator neuroanalog of von Neumann's microwave computer},
journal = {Biosystems},
volume = {136},
pages = {99-104},
year = {2015},
note = {Selected papers presented at the Eleventh International Workshop on Neural Coding, Versailles, France, 2014},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2015.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0303264715000891},
author = {Frank Hoppensteadt},
keywords = {Neuromorphic computing, Spintronics, Spin torque oscillator, Spin waves, Logic machine, Nanomagnetics, Von Neumann microwave computer},
abstract = {Frequency and phase of neural activity play important roles in the behaving brain. The emerging understanding of these roles has been informed by the design of analog devices that have been important to neuroscience, among them the neuroanalog computer developed by O. Schmitt and A. Hodgkin in the 1930s. Later J. von Neumann, in a search for high performance computing using microwaves, invented a logic machine based on crystal diodes that can perform logic functions including binary arithmetic. Described here is an embodiment of his machine using nano-magnetics. Electrical currents through point contacts on a ferromagnetic thin film can create oscillations in the magnetization of the film. Under natural conditions these properties of a ferromagnetic thin film may be described by a nonlinear Schrödinger equation for the film's magnetization. Radiating solutions of this system are referred to as spin waves, and communication within the film may be by spin waves or by directed graphs of electrical connections. It is shown here how to formulate a STO logic machine, and by computer simulation how this machine can perform several computations simultaneously using multiplexing of inputs, that this system can evaluate iterated logic functions, and that spin waves may communicate frequency, phase and binary information. Neural tissue and the Schmitt-Hodgkin, von Neumann and STO devices share a common bifurcation structure, although these systems operate on vastly different space and time scales; namely, all may exhibit Andronov-Hopf bifurcations. This suggests that neural circuits may be capable of the computational functionality as described by von Neumann.}
}
@article{COHEN2024545,
title = {Digital, Technological and AI Skills for Smart Production Work Environment},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {19},
pages = {545-550},
year = {2024},
note = {18th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.09.269},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324016987},
author = {Yuval Cohen and Hila Chalutz–Ben Gal},
keywords = {Skills, Collaborative Work, Smart Manufacturing, Industry 5.0, Human in the loop, Cobot},
abstract = {This paper analyses the past and anticipated developments in collaborative smart production work environment and points at the required skills to best utilize and flourish in this newly formed work environment. The paper identifies the new work requirements using the job type and its related required technologies and maps the work requirements to the set of skills that may fulfill these requirements. An important notion in this paper is that a shopfloor usually involves several different work environments, each with its unique set of work requirements and associated skills. Thus, tailoring a subset of skills to these set of requirements is the suggested strategy. We use a small example of assembly shopfloor for illustrating the proposed approach. Finally, we propose future research related to this study.}
}
@article{PESTI20132487,
title = {Symposium: Experimental design for poultry production and genomics research1 1Presented as part of the Experimental Design for Poultry Production and Genomics Research Symposium at the Poultry Science Association's annual meeting in Athens, Georgia, July 12, 2012.},
journal = {Poultry Science},
volume = {92},
number = {9},
pages = {2487-2489},
year = {2013},
issn = {0032-5791},
doi = {https://doi.org/10.3382/ps.2012-02733},
url = {https://www.sciencedirect.com/science/article/pii/S0032579119394623},
author = {Gene M. Pesti and Samuel E. Aggrey and Bryan I. Fancher},
keywords = {statistics, biometrics, experimental design, genomics},
abstract = {This symposium dealt with the theoretical and practical aspects of choosing and evaluating experimental designs, and how experimental results may be related to poultry production through modeling. Additionally, recent advances in techniques for generating high-throughput genomic sequencing data, genomic breeding values, genomics selection, and genome-wide association studies have provided unique computational challenges to the poultry industry. Such challenges were presented and discussed.}
}
@article{THOMPSON2021104336,
title = {The impact of learning to code on elementary students’ writing skills},
journal = {Computers & Education},
volume = {175},
pages = {104336},
year = {2021},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2021.104336},
url = {https://www.sciencedirect.com/science/article/pii/S036013152100213X},
author = {Jane Thompson and Gina Childers},
keywords = {Coding, Writing skills, Instructional strategy, Elementary education},
abstract = {The purpose of this research was to investigate how learning to code, using Google's CS First's Storytelling lessons, impacted elementary students' writing skills, writing stamina, and perceptions of writing abilities. In this mixed methods study, 49 fifth grade students participated in Google's CS First's Storytelling lessons focused on developing stories through coding during a school district's summer program. Fifth grade students (elementary level) completed pre- and post-writing assessments to measure 1) overall writing skills, 2) development of ideas, writing organization, grammar and usage, and mechanics, and 3) writing stamina. After completing the lessons and assessments, 10 students participated in a semi-structured interview to document perceptions of their writing abilities and engagement in writing. The results of this study show there was a significant increase in the overall scores on a pre- and post-writing assessment as well as writing stamina, and the interviews revealed that students were motivated in the storytelling process through narrative writing. The findings of this study support the implementation of coding programs as a tool for classroom instruction and the promotion of the development of writing skills for elementary students.}
}
@incollection{GHIORSO2015143,
title = {Chapter 6 - Chemical Thermodynamics and the Study of Magmas},
editor = {Haraldur Sigurdsson},
booktitle = {The Encyclopedia of Volcanoes (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Amsterdam},
pages = {143-161},
year = {2015},
isbn = {978-0-12-385938-9},
doi = {https://doi.org/10.1016/B978-0-12-385938-9.00006-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780123859389000067},
author = {Mark S. Ghiorso and Guilherme A.R. Gualda},
keywords = {Chemical thermodynamics, Crystallization, Geobarometers, Geothermometers, Heat transfer, Magma chambers, Minimization, Open system, Phase equilibria, Thermodynamic potential},
abstract = {This chapter gives a brief overview of the application of chemical thermodynamics to magmatic systems. Topics covered include thermodynamic potentials and their application to various magma chamber evolution scenarios, including open systems and boundary conditions dictated by external controls on heat transfer and chamber volume. In addition, methods of finding the global minima of thermodynamic potentials are discussed along with algorithms for stable phase detection. The chapter ends with a summary of available tools of computational chemical thermodynamics that implement (1) geothermometers, geobarometers, and geohygrometers, (2) mineral and melt thermodynamic properties, (3) solubility calculators for volatiles in magmatic systems, and (4) computation of phase equilibria.}
}
@article{BALDWIN2023100089,
title = {Transformations between rotational and translational invariants formulated in reciprocal spaces},
journal = {Journal of Structural Biology: X},
volume = {7},
pages = {100089},
year = {2023},
issn = {2590-1524},
doi = {https://doi.org/10.1016/j.yjsbx.2023.100089},
url = {https://www.sciencedirect.com/science/article/pii/S2590152423000053},
author = {Philip R. Baldwin},
keywords = {Bispectrum, Wilson statistics, Shape analysis},
abstract = {Correlation functions play an important role in the theoretical underpinnings of many disparate areas of the physical sciences: in particular, scattering theory. More recently, they have become useful in the classification of objects in areas such as computer vision and our area of cryoEM. Our primary classification scheme in the cryoEM image processing system, EMAN2, is now based on third order invariants formulated in Fourier space. This allows a factor of 8 speed up in the two classification procedures inherent in our software pipeline, because it allows for classification without the need for computationally costly alignment procedures. In this work, we address several formal and practical aspects of such multispectral invariants. We show that we can formulate such invariants in the representation in which the original signal is most compact. We explicitly construct transformations between invariants in different orientations for arbitrary order of correlation functions and dimension. We demonstrate that third order invariants distinguish 2D mirrored patterns (unlike the radial power spectrum), which is a fundamental aspects of its classification efficacy. We show the limitations of 3rd order invariants also, by giving an example of a wide family of patterns with identical (vanishing) set of 3rd order invariants. For sufficiently rich patterns, the third order invariants should distinguish typical images, textures and patterns.}
}
@article{LEVINE2024105790,
title = {When rules are over-ruled: Virtual bargaining as a contractualist method of moral judgment},
journal = {Cognition},
volume = {250},
pages = {105790},
year = {2024},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2024.105790},
url = {https://www.sciencedirect.com/science/article/pii/S0010027724000763},
author = {Sydney Levine and Max Kleiman-Weiner and Nick Chater and Fiery Cushman and Joshua B. Tenenbaum},
keywords = {Moral judgment, Contractualism, Virtual bargaining},
abstract = {Rules help guide our behavior—particularly in complex social contexts. But rules sometimes give us the “wrong” answer. How do we know when it is okay to break the rules? In this paper, we argue that we sometimes use contractualist (agreement-based) mechanisms to determine when a rule can be broken. Our model draws on a theory of social interactions – “virtual bargaining” – that assumes that actors engage in a simulated bargaining process when navigating the social world. We present experimental data which suggests that rule-breaking decisions are sometimes driven by virtual bargaining and show that these data cannot be explained by more traditional rule-based or outcome-based approaches.}
}
@article{TAYAN2024100513,
title = {Considerations for adapting higher education technology courses for AI large language models: A critical review of the impact of ChatGPT},
journal = {Machine Learning with Applications},
volume = {15},
pages = {100513},
year = {2024},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2023.100513},
url = {https://www.sciencedirect.com/science/article/pii/S266682702300066X},
author = {Omar Tayan and Ali Hassan and Khaled Khankan and Sanaa Askool},
keywords = {Artificial intelligence, ChatGPT, Higher education, Machine learning},
abstract = {Following the very recent launch of the ChatGPT chatbot, numerous comments and speculations were posted concerning the potential aspects of society that are expected to benefit from this AI revolution. In particular, the education sector is considered as one of the primary domains affected by this application, the impact of which remains yet to be fully understood. Furthermore, many Higher Education institutions are required to get to terms with its impact on teaching and learning, and to clarify their stances on the use of ChatGPT software. This study was developed to investigate some critical case studies considered as relevant to the inevitable re-evaluation of educational aspects needed, ranging from academic missions to student and course learning outcomes and its ethical uses. Following a review of some of the pros and cons of ChatGPT in the higher educational sector, this paper shall demonstrate several case studies of early trials in teaching and learning assessments related to various specializations. Next, the ability of some well-known AI detector software and analyzed in terms of their capacity to successfully detect AI-generated content. Analysis shall be made of the foreseen impact on important aspects including challenges and benefits related to its use in course assessments as well as academic integrity and ethical use. The study concludes with a set of recommendations made from our findings and benchmarks obtained from top universities in order to assist faculty members and decision makers at Higher Education institutions concerning their response strategy and use of ChatGPT.}
}
@incollection{CAPLETTE2017905,
title = {Chapter 36 - The Time Course of Object, Scene, and Face Categorization},
editor = {Henri Cohen and Claire Lefebvre},
booktitle = {Handbook of Categorization in Cognitive Science (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {San Diego},
pages = {905-930},
year = {2017},
isbn = {978-0-08-101107-2},
doi = {https://doi.org/10.1016/B978-0-08-101107-2.00036-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780081011072000361},
author = {Laurent Caplette and Éric McCabe and Caroline Blais and Frédéric Gosselin},
keywords = {Categorization, attention, vision, temporal processing, object recognition, scene recognition, face recognition},
abstract = {We first describe Strategy Length & Internal Practicability (SLIP), a formal model for thinking about categorization, in particular about the time course of categorization. We then discuss an early application of this model to basic-levelness. We then turn to aspects of the time course of categorization that have been neglected in the categorization literature: our limited processing capacities; the necessity of having a flexible categorization apparatus; and the paradox that this inexorably brings about. We propose a twofold resolution of this paradox, attempting, in the process, to bridge work done on categorization in vision, neuropsychology, and physiology.}
}
@article{ALVAREZ2020100814,
title = {The design of tasks that address applications to teaching secondary mathematics for use in undergraduate mathematics courses},
journal = {The Journal of Mathematical Behavior},
volume = {60},
pages = {100814},
year = {2020},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2020.100814},
url = {https://www.sciencedirect.com/science/article/pii/S073231232030078X},
author = {James A.M. Álvarez and Elizabeth G. Arnold and Elizabeth A. Burroughs and Elizabeth W. Fulton and Andrew Kercher},
keywords = {Design principles, Applications to teaching, Secondary mathematics, Mathematical knowledge for teaching},
abstract = {This paper describes theoretical design principles emerging from the development of tasks for standard undergraduate mathematics courses that address applications to teaching secondary mathematics. While researchers recognize that mathematical knowledge for teaching is a form of applied mathematics, applications to teaching remain largely absent from curriculum resources for courses for mathematics majors. We developed various materials that contain applications to teaching that have been integrated into four standard undergraduate mathematics courses. Three primary principles influenced the design of the tasks that prepare future teachers to learn and apply mathematics in a manner central to their future work. Additionally, this paper provides guidance for instructors desiring to develop or implement similar applications. The process of developing these tasks underscores the importance of key features regarding the roles of human beings in the tasks, the intentional focus on advanced content connected to school mathematics, and the integration of active engagement strategies.}
}
@article{XU2024100682,
title = {Intelligent e-learning system in the development of preschool music education based on digital audio technology},
journal = {Entertainment Computing},
volume = {50},
pages = {100682},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100682},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124000508},
author = {Yajun Xu},
keywords = {Digital audio technology, Intelligent voice, Entertainment robots, Preschool education, Music education, Development mode},
abstract = {In the current rapidly developing digital era, intelligent learning has become an important trend in preschool music education, and intelligent voice entertainment robots have become an emerging educational tool. This study aims to provide an intelligent preschool music education model by using digital audio technology to promote children's learning and music perception ability. The study explores the current challenges facing preschool music education, adopts an intelligent preschool music education model based on digital audio technology, utilizes advanced audio processing technology, converts music resources into digital audio formats, and builds an audio library. Through the use of intelligent learning system, according to the individual characteristics and learning needs of children, to provide them with personalized learning materials and learning paths. Finally, based on machine learning algorithms, music education content and interactive functions are integrated into the robot to provide a personalized learning experience. The results show that the study found that intelligent voice entertainment robots can effectively assist students in learning music knowledge and skills, improve learning motivation and interest, and promote interaction with robots.}
}
@article{MCCARTNEY2022108556,
title = {A zero-shot deep metric learning approach to Brain–Computer Interfaces for image retrieval},
journal = {Knowledge-Based Systems},
volume = {246},
pages = {108556},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108556},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122002477},
author = {Ben McCartney and Barry Devereux and Jesus Martinez-del-Rincon},
keywords = {Brain–Computer Interfaces, Metric learning, Computer vision, EEG},
abstract = {In this paper we propose a deep learning based approach for image retrieval using EEG. Our approach makes use of a multi-modal deep neural network based on metric learning, where the EEG signal from a user observing an image is mapped together with visual information extracted from the image. The inspiration behind this work is the vision of a system which allows the user to navigate their image catalogue just by thinking about the image they want to see. Thanks to our metric learning approach, the system is scalable in that it can operate with new images that have never been used in training, resulting in a zero-shot image retrieval system. This framework is tested in two different standard EEG image-viewing datasets, where we demonstrate state-of-the-art results in this complex scenario.}
}
@article{RUNCO2023100063,
title = {AI can only produce artificial creativity},
journal = {Journal of Creativity},
volume = {33},
number = {3},
pages = {100063},
year = {2023},
issn = {2713-3745},
doi = {https://doi.org/10.1016/j.yjoc.2023.100063},
url = {https://www.sciencedirect.com/science/article/pii/S2713374523000225},
author = {Mark A. Runco},
keywords = {Artificial creativity, AI, authenticity, Intentionality, Pseudo-creativity, Standard definition of creativity, Emergence},
abstract = {This article (a) draws from various theories of creativity (e.g., 4P and 6P theories) and (b) uses several concepts from the creativity literature (e.g., self-actualization, emergence) to evaluate the claim that AI can be creative. This approach suggests that, at most, the output of AI represents products which, although lacking, may be attributed with creativity. Such attributions are often mistaken, and, significantly, products say little about the underlying process. Indeed, criticisms previously leveled at the view that the social recognition of products is required of creativity also apply to AI output. Several examples of products and overt actions that have been mistakenly attributed with creativity are discussed. The most telling of these is the ostensible emergence by a machine. The conclusion is that it makes no sense to refer to “creative AI.” One alternative is to extend the concept of “artificial intelligence” to creativity, which gives us “artificial creativity” as the label for what computers can do. Artificial creativity may be original and effective but it lacks several things that characterize human creativity. Thus it may be the most accurate to recognize that the output of AI as a kind of pseudo-creativity.}
}
@article{NIKOLIC2024129757,
title = {Quantum statistical mechanics from a Bohmian perspective},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {641},
pages = {129757},
year = {2024},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2024.129757},
url = {https://www.sciencedirect.com/science/article/pii/S0378437124002668},
author = {Hrvoje Nikolić},
keywords = {Quantum statistical mechanics, Bohmian mechanics, Probability current, Open system},
abstract = {We develop a general formulation of quantum statistical mechanics in terms of probability currents that satisfy continuity equations in the multi-particle position space, for closed and open systems with a fixed number of particles. The continuity equation for any closed or open system suggests a natural Bohmian interpretation in terms of microscopic particle trajectories, that make the same measurable predictions as standard quantum theory. The microscopic trajectories are not directly observable, but provide a general, simple and intuitive microscopic interpretation of macroscopic phenomena in quantum statistical mechanics. In particular, we discuss how various notions of entropy, proper and improper mixtures, and thermodynamics are understood from the Bohmian perspective.}
}
@incollection{VALLERO2014929,
title = {Chapter 32 - Sustainable Approaches},
editor = {Daniel Vallero},
booktitle = {Fundamentals of Air Pollution (Fifth Edition)},
publisher = {Academic Press},
edition = {Fifth Edition},
address = {Boston},
pages = {929-952},
year = {2014},
isbn = {978-0-12-401733-7},
doi = {https://doi.org/10.1016/B978-0-12-401733-7.00032-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780124017337000323},
author = {Daniel Vallero},
keywords = {American society of mechanical engineers (ASME), Aquatic toxicity, Atmospheric oxidation, Bathtub curve, Benchmark, Benefit-cost ratio, Bioconcentration, Decision force field, Dematerialization, Design failure, Design for disassembly (DfD), Design for the environment (DfE), Design for the recyling (DfR), Energy return on the energy investment (EROEI), Ethanol, Exposure, Fuel change, Green algae, Green engineering, Green technology, Half-life, Hazard, Hazardous air pollutant (HAP), Henry's law, Holdup, Industrial ecology, Life cycle analysis (LCA), Life cycle assessment, Material, energy and waste (MEW) flow, Operation and maintenance (O&M), Oxides of nitrogen (NO), Pollution prevention, Pollution prevention act, Process change, Relative risk, Reliability, Remedial action, Resource conservation and recovery act (RCRA), Risk trade-off, Sludge sorption, Solubility, Sustainability, Thermodynamic efficiency, Utility, Vapor pressure, Waste audit, Waste minimization, WWT, WWTP},
abstract = {This chapter considers alternative means of achieving acceptable air quality in addition to or instead of the controls discussed in previous chapters. Air quality is considered from a systems thinking perspective. The concept of engineering reliability is applied to air pollution. The advantages and disadvantages of utility and benefit-cost analyses are discussed, with recommendations of other means of determining environmental value and evaluating risk trade-offs, especially life cycle analysis, green engineering and industrial ecology tools.}
}
@article{KHOSRAVI2024,
title = {AI-assisted Optimal Energy Conversion for Cost-Effective and Sustainable Power Production from Biomass-Fueled SOFC Equipped with Hydrogen Production/Injection},
journal = {Process Safety and Environmental Protection},
year = {2024},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2024.08.045},
url = {https://www.sciencedirect.com/science/article/pii/S0957582024010176},
author = {Mohammadreza Khosravi and Shadi Bashiri Mousavi and Pouria Ahmadi and Amirmohammad Behzadi and Sasan Sadrizadeh},
keywords = {Biomass, Solid oxide fuel cell, vanadium chlorine, super-critical CO cycle, multi-objective optimization},
abstract = {This study introduces a novel energy conversion and management framework to reduce carbon emissions in the energy sector and expedite the global shift towards sustainable practices. The system is driven by biomass-based solid oxide fuel cells for efficient power generation. Central to this approach lies the integration of additional hydrogen injection provided by a thermally-driven vanadium chloride cycle, aiming to enhance the quality of the syngas entering the fuel cells. The system is also combined with a super-critical CO2 cycle that generates power by passively enhancing performance through flue gas condensation. The proposed model's feasibility is evaluated in depth, techno-economically, considering thermodynamics and specific cost theories. As part of artificial intelligence, a neural network model is coupled with the genetic algorithm to determine the best operating status while minimizing computation time. According to the results, the suggested new integration results in higher efficiency and lower cost than a similar system without hydrogen injection. The results further show that the triple-objective optimization achieves output power, second-law efficiency, and overall system cost of 3425kW, 48.5%, and 2.3M$/year, respectively. Eventually, the gasifier is the main contributor to the highest level of exergy destruction, and fuel utilization and current density are the most important parameters in modeling.}
}
@article{SESTER2021301121,
title = {A comparative study of support vector machine and neural networks for file type identification using n-gram analysis},
journal = {Forensic Science International: Digital Investigation},
volume = {36},
pages = {301121},
year = {2021},
note = {DFRWS 2021 EU - Selected Papers and Extended Abstracts of the Eighth Annual DFRWS Europe Conference},
issn = {2666-2817},
doi = {https://doi.org/10.1016/j.fsidi.2021.301121},
url = {https://www.sciencedirect.com/science/article/pii/S2666281721000184},
author = {Joachim Sester and Darren Hayes and Mark Scanlon and Nhien-An Le-Khac},
keywords = {File type identification, n-grams analysis, Forensic analysis, Neural networks, Support vector machine},
abstract = {File type identification (FTI) has become a major discipline for anti-virus developers, firewall designers and for forensic cybercrime investigators. Over the past few years, research has seen the introduction of several classifiers and features. One of these advances is the so-called n-grams analysis, which is an interpretation of statistical counting in classified fragments. Recently, n-grams based approaches were already successfully combined with computational intelligence classifiers. However, the academic body of literature is scant when it comes to a comprehensive explanation of machine learning based approaches such as neural networks (NN) or support vector machines (SVM). For example, how the input parameters, including learning rate, different values of n for n-grams, etc. influence the results. In addition, very few studies have compared the scalability of NN vs. SVM approaches. Therefore, a systematic research in comparing different approaches is needed to address these questions. Hence, this paper investigates this type of comparison, by focusing on the n-gram analysis as a feature for the two different classifiers: SVMs and NNs. This paper details our experiments with two NNs and four SVMs, using linear kernels and RBF kernels on RealDC datasets. In general, we found that SVM-based approaches performed better than the NN, but their scalability is still a challenge.}
}
@article{LIM202256,
title = {XANDAR PHARMACEUTICAL: A model plant for process engineering education},
journal = {Education for Chemical Engineers},
volume = {40},
pages = {56-68},
year = {2022},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2022.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1749772822000173},
author = {Teng Shuen Lim and Zong Lin Chia and Song Yuan Seah and Shin Yee Wong},
keywords = {Pharmaceutical, 3D printed plant model, Industrial relevance, Visualization, Hands-on},
abstract = {This study explores the implementation of a detailed model pharmaceutical production facility in an undergraduate engineering class. Xandar Pharmaceuticals (XP), a fictitious manufacturer, was created and presented to undergraduate engineering students during a current good manufacturing practices (cGMP11current Good Manufacturing Practices.) course in two forms: (1) 3D virtual model and (2) 3D printed model. Data was collected from three separate cohorts over three years with a total of 197 participants. Surveys would gauge student’s sentiments and collect feedback, while quizzes assessed technical understanding. Statistical analysis and effect size calculations would evaluate the differences among the three cohorts. Survey results indicate the 3D printed model has small positive effects on study vs control (groups) regarding understanding of general industry related functions and practices. The 3D printed model also improved students’ interest in critical thinking and investigation. Qualitative feedback and sentiment analysis indicate the model was well received by students and received positive feedback related to visualization, industrial relevance, and student engagement. Use of the 3D printed model (but not the 3D virtual model) has had positive quantitative effects on student quiz scores and feedback. Qualitative improvements to student attitudes and interest are encouraging and suggest further use of 3D printed models in other courses may be beneficial.}
}
@article{SELL2023210,
title = {Societal institutions echo evolved human nature: An analysis of the Western criminal justice system and its relation to anger},
journal = {Evolution and Human Behavior},
volume = {44},
number = {3},
pages = {210-221},
year = {2023},
note = {Special Issue: Evolution, Justice, and the Law},
issn = {1090-5138},
doi = {https://doi.org/10.1016/j.evolhumbehav.2023.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S1090513823000077},
author = {Aaron Sell and Daniel Sznycer},
keywords = {Anger, Hatred, Recalibrational theory, Neutralization theory, Criminal justice, Evolutionary psychology},
abstract = {Social institutions make use of collective power to shape individual behavior. Attempts to modify these institutions to improve the welfare, fairness, or equity of a society will benefit from having an accurate view of human nature so one can design the interventions in ways that actually lead to the desired outcome. Therefore, evolutionary psychology - i.e. the study of human nature - is particularly relevant when creating or reforming institutions, and instrumental in minimizing the frequent unintended consequences that institutional design often brings about. Here, we review the computational structure of human anger, and then analyze the fit between Western criminal justice systems and this universal emotion. This analysis reveals structural correspondences between anger and features of the criminal justice system, although important anomalies are noted. Finally, we offer an evolutionary account of the emotion of hatred and show how hatred can account for some failures in the criminal justice system (e.g., episodes of extra-judicial killing – “lynching”). We suggest that, ultimately, societal institutions will succeed or fail based on how compatible they are with human nature.}
}
@article{ROUGIER2009155,
title = {Implicit and explicit representations},
journal = {Neural Networks},
volume = {22},
number = {2},
pages = {155-160},
year = {2009},
note = {What it Means to Communicate},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2009.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608009000112},
author = {Nicolas P. Rougier},
keywords = {Computational neuroscience, Representation, Symbol, Embodied cognition},
abstract = {During the past decades, the symbol grounding problem, as has been identified by Harnard [Harnard, S. (1990). The symbol grounding problem. Physica D: Nonlinear Phenomena, 42, 335–346], became a prominent problem in the cognitive science society. The idea that a symbol is much more than a mere meaningless token that can be processed through some algorithm, sheds new light on higher brain functions such as language and cognition. We present in this article a computational framework that may help in our understanding of the nature of grounded representations. Two models are briefly introduced that aim at emphasizing the difference we make between implicit and explicit representations.}
}
@article{ZIMMERMANN2023108395,
title = {Experimental and numerical methods to ensure comprehensible and replicable alternating current electrical stimulation experiments},
journal = {Bioelectrochemistry},
volume = {151},
pages = {108395},
year = {2023},
issn = {1567-5394},
doi = {https://doi.org/10.1016/j.bioelechem.2023.108395},
url = {https://www.sciencedirect.com/science/article/pii/S1567539423000324},
author = {Julius Zimmermann and Franziska Sahm and Nils Arbeiter and Henning Bathel and Zezhong Song and Rainer Bader and Anika Jonitz-Heincke and Ursula {van Rienen}},
keywords = {Electrical stimulation, Computational modelling, Computational electromagnetics, Electrochemical impedance spectroscopy, Regenerative medicine, Replicability},
abstract = {Electrical stimulation has received increasing attention for decades for its application in regenerative medicine. Applications range from bone growth stimulation over cartilage regeneration to deep brain stimulation. Despite all research efforts, translation into clinical use has not yet been achieved in all fields. Recent critical assessments have identified limited documentation and monitoring of preclinical in vitro and in vivo experiments as possible reasons hampering clinical translation. In this work, we present experimental and numerical methods to determine the crucial quantities of electrical stimulation such as the electric field or current density. Knowing the stimulation quantities contributes to comprehending the biological response to electrical stimulation and to finally developing a reliable dose–response curve. To demonstrate the methods, we consider a direct contact electrical stimulation experiment that stands representative for a broad class of stimulation experiments. Electrochemical effects are addressed and methods to integrate them into numerical simulations are evaluated. A focus is laid on affordable lab equipment and reproducible open-source software solutions. Finally, clear guidelines to ensure replicability of electrical stimulation experiments are formulated.}
}
@article{ROYCHOWDHURY2004105,
title = {Diagnosis of the diseases––using a GA-fuzzy approach},
journal = {Information Sciences},
volume = {162},
number = {2},
pages = {105-120},
year = {2004},
note = {Medical Expert Systems},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2004.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0020025504000660},
author = {Anish Roychowdhury and Dilip Kumar Pratihar and Nilav Bose and K.P Sankaranarayanan and N Sudhahar},
keywords = {Diagnosis, Jaundice, Pneumonia, GA-fuzzy approach},
abstract = {The objective of our study is to design an expert system by modelling the knowledge and thinking process of a doctor. A fuzzy logic controller (FLC) is used to model the process and a genetic algorithm (GA) helps to select a number of good rules from a manually constructed large rule base of an FLC, based on the opinion of 10 doctors. The GA-based tuning is done off-line. Once the optimized rule base of the FLC is obtained, it can diagnose the disease, on-line. The scope of the present work has been extended to two diseases, namely Pneumonia and Jaundice. The symptoms of each disease are fed as inputs to the FLC and the output, i.e., grade of a disease is determined.}
}
@article{GROSS20173,
title = {Prospects and problems for standardizing model validation in systems biology},
journal = {Progress in Biophysics and Molecular Biology},
volume = {129},
pages = {3-12},
year = {2017},
note = {Validation of Computer Modelling},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2017.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0079610716300177},
author = {Fridolin Gross and Miles MacLeod},
keywords = {Systems biology, Modeling, Standardization, Validation, Model selection},
abstract = {There are currently no widely shared criteria by which to assess the validity of computational models in systems biology. Here we discuss the feasibility and desirability of implementing validation standards for modeling. Having such a standard would facilitate journal review, interdisciplinary collaboration, model exchange, and be especially relevant for applications close to medical practice. However, even though the production of predictively valid models is considered a central goal, in practice modeling in systems biology employs a variety of model structures and model-building practices. These serve a variety of purposes, many of which are heuristic and do not seem to require strict validation criteria and may even be restricted by them. Moreover, given the current situation in systems biology, implementing a validation standard would face serious technical obstacles mostly due to the quality of available empirical data. We advocate a cautious approach to standardization. However even though rigorous standardization seems premature at this point, raising the issue helps us develop better insights into the practices of systems biology and the technical problems modelers face validating models. Further it allows us to identify certain technical validation issues which hold regardless of modeling context and purpose. Informal guidelines could in fact play a role in the field by helping modelers handle these.}
}
@article{DELIMANETO2018225,
title = {A semiotic-inspired machine for personalized multi-criteria intelligent decision support},
journal = {Data & Knowledge Engineering},
volume = {117},
pages = {225-238},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X17300757},
author = {Fernando Buarque {de Lima Neto} and Denis Mayr {Lima Martins} and Gottfried Vossen},
keywords = {Multi-criteria decision support, Computational intelligence, Computational semiotics, Intelligent semiotic machine},
abstract = {The need for appropriate decisions to tackle complex problems increases every day. Selecting destinations for vacation, comparing and optimizing resources to create valuable products, or purchasing a suitable car are just a few examples of puzzling situations in which there is no standard form to find an appropriate solution. Such scenarios become arduous when the number of possibilities, restrictions, and factors affecting the decision rise, thereby turning decision makers into almost mere spectators. In such circumstances, decision support systems (DSS) can play an important role in guiding people and organizations towards more accurate decision making. However, conventional DSS lack the necessary adaptability to account for dynamic changes and are frequently inadequate to tackle the subjectivity inherent in decision-maker's preferences and intention. We argue that these shortcomings can be addressed by a suitable combination of Semiotic Theory and Computational Intelligence algorithms, which together can make up a new generation of DSS. In this article, a formal description of an Intelligent Semiotic Machine is provided and tried out in practical decision contexts. The results obtained show that our approach can provide well-suited decisions based on user preferences, achieving appropriateness while fanning out subjective options without losing decision context, objectivity, or accuracy.}
}
@article{ROBERTSON2008436,
title = {New frontiers in space propulsion sciences},
journal = {Energy Conversion and Management},
volume = {49},
number = {3},
pages = {436-452},
year = {2008},
note = {Space Nuclear Power and Propulsion},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2007.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S019689040700369X},
author = {Glen A. Robertson and P.A. Murad and Eric Davis},
keywords = {Space propulsion, Warp drive, Worm Holes, EM propulsion},
abstract = {Mankind’s destiny points toward a quest for the stars. Realistically, it is difficult to achieve this using current space propulsion science and develop the prerequisite technologies, which for the most part requires the use of massive amounts of propellant to be expelled from the system. Therefore, creative approaches are needed to reduce or eliminate the need for a propellant. Many researchers have identified several unusual approaches that represent immature theories based upon highly advanced concepts. These theories and concepts could lead to creating the enabling technologies and forward thinking necessary to eventually result in developing new directions in space propulsion science. In this paper, some of these theoretical and technological concepts are examined – approaches based upon Einstein’s General Theory of Relativity, spacetime curvature, superconductivity, and newer ideas where questions are raised regarding conservation theorems and if some of the governing laws of physics, as we know them, could be violated or are even valid. These conceptual ideas vary from traversable wormholes, Krasnikov tubes and Alcubierre’s warpdrive to Electromagnetic (EM) field propulsion with possible hybrid systems that incorporate our current limited understanding of zero point fields and quantum mechanics.}
}
@article{LIN2024110730,
title = {MetaIBM: A Python-based library for individual-based modelling of eco-evolutionary dynamics in spatial-explicit metacommunities},
journal = {Ecological Modelling},
volume = {492},
pages = {110730},
year = {2024},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2024.110730},
url = {https://www.sciencedirect.com/science/article/pii/S0304380024001182},
author = {Jian-Hao Lin and Yu-Juan Quan and Bo-Ping Han},
keywords = {Community assembly, Eco-evolutionary processes, Individual-based model, Landscape network, Metacommunity, Python library},
abstract = {Individual-based modelling (IBM) is a powerful tool for simulating complex biological communities. By defining a population as comprising individuals that differ from one another, IBM can simulate the assembly and organisation of complex communities under various eco-evolutionary processes in a large spatial scale, with tremendous variables or parameters considered simultaneously. IBM disentangles a complex system into various sub-systems interacting with each other, allowing us to develop a unified library with a modular design for a wide range of complex scenarios in community assembly. In such a library, a number of parameters-controlled processes can be primitively coded as the sub-systems (or sub-models). Here, we released a Python-coded library as a framework for Metacommunity Individual-based Modelling (MetaIBM). As an open-source library, the MetaIBM has several merits, including: (a) it can be used to simulate a wide range of ecological problems of metacommunities. The metacommunity landscape and its environment gradients can be designed flexibly by users. Users can selectively turn off or on and set up parameters-controlled ecological processes according to their needs. (b) It adopts optimised algorithms and adapts to the high-performance computing devices, so that the users can explore a wide range of parameters space synchronously within a reasonable time. (c) It can be used to simulate a group of communities with up to millions of unique individuals, which is an originally plain portrayal of natural communities. To guide potential users, we provided the source codes of the library and a user manual. In the present article, we gave four examples to demonstrate how to design and model a metacommunity using the MetaIBM, simulating the community assembly in an islands-mainland model under the metacommunity framework with (a) neutral assumptions, (b) niche assumptions, (c) slow evolution scenarios, (d) rapid evolution scenarios. The examples showed that the MetaIBM can efficiently fit the community assembly, and reveal several intrigued species diversity patterns under the interaction of evolutionary processes and dispersal processes at multiple scales. The MetaIBM will be continuously maintained and updated to provide more functions in the future.}
}
@incollection{CLEEREMANS20012584,
title = {Conscious and Unconscious Processes in Cognition},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {2584-2589},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/03560-9},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767035609},
author = {A. Cleeremans},
abstract = {Characterizing the relationships between conscious and unconscious processes is one of the most important and long-standing goals of cognitive psychology. Renewed interest in the nature of consciousness—long considered not to be scientifically explorable—as well as the increasingly widespread availability of functional brain-imaging techniques, now offer the possibility of detailed exploration of the neural, behavioral, and computational correlates of conscious and unconscious cognition. This article reviews some of the relevant experimental work, highlights the methodological challenges involved in establishing the extent to which cognition can occur unconsciously, and situates ongoing debates in the theoretical context provided by current thinking about consciousness.}
}
@article{BOUTTIER202259,
title = {Circular inference predicts nonuniform overactivation and dysconnectivity in brain-wide connectomes},
journal = {Schizophrenia Research},
volume = {245},
pages = {59-67},
year = {2022},
note = {Computational Approaches to Understanding Psychosis},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2020.12.045},
url = {https://www.sciencedirect.com/science/article/pii/S0920996421000311},
author = {Vincent Bouttier and Suhrit Duttagupta and Sophie Denève and Renaud Jardri},
keywords = {Inference, Connectome, Belief, Schizophrenia, Graph, Circular},
abstract = {Schizophrenia is a severe mental disorder whose neural basis remains difficult to ascertain. Among the available pathophysiological theories, recent work has pointed towards subtle perturbations in the excitation-inhibition (E/I) balance within different neural circuits. Computational approaches have suggested interesting mechanisms that can account for both E/I imbalances and psychotic symptoms. Based on hierarchical neural networks propagating information through a message-passing algorithm, it was hypothesized that changes in the E/I ratio could cause a “circular belief propagation” in which bottom-up and top-down information reverberate. This circular inference (CI) was proposed to account for the clinical features of schizophrenia. Under this assumption, this paper examined the impact of CI on network dynamics in light of brain imaging findings related to psychosis. Using brain-inspired graphical models, we show that CI causes overconfidence and overactivation most specifically at the level of connector hubs (e.g., nodes with many connections allowing integration across networks). By also measuring functional connectivity in these graphs, we provide evidence that CI is able to predict specific changes in modularity known to be associated with schizophrenia. Altogether, these findings suggest that the CI framework may facilitate behavioral and neural research on the multifaceted nature of psychosis.}
}
@article{RUPNOW2022100998,
title = {Children’s construction of a volume calculation algorithm for a rectangular prism with a dynamic virtual manipulative},
journal = {The Journal of Mathematical Behavior},
volume = {67},
pages = {100998},
year = {2022},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2022.100998},
url = {https://www.sciencedirect.com/science/article/pii/S0732312322000669},
author = {Theodore J. Rupnow and Jenna R. O’Dell and Jeffrey E. Barrett and Craig J. Cullen and Douglas H. Clements and Julie Sarama and George Rutherford},
keywords = {Volume measurement, Virtual manipulatives, Constructed algorithms, Children’s reasoning about volume},
abstract = {We report on the process of children’s construction of a volume calculation algorithm for rectangular prisms. We provided third- and fourth-grade students with traditional volume tasks and a dynamic virtual manipulative to support their three-dimensional reasoning and use of unit cubes to structure space. We investigated the challenges faced, evolving understandings, and supports for growth. We found four threads of understanding we called interpretation, structure, representation, and numeration, that interacted in complex ways as students constructed volume calculation algorithms. Across six patterns of growth, we found that these threads could have both positive and negative influences on one another. The representation thread tended to have the strongest dragging influence on coordinated understanding, and the structuring thread tended to produce the most enduring conceptualizations. We also found that feedback from the dynamic virtual manipulative and the interviewer played a critical role in overcoming challenges and reaching new understandings about volume.}
}
@article{YAGER20241933,
title = {Towards a science exocortex},
journal = {Digital Discovery},
volume = {3},
number = {10},
pages = {1933-1957},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d4dd00178h},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X2400158X},
author = {Kevin G. Yager},
abstract = {Artificial intelligence (AI) methods are poised to revolutionize intellectual work, with generative AI enabling automation of text analysis, text generation, and simple decision making or reasoning. The impact to science is only just beginning, but the opportunity is significant since scientific research relies fundamentally on extended chains of cognitive work. Here, we review the state of the art in agentic AI systems, and discuss how these methods could be extended to have even greater impact on science. We propose the development of an exocortex, a synthetic extension of a person's cognition. A science exocortex could be designed as a swarm of AI agents, with each agent individually streamlining specific researcher tasks, and whose inter-communication leads to emergent behavior that greatly extend the researcher's cognition and volition.}
}
@article{LI2024e37978,
title = {Innovative integration of sustainable technologies in educational programs: Fostering freshwater production and environmental preservation awareness},
journal = {Heliyon},
volume = {10},
number = {19},
pages = {e37978},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e37978},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024140091},
author = {XiaoKe Li},
keywords = {Sustainable desalination technologies, Educational programs, Freshwater production, Environmental preservation, Renewable energy sources},
abstract = {This study highlights the integration of sustainable desalination technologies into educational programs to raise awareness of freshwater production and environmental preservation. Through a comprehensive curriculum, students explore innovative methods such as pressure-retarded osmosis, multi-effect desalination, and seawater source heat pumps, all powered by renewable seawater thermal energy. The curriculum emphasizes the importance of reducing greenhouse gas emissions, lowering reliance on fossil fuels, and protecting aquatic ecosystems. Students engage in practical evaluations of energy efficiency, economic viability, and environmental impact. Sensitivity analyses are incorporated to help students identify critical factors affecting system performance and optimize operational conditions for various modes. Through hands-on experiments, students learn that components like the heat pump condenser contribute the most to energy loss (29 %), followed by the expansion valve (12 %), compressor (11 %), and seawater heat exchanger (8 %). Economic analyses reveal that while the heat pump condenser and seawater heat exchanger have the lowest financial impact (0.33 % and 2.48 %, respectively), the pressure-retarded osmosis and compressor units have the highest (100 % and 60.3 %, respectively). The findings demonstrate that an optimally designed desalination system can produce freshwater at 80 % lower costs compared to traditional plants, while also reducing carbon emissions by 15 %. Educational experiments also show that integrating pressure-retarded osmosis downstream of multi-effect desalination significantly reduces brine salinity and temperature, highlighting the system's potential for environmental sustainability. This approach not only fosters a deeper understanding of desalination technologies but also equips students with the tools to address future global water challenges.}
}
@article{LUND2012192,
title = {The economic crisis and sustainable development: The design of job creation strategies by use of concrete institutional economics},
journal = {Energy},
volume = {43},
number = {1},
pages = {192-200},
year = {2012},
note = {2nd International Meeting on Cleaner Combustion (CM0901-Detailed Chemical Models for Cleaner Combustion)},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2012.02.075},
url = {https://www.sciencedirect.com/science/article/pii/S0360544212001892},
author = {Henrik Lund and Frede Hvelplund},
keywords = {Sustainable energy planning, Energy and Job creation, Renewable energy and economic growth},
abstract = {This paper presents Concrete Institutional Economics as an economic paradigm to understand how the wish for sustainable energy in times of economic crisis can be used to generate jobs as well as economic growth. In most countries, including European countries, the USA and China, the implementation of sustainable energy solutions involves the replacement of imported fossil fuels by substantial investments in energy conservation and renewable energy (RE). In such situation, it becomes increasingly essential to develop economic thinking and economic models that can analyse the concrete institutions in which the market is embedded. This paper presents such tools and methodologies and applies them to the case of the Danish heating sector. The case shows how investments in decreasing fossil fuels and CO2 emissions can be made in a way in which they have a positive influence on job creation and economic development as well as public expenditures.}
}
@article{UMNEY2018201,
title = {Designing frames: The use of precedents in parliamentary debate},
journal = {Design Studies},
volume = {54},
pages = {201-218},
year = {2018},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2017.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X17300789},
author = {Darren Umney and Peter Lloyd},
keywords = {design process, design precedents, framing, discourse analysis, legislation},
abstract = {Using the naturally-occurring data of official UK Parliamentary transcripts for the development of a new high speed rail project, this paper takes one characteristic of the design process, the use of precedent, to explore how problems and solutions are framed during discussion. In contrast to accounts of reframing that describe one big insight changing the design process we show how one particular precedent allows a series of attempts at reframing to take place in discussion. We conclude by arguing that precedents enable a diffusion of semi-objective meaning in discussion, similar to a prototype in a more conventional design process. This contrasts with other types of discourse elements, such as storytelling, that function through the subjective accumulation of meaning.}
}
@article{LUZZATI2022107593,
title = {Information overload and environmental degradation: Learning from H.A. Simon and W. Wenders},
journal = {Ecological Economics},
volume = {202},
pages = {107593},
year = {2022},
issn = {0921-8009},
doi = {https://doi.org/10.1016/j.ecolecon.2022.107593},
url = {https://www.sciencedirect.com/science/article/pii/S0921800922002555},
author = {Tommaso Luzzati and Ilaria Tucci and Pietro Guarnieri},
keywords = {Information overload, Knowledge, Awareness, Individual decision-making, Environmental concern, H.A. Simon, W. Wenders, Film},
abstract = {This paper discusses the relevance of information overload for explaining environmental degradation, insofar it can reduce individuals' awareness of the unsustainable side-effects of their choices. This “myopia” is reinforced by the increased distance from nature in everyday life brought about by the abundance of exosomatic energy. The departure point of the paper is to show that two outstanding intellectuals, engaged in very different fields, have set forth very similar reflections on the effects of information overload, namely the film director Wim Wenders and the social scientist, really a polymath, Herbert Simon, whose relevance to ecological economics has been recognised. The presentation of their ideas is then complemented by a presentation of the state of the art on information overload, which allows moving to our core argument about environmental degradation.}
}
@incollection{MOOSAVI2024193,
title = {Chapter 15 - Constraint optimization: solving engineering design problems using Whale Optimization Algorithm (WOA)},
editor = {Seyedali Mirjalili},
booktitle = {Handbook of Whale Optimization Algorithm},
publisher = {Academic Press},
pages = {193-216},
year = {2024},
isbn = {978-0-323-95365-8},
doi = {https://doi.org/10.1016/B978-0-32-395365-8.00021-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032395365800021X},
author = {Syed Kumayl Raza Moosavi and Malik Naveed Akhter and Muhammad Hamza Zafar and Majad Mansoor},
keywords = {Constraint optimization, Meta-heuristic algorithm, Machine learning, Whale optimization algorithm},
abstract = {The presence of constraints in an engineering design problem complicates the search space solution and reduces the feasible region finding capabilities. Any particular constrained design problem is subject to numerous iterations of trial-and-error to find an optimal constraint handling methodology and fine tuning its requisite parameters. A drawback from such an approach is that it requires intensive computational load specially if the cost function is resourcefully expensive to locate. The work presented in this work suggests the use of a meta-heuristic algorithm namely; Whale Optimization Algorithm (WOA), to solve the constraint optimization problems. The nature inspired algorithm follows a spiral bubble-net hunting strategy, thereby it does not get stuck on a local minima solution even if the search space is discontinuous. For the validation and verification of the algorithm, WOA is applied against 12 structural engineering optimization problems reported in research literature. Performance of the algorithm is further gauged by drawing a comparison with other state-of-the-art meta-heuristic algorithms. Results indicate that the WOA algorithm by far provides the better optimal solutions than the existing methods. Finally, the salient features and future implications are discussed in detail.}
}
@article{DAMBROT2020110,
title = {Theoretical and hypothetical pathways to real-time neuromorphic AGI/post-AGI ecosystems},
journal = {Procedia Computer Science},
volume = {169},
pages = {110-122},
year = {2020},
note = {Postproceedings of the 10th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2019 (Tenth Annual Meeting of the BICA Society), held August 15-19, 2019 in Seattle, Washington, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.122},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920302453},
author = {S. Mason Dambrot},
keywords = {artificial general intelligence, counterfactual quantum entanglement, enplants, graphene, mediated artificial superintelligence, neural prosthetics, photonics, recurrent neural networks, spintronics, synthetic genomics, transdisciplinarity, transentity universal intelligence},
abstract = {While Homo sapiens is without doubt our planet’s most advanced species capable of imagining, creating and implementing tools, one of the many observable trends in evolution is the accelerating merger of biology and technology at increasing levels of scale. This is not surprising, given that our technology can be seen from a perspective in which the sensorimotor and, subsequently, prefrontal areas of our brain increasingly extending its motor (as did our evolutionary predecessors), perceptual, and—with computational advances, cognitive and memory capacities—into the exogenous environment. As such, this trajectory has taken us to a point in the above-mentioned merger at which the brain itself is beginning to meld with its physically expressed hardware and software counterparts—functionally at first, but increasingly structurally as well, initially by way of neural prostheses and brain-machine interfaces. Envisioning the extension of this trend, I propose theoretical technological pathways to a point at which humans and non-biological human counterparts may have the option to have identical neural substrates that—when integrated with Artificial General Intelligence (AGI), counterfactual quantum communications and computation, and AGI ecosystems—provide a global advance in shared knowledge and cognitive function while ameliorating current concerns associated with advanced AGI, as well as suggesting (and, if realized, accelerating) the far-future emergence of Transentity Universal Intelligence (TUI).}
}
@article{JU2024123327,
title = {Locating influence sources in social network by senders and receivers spaces mapping},
journal = {Expert Systems with Applications},
volume = {248},
pages = {123327},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.123327},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424001921},
author = {Weijia Ju and Yixin Chen and Ling Chen and Bin Li},
keywords = {Online social network, Influence source locating, Latent space, Representation learning},
abstract = {Influence source locating is important for misinformation detecting and blocking. However, most of existing multiple sources locating methods use only the local structure of the nodes or the shortest path between them. In addition, some methods do not consider the influencing times of the observed nodes and the mutual effect between the influence cascades originated from various sources. These factors hinder these methods from obtaining high-quality multi-source detection results. To overcome such shortcomings, it is necessary to analyze the nodes’ latent structure characteristics in spreading and receiving the influences. This paper presents a representation learning-based approach to detect the influence sources. The algorithm detects the sources using the topological features of the influenced observed nodes. Firstly, a set of candidate sources is constructed by eliminating some nodes which obviously cannot influence the observed ones. The latent spaces of influence senders and receivers are defined to reveal the nodes' features in influence propagation. The nodes are mapped into the mentioned two latent spaces according to their influencing probabilities and influenced times. The latent spaces establish an influence propagation model, where each node’s representations can be used to obtain the probability that it becomes a source. To optimize the propagation model, negative sampling method is used to reduce the computation time. Our experimental results on data sets of 5 real networks and 3 synthetic networks demonstrate that precision of the result by our algorithm is on average 10 % higher than those of the other similar algorithms.}
}
@article{WU2021435,
title = {A lightweight SM2-based security authentication scheme for smart grids},
journal = {Alexandria Engineering Journal},
volume = {60},
number = {1},
pages = {435-446},
year = {2021},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2020.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S1110016820304488},
author = {Kehe Wu and Rui Cheng and Wenchao Cui and Wei Li},
keywords = {Smart grids, SM2, Mutual identity authentication, Key agreement},
abstract = {With the increasing openness of smart grids, a large quantity of power terminals will be widely applied in systems of smart grids by various access modes (wired, wireless, satellite). In order to ensure the integrity and confidentiality of the data and the security of the communications between power terminals and the smart grid intranet, a lightweight security authentication and key agreement scheme is proposed in this article. The scheme provides mutual authentication based on the SM2 algorithm and key agreement which can prevent various attacks. Furthermore experimental evaluation and comparative analysis is conducted to indicate that the proposed scheme can resist many types of attacks with lower amounts of total computational resources and lower communications bandwidth. The analysis shows that the proposed scheme is suitable for smart grid.}
}
@article{VANESSEN2021109218,
title = {Screening wave conditions for the occurrence of green water events on sailing ships},
journal = {Ocean Engineering},
volume = {234},
pages = {109218},
year = {2021},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.109218},
url = {https://www.sciencedirect.com/science/article/pii/S0029801821006478},
author = {Sanne M. {van Essen} and Charles Monroy and Zhirong Shen and Joop Helder and Dae-Hyun Kim and Sopheak Seng and Zhongfu Ge},
keywords = {Screening, Critical events, Extreme events, Waves, Green water, Design loads, Multi-fidelity approach, Potential flow, Coarse mesh CFD},
abstract = {Design loads for extreme wave events on ships, such as slamming and green water, are hard to define. These events depend on details in the incoming waves, ship motions and structure layout, which requires high-fidelity tools such as CFD or experiments to obtain the correct loads. These tools (presently) do not have the capability to fully resolve the long-term statistics of rare events in all metocean conditions over the ship’s lifetime. The idea of ‘screening’ is to use lower-fidelity numerical methods to identify the occurrence of extreme load events based on an indicator. A good indicator has a strong correlation to the design load, but is easier to calculate. A high-fidelity tool can then be used to find the loads in these events. The low-fidelity statistics and the high-fidelity loads can be combined to define a design load and its probability. The present study compares different numerical screening indicators for green water loads on a containership against experiments. The quality of the identification of the critical events and the required computational time served as comparison metrics. This showed that screening both with potential flow tools and with coarse mesh CFD tools is feasible, provided the indicator, grid, time step and wave input settings are well chosen. The results from coarse mesh CFD are slightly better than from potential flow, but the computational costs are much higher. The results also show that the peaks and steepness of the relative wave elevation around the bow are suitable green water load indicators, as well as the undisturbed wave crests at the bow. Fine mesh CFD calculations were done for the identified events based on an example indicator, which resulted in a green water load distribution very close to that of the experiments. This study shows that screening could potentially reduce the required high-fidelity modelling time with up to ∼90% compared to common practice.}
}
@incollection{ROSCHELLE20071,
title = {Designing Networked Handheld Devices to Enhance School Learning},
editor = {Marvin V. Zelkowitz},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {70},
pages = {1-60},
year = {2007},
issn = {0065-2458},
doi = {https://doi.org/10.1016/S0065-2458(06)70001-8},
url = {https://www.sciencedirect.com/science/article/pii/S0065245806700018},
author = {Jeremy Roschelle and Charles Patton and Deborah Tatar},
abstract = {Handheld devices, especially networked handheld devices, are growing in importance in education, largely because their affordability and accessibility create an opportunity for educators to transition from occasional, supplemental use of computers, to frequent and integral use of portable computational technology. Why and how might these new devices enhance school learning? We begin by discussing a simple but important factor: networked handhelds can allow a 1:1 student:device ratio for the first time, enabling ready-at-hand access to technology throughout the school day and throughout the learner's personal life. We argue that designers need to understand the capabilities of the new generation of handheld computers and wireless networks that are most relevant for learning. We follow this with a discussion of Learning Science theories that connect those capabilities to enhanced learning. The capabilities and features feed into design practices. We describe a set of example applications that are arising from the capabilities, theories and design practices previously described. Finally, we close with a discussion of the challenge of scale.}
}
@article{CAHILL20172131,
title = {Building a Community of Practice to Prepare the HPC Workforce},
journal = {Procedia Computer Science},
volume = {108},
pages = {2131-2140},
year = {2017},
note = {International Conference on Computational Science, ICCS 2017, 12-14 June 2017, Zurich, Switzerland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.05.059},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917305902},
author = {Katharine J. Cahill and Scott Lathrop and Steven Gordon},
keywords = {HPC workforce, petascale computing, on-line education, graduate education, SPOC course},
abstract = {It has been well documented for more than 30 years, that significantly more effort is needed to prepare the HPC workforce needed today and well into the future. The Blue Waters Virtual School of Computational Science (VSCSE) provides an innovative model for addressing this critical need. The VSCSE uses a Small Private Online Course (SPOC) approach to providing graduate level credit courses to students at multiple institutions. In this paper, we describe the rationale for this approach, a description of the implementation, findings from external evaluations, and lessons learned. The paper concludes with recommendations for future strategies to build on this work to address the workforce needs of our global society.}
}
@article{CORDASCO201815,
title = {Distributed MASON: A scalable distributed multi-agent simulation environment},
journal = {Simulation Modelling Practice and Theory},
volume = {89},
pages = {15-34},
year = {2018},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2018.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X18301230},
author = {Gennaro Cordasco and Vittorio Scarano and Carmine Spagnuolo},
keywords = {Agent-based simulation, Parallel computing, Distributed computing, Scalable computational science, Cloud computing},
abstract = {Computational Social Science (CSS) involves interdisciplinary fields and exploits computational methods, such as social network analysis as well as computer simulation with the goal of better understanding social phenomena. Agent-Based Models (ABMs) represent an effective research tool for CSS and consist of a class of models, which, aim to emulate or predict complex phenomena through a set of simple rules (i.e., independent actions, interactions and adaptation), performed by multiple agents. The efficiency and scalability of ABMs systems are typically obtained distributing the overall computation on several machines, which interact with each other in order to simulate a specific model. Unfortunately, the design of a distributed simulation model is particularly challenging, especially for domain experts who sporadically are computer scientists and are not used to developing parallel code. D-MASON framework is a distributed version of the MASON library for designing and executing ABMs in a distributed environment ensuring scalability and easiness. D-MASON enable the developer to exploit the computing power of distributed environment in a transparent manner; the developer has to do simple incremental modifications to existing MASON models, without re-designing them. This paper presents several novel features and architectural improvements introduced in the D-MASON framework: an improved space partitioning strategy, a distributed 3D field, a distributed network field, a decentralized communication layer, a novel memory consistency mechanism and the integration to cloud environments. Full documentation, additional tutorials, and other material can be found at https://github.com/isislab-unisa/dmason where the framework can be downloaded.}
}
@incollection{ZEYER2015235,
title = {11 - For the mutual benefit: Health information provision in the science classroom},
editor = {Catherine {Arnott Smith} and Alla Keselman},
booktitle = {Meeting Health Information Needs Outside Of Healthcare},
publisher = {Chandos Publishing},
pages = {235-261},
year = {2015},
isbn = {978-0-08-100248-3},
doi = {https://doi.org/10.1016/B978-0-08-100248-3.00011-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780081002483000111},
author = {Albert Zeyer and Daniel M. Levin and Alla Keselman},
keywords = {Health education, Health literacy, Information, Knowledge, Science education},
abstract = {In this chapter, the authors argue that the school science classroom should help students deal with complex real-life information about health and disease. They also discuss means by which curriculum and instruction in science education can be tied to these issues. The chapter reviews opportunities and challenges presented to individuals by the expectations of participatory health care, focusing on models of health literacy that can help understand and address the challenges. The authors argue that the problem of ensuring effective information use often lies in a transmission approach to health information provision. Transmitted knowledge is often not understood nor applied, as demonstrated in studies of human papillomavirus vaccination education. An alternative to knowledge transmission is the approach that aims to foster critical literacy, which is grounded in critical thinking essential to the practice of science. The chapter reviews a number of interdisciplinary science education activities that introduce health issues in the context of biology, physics, and chemistry education, ensuring deep understanding needed for developing critical literacy. It also discusses science education approaches and theories that encourage the development of deep, culturally meaningful science knowledge. Finally, the chapter reviews professional development and the role of various professionals, including teachers and librarians, in the collaborative endeavor of effective health information provision.}
}
@incollection{ELLIS202362,
title = {9.05 - NMR of carboranes},
editor = {Jan Reedijk and Kenneth R. Poeppelmeier},
booktitle = {Comprehensive Inorganic Chemistry III (Third Edition)},
publisher = {Elsevier},
edition = {Third Edition},
address = {Oxford},
pages = {62-106},
year = {2023},
isbn = {978-0-12-823153-1},
doi = {https://doi.org/10.1016/B978-0-12-823144-9.00058-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128231449000583},
author = {David Ellis},
keywords = {Antipodal effect, Boranes, Boron NMR, Carboranes, Computational chemistry, Dynamic NMR, Inorganic chemistry, NICS, NMR spectroscopy, Organometallics, Solid-state NMR},
abstract = {This contribution presents a survey of the literature on the NMR of carboranes from the early years of study to the present day. Following an introduction to the subject, subsequent sections deal with the principal nuclei concerned, namely the isotopes of boron, (overwhelmingly 11B), 1H, 13C and 19F (the latter as there are many examples of clusters where terminal B-Hs are substituted by B-F units, fluorine may therefore be regarded as a significant tool in the field). Despite the title, some literature on boron clusters (carbon-free) is included where it is felt they can illuminate the topic, also metallaboranes and metallacarboranes, with similar justification. Many journal articles describing synthetic and structural studies of carboranes will heavily feature NMR as a technique of characterization. Reference to these is limited, attention being focussed on work where novel or pioneering NMR concepts and discoveries are described. Significant attention is paid to computational methods, especially in relation to 11B NMR, and to the Antipodal Effect, a rich area of study over many years, both computationally and spectroscopically. The utility of NICS (nucleus-independent chemical shift) values in assessing three-dimensional aromaticity of some carboranes, is covered, including comparison with more conventional organic aromatics. Finally, there is included a relatively brief survey of instrumental methods, including 2D techniques, solid-state, and dynamic NMR spectroscopy. There is a vast literature on the NMR of carboranes and this chapter can only be a portal into that space, the reader is directed to the many relevant reviews, referenced in this work, for further information.}
}
@article{COLOM2017385,
title = {Collaborative building of behavioural models based on internet of things},
journal = {Computers & Electrical Engineering},
volume = {58},
pages = {385-396},
year = {2017},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2016.08.019},
url = {https://www.sciencedirect.com/science/article/pii/S0045790616302191},
author = {José Francisco Colom and Higinio Mora and David Gil and María Teresa Signes-Pont},
keywords = {Social internet of things, Big data, Embedded systems, Healthcare, Distributed system framework},
abstract = {This paper proposes a new framework that takes advantage of the computing capabilities provided by the Internet of Thing (IoT) paradigm in order to support collaborative applications. It looks at the requirements needed to run a wide range of computing tasks on a set of devices in the user environment with limited computing resources. This approach contributes to building the social dimension of the IoT by enabling the addition of computing resources accessible to the user without harming the other activities for which the IoT devices are intended. The framework mainly includes a model of the computing load, a scheduling mechanism and a handover procedure for transferring tasks between available devices. The experiments show the feasibility of the approach and compare different implementation alternatives.}
}
@article{LIU2024101276,
title = {Modeling quick autonomous response for virtual characters in safety education games},
journal = {Cognitive Systems Research},
volume = {88},
pages = {101276},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101276},
url = {https://www.sciencedirect.com/science/article/pii/S1389041724000706},
author = {Tingting Liu and Zhen Liu and Yuanyi Wang and Yanjie Chai},
keywords = {Behavior, Emotion, Motivation, Perception, Virtual Character},
abstract = {Serious games have a wide range of applications. Modeling virtual character behaviors and emotions is a challenging task in developing serious games. To generate real-time responses, behavioral and emotional models must be simple and effective. Existing studies have paid little attention to the semantic understanding of virtual characters to external stimuli and have not effectively linked perceived semantics and motivation. This paper proposes a cognitive structure for the virtual character. The structure contains multiple modules: perception, personality, motivation, behavior, and emotion. Based on psychological theory, a semantic table that connects external stimuli, motivations, behaviors, and emotions is designed for each virtual character. Perceptivity is introduced to measure the degree of perception. According to Maslow’s motivation theory, a quantitative description of motivation is given and a discriminating method is proposed to generate behaviors and emotions. A prototype of a serious game is developed to verify the validity of the proposed method. The experimental results show that the proposed method can simulate the behavior and emotion of virtual characters in real time and will enhance the immersion of serious games.}
}
@article{COHEN2017208,
title = {Where Does EEG Come From and What Does It Mean?},
journal = {Trends in Neurosciences},
volume = {40},
number = {4},
pages = {208-218},
year = {2017},
issn = {0166-2236},
doi = {https://doi.org/10.1016/j.tins.2017.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0166223617300243},
author = {Michael X Cohen},
keywords = {EEG, neural microcircuit, oscillations, electrophysiology, computation},
abstract = {Electroencephalography (EEG) has been instrumental in making discoveries about cognition, brain function, and dysfunction. However, where do EEG signals come from and what do they mean? The purpose of this paper is to argue that we know shockingly little about the answer to this question, to highlight what we do know, how important the answers are, and how modern neuroscience technologies that allow us to measure and manipulate neural circuits with high spatiotemporal accuracy might finally bring us some answers. Neural oscillations are perhaps the best feature of EEG to use as anchors because oscillations are observed and are studied at multiple spatiotemporal scales of the brain, in multiple species, and are widely implicated in cognition and in neural computations.}
}
@article{CHENG2023100171,
title = {Leading digital transformation and eliminating barriers for teachers to incorporate artificial intelligence in basic education in Hong Kong},
journal = {Computers and Education: Artificial Intelligence},
volume = {5},
pages = {100171},
year = {2023},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2023.100171},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X23000504},
author = {Eric Chi Keung Cheng and Tianchong Wang},
keywords = {Digital leadership, Teachers' barriers to change, Artificial intelligence in education, AIED},
abstract = {Artificial Intelligence (AI) has rapidly emerged as a transformative force across diverse sectors worldwide, with policy initiatives signalling its rising prominence. In the realm of education, AI holds promise for revolutionising learning and teaching. However, the incorporation of AI in basic education faces significant challenges for many schools, particularly when explicit curriculum guidelines are absent, and teachers have limited exposure to AI technologies. This study delves into the importance of integrating Artificial Intelligence in Education (AIED), particularly in primary and secondary school settings, with a specific emphasis on the obstacles and facilitators. The research examines the impact of digital leadership and its role in mitigating teachers' barriers to AIED incorporations. These external variables are systematically outlined within the conceptual framework proposed by this study. Furthermore, it conceptualises and considers three AIED approaches—Learning from AI, Learning about AI, and Learning with AI—as internal variables central to basic-education AIED incorporations, as highlighted in previous research. The study gathers quantitative data through a self-constructed questionnaire survey from 204 school principals and management-level teachers across 60 primary and secondary schools in Hong Kong, capturing their perceptions of AIED incorporation. A structural equation model (SEM) is adopted to analyse the relationships between digital leadership, internal and external teacher barriers, and the three AIED approaches. The findings underscore that digital leadership is a significant factor in facilitating AI incorporations in schools, and that teachers' internal and external barriers considerably affect Learning about AI. The paper concludes by proposing strategies to enhance the capabilities of school leaders and teachers in promoting AIED in basic education.}
}
@incollection{CARSTON2006559,
title = {Language of Thought},
editor = {Keith Brown},
booktitle = {Encyclopedia of Language & Linguistics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {559-561},
year = {2006},
isbn = {978-0-08-044854-1},
doi = {https://doi.org/10.1016/B0-08-044854-2/04780-5},
url = {https://www.sciencedirect.com/science/article/pii/B0080448542047805},
author = {R. Carston},
keywords = {biosemantics, computational theory of mind, connectionism, intentional realism, intentionality, Mentalese, methodological solipsism, productivity (of thought), propositional attitude, psychosemantics, representational theory of mind, syntactic structure, systematicity (of thought)},
abstract = {Two key aspects of human public languages are syntax and semantics, where syntax concerns the combinatorial structure of linguistic expressions and semantics refers to their content or meaning. So, the claim that humans have a language of thought, defended in particular by Jerry Fodor, amounts to the view that thoughts are representational (semantic) and thought processes are computational, that is, they involve transformations of symbolic structures on the basis of their formal (syntactic) properties. The fact that thought, like language, exhibits ‘productivity’ and ‘systematicity’ argues for a system of mental representation that has language-like structure.}
}
@article{GILES20221,
title = {A retrospective study of antivenom-associated adverse reaction and anaphylaxis at Ngwelezana Hospital, South Africa},
journal = {Toxicon},
volume = {217},
pages = {1-4},
year = {2022},
issn = {0041-0101},
doi = {https://doi.org/10.1016/j.toxicon.2022.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0041010122002227},
author = {T. Giles and S.R. Čačala and D. Wood and J. Klopper and G.V. Oosthuizen},
keywords = {Antivenom, Anaphylaxis, Ngwelezana, South Africa},
abstract = {Background
Snakebite victims are commonly seen in KwaZulu-Natal Hospitals, with only a minority of patients requiring antivenom. This study reviewed antivenom-associated adverse events at our institution, after administration of the South African Vaccine Producers (SAVP) polyvalent antivenom.
Methods
A retrospective review, over 52 months (January 2016–April 2020), of patients who received antivenom. Demographics, clinical details and clinical course following antivenom administration were analysed.
Results
Emergency department doctors treated 758 snakebites; 156 patients were admitted of which 51 (33%) received antivenom. Indications for antivenom included: neurotoxicity (24%), haemotoxicity (18%) and significant cytotoxicity (58%). Antivenom-associated adverse events occurred in 61% of patients; with 47% developing anaphylaxis requiring adrenaline infusion. There was a higher incidence of anaphylaxis in children (57%) than in adults (40%), p = 0.55. There was no association between antivenom dose and anaphylaxis. No benefit was noted with adrenaline premedication (p = 0.64), nor with the addition of antihistamine or steroid pre-medicants to adrenaline (p = 0.61). Multivariable logistic regression identified age as a predictor for anaphylaxis, but not dose or duration of antivenom and not any particular form of premedication. Intubation was required in 29% of patients developing anaphylaxis. There were no deaths and all patients made full recovery.
Conclusion
Almost half of the patients at Ngwelezana hospital in Kwazulu-Natal receiving the SAVP polyvalent antivenom developed anaphylaxis requiring adrenaline infusion, with children at higher risk. The administration of this antivenom must only be given for valid indications, in a high-care environment by medical personnel ready to manage anaphylactic shock. The addition of antihistamine and corticosteroids to adrenaline for premedication has no added benefit.}
}
@article{IKENMEYER2022106243,
title = {A note on VNP-completeness and border complexity},
journal = {Information Processing Letters},
volume = {176},
pages = {106243},
year = {2022},
issn = {0020-0190},
doi = {https://doi.org/10.1016/j.ipl.2021.106243},
url = {https://www.sciencedirect.com/science/article/pii/S0020019021001587},
author = {Christian Ikenmeyer and Abhiroop Sanyal},
keywords = {Theory of computation, Computational complexity, Algebraic complexity theory, Border complexity, Reductions},
abstract = {In 1979 Valiant introduced the complexity class VNP of p-definable families of polynomials, he defined the reduction notion known as p-projection and he proved that the permanent polynomial and the Hamiltonian cycle polynomial are VNP-complete under p-projections. In 2001 Mulmuley and Sohoni (and independently Bürgisser) introduced the notion of border complexity to the study of the algebraic complexity of polynomials. In this algebraic machine model, instead of insisting on exact computation, approximations are allowed. This gives VNP the structure of a topological space. In this short note we study the set VNPC of VNP-complete polynomials. We show that the complement VNP ∖ VNPC lies dense in VNP. Quite surprisingly, we also prove that VNPC lies dense in VNP. We prove analogous statements for the complexity classes VF, VBP, and VP. The density of VNP ∖ VNPC holds for several different reduction notions: p-projections, border p-projections, c-reductions, and border c-reductions. We compare the relationships of the completeness notions under these reductions and separate most of the corresponding sets. Border reduction notions were introduced by Bringmann, Ikenmeyer, and Zuiddam ((2018) [7]). Our paper is the first structured study of border reduction notions.}
}
@article{2024100671,
title = {Erratum regarding missing Declaration of Competing Interest statements in previously published articles},
journal = {International Journal of Child-Computer Interaction},
volume = {41},
pages = {100671},
year = {2024},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100671},
url = {https://www.sciencedirect.com/science/article/pii/S2212868924000394}
}
@article{NAPIER2014331,
title = {Insight into the numerical challenges of implementing 2-dimensional SOA models in atmospheric chemical transport models},
journal = {Atmospheric Environment},
volume = {96},
pages = {331-344},
year = {2014},
issn = {1352-2310},
doi = {https://doi.org/10.1016/j.atmosenv.2014.07.048},
url = {https://www.sciencedirect.com/science/article/pii/S1352231014005780},
author = {W.J. Napier and J.J. Ensberg and J.H. Seinfeld},
keywords = {Secondary organic aerosol, 2-Dimensional SOA model, Chemical transport model, Probability distribution, Computational efficiency},
abstract = {The new generation of secondary organic aerosol (SOA) models that represent gas- and particle-phase chemistry and thermodynamic partitioning using discrete two-dimensional grids (e.g. SOM, 2D-VBS) cannot be efficiently implemented into three-dimensional atmospheric chemical transport models (CTMs) due to the large number of bins (tracers) required. In this study, we introduce a novel mathematical framework, termed the Oxidation State/Volatility Moment Method, that is designed to address these computational burdens so as to allow the new generation of SOA models to be implemented into CTMs. This is accomplished by mapping the two-dimensional grids onto probability distributions that conserve carbon and oxygen mass. Assessment of the Moment Method strengths (speed, carbon and oxygen conservation) and weaknesses (numerical drift) provide valuable insight that can guide future development of SOA modules for atmospheric CTMs.}
}
@article{MCMURRAN2023102349,
title = {The relationship between students' gender and their confidence in the correctness of their solutions to complex and difficult mathematics problems},
journal = {Learning and Individual Differences},
volume = {107},
pages = {102349},
year = {2023},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2023.102349},
url = {https://www.sciencedirect.com/science/article/pii/S1041608023000936},
author = {Meaghan McMurran and David Weisbart and Kinnari Atit},
keywords = {Gender, Mathematics, Confidence, Calibration, Problem difficulty},
abstract = {It is well established that men more frequently exhibit over-confidence than women in mathematics. Less is known about the relationship between gender and problem-specific confidence judgments and whether this relationship depends on problem difficulty. To investigate the relationship between gender, problem difficulty, and problem-specific confidence judgments, including how calibrated and under/over-confident men and women are, we examine data from 349 women and 279 men who were instructed to solve 13 difficult and complex mathematics problems and report their confidence in the correctness of their solutions. We find that men were more confident than women and that the gender gap in confidence decreases with increasing problem difficulty. Women were better calibrated than men and the gender gap in calibration increased with problem difficulty. As problem difficulty increased, women, but not men, transitioned from being under- to over-confident.}
}
@article{SANTASORIANO2021102035,
title = {Engaging universe 4.0: The case for forming a public relations-strategic intelligence hybrid},
journal = {Public Relations Review},
volume = {47},
number = {2},
pages = {102035},
year = {2021},
issn = {0363-8111},
doi = {https://doi.org/10.1016/j.pubrev.2021.102035},
url = {https://www.sciencedirect.com/science/article/pii/S0363811121000278},
author = {Alba {Santa Soriano} and Rosa María {Torres Valdés}},
keywords = {Public relations, Strategic intelligence, Universe 4.0, Disruption, Engagement, Hybridity},
abstract = {In this article we follow a range of significant academics, practitioners and policy makers in highlighting the need to engage with the disruptive digital transformation of the fourth industrial revolution. Often called universe 4.0, this is an economy based on data that dilutes, in an unprecedented fashion, the boundaries between the physical, biological and digital world in all spheres of society. In having to face up to the uncertainty, complexity and speed associated with this global challenge, organisations of all kinds will be impelled to rethink future skills, jobs and business models. We focus on how this uncertain situation affects public relations. We suggest that the resulting paradoxes and controversies become an opportunity to research and reflect on the past and present of our discipline and redirect attention to the processes, strategies, and tools of intervention required for improved contemporary and future effectiveness. This present study analyses universe 4.0 processes affecting both the practice and the social function of public relations and argues for close linkages with multidimensional strategic intelligences and disruptive technologies based on artificial intelligence. It also involves conducting exploratory qualitative research based on a bibliometric analysis of specialised literature; undertaking content analysis via computational linguistics techniques; and applying Delphi methodology to consider public relations in universe 4.0. Our findings suggest not only that strategic intelligence is under researched in our field, but that a more developed public relations intelligence capable of adapting to universe 4.0 needs to be a hybrid of existing public relations and contemporary strategic intelligence.}
}
@article{ZHI2024120154,
title = {An efficient conflict analysis method based on splitting and merging of formal contexts},
journal = {Information Sciences},
volume = {661},
pages = {120154},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.120154},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524000677},
author = {Huilai Zhi and Zhenhao Qi and Yinan Li},
keywords = {Conflict analysis, Information fusion, Formal concept analysis, Three-way decision},
abstract = {Conflict situations are widespread nearly in all corners of social life, and the efficiency of conflict analysis still has much room especially for large-scale data sets. To this end, this study presents an information fusion method of fast conflict analysis based on formal concept analysis. Firstly, a novel type of three-way concepts is defined to neatly deal with three-valued formal contexts. Then, fast computation of conflict analysis is explored by using an information fusion technique. Specifically, conflict analysis based on horizontal splitting and merging of formal contexts and the one based on vertical splitting and merging of formal contexts are respectively investigated. Finally, systematic experiments are carried out on both synthetic data sets and real cases to evaluate the performance of the proposed method. According to the experimental results, conflict analysis based on vertical splitting and merging of formal contexts can effectively improve the performance, which may not be realized by using the horizontal splitting and merging strategy in some cases. This study may shed light on formal concept analysis based multi-source big data analysis.}
}
@incollection{BUCHANAN2014183,
title = {Chapter Seven - Edge Replacement and Minimality as Models of Causal Inference in Children},
editor = {Janette B. Benson},
series = {Advances in Child Development and Behavior},
publisher = {JAI},
volume = {46},
pages = {183-213},
year = {2014},
issn = {0065-2407},
doi = {https://doi.org/10.1016/B978-0-12-800285-8.00007-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128002858000078},
author = {David W. Buchanan and David M. Sobel},
keywords = {Causal reasoning, Causal graphical models, Edge replacement, Cognitive Development, Computational Models},
abstract = {Recently, much research has focused on causal graphical models (CGMs) as a computational-level description of how children represent cause and effect. While this research program has shown promise, there are aspects of causal reasoning that CGMs have difficulty accommodating. We propose a new formalism that amends CGMs. This edge replacement grammar formalizes one existing and one novel theoretical commitment. The existing idea is that children are determinists, in the sense that they believe that apparent randomness comes from hidden complexity, rather than inherent nondeterminism in the world. The new idea is that children think of causation as a branching process: causal relations grow not directly from the cause, but from existing relations between the cause and other effects. We have shown elsewhere that these two commitments together, when formalized, can explain and quantitatively fit the otherwise puzzling effect of nonindependence observed in the adult causal reasoning literature. We then test the qualitative predictions of this new formalism on children in a series of three experiments.}
}
@article{BRADLEY2016400,
title = {Jet flame heights, lift-off distances, and mean flame surface density for extensive ranges of fuels and flow rates},
journal = {Combustion and Flame},
volume = {164},
pages = {400-409},
year = {2016},
issn = {0010-2180},
doi = {https://doi.org/10.1016/j.combustflame.2015.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0010218015003120},
author = {Derek Bradley and Philip H. Gaskell and Xiaojun Gu and Adriana Palacios},
keywords = {Jet flame height, Lift-off distance, Flamelet modelling, “Fracking”, Jet flame stability, Mean flame surface density},
abstract = {An extensive review and re-thinking of jet flame heights and structure, extending into the choked/supersonic regime is presented, with discussion of the limitations of previous flame height correlations. Completely new dimensionless correlations for the plume heights, lift-off distances, and mean flame surface densities of atmospheric jet flames, in the absence of a cross wind, are presented. It was found that the same flow rate parameter could be used to correlate both plume heights and flame lift-off distances. These are related to the flame structure, jet flame instability, and flame extinction stretch rates, as revealed by complementary experiments and computational studies. The correlations are based on a vast experimental data base, covering 880 flame heights. They encompass pool fires and flares, as well as choked and unchoked jet flames of CH4, C2H2, C2H4, C3H8, C4H10 and H2, over a wide range of conditions. Supply pressures range from 0.06 to 90 MPa, discharge diameters from 4 × 10−4 to 1.32 m, and flame heights from 0.08 to 110 m. The computational studies enabled reaction zone volumes to be estimated, as a proportion of the plume volumes, measured from flame photographs, and temperature contours. This enabled mean flame surface densities to be estimated, together with mean volumetric heat releases rates. There is evidence of a “saturation” mean surface density and increases in turbulent burn rates being accomplished by near pro rata increases in the overall volume of reacting mixture.}
}
@article{MAKRIDIS201328,
title = {Offshore wind power resource availability and prospects: A global approach},
journal = {Environmental Science & Policy},
volume = {33},
pages = {28-40},
year = {2013},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2013.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S146290111300097X},
author = {Christos Makridis},
keywords = {Offshore wind energy, Renewable energy, Global perspective, Renewable energy investment},
abstract = {In the absence of structural incentives that price negative externalities, renewable energies rely primarily on investors’ expectations of future performance to succeed in the marketplace. While there have been many disparate regional analyses of the prospects for clean energy, in particular wind, there is yet a cohesive framework for thinking about global interactions. Using data from the National Renewable Energy Laboratory (NREL), the article addresses three shortcomings in empirical renewable policy literature. First, the article briefly synthesizes the current state of the offshore wind literature. Second, the article develops a linear programming model to assess the relative prospects of offshore wind energy throughout seven world regions: Organization for Economic Co-operation and Development (OECD) North America, OECD Europe, OECD Asia and Eurasia, Non-OECD Europe and Eurasia, Non-OECD Asia, Africa, and Central & South America. Third, the article applies the Interactive Agency Model (IAM) as a systems-level framework for thinking about offshore wind development in the presence of social, economic, and institutional attributes. Results suggest that OECD Asia and Eurasia, OECD Europe, and non-OECD Europe and Eurasia, respectively, have the highest potential for offshore wind sector performance. Despite simplifying assumptions, this article presents one of the first evaluations of global offshore wind energy potential for policymakers and industry to consider in crafting future renewable energy investment decisions.}
}
@article{MAO2022e10011,
title = {Development of an innovative data-driven system to generate descriptive prediction equation of dielectric constant on small sample sets},
journal = {Heliyon},
volume = {8},
number = {8},
pages = {e10011},
year = {2022},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2022.e10011},
url = {https://www.sciencedirect.com/science/article/pii/S2405844022012993},
author = {Jiashun Mao and Amir Zeb and Min Sung Kim and Hyeon-Nae Jeon and Jianmin Wang and Shenghui Guan and Kyoung Tai NO},
keywords = {Dielectric constant, Data-driven framework, Explainable, Small sample sets, Variable relationship network, QSPR},
abstract = {Dielectric constant (DC, ε) is a fundamental parameter in material sciences to measure polarizability of the system. In industrial processes, its value is an imperative indicator, which demonstrates the dielectric property of material and compiles information including separation information, chemical equilibrium, chemical reactivity analysis, and solubility modeling. Since, the available ε-prediction models are fairly primitive and frequently suffer from serious failures especially when deals with strong polar compounds. Therefore, we have developed a novel data-driven system to improve the efficiency and wide-range applicability of ε using in material sciences. This innovative scheme adopts the correlation distance and genetic algorithm to discriminate features’ combination and avoid overfitting. Herein, the prediction output of the single ML model as a coding to estimate the target value by simulating the layer-by-layer extraction in deep learning, and enabling instant search for the optimal combination of features is recruited. Our model established an improved correlation value of 0.956 with target as compared to the previously available best traditional ML result of 0.877. Our framework established a profound improvement, especially for material systems possessing ε value >50. In terms of interpretability, we have derived a conceptual computational equation from a minimum generating tree. Our innovative data-driven system is preferentially superior over other methods due to its application for the prediction of dielectric constants as well as for the prediction of overall micro and macro-properties of any multi-components complex.}
}
@article{MEDFORD201536,
title = {From the Sabatier principle to a predictive theory of transition-metal heterogeneous catalysis},
journal = {Journal of Catalysis},
volume = {328},
pages = {36-42},
year = {2015},
note = {Special Issue: The Impact of Haldor Topsøe on Catalysis},
issn = {0021-9517},
doi = {https://doi.org/10.1016/j.jcat.2014.12.033},
url = {https://www.sciencedirect.com/science/article/pii/S0021951714003686},
author = {Andrew J. Medford and Aleksandra Vojvodic and Jens S. Hummelshøj and Johannes Voss and Frank Abild-Pedersen and Felix Studt and Thomas Bligaard and Anders Nilsson and Jens K. Nørskov},
keywords = {Heterogeneous catalysis, Transition metals, Theory, Computational catalysis, DFT, Sabatier principle, Scaling relation, Descriptor},
abstract = {We discuss three concepts that have made it possible to develop a quantitative understanding of trends in transition-metal catalysis: scaling relations, activity maps, and the d-band model. Scaling relations are correlations between surface bond energies of different adsorbed species including transition states; they open the possibility of mapping the many parameters determining the rate of a full catalytic reaction onto a few descriptors. The resulting activity map can be viewed as a quantitative implementation of the classical Sabatier principle, which states that there is an optimum “bond strength” defining the best catalyst for a given reaction. In the modern version, the scaling relations determine the relevant “bond strengths” and the fact that these descriptors can be measured or calculated makes it a quantitative theory of catalysis that can be tested experimentally by making specific predictions of new catalysts. The quantitative aspect of the model therefore provides new possibilities in catalyst design. Finally, the d-band model provides an understanding of the scaling relations and variations in catalytic activity in terms of the electronic structure of the transition-metal surface.}
}
@article{VALERY201844,
title = {A collaborative CPU–GPU approach for principal component analysis on mobile heterogeneous platforms},
journal = {Journal of Parallel and Distributed Computing},
volume = {120},
pages = {44-61},
year = {2018},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2018.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0743731518303411},
author = {Olivier Valery and Pangfeng Liu and Jan-Jan Wu},
keywords = {OpenCL, GPGPU, Mobile computing, Heterogeneous system, PCA, Energy efficient, Acceleration, Data analysis, Machine learning},
abstract = {The advent of the modern GPU architecture has enabled computers to use General Purpose GPU capabilities (GPGPU) to tackle large scale problem at a low computational cost. This technological innovation is also available on mobile devices, addressing one of the primary problems with recent devices: the power envelope. Unfortunately, recent mobile GPUs suffer from a lack of accuracy that can prevent them from running any large scale data analysis tasks, such as principal component analysis (Shlens, 0000) (PCA). The goal of our work is to address this limitation by combining the high precision available on a CPU with the power efficiency of a mobile GPU. In this paper, we exploit the shared memory architecture of mobile devices in order to enhance the CPU–GPU collaboration and speed up PCA computation without sacrificing precision. Experimental results suggest that such an approach drastically reduces the power consumption of the mobile device while accelerating the overall workload. More generally, we claim that this approach can be extended to accelerate other vectorized computations on mobile devices while still maintaining numerical accuracy.}
}
@article{DRAWEL2017632,
title = {Reasoning about Trust and Time in a System of Agents},
journal = {Procedia Computer Science},
volume = {109},
pages = {632-639},
year = {2017},
note = {8th International Conference on Ambient Systems, Networks and Technologies, ANT-2017 and the 7th International Conference on Sustainable Energy Information Technology, SEIT 2017, 16-19 May 2017, Madeira, Portugal},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.05.369},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917310384},
author = {Nagat Drawel and Jamal Bentahar and Elhadi Shakshuki},
keywords = {Multi-Agent Systems (MASs), trust, temporal logic},
abstract = {Abstract:
The study of trust in Multi-Agent Systems (MASs) has been an area of interest for many researchers over the last years. This is due to the fact that trust is the basis for agent communication wherein entities have to operate in a dynamic and uncertain environment. Several approaches have been proposed to define logical semantics for trust in MASs. However, these approaches are limited to reason about trust based on the sole agents’ mental states. Therefore, this paper considers trust from a high-level abstraction based on the social correct behaviors of agents. Specifically, we propose a logical framework that allows us to reason about unconditional trust and time. In particular, we introduce a new logical language called Trust Computation Tree logic (TCTL) that extends the Computation Tree Logic (CTL) with a new modality to represent trust. We describe the semantics by extending the interpreted systems formalism and consider a set of reasoning rules along with proofs to support our logic. Finally, we evaluate our approach using a real-life case study in the e-business domain to explain our proposed logic in a practical application.}
}