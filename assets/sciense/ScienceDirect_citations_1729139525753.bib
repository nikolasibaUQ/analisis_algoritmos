@article{WILLS2019104027,
title = {Reflexivity, coding and quantum biology},
journal = {Biosystems},
volume = {185},
pages = {104027},
year = {2019},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2019.104027},
url = {https://www.sciencedirect.com/science/article/pii/S0303264719302394},
author = {Peter R Wills},
keywords = {Genetic coding, Reflexivity, Quantum biology, Information, Autocatalysis, Aminoacyl-tRRNA synthetase (aaRS)},
abstract = {Biological systems are fundamentally computational in that they process information in an apparently purposeful fashion rather than just transferring bits of it in a purely syntactical manner. Biological information, such has genetic information stored in DNA sequences, has semantic content. It carries meaning that is defined by the molecular context of its cellular environment. Information processing in biological systems displays an inherent reflexivity, a tendency for the computational information-processing to be “about” the behaviour of the molecules that participate in the computational process. This is most evident in the operation of the genetic code, where the specificity of the reactions catalysed by the aminoacyl-tRNA synthetase (aaRS) enzymes is required to be self-sustaining. A cell’s suite of aaRS enzymes completes a reflexively autocatalytic set of molecular components capable of making themselves through the operation of the code. This set requires the existence of a body of reflexive information to be stored in an organism’s genome. The genetic code is a reflexively self-organised mapping of the chemical properties of amino acid sidechains onto codon “tokens”. It is a highly evolved symbolic system of chemical self-description. Although molecular biological coding is generally portrayed in terms of classical bit-transfer events, various biochemical events explicitly require quantum coherence for their occurrence. Whether the implicit transfer of quantum information, qbits, is indicative of wide-ranging quantum computation in living systems is currently the subject of extensive investigation and speculation in the field of Quantum Biology.}
}
@article{HADJTAIEB2014238,
title = {Ontology-based approach for measuring semantic similarity},
journal = {Engineering Applications of Artificial Intelligence},
volume = {36},
pages = {238-261},
year = {2014},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2014.07.015},
url = {https://www.sciencedirect.com/science/article/pii/S0952197614001833},
author = {Mohamed Ali {Hadj Taieb} and Mohamed {Ben Aouicha} and Abdelmajid {Ben Hamadou}},
keywords = {Semantic similarity, WordNet ontology, Taxonomic knowledge, Taxonomical parameters},
abstract = {The challenge of measuring semantic similarity between words is to find a method that can simulate the thinking process of human. The use of computers to quantify and compare semantic similarities has become an important area of research in various fields, including artificial intelligence, knowledge management, information retrieval and natural language processing. The development of efficient measures for the computation of concept similarity is fundamental for computational semantics. Several computational measures rely on knowledge resources to quantify semantic similarity, such as the WordNet « is a » taxonomy. Several of these measures are based on taxonomical parameters to achieve the best expression possible for the semantics of content. This paper presents a new measure for quantifying the degree of the semantic similarity between concepts and words based on the WordNet hierarchy and using a number of topological parameters related to the “is a” taxonomy. Our proposal combines, in a complementary way, the hyponyms and depth parameters. This measure takes the problem of fine granularity into account. It is argued, however, that WordNet sense distinctions are highly fine-grained even for humans. We, therefore, propose a new method to quantify the hyponyms subgraph of a given concept based on depth distribution. Common nouns datasets (RG65, MC30 and AG203), medical terms dataset (MED38) and verbs dataset (YP130) formed by word pairs are used in the assessment. We start by calculating semantic similarities and then compute the correlation coefficient between human judgement and computational measures. The results demonstrate that, compared to other currently available computational methods, the measure presented in this study yields into better levels of performance. Compared to several measures, it shows good accuracy covering all the pairwises of the verbs dataset YP130.}
}
@article{CLARO20121042,
title = {Assessment of 21st century ICT skills in Chile: Test design and results from high school level students},
journal = {Computers & Education},
volume = {59},
number = {3},
pages = {1042-1053},
year = {2012},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2012.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0360131512000887},
author = {Magdalena Claro and David D. Preiss and Ernesto {San Martín} and Ignacio Jara and J. Enrique Hinostroza and Susana Valenzuela and Flavio Cortes and Miguel Nussbaum},
keywords = {ICT skills, 21st century skills, Information literacy, Computer literacy, Higher-order thinking skills, Digital divide, Rasch model},
abstract = {This paper describes a study that evaluates fifteen-year-old Chilean students Information and Communication Technology (ICT) skills. The paper presents an operational definition of ICT skills, an instrument measuring these skills as well as the students' results in the test. The definition of ICT skills used considers Chile's curricular framework, functional and cognitive skills. Specifically, ICT skills were defined as the capacity to solve problems of information, communication and knowledge in digital environments. A performance-based assessment was designed in a virtual environment to measure these skills. The analysis of the results showed that the majority of students were able to solve tasks related to the use of information as consumers, i.e., approximately three quarters of the students were able to search for information and half of them were also able to organize and manage digital information. Additionally, they show that very few students were able to succeed in tasks related to the use of information as producers, i.e., only one third of the students were able to develop their own ideas in a digital environment and less than one fifth were able to refine digital information and create a representation in a digital environment. Socioeconomic group, access, daily use and confidence in doing ICT-related activities were all positively associated with higher scores, showing the need to implement strategies to compensate this inequality, possibly by explicitly defining these aims in the national curriculum.}
}
@article{FERNANDES202191,
title = {Pruning of generative adversarial neural networks for medical imaging diagnostics with evolution strategy},
journal = {Information Sciences},
volume = {558},
pages = {91-102},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.12.086},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521000189},
author = {Francisco Erivaldo Fernandes and Gary G. Yen},
keywords = {Deep Neural Networks, Convolutional Neural Networks, Generative Adversarial Networks, Medical Imaging Diagnostics, Evolution Strategy, Pruning},
abstract = {Deep Convolutional Neural Networks (DCNNs) have the potential to revolutionize the field of Medical Imaging Diagnostics due to their capabilities of learning by using only raw data. However, DCNNs can only learn when trained using thousands of data points, which is not always available when dealing with medical data. Moreover, due to patient privacy concerns and the small prevalence of certain diseases in the population, medical data often presents unbalanced classes and fewer data points than other data types. Researchers often rely on Generative Adversarial Networks (GANs) to synthesize more data from a given distribution to solve this problem. Nevertheless, GANs are computationally intensive models requiring the use of powerful hardware to run. In the present work, an algorithm for pruning GANs based on Evolution Strategy (ES) and Multi-Criteria Decision Making (MCDM) is proposed in which a model with the best trade-off between computational complexity and synthesis performance can be found without the use of any trade-off parameter. In the proposed algorithm, the model with the best trade-off is defined geometrically as the candidate solution with the minimum Manhattan distance (MMD) in a two-dimensional objective space established by the number of Floating-Point Operations (FLOPs) and the Wasserstein distance of all candidate solutions, also known as the knee solution. The results show that the pruned GAN model achieves similar performance compared with the original model with up to 70% fewer Floating-Point Operations.}
}
@article{WANG2024104007,
title = {An efficient certificateless blockchain-enabled authentication scheme to secure producer mobility in named data networks},
journal = {Journal of Network and Computer Applications},
volume = {232},
pages = {104007},
year = {2024},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2024.104007},
url = {https://www.sciencedirect.com/science/article/pii/S108480452400184X},
author = {Cong Wang and Tong Zhou and Maode Ma and Yuwen Xiong and Xiankun Zhang and Chao Liu},
keywords = {NDN, Producer, Mobile, Certificateless, Authentication, Blockchain},
abstract = {Named Data Networking (NDN) aims to establish an efficient content delivery architecture. In NDN, secure and effective identity authentication schemes ensure secure communication between producers and routers. Currently, there is no feasible solution to perform identity authentication of mobile producers in NDNs. Identity authentication schemes in other networks are either weak in security or performance, such as privacy leakage, difficulty to establish cross-domain trust, and long handover delays, and are not fully adaptable to the security requirements of NDNs. Additionally, the mobility of producers was not fully considered in the initial design of NDNs. This paper first revises the structure of packets and routers to support the identity authentication and mobility of producers. On this basis, this paper proposes a secure and efficient certificateless ECC-based producer identity authentication scheme (CL-BPA), which includes initial authentication and re-authentication, aimed at achieving rapid switch authentication and integrating blockchain to solve single-point failure issues. Using the Canetti and Krawczyk (CK) adversarial model and informal security analysis, the proposed CL-BPA scheme is demonstrated to be resistant to anonymity attacks, identity forgery attacks, and man-in-the-middle attacks. The performance analysis demonstrates that the proposed CL-BPA scheme exhibits excellent capabilities in terms of computation delay, communication cost, smart contract execution time, average response delay, and throughput.}
}
@incollection{DEDEOGLU2023251,
title = {Chapter Nine - Blockchain meets edge-AI for food supply chain traceability and provenance},
editor = {Joost Laurus Dinant Nelis and Aristeidis S. Tsagkaris},
series = {Comprehensive Analytical Chemistry},
publisher = {Elsevier},
volume = {101},
pages = {251-275},
year = {2023},
booktitle = {Smartphones for Chemical Analysis: From Proof-of-concept to Analytical Applications},
issn = {0166-526X},
doi = {https://doi.org/10.1016/bs.coac.2022.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0166526X22001064},
author = {Volkan Dedeoglu and Sidra Malik and Gowri Ramachandran and Shantanu Pal and Raja Jurdak},
keywords = {Blockchain, Edge AI, Traceability, Provenance, Supply Chains},
abstract = {Food supply chains are increasingly digitised and automated through the use of technologies such as Internet-of-Things (IoT), blockchain and Artificial Intelligence (AI). Such digitization efforts often rely on cloud computing, which creates bandwidth overhead, high latency, security and privacy challenges. In this chapter, we propose the use of edge AI, which is a computing paradigm that combines edge computing and AI, to complete computing tasks close to the sensor data sources. Edge AI can promote greater scalability and avoid the security and privacy challenges of centralised cloud computing. The chapter introduces the provenance and traceability requirements of food supply chains and the digitization of these supply chains through blockchain, IoT, and AI. The chapter also proposes the use of smartphone integrated sensors to provide unique physical, chemical, or biological signatures of food supply chain products, and to conduct the necessary computations on the smartphone. The proposed Edge AI approach to supply chain digitization sets the scene for greater resilience in modern digital supply chains.}
}
@article{PLUZHNIKOVA202334,
title = {The Human Factor and the Problem of Transport Safety in Modern Conditions},
journal = {Transportation Research Procedia},
volume = {68},
pages = {34-39},
year = {2023},
note = {XIII International Conference on Transport Infrastructure: Territory Development and Sustainability},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2023.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S2352146523000078},
author = {N.N. Pluzhnikova},
keywords = {Transport, transport robotics, intelligence, IT-technologies, human},
abstract = {The article is devoted to the analysis of the interaction between man and artificial intelligence to ensure the safety of transport. The author analyzes intelligence and IT technologies on the example of the development of transport robotics. To consider this problem the author refers to cognitive developments in this area, and also indicates the philosophical problems of the development of transport robotics. The article uses such methods as comparative and structural analysis.}
}
@article{STEGER2021127,
title = {Mental models of a social-ecological system facilitate social learning among a diverse management team},
journal = {Environmental Science & Policy},
volume = {122},
pages = {127-138},
year = {2021},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2021.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S1462901121001039},
author = {Cara Steger and Kflay Gebrehiwot and Shambel Alemu Chengere and Jake Marinkovich and Bikila Warkineh Dullo and Sisay Wube Zewde and Julia A. Klein},
keywords = {Participatory modeling, Social learning, Collaborative environmental management, Community-based conservation, Social-ecological systems, Ethiopia},
abstract = {Managing social-ecological systems increasingly requires collaboration among diverse teams with a wide range of worldviews and perspectives. Increased attention to the social and cultural factors that shape environmental outcomes is needed for these collaborative teams to function effectively. Mental models are cognitive representations of the external world which guide an individual’s thinking, decision-making, and behavior. They are critical elements of collaborative environmental management because they shape our understanding of social-ecological systems, our perceptions of environmental problems, and our preferences for certain management actions. In this paper, we describe an iterative process of constructing and revising mental models at both individual and small group levels over the course of a year in a community-based conservation area in the Ethiopian highlands. We compared mental models of the conservation area from four groups involved in management to identify commonalities and differences in the way people conceptualize the area. While we found high variability in mental models both within and across groups, most participants perceived social, economic, and political variables to be the key drivers of change in this system. Economic variables were also identified as key sensitivities, along with biotic and livelihood variables. However, groups differed considerably in how they thought about relationships between these variables, particularly political and economic variables. We used interviews with participants to assess how they learned throughout the mental modeling process, finding evidence of changes to stakeholder relationships, system understanding, and the time horizons used in planning. Women farmers differed from other groups at multiple stages in our process, both in the structure of the models they produced and in the social learning they experienced. Our study was strengthened by the iterative process that allowed individuals and small groups to reflect on their own understanding and share it with others, resulting in increased communication, mutual respect, and understanding among members of the management team. These findings point to the complementarity of both individual and group-level mental modeling for nuanced system understanding, and emphasize the need for diverse perspectives in collaborative environmental management in order for holistic understanding of both problems and solutions to emerge.}
}
@incollection{KLOCKING202597,
title = {Geochemical databases},
editor = {Ariel Anbar and Dominique Weis},
booktitle = {Treatise on Geochemistry (Third edition)},
publisher = {Elsevier},
edition = {Third edition},
address = {Oxford},
pages = {97-135},
year = {2025},
isbn = {978-0-323-99763-8},
doi = {https://doi.org/10.1016/B978-0-323-99762-1.00123-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323997621001236},
author = {Marthe Klöcking and Kerstin A. Lehnert and Lesley Wyborn},
keywords = {Artificial intelligence, CARE, Community standards, Data ethics, Data management, Databases, FAIR, Geochemistry, Machine learning, Machine readable data, Repository, TRUST},
abstract = {Geochemistry is a data-driven discipline. Modern laboratories produce highly diverse data, and the recent exponential increase in data volumes is challenging established practices and capabilities for organizing, analyzing, preserving, and accessing these data. At the same time, sophisticated computational techniques, including machine learning, are increasingly applied to geochemical research questions, which require easy access to large volumes of high-quality, well-organized, and standardized data. Data management has been important since the beginning of geochemistry but has recently become a necessity for the discipline to thrive in the age of digitalization and artificial intelligence. This paper summarizes the landscape of geochemical databases, distinguishing different types of data systems based on their purpose, and their evolution in a historic context. We apply the life cycle model of geochemical data; explain the relevance of current standards, practices, and policies that determine the design of modern geochemical databases and data management; the ethics of data reuse such as data ownership, data attribution, and data citation; and finally create a vision for the future of geochemical databases: data being born digital, connected to agreed community standards, and contributing to global democratization of geochemical data.}
}
@article{QIAO2024120105,
title = {Towards retraining-free RNA modification prediction with incremental learning},
journal = {Information Sciences},
volume = {660},
pages = {120105},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.120105},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524000185},
author = {Jianbo Qiao and Junru Jin and Haoqing Yu and Leyi Wei},
keywords = {RNA modification, Deep learning, Incremental learning},
abstract = {RNA modifications are important for deciphering the function of cells and their regulatory mechanisms. In recent years, researchers have developed many deep learning methods to identify specific modifications. However, these methods require model retraining for each new RNA modification and cannot progressively identify the newly identified RNA modifications. To address this challenge, we propose an innovative incremental learning framework that incorporates multiple incremental learning methods. Our experimental results confirm the efficacy of incremental learning strategies in addressing the RNA modification challenge. By uniquely targeting 10 RNA modification types in a class incremental setting, our framework exhibits superior performance. Notably, it can be extended to new category methylation predictions without the need for retraining with previous data, improving computational efficiency. Through the accumulation of knowledge, the model is able to evolve and continuously learn the differences across methylation, mitigating the problem of catastrophic forgetting during deep learning model training. Overall, our framework provides various alternatives to enhance the prediction of novel RNA modifications and illuminates the potential of incremental learning in tacking numerous genome data.}
}
@incollection{YADEN2023849,
title = {Reintroducing “development” into theories of the acquisition and growth of early literacy: developmental science approaches and the cultural-historical perspective of L. S. Vygotsky},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {849-865},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.07103-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305071037},
author = {David B. Yaden and Camille Martinez-Yaden},
keywords = {Microgenetic, Developmental science, Optimizing equilibration, Process-relational, Relational-developmental-system, Backward transition, Early writing, Overlapping waves theory, Prospective models, Retrospective models, Hyperbolic geometry},
abstract = {This chapter contrasts the differences between computational “retrospective” models of early writing achievement whose elements represent static states of being and “prospective” models based upon the principles of developmental science and a process-relational-developmental framework which characterizes early writing performances always in the process of “becoming.” The chapter highlights these differences using examples from Spanish-speaking and Chinese/English emergent bilinguals to illustrate the various patterns of writing development captured in a Piagetian/Vygotskian-inspired early writing assessment. The children's simultaneous display of multiple conceptualizations of the notational system in Spanish, English, and Chinese is interpreted as reflecting aspects of Siegler's “overlapping waves theory” and Piaget's “optimizing equilibration.”}
}
@article{CHEN2021105850,
title = {Coupled crash mechanics and biomechanics of aircraft structures and passengers},
journal = {Communications in Nonlinear Science and Numerical Simulation},
volume = {101},
pages = {105850},
year = {2021},
issn = {1007-5704},
doi = {https://doi.org/10.1016/j.cnsns.2021.105850},
url = {https://www.sciencedirect.com/science/article/pii/S1007570421001623},
author = {Goong Chen and Jing Yang and Alexey Sergeev and Mingwei Wang and Chunqiu Wei and Jean Yeh and Philip J. Morris and Noah J. Fournier and Yining Chen and Xingong Cheng and Donghui Yang and Shuhuang Xiang and Marlan O. Scully},
keywords = {Aircraft crash mechanics, Passenger biomechanics, Aircraft structural components andfixtures, Injury analysis, LS-DYNA modeling, Supercomputer simulations, Vibration},
abstract = {The DYCAST (Dynamic Crash Analysis of Structures) experiments that started at NASA Langley Research Center during the late 1970s have greatly influenced the methodology and thinking of aircraft crashworthiness and survivability studies, and was continued and refined at other aerospace establishments. Nevertheless, so far most of the existing work has emphasized the impact damage to the aircraft section. Issues related to potential passenger injuries have not been properly addressed in the literature, to the best of our knowledge. Here, we study the DYCAST problem integrally by treating and combining impact damage and passenger injuries altogether. We develop the biomechanics by way of modal analysis of passenger dummy motions coupled with the vibration of aircraft structures in order to understand their basic interactions. Two types of mechanical dummies are used in this study. Such a modal analysis can help identify basic injury types, but is valid only in the constructed models, linear regime. However, we are able to extend the linear elastic model to a nonlinear elastoplastic computational model by using the versatile software LS-DYNA as the platform. Computer simulations are carried out on the supercomputer clusters and the numerical results are rendered into video animations for visualization and analysis. One can see, for example, how the passenger-dummy interactive motions with the fuselage and fixtures and the potential injuries caused in the event of general aircraft crashes on a fractal domain.}
}
@incollection{NG202451,
title = {Chapter 4 - System modeling and mapping},
editor = {Kok Siew Ng and Elias Martinez Hernandez and Aki Yamaguchi},
booktitle = {A New Systems Thinking Approach to Sustainable Resource Management},
publisher = {Elsevier},
pages = {51-140},
year = {2024},
isbn = {978-0-323-99869-7},
doi = {https://doi.org/10.1016/B978-0-323-99869-7.00003-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323998697000036},
author = {Kok Siew Ng and Elias Martinez Hernandez and Aki Yamaguchi},
keywords = {Causal loop diagram, GIS, MFA, Resource availability, Sensitivity and uncertainty analyses, System dynamics},
abstract = {This chapter presents a series of representative techniques for system modeling and mapping, including resource availability analysis, material flow analysis, system dynamics, and sensitivity and uncertainty analyses. These are among the well-established computational modeling methods adopted widely in engineering, environmental, and social science disciplines. They are particularly useful in the context of resource and waste management, providing clearer visualization of the system and enabling reliable prediction of system behavior. Resource availability assessment provides a bird's eye view of the interdependencies among resources and the overall feasibility for a system to operate with the available resources. Material flow analysis facilitates a robust mapping of resource consumption, production, and losses, offering insights for identifying opportunities to improve resource efficiency and minimize waste. System dynamics enables us to understand the complex behavior of a system through exploring the interaction of different factors, serving as a forecasting tool for future scenarios. Sensitivity analysis determines the system's responsiveness to different input values, identifying the most influential inputs in the system's response. Uncertainty analysis quantifies variations and uncertainties in potential system responses due to variations in inputs.}
}
@article{JAIN2023119859,
title = {Optimized levy flight model for heart disease prediction using CNN framework in big data application},
journal = {Expert Systems with Applications},
volume = {223},
pages = {119859},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.119859},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423003603},
author = {Arushi Jain and Annavarapu {Chandra Sekhara Rao} and Praphula {Kumar Jain} and Yu-Chen Hu},
keywords = {Optimization, Heart disease prediction, Convolution neural networks, Big data, Swarm intelligence algorithm},
abstract = {Cardiac disease is one of the most complex diseases globally. It affects the lives of humans critically. It is essential for accurate and timely diagnosis to treat heart failure and prevent the disease. In most aspects, it was not so successful with the traditional method, which uses past medical history. Many existing models had several types of the loss function in traditional CNN can lead to misidentification of the model. To solve this problem, so many scholars have used the swarm intelligence algorithm, but most of these techniques are stuck in the local minima and suffer from premature convergence. In the proposed method, we build up the Levy Flight – Convolutional Neural Network (LV-CNN) depending on the diagnostic system using heart disease image data set for heart disease assessment. Initially, the input Big Data images are resized to reduce the computational complexity of the system. Then, those resized images are subject to the proposed LV-CNN model. Therefore, the LV approach is integrated with the Sunflower Optimization Algorithm (SFO) to reduce loss function occurring in the CNN architecture. Such a combination helps the SFO algorithm avoid trapping in local minima due to the random walk of the levy flight. The proposed algorithm will be simulated using the MATLAB tool and tested experimentally in terms of accuracy is 95.74%, specificity is 0.96%, the error rate is 0.35, and time consumption is 9.71 s. This comparative analysis revealed that the excellence of the proposed model.}
}
@article{ZHEN2024,
title = {A stochastic programming model for designing bus bridging services under metro disruptions},
journal = {Transportation Letters},
year = {2024},
issn = {1942-7867},
doi = {https://doi.org/10.1080/19427867.2024.2327811},
url = {https://www.sciencedirect.com/science/article/pii/S1942786724000146},
author = {Lu Zhen and Xueqin Du and Haolin Li and Zanyang Wu},
keywords = {Urban metro system, bus bridging, schedule, passenger assignment, stochastic programming, tabu search algorithm},
abstract = {ABSTRACT
With the growing reliance on urban metro networks, any accidental disruption can lead to rapid degradation and significant economic losses. Bus bridging services are common and efficient ways to minimize such adverse impacts. In this study, we investigate the problem of designing bus bridging services in response to unexpected metro disruptions, and propose a routing strategy with multiple bridging routes. In particular, to respond to uncertain factors such as passenger arrivals and bus travel times in the disruption environment, we develop a two-stage stochastic programming model for the collaborative optimization of bus bridging routes, schedules, and passenger assignments. To solve the computational challenges arising with the proposed model, a tailored tabu search algorithm is developed. Finally, several sets of numerical experiments are conducted and experimental results reveal that our proposed routing strategy can effectively improve the service level for the affected passengers during metro disruptions.}
}
@article{PANDEY2018141,
title = {Understanding the mechanics of creep deformation to develop a surrogate model for contact assessment in CANDU® fuel channels},
journal = {Nuclear Engineering and Design},
volume = {330},
pages = {141-156},
year = {2018},
issn = {0029-5493},
doi = {https://doi.org/10.1016/j.nucengdes.2018.01.032},
url = {https://www.sciencedirect.com/science/article/pii/S0029549318300323},
author = {M.D. Pandey and F.J. Tallavo and N.C. Christodoulou and B. Leitch and G.A. Bickel},
keywords = {Fuel channel, Creep deformation, Finite element method, Surrogate model, Pressure tube, Calandria tube, Contact assessment, Zirconium alloy, Probabilistic assessments},
abstract = {A key element of the fuel channel life cycle management in CANDU® reactors is to prevent contact between the pressure tube (PT) and the calandria tube (CT) and to avoid the development of hydride blisters that lead to delayed hydride cracking of the PT. The PT-CT contact is the result of in-reactor deformation due to irradiation induced creep of the fuel channel assembly, which in turn is affected by uncertainties associated with various parameters like material properties, dimensional changes in the channel and boundary conditions (e.g., end slopes) of the channel. To account for these uncertainties, probabilistic assessment methods are developed to evaluate the risk of PT-CT contact and demonstrate compliance with provisions of the CSA Standard N285.8. Currently, a simulation is based on probabilistic assessments in which input parameters to a finite element model (FEM) of creep deformation are randomly sampled from their respective distributions. A simulation model involves numerous repetitive solutions of the FEM model to determine the probability distribution of PT-CT gap and the time to contact. Since the creep deformation analysis using FEM is computationally involved, this brute force Monte Carlo simulation method is not an efficient way to carry out the probabilistic assessment of the reactor core. This paper proposes a new line of thinking for probabilistic assessments of PT-CT contact in fuel channels, which is based on replacing a full FEM model by a surrogate model of a much simpler analytical form. The surrogate model not only simplifies the creep deformation analysis, but also provides a more logical basis for probabilistic assessments. This paper presents an insightful analysis of creep deformation and shows that a simple surrogate model can be developed to predict the PT-CT gap as a linear function of two primary random variables, namely, a creep factor and end slopes. This simplified representation has a far reaching effect on the probabilistic assessment of fuel channels.}
}
@incollection{COPLIEN201425,
title = {Chapter 2 - The DCI Paradigm: Taking Object Orientation into the Architecture World},
editor = {Muhammad {Ali Babar} and Alan W. Brown and Ivan Mistrik},
booktitle = {Agile Software Architecture},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {25-62},
year = {2014},
isbn = {978-0-12-407772-0},
doi = {https://doi.org/10.1016/B978-0-12-407772-0.00002-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780124077720000022},
author = {James O. Coplien and Trygve Reenskaug},
keywords = {use case, mental models, postmodernism, restricted object orientation, full object orientation.},
abstract = {Abstract
We find surprisingly strong parallels in a playful comparison of the progression of thought in the architecture of the built world and its namesake in software. While some architectural progression in both fields owes to fashion, much more of it owes to learning—in both the field of design and collective human endeavor. We have been working on a paradigm called DCI (Data, Context, and Interaction) that places the human experiences of design and use of programs equally at center stage. It brings software design out of the technology-laced modern school of the 1980s into a postmodern era that places human experience at the center. DCI offers a vision of computers and people being mutually alive in the sense of Christopher Alexander’s great design. DCI opens a dialog contrasting metaphors of collective human reasoning and Kay’s vision of object computation, as well as a dialog between the schools of design in the built world and in software.}
}
@article{HEIRDSFIELD2004443,
title = {Factors affecting the process of proficient mental addition and subtraction: case studies of flexible and inflexible computers},
journal = {The Journal of Mathematical Behavior},
volume = {23},
number = {4},
pages = {443-463},
year = {2004},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2004.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0732312304000495},
author = {Ann M. Heirdsfield and Tom J. Cooper},
keywords = {Mental computation, Addition, Subtraction, Accuracy},
abstract = {The relationship between mental computation and number sense is complex: mental computation can facilitate number sense when students are encouraged to be flexible, but flexibility and number sense is neither sufficient nor necessary for accuracy in mental computation. It is possible for familiarity with a strategy to compensate for a lack of number sense and inefficient processes. This study reports on six case studies exploring Year 3 students’ procedures for and understanding of mental addition and subtraction, and understanding of number sense and other cognitive, metacognitive, and affective factors associated with mental computation. The case studies indicate that the mental computation process is composed of four stages in which cognitive, metacognitive and affective factors operate differently for flexible and inflexible computers. The authors propose a model in which the differences between computer types are seen in terms of the application of different knowledges in number facts, numeration, effect of operation on number, and beliefs and metacognition on strategy choice and strategy implementation.}
}
@article{FRENCH2023100030,
title = {Reflections on 50 years of MCDM: Issues and future research needs},
journal = {EURO Journal on Decision Processes},
volume = {11},
pages = {100030},
year = {2023},
issn = {2193-9438},
doi = {https://doi.org/10.1016/j.ejdp.2023.100030},
url = {https://www.sciencedirect.com/science/article/pii/S2193943823000031},
author = {Simon French},
keywords = {Behavioural decision studies, Bayesian analysis, Conflicting objectives, Cynefin, Multiple criteria decision-making (MCDM), Uncertainty},
abstract = {Modern discussions of multiple criteria decision-making extend back about half a century. I reflect on key developments, schools of thought and controversies that have taken place over the period, arguing that perhaps those of us in different schools focus too much on our differences and do not capitalise enough on what we share in common. Moreover, the differences between schools are indications of their respective weaknesses and can drive improvements in each. The discussion points to a number of issues and research needs that the community needs to address.}
}
@incollection{CHIB20013569,
title = {Chapter 57 - Markov Chain Monte Carlo Methods: Computation and Inference},
editor = {James J. Heckman and Edward Leamer},
series = {Handbook of Econometrics},
publisher = {Elsevier},
volume = {5},
pages = {3569-3649},
year = {2001},
issn = {1573-4412},
doi = {https://doi.org/10.1016/S1573-4412(01)05010-3},
url = {https://www.sciencedirect.com/science/article/pii/S1573441201050103},
author = {Siddhartha Chib},
keywords = {:, Cl, C4},
abstract = {This chapter reviews the recent developments in Markov chain Monte Carlo simulation methods. These methods, which are concerned with the simulation of high dimensional probability distributions, have gained enormous prominence and revolutionized Bayesian statistics. The chapter, provides background on the relevant Markov chain theory and provides detailed information on the theory and practice of Markov chain sampling based on the Metropolis–Hastings and Gibbs sampling algorithms. Convergence diagnostics and strategies for implementation are also discussed. A number of examples drawn from Bayesian statistics are used to illustrate the ideas. The chapter also covers in detail the application of MCMC methods to the problems of prediction and model choice.}
}
@incollection{STANDEN2024,
title = {Studying the locomotory habits in fish reveals six tenets of effective science},
series = {Fish Physiology},
publisher = {Academic Press},
year = {2024},
issn = {1546-5098},
doi = {https://doi.org/10.1016/bs.fp.2024.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S1546509824000050},
author = {Emily M. Standen},
keywords = {Fish swimming modes, Fins, History of fish swimming, Scientific method, Data analysis, Science communication, Tenets of effective science, Data value, Data scale},
abstract = {Emily M. Standen discusses how she has been impacted by C.C. Lindsey's chapter on “Form, Function and Locomotory Habits in Fish,” in Fish Physiology, Volume 7 published in 1978. Fish locomotion has been a widely studied field. Physicists, mathematicians, engineers and biologists have added to our understanding of how fish move through water, through different substrates and over land. Since the 1978 publication of C.C. Lindsey's chapter on Form, function and locomotory habits in fish, there have been remarkable advancements in the technology and tools we use to measure, quantify, calculate and model fish motion. This article argues that although new technologies are adding much higher resolution data on different aspects of fish swimming, they support the hypotheses and understanding that was already laid out by early experimentalists using simple and elegant techniques. As a celebration of the enduring impact C.C. Lindsey's 1978 chapter has had on the field of fish swimming, this article reflects upon how the history of fish swimming, and Lindsey's remarkable breadth and clarity in reviewing it, clearly shows the importance of how we approach our science, how we reflect back on the body of literature that make up whatever field we work in, and what we can do to ensure we move knowledge forward in the most beneficial way possible. Despite huge leaps forward in the technologies used to study and compute fish locomotor habits, Lindsey's chapter remains a thorough and impressive summary of the work of so many early scientists, that still guide and influence the questions that surround fish locomotion today.}
}
@article{HORVATH2015161,
title = {Ubiquitous computer aided design: A broken promise or a Sleeping Beauty?},
journal = {Computer-Aided Design},
volume = {59},
pages = {161-175},
year = {2015},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2014.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0010448514002358},
author = {Imre Horváth and Regine W. Vroom},
keywords = {Ubiquitous computing, Computer aided design, Ubiquitous design enablers, Competing technology exploitation, Ubiquitous CAD applications},
abstract = {As a novel computational approach, ubiquitous computing was emerging at the beginning of the 1980s and has reached a rather mature level by now. It assumes that computing can be available anywhere, anytime and in any context due to technological developments, social demands and calm implementations. Over the years, the opportunities of this computing paradigm have been explored and the benefits have been exploited successfully in many application fields. This survey paper addresses ubiquitous computing from the perspective of enabling computer aided design. The specific objectives of the reported survey are to: (i) give an overall account of the current status of ubiquitous computing and technologies, (ii) cast light on how ubiquitous computing has influenced the development of CAD systems, tools, and methods, and (iii) critically investigate future development opportunities of ubiquitous computing enabled computer aided design. First, the paper discusses the principles and typical technologies of ubiquitous computing. Then, the development and spectrum of the so-called standard computer aided design tasks are analyzed from a computational point of view. Afterwards, the already implemented design enabling functionalities are discussed and some additional functional possibilities are considered. The literature provides evidence that ubiquitous computing has not managed to revolutionize the methodologies or the systems of computer aided design so far, though many researchers intensively studied the affordances and the application possibilities of ubiquitous technologies. One reason is that ubiquitous computing technologies had in the last two decades to compete with other kinds of computational technologies, such as high-capacity computing, high-speed networking, immersive virtual reality, knowledge ontologies, smart software agents, mobile communication, etc., which had a much stronger influence on the development of computer aided design methods and systems. In combination with the rather conservative and conventionalist industrial practice of CAD system development and application, this may explain why the ubiquitous computing revolution remained weak in computer aided design. The literature clearly indicates that application of ubiquitous technologies did not lead to radically new functionalities that could have been exploited by the concerned industries. Consequently, it seems to be possible that computer aided design simply steps over the paradigm of ubiquitous computing and expects new functionalities from the emerging new computing paradigms, such as brain–computer interfacing, cyber–physical computing, biological computing, or quantum computing.}
}
@article{UCAR2017249,
title = {Managing disruptions in the multi-depot vehicle scheduling problem},
journal = {Transportation Research Part B: Methodological},
volume = {105},
pages = {249-269},
year = {2017},
issn = {0191-2615},
doi = {https://doi.org/10.1016/j.trb.2017.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0191261516305495},
author = {Ezgi Uçar and Ş. {İlker Birbil} and İbrahim Muter},
keywords = {Multi-depot vehicle scheduling, Robust planning, Column-and-row generation},
abstract = {We consider two types of disruptions arising in the multi-depot vehicle scheduling; the delays and the extra trips. These disruptions may or may not occur during operations, and hence they need to be indirectly incorporated into the planned schedule by anticipating their likely occurrence times. We present a unique recovery method to handle these potential disruptions. Our method is based on partially swapping two planned routes in such a way that the effect on the planned schedule is minimal, if these disruptions are actually realized. The mathematical programming model for the multi-depot vehicle scheduling problem, which incorporates these robustness considerations, possesses a special structure. This special structure causes the conventional column generation method fall short as the resulting problem grows also row-wise when columns are generated. We design an exact simultaneous column-and-row generation algorithm to find a valid lower-bound. The novel aspect of this algorithm is the pricing subproblem, which generates pairs of routes that form recovery solutions. Compromising on exactness, we modify this algorithm in order to enable it to solve practical-sized instances efficiently. This heuristic algorithm is shown to provide very tight bounds on the randomly generated instances in a short computation time.}
}
@article{BUAH2021103269,
title = {Augmenting the communication and engagement toolkit for CO2 capture and storage projects},
journal = {International Journal of Greenhouse Gas Control},
volume = {107},
pages = {103269},
year = {2021},
issn = {1750-5836},
doi = {https://doi.org/10.1016/j.ijggc.2021.103269},
url = {https://www.sciencedirect.com/science/article/pii/S1750583621000219},
author = {Eric Buah and Lassi Linnanen and Huapeng Wu},
keywords = {Artificial intelligence, CO capture and storage, CCS communication and engagement, CCS toolkit, CCS SWOT, Deep neural network algorithm, Fuzzy logic, Fuzzy deep learning},
abstract = {This paper revisits the Communication and Engagement Toolkit for CO2 Capture and Storage (CCS) projects proposed by Ashworth and colleagues in collaboration with the Global CCS Institute. The paper proposes a new method for understanding the social context where CCS will be deployed based on the toolkit. In practice, the proposed method can be used to harness social data collected on the CCS project. The outcome of this application is a development of a predictive tool for gaining insight into the future, to guide strategic decisions that may enhance deployment. Methodologically, the proposed predictive tool is an artificial intelligence (AI) tool. It uses fuzzy deep neural network to develop computational ability to reason about the social behavior. The hybridization of fuzzy logic and deep neural network algorithms make the predictive tool an explainable AI system. It means that the prediction of the algorithm is interpretable using fuzzy logical rules. The practical feasibility of the proposed system has been demonstrated using an experimental sample of 198 volunteers. Their perceptions, emotions and sentiments were tested using a standard questionnaire from the literature, on a hypothetical CCS project based on 26 predictors. The generalizability of the algorithm to predict future reactions was tested on, 84 out-of-sample respondents. In the simulation experiment, we observed an approximately 90 % performance. This performance was measured when the algorithm's predictions were compared to the self- reported reactions of the out of sample subjects. The implication of the proposed tool to enhance the predictive power of the conventional CCS Communication and Engagement tool is discussed © 2020 xx. Hosting by Elsevier B.V. All rights reserved.}
}
@article{CALDEIRA2025102657,
title = {Model compression techniques in biometrics applications: A survey},
journal = {Information Fusion},
volume = {114},
pages = {102657},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102657},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524004354},
author = {Eduarda Caldeira and Pedro C. Neto and Marco Huber and Naser Damer and Ana F. Sequeira},
keywords = {Compression, Knowledge distillation, Quantization, Pruning, Biometrics, Bias},
abstract = {The development of deep learning algorithms has extensively empowered humanity’s task automatization capacity. However, the huge improvement in the performance of these models is highly correlated with their increasing level of complexity, limiting their usefulness in human-oriented applications, which are usually deployed in resource-constrained devices. This led to the development of compression techniques that drastically reduce the computational and memory costs of deep learning models without significant performance degradation. These compressed models are especially essential when implementing multi-model fusion solutions where multiple models are required to operate simultaneously. This paper aims to systematize the current literature on this topic by presenting a comprehensive survey of model compression techniques in biometrics applications, namely quantization, knowledge distillation and pruning. We conduct a critical analysis of the comparative value of these techniques, focusing on their advantages and disadvantages and presenting suggestions for future work directions that can potentially improve the current methods. Additionally, we discuss and analyze the link between model bias and model compression, highlighting the need to direct compression research toward model fairness in future works.}
}
@article{SAMSONOVICH2015235,
title = {Cognitive Processes in Preparation for Problem Solving},
journal = {Procedia Computer Science},
volume = {71},
pages = {235-247},
year = {2015},
note = {6th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2015, 6-8 November Lyon, France},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.218},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915036790},
author = {Alexei V. Samsonovich and Anastasia Kitsantas and Ellen O’Brien and Kenneth A. {De Jong}},
keywords = {self-regulation, planning, metacognition, intelligent tutoring systems},
abstract = {The aim of this study was to examine the role of a software tool in diagnosing student's thinking during problem solving in mathematics with 41 college students. Students were asked to select relevant steps, facts and strategies represented on the screen and connect them by arrows, indicating their plan of solution. Only after the diagram was completed, students were allowed to solve the problem. The findings are: (i) forward chaining is significantly more predominant, and backward chaining is significantly less frequent, compared to other possibilities or arrow entering. This result is unexpected, because classical planning methods produce backward chaining in this task. (ii) Students scoring in the middle are more likely to enter convergent pairs of arrows compared to students who scored low or high. This finding enables diagnosing student problem solving. Both findings imply constraints on selection of cognitive architectures used for modeling student problem solving.}
}
@article{DIRKSEN2022102994,
title = {From agent to action: The use of ethnographic social simulation for crime research},
journal = {Futures},
volume = {142},
pages = {102994},
year = {2022},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2022.102994},
url = {https://www.sciencedirect.com/science/article/pii/S0016328722000945},
author = {Vanessa Dirksen and Martin Neumann and Ulf Lotzmann},
keywords = {Agent-based modelling, Complementarity, Computational social science, Simulation, Ethnography, Policing},
abstract = {This paper proposes a methodology for grounding agent-based social simulation in ethnographic data, using the example of crime research. The application of computational tools in crime research typically entails a removal of the “intelligible frame” of criminal behaviour and, hence, of meaningful evidence. Ethnography is a microscopic research tradition geared towards the preservation of contextualized meaning deemed essential for the exploration of the variety of prospective alternative scenarios and, hence, of plausible futures. On the basis of exemplary empirical material from a qualitative study on the transit trade of cocaine in the Netherlands, this paper looks into the complementarity and potential integration of the research traditions of ethnography and agent-based modelling. That is to say, it explores the compatibility of the formal languages of both these domains and the mutual benefit of “stitching together” these at first sight very different methods. The ethnographic approach to social simulation specifies the what-if relations of traditional/conventional ABM modelling into condition-action sequences. As we contend, it is exactly this more microscopic level of condition-action sequences that is needed to facilitate ”thick description” and, in turn, enable the grounding of ABM in meaningful evidence.}
}
@article{CAIRNS201964,
title = {Future design of accessibility in games: A design vocabulary},
journal = {International Journal of Human-Computer Studies},
volume = {131},
pages = {64-71},
year = {2019},
note = {50 years of the International Journal of Human-Computer Studies. Reflections on the past, present and future of human-centred technologies},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2019.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S1071581919300801},
author = {Paul Cairns and Christopher Power and Mark Barlet and Greg Haynes},
keywords = {Digital games, Accessibility, Guidelines, Design vocabulary, Accessible player experiences},
abstract = {Games represent one of the most significant cultural artefacts of this century. They are a massive force in economies around the world and are enjoyed by millions of players worldwide. With their cultural significance firmly in place, it is important to ensure that all people can participate in and play games in order to feel included in our wider society. For people with disabilities, games in particular provide a cultural outlet where they can be included with everyone else, and enabled to do things on an even footing with their non-disabled peers. However, this only happens if we create the necessary design environments that provide inclusive opportunities to game alongside the rest of the player base. Guidelines have been successful in raising awareness of accessibility in games and still function well for evaluating finished games. However, they are not the generative design thinking tools that developers need. Further in being divided to address specific disabilities, they are not capturing the diversity of needs of players with disabilities and the personalised and idiosyncratic adaptations that they make in order to play. We therefore propose developing a vocabulary and language of game accessibility which is no longer about whether someone can perceive or operate an interactive technology, but instead as to whether they can have the experience they want to have. We propose the structure for such a vocabulary showing that it needs to distinguish between access to controls, enablement to meet the challenges of the game and the player experience itself. We show how the intermediate-level knowledge embodied in guidelines can be reformulated in this way to be more generative and so support designers to develop games that deliver accessible player experiences.}
}
@article{DERBEL2014731,
title = {Distributed localized bi-objective search},
journal = {European Journal of Operational Research},
volume = {239},
number = {3},
pages = {731-743},
year = {2014},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2014.05.040},
url = {https://www.sciencedirect.com/science/article/pii/S0377221714004639},
author = {Bilel Derbel and Jérémie Humeau and Arnaud Liefooghe and Sébastien Verel},
keywords = {Multiple objective programming, Combinatorial optimization, Parallel and distributed computing, Evolutionary computation},
abstract = {We propose a new distributed heuristic for approximating the Pareto set of bi-objective optimization problems. Our approach is at the crossroads of parallel cooperative computation, objective space decomposition, and adaptive search. Given a number of computing nodes, we self-coordinate them locally, in order to cooperatively search different regions of the Pareto front. This offers a trade-off between a fully independent approach, where each node would operate independently of the others, and a fully centralized approach, where a global knowledge of the entire population is required at every step. More specifically, the population of solutions is structured and mapped into computing nodes. As local information, every node uses only the positions of its neighbors in the objective space and evolves its local solution based on what we term a ‘localized fitness function’. This has the effect of making the distributed search evolve, over all nodes, to a high quality approximation set, with minimum communications. We deploy our distributed algorithm using a computer cluster of hundreds of cores and study its properties and performance on ρMNK-landscapes. Through extensive large-scale experiments, our approach is shown to be very effective in terms of approximation quality, computational time and scalability.}
}
@article{RONI2022100796,
title = {Integrated water-power system resiliency quantification, challenge and opportunity},
journal = {Energy Strategy Reviews},
volume = {39},
pages = {100796},
year = {2022},
issn = {2211-467X},
doi = {https://doi.org/10.1016/j.esr.2021.100796},
url = {https://www.sciencedirect.com/science/article/pii/S2211467X21001796},
author = {Mohammad S. Roni and Thomas Mosier and Tzvi D. Feinberg and Timothy McJunkin and Ange-Lionel Toba and Liam D. Boire and Luis Rodriguez-Garcia and Majid Majidi and Masood Parvania},
keywords = {Resiliency, Irrigation, Integrated water-power system, Optimization},
abstract = {Resiliency has been studied in the power and water systems separately. Often the resiliency study is not so comprehensive as to understand interdependent, integrated water and power systems. This research outlines the relevant factors necessary to understand and advance quantification of such integrated systems. It also presents a review of integrated water-power systems resiliency. Based on literature survey and identification of challenges, the authors present quantification and computational steps needed to understand integrated water-power systems resiliency. A conceptual framework is proposed to quantify integrated water-power system resiliency. Finally, the authors presented an opportunity for improved water and power system resilience.}
}
@article{XU2024102292,
title = {Hierarchical spatio-temporal graph convolutional neural networks for traffic data imputation},
journal = {Information Fusion},
volume = {106},
pages = {102292},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102292},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524000708},
author = {Dongwei Xu and Hang Peng and Yufu Tang and Haifeng Guo},
keywords = {Traffic data imputation, Hierarchical representation, Graph convolution network, Spatio-temporal features},
abstract = {The quality of traffic services depends on the accuracy and completeness of the collected traffic data. However,the existing traffic data imputation methods usually only rely on the predefined road network structure to capture the spatio-temporal features and only consider the imputation effect from a single perspective, which are very limited for imputation of different missing patterns of road traffic data. In this paper, we propose a novel deep learning framework called Hierarchical Spatio-temporal Graph Convolutional Neural Networks(HSTGCN) to impute traffic data,through the macro layer and the road layer. The model constructs macro graph of the road network based on the data temporal correlation clustering, which can mine the temporal dependencies of road traffic data from a hierarchical perspective. Besides, a temporal attention mechanism and adaptive adjacency matrix are introduced in the road layer to better extract the spatio-temporal information of the road traffic data. Finally, we use graph convolution neural networks to learn the spatio-temporal feature representations of the road layer and macro layer, which are then fused to achieve data imputation. To illustrate the efficient performance of the model, experiments are conducted on traffic data collected from California and Seattle. The proposed model performs better than the comparison model for traffic data imputation.}
}
@article{BERGER2024537,
title = {Enmeshed with the digital: satellite navigation and the phenomenology of drivers’ spaces},
journal = {Mobilities},
volume = {19},
number = {3},
pages = {537-555},
year = {2024},
issn = {1745-0101},
doi = {https://doi.org/10.1080/17450101.2023.2285304},
url = {https://www.sciencedirect.com/science/article/pii/S1745010123001431},
author = {Viktor Berger},
keywords = {Satellite navigation, GPS, driving, automobilities, Merleau-Ponty, hybrid spaces, mesh, mediatization},
abstract = {This paper aims to develop a theoretical interpretation of how satellite navigation transforms drivers’ experience of automotive spaces. The use of satellite navigation has, so far, been predominantly studied from a cognitivist perspective based on the computer model of cognition and the theory of spatial disengagement. Experimental studies have concluded that over-reliance on digital navigation tools diminishes spatial orientation and spatial memory. According to the dominant interpretation, satellite navigation causes disengagement from space. After addressing these approaches, the paper introduces an embodied perspective of satellite navigation. This is accomplished by applying the phenomenology of perception of Maurice Merleau-Ponty, whose notions, such as perception, body schema, motor habit, and virtual body, illuminate otherwise undertheorized dimensions of drivers’ spaces. By using digital tools for wayfinding, drivers’ body schema, virtual body, and perception of space are modified, thereby enabling an engagement with convoluted ‘mesh spaces.’ This new term is integral to the interpretation of drivers’ spaces, as well as being distinct from that of ‘hybrid space,’ although both aim to conceptualize spaces, including physical objects and their visual representations. Conclusions will be drawn against the broader context of the mediatization of everyday life.}
}
@article{ADAMOVIC2024100604,
title = {Streamlined approach to 2nd/3rd graders learning basic programming concepts},
journal = {Entertainment Computing},
volume = {48},
pages = {100604},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2023.100604},
url = {https://www.sciencedirect.com/science/article/pii/S1875952123000599},
author = {Milan Đ. Adamović and Dragan V. Ivetić},
keywords = {Video games, Edutainment, Programming, School},
abstract = {There is a growing need to teach schoolchildren programming at an increasingly younger age. The goal of this study is to determine if it is possible to teach schoolchildren basic programming concepts in a streamlined manner. In order to present the new knowledge in a way schoolchildren could understand easily, analogies between basic programming concepts and traffic were used. A simple video game was developed with this in mind and an effort was made to avoid design pitfalls commonly found in edutainment titles. The study involved 112 schoolchildren ages 7 to 9. Test group and control group were given a pre-test, a re-test and a post-test. The re-test and the post-test respectively showed 16% and 7% score difference in favor of the test group. Focusing on questions featuring content analogous to basic programming concepts showed 36% and 20% difference in scores.}
}
@incollection{SULLIVAN2008XIX,
title = {Preface},
series = {Methods in Cell Biology},
publisher = {Academic Press},
volume = {85},
pages = {XIX-XX},
year = {2008},
booktitle = {Fluorescent Proteins},
issn = {0091-679X},
doi = {https://doi.org/10.1016/S0091-679X(08)85026-1},
url = {https://www.sciencedirect.com/science/article/pii/S0091679X08850261},
author = {Kevin F. Sullivan},
abstract = {Publisher Summary
The chapter highlights the content of the book “Fluorescent proteins 2nd edition.” The book discusses the rich palette of autofluorescent proteins that now spans the spectral range from blue to deep red. From presentation of the ideas and concepts that provide the foundation for methods through discussing the factual knowledge and sources required to design experiments to the detailed exposition of actual experimental protocols, the chapters in this book combine to provide an essential tool for thinking about using genetically encoded fluorescent molecules. The experimental goals and systems discussed in the chapter present range from biophysical interrogation of individual molecules to the analysis of the behavior of cell populations in whole animals. The range of autofluorescent proteins available is presented and discussed in several chapters of the book. The construction of FP fusions is also discussed in several contexts, from developing biosensors and optimizing FRET to constructing intramolecular fusions and hemi-FP chimeras used for detecting protien–protein interactions.}
}
@incollection{BRANDT2006136,
title = {Grammatology},
editor = {Keith Brown},
booktitle = {Encyclopedia of Language & Linguistics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {136-140},
year = {2006},
isbn = {978-0-08-044854-1},
doi = {https://doi.org/10.1016/B0-08-044854-2/00355-2},
url = {https://www.sciencedirect.com/science/article/pii/B0080448542003552},
author = {P.A. Brandt},
keywords = {Barthes, deconstruction, Derrida, differance, grammatology, Lacan, logocentrism, McLuhan, phenomenology, Saussure, sign, symbolization, writing},
abstract = {Grammatology, the study of writing systems and processes, is presented through the French philosopher Jacques Derrida's critique of the concept of meaning, following from semiology and the notion of sign as such in Western thinking: ‘logocentrism.’ This critical view of writing was the starting point of deconstruction in literary criticism. It is argued in this article that written texts are different from oral utterances as concerns their enunciation, or speaker role, and that this circumstance deeply affects their interpretation. The difference could explain the sacralization of texts and text volumes, their cultural status and importance. The computer age is about to transform the relation between oral and written communication, so that we now can have written dialogue in addition to oral dialogue.}
}
@article{HAMMOND19951593,
title = {Implementation and performance issues of a massively parallel atmospheric model},
journal = {Parallel Computing},
volume = {21},
number = {10},
pages = {1593-1619},
year = {1995},
note = {Climate and weather modeling},
issn = {0167-8191},
doi = {https://doi.org/10.1016/0167-8191(95)01017-9},
url = {https://www.sciencedirect.com/science/article/pii/0167819195010179},
author = {Steven W. Hammond and Richard D. Loft and John M. Dennis and Richard K. Sato},
keywords = {Atmospheric general circulation modeling, Climate modeling, Data parallelism, Spectral transform, Semi-Lagrangian transport},
abstract = {We present implementation and performance issues of a data parallel version of the National Center for Atmospheric Research (NCAR) Community Climate Model (CCM2). We describe automatic conversion tools used to aid in converting a production code written for a traditional vector architecture to data parallel code suitable for the Thinking Machines Corporation CM-5. Also, we describe the 3-D transposition method used to parallelize the spherical harmonic transforms in CCM2. This method employs dynamic data mapping techniques to improve data locality and parallel efficiency of these computations. We present performance data for the 3-D transposition method on the CM-5 for machine size up to 512 processors. We conclude that the parallel performance of the 3-D transposition method is adversely affected on the CM-5 by short vector lengths and array padding. We also find that the CM-5 spherical harmonic transforms spend about 70% of their execution time in communication. We detail a transposition-based data parallel implementation of the semi-Lagrangian Transport (SLT) algorithm used in CCM2. We analyze two approaches to parallelizing the SLT, called the departure point and arrival point based methods. We develop a performance model for choosing between these methods. We present SLT performance data which shows that the localized horizontal interpolation in the SLT takes 70% of the time, while the data remapping itself only require approximately 16%. We discuss the importance of scalable I/O to CCM2, and present the I/O rates measured on the CM-5. We compare the performance of the data parallel version of CCM2 on a 32-processor CM-5 with the optimized vector code running on a single processor Cray Y-MP. We show that the CM-5 code is 75% faster. We also give the overall performance of CCM2 running at higher resolutions on different numbers of CM-5 processors. We conclude by discussing the significance of these results and their implications for data parallel climate models.}
}
@article{BELLA2023123268,
title = {Vibrationally resolved deep–red circularly polarised luminescence spectra of C70 derivative through Gaussian curvature analysis of ground and excited states},
journal = {Journal of Molecular Liquids},
volume = {391},
pages = {123268},
year = {2023},
issn = {0167-7322},
doi = {https://doi.org/10.1016/j.molliq.2023.123268},
url = {https://www.sciencedirect.com/science/article/pii/S0167732223020743},
author = {Giovanni Bella and Giuseppe Bruno and Antonio Santoro},
keywords = {Fullerene, Chirality, Curvature, Vibronic, Circularly polarized luminescence},
abstract = {Over the last years, the interaction of fullerene with circularly polarized light has attracted growing attention for potential electronic and optical applications. However, in literature there is only one example of fullerene derivative capable of emitting circularly polarized light, showing an active circularly polarized luminescence (CPL) signal in the deep-red visible region. This unique luminophore offered us the possibility to study the connection between the topological features of C70 spheroid and its chiral emission properties. In light of these considerations, we proposed a theoretical protocol that combines three different step: (1) The Ball Pivoting Algorithm for C70 surface reconstruction. (2) The discrete gaussian curvature analysis in the ground (S0) and excited states (S1). (3) The computation of the vibrationally-resolved CPL spectrum. The first step allowed us to extract useful information that linked the topological shape of C70 to the sp2 carbon network chemistry. The DFT benchmark in the second step guided us in grasping the best functional for the C70 curvature simulation in the ground state, spotlighting how B97D3 excellently succeed for this task. The curvature investigation in the first excited state showed that (for all the twenty exchange–correlation functional tested) the C70 fragment is more curved in S1 than in S0. The final step collected the topological information from the previous sections to provide a detailed overview of the theoretical factors (such as the quantum formalism, the potential energy surface description and the transition dipole moment approximation) impacting on the C70 vibrationally resolved CPL spectrum. We found that the adiabatic hessian model coupled with the Franck-Condon Herzberg-Teller approximation computed at PW6B95D3/6-311G(d,p) level provides excellent results in emulating the band-shape and position of the experimental CPL spectrum.}
}
@article{PLANT20173335,
title = {Can a systems approach produce a better understanding of mood disorders?},
journal = {Biochimica et Biophysica Acta (BBA) - General Subjects},
volume = {1861},
number = {1, Part A},
pages = {3335-3344},
year = {2017},
issn = {0304-4165},
doi = {https://doi.org/10.1016/j.bbagen.2016.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S0304416516303051},
author = {Nick Plant},
keywords = {Affective disorder, Bipolar disorder, Computational biology, Drug development, Systems biology},
abstract = {Background
One in twenty-five people suffer from a mood disorder. Current treatments are sub-optimal with poor patient response and uncertain modes-of-action. There is thus a need to better understand underlying mechanisms that determine mood, and how these go wrong in affective disorders. Systems biology approaches have yielded important biological discoveries for other complex diseases such as cancer, and their potential in affective disorders will be reviewed.
Scope of review
This review will provide a general background to affective disorders, plus an outline of experimental and computational systems biology. The current application of these approaches in understanding affective disorders will be considered, and future recommendations made.
Major conclusions
Experimental systems biology has been applied to the study of affective disorders, especially at the genome and transcriptomic levels. However, data generation has been slowed by a lack of human tissue or suitable animal models. At present, computational systems biology has only be applied to understanding affective disorders on a few occasions. These studies provide sufficient novel biological insight to motivate further use of computational biology in this field.
General significance
In common with many complex diseases much time and money has been spent on the generation of large-scale experimental datasets. The next step is to use the emerging computational approaches, predominantly developed in the field of oncology, to leverage the most biological insight from these datasets. This will lead to the critical breakthroughs required for more effective diagnosis, stratification and treatment of affective disorders.}
}
@article{RUBIN2023104955,
title = {Cartography of the multiple formal systems of molecular autopoiesis: from the biology of cognition and enaction to anticipation and active inference},
journal = {Biosystems},
volume = {230},
pages = {104955},
year = {2023},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2023.104955},
url = {https://www.sciencedirect.com/science/article/pii/S0303264723001302},
author = {Sergio Rubin},
keywords = {Self-fabrication, Operational closure, Closure to efficient causation, Calculus of self-reference, Non-algorithmic, Enaction, Final cause},
abstract = {A rich literature has grown up over the years that bears with autopoiesis, which tends to assume that it is a model, a theory, a principle, a definition of life, a property, refers to self-organization or even to hastily conclude that it is hylomorphic, hylozoist, in need of reformulation or to be overcome, making its status even more unclear. Maturana insists that autopoiesis is none of these and rather it is the causal organization of living systems as natural systems (NS) such that when it stops, they die. He calls this molecular autopoiesis (MA), which comprises two domains of existence: that of the self-producing organization (self-fabrication) and that of the structural coupling/enaction (cognition). Like all-NS in the universe, MA is amenable to be defined in theoretical terms, i.e. encoded in mathematical models and/or formal systems (FS). Framing the multiple formal systems of autopoiesis (FSA) into the Rosen's modeling relation (a process of bringing into equivalence the causality of NS and the inferential rules of FS), allows a classification of FSA into analytical categories, most importantly Turing machine (algorithmic) vs non-Turing machine (non-algorithmic) based, and FSA with a purely reactive mathematical image as cybernetic systems, i.e. feedbacks based, or conversely, as anticipatory systems making active inferences. It is thus the intent of the present work to advance the precision with which different FS may be observed to comply (preserve correspondence) with MA in its worldly state as a NS. The modeling relation between MA and the range of FS proposed as potentially illuminating their processes forecloses the applicability of Turing-based algorithmic computational models. This outcome indicates that MA, as modelled through Varela's calculus of self-reference or more especially through Rosen's (M,R)-system, is essentially anticipatory without violating structural determinism nor causality whatsoever, hence enaction may involve it. This quality may capture a fundamentally different mode of being in living systems as opposed to mechanical-computational systems. Implications in different fields of biology from the origin of life to planetary biology as well as in cognitive science and artificial intelligence are of interest.}
}
@incollection{NIE20181939,
title = {Land use modeling and optimization based on food-energy-water nexus: a case study on crop-livestock systems},
editor = {Mario R. Eden and Marianthi G. Ierapetritou and Gavin P. Towler},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {44},
pages = {1939-1944},
year = {2018},
booktitle = {13th International Symposium on Process Systems Engineering (PSE 2018)},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-64241-7.50318-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780444642417503189},
author = {Yaling Nie and Styliani Avraamidou and Jie Li and Xin Xiao and Efstratios N. Pistikopoulos},
keywords = {land use, nexus, data-driven modeling, global optimization},
abstract = {Efficient land use in agricultural systems is a complicated decision-making problem with resource competitions and conflicting objectives. Systematic thinking based on food-energy-water (FEW) nexus is a necessity for modeling and optimization of the systems. However, challenges arise in making decisions while encountering conflicting objectives, limited data and coupling components. To address these challenges, we developed a global optimization-based land allocation framework, which provides an adaptive data-driven modeling method based on limited realistic data to predict yields for production components, a FEW index to help solve the multi-objective optimization problem and carry out assessments. Computational results indicate that the framework can provide valuable production models and a comprehensive FEW index to select strategies for optimal land allocation and limit stresses in the FEW nexus.}
}
@article{VERDUZCO2022103189,
title = {CALRECOD — A software for Computed Aided Learning of REinforced COncrete structural Design},
journal = {Advances in Engineering Software},
volume = {172},
pages = {103189},
year = {2022},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2022.103189},
url = {https://www.sciencedirect.com/science/article/pii/S0965997822000965},
author = {Luis Fernando Verduzco and Jaime Horta and Miguel A. Pérez Lara y Hernández and Juan Bosco Hernández},
keywords = {CALRECOD, Reinforced concrete structures, High education, Computed aided learning, Optimization methods},
abstract = {It is presented the development and implementation of a new computed aided learning MatLab Toolbox for the design of reinforced concrete structures named as CALRECOD for their abbreviation Computer Aided Learning of Reinforced Concrete Design. Such development emerges as the result of a series of research works in the Autonomous University of Queretaro with the main purpose of improving the way in which the design of reinforced concrete structures is taught in high education institutions. CALRECOD uses optimization methods and algorithms to aid students in their design interaction learning so that they are able to compare their own designs and what commercial software delivers with optimal ones given certain load conditions on the elements or structures. The software consists almost entirely of MatLab functions (.m files) and the ACI 318-19 code is taken as their main design reference to make it internationally useful, although in some cases the Mexican code NTC-17 specifications are used. Besides MatLab functions, the software consists as well of ANSYS SpaceClaim script functions (.scscript files) as an additional tool for the aid in the visualization of design results in a 3D space in the software ANSYS SpaceClaim. CALRECOD has proven to be versatile, flexible and of easy use with a huge potential to increase learning outcomes for students in high education programs related with the design of reinforced concrete structures as well as to enhance the creation of efficient interactive environments for researchers and academics focused on the development of new design and analysis methods for such structures. With their optimization design functions, a solid comparison platform of designs’ performance could be laid out, and with its extended function design packages for structural systems, reinforced concrete design courses could be enhanced in a great deal regarding their program content’s scope. The software can be found at: https://github.com/calrecod/CALRECOD.}
}
@article{BAMMER2008875,
title = {Enhancing research collaborations: Three key management challenges},
journal = {Research Policy},
volume = {37},
number = {5},
pages = {875-887},
year = {2008},
issn = {0048-7333},
doi = {https://doi.org/10.1016/j.respol.2008.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0048733308000528},
author = {Gabriele Bammer},
keywords = {Collaboration, Integration, Boundary, Authorization, Evaluation},
abstract = {This conceptual paper explores three areas of research collaboration: (a) effectively harnessing differences, (b) setting defensible boundaries and (c) gaining legitimate authorization. The focus is on their potential lessons for individuals leading and managing research collaborations, evaluation of research partnerships and areas for further investigation. Examples from three partnerships – building the atomic bomb, the Human Genome Project and the World Commission on Dams – are used to highlight key elements of the ideas presented. The paper provides a framework for systematically thinking about integration of different perspectives and other elements essential to any particular collaboration. It also sketches out ideas for (1) managing differences which may destroy partnerships, (2) deciding what the collaboration should encompass, (3) understanding and accommodating forces which may distort what the collaboration is able to achieve, and (4) enlisting necessary supporters while preserving research independence.}
}
@article{TERZIYAN20242540,
title = {Can ChatGPT Challenge the Scientific Impact of Published Research, Particularly in the Context of Industry 4.0 and Smart Manufacturing?},
journal = {Procedia Computer Science},
volume = {232},
pages = {2540-2550},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.02.072},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924002497},
author = {Vagan Terziyan and Olena Kaikova and Mariia Golovianko and Oleksandra Vitko},
keywords = {Artificial Intelligence, ChatGPT, Industry 4.0, Smart Manufacturing, academic impact},
abstract = {The released ChatGPT as a powerful language model is capable of assisting with a wide range of tasks, including answering questions, summarizing, paraphrasing, proofreading, classifying, and integrating texts. In this study, we tested ChatGPT capability to assist researchers in evaluating the academic articles’ contribution. We suggest a dialogue schema in which ChatGPT is asked to answer research questions from the target article and then to compare its own answers with the answers from the article. Finally, ChatGPT is asked to integrate both solutions coherently. We experimented with Proceedings of ISM-2022 Conference on Industry 4.0 and Smart Manufacturing, utilizing explicit research questions. The chat context enabled assessing studied articles’ contributions to Industry 4.0, uncovering advancements beyond the state-of-the-art. However, ChatGPT demonstrates limitations in content understanding and contribution evaluation. We conclude that while it collaborates with humans on academic tasks, human guidance remains essential, while ChatGPT's assistance efficiently complements traditional academic processes.}
}
@article{SUN2012227,
title = {Memory systems within a cognitive architecture},
journal = {New Ideas in Psychology},
volume = {30},
number = {2},
pages = {227-240},
year = {2012},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2011.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X11000729},
author = {Ron Sun},
keywords = {Memory, Cognitive architecture, CLARION},
abstract = {This article addresses the division of memory systems in relation to an overall cognitive architecture. As understanding the architecture is essential to understanding the mind, developing computational cognitive architectures is an important enterprise in computational psychology (computational cognitive modeling). The article proposes a set of hypotheses concerning memory systems from the standpoint of a cognitive architecture, in particular, the four-way division of memory (including explicit and implicit procedural memory and explicit and implicit declarative memory). It then discusses in detail how these hypotheses may be validated through examining qualitatively the literature on memory. A quick review follows of computational simulations of a variety of quantitative data (which are not limited to narrowly conceived “memory tasks”). Results of accounting for both qualitative and quantitative data point to the promise of this approach.}
}
@article{KELLIHER201536,
title = {Design futures in action: Documenting experiential futures for participatory audiences},
journal = {Futures},
volume = {70},
pages = {36-47},
year = {2015},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2014.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0016328714001980},
author = {Aisling Kelliher and Daragh Byrne},
keywords = {Documentation, Multimedia, Experiential futures, Summarization, Representation, Annotation, Exhibition, Social platform},
abstract = {The futures field demonstrates a willing openness in embracing methodologies, approaches, and influences from a diversity of disciplines and perspectives. This plurality of practice is evidenced in a growing body of work that increasingly embodies futures thinking in the design of everyday material and networked experiences. The intersection of design and futures produces artifacts, applications and interactions created to provoke dialog in an accessible manner. As part of the Futures special issue on the Emerge: Artists and Scientists Redesign the Future event, this article describes the documentation and public representation of the creative outcomes from nine Emerge design futures workshops. These workshops provided a rich opportunity to study how designers and futurists collaboratively engage, implement and communicate alternative futures. The goal of the documentation effort described is to capture the experience of creating experiential futures and extend the capacity for developing social foresight through a participatory exhibit and online social platform.}
}
@article{JACOB2022470,
title = {Algorithmic Approaches to Classify Autism Spectrum Disorders: A Research Perspective},
journal = {Procedia Computer Science},
volume = {201},
pages = {470-477},
year = {2022},
note = {The 13th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 5th International Conference on Emerging Data and Industry 4.0 (EDI40)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.03.061},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922004744},
author = {Shomona Gracia Jacob and Majdi Mohammed {Bait Ali Sulaiman} and Bensujin Bennet},
keywords = {Machine learning, Supervised learning, Pattern discovery, Autism disorder, Data Mining},
abstract = {Autism Spectrum Disorder (ASD) is a neurodevelopmental disability that exhibits sluggish progress in vocal development, restricted interest in normal activity and repetitive disoriented behavior. This syndrome, has gained a lot of attention due to its prevalence among children across all countries and from different economic backgrounds. However, ASD detection and treatment yet remains in its infancy due to the lack of awareness among parents, limited screening of proper developmental milestones and a dearth of diagnostic tools to classify this syndrome with convincing accuracy. Recent studies report that scalable biomarkers for early detection have made little progress in research due to the erraticism of this disorder. Moreover, the study on developing tools or applications for parents, teachers, and healthcare workers to identify children who exhibit any form of autism is still a work in progress. The research work undertaken in this paper presents an analysis of supervised machine learning algorithms on mining interesting details that link the diverse nature of ASD and the possibility of computationally detecting markers for the syndrome. The preliminary findings on the performance of traditional machine learning algorithms in ASD classification is reported with the possibility of integrating deep learning architectures for ASD detection and therapy.}
}
@incollection{YANG202333,
title = {Chapter 2 - Machine learning for solid mechanics},
editor = {Yuebing Zheng and Zilong Wu},
booktitle = {Intelligent Nanotechnology},
publisher = {Elsevier},
pages = {33-45},
year = {2023},
series = {Materials Today},
isbn = {978-0-323-85796-3},
doi = {https://doi.org/10.1016/B978-0-323-85796-3.00002-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323857963000020},
author = {Charles Yang and Zhizhou Zhang and Grace X. Gu},
keywords = {Solid mechanics, Inverse design, Physics-informed deep learning, Graph neural networks},
abstract = {Solid mechanics is an important field responsible for the robust designs of humanity's greatest engineering accomplishments, from skyscrapers to airplanes to the space shuttle. A burst of new manufacturing techniques and novel next-generation materials is ushering in a new age of engineering revolving around sustainable development. In this chapter, we outline how artificial intelligence (AI) can help scientists and engineers manage the increasing complexity and computational requirements in solid mechanics fields. Two common problem-solving frameworks, forward and inverse design, as well as two promising new AI-based approaches, physics-informed deep learning and graph neural networks, are covered.}
}
@article{GALLISTEL2017498,
title = {The Coding Question},
journal = {Trends in Cognitive Sciences},
volume = {21},
number = {7},
pages = {498-508},
year = {2017},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2017.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S1364661317300852},
author = {C.R. Gallistel},
abstract = {Recent electrophysiological results imply that the duration of the stimulus onset asynchrony in eyeblink conditioning is encoded by a mechanism intrinsic to the cerebellar Purkinje cell. This raises the general question – how is quantitative information (durations, distances, rates, probabilities, amounts, etc.) transmitted by spike trains and encoded into engrams? The usual assumption is that information is transmitted by firing rates. However, rate codes are energetically inefficient and computationally awkward. A combinatorial code is more plausible. If the engram consists of altered synaptic conductances (the usual assumption), then we must ask how numbers may be written to synapses. It is much easier to formulate a coding hypothesis if the engram is realized by a cell-intrinsic molecular mechanism.}
}
@article{KOTYRA2023105613,
title = {High-performance watershed delineation algorithm for GPU using CUDA and OpenMP},
journal = {Environmental Modelling & Software},
volume = {160},
pages = {105613},
year = {2023},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105613},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222003139},
author = {Bartłomiej Kotyra},
keywords = {Watershed delineation, GIS, Parallel algorithms, GPU, CUDA, OpenMP},
abstract = {Watershed delineation is one of the fundamental tasks in hydrological studies. Tools for extracting watersheds from digital elevation models and flow direction rasters are commonly implemented in GIS software packages. However, the performance of available techniques and algorithms often turns out to be far from sufficient, especially when working with large datasets. While modern hardware offers high computing performance through massive parallelism, there is still a need for algorithms that can effectively use these capabilities. This paper proposes an algorithm for rapid watershed delineation directly from flow direction rasters, using the possibilities offered by modern GPU devices. Performance measurements show a significant reduction in execution time compared to other parallel solutions proposed for this task in the literature. Moreover, this implementation makes it possible to delineate multiple watersheds from the same dataset simultaneously, each having one or more outlet cells, with virtually no additional computational cost.}
}
@article{JANSEN2022100020,
title = {The illusion of data validity: Why numbers about people are likely wrong},
journal = {Data and Information Management},
volume = {6},
number = {4},
pages = {100020},
year = {2022},
issn = {2543-9251},
doi = {https://doi.org/10.1016/j.dim.2022.100020},
url = {https://www.sciencedirect.com/science/article/pii/S2543925122001188},
author = {Bernard J. Jansen and Joni Salminen and Soon-gyo Jung and Hind Almerekhi},
keywords = {People data, Measurement, Quantitative paradigm, Statistics},
abstract = {This reflection article addresses a difficulty faced by scholars and practitioners working with numbers about people, which is that those who study people want numerical data about these people. Unfortunately, time and time again, this numerical data about people is wrong. Addressing the potential causes of this wrongness, we present examples of analyzing people numbers, i.e., numbers derived from digital data by or about people, and discuss the comforting illusion of data validity. We first lay a foundation by highlighting potential inaccuracies in collecting people data, such as selection bias. Then, we discuss inaccuracies in analyzing people data, such as the flaw of averages, followed by a discussion of errors that are made when trying to make sense of people data through techniques such as posterior labeling. Finally, we discuss a root cause of people data often being wrong – the conceptual conundrum of thinking the numbers are counts when they are actually measures. Practical solutions to address this illusion of data validity are proposed. The implications for theories derived from people data are also highlighted, namely that these people theories are generally wrong as they are often derived from people numbers that are wrong.}
}
@article{ZHUANG2024e29830,
title = {Artificial multi-verse optimisation for predicting the effect of ideological and political theory course},
journal = {Heliyon},
volume = {10},
number = {9},
pages = {e29830},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e29830},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024058614},
author = {Xingzhong Zhuang and Zhaodi Yi and Yuqing Wang and Yi Chen and Sudan Yu},
keywords = {Teaching sufficiency, Artificial multi-verse optimizer, Classification, Art ideological and political theory course},
abstract = {Enhancing teaching sufficiency is crucial because low teaching efficiency has always been a widespread issue in ideological and political theory course. Evaluating data on the course is obtained from a freshmen class of 2022 using questionnaires. The data is organised and condensed for mining and analysis. Subsequently, an intelligent artificial multi-verse optimizer (AMVO) method s developed to predict the effect of ideological and political theory course. The proposed AMVO approach was tested against various cutting-edge algorithms to demonstrate its effectiveness and stability on the benchmark functions. The experimental results indicated that AMVO ranked first among the 23 test functions. Furthermore, the binary AMVO enhanced k-nearest neighbour classifier had excellent performance in the art ideological and political theory course in terms of error rate, accuracy, specificity and sensitivity. This model can predict the overall evaluation attitude of freshmen towards the course based on the dataset. In addition, we can further analyse the potential correlations between factors that enhance the intellectual and political content of the course. This model can further refine the evaluation of ideological and political courses by teachers and students in our school, thereby achieving the fundamental goal of moral cultivation.}
}
@article{ROMANATO2000277,
title = {Computation of the strain field generated by dislocations with a position-dependent Burgers' vector distribution},
journal = {Micron},
volume = {31},
number = {3},
pages = {277-283},
year = {2000},
issn = {0968-4328},
doi = {https://doi.org/10.1016/S0968-4328(99)00094-3},
url = {https://www.sciencedirect.com/science/article/pii/S0968432899000943},
author = {F Romanato and M Natali and E Napolitani and A Drigo},
keywords = {Burgers' vector, Misfit dislocation, Reciprocal space maps},
abstract = {A new phenomenon of strain relaxation will be presented. In a series of InxGa1−xAs graded composition buffer layers grown on well cut (001) GaAs substrates, a curvature of the epilayer lattice has been found, i.e. a tilt of the epilayer lattice orientation with respect to the substrate which varies coherently along the sample surface on the scale of several mm. The most recent data analysis performed on a buffer layer compositionally graded with a six-step profile shows also a thickness functional dependence of the curvature. The epilayer lattice curvature has been attributed to a coherent lateral distribution of the Burgers’ vectors. An analytical model has been developed in the framework of the continuum elasticity theory to compute the related strain field. The results show small but unexpected contributions to the parallel strain.}
}
@article{LOU2023102236,
title = {A function-behavior mapping approach for product conceptual design inspired by memory mechanism},
journal = {Advanced Engineering Informatics},
volume = {58},
pages = {102236},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102236},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623003646},
author = {Shanhe Lou and Yixiong Feng and Yicong Gao and Hao Zheng and Tao Peng and Jianrong Tan},
keywords = {Conceptual design, Situated function-behavior-structure, Memory mechanism, Reinforcement learning},
abstract = {Conceptual design is a pivotal stage for new product development that relies more on designers to solve open-ended and ill-defined problems. Situated function-behavior-structure ontology is an acknowledged method to facilitate conceptual design in a goal-oriented way. However, it depends on the subjective cognition abilities of designers, which are influenced by limited memory and reasoning capacities. Developing computer-aided methods grounded in this ontology holds significant promise in enhancing designers' cognitive abilities. This study delves into the function-behavior (F-B) mapping process. It explores the effect of working memory and long-term memory on design cognition and introduces a memory-inspired reinforcement learning framework for F-B mapping. The Markov decision process is then adopted to formalize F-B mapping while motivation-driven Q learning is employed by the design agent to learn knowledge from historical design cases. The learned state-action value matrix can be applied to guide the designer in selecting feasible behaviors for the specific function requirement. The proposed approach empowers design agents with self-learning and self-evolving capacities. A case study on the F-B mapping of a traction system is conducted to illustrate the feasibility and practicability of the proposed approach.}
}
@article{ALEXOPOULOS2024,
title = {AJGP Solicits Papers Aimed to Enrich Geriatric Psychiatry},
journal = {The American Journal of Geriatric Psychiatry},
year = {2024},
issn = {1064-7481},
doi = {https://doi.org/10.1016/j.jagp.2024.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S1064748124004482},
author = {George S. Alexopoulos}
}
@article{CHUDERSKI2014258,
title = {How well can storage capacity, executive control, and fluid reasoning explain insight problem solving},
journal = {Intelligence},
volume = {46},
pages = {258-270},
year = {2014},
issn = {0160-2896},
doi = {https://doi.org/10.1016/j.intell.2014.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0160289614001020},
author = {Adam Chuderski},
keywords = {Insight problem solving, Fluid reasoning, Working memory, Storage capacity, Executive control},
abstract = {Previous studies have found discrepant results on the relationship between insight problem solving and the processes underlying analytic thinking: storage capacity, executive control (two components of working memory; WM), as well as fluid reasoning. Some research showed that WM and/or reasoning are positively related to insight, supporting the “nothing-special” account, whereas other studies demonstrated null or negative relationships favoring the “special-process” view. This study examined a large sample with a battery of insight, reasoning, and WM tasks, to estimate the pattern of links between investigated constructs using structural equation modeling. WM and reasoning together explained about two thirds of the variance in insight. Both WM components similarly contributed to insight. WM's contribution was mediated by reasoning. These results support the nothing-special view. However, after WM variance was partialed out, the link between insight and reasoning substantially weakened, that makes room for the special-process view. Both accounts can be integrated in the view that insight is “nothing special with special add-ons” – the latter understood as the processes and strategies specific only to insight problem solving.}
}
@article{WU2024111235,
title = {Intelligent strategic bidding in competitive electricity markets using multi-agent simulation and deep reinforcement learning},
journal = {Applied Soft Computing},
volume = {152},
pages = {111235},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111235},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624000097},
author = {Jiahui Wu and Jidong Wang and Xiangyu Kong},
keywords = {Intelligent bidding strategy, Competitive electricity markets, Multi-agent simulation(MAS), Deep reinforcement learning(DRL), Async n-step QL, Improved Async n-step QL},
abstract = {Aiming at the lack of comprehension of agents in Multi-Agent Simulation (MAS) based on classic Reinforcement Learning algorithms of competitive electricity markets, an intelligent strategic bidding method using Deep Reinforcement Learning (DRL) and MAS is proposed in this paper, which not only can provide more intelligent strategies for market participants to maximize their profits, but can enhance the performance of simulation models dealing with high-dimensional continuous data in electricity markets. Firstly, a theoretical framework of intelligent strategic bidding in competitive electricity markets based on MAS and DRL is proposed, and the process of intelligent bidding in electricity markets based on MAS and DRL is described. Then, three MAS models of intelligent strategic bidding are built based on three classic DRL algorithms, including Deep Q-Network (DQN), Double Deep Q-Network (DDQN), and Asynchronous n-step Q-learning (Async n-step QL), and three algorithms’ convergence speed, computational efficiency, and response sensitivity are compared and analyzed. Finally, a novel Improved Async n-step QL (IAsync n-step QL) algorithm is proposed, the MAS model based on the IAsync n-step QL algorithm for intelligent strategic bidding is established. Simulation results show that the model using the novel DRL algorithm is more profitable and responsive than the classic DRL algorithms.}
}
@article{IM2009193,
title = {Diagnosing skills of statistical hypothesis testing using the Rule Space Method},
journal = {Studies in Educational Evaluation},
volume = {35},
number = {4},
pages = {193-199},
year = {2009},
issn = {0191-491X},
doi = {https://doi.org/10.1016/j.stueduc.2009.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0191491X0900042X},
author = {Seongah Im and Yue Yin},
keywords = {Cognitive diagnostic assessment, Rule Space Method, Statistical hypothesis testing, Educational evaluation},
abstract = {This study illustrated the use of the Rule Space Method to diagnose students’ proficiencies in, skills and knowledge of statistical hypothesis testing. Participants included 96 undergraduate and, graduate students, of whom 94 were classified into one or more of the knowledge states identified by, the rule space analysis. Analysis at the level of proficiency groups showed that the critical difference, between low and medium proficiency groups was the understanding of statistical concepts and, knowledge while the critical skill discriminating the medium proficiency group from the high, proficiency group was to mange complex computational procedures. In addition, attribute profiles of, two students showed how students with the same total score can possess different strengths and, weaknesses.}
}
@article{EISENBERGER2004294,
title = {Why rejection hurts: a common neural alarm system for physical and social pain},
journal = {Trends in Cognitive Sciences},
volume = {8},
number = {7},
pages = {294-300},
year = {2004},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2004.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S1364661304001433},
author = {Naomi I. Eisenberger and Matthew D. Lieberman},
abstract = {Numerous languages characterize ‘social pain’, the feelings resulting from social estrangement, with words typically reserved for describing physical pain (‘broken heart’, ‘broken bones’) and perhaps for good reason. It has been suggested that, in mammalian species, the social-attachment system borrowed the computations of the pain system to prevent the potentially harmful consequences of social separation. Mounting evidence from the animal lesion and human neuroimaging literatures suggests that physical and social pain overlap in their underlying neural circuitry and computational processes. We review evidence suggesting that the anterior cingulate cortex plays a key role in the physical–social pain overlap. We also suggest that the physical–social pain circuitry might share components of a broader neural alarm system.}
}
@article{BURIGANA2024102875,
title = {Bayesian networks and knowledge structures in cognitive assessment: Remarks on basic comparable aspects},
journal = {Journal of Mathematical Psychology},
volume = {123},
pages = {102875},
year = {2024},
issn = {0022-2496},
doi = {https://doi.org/10.1016/j.jmp.2024.102875},
url = {https://www.sciencedirect.com/science/article/pii/S0022249624000440},
author = {Luigi Burigana},
keywords = {Knowledge assessment, Knowledge structure, Bayesian network, Probabilistic graphical model, Probabilistic inference},
abstract = {Two theories of current interest and of mathematical and computational substance concerning knowledge assessment in education are discussed. These are the theory of knowledge structures and the theory of Bayesian networks as specifically related to educational assessment. In four separate sections, the two theories are compared by considering the sets of variables involved in their models, the set-theoretical and relational constructs defined on those variables, the probabilistic assumptions and properties, and the problems addressed by the theories in constructing their models. For the comparison, a common-base system of symbols and terms is adopted, which overcomes the peculiarities of expression in the corresponding streams of literature. This system gives us a better recognition of the similarities and differences between the two paradigms, and a precise appreciation of their arguments and abilities.}
}
@article{OZTOP2022106240,
title = {Analysis of melting of phase change material block inserted to an open cavity},
journal = {International Communications in Heat and Mass Transfer},
volume = {137},
pages = {106240},
year = {2022},
issn = {0735-1933},
doi = {https://doi.org/10.1016/j.icheatmasstransfer.2022.106240},
url = {https://www.sciencedirect.com/science/article/pii/S0735193322003621},
author = {Hakan F. Öztop and Hakan Coşanay and Fatih Selimefendigil and Nidal Abu-Hamdeh},
keywords = {Partially open cavity, PCM, Melting, Computational, Finned heater},
abstract = {A numerical work has been conducted to explore the effects of opening parameters on melting of phase change material (PCM) during natural convection in a partially open enclosure. A finned heater is located on bottom wall while the remaining parts are insulated. Paraffin wax is used as PCM and two-dimensional time dependent analysis is performed by using the finite volume method for the parameters of location of opening and temperature difference. The governing parameters for the study are chosen for the range of Ra = 1.45 × 108 ≤ Ra ≤ Ra = 1.97 × 108, 0.25 ≤ w/H ≤ 0.75 and 0.25 ≤ c/H ≤ 0.75. It is found that both opening ratio and opening length are effective parameter on melting time and these can be used as control parameters for improving the energy efficiency. Also, heat transfer can be controlled by using PCM inserted block and opening parameters. Among different cases of opening ratios and locations of opening, the most favorable configuration is obtained at Ra = 1.97 × 108, w/H = 0.25, c/H = 0.25 while average heat transfer enhancement by about 60% is achieved. At the lowest and highest value of Rayleigh numbers, the most favorable location of the opening is obtained at c/H = 0.25 in order to have the highest reduction amount of phase completion time.}
}
@article{HU2019202,
title = {Effective Connectivity of the Fronto-Parietal Network during the Tangram Task in a Natural Environment},
journal = {Neuroscience},
volume = {422},
pages = {202-211},
year = {2019},
issn = {0306-4522},
doi = {https://doi.org/10.1016/j.neuroscience.2019.09.021},
url = {https://www.sciencedirect.com/science/article/pii/S0306452219306669},
author = {Zhishan Hu and Keng-Fong Lam and Zhen Yuan},
keywords = {Tangram, Visuospatial reasoning, Granger causality, fNIRS, Brain networks},
abstract = {Although the neural basis underlying visuospatial reasoning has been widely explored by neuroimaging techniques, the brain activation patterns during naturalistic visuospatial reasoning such as tangram remains unclear. In this study, the directional functional connectivity of fronto-parietal networks during the tangram task was carefully inspected by using combined functional near-infrared spectroscopy (fNIRS) and conditional Granger causality analysis (GCA). Meanwhile, the causal networks during the traditional spatial reasoning task were also characterized to exhibit the differences with those during the tangram task. We discovered that the tangram task in a natural environment showed enhanced activation in the fronto-parietal regions, particularly the frontal cortex. In addition, a strong directional connectivity from the right prefrontal cortex to left angular gyrus was detected for the complex spatial reasoning condition of spatial reasoning task, whereas no effective connectivity was identified between the frontal and parietal cortices during the tangram task. Further correlation analyses showed that the behavioral performance in the spatial reasoning rather than the tangram task manifested the relationship with the connectivity between the frontal and parietal cortex. Our findings demonstrate that the tangram task measures a different aspect of the visuospatial reasoning ability which requires more trial-and-error strategies and creative thinking rather than inductive reasoning. In particular, the frontal cortex is mostly involved in tangram puzzle-solving, whereas the interaction between frontal and parietal cortices is regulated by the hands-on experience during the tangram task.}
}
@article{1987755,
title = {Computation of signatures of linear airgun arrays: Vaage, S. and B. Ursin, 1987. Geophys. Prospect., 35(3):281–287. SERES A/S, P.O. Box 1965, Moholtan, 7001 Trondheim, Norway},
journal = {Deep Sea Research Part B. Oceanographic Literature Review},
volume = {34},
number = {9},
pages = {755},
year = {1987},
issn = {0198-0254},
doi = {https://doi.org/10.1016/0198-0254(87)90164-6},
url = {https://www.sciencedirect.com/science/article/pii/0198025487901646}
}
@article{BROWN2022110672,
title = {“Deep reinforcement learning for engineering design through topology optimization of elementally discretized design domains”},
journal = {Materials & Design},
volume = {218},
pages = {110672},
year = {2022},
issn = {0264-1275},
doi = {https://doi.org/10.1016/j.matdes.2022.110672},
url = {https://www.sciencedirect.com/science/article/pii/S0264127522002933},
author = {Nathan K. Brown and Anthony P. Garland and Georges M. Fadel and Gang Li},
keywords = {Reinforcement learning, Topology optimization, Deep learning, Engineering design, Structural design, Data-driven},
abstract = {Advances in machine learning algorithms and increased computational efficiencies give engineers new capabilities and tools to apply to engineering design. Machine learning models can approximate complex functions and, therefore, can be useful for various tasks in the engineering design workflow. This paper investigates using reinforcement learning (RL), a subset of machine learning that teaches an agent to complete a task through accumulating experiences in an interactive environment, to automate the designing of 2D discretized topologies. RL agents use past experiences to learn sequential sets of actions to best achieve some objective. In the proposed environment, an RL agent can make sequential decisions to design a topology by removing elements to best satisfy compliance minimization objectives. After each action, the agent receives feedback by evaluating how well the current topology satisfies the design objectives. After training, the agent was tasked with designing optimal topologies under various load cases. The agent's proposed designs had similar or better compliance minimization performance to those produced by traditional gradient-based topology optimization methods. These results show that a deep RL agent can learn generalized design strategies to satisfy multi-objective design tasks and, therefore, shows promise as a tool for arbitrarily complex design problems across many domains.}
}
@article{GOLTSOS2022397,
title = {Inventory – forecasting: Mind the gap},
journal = {European Journal of Operational Research},
volume = {299},
number = {2},
pages = {397-419},
year = {2022},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2021.07.040},
url = {https://www.sciencedirect.com/science/article/pii/S0377221721006500},
author = {Thanos E. Goltsos and Aris A. Syntetos and Christoph H. Glock and George Ioannou},
keywords = {Forecasting, Inventory control, Inventory forecasting, Literature review},
abstract = {We are concerned with the interaction and integration between demand forecasting and inventory control, in the context of supply chain operations. The majority of the literature is fragmented. Forecasting research more often than not assumes forecasting to be an end in itself, disregarding any subsequent stages of computation that are needed to transform forecasts into replenishment decisions. Conversely, most contributions in inventory theory assume that demand (and its parameters) are known, in effect disregarding any preceding stages of computation. Explicit recognition of these shortcomings is an important step towards more realistic theoretical developments, but still not particularly helpful unless they are somehow addressed. Even then, forecasts often constitute exogenous variables that serially feed into a stock control model. Finally, there is a small but growing stream of research that is explicitly built around jointly tackling the inventory forecasting question. We introduce a framework to define four levels of integration: from disregarding, to acknowledging, to partly addressing, to fully understanding the interactions. Focusing on the last two, we conduct a structured review of relevant (integrated) academic contributions in the area of forecasting and inventory control and argue for their classification with regard to integration. We show that the development from one level to another is in many cases chronological in order, but also associated with specific schools of thought. We also argue that although movement from one level to another adds realism, it also adds complexity in terms of actual implementations, and thus a trade-off exists. The article makes a contribution into an area that has always been fragmented despite the importance of bringing the forecasting and inventory communities together to solve problems of common interest. We close with an indicative agenda for further research and a call for more theoretical contributions, but also more work that would help to expand the empirical knowledge base in this area.}
}
@article{YUROVSKY201873,
title = {A communicative approach to early word learning},
journal = {New Ideas in Psychology},
volume = {50},
pages = {73-79},
year = {2018},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2017.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X17300077},
author = {Daniel Yurovsky},
keywords = {Language acquisition, Learning, Cognitive development},
abstract = {Young children learn the meanings of thousands of words by the time they can run down the street. Many efforts to explain this rapid development begin by assuming that the computational-level problem being solved is acquisition. Consequently, work in this line has sought to understand how children infer the meanings of words from cues in the communicative signals of the speakers around them. I will argue, however, that this formulation of the problem is backwards: the computational problem is communication, and language acquisition provides cues about how to communicate successfully. Under this framing, the natural unit of analysis is not the child, but the parent-child dyad. A necessary consequence of this shift is the realization that the statistical structure of the input to the child is itself dependent on the child. This dependency radically simplifies the computational problem of learning and using language.}
}
@article{JYOTSNA20231270,
title = {IntelEye: An Intelligent Tool for the Detection of Stressful State based on Eye Gaze Data While Watching Video},
journal = {Procedia Computer Science},
volume = {218},
pages = {1270-1279},
year = {2023},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.105},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923001059},
author = {C. Jyotsna and J. Amudha and Amritanshu Ram and Giandomenico Nollo},
keywords = {Eye Tracking, Mental Health, Real Time Monitoring, K-Nearest Neighbour, Eye Gaze Measures, Welch Two Sample t-test},
abstract = {Technology to monitor mental health is gaining popularity as it helps to improve the cognitive and behavioral performance of an individual. Considering the growing need to monitor mental health, there is subsequent research in continuous and real-time monitoring technologies that can increase the quality of life by reducing the cost of health care. Eye tracking technology has played a significant role in monitoring a person's mental health. An intelligent system can apply several computational procedures to extract meaningful information from the massive physiological data obtained from eye tracking. The proposed model IntelEye is a tool to detect the stressful states of an individual while watching calm and stressful videos. The eye gaze measures based on pupil diameter, fixation, and blink were used for detecting stressful conditions. The data was collected from hospital employees, and the K Nearest Neighbor algorithm could successfully recognize the stressful states and the corresponding gaze location during stressful situations. IntelEye is not only identifying the stressful states but also has the novelty of identifying the scene and gaze location, making them stressful while watching the video.}
}
@article{SHAFFER202041,
title = {Artificial intelligence products reshape accounting: time to re-train},
journal = {Development and Learning in Organizations: An International Journal},
volume = {34},
number = {6},
pages = {41-43},
year = {2020},
issn = {1477-7282},
doi = {https://doi.org/10.1108/DLO-10-2019-0242},
url = {https://www.sciencedirect.com/science/article/pii/S1477728220001288},
author = {Kathie J. Shaffer and Carol J. Gaumer and Kiersten P. Bradley},
keywords = {Organizational change, Accounting, Artificial intelligence},
abstract = {Purpose
Managers are expected to increase productivity in the most cost-efficient manner, using all available resources and, “work smarter.” As technology improves, there is greater incentive for managers to invest in options where automation becomes less expensive than the high cost of human capital. When repetitive tasks can be accurately duplicated through automation, the decision becomes a fait accompli. Advances in artificial intelligence (AI) or synthetic intelligence that simulates human intellectual function has significant impact potential in the service sector. This paper examines productivity efficiencies sought through artificial intelligence and the need for re-training, specifically in the accounting profession.
Design/methodology/approach
This is a conceptual paper for practitioners without research methodology.
Findings
The accounting profession 10 years from now will look noticeably different than it does now. The accountants, who embrace the new technologies, like artificial intelligence, will survive and even thrive by becoming more specialized. This will require training and, in some instances, re-training. Organizations must be willing to absorb those development costs. I hope that new graduates will enter the profession with updated skills providing added value for organizations and employers who started into the profession many years ago. The biggest challenge may lie in the re-training of accountants who have been in practice for many years and managing the resistance to change. Employers must first set the example by accepting the inevitable and then encourage and support employees to improve and update their skills. Additionally, they will have to coach employees through the changes with reassurance that those who embrace the change will experience less chance of job elimination. Embracing the available technology will enable firms to serve clients more efficiently and effectively by providing up to date business solutions regardless of the services being offered.
Research limitations/implications
There is no empirical research in this paper. It is a conceptual piece looking at the changing organization in accounting, specifically due to artificial intelligence.
Practical implications
Accounting firms that focus on basic accounting functions should find new services to offer. The same clients can be served, but at a higher-level. Accountants will offer more value to clients by detecting patterns and trends when more time can be devoted to analysis. Helping clients beyond the preparation of documents requires that accountants understand the current market conditions and potential effects of inflation and, engage in more critical thinking while at the same time be able to teach clients and help them understand at the higher level. Just as accountants’ responsibilities and duties will be transformed through the integration of AI, accounting education must be altered.
Social implications
Implications related to the workplace are only discussed in this paper.
Originality/value
It is not completely original. It is a compilation of research that is out there as a means to address critical workforce training needs in accounting as technology moves forward.}
}
@article{BENDER2024156,
title = {Dimension results for extremal-generic polynomial systems over complete toric varieties},
journal = {Journal of Algebra},
volume = {646},
pages = {156-182},
year = {2024},
issn = {0021-8693},
doi = {https://doi.org/10.1016/j.jalgebra.2024.01.029},
url = {https://www.sciencedirect.com/science/article/pii/S0021869324000553},
author = {Matías Bender and Pierre-Jean Spaenlehauer},
keywords = {Sparse polynomial systems, Toric varieties},
abstract = {We study polynomial systems with prescribed monomial supports in the Cox ring of a toric variety built from a complete polyhedral fan. We present combinatorial formulas for the dimension of their associated subvarieties under genericity assumptions on the coefficients of the polynomials. Using these formulas, we identify at which degrees generic systems in polytopal algebras form regular sequences. Our motivation comes from sparse elimination theory, where knowing the expected dimension of these subvarieties leads to specialized algorithms and to large speed-ups for solving sparse polynomial systems. As a special case, we classify the degrees at which regular sequences defined by weighted homogeneous polynomials can be found, answering an open question in the Gröbner bases literature. We also show that deciding whether a sparse system is generically a regular sequence in a polytopal algebra is hard from the point of view of theoretical computational complexity.}
}
@article{AHMAD20225041,
title = {Decision Level Fusion Using Hybrid Classifier for Mental Disease Classification},
journal = {Computers, Materials and Continua},
volume = {72},
number = {3},
pages = {5041-5058},
year = {2022},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2022.026077},
url = {https://www.sciencedirect.com/science/article/pii/S1546221822009067},
author = {Maqsood Ahmad and Noorhaniza Wahid and Rahayu A Hamid and Saima Sadiq and Arif Mehmood and Gyu Sang Choi},
keywords = {Mental health diagnosis, machine learning, depression, shrewd probing, diagnostic approach},
abstract = {Mental health signifies the emotional, social, and psychological well-being of a person. It also affects the way of thinking, feeling, and situation handling of a person. Stable mental health helps in working with full potential in all stages of life from childhood to adulthood therefore it is of significant importance to find out the onset of the mental disease in order to maintain balance in life. Mental health problems are rising globally and constituting a burden on healthcare systems. Early diagnosis can help the professionals in the treatment that may lead to complications if they remain untreated. The machine learning models are highly prevalent for medical data analysis, disease diagnosis, and psychiatric nosology. This research addresses the challenge of detecting six major psychological disorders, namely, Anxiety, Bipolar Disorder, Conversion Disorder, Depression, Mental Retardation and Schizophrenia. These challenges are mined by applying decision level fusion of supervised machine learning algorithms. A dataset was collected from a clinical psychologist consisting of 1771 observations that we used for training and testing the models. Furthermore, to reduce the impact of a conflicting decision, a voting scheme Shrewd Probing Prediction Model (SPPM) is introduced to get output from ensemble model of Random Forest and Gradient Boosting Machine (RF + GBM). This research provides an intuitive solution for mental disorder analysis among different target class labels or groups. A framework is proposed for determining the mental health problem of patients using observations of medical experts. The framework consists of an ensemble model based on RF and GBM with a novel SPPM technique. This proposed decision level fusion approach by combining RF + GBM with SPPM-MIN significantly improves the performance in terms of Accuracy, Precision, Recall, and F1-score with 71\%, 73\%, 71\% and 71\% respectively. This framework seems suitable in the case of huge and more diverse multi-class datasets. Furthermore, three vector spaces based on TF-IDF (unigram, bi-gram, and tri-gram) are also tested on the machine learning models and the proposed model.}
}
@article{ABDALLAH2024102341,
title = {An evaluation of the use of air cooling to enhance photovoltaic performance},
journal = {Thermal Science and Engineering Progress},
volume = {47},
pages = {102341},
year = {2024},
issn = {2451-9049},
doi = {https://doi.org/10.1016/j.tsep.2023.102341},
url = {https://www.sciencedirect.com/science/article/pii/S2451904923006947},
author = {Ramez Abdallah and Tamer Haddad and Mohammad Zayed and Adel Juaidi and Tareq Salameh},
keywords = {Photovoltaic, ANSYS fluent, CFD, PV cooling, Heat sink},
abstract = {The rapid rise in global energy consumption and its consequences on climate change has made incorporating renewable energy sources like solar photovoltaics into the building envelope easier. However, in spite of extensive uses and significant technological advances, the lower solar panel efficiencies caused by high temperatures remain a significant barrier to the viability of deploying photovoltaic technology in regions with hot climates utilizing computational fluid dynamics (CFD). This research examines the cooling effectiveness of air-cooled photovoltaic (PV) under the climate of Nablus - Palestine. This study presents a numerical model designed to cool solar panels using various air-cooled channel configurations. Rectangular fins made of high thermal conductivity materials such as copper were used in this study. The parametric study was based on the changing baseplate thickness, fin spacing, height, and thickness through a stepwise optimization process to enhance the heat transfer mechanism. The results show that the optimum design of average volume temperatures for the PV cell models in air-cooled channel configurations with and without fins were 40.28 °C and 42.58 °C, respectively. The optimum design was obtained at 3, 110, 60, and 4 mm for baseplate thickness, fin spacing, height, and thickness, respectively. This optimum design was responsible for the average PV panel temperature drop by 1.6 %, 1.3 %, 5.9 %, and 6.2 % for baseplate thickness, fin spacing, height, and thickness, respectively. The optimum design of an air-cooled cooling channel for PV is an important insight provided by this work, and it may help in the future development of more effective and affordable cooling methods.}
}
@article{PINEDA2024101204,
title = {The mode of computing},
journal = {Cognitive Systems Research},
volume = {84},
pages = {101204},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.101204},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723001389},
author = {Luis A. Pineda},
keywords = {Mode of computing, Natural computing, Representation, Interpretation, Consciousness},
abstract = {The Turing Machine is the paradigmatic case of computing machines, but there are others such as analogical, connectionist, quantum and diverse forms of unconventional computing, each based on a particular intuition of the phenomenon of computing. This variety can be captured in terms of system levels, re-interpreting and generalizing Newell’s hierarchy, which includes the knowledge level at the top and the symbol level immediately below it. In this re-interpretation the knowledge level consists of human knowledge and the symbol level is generalized into a new level that here is called The Mode of Computing. Mental processes performed by natural brains are often thought of informally as computing processes and that the brain is alike to computing machinery. However, if natural computing does exist it should be characterized on its own. A proposal to such an effect is that natural computing appeared when interpretations were first made by biological entities, so natural computing and interpreting are two aspects of the same phenomenon, or that consciousness and experience are the manifestations of computing/interpreting. By analogy with computing machinery, there must be a system level at the top of the neural circuitry and directly below the knowledge level that is named here The mode of Natural Computing. If it turns out that such putative object does not exist the proposition that the mind is a computing process should be dropped; but characterizing it would come with solving the hard problem of consciousness.}
}
@article{HARA20239703,
title = {Reorganizing Cyber-Physical Configurations using User Activities for Human-in-the-Loop Cyber-Physical Systems},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {9703-9708},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.281},
url = {https://www.sciencedirect.com/science/article/pii/S240589632300633X},
author = {Tatsunori Hara and Yuki Okada and Jun Ota},
keywords = {Design, modelling and analysis of HMS, human-centered automation and design, cyber-physical system, design structure matrix, data utilization, product service system, smart home},
abstract = {This paper proposes a “human-in-the-loop design structure matrix (DSM)” method for understanding and reorganizing the structures of multiple cyber-physical systems (CPSs), focusing on user activities. By incorporating user activities as integral components in system engineering techniques, this method contributes to the design literature on human-in-the-loop CPS. Using the illustrative case of a smart home, we obtained the following types of clusters to review the structure and cyber-physical configurations of CPSs: clusters that retain the target user activity, clusters decoupled from the target user activity, and clusters across two different user activities. In the second type, we found a hub cluster regarding the cyber process of notifications to users, which enabled diverse data utilization among the clusters.}
}
@article{REN2025100774,
title = {Immersive E-learning mode application in Chinese language teaching system based on big data recommendation algorithm},
journal = {Entertainment Computing},
volume = {52},
pages = {100774},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100774},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124001423},
author = {Chunjiao Ren},
keywords = {Big data, Interest recommendation algorithm, Immersive, E-Learning mode, Chinese teaching system},
abstract = {With the rapid development of information technology, E-Learning has become an innovative teaching method. However, in the field of Chinese teaching, how to provide effective learning resources and recommendation algorithms in E-Learning mode is still a challenge. This study aims to improve the effectiveness of Chinese teaching system and students’ learning outcomes through an immersive E-Learning model based on big data interest recommendation algorithm. This paper adopts an immersive E-Learning model based on big data interest recommendation algorithm, and constructs a Chinese teaching system. The web crawler is used to fully collect the experimental data and collate it in a targeted manner, and the required data is screened out by using a more efficient separation method. Adding big data recommendation algorithm to the system of this paper can not only record and analyze historical behaviors of users, but also recommend data information according to users’ interests, so that users can clarify their real information needs. By testing the system’s professional ability and recording the experimental data, this paper finds that the overall performance of this Chinese language teaching system is very good, and can achieve the original expected design purpose. In addition, the system largely solves the problem that the traditional system based on data recommendation algorithm is difficult to carry out effective recommendation smoothly when the total amount of data is too large.}
}
@article{CURTO201911,
title = {Relating network connectivity to dynamics: opportunities and challenges for theoretical neuroscience},
journal = {Current Opinion in Neurobiology},
volume = {58},
pages = {11-20},
year = {2019},
note = {Computational Neuroscience},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2019.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0959438819300443},
author = {Carina Curto and Katherine Morrison},
abstract = {We review recent work relating network connectivity to the dynamics of neural activity. While concepts stemming from network science provide a valuable starting point, the interpretation of graph-theoretic structures and measures can be highly dependent on the dynamics associated to the network. Properties that are quite meaningful for linear dynamics, such as random walk and network flow models, may be of limited relevance in the neuroscience setting. Theoretical and computational neuroscience are playing a vital role in understanding the relationship between network connectivity and the nonlinear dynamics associated to neural networks.}
}
@article{GRAMELSBERGER2011296,
title = {What do numerical (climate) models really represent?},
journal = {Studies in History and Philosophy of Science Part A},
volume = {42},
number = {2},
pages = {296-302},
year = {2011},
note = {Model-Based Representation in Scientific Practice},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2010.11.037},
url = {https://www.sciencedirect.com/science/article/pii/S0039368110001160},
author = {Gabriele Gramelsberger},
keywords = {Climate models, Computing, Symbolic forms, Scientific modelling},
abstract = {The translation of a mathematical model into a numerical one employs various modifications in order to make the model accessible for computation. Such modifications include discretizations, approximations, heuristic assumptions, and other methods. The paper investigates the divergent styles of mathematical and numerical models in the case of a specific piece of code in a current atmospheric model. Cognizance of these modifications means that the question of the role and function of scientific models has to be reworked. Neither are numerical models pure intermediaries between theory and data, nor are they autonomous tools of inquiry. Instead, theory and data are transformed into a new symbolic form of research due to the fact that computation has become an essential requirement for every scientific practice. Therefore the question is posed: What do numerical (climate) models really represent?}
}
@article{DELEURAN201671,
title = {Exploratory Topology Modelling of Form-active Hybrid Structures},
journal = {Procedia Engineering},
volume = {155},
pages = {71-80},
year = {2016},
note = {TENSINET – COST TU1303 International Symposium 2016 "Novel structural skins - Improving sustainability and efficiency through new structural textile materials and designs"},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2016.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S187770581632149X},
author = {Anders Holden Deleuran and Mark Pauly and Martin Tamke and Ida Friis Tinning and Mette Ramsgaard Thomsen},
keywords = {Form-Active, Hybrid Structures, Topology, Shaping, Form-Finding, Interactive Modelling, Design Space Search},
abstract = {The development of novel form-active hybrid structures (FAHS) is impeded by a lack of modelling tools that allow for exploratory topology modelling of shaped assemblies. We present a flexible and real-time computational design modelling pipeline developed for the exploratory modelling of FAHS that enables designers and engineers to iteratively construct and manipulate form-active hybrid assembly topology on the fly. The pipeline implements Kangaroo2's projection-based methods for modelling hybrid structures consisting of slender beams and cable networks. A selection of design modelling sketches is presented in which the developed modelling pipeline has been integrated to explore the design space delineated by FAHS.}
}
@article{COCHRAN20201237,
title = {Sustainable Enterprise Design 4.0: Addressing Industry 4.0 Technologies from the Perspective of Sustainability},
journal = {Procedia Manufacturing},
volume = {51},
pages = {1237-1244},
year = {2020},
note = {30th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.10.173},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920320308},
author = {David S. Cochran and Erwin Rauch},
keywords = {Industry 4.0, sustainability, triple bottom line, sustainable enterprise design, industrial revolution},
abstract = {The introduction of Industry 4.0 and sustainability in production is currently on everyone’s mind. However, companies face difficulties to address these trends in their long-term enterprise strategy and design. Industry 4.0 promises strategic advantages for companies in many respects, but there is a lack of instruments and concepts for integrating emerging technologies in an overall enterprise system design. Similarly, the multiple perspectives regarding economic, environmental and social sustainability provide a framework for thinking about a strategy for sustainable enterprise design. Based on the three principles presented in this paper for Sustainable Enterprise Design, this article aims to present an approach to better address sustainability as well as Industry 4.0 in terms of a long-term strategic, enterprise design that is sustainable. As a result, a list of needs, functional requirements as well as possible Industry 4.0 physical solutions is proposed to achieve a long-term sustainable enterprise design. The consequence of the perspective of an enterprise as a system that can be designed provides a rigorous approach that takes advantage of Industry 4.0 technologies and the multiple perspectives and candidate physical solutions that the research community offers.}
}
@article{FAIRHURST1980447,
title = {Towards a rationale for neural stability: A model of neural computation and network architecture},
journal = {International Journal of Bio-Medical Computing},
volume = {11},
number = {6},
pages = {447-459},
year = {1980},
issn = {0020-7101},
doi = {https://doi.org/10.1016/0020-7101(80)90012-4},
url = {https://www.sciencedirect.com/science/article/pii/0020710180900124},
author = {M.C. Fairhurst and G.P. Goutos},
abstract = {Networks of Boolean processing cells with low connectivity are known to be inherently stable, in the sense that they exhibit only limited reverberatory activity among their possible state transitions. This paper discusses the value of such a network as a functional model of a neural system and, in the light of the observed decrease in stability with increasing cell connectivity, seeks to identify the features of network architecture and cell computation which act to protect network stability, thereby providing a framework for an understanding of neural stability.}
}
@article{PETIT2018135,
title = {Combining eco-social and environmental indicators to assess the sustainability performance of a food value chain: A case study},
journal = {Journal of Cleaner Production},
volume = {191},
pages = {135-143},
year = {2018},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2018.04.156},
url = {https://www.sciencedirect.com/science/article/pii/S0959652618311831},
author = {Gaëlle Petit and Caroline Sablayrolles and Gwenola {Yannou-Le Bris}},
keywords = {Life cycle assessment, Pork value chain, Sustainability, Metrics, Indicator, Framework},
abstract = {Stakeholders are increasingly demanding transparency on food value chain sustainability performance. Today there is no standard framework to meet this demand and support defining indicators to be used to conduct an overall sustainable performance assessment. This paper mobilizes existing frameworks and indicators to build new sustainable performance metrics for actors willing to work together for their value chain sustainability. Popular methods or tools for assessing dimensions of agrifood products or activities are selected and analyzed to determine how they could contribute to this metric. The analysis aims to distinguish the sustainable development pillars addressed (economic, environmental and/or social), the frames concerned (life cycle thinking or not; multi-actor or not), and the focus of performance measured (drivers, pressures, states, impacts, responses). This categorization is then used to develop a proposal for specifications adapted to food value chain sustainability performance assessment. The applicability of the framework is demonstrated through a case study in a pork agrifood value chain.}
}
@article{KHODADADI2022104354,
title = {Design exploration by using a genetic algorithm and the Theory of Inventive Problem Solving (TRIZ)},
journal = {Automation in Construction},
volume = {141},
pages = {104354},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104354},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522002278},
author = {Anahita Khodadadi and Peter {von Buelow}},
keywords = {Conceptual design, Design exploration, TRIZ, Genetic algorithm, Multi-objective design},
abstract = {This paper presents a computational design exploration method called GA+TRIZ, which aids designers in defining the design problem clearly, making a parametric model where pertinent variables are included, obtaining a series of suitable solutions, and resolving existing conflicts among design objectives. The goal is to include the designer's qualitative and performance-based quantitative design goals in the design process, while promoting innovative ideas for resolving contradictory design objectives. The method employed is a Genetic Algorithm (GA), earlier implemented in an automated design exploration process called ParaGen, in combination with the Theory of Inventive Problem Solving (TRIZ), a novel methodology to assist architects and structural engineers in the conceptual phase of design. The GA+TRIZ method promotes automated design exploration, investigation of unexpected solutions, and continuous interaction with the computational generating system. Finally, this paper presents two examples that illustrate how the GA+TRIZ method assists designers in problem structuring, design exploration, and decision-making.}
}
@article{ASHTIANI201549,
title = {A survey of quantum-like approaches to decision making and cognition},
journal = {Mathematical Social Sciences},
volume = {75},
pages = {49-80},
year = {2015},
issn = {0165-4896},
doi = {https://doi.org/10.1016/j.mathsocsci.2015.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0165489615000165},
author = {Mehrdad Ashtiani and Mohammad Abdollahi Azgomi},
abstract = {There has always been a steady interest in how humans make decisions amongst researchers from various fields. Based on this interest, many approaches such as rational choice theory or expected utility hypothesis have been proposed. Although these approaches provide a suitable ground for modeling the decision making process of humans, they are unable to explain the corresponding irrationalities and existing paradoxes and fallacies. Recently, a new formulation of decision theory that can correctly describe these paradoxes and possibly provide a unified and general theory of decision making has been proposed. This new formulation is founded based on the application of the mathematical structure of quantum theory to the fields of human decision making and cognition. It is shown that by applying these quantum-like models, one can better describe the uncertainty, ambiguity, emotions and risks involved in the human decision making process. Even in computational environments, an agent that follows the correct patterns of human decision making will have a better functionality in performing its role as a proxy for a real user. In this paper, we present a comprehensive survey of the researches and the corresponding recent developments. Finally, the benefits of leveraging the quantum-like modeling approaches in computational domains and the existing challenges and limitations currently facing the field are discussed.}
}
@article{LI2022296,
title = {The role of information structures in game-theoretic multi-agent learning},
journal = {Annual Reviews in Control},
volume = {53},
pages = {296-314},
year = {2022},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2022.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S1367578822000086},
author = {Tao Li and Yuhan Zhao and Quanyan Zhu},
keywords = {Multi-agent learning, Information structures, Reinforcement learning, Belief generation, Game theory, Value of Information},
abstract = {Multi-agent learning (MAL) studies how agents learn to behave optimally and adaptively from their experience when interacting with other agents in dynamic environments. The outcome of a MAL process is jointly determined by all agents’ decision-making. Hence, each agent needs to think strategically about others’ sequential moves, when planning future actions. The strategic interactions among agents makes MAL go beyond the direct extension of single-agent learning to multiple agents. With the strategic thinking, each agent aims to build a subjective model of others decision-making using its observations. Such modeling is directly influenced by agents’ perception during the learning process, which is called the information structure of the agent’s learning. As it determines the input to MAL processes, information structures play a significant role in the learning mechanisms of the agents. This review creates a taxonomy of MAL and establishes a unified and systematic way to understand MAL from the perspective of information structures. We define three fundamental components of MAL: the information structure (i.e., what the agent can observe), the belief generation (i.e., how the agent forms a belief about others based on the observations), as well as the policy generation (i.e., how the agent generates its policy based on its belief). In addition, this taxonomy enables the classification of a wide range of state-of-the-art algorithms into four categories based on the belief-generation mechanisms of the opponents, including stationary, conjectured, calibrated, and sophisticated opponents. We introduce Value of Information (VoI) as a metric to quantify the impact of different information structures on MAL. Finally, we discuss the strengths and limitations of algorithms from different categories and point to promising avenues of future research.}
}
@incollection{HARTSON2019293,
title = {Chapter 14 - Generative Design: Ideation, Sketching, and Critiquing},
editor = {Rex Hartson and Pardha Pyla},
booktitle = {The UX Book (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {293-325},
year = {2019},
isbn = {978-0-12-805342-3},
doi = {https://doi.org/10.1016/B978-0-12-805342-3.00014-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012805342300014X},
author = {Rex Hartson and Pardha Pyla},
keywords = {Design thinking, Immersion, Synthesis, Ideas, Ideation, Sketching, Critiquing, Ideation informers, Ideation catalysts, Ideation techniques, Rules of engagement},
abstract = {In this chapter we get into the UX design process, starting with generative design or design creation. The overarching objective of design creation is to formulate a plan for how the system will be structured to satisfy the ecological, interaction, and emotional needs of users. Compared to usage research (a study of things as they currently are), design (about lateral thinking and generating new ideas to make things better) is less procedural and more creative. Generative design is an intertwining of ideation (brainstorming), sketching (capturing and exploring design ideas visually), and critiquing (evaluation, review, and judgment). Ideation is collaborative, iterative, and exploratory, being informed via usage research data and models and design catalysts and is supported by a number of ideation techniques. Sketching is an essential embodied partner of ideation; you are not doing design if you are not sketching. A sketch is a conversation about design. Much of the work in design occurs within the ecological perspective, the interaction perspective, and/or the emotional perspective. The process works best if the team follows certain “rules of engagement.”}
}
@article{SISTLA20212464,
title = {Evaluating the performance of nature inspired algorithms using 52-bar steel truss subjected to dynamic load},
journal = {Materials Today: Proceedings},
volume = {38},
pages = {2464-2470},
year = {2021},
note = {International Conference & Exposition on Mechanical, Material and Manufacturing Technology (ICE3MT)},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.07.390},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320355000},
author = {Saiteja Sistla and J.S. {Kalyana Rama}},
keywords = {Moth flame optimizer, Salp Swarm Algorithm, Whale optimization algorithm, MATLAB, Steel truss},
abstract = {Linear programing problem revolutionized during the world war since it has helped the army to minimize the costs and increase the efficiency in battlefield. Since then optimization techniques have gained popularity in various fields like science & technology, biology, mathematics etc. Nature inspired algorithms mimic the nature’s behavior in order to achieve certain optimization objectives which can achieve productive results to complex problems. Classical algorithms like Genetic algorithm, Particle swarm optimization etc. are widely known but the accuracy of the solution for complex engineering problems is less. Performance evaluation of latest nature inspired algorithms i.e. Moth flame optimizer, Salp Swarm optimizer and Whale optimizer is carried out in the present study. These algorithms are proposed recently and they possess salient features like better convergence rate, avoiding the local optimum and robustness, which is the motivation behind choosing these algorithms. A 52-bar steel truss has been chosen for the present study to assess the performance of the chosen optimization techniques. The behavior of steel truss subjected to two different ground motions is also assessed using the three optimization techniques. A comparative study is done to assess the performance of the chosen techniques. MATLAB is adopted for the simulation of chosen problem statement. Based on the results it is observed that Mouth Flame Optimizer has better performance in terms of accuracy, convergence rate and computational time and is suggested for various types of mechanical and structural problems involving 52-bar trusses.}
}
@article{LIBERATORE2024103456,
title = {The ghosts of forgotten things: A study on size after forgetting},
journal = {Annals of Pure and Applied Logic},
volume = {175},
number = {8},
pages = {103456},
year = {2024},
issn = {0168-0072},
doi = {https://doi.org/10.1016/j.apal.2024.103456},
url = {https://www.sciencedirect.com/science/article/pii/S016800722400054X},
author = {Paolo Liberatore},
keywords = {Logical forgetting, Boolean minimization},
abstract = {Forgetting is removing variables from a logical formula while preserving the constraints on the other variables. In spite of reducing information, it does not always decrease the size of the formula and may sometimes increase it. This article discusses the implications of such an increase and analyzes the computational properties of the phenomenon. Given a propositional Horn formula, a set of variables and a maximum allowed size, deciding whether forgetting the variables from the formula can be expressed in that size is Dp-hard in Σ2p. The same problem for unrestricted CNF propositional formulae is D2p-hard in Σ3p.}
}
@article{VELOSO2023104997,
title = {Spatial synthesis for architectural design as an interactive simulation with multiple agents},
journal = {Automation in Construction},
volume = {154},
pages = {104997},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.104997},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523002571},
author = {Pedro Veloso and Ramesh Krishnamurti},
keywords = {Spatial synthesis, Interactive simulation, Artificial intelligence, Agent-based modeling, Multi-agent deep reinforcement learning},
abstract = {Motivated by reflection-in-action in architectural design, this article introduces a spatial synthesis artifact that relies on multi-agent reinforcement learning to address spatial goals with fine-grained control in a simulation. It relies on parameter sharing with proximal policy optimization and a parameterized reward function to train robust agent policies in random environments with random spatial problems. The agents are evaluated in three design cases: a house design with 12 agents in three sites, a museum with 18 agents in an interstitial urban site, and a speculative design of a housing complex with 96 agents on a large empty site. The policies performed well in all the cases and produced morphologically consistent solutions. However, in cases with a larger number of agents, the system largely benefited from a spring layout algorithm for the initialization. Future research will address more complex spatial synthesis problems and mechanisms for human-computer interaction.}
}
@article{HAHN2020363,
title = {Argument Quality in Real World Argumentation},
journal = {Trends in Cognitive Sciences},
volume = {24},
number = {5},
pages = {363-374},
year = {2020},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2020.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S1364661320300206},
author = {Ulrike Hahn},
keywords = {argument quality, logic, probability, rationality},
abstract = {The idea of resolving dispute through the exchange of arguments and reasons has been central to society for millennia. We exchange arguments as a way of getting at the truth in contexts as diverse as science, the court room, and our everyday lives. In democracies, political decisions should be negotiated through argument, not deception, or even worse, brute force. If argument is to lead to the truth or to good decisions, then some arguments must be better than others and ‘argument strength’ must have some meaningful connection with truth. Can argument strength be measured in a way that tracks an objective relationship with truth and not just mere persuasiveness? This article describes recent developments in providing such measures.}
}
@article{ZHU2024111294,
title = {Grey wolf optimizer based deep learning mechanism for music composition with data analysis},
journal = {Applied Soft Computing},
volume = {153},
pages = {111294},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111294},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624000681},
author = {Qian Zhu and Achyut Shankar and Carsten Maple},
keywords = {Music composition, LSTM, GWO, MIDI, Data analysis},
abstract = {Music composition using artificial intelligence has gained increasing research attention recently. However, existing methods often generate music that needs more coherence and authenticity. This paper proposes an evolutionary computation-based deep learning approach for music composition with data analysis. Specifically, we utilize long short-term memory (LSTM) networks for generating melodic sequences and adopt a grey wolf optimizer to optimize LSTM hyperparameters. The training data is first converted to musical instrument digital interface (MIDI) format for data analysis, and melody lines are extracted using a similarity matrix method. The MIDI data is then encoded for input into the LSTM networks. The generated music is evaluated using objective metrics like mean squared error and subjective methods, including surveys of music professionals. Comparisons made to benchmark algorithms like generative adversarial networks demonstrate the advantages of our approach in accurately capturing tone, rhythm, artistic conception, and other attributes of high-quality music. The proposed mechanism provides a practical framework for AI-based music generation while ensuring authenticity.}
}
@article{CARROLL200049,
title = {Use of student-constructed number stories in a reform-based curriculum},
journal = {The Journal of Mathematical Behavior},
volume = {19},
number = {1},
pages = {49-62},
year = {2000},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(00)00038-9},
url = {https://www.sciencedirect.com/science/article/pii/S0732312300000389},
author = {William M Carroll and Karen C Fuson and Ann Diamond},
keywords = {Reform-based curriculum, Number stories, Invented procedures},
abstract = {Twelve classes using the reform-based curriculum, Everyday Mathematics (EM), were observed early in first grade. The two lessons observed involved students generating and solving addition and subtraction number stories. In these lessons, teachers were directed to help students link these number stories to representations (pictures or objects) and equations. Because this curriculum emphasizes invented procedures and number sense, the lessons also call for whole-class discussions of students' solutions. Further, the curriculum assumes that teachers will build upon and extend the children's mathematical thinking, highlighting these alternative solution methods and supporting the students' explanations. Results show that students were successful at making up, telling, and solving number stories and used a range of solution methods, including the mathematical representations available in the classrooms. However, only about three-quarters of the teachers established explicit links between the stories and mathematical representations, with fewer than half representing the stories as numbers and equations. Although student-based explanations play an important role in helping children develop solution procedures with understanding, solution methods were only elicited in half of the classes observed, and multiple methods in one-fourth of the classes. Implications for reform curricula, especially how they might clarify new goals for teachers, are discussed.}
}
@article{COOPER200672,
title = {Definability as hypercomputational effect},
journal = {Applied Mathematics and Computation},
volume = {178},
number = {1},
pages = {72-82},
year = {2006},
note = {Special Issue on Hypercomputation},
issn = {0096-3003},
doi = {https://doi.org/10.1016/j.amc.2005.09.072},
url = {https://www.sciencedirect.com/science/article/pii/S0096300305008350},
author = {S. Barry Cooper},
keywords = {Computability, Definability, Hypercomputation},
abstract = {The classical simulation of physical processes using standard models of computation is fraught with problems. On the other hand, attempts at modelling real-world computation with the aim of isolating its hypercomputational content have struggled to convince. We argue that a better basic understanding can be achieved through computability theoretic deconstruction of those physical phenomena most resistant to classical simulation. From this we may be able to better assess whether the hypercomputational enterprise is proleptic computer science, or of mainly philosophical interest.}
}
@article{CARAMIA2022100040,
title = {Sustainable two stage supply chain management: A quadratic optimization approach with a quadratic constraint},
journal = {EURO Journal on Computational Optimization},
volume = {10},
pages = {100040},
year = {2022},
issn = {2192-4406},
doi = {https://doi.org/10.1016/j.ejco.2022.100040},
url = {https://www.sciencedirect.com/science/article/pii/S2192440622000168},
author = {Massimiliano Caramia and Giuseppe Stecca},
keywords = {Supply chain optimization, Green management, Successive linear approximations},
abstract = {Designing a supply chain to comply with environmental policy requires awareness of how work and/or production methods impact the environment and what needs to be done to reduce those environmental impacts and make the company more sustainable. This is a dynamic process that occurs at both the strategic and operational levels. However, being environmentally friendly does not necessarily mean improving the efficiency of the system at the same time. Therefore, when allocating a production budget in a supply chain that implements the green paradigm, it is necessary to figure out how to properly recover costs in order to improve both sustainability and routine operations, offsetting the negative environmental impact of logistics and production without compromising the efficiency of the processes to be executed. In this paper, we study the latter problem in detail, focusing on the CO2 emissions generated by the transportation from suppliers to production sites, and by the production activities carried out in each plant. We do this using a novel mathematical model that has a quadratic objective function and all linear constraints except one, which is also quadratic, and models the constraint on the budget that can be used for green investments caused by the increasing internal complexity created by large production flows in the production nodes of the supply network. To solve this model, we propose a multistart algorithm based on successive linear approximations. Computational results show the effectiveness of our proposal.}
}
@article{ROUSE201472,
title = {Human interaction with policy flight simulators},
journal = {Applied Ergonomics},
volume = {45},
number = {1},
pages = {72-77},
year = {2014},
note = {Systems Ergonomics/Human Factors},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2013.03.019},
url = {https://www.sciencedirect.com/science/article/pii/S0003687013000604},
author = {William B. Rouse},
keywords = {Computational modeling, Interactive visualization, Policy flight simulators},
abstract = {Policy flight simulators are designed for the purpose of exploring alternative management policies at levels ranging from individual organizations to national strategy. This article focuses on how such simulators are developed and on the nature of how people interact with these simulators. These interactions almost always involve groups of people rather than individuals, often with different stakeholders in conflict about priorities and courses of action. The ways in which these interactions are framed and conducted are discussed, as well as the nature of typical results.}
}
@incollection{MEY20065,
title = {Pragmatic Acts},
editor = {Keith Brown},
booktitle = {Encyclopedia of Language & Linguistics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {5-12},
year = {2006},
isbn = {978-0-08-044854-1},
doi = {https://doi.org/10.1016/B0-08-044854-2/00386-2},
url = {https://www.sciencedirect.com/science/article/pii/B0080448542003862},
author = {J.L. Mey},
keywords = {Affordance, common sense, extralinguistic acts, intention, intentionality, interactional situation, irony, power, pract, pragmatic acts, pragmeme, presupposition, situated action, situated context, situation, speech acting, speech acts, utterance},
abstract = {Recently, the established way of thinking about speech acts (in the tradition of Austin, Searle, Grice, and their followers) has undergone a remarkable change. From being an effort to represent human words in terms of what they ‘do’ (Austin), or how they can be used to produce ‘speech acts’ (Searle) or to generate ‘implicatures’ (Grice), the focus has shifted to the situation in which words are spoken and how this contributes to understanding the utterance, or even how the situation can predefine and to a degree determine what can be said. The upshot of these considerations is that we need a new theory, one that takes into account the inter- and transactional aspects of speech acting. This article proposes such a theory under the label of ‘pragmatic acts’ – acts that work not just by their wording but also by their being embedded in a situation in which humans act, with everything that humans bring to their interactional forum, including body movements, emotions, and so on.}
}
@article{SHAHRYARI2021101395,
title = {Energy and task completion time trade-off for task offloading in fog-enabled IoT networks},
journal = {Pervasive and Mobile Computing},
volume = {74},
pages = {101395},
year = {2021},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2021.101395},
url = {https://www.sciencedirect.com/science/article/pii/S1574119221000535},
author = {Om-Kolsoom Shahryari and Hossein Pedram and Vahid Khajehvand and Mehdi Dehghan TakhtFooladi},
keywords = {Internet of Things, Fog computing, Task offloading, Genetic algorithm, Particle swarm optimization, Resource allocation},
abstract = {In order to improve the quality of experience in executing computation-intensive tasks of real-time IoT applications in a fog-enabled IoT network, resource-constrained IoT devices can offload the tasks to resource-rich nearby fog nodes. It causes a reduction in energy consumption compared with local processing, although it extends task completion time due to communication latency. In this paper, we propose a task offloading scheme that optimizes task offloading decision, fog node selection, and computation resource allocation, investigating the trade-off between task completion time and energy consumption. Weighting coefficients of time and energy consumption are determined based on specific demands of the user and residual energy of devices’ battery. Accordingly, we formulate the task offloading problem as a mixed-integer nonlinear program (MINLP), which is NP-hard. A sub-optimal algorithm based on the hybrid of genetic algorithm and particle swarm optimization is designed to solve the formulated problem. Extensive simulations prove the convergence of the proposed algorithm and its superior performance in comparison with baseline schemes.}
}
@article{ADAMS2010324,
title = {Why we still need a mark of the cognitive},
journal = {Cognitive Systems Research},
volume = {11},
number = {4},
pages = {324-331},
year = {2010},
note = {Special Issue on Extended Mind},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2010.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041710000331},
author = {Frederick Adams},
keywords = {Causal spread, Cognitive, Cognitive process, Computation, Constitutes, Content, Coupling, Extended mind, Information processing, Parity principle, Representation, Semantics, Think},
abstract = {What makes a process a cognitive process? I’m not just asking for a list of cognitive processes, but for what makes an item on that list a cognitive process. Why should it be on the list? This is a question that has been ignored far too long in the domain of research calling itself cognitive science. It is time to give an answer and that is what I propose in this paper. I contrast my answer with others that have been given and defend the need against some claims in the literature that a mark of the cognitive is not needed.}
}
@article{DAWKINS2012331,
title = {Metaphor as a possible pathway to more formal understanding of the definition of sequence convergence},
journal = {The Journal of Mathematical Behavior},
volume = {31},
number = {3},
pages = {331-343},
year = {2012},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2012.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0732312312000156},
author = {Paul Christian Dawkins},
keywords = {Real analysis, Sequence convergence, Defining, Realistic Mathematics Education, Transition to advanced mathematical thinking},
abstract = {This study presents how the introduction of a metaphor for sequence convergence constituted an experientially real context in which an undergraduate real analysis student developed a property-based definition of sequence convergence. I use elements from Zandieh and Rasmussen's (2010) Defining as a Mathematical Activity framework to trace the transformation of the student's conception from a non-standard, personal concept definition rooted in the metaphor to a concept definition for sequence convergence compatible with the standard definition. This account of the development of the definition of sequence convergence differs from prior research in the sense that it began neither with examples or visual notions, nor with the statement of the formal definition. This study contributes to the Realistic Mathematics Education literature as it documents a student's progression through the definition-of and definition-for stages of mathematical activity in an interactive lecture classroom context.}
}
@article{BARON2019319,
title = {Machine Learning and Other Emerging Decision Support Tools},
journal = {Clinics in Laboratory Medicine},
volume = {39},
number = {2},
pages = {319-331},
year = {2019},
note = {Clinical Decision Support: Tools, Strategies, and Emerging Technologies},
issn = {0272-2712},
doi = {https://doi.org/10.1016/j.cll.2019.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S0272271219300101},
author = {Jason M. Baron and Danielle E. Kurant and Anand S. Dighe},
keywords = {Machine learning, Clinical decision support, Artificial intelligence, Knowledge discovery, Computational pathology}
}
@article{BRAUN2013400,
title = {Custom fabric ventures: An instructional resource in job costing for the introductory managerial accounting course},
journal = {Journal of Accounting Education},
volume = {31},
number = {4},
pages = {400-429},
year = {2013},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2013.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0748575113000511},
author = {Karen W. Braun},
keywords = {Job costing, Introductory managerial accounting, Active learning, Instructional resource},
abstract = {Job costing is a core foundational concept in the introductory managerial accounting course. The purpose of this instructional resource (IR) is to provide a thorough hands-on, active learning resource that will allow introductory students to experience a full set of accounting and management activities necessary to produce a job and assign production costs to it. For example, the IR requires students to analyze overhead costs, determine the optimal job size, schedule production, calculate the amount of materials to purchase, complete material requisitions, update raw materials records, analyze labor time records, complete a job cost record and address critical thinking questions. The IR was developed for use in a “flipped classroom” in which students work under the guidance of the instructor, but could alternatively be assigned as an unsupervised out-of-class assignment or on-line project. Since the IR was specifically developed as a learning tool for novice introductory managerial accounting students, adequate guidance is provided throughout the activity. However, to add realism and challenge students to think beyond the confines of simple mechanics, management and accounting issues are seeded throughout. Student feedback indicates that the IR not only helps students learn how a job costing system operates, but also helps students become aware of management decisions and accounting issues that impact the costs assigned to a job.}
}
@article{KIM2021188548,
title = {A primer on applying AI synergistically with domain expertise to oncology},
journal = {Biochimica et Biophysica Acta (BBA) - Reviews on Cancer},
volume = {1876},
number = {1},
pages = {188548},
year = {2021},
issn = {0304-419X},
doi = {https://doi.org/10.1016/j.bbcan.2021.188548},
url = {https://www.sciencedirect.com/science/article/pii/S0304419X21000457},
author = {Jason Kim and Rebecca Kusko and Benjamin Zeskind and Jenny Zhang and Renan Escalante-Chong},
abstract = {Background
The concurrent growth of large-scale oncology data alongside the computational methods with which to analyze and model it has created a promising environment for revolutionizing cancer diagnosis, treatment, prevention, and drug discovery. Computational methods applied to large datasets have accelerated the drug discovery process by reducing bottlenecks and widening the search space beyond what is experimentally tractable. As the research community gains understanding of the myriad genetic underpinnings of cancer via sequencing, imaging, screens, and more that are ingested, transformed, and modeled by top open-source machine learning and artificial intelligence tools readily available, the next big drug candidate might seem merely an “Enter” key away. Of course, the reality is more convoluted, but still promising.
Scope of review
We present methods to approach the process of building an AI model, with strong emphasis on the aspects of model development we believe to be crucial to success but that are not commonly discussed: diligence in posing questions, identifying suitable datasets and curating them, and collaborating closely with biology and oncology experts while designing and evaluating the model. Digital pathology, Electronic Health Records, and other data types outside of high-throughput molecular data are reviewed well by others and outside of the scope of this review. This review emphasizes the importance of considering the limitations of the datasets, computational methods, and our minds when designing AI models. For example, datasets can be biased towards areas of research interest, funding, and particular patient populations. Neural networks may learn representations and correlations within the data that are grounded not in biological phenomena, but statistical anomalies erroneously extracted from the training data. Researchers may mis-interpret or over-interpret the output, or design and evaluate the training process such that the resultant model generalizes poorly. Fortunately, awareness of the strengths and limitations of applying data analytics and AI to drug discovery enables us to leverage them carefully and insightfully while maximizing their utility. These applications when performed in close collaboration with domain experts, together with continuous critical evaluation, generation of new data to minimize known blind spots as they are found, and rigorous experimental validation, increases the success rate of the study. We will discuss applications including AI-assisted target identification, drug repurposing, patient stratification, and gene prioritization.
Major conclusions
Data analytics and AI have demonstrated capabilities to revolutionize cancer research, prevention, and treatment by maximizing our understanding and use of the expanding panoply of experimental data. However, to separate promise from true utility, computational tools must be carefully designed, critically evaluated, and constantly improved. Once that is achieved, a human-computer hybrid discovery process will outperform one driven by each alone.
General significance
This review highlights the challenges and promise of synergizing predictive AI models with human expertise towards greater understanding of cancer.}
}
@article{GACS19811,
title = {Causal nets or what is a deterministic computation?},
journal = {Information and Control},
volume = {51},
number = {1},
pages = {1-19},
year = {1981},
issn = {0019-9958},
doi = {https://doi.org/10.1016/S0019-9958(81)90058-9},
url = {https://www.sciencedirect.com/science/article/pii/S0019995881900589},
author = {Péter Gács and Leonid A. Levin},
abstract = {The network approach to computation is more direct and “physical” than the one based on some specific computing devices (like Turing machines). However, the size of a usual—e.g., Boolean—network does not reflect the complexity of computing the corresponding function, since a small network may be very hard to find even if it exists. A history of the work of a particular computing device can be described as a network satisfying some restrictions. The size of this network reflects the complexity of the problem, but the restrictions are usually somewhat arbitrary and even awkward. Causal nets are restricted only by determinism (causality) and locality of interaction. Their geometrical characteristics do reflect computational complexities. And various imaginary computer devices are easy to express in their terms. The elementarity of this concept may help bringing geometrical and algebraic (and maybe even physical) methods into the theory of computations. This hope is supported by the group-theoretical criterion given in this paper for computability from symmetrical initial configurations.}
}