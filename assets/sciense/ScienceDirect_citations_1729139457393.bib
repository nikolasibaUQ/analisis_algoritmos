@article{HUANG1993717,
title = {An investigation of gender differences in cognitive abilities among Chinese high school students},
journal = {Personality and Individual Differences},
volume = {15},
number = {6},
pages = {717-719},
year = {1993},
issn = {0191-8869},
doi = {https://doi.org/10.1016/0191-8869(93)90012-R},
url = {https://www.sciencedirect.com/science/article/pii/019188699390012R},
author = {Jiafen Huang},
abstract = {This study investigated gender differences in 11 cognitive tests from a sample of grade 11 students in Shanghai, China. Research found that the girls outperformed boys significantly on Word Knowledge and Word Span tasks, and also on a Computational Speed and Accuracy test. Boys outperformed girls only on the Paper Folding test. Factor based scores showed that girls were superior to boys on memory, and verbal composites, whereas boys were superior to girls on the spatial composites. No gender differences were found on the Mathematical Thinking test and other reasoning tests. The research findings seemed to suggest that where the social conditions were more uniform the gender differences on visual-spatial and mathematical reasoning skills would be smaller.}
}
@article{SIGAYRET2022104505,
title = {Unplugged or plugged-in programming learning: A comparative experimental study},
journal = {Computers & Education},
volume = {184},
pages = {104505},
year = {2022},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2022.104505},
url = {https://www.sciencedirect.com/science/article/pii/S0360131522000768},
author = {Kevin Sigayret and André Tricot and Nathalie Blanc},
keywords = {Elementary education, Improving classroom teaching, Programming and programming languages, Teaching/learning strategies},
abstract = {In recent years, computer programming has reappeared in school curricula with the aim of transmitting knowledge and skills beyond the simple ability to code. However, there are different ways of teaching this subject and very few experimental studies compare plugged-in and unplugged programming learning. The purpose of this study is to highlight the impact of plugged-in or unplugged learning on students' performance and subjective experience. To this end, we designed an experimental study with 217 primary school students divided into two groups and we measured their knowledge of computational concepts, ability to solve algorithmic problem, motivation toward the instruction, self-belief and attitude toward science. The programming sessions were designed to be similar between the two conditions, only the tools were different. Computers and Scratch software were used in the plugged-in group while the unplugged group used paper instructions, pictures, figurines and body movements instead. The results show better learning performance in the plugged-in group. Furthermore, although motivation dropped slightly in both groups, this drop was only significant in the unplugged condition. Gender also seems to be an important factor, as girls exhibit a lower post-test motivation and a lower willingness to pursue their practice in programming outside the school context. However, this effect on motivation was only observable in the plugged-in group which suggests that educational programming software may have a positive but gendered motivational impact.}
}
@article{NAVLAKHA201864,
title = {Network Design and the Brain},
journal = {Trends in Cognitive Sciences},
volume = {22},
number = {1},
pages = {64-78},
year = {2018},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2017.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S1364661317302000},
author = {Saket Navlakha and Ziv Bar-Joseph and Alison L. Barth},
abstract = {Neural circuits have evolved to accommodate similar information processing challenges as those faced by engineered systems. Here, we compare neural versus engineering strategies for constructing networks. During circuit development, synapses are overproduced and then pruned back over time, whereas in engineered networks, connections are initially sparse and are then added over time. We provide a computational perspective on these two different approaches, including discussion of how and why they are used, insights that one can provide the other, and areas for future joint investigation. By thinking algorithmically about the goals, constraints, and optimization principles used by neural circuits, we can develop brain-derived strategies for enhancing network design, while also stimulating experimental hypotheses about circuit development and function.}
}
@article{GEMMELL201720,
title = {Establishing the structures within populations of models},
journal = {Progress in Biophysics and Molecular Biology},
volume = {129},
pages = {20-24},
year = {2017},
note = {Validation of Computer Modelling},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2017.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0079610716300128},
author = {Philip M. Gemmell},
abstract = {As computational biology matures as a field, increasing attention is being paid to the relation of computational models to their target. One aspect of this is addressing how computational models can appropriately reproduce the variation seen in experimental data, with one solution being to use populations of models united by a common set of equations (the framework), with each individual member of the population (each model) possessing its own unique set of equation parameters. These model populations are then calibrated and validated against experimental data, and as a whole reproduce the experimentally observed variation. The primary focus of validation thus becomes the population, with the individual models' validation seemingly deriving from their membership of this population. The role of individual models within the population is not clear, with uncertainty regarding the relationship between individual models and the population they make up. This work examines the role of models within the population, how they relate to the population they make up, and how both can be said to be validated in this context.}
}
@article{ZHOU2022100001,
title = {Science in One Health: A new journal with a new approach},
journal = {Science in One Health},
volume = {1},
pages = {100001},
year = {2022},
issn = {2949-7043},
doi = {https://doi.org/10.1016/j.soh.2022.100001},
url = {https://www.sciencedirect.com/science/article/pii/S2949704322000014},
author = {Xiao-Nong Zhou and Marcel Tanner},
keywords = {One Health, Human health, Animal health, Ecosystem health, Research and implementation science},
abstract = {One Health recognizes the close links and interdependence among human health, animal health and environmental health. With the pandemic of COVID-19 and the risk of many emerging or reemerging infectious diseases of zoonotic nature as well as the spreading antimicrobial resistance, One Health has become one of top concerns globally, as it entails the essential global public health challenges from antimicrobial resistance over zoonoses, to climate change, food security and societal well-being. Research priorities in One Health include the study on interactions of human-animal-plants-nature ecology interface, systems thinking, integrated surveillance and response systems, and the overall One Health governance as part of the global health and sustainability governance. The now launched journal, Science in One Health, aims to be a resource platform that disseminates scientific evidence, knowledge, and tools on the One Health approaches and respective possible socio-ecological interventions. Thus, aims at providing fruitful exchanges of information and experience among researchers, and decision makers as well as public health actors.}
}
@article{BIRHANE2021100205,
title = {Algorithmic injustice: a relational ethics approach},
journal = {Patterns},
volume = {2},
number = {2},
pages = {100205},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100205},
url = {https://www.sciencedirect.com/science/article/pii/S2666389921000155},
author = {Abeba Birhane},
keywords = {justice, ethics, Afro-feminism, relational epistemology, data science, complex systems, enaction, embodiment, artificial intelligence, machine learning},
abstract = {Summary
It has become trivial to point out that algorithmic systems increasingly pervade the social sphere. Improved efficiency—the hallmark of these systems—drives their mass integration into day-to-day life. However, as a robust body of research in the area of algorithmic injustice shows, algorithmic systems, especially when used to sort and predict social outcomes, are not only inadequate but also perpetuate harm. In particular, a persistent and recurrent trend within the literature indicates that society's most vulnerable are disproportionally impacted. When algorithmic injustice and harm are brought to the fore, most of the solutions on offer (1) revolve around technical solutions and (2) do not center disproportionally impacted communities. This paper proposes a fundamental shift—from rational to relational—in thinking about personhood, data, justice, and everything in between, and places ethics as something that goes above and beyond technical solutions. Outlining the idea of ethics built on the foundations of relationality, this paper calls for a rethinking of justice and ethics as a set of broad, contingent, and fluid concepts and down-to-earth practices that are best viewed as a habit and not a mere methodology for data science. As such, this paper mainly offers critical examinations and reflection and not “solutions.”}
}
@article{BUCHBERGER2006470,
title = {Theorema: Towards computer-aided mathematical theory exploration},
journal = {Journal of Applied Logic},
volume = {4},
number = {4},
pages = {470-504},
year = {2006},
note = {Towards Computer Aided Mathematics},
issn = {1570-8683},
doi = {https://doi.org/10.1016/j.jal.2005.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S1570868305000716},
author = {Bruno Buchberger and Adrian Crǎciun and Tudor Jebelean and Laura Kovács and Temur Kutsia and Koji Nakagawa and Florina Piroi and Nikolaj Popov and Judit Robu and Markus Rosenkranz and Wolfgang Windsteiger},
keywords = {Mathematical assistant, Automated reasoning, Theory exploration, “Lazy Thinking”, Theorema},
abstract = {Theorema is a project that aims at supporting the entire process of mathematical theory exploration within one coherent logic and software system. This survey paper illustrates the style of Theorema-supported mathematical theory exploration by a case study (the automated synthesis of an algorithm for the construction of Gröbner Bases) and gives an overview on some reasoners and organizational tools for theory exploration developed in the Theorema project.}
}
@article{PARKER20161,
title = {Coastal planning should be based on proven sea level data},
journal = {Ocean & Coastal Management},
volume = {124},
pages = {1-9},
year = {2016},
issn = {0964-5691},
doi = {https://doi.org/10.1016/j.ocecoaman.2016.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0964569116300205},
author = {A. Parker and C.D. Ollier},
keywords = {Sea level, Measurements, Computations, Tide gauges, Coastal management},
abstract = {There are two related measures of sea level, the absolute sea level, which is the increase in the sea level in an absolute reference frame, and relative sea level, which is the increase in sea level recorded by tide gauges. The first measure is a rather abstract computation, far from being reliable, and is preferred by activists and politicians for no scientific reason. For local and global problems it is better to use local tide gauge data. Proper coastal management should be based on proved measurements of sea level. Tide gauges provide the most reliable measurements, and best data to assess the rate of change. We show as the naïve averaging of all the tide gauges included in the PSMSL surveys show “relative” rates of rise about +1.04 mm/year (570 tide gauges of any length). If we consider only 100 tide gauges with more than 80 years of recording the rise is only +0.25 mm/year. This naïve averaging has been stable and shows that the sea levels are slowly rising but not accelerating. We also show as the additional information provided by GPS and satellite altimetry is of very little help. Computations of “absolute” sea levels suffer from inaccuracies with errors larger than the estimated trends. The GPS is more reliable than satellite altimetry, but the accuracy of the estimation of the vertical velocity at GPS domes is still well above ±1 mm/year and the relative motion of tide gauges vs. GPS domes is mostly unassessed. The satellite altimetry returns a noisy signal so that a +3.2 mm/year trend is only achieved by arbitrary “corrections”. We conclude that if the sea levels are only oscillating about constant trends everywhere as suggested by the tide gauges, then the effects of climate change are negligible, and the local patterns may be used for local coastal planning without any need of purely speculative global trends based on emission scenarios. Ocean and coastal management should acknowledge all these facts. As the relative rates of rises are stable worldwide, coastal protection should be introduced only where the rate of rise of sea levels as determined from historical data show a tangible short term threat. As the first signs the sea levels will rise catastrophically within few years are nowhere to be seen, people should start really thinking about the warnings not to demolish everything for a case nobody knows will indeed happen.}
}
@incollection{JACOBLOPES202177,
title = {Chapter 5 - Assistant’s tools toward life cycle assessment},
editor = {Eduardo Jacob-Lopes and Leila Queiroz Zepka and Mariany Costa Deprá},
booktitle = {Sustainability Metrics and Indicators of Environmental Impact},
publisher = {Elsevier},
pages = {77-90},
year = {2021},
isbn = {978-0-12-823411-2},
doi = {https://doi.org/10.1016/B978-0-12-823411-2.00006-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234112000062},
author = {Eduardo Jacob-Lopes and Leila Queiroz Zepka and Mariany Costa Deprá},
keywords = {Theoretical approach, Sustainability metrics, Life cycle thinking, Social life cycle assessment, Life cycle costing, Life cycle sustainability assessment},
abstract = {This chapter aims to elucidate the main assistant’s tools created to assist in a global assessment of sustainability metrics and indicators. To this end, the chapter will provide a general review of the main assistant’s tools, considering the Life cycle thinking, social life cycle assessment, life cycle cost, and life cycle sustainability assessment tool. In addition, it guides some necessary criteria to be followed to apply each of these tools. Finally, this compilation of information strongly suggests, at the end of the chapter, the application of sensitivity analyses at the end of the process evaluations.}
}
@article{201119,
title = {Evolution of cognition might be down to brain chemistry},
journal = {New Scientist},
volume = {210},
number = {2806},
pages = {19},
year = {2011},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(11)60726-4},
url = {https://www.sciencedirect.com/science/article/pii/S0262407911607264},
abstract = {The prefrontal cortex, the “thinking” part of our brain, has a radically different chemical balance to that of chimps and macaques}
}
@article{GAGNE201889,
title = {When planning to survive goes wrong: predicting the future and replaying the past in anxiety and PTSD},
journal = {Current Opinion in Behavioral Sciences},
volume = {24},
pages = {89-95},
year = {2018},
note = {Survival circuits},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2018.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S2352154618300305},
author = {Christopher Gagne and Peter Dayan and Sonia J Bishop},
abstract = {We increase our probability of survival and wellbeing by minimizing our exposure to rare, extremely negative events. In this article, we examine the computations used to predict and avoid such events and to update our models of the world and action policies after their occurrence. We also consider how these computations might go wrong in anxiety disorders and Post Traumatic Stress Disorder (PTSD). We review evidence that anxiety is linked to increased simulations of the future occurrence of high cost negative events and to elevated estimates of the probability of occurrence of such events. We also review psychological theories of PTSD in the light of newer, computational models of updating through replay and simulation. We consider whether pathological levels of re-experiencing symptomatology might reflect problems reconciling the traumatic outcome with overly optimistic priors or difficulties terminating off-line simulation focused on negative events and over-generalization to states sharing features with those antecedent to the trauma.}
}
@article{CUI2024101074,
title = {AI-enhanced collective intelligence},
journal = {Patterns},
pages = {101074},
year = {2024},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2024.101074},
url = {https://www.sciencedirect.com/science/article/pii/S2666389924002332},
author = {Hao Cui and Taha Yasseri},
keywords = {AI, collective intelligence, hybrid intelligence, multi-agent systems, human-machine networks, human-machine intelligence},
abstract = {Summary
Current societal challenges exceed the capacity of humans operating either alone or collectively. As AI evolves, its role within human collectives will vary from an assistive tool to a participatory member. Humans and AI possess complementary capabilities that, together, can surpass the collective intelligence of either humans or AI in isolation. However, the interactions in human-AI systems are inherently complex, involving intricate processes and interdependencies. This review incorporates perspectives from complex network science to conceptualize a multilayer representation of human-AI collective intelligence, comprising cognition, physical, and information layers. Within this multilayer network, humans and AI agents exhibit varying characteristics; humans differ in diversity from surface-level to deep-level attributes, while AI agents range in degrees of functionality and anthropomorphism. We explore how agents’ diversity and interactions influence the system’s collective intelligence and analyze real-world instances of AI-enhanced collective intelligence. We conclude by considering potential challenges and future developments in this field.}
}
@article{INIGUEZLOMELI2024105106,
title = {A hardware architecture for single and multiple ellipse detection using genetic algorithms and high-level synthesis tools},
journal = {Microprocessors and Microsystems},
volume = {111},
pages = {105106},
year = {2024},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2024.105106},
url = {https://www.sciencedirect.com/science/article/pii/S0141933124001017},
author = {Francisco J. Iñiguez-Lomeli and Carlos H. Garcia-Capulin and Horacio Rostro-Gonzalez},
keywords = {Ellipse detection, Genetic algorithm, System-on-a-Chip (SoC), High-level synthesis (HLS), Hardware implementation, FPGA},
abstract = {Ellipse detection techniques are often developed and validated in software environments, neglecting the critical consideration of computational efficiency and resource constraints prevalent in embedded systems. Furthermore, programmable logic devices, notably Field Programmable Gate Arrays (FPGAs), have emerged as indispensable assets for enhancing performance and expediting various processing applications. In the realm of computational efficiency, hardware implementations have the flexibility to tailor the required arithmetic for various applications using fixed-point representation. This approach enables faster computations while upholding adequate accuracy, resulting in reduced resource and energy consumption compared to software applications that rely on higher clock speeds, which often lead to increased resource and energy consumption. Additionally, hardware solutions provide portability and are suitable for resource-constrained and battery-powered applications. This study introduces a novel hardware architecture in the form of an intellectual property core that harnesses the capabilities of a genetic algorithm to detect single and multi ellipses in digital images. In general, genetic algorithms have been demonstrated to be an alternative that shows better results than those based on traditional methods such as the Hough Transform and Random Sample Consensus, particularly in terms of accuracy, flexibility, and robustness. Our genetic algorithm randomly takes five edge points as parameters from the image tested, creating an individual treated as a potential candidate ellipse. The fitness evaluation function determines whether the candidate ellipse truly exists in the image space. The core is designed using Vitis High-Level Synthesis (HLS), a powerful tool that converts C or C＋＋functions into Register-Transfer Level (RTL) code, including VHDL and Verilog. The implementation and testing of the ellipse detection system were carried out on the PYNQ-Z1, a cost-effective development board housing the Xilinx Zynq-7000 System-on-Chip (SoC). PYNQ, an open-source framework, seamlessly integrates programmable logic with a dual-core ARM Cortex-A9 processor, offering the flexibility of Python programming for the onboard SoC processor. The experimental results, based on synthetic and real images, some of them with the presence of noise processed by the developed ellipse detection system, highlight the intellectual property core’s exceptional suitability for resource-constrained embedded systems. Notably, it achieves remarkable performance and accuracy rates, consistently exceeding 99% in most cases. This research aims to contribute to the advancement of hardware-accelerated ellipse detection, catering to the demanding requirements of real-time applications while minimizing resource consumption.}
}
@article{YOSHIOKA2024114985,
title = {An escort replicator dynamic with a continuous action space and its application to resource management},
journal = {Chaos, Solitons & Fractals},
volume = {185},
pages = {114985},
year = {2024},
issn = {0960-0779},
doi = {https://doi.org/10.1016/j.chaos.2024.114985},
url = {https://www.sciencedirect.com/science/article/pii/S096007792400537X},
author = {Hidekazu Yoshioka},
keywords = {Evolutionary game, Escort replicator dynamic, Kaniadakis escort function, Numerical computation, Application to sustainable resource management},
abstract = {The escort replicator dynamic (ERD) is a version of the replicator dynamic in evolutionary games where the utility-driven decision-making process is modulated due to the information costs to be paid by players. The escort function as a coefficient to distort the decision-making determines the behavior of solutions to the ERD, whereas its investigations are still not sufficient. Particularly, the ERD was investigated in finite-action settings in the previous studies, while that with a continuum of actions has not been studied well. In this paper, we formulate and analyze the ERD with a continuum of actions represented by a bounded interval. Our ERD is a partial integro-differential equation whose well-posedness is nontrivial because of specific nonlocal terms arising from the escort function. The Kaniadakis escort function is chosen as a major example of the escort function, with which we obtain the unique existence of solutions to the ERD. We also discuss cases with the other escort functions, such as the power and constant ones, and suggest that the growth and regularity behaviors of the escort function are crucial. Finally, we computationally apply the ERD to problems related to sustainable environmental and resource management.}
}
@article{CHANG2023101823,
title = {Stakeholder requirement evaluation of smart industrial service ecosystem under Pythagorean fuzzy environment for complex industrial contexts: A case study of renewable energy park},
journal = {Advanced Engineering Informatics},
volume = {55},
pages = {101823},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101823},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622002816},
author = {Yuan Chang and Xinguo Ming and Zhihua Chen and Tongtong Zhou and Xiaoqiang Liao and Wenyan Song},
keywords = {Smart industrial product-service system (IPS), Requirement evaluation, Service ecosystem, Pythagorean fuzzy sets, Multi-criteria decision making, Viable systems model},
abstract = {This study focuses on ways to systematically evaluate stakeholder requirements when developing a smart industrial service ecosystem (SISE) in a complex industrial context. The SISE development requires considering the service requirement from both the complex industrial context and service ecosystem manners. This study proposes a systematic framework for stakeholder requirement evaluation in SISE. The first part of the framework is the industrial context-viable system model with ecological thinking (IC-VESM) to elicit the service requirements for the SISE, which facilitates a systematic analysis of the service value proposition and service requirement elicitation in the operational lifecycle of an entire industrial context. This second part of the framework proposes a method for evaluating service requirements that is both feasible and systematic. This is achieved by combining the Fuzzy Kano and AHP methods in a Pythagorean fuzzy (PF) environment. The PF Kano computes the categories and determines the weights of service requirements from a consumer perspective, while the PF AHP hierarchically analyzes the service requirements and provides pairwise comparison paths for design experts. Finally, an illustrative case study in a renewable energy context was used to demonstrate the feasibility and effectiveness of the methodology. The proposed theoretical model provides more reliable and systematic outcomes than traditional methods when eliciting service requirements and evaluating complex smart industrial service solutions. The study has practical implications by providing useful insights for companies to recognize key smart service requirements in complex industrial contexts and to improve sustainable development.}
}
@article{EDLA2015254,
title = {Is heart rate variability better than routine vital signs for prehospital identification of major hemorrhage?},
journal = {The American Journal of Emergency Medicine},
volume = {33},
number = {2},
pages = {254-261},
year = {2015},
issn = {0735-6757},
doi = {https://doi.org/10.1016/j.ajem.2014.11.046},
url = {https://www.sciencedirect.com/science/article/pii/S073567571400881X},
author = {Shwetha Edla and Andrew T. Reisner and Jianbo Liu and Victor A. Convertino and Robert Carter and Jaques Reifman},
abstract = {Objective
During initial assessment of trauma patients, metrics of heart rate variability (HRV) have been associated with high-risk clinical conditions. Yet, despite numerous studies, the potential of HRV to improve clinical outcomes remains unclear. Our objective was to evaluate whether HRV metrics provide additional diagnostic information, beyond routine vital signs, for making a specific clinical assessment: identification of hemorrhaging patients who receive packed red blood cell (PRBC) transfusion.
Methods
Adult prehospital trauma patients were analyzed retrospectively, excluding those who lacked a complete set of reliable vital signs and a clean electrocardiogram for computation of HRV metrics. We also excluded patients who did not survive to admission. The primary outcome was hemorrhagic injury plus different PRBC transfusion volumes. We performed multivariate regression analysis using HRV metrics and routine vital signs to test the hypothesis that HRV metrics could improve the diagnosis of hemorrhagic injury plus PRBC transfusion vs routine vital signs alone.
Results
As univariate predictors, HRV metrics in a data set of 402 subjects had comparable areas under receiver operating characteristic curves compared with routine vital signs. In multivariate regression models containing routine vital signs, HRV parameters were significant (P < .05) but yielded areas under receiver operating characteristic curves with minimal, nonsignificant improvements (+0.00 to +0.05).
Conclusions
A novel diagnostic test should improve diagnostic thinking and allow for better decision making in a significant fraction of cases. Our findings do not support that HRV metrics add value over routine vital signs in terms of prehospital identification of hemorrhaging patients who receive PRBC transfusion.}
}
@article{SILVA2017137,
title = {Evaluating the usefulness of the structural accessibility layer for planning practice – Planning practitioners’ perception},
journal = {Transportation Research Part A: Policy and Practice},
volume = {104},
pages = {137-149},
year = {2017},
issn = {0965-8564},
doi = {https://doi.org/10.1016/j.tra.2017.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S0965856417304755},
author = {Cecília Silva and Tiago Patatas and Ana Amante},
keywords = {Accessibility instrument, Implementation gap, Planning practice, Usefulness in practice},
abstract = {There has been a growing attention on accessibility concepts from both planning practice and research recognising their relevance in understanding the evolution of urban areas. However, despite the large number of accessibility measures available in the literature, they are not widely used to support urban planning practices. Much has been said about the implementation gap of Planning Support Systems with a significant attention paid to usability and more recently to the usefulness of Accessibility Instruments. The paper aims to assess the usefulness of a specific accessibility instrument – the Structural Accessibility Layer (SAL) – and by doing so exploring the strengths of accessibility instruments holding similar characteristics. To this end, we follow a multidimensional assessment framework under development in the Planning Support System literature. This paper explores the main findings of a workshop bringing together local planning practitioners and the developers of the SAL in an experiment using the SAL. The assessment of usefulness of SAL identified the instrument’s strengths with regard to insight into participants’ assumptions, communication, commitment and development of shared language. Regardless, the low fit between planning concerns of participants (in this case study context) and of the SAL seemed to limit its potential use in practice and as such undermines the strengths identified in the usefulness assessment. The assessment developed here only partially confirmed objectives and purposes defined for the SAL. Results confirm the usefulness of the SAL as diagnosis tool, however, the ability of the SAL to contribute to a joint thinking of land use and transport constraints on mobility was not confirmed. Finally, this research raises questions on the role of PSS in changing strategic thinking in planning and how this might conflict with the current PSS research concern in improving usefulness of tools.}
}
@article{KNIGHT20061084,
title = {‘When I first came here, I thought medicine was black and white’: Making sense of medical students’ ways of knowing},
journal = {Social Science & Medicine},
volume = {63},
number = {4},
pages = {1084-1096},
year = {2006},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2006.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S0277953606000451},
author = {Lynn Valerie Knight and Karen Mattick},
keywords = {Medical training, Professional knowledge, Epistemology, Evidence-based medicine, United Kingdom},
abstract = {Personal beliefs about what knowledge is and how we understand, integrate and apply knowledge (known as personal epistemologies) are entrenched in the process of decision-making. Evidence-based medicine in all its forms brings with it the need for an ever more sophisticated appreciation of individual patients’ perspectives and ‘scientific’ perspectives within the clinical encounter. However, current theoretical perspectives on personal epistemology focus more on scientific ways of knowing where knowledge is abstracted and logical. We conducted semi-structured interviews to investigate medical students’ personal epistemological thinking towards the end of their second year of training at a new medical school in the South West of England. Whilst responses were varied, students appeared to express predominantly simplistic levels of epistemological thinking according to current developmental models of personal epistemology. However, the process of professional identity formation together with epistemological thinking brought together both scientific and experiential ways of knowing in a way that has largely been ignored by current theorists in the domain of personal epistemology.}
}
@article{CALUDE2023844,
title = {What perceptron neural networks are (not) good for?},
journal = {Information Sciences},
volume = {621},
pages = {844-857},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.11.083},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522013743},
author = {Cristian S. Calude and Shahrokh Heidari and Joseph Sifakis},
keywords = {Perceptrons, Sensitive and robust functions, Quantum computing},
abstract = {Perceptron Neural Networks (PNNs) are essential components of intelligent systems because they produce efficient solutions to problems of overwhelming complexity for conventional computing methods. Many papers show that PNNs can approximate a wide variety of functions, but comparatively, very few discuss their limitations and the scope of this paper. To this aim, we define two classes of Boolean functions – sensitive and robust –, and prove that an exponentially large set of sensitive functions are exponentially difficult to compute by multi-layer PNNs (hence incomputable by single-layer PNNs). A comparatively large set of functions in the second one, but not all, are computable by single-layer PNNs. Finally, we used polynomial threshold PNNs to compute all Boolean functions with quantum annealing and present in detail a QUBO computation on the D-Wave Advantage. These results confirm that the successes of PNNs, or lack of them, are in part determined by properties of the learned data sets and suggest that sensitive functions may not be (efficiently) computed by PNNs.}
}
@article{LI20234116,
title = {A call for caution in the era of AI-accelerated materials science},
journal = {Matter},
volume = {6},
number = {12},
pages = {4116-4117},
year = {2023},
issn = {2590-2385},
doi = {https://doi.org/10.1016/j.matt.2023.10.027},
url = {https://www.sciencedirect.com/science/article/pii/S2590238523005283},
author = {Kangming Li and Edward Kim and Yao Fehlis and Daniel Persaud and Brian DeCost and Michael Greenwood and Jason Hattrick-Simpers},
abstract = {It is safe to state that the field of matter has successfully entered the fourth paradigm, where machine learning and artificial intelligence (AI) are universally seen as useful, if not truly intelligent. AI’s utilization is near-ubiquitous from the prediction of novel materials to reducing computational overhead for material simulations; its value has been demonstrated time and again by both theorists and experimentalists. There is, however, a worrying trend toward large datasets and overparameterized models being all we need to accelerate science through accurate and robust machine learning systems.}
}
@article{GARCIACAIRASCO2021107930,
title = {Searching for a paradigm shift in the research on the epilepsies and associated neuropsychiatric comorbidities. From ancient historical knowledge to the challenge of contemporary systems complexity and emergent functions},
journal = {Epilepsy & Behavior},
volume = {121},
pages = {107930},
year = {2021},
note = {NEWroscience 2018},
issn = {1525-5050},
doi = {https://doi.org/10.1016/j.yebeh.2021.107930},
url = {https://www.sciencedirect.com/science/article/pii/S1525505021001645},
author = {Norberto Garcia-Cairasco and Guilherme Podolsky-Gondim and Julian Tejada},
keywords = {Ancestral knowledge, Superstitious versus scientific knowledge, Epilepsies and neuropsychiatric comorbidities, Clinical semiology and neurosurgery methods, experimental and computational modeling, Complexity and emergent properties},
abstract = {In this review, we will discuss in four scenarios our challenges to offer possible solutions for the puzzle associated with the epilepsies and neuropsychiatric comorbidities. We need to recognize that (1) since quite old times, human wisdom was linked to the plural (distinct global places/cultures) perception of the Universe we are in, with deep respect for earth and nature. Plural ancestral knowledge was added with the scientific methods; however, their joint efforts are the ideal scenario; (2) human behavior is not different than animal behavior, in essence the product of Darwinian natural selection; knowledge of animal and human behavior are complementary; (3) the expression of human behavior follows the same rules that complex systems with emergent properties, therefore, we can measure events in human, clinical, neurobiological situations with complexity systems’ tools; (4) we can use the semiology of epilepsies and comorbidities, their neural substrates, and potential treatments (including experimental/computational modeling, neurosurgical interventions), as a source and collection of integrated big data to predict with them (e.g.: machine/deep learning) diagnosis/prognosis, individualized solutions (precision medicine), basic underlying mechanisms and molecular targets. Once the group of symptoms/signals (with a myriad of changing definitions and interpretations over time) and their specific sequences are determined, in epileptology research and clinical settings, the use of modern and contemporary techniques such as neuroanatomical maps, surface electroencephalogram and stereoelectroencephalography (SEEG) and imaging (MRI, BOLD, DTI, SPECT/PET), neuropsychological testing, among others, are auxiliary in the determination of the best electroclinical hypothesis, and help design a specific treatment, usually as the first attempt, with available pharmacological resources. On top of ancient knowledge, currently known and potentially new antiepileptic drugs, alternative treatments and mechanisms are usually produced as a consequence of the hard, multidisciplinary, and integrated studies of clinicians, surgeons, and basic scientists, all over the world. The existence of pharmacoresistant patients, calls for search of other solutions, being along the decades the surgeries the most common interventions, such as resective procedures (i.e., selective or standard lobectomy, lesionectomy), callosotomy, hemispherectomy and hemispherotomy, added by vagus nerve stimulation (VNS), deep brain stimulation (DBS), neuromodulation, and more recently focal minimal or noninvasive ablation. What is critical when we consider the pharmacoresistance aspect with the potential solution through surgery, is still the pursuit of localization-dependent regions (e.g.: epileptogenic zone (EZ)), in order to decide, no matter how sophisticated are the brain mapping tools (EEG and MRI), the size and location of the tissue to be removed. Mimicking the semiology and studying potential neural mechanisms and molecular targets – by means of experimental and computational modeling – are fundamental steps of the whole process. Concluding, with the conjunction of ancient knowledge, coupled to critical and creative contemporary, scientific (not dogmatic) clinical/surgical, and experimental/computational contributions, a better world and of improved quality of life can be offered to the people with epilepsy and neuropsychiatric comorbidities, who are still waiting (as well as the scientists) for a paradigm shift in epileptology, both in the Basic Science, Computational, Clinical, and Neurosurgical Arenas. This article is part of the Special Issue “NEWroscience 2018”.}
}
@incollection{AKAL202471,
title = {Chapter Four - AI methods in microbial metabolite determination},
editor = {Akanksha Srivastava and Vaibhav Mishra},
series = {Methods in Microbiology},
publisher = {Academic Press},
volume = {55},
pages = {71-85},
year = {2024},
booktitle = {Artificial Intelligence in Microbiology: Scope and Challenges Volume 1},
issn = {0580-9517},
doi = {https://doi.org/10.1016/bs.mim.2024.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0580951724000023},
author = {H. Ceren Akal and Rumeysa Nur Kara-Aktaş and Sebnem Ozturkoglu-Budak},
keywords = {Metabolite, Microorganism-derived, Computational, Artificial intelligence},
abstract = {The multitude of microorganism species and the amount of data requiring examination is increasing day by day, which has made it very difficult to make informative determinations and analysis to be conducted by human labour. Artificial intelligence (AI) applications are crucial in mitigating these difficulties. AI is a multidisciplinary field that tries to imitate human-like abilities through learning, analysing, problem-solving and interpretation via digital systems. It can take part in many fields where human labour is required. It is widely used in various scientific disciplines and industries, including biotechnology, microbiology, medicine, etc. Machine learning, a subbranch of AI, is one of the most frequently used auxiliary methods. Critical topics are examined rapidly and meaningfully via machine-learning such as drug production, microbial detection, antimicrobial resistance, vaccine predictions, and disease diagnoses. The aim of this chapter is to highlight the relevance of computational methods for the determination of microbial metabolites which are mainly described in literatures. These computational methods are related with the advanced AI tools of data/genome mining, multivariate data analysis, molecular networking, mathematical modelling, and optimization. These novel methods create new perspectives to the isolation and/or determination of microbial metabolites which are unwanted or essential to human health.}
}
@incollection{MUBAYI2017249,
title = {Chapter 10 - Computational Modeling Approaches Linking Health and Social Sciences: Sensitivity of Social Determinants on the Patterns of Health Risk Behaviors and Diseases},
editor = {Arni S.R. {Srinivasa Rao} and Saumyadipta Pyne and C.R. Rao},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {36},
pages = {249-304},
year = {2017},
booktitle = {Disease Modelling and Public Health, Part A},
issn = {0169-7161},
doi = {https://doi.org/10.1016/bs.host.2017.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0169716117300172},
author = {Anuj Mubayi},
keywords = {Health risk behaviors, Dynamic models, Data mining, Sensitivity and uncertainty analysis, Ecological models, Social influences},
abstract = {Developing health promotion programs that support healthy lifestyle behaviors require comprehensive understanding of mechanisms that drive such complex social systems. Policy makers can use models and theories to guide this process at the individuals, groups, and communities levels. Individuals can have multiple risky health behaviors including physical inactivity, unhealthy diets, smoking, and alcohol drinking that are often shaped by social and ecological factors. Collective understanding of these factors can provide ability to design and evaluate intervention programs that can change unhealthy or risky behaviors over long period of time. However, it is overwhelming task to optimize intervention based on only empirical and/or cross-sectional studies. Effective long lasting intervention needs a thorough understanding of the role of social and environmental mechanisms at multiple scales on the dynamics of health behaviors. Recent mathematical and computational methods developed in other fields, such as epidemiology and finance, can provide systematic and in-depth understanding of mechanisms. However, the use of such methods in social and behaviors sciences have been limited. In this chapter, some real life working examples of social health behaviors problems are provided which uses some cutting edge methods from dynamical systems and data mining to uncertainty quantification.}
}
@article{CRUTCHFIELD199411,
title = {The calculi of emergence: computation, dynamics and induction},
journal = {Physica D: Nonlinear Phenomena},
volume = {75},
number = {1},
pages = {11-54},
year = {1994},
issn = {0167-2789},
doi = {https://doi.org/10.1016/0167-2789(94)90273-9},
url = {https://www.sciencedirect.com/science/article/pii/0167278994902739},
author = {James P. Crutchfield},
abstract = {Defining structure and detecting the emergence of complexity in nature are inherently subjective, though essential, scientific activities. Despite the difficulties, these problems can be analyzed in terms of how model-building observers infer from measurements the computational capabilities embedded in nonlinear processes. An observer's notion of what is ordered, what is random, and what is complex in its environment depends directly on its computational resources: the amount of raw measurement data, of memory, and of time available for estimation and inference. The discovery of structure in an environment depends more critically and subtlely though on how those resources are organized. The descriptive power of the observer's chosen (or implicit) computational model class, for example, can be an overwhelming determinant in finding regularity in data. This paper presents an overview of an inductive framework-hierarchical ϵ-machine reconstruction—in which the emergence of complexity is associated with the innovation of new computational model classes. Complexity metrics for detecting structure and quantifying emergence, along with an analysis of the constraints on the dynamics of innovation, are outlined. Illustrative examples are drawn from the onset of unpredictability in nonlinear systems, finitary nondeterministic processes, and cellular automata pattern recognition. They demonstrate how finite inference resources drive the innovation of new structures and so lead to the emergence of complexity.}
}
@article{SPARAPANI2023102186,
title = {Factors associated with classroom participation in preschool through third grade learners on the autism spectrum},
journal = {Research in Autism Spectrum Disorders},
volume = {105},
pages = {102186},
year = {2023},
issn = {1750-9467},
doi = {https://doi.org/10.1016/j.rasd.2023.102186},
url = {https://www.sciencedirect.com/science/article/pii/S1750946723000867},
author = {Nicole Sparapani and Nancy Tseng and Laurel Towers and Sandy Birkeneder and Sana Karimi and Cameron J. Alexander and Johanna Vega Garcia and Taffeta Wood and Amanda Dimachkie Nunnally},
keywords = {Autism, Instructional opportunities, Mathematical tasks, Teacher language, Active engagement, Spontaneous communication},
abstract = {Background
Access to mathematics instruction that involves opportunities for critical thinking and procedural fluency promotes mathematics learning. Studies have outlined effective strategies for teaching mathematics to children on the autism spectrum, however, the focus of these interventions often represent a narrow set of mathematical skills and concepts centered on procedural learning without linking ideas to underlying concepts.
Methods
This study utilized classroom video observations to evaluate the variability in and nature of mathematical learning opportunities presented to 76 autistic students within 49 preschool–3rd grade general and special education learning contexts. We examined teacher instructional practices and student participation across 109 mathematical tasks within larger mathematics lessons.
Results
Students were most often presented with mathematical tasks that required low-level cognitive demand, such as tasks focusing on rote memorization and practicing predetermined steps to solve basic algorithms. Furthermore, the nature of the mathematical task was linked with the language that teachers used, and this in turn, was associated with students’ participation within the learning opportunity.
Conclusions
Our findings indicate that features of talk within specific types of mathematical tasks, including math-related talk and responsive language, were associated with increased student active engagement and spontaneous communication. The knowledge gained from this study contributes to the development of optimized instructional practices for school-aged children on the autism spectrum—information that could be used to prepare both preservice and in-service teachers.}
}
@article{RULE2020900,
title = {The Child as Hacker},
journal = {Trends in Cognitive Sciences},
volume = {24},
number = {11},
pages = {900-915},
year = {2020},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2020.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S1364661320301741},
author = {Joshua S. Rule and Joshua B. Tenenbaum and Steven T. Piantadosi},
keywords = {Learning and cognitive development, Language of thought, Hacking, Computational modeling, Program induction},
abstract = {The scope of human learning and development poses a radical challenge for cognitive science. We propose that developmental theories can address this challenge by adopting perspectives from computer science. Many of our best models treat learning as analogous to computer programming because symbolic programs provide the most compelling account of sophisticated mental representations. We specifically propose that children’s learning is analogous to a particular style of programming called hacking, making code better along many dimensions through an open-ended set of goals and activities. By contrast to existing theories, which depend primarily on local search and simple metrics, this view highlights the many features of good mental representations and the multiple complementary processes children use to create them.}
}
@article{KATTERFELDT201872,
title = {Physical computing with plug-and-play toolkits:Key recommendations for collaborative learning implementations},
journal = {International Journal of Child-Computer Interaction},
volume = {17},
pages = {72-82},
year = {2018},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2018.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212868917300351},
author = {Eva-Sophie Katterfeldt and Mutlu Cukurova and Daniel Spikol and David Cuartielles},
keywords = {Collaborative learning, Education, Motivation, Physical computing, Programming, Toolkit},
abstract = {Physical computing toolkits have long been used in educational contexts to learn about computational concepts by engaging in the making of interactive projects. This paper presents a comprehensive toolkit that can help educators teach programming with an emphasis on collaboration, and provides suggestions for its effective pedagogical implementation. The toolkit comprises the Talkoo kit with physical computing plug-and-play modules and a visual programming environment. The key suggestions are inspired by the results of the evaluation studies which show that children (aged 14–18 in a sample group of 34 students) are well motivated when working with the toolkit but lack confidence in the kit’s support for collaborative learning. If the intention is to move beyond tools and code in computer education to community and context, thus encouraging computational participation, collaboration should be considered as a key aspect of physical computing activities. Our approach expands the field of programming with physical computing for teenage children with a focus on empowering teachers and students with not only a kit but also its appropriate classroom implementation for collaborative learning.}
}
@article{LIN2024200448,
title = {Maximizing the Spread of Information through Content Optimization},
journal = {Intelligent Systems with Applications},
pages = {200448},
year = {2024},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2024.200448},
url = {https://www.sciencedirect.com/science/article/pii/S2667305324001224},
author = {Lei Lin and Yihua Du and Shibo Zhao and Wenkang Jiang and Qirui Tang and Li Xu},
keywords = {Computational social science, human-in-the-loop, system simulation and optimization, computational journalism, computational advertisement},
abstract = {As data-driven prediction models advance, an increasing number of people are enjoying news personalized to their interests. The primary problem such recommendation models solve is to precisely match information with users and, in so doing, ensure that news spreads with greater efficiency. However, these techniques only help the media platform; they do not help those who produce the news. Hence, we devised a propagation framework based on a human-in-the-loop simulation that helps content authors maximize the spread of their messages through social networks. The framework works by acting on feedback provided by the simulation model. Additionally, the spread of information is formulated as a multi-objective optimization problem in which propagation is data-driven and simulated with machine learning techniques that leverage data on the historical behaviors of users. We additionally describe an implementation for this framework as an example of how the framework might be used in real life. On the practical side, the implementation uses text data from a blog to simulate the message's propagation, while, from a technical point of view, the multi-objective optimization problem is divided into an information retrieval problem and an integer programming problem, the results of which are fed back into the content editor as content operation strategies. A case study with the Sina Weibo microblog site not only validates the framework but also provides practitioners with insights into how to maximize the spread of information through social networking platforms. The results show that the proposed propagation framework is capable of increasing retweets by 7.9575%. As an interesting aside, our experiments also show that the Weibo retweet lottery is both popular and a highly effective mechanism for increasing reposts.}
}
@article{LOPEZSILVA202446,
title = {‘Are these my thoughts?’: A 20-year prospective study of thought insertion, thought withdrawal, thought broadcasting, and their relationship to auditory verbal hallucinations},
journal = {Schizophrenia Research},
volume = {265},
pages = {46-57},
year = {2024},
note = {Hallucinations: Neurobiology and Patient Experience},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2022.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0920996422002778},
author = {Pablo López-Silva and Martin Harrow and Thomas H. Jobe and Michele Tufano and Helen Harrow and Cherise Rosen},
keywords = {Schizophrenia, Psychosis, Thought insertion, Thought withdrawal, Thought broadcasting, Auditory-verbal hallucinations},
abstract = {The co-occurrence of delusions and other symptoms at the onset of psychosis is a challenge for theories about the aetiology of psychosis. This paper explores the relatedness of delusions about the experience of thinking (thought insertion, thought withdrawal, and thought broadcasting) and auditory verbal hallucinations by describing their trajectories over a 20-year period in individuals diagnosed with schizophrenia, affective and other psychosis, and unipolar depression nonpsychosis. The sample consisted of 407 participants who were recruited at index hospitalization and evaluated over six follow-ups over 20 years. The symptom structure associated with thought insertion included auditory verbal hallucinations, somatic hallucinations, other hallucinations, delusions of thought-dissemination, delusions of control, delusion of self-depreciation, depersonalization and anxiety. The symptom constellation of thought withdrawal included somatic hallucinations, other hallucinations, delusions of thought dissemination, delusions of control, sexual delusions, depersonalization, negative symptoms, depression, and anxiety. The symptom constellation of thought broadcasting included auditory verbal hallucinations, somatic hallucinations, delusions of thought-dissemination, delusion of self-depreciation, fantastic delusions, sexual delusions, and depersonalization. Auditory verbal hallucinations and delusions of self-depreciation were significantly associated with both thought insertion and thought broadcasting. Thought insertion and thought withdrawal were significantly associated with other hallucinations, delusions of control, and anxiety; thought withdrawal and thought broadcasting were significantly related to sexual delusions. We hypothesize that specific symptom constellations over time might be explained as the product of pseudo-coherent realities created to give meaning to the experience of the world and the self of individuals in psychosis based on both prior top-down and ongoing bottom-up elements.}
}
@article{BINA2020102475,
title = {Beyond techno-utopia and its discontents: On the role of utopianism and speculative fiction in shaping alternatives to the smart city imaginary},
journal = {Futures},
volume = {115},
pages = {102475},
year = {2020},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2019.102475},
url = {https://www.sciencedirect.com/science/article/pii/S0016328719303374},
author = {Olivia Bina and Andy Inch and Lavínia Pereira},
keywords = {Smart cities, Ways of knowing, Urban imaginaries, Utopianism, Fiction},
abstract = {In recent years, the ösmart city’ has become established in policy and planning discourse, embedding visions of an urban future where ubiquitous technology offers efficient solutions to the pathologies of the contemporary city. In response, a rapidly growing social-scientific literature is critically exploring how the smart city imaginary (SCI) promotes ötechno-utopian’ fantasies, ignoring the risks of a technologically determined future. In this paper we begin by considering SCI as emblematic of the colonization of contemporary (urban) futures by vested interests, arguing for the need for diverse and plural imaginaries and thus for a re-engagement of the social sciences. We explore how critical social scientific contributions to shaping futures might be deepened through further engagement with utopian theory and speculative fiction, two traditions of future-orientated thinking that seek to combine critique with constructive thinking about alternatives. We therefore contribute to ö50 + 50 Theme 2: Framing Futures in 2068-the limits of and opportunities for futures research’ by 1) extending critique of contemporary claims about (smart urban) futures, and; 2) exploring how utopianism and fiction can expand ways of thinking, imagining and knowing futures.}
}
@article{DALLAT2019266,
title = {Risky systems versus risky people: To what extent do risk assessment methods consider the systems approach to accident causation? A review of the literature},
journal = {Safety Science},
volume = {119},
pages = {266-279},
year = {2019},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2017.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0925753517305295},
author = {Clare Dallat and Paul M. Salmon and Natassia Goode},
keywords = {Risk, Risk assessment, Risk assessment methods, Systems thinking},
abstract = {Accidents are now widely acknowledged to be a systems phenomenon. As part of a proactive approach to safety management, organisations use risk assessment methods to identify the hazards and associated risks that may lead to accidents. Although there is an extensive body of literature on the need for a systems thinking approach in accident analysis, little has been said regarding the theoretical underpinnings of risk assessment methods. The aim of this paper was to systematically review the risk assessment methods presented in the literature and evaluate the extent to which they are underpinned by a systems thinking approach. A total of 342 methods spanning a range of safety-critical domains were evaluated using Rasmussen’s tenets of accident causation. A key finding is that the majority of existing risk assessment methods are not consistent with Rasmussen’s model of accident causation (arguably the most popular model in safety science circles). Instead, the majority of risk assessment methods focus on risks at the so called sharp-end and largely view accidents as emerging from a linear, or chain-of-events process. This overlooks emergent risks at other levels of the system, including supervisory, managerial, regulatory and government levels. The findings therefore suggest that the majority of existing risk assessment methods may be inadequate for identifying hazards and analysing risks within complex sociotechnical systems. The implications for risk assessment practice are discussed.}
}
@article{ROBINSON20231189,
title = {Opportunities and challenges for microbiomics in ecosystem restoration},
journal = {Trends in Ecology & Evolution},
volume = {38},
number = {12},
pages = {1189-1202},
year = {2023},
issn = {0169-5347},
doi = {https://doi.org/10.1016/j.tree.2023.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0169534723002112},
author = {Jake M. Robinson and Riley Hodgson and Siegfried L. Krauss and Craig Liddicoat and Ashish A. Malik and Belinda C. Martin and Jakki J. Mohr and David Moreno-Mateos and Miriam Muñoz-Rojas and Shawn D. Peddle and Martin F. Breed},
keywords = {ecosystem restoration, microbiome, microbiomics, metagenomics, restoration ecology, innovation},
abstract = {Microbiomics is the science of characterizing microbial community structure, function, and dynamics. It has great potential to advance our understanding of plant–soil–microbe processes and interaction networks which can be applied to improve ecosystem restoration. However, microbiomics may be perceived as complex and the technology is not accessible to all. The opportunities of microbiomics in restoration ecology are considerable, but so are the practical challenges. Applying microbiomics in restoration must move beyond compositional assessments to incorporate tools to study the complexity of ecosystem recovery. Advances in metaomic tools provide unprecedented possibilities to aid restoration interventions. Moreover, complementary non-omic applications, such as microbial inoculants and biopriming, have the potential to improve restoration objectives by enhancing the establishment and health of vegetation communities.}
}
@incollection{KERN202469,
title = {Chapter 5 - The turbinates—an overview},
editor = {Eugene Barton Kern and Oren Friedman},
booktitle = {Empty Nose Syndrome},
publisher = {Elsevier},
pages = {69-96},
year = {2024},
isbn = {978-0-443-10715-3},
doi = {https://doi.org/10.1016/B978-0-443-10715-3.00005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443107153000056},
author = {Eugene Barton Kern and Oren Friedman},
keywords = {Acetylcholine, secondary atrophic rhinitis, autonomic nervous system, turbinate anatomy, middle turbinate anatomy, inferior turbinate anatomy, capacitance vessels (sinusoids), “diffusor function,” functional residual capacity of the nose (FRCn), “,” hypertrophy (increase in cell ), hyperplasia (increase in cell ), nasal cycle, nasal obstruction, on-urgical urbinate eduction djunctive rocedure (n-sTRAP), out-fracture (lateralization), squamous metaplasia, submucous resection, ozaena, “resistor function,” total inferior turbinectomy, turbinates, turbinectomy, turbinoplasty, acoustic rhinometry, and rhinomanometry},
abstract = {This chapter presents an overview of the turbinates. To the best of our knowledge, it was a New Yorker, William M. Jarvis, MD, who in 1882 described three cases of utilizing a snare to affect a partial turbinectomy. At the dawn of the 20th century, most surgeons were promoting total inferior turbinectomy not only for nasal airway obstruction but astoundingly also for hearing impairment and tinnitus. The turbinate enlargement or “hypertrophy” is neither the cause nor a complication of hearing loss. Fortunately, and for the most part, dazed blunders and egregious errors in thinking by esteemed experts, for the most part, have remedied itself through scientific studies, since the late 19th century. This chapter traces the more than a century long history of turbinate thinking and surgery offering both sides of the turbinate debate in their “own words.” All the various procedures used to reduce the inferior turbinate are presented. To be as fair minded as possible, numerous authors are quoted, spanning more than one hundred years; some observed and reported the serious adverse effects of aggressive turbinate surgery, pleading for a conservative approach to inferior turbinate surgical intervention, while others claimed that turbinectomy was without any serious sequelae which is challenged by the facts.}
}
@article{SIMPSON2017166,
title = {Preparing industry for additive manufacturing and its applications: Summary & recommendations from a National Science Foundation workshop},
journal = {Additive Manufacturing},
volume = {13},
pages = {166-178},
year = {2017},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2016.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S2214860416302019},
author = {Timothy W. Simpson and Christopher B. Williams and Michael Hripko},
keywords = {Additive manufacturing, Design for additive manufacturing, STEM education, 3D printing, Workforce development},
abstract = {Accompanying the increasing advances and interest in Additive Manufacturing (AM) technologies is an increasing demand for an industrial workforce that is knowledgeable about the technologies and how to apply them to solve real-world problems. As a step towards addressing this knowledge gap, a workshop was held at the National Science Foundation (NSF) to discuss the educational needs to prepare industry for AM and its use in different fields. The workshop participants – 66 representatives from academia, industry, and government – identified several key educational themes: (1) AM processes and process/material relationships, (2) engineering fundamentals with an emphasis on materials science and manufacturing, (3) professional skills for problem solving and critical thinking, (4) design practices and tools that leverage the design freedom enabled by AM, and (5) cross-functional teaming and ideation techniques to nurture creativity. This paper summarizes the industry speakers and presentations from the workshop, along with several new educational partnerships identified by small working groups. Based on the presentations and partnerships, the following recommendations are offered to advance the AM workforce. First, ensure that all AM curricula provide students with an understanding of (i) AM and traditional manufacturing processes to enable them to effectively select the appropriate process for product realization; (ii) the relationships between AM processes and material properties; and (iii) “Design for AM”, including computational tools for AM design as well as frameworks for process selection, costing, and solution generation that take advantage of AM capabilities. Second, establish a national network for AM education that, by leveraging existing “distributed” educational models and NSF’s Advanced Technology Education (ATE) Programs, provides open source resources as well as packaged activities, courses, and curricula for all educational levels (K-Gray). Third, support K-12 educational programs in STEAM (STEM plus the arts) and across all formal and informal learning environments in order to learn the unique capabilities of AM while engaging students in hands-on, tactile, and visual learning activities to prepare them for jobs in industry while learning how to think differently when designing for AM. Fourth, provide support for collaborative and community-oriented maker spaces that promote awareness of AM among the public and provide AM training programs for incumbent workers in industry and students seeking alternative pathways to gain AM knowledge and experience. Recommendations for scaling and coordination across local, regional, and national levels are also discussed to create synergies among the proposed activities and existing efforts.}
}
@article{BESHKOV2024109370,
title = {Topological structure of population activity in mouse visual cortex encodes densely sampled stimulus rotations},
journal = {iScience},
volume = {27},
number = {4},
pages = {109370},
year = {2024},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2024.109370},
url = {https://www.sciencedirect.com/science/article/pii/S2589004224005911},
author = {Kosio Beshkov and Marianne Fyhn and Torkel Hafting and Gaute T. Einevoll},
keywords = {Neuroscience, Sensory neuroscience, Cognitive neuroscience},
abstract = {Summary
The primary visual cortex is one of the most well understood regions supporting the processing involved in sensory computation. Following the popularization of high-density neural recordings, it has been observed that the activity of large neural populations is often constrained to low dimensional manifolds. In this work, we quantify the structure of such neural manifolds in the visual cortex. We do this by analyzing publicly available two-photon optical recordings of mouse primary visual cortex in response to visual stimuli with a densely sampled rotation angle. Using a geodesic metric along with persistent homology, we discover that population activity in response to such stimuli generates a circular manifold, encoding the angle of rotation. Furthermore, we observe that this circular manifold is expressed differently in subpopulations of neurons with differing orientation and direction selectivity. Finally, we discuss some of the obstacles to reliably retrieving the truthful topology generated by a neural population.}
}
@article{ALANO20221,
title = {Professor Richard Carter (1945–2021)},
journal = {Trends in Parasitology},
volume = {38},
number = {1},
pages = {1-3},
year = {2022},
issn = {1471-4922},
doi = {https://doi.org/10.1016/j.pt.2021.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S1471492221002609},
author = {Pietro Alano and Richard Culleton and Christian Doerig and Louis Miller},
abstract = {The malaria research community lost a pioneer when Professor Richard Carter passed away at the age of 76 on 4 September 2021. Richard was an exceptionally brilliant malariologist, always inquisitive and gifted with an unorthodox way of thinking.}
}
@article{JOO2024226,
title = {Teaching and Learning Model for Artificial Intelligence Education},
journal = {Procedia Computer Science},
volume = {239},
pages = {226-233},
year = {2024},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2023},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.06.166},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924014091},
author = {Kil Hong Joo and Nam Hun Park},
keywords = {Artificial intelligence education, Lower grades in elementary school},
abstract = {AI education aims to nurture convergence talents equipped with various knowledge and AI capabilities. However, the educational programs developed so far are designed for less than 10 sessions. This is insufficient time for students to understand artificial intelligence algorithms, utilize the learned principles of artificial intelligence, and expand by converging with various knowledge. Since the developmental characteristics of students in the lower grades of elementary school are different, it is difficult to apply them as they are. Therefore, it is necessary to study the following AI education teaching and learning methods suitable for lower grade students. In this study, we develop a teaching system design model for artificial intelligence-based subject convergence education for elementary school students in the lower grades, and based on this, design and apply an artificial intelligence-based subject convergence education program. Experiments were conducted with 47 second-year elementary school students, and students’ responses were better in the AI-based convergence education program than in general subject classes in terms of interest, understanding, and expectations for classes.}
}
@incollection{BUJA2005391,
title = {14 - Computational Methods for High-Dimensional Rotations in Data Visualization},
editor = {C.R. Rao and E.J. Wegman and J.L. Solka},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {24},
pages = {391-413},
year = {2005},
booktitle = {Data Mining and Data Visualization},
issn = {0169-7161},
doi = {https://doi.org/10.1016/S0169-7161(04)24014-7},
url = {https://www.sciencedirect.com/science/article/pii/S0169716104240147},
author = {Andreas Buja and Dianne Cook and Daniel Asimov and Catherine Hurley},
abstract = {There exist many methods for visualizing complex relations among variables of a multivariate dataset. For pairs of quantitative variables, the method of choice is the scatterplot. For triples of quantitative variables, the method of choice is 3D data rotations. Such rotations let us perceive structure among three variables as shape of point scatters in virtual 3D space. Although not obvious, three-dimensional data rotations can be extended to higher dimensions. The mathematical construction of high-dimensional data rotations, however, is not an intuitive generalization. Whereas three-dimensional data rotations are thought of as rotations of an object in space, a proper framework for their high-dimensional extension is better based on rotations of a low-dimensional projection in high-dimensional space. The term “data rotations” is therefore a misnomer, and something along the lines of “high-to-low dimensional data projections” would be technically more accurate. To be useful, virtual rotations need to be under interactive user control, and they need to be animated. We therefore require projections not as static pictures but as movies under user control. Movies, however, are mathematically speaking one-parameter families of pictures. This article is therefore about one-parameter families of low-dimensional projections in high-dimensional data spaces. We describe several algorithms for dynamic projections, all based on the idea of smoothly interpolating a discrete sequence of projections. The algorithms lend themselves to the implementation of interactive visual exploration tools of high-dimensional data, such as so-called grand tours, guided tours and manual tours.}
}
@incollection{MARWALA202185,
title = {Chapter 7 - Bounded rational counterfactuals},
editor = {Tshilidzi Marwala},
booktitle = {Rational Machines and Artificial Intelligence},
publisher = {Academic Press},
pages = {85-96},
year = {2021},
isbn = {978-0-12-820676-8},
doi = {https://doi.org/10.1016/B978-0-12-820676-8.00012-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128206768000120},
author = {Tshilidzi Marwala},
keywords = {Rational counterfactuals, Bounded rationality, Optimization, Artificial intelligence},
abstract = {The rational counterfactual is identified from the factual and the knowledge of the laws that govern the relationships between the antecedent and the consequent of the factual, which maximizes the attainment of the desired consequent. However, the attainment of the desired consequent is not perfect and is, in fact, limited, which makes these counterfactuals bounded rational counterfactuals. In counterfactual thinking, factual statements such as “The COVID-19 afflicted the world, and the world economy contracted by 3%,” has the counterfactual “The COVID-19 did not afflict the world, and the world economy grew by 3%.” In this chapter, we use intelligent machines that use AI to build bounded rational counterfactuals. It is observed that intelligent machines can achieve bounded rational counterfactual better than human agents. In general, quantifiable factual easily has bounded rational counterfactuals when compared to qualitative counterfactuals.}
}
@article{WATHEN2022176,
title = {Some observations on preconditioning for non-self-adjoint and time-dependent problems},
journal = {Computers & Mathematics with Applications},
volume = {116},
pages = {176-180},
year = {2022},
note = {New trends in Computational Methods for PDEs},
issn = {0898-1221},
doi = {https://doi.org/10.1016/j.camwa.2021.05.037},
url = {https://www.sciencedirect.com/science/article/pii/S0898122121002388},
author = {Andy Wathen},
keywords = {Iterative methods, Preconditioning, Non-self-adjoint problems, Parallel-in-time computation},
abstract = {Numerical Linear Algebra—specifically the computational solution of equations—forms a significant part of Computational Methods for Partial Differential Equations. Here we discuss the contrast between the solution of symmetric systems of equations that arise from self-adjoint problems and non-symmetric systems that arise from non-self-adjoint problems when iterative methods are employed; such methods are the only feasible methods for very large scale computation with PDEs. We then go on to consider non-symmetric all-at-once systems that arise in approximation of time-dependent problems, discuss causality and the parallel-in-time paradigm, suggesting an approach that involves preconditioning initial value problems with time-periodic problems.}
}
@article{SENVAR20161140,
title = {Hospital Site Selection via Hesitant Fuzzy TOPSIS},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {12},
pages = {1140-1145},
year = {2016},
note = {8th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.07.656},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316309296},
author = {Ozlem Senvar and Irem Otay and Eda Bolturk},
keywords = {Facility Layout, Location Selection, Multi criteria decision making (MCDM), TOPSIS, Hesitant fuzzy set (HFS)},
abstract = {This study handles the problem of establishing a well-organized and distributed network of a hospital that delivers its services to the target population. We propose a new multi criteria decision making (MCDM) process that integrates hesitant fuzzy sets (HFSs) to Technique for Order Preference by Similarity to Ideal Solution (TOPSIS). The MCDM process defined under uncertainties are perfectly defined by HFSs reflecting comprehensively hesitant thinking of decision makers. Our proposed methodology is implemented to select the optimum site for a new hospital in Istanbul.}
}
@article{BAUSO201776,
title = {Consensus via multi-population robust mean-field games},
journal = {Systems & Control Letters},
volume = {107},
pages = {76-83},
year = {2017},
issn = {0167-6911},
doi = {https://doi.org/10.1016/j.sysconle.2017.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167691117301287},
author = {D. Bauso},
keywords = {Synchronization, Consensus, Mean-field games},
abstract = {In less prescriptive environments where individuals are told ‘what to do’ but not ‘how to do’, synchronization can be a byproduct of strategic thinking, prediction, and local interactions. We prove this in the context of multi-population robust mean-field games. The model sheds light on a multi-scale phenomenon involving fast synchronization within the same population and slow inter-cluster oscillation between different populations.}
}
@article{BERNUS201583,
title = {Enterprise architecture: Twenty years of the GERAM framework},
journal = {Annual Reviews in Control},
volume = {39},
pages = {83-93},
year = {2015},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2015.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S1367578815000097},
author = {Peter Bernus and Ovidiu Noran and Arturo Molina},
abstract = {Apart from the 20-year anniversary in 2014 of the first publication of the GERAM (‘Generalised Enterprise Reference Architecture and Methodology’) Enterprise Architecture Framework, the timeliness of this paper lies in the new interest in the use of systems theory in enterprise architecture (EA), and consequently, ‘light-weight’ architecture frameworks (AFs). Thus, this paper is about the use of systems thinking and systems theory in EA and about how it is possible to reconcile and understand, based on a single overarching framework, the interplay of two major enterprise change endeavours: on one hand enterprise engineering (i.e. deliberate change) and on the other hand evolutionary, organic change. The paper also demonstrates how such change processes can be illustrated by employing systems thinking to construct dynamic business models; the evolution of these concepts is exemplified using past applications in networked enterprise building, and more recent proposals in environmental-, disaster- and healthcare management. Finally, the paper attempts to plot the way GERAM, as a framework to think about the creation and evolution of complex socio-technical systems, will continue to contribute to the society in the context of future challenges and emerging opportunities.}
}
@article{GANAPATHY20158064,
title = {Optimum steepest descent higher level learning radial basis function network},
journal = {Expert Systems with Applications},
volume = {42},
number = {21},
pages = {8064-8077},
year = {2015},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2015.06.036},
url = {https://www.sciencedirect.com/science/article/pii/S0957417415004388},
author = {Kirupa Ganapathy and V. Vaidehi and Jesintha B. Chandrasekar},
keywords = {Neural network, Radial basis function, Dynamic learning, Optimum steepest descent, Higher level components, Healthcare},
abstract = {Dynamically changing real world applications, demands for rapid and accurate machine learning algorithm. In neural network based machine learning algorithms, radial basis function (RBF) network is a simple supervised learning feed forward network. With its simplicity, this network is highly suitable to model and control the nonlinear systems. Existing RBF networks in literature are applied to static applications and also faces challenges such as increased model size, neuron removal, improper center selection etc leading to erroneous output. To overcome the challenges and handle complex real world problems, this paper proposes a new optimum steepest descent based higher level learning radial basis function network (OSDHL-RBFN). The proposed OSDHL-RBFN implements major components inspired from the human brain for efficient learning, adaptive structure and accurate classification. Higher level learning and thinking components of the proposed network are sample deletion, neuron addition, neuron migration, sample navigation and neuroplasticity. These components helps the classifier to think before learning the samples and regulates the learning strategy. The knowledge gained from the trained samples are used by the network to identify the incomplete sample, optimal center and bond strength of hidden & output neurons. Adaptive network structure is employed to minimize classification error. The proposed work also uses optimum steepest descent method for weight parameter update to minimize the sum square error. OSDHL-RBFN is tested and evaluated in both static and dynamic environments on nine benchmark classification (binary and multiclass) problems for balanced, unbalanced, small, large, low dimensional and high dimensional datasets. The overall and class wise efficiency of OSDHL-RBFN is improved when compared to other RBFN’s in the literature. The performance results clearly show that the proposed OSDHL-RBFN reduces the architecture complexity and computation time compared to other RBFN’s. Overall, the proposed OSDHL-RBFN is efficient and suitable for dynamic real world applications in terms of detection time and accuracy. As a case study, OSDHL-RBFN is implemented in real time remote health monitoring application for classifying the various abnormality levels in vital parameters.}
}
@incollection{TILLAS2017101,
title = {Chapter 7 - On the Redundancies of “Social Agency”},
editor = {Jon Leefmann and Elisabeth Hildt},
booktitle = {The Human Sciences after the Decade of the Brain},
publisher = {Academic Press},
address = {San Diego},
pages = {101-120},
year = {2017},
isbn = {978-0-12-804205-2},
doi = {https://doi.org/10.1016/B978-0-12-804205-2.00007-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128042052000070},
author = {A. Tillas},
keywords = {Structure, agency, concepts, intuitions, decision-making, actions},
abstract = {This chapter presents a philosophical argument about the “structure vs agency” debate—one of the central debates in social sciences. I do not argue for the primacy of either of the two but suggest an empirically vindicated view about the nature of thinking, in light of which the traditional debate as well as the notion of “social agency,” is redundant. I argue that thinking is contingent on the weightings of the synaptic connections between neuronal groups grounding it. In turn, socialization is a process of adjusting or conditioning the appropriate synaptic connection weightings. Both conscious (reasoning) and unconscious (intuitions) determinants of sociologically nontrivial actions derive from perceptual encounters with our sociophysical environment. In turn, agents—as social scientists use the term—simply do not exist. Finally, I appeal to neuroscientific evidence and show that we still qualify as agents, if only with regards to sociologically trivial actions.}
}
@article{KIM2024110348,
title = {Hierarchical aerial offload computing algorithm based on the Stackelberg-evolutionary game model},
journal = {Computer Networks},
volume = {245},
pages = {110348},
year = {2024},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2024.110348},
url = {https://www.sciencedirect.com/science/article/pii/S1389128624001804},
author = {Sungwook Kim},
keywords = {Aerial access networks, High-altitude platforms, Unmanned aerial vehicles, Stackelberg-evolutionary game, Multi-objective bargaining solution},
abstract = {Aerial access networks have been envisioned as a promising 6 G technology to enhance the service experience in underserved areas where terrestrial base stations do not exist. In such scenarios, a hierarchical model of high-altitude platforms (HAPs) and unmanned aerial vehicles (UAVs) is considered to provide aerial computing services for ground Internet of Things (IoT) devices. In this study, we investigate a hierarchical aerial computing system to optimally orchestrate the limited computation resources in both HAPs and UAVs. For offloading services, we formulate a joint resource allocation problem to maximize service satisfaction for terrestrial IoT devices. To solve this problem, we employ the ideas of game theory with centralized decision and decentralized execution. Through the Stackelberg-evolutionary game model, the HAP works as a leader, and selects its price strategy based on the evolutionary learning process. As followers, individual UAVs make decisions to partially offload their computing tasks by considering different objectives. According to the interactive control paradigm, our proposed method can get reciprocal advantages for HAPs, UAVs, and ground IoT devices while adaptively handling dynamic aerial network conditions. Finally, extensive simulation results verify the efficiency of our proposed algorithm to increase the usability of edge servers’ computational resources. Compared with other existing state-of-the-art aerial network offloading protocols, we can improve the profits of system throughput, resource usability and UAV fairness up to 10 %, 10 %, and 15 %, respectively.}
}
@article{BLOTE2000221,
title = {Mental computation and conceptual understanding},
journal = {Learning and Instruction},
volume = {10},
number = {3},
pages = {221-247},
year = {2000},
issn = {0959-4752},
doi = {https://doi.org/10.1016/S0959-4752(99)00028-6},
url = {https://www.sciencedirect.com/science/article/pii/S0959475299000286},
author = {Anke W. Blöte and Anton S. Klein and Meindert Beishuizen},
keywords = {Arithmetic, Procedural flexibility, Conceptual understanding},
abstract = {The goal of this study was to assess the strategic flexibility of students in mental arithmetic up to the number 100. Sixty Dutch second-graders who took part in an experimental ‘realistic arithmetic’ program participated in the study. Results showed that students' preference for certain mathematical procedures depended on the number characteristics of the problems. This indicates that the students had a good conceptual understanding of numbers and procedures. Their actual use of these procedures, however, was somewhat limited. Most problems were solved within a sequential structure. A completely different procedure was used for solving subtraction problems that had a very small difference between the two numbers. Furthermore, it was found that a substantial increase in the students' use of a base-ten procedure occurred after the introduction of this procedure in the mathematics curriculum. Students' preference for this procedure also increased, although to a lesser extent. Another finding of the study was that students exhibited more flexible strategic behaviour with context problems than with numerical-expression problems.}
}
@article{AGARWAL1992251,
title = {Computational fluid dynamics on parallel processors},
journal = {Computing Systems in Engineering},
volume = {3},
number = {1},
pages = {251-259},
year = {1992},
note = {High-Performance Computing for Flight Vehicles},
issn = {0956-0521},
doi = {https://doi.org/10.1016/0956-0521(92)90110-5},
url = {https://www.sciencedirect.com/science/article/pii/0956052192901105},
author = {R.K. Agarwal and J.C. Lewis},
abstract = {Greater computational power is needed for solving computational fluid dynamics problems of interest in engineering design. Parallel computers offer the promise of providing orders of magnitude increases in computational power compared with current uniprocessor vector supercomputers. This paper is mainly concerned with the implementation of a three-dimensional Navier-Stokes code MDNS3D on concurrent computers with grain sizes ranging from fine to coarse. An overview of commercially available parallel machines and the current state of the art in parallel algorithms is presented. The implementation of MDNS3D on machines such as the CRAY Y-MP/8, IBM 3090S, BBN Butterfly II, Intel iPSC/2, Symult 2010, MASPAR, and the Connection Machine CM-2, is described. Particular attention is paid to differences in implementation on SIMD and MIMD architectures. Factors affecting the performance of the code on different architectures are addressed. In addition, user interface and software portability issues are considered for various machines. Finally, future trends in parallel hardware and software development are assessed, and the factors important in determining the most suitable architecture for performing very large scale calculations are discussed.}
}
@article{MOSTERT202448,
title = {The Shortfalls of Mental Health Compartment Models: A Call to Improve Mental Health Investment Cases in Developing Countries},
journal = {Value in Health Regional Issues},
volume = {41},
pages = {48-53},
year = {2024},
issn = {2212-1099},
doi = {https://doi.org/10.1016/j.vhri.2023.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S2212109923001449},
author = {Cyprian M. Mostert and Andrew Aballa and Linda Khakali and Willie Njoroge and Jasmit Shah and Samim Hasham and Zul Merali and Lukoye Atwoli},
keywords = {developing countries, investment cases, mental health compartment model},
abstract = {Objectives
There are irregularities in investment cases generated by the Mental Health Compartment Model. We discuss these irregularities and highlight the costing techniques that may be introduced to improve mental health investment cases.
Methods
This analysis uses data from the World Bank, the World Health Organization Mental Health Compartment Model, the United Nations Development Program, the Kenya Ministry of Health, and Statistics from the Kenyan National Commission of Human Rights.
Results
We demonstrate that the Mental Health Compartment Model produces irrelevant outcomes that are not helpful for clinical settings. The model inflated the productivity gains generated from mental health investment. In some cases, the model underestimated the economic costs of mental health. Such limitation renders the investment cases poor in providing valuable intervention points from the perspectives of both the users and the providers.
Conclusions
There is a need for further calibration and validation of the investment case outcomes. The current estimated results cannot be used to guide service provision, research, and mental health programming comprehensively.}
}
@incollection{TEZDUYAR199521,
title = { - Massively parallel finite element computation of 3d flows - mesh update strategies in computation of moving boundaries and interfaces°†},
editor = {A. Ecer and J. Hauser and P. Leca and J. Periaux},
booktitle = {Parallel Computational Fluid Dynamics 1993},
publisher = {North-Holland},
address = {Amsterdam},
pages = {21-30},
year = {1995},
isbn = {978-0-444-81999-4},
doi = {https://doi.org/10.1016/B978-044481999-4/50131-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780444819994501316},
author = {T. Tezduyar and S. Aliabadi and M. Behr and A. Johnson and S. Mittal},
abstract = {Publisher Summary
This chapter describes the parallel implicit finite element computations of compressible and incompressible flows with the Connection Machine (CM)—CM-200 and CM-5. The parallel implementations are based on the assumption that the mesh is unstructured. The computations of flow problems involving moving boundaries and interfaces are achieved by using the deformable-spatial-domain or stabilized space-time method. In this method, with special mesh update strategies, the frequency of remeshing is minimized. This avoids the projection errors generated by remeshing and also avoids the cost associated with repeated mesh generation and parallelization setup. This method and its implementation on the massively parallel supercomputers provide a new capability to solve a large class of practical problems involving free surfaces, two-liquid interfaces, and fluid-structure interactions. Now 3D incompressible flow computations can be carried out at sustained speeds of up to 7.0 GigaFLOPS on the CM-5. The 3D compressible flow computations are carried out at sustained speeds of up to 12.2 GigaFLOPS on the CM-5. This parallel performance is significant in the sense that now there is a new level of computational capability in finite element solution of 3D flow problems. Several 3D flow problems are solved using these parallel and update mesh strategies. The chapter discusses the computation of incompressible flow occurring between two concentric cylinders, sloshing in a liquid-filled container subjected to vertical vibrations, and supersonic flow past a delta-wing.}
}
@article{GAO2022509,
title = {An integrated simulation method for PVSS parametric design using multi-objective optimization},
journal = {Frontiers of Architectural Research},
volume = {11},
number = {3},
pages = {509-526},
year = {2022},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2021.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S209526352100087X},
author = {Qing Gao and Ying Yang and Qian Wang},
keywords = {Integrated simulation, PV shading System, Parametric design, Multi-objective optimization, Thermal-daylighting balance},
abstract = {An adequate strategy for achieving energy efficiency when designing a photovoltaic shading system (PVSS) shall find an equilibrium between sunlight heat gain and daylight transmittances through effective analysis tools in a building's early design phases. However, traditional simulation methods are either time-consuming or lacking architectonical thinking. This paper proposes a new method for architects to integrate thermal and daylighting performance by using parametric script modelling and optimize their balance with multi-objective optimization (MOO) algorithm in PVSS design. A case study was conducted to demonstrate the workflow of proposed integrated simulation method in PVSS design, and further compared the results with that of three single-objective optimizations under the same design requirement. The findings show that the integrated framework is a feasible method for PVSS design and can be extended into the design of other advance shading system or building integrated photovoltaic.}
}
@article{SMYE2022105015,
title = {Interdisciplinary approaches to metastasis},
journal = {iScience},
volume = {25},
number = {9},
pages = {105015},
year = {2022},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2022.105015},
url = {https://www.sciencedirect.com/science/article/pii/S2589004222012871},
author = {Stephen W. Smye and Robert A. Gatenby},
abstract = {Summary
Interdisciplinary research is making a significant contribution to understanding metastasis - one of the grand challenges in cancer research. Examples drawn from apparently unconnected areas of physics, and described at a recent workshop on metastasis, illustrate the value of interdiscplinary thinking.}
}
@article{VIGNAPIANO201999,
title = {Disorganization and cognitive impairment in schizophrenia: New insights from electrophysiological findings},
journal = {International Journal of Psychophysiology},
volume = {145},
pages = {99-108},
year = {2019},
note = {The Neurophysiology of Schizophrenia: A Critical Update},
issn = {0167-8760},
doi = {https://doi.org/10.1016/j.ijpsycho.2019.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S016787601830984X},
author = {Annarita Vignapiano and Thomas Koenig and Armida Mucci and Giulia M. Giordano and Antonella Amodio and Mario Altamura and Antonello Bellomo and Roberto Brugnoli and Giulio Corrivetti and Giorgio {Di Lorenzo} and Paolo Girardi and Palmiero Monteleone and Cinzia Niolu and Silvana Galderisi and Mario Maj},
keywords = {Disorganization dimension, Difficulty in abstract thinking, Neurocognitive domains, Alpha rhythm, Spectral power, Topographic analysis},
abstract = {In subjects with schizophrenia (SCZ), the disorganization dimension is a strong predictor of real-life functioning. “Conceptual disorganization” (P2), “Difficulty in abstract thinking” (N5) and “Poor attention” (G11) are core features of the disorganization factor, evaluated using the Positive and Negative Syndrome Scale. The heterogeneity of this dimension and its overlap with neurocognitive deficits are still debated. Within the multicenter study of the Italian Network for Research on Psychoses, we investigated electrophysiological and neurocognitive correlates of disorganization and its component items to assess the heterogeneity of this dimension and its possible overlap with neurocognitive deficits. Resting state EEG was recorded in 145 stabilized SCZ and 69 matched healthy controls (HC). Spectral amplitude was averaged in ten frequency bands. Neurocognitive domains were assessed by MATRICS Consensus Cognitive Battery (MCCB). RAndomization Graphical User software explored band spectral amplitude differences between groups and correlations with disorganization and MCCB scores in SCZ. Correlations between disorganization and MCCB scores were also investigated. Compared to HC, SCZ showed increased delta, theta, and beta 1 and decreased alpha 2 activity. A negative correlation between alpha 1 and disorganization was observed in SCZ. At the item level, only “N5” showed the same correlation. MCCB neurocognitive composite score was associated with disorganization, “P2” and “N5”. Our findings suggest only a partial overlap between disorganization and neurocognitive impairment. The association of alpha 1 with the “N5” item suggests that some aspects of disorganization could be underpinned by the impairment of basic neurobiological functions that are only partially evaluated using MCCB.}
}
@article{KUAI2021150,
title = {Multi-source brain computing with systematic fusion for smart health},
journal = {Information Fusion},
volume = {75},
pages = {150-167},
year = {2021},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2021.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S1566253521000646},
author = {Hongzhi Kuai and Ning Zhong and Jianhui Chen and Yang Yang and Xiaofei Zhang and Peipeng Liang and Kazuyuki Imamura and Lianfang Ma and Haiyuan Wang},
keywords = {Brain computing, Brain informatics, Data-Brain, Smart health, Systematic fusion, Intelligence system},
abstract = {With the progress of artificial intelligence, big data and functional neuroimaging technologies, brain computing has rapidly advanced our understanding of brain intelligence and brain disorders. We argue that existing data analytical methods have become insufficient for brain computing when dealing with multiple brain big data sources, because such methods mainly focus on flattening strategies and fail to work well for systematic understanding of the constituent elements of cognition, emotion and disease, as well as the intra- and inter-relations within and among themselves. To address this problem, we present in this paper a novel multi-source brain computing platform by Data-Brain driven systematic fusion. First, we formalize a series of behaviors surrounding the Brain Informatics-based investigation process, and present a conceptual model to systematically represent content and context of functional neuroimaging data. Then, we propose the systematic brain computing framework with multi-aspect fusion and inference to understand brain specificity and give uncertainty quantification, as well as its inspiration and applications for translational studies on brain health. In particular, a graph matching-based task search algorithm is introduced to help systematic experimental design and data sampling with multiple cognitive tasks. The study increases the interpretability and transparency of brain computing findings by inferring and testing multiple hypotheses taking into consideration the effect of evidence combination. Finally, multiple sources of knowledge (K), information (I) and data (D) are driven by a KID loop as the thinking space to inspire never-ending learning and multi-dimensional interactions in the connected social–cyber–physical spaces. Experimental results have demonstrated the efficacy of the proposed brain computing method with systematic fusion.}
}
@article{SCHUELLER1997197,
title = {A state-of-the-art report on computational stochastic mechanics},
journal = {Probabilistic Engineering Mechanics},
volume = {12},
number = {4},
pages = {197-321},
year = {1997},
note = {A State-of-the-Art Report on Computational Stochastic Mechanics},
issn = {0266-8920},
doi = {https://doi.org/10.1016/S0266-8920(97)00003-9},
url = {https://www.sciencedirect.com/science/article/pii/S0266892097000039},
author = {G.I. Schuëller}
}
@article{GUNNING2021169,
title = {Brain-based mechanisms of late-life depression: Implications for novel interventions},
journal = {Seminars in Cell & Developmental Biology},
volume = {116},
pages = {169-179},
year = {2021},
note = {Special Issue: Myelin edited by Gonçalo Castelo-Branco and Roman Chrast / Special issue: Aging in the nervous system edited by Mara Mather},
issn = {1084-9521},
doi = {https://doi.org/10.1016/j.semcdb.2021.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1084952121001117},
author = {Faith M. Gunning and Lauren E. Oberlin and Maddy Schier and Lindsay W. Victoria},
keywords = {Aging, Depression, Functional connectivity, White matter, Apathy, Executive function},
abstract = {Late-life depression (LLD) is a particularly debilitating illness. Older adults suffering from depression commonly experience poor outcomes in response to antidepressant treatments, medical comorbidities, and declines in daily functioning. This review aims to further our understanding of the brain network dysfunctions underlying LLD that contribute to disrupted cognitive and affective processes and corresponding clinical manifestations. We provide an overview of a network model of LLD that integrates the salience network, the default mode network (DMN) and the executive control network (ECN). We discuss the brain-based structural and functional mechanisms of LLD with an emphasis on their link to clinical subtypes that often fail to respond to available treatments. Understanding the brain networks that underlie these disrupted processes can inform the development of targeted interventions for LLD. We propose behavioral, cognitive, or computational approaches to identifying novel, personalized interventions that may more effectively target the key cognitive and affective symptoms of LLD.}
}
@article{TWORZYDLO199387,
title = {Towards an automated environment in computational mechanics},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {104},
number = {1},
pages = {87-143},
year = {1993},
issn = {0045-7825},
doi = {https://doi.org/10.1016/0045-7825(93)90208-F},
url = {https://www.sciencedirect.com/science/article/pii/004578259390208F},
author = {W.W. Tworzydlo and J.T. Oden},
abstract = {Effective methods leading to an automated, computer-based solution of complex engineering design problems are studied in this paper. In particular, methods of automation of the finite element analyses are of primary interest here. This includes algorithmic approaches, based on error estimation. adaptivity and smart algorithms, as well as heuristic approaches based on methods of knowledge engineering. A computational environment, which interactively couples h-p adaptive finite element methods with object oriented programming and expert system tools, is presented. Several examples illustrate the merit and potential of the approaches studied here and confirm the feasibility of developing fully automated design environments.}
}
@article{CHING201765,
title = {Children's understanding of the commutativity and complement principles: A latent profile analysis},
journal = {Learning and Instruction},
volume = {47},
pages = {65-79},
year = {2017},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2016.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0959475216301906},
author = {Boby Ho-Hong Ching and Terezinha Nunes},
keywords = {Additive reasoning, Commutativity principle, Complement principle, Latent profile analysis},
abstract = {This study examined patterns of individual differences in the acquisition of the knowledge of the commutativity and complement principles in 115 five-to six-year-old children and explored the role of concrete materials in helping children understand the prinicples. On the basis of latent profile analysis, four groups of children were identified: The first group succeeded in commutativity tasks with concrete materials but in no other tasks; the second succeeded in commutativity tasks in both concrete and abstract conditions, but not in complement tasks; the third group succeeded in all commutativity tasks and in complement tasks with concrete materials, and the final group succeeded in all the tasks. The four groups of children suggest a developmental trend – (1) Knowledge of the commutativity and of the complement principles seems to develop from thinking in the context of specific quantities to thinking about more abstract symbols; (2) There may be an order of understanding of the principles – from the commutativity to the complement principle; (3) Children may acquire the knowledge of the commutativity principle in the more abstract tasks before they start to acquire the knowledge of the complement principle. This study contributes to the literature by showing that assessing additive reasoning in different ways and identifying profiles with classification analyses may be useful for educators to understand more about the developmental stage where each child is placed. It appears that a more fine-grained assessment of additive reasoning can be achieved by incorporating both concrete materials and relatively abstract symbols in the assessment.}
}
@article{NARIMANI202441,
title = {Intelligent Control for Aerospace Engineers: A Novel Educational Framework},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {16},
pages = {41-46},
year = {2024},
note = {2nd IFAC Workshop on Aerospace Control Education - WACE 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.08.459},
url = {https://www.sciencedirect.com/science/article/pii/S240589632401228X},
author = {Mohammad Narimani and Seyyed Ali Emami and Paolo Castaldi},
keywords = {Aerospace control education, Intelligent control systems, Neural networks, Reinforcement learning, Model-based control, Adaptive control, Expert systems},
abstract = {The integration of intelligent control techniques into aerospace engineering education remains a challenge. This paper presents a novel approach for teaching intelligent control specifically designed for aerospace engineers, bridging the gap between theoretical foundations and practical applications. The proposed framework encompasses a comprehensive curriculum covering model-based and model-free approaches, leveraging neural networks, reinforcement learning, and other computational intelligence techniques. It emphasizes hands-on experiences through simulation-based exercises, hardware-in-the-loop experiments, and design projects tailored to different aerospace vehicle categories, including multi-rotor UAVs, helicopters, fixed-wing aircraft, and Hypersonic Flight Vehicles. The framework also addresses assessment methods, industry collaborations, and case studies to enhance student learning outcomes.}
}
@article{WESSMANENZINGER2019105,
title = {Grade 5 children’s drawings for integer addition and subtraction open number sentences},
journal = {The Journal of Mathematical Behavior},
volume = {53},
pages = {105-128},
year = {2019},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2018.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0732312317300731},
author = {Nicole M. Wessman-Enzinger},
keywords = {Integers, Integer addition and subtraction, Learner-generated drawings, Number line, Models, Instructional models, Student thinking},
abstract = {Three Grade 5 children participated in a microgenetic study embedded in 12-week teaching experiment on integer addition and subtraction. They solved open number sentences in four individual sessions across the 12-weeks and produced drawings. Through the lens of learner-generated drawings and qualitative analysis, these drawings provide perspective into the children’s thinking about integer addition and subtraction. The following categories are described: Single and Double Set of Objects, Number Sequences, Empty Number Lines, Number Lines, Number Sentences, Sign Emphasis, and Answer in Box Only. One student drew sets of objects frequently and the other students drew number lines more. Descriptions of how use of their drawings changed over time are provided. Implications point to a re-examination of integer instructional models and insight into potential learning progressions.}
}
@article{MIYAMOTO2022231473,
title = {Data-driven optimization of 3D battery design},
journal = {Journal of Power Sources},
volume = {536},
pages = {231473},
year = {2022},
issn = {0378-7753},
doi = {https://doi.org/10.1016/j.jpowsour.2022.231473},
url = {https://www.sciencedirect.com/science/article/pii/S0378775322004803},
author = {Kaito Miyamoto and Scott R. Broderick and Krishna Rajan},
keywords = {Lithium-ion batteries, 3D miniature batteries, Optimization of 3D battery architecture, Machine learning, Multiobjective optimization},
abstract = {To power microelectronics for the internet-of-things applications, high-performance miniature batteries, called microbatteries, are critically important. Given their limited size, the three-dimensional design of microbatteries is key to maximizing their performance. Therefore, a computational strategy to identify the target battery architecture has major implications for performance improvement. In this paper, we propose a data-driven 3D battery optimization system at the full cell level that combines an automatic geometry generator based on Monte Carlo Tree Search and highly accurate machine-learning-based performance simulators. The performance of the proposed method is demonstrated by designing high-performance 3D batteries with more than 5.5 times efficiency compared with the approach based on a randomized algorithm. One of the designed geometries displayed greater power and energy densities due to more than 10% reduced internal resistance than the reported state-of-the-art geometry at the current density of higher than 15.8 mA/cm2. The results demonstrate the effectiveness of the method.}
}
@article{HEGG201856,
title = {Preservice teacher proficiency with transformations-based congruence proofs after a college proof-based geometry class},
journal = {The Journal of Mathematical Behavior},
volume = {51},
pages = {56-70},
year = {2018},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2018.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0732312317301153},
author = {Meredith Hegg and Dimitri Papadopoulos and Brian Katz and Timothy Fukawa-Connelly},
keywords = {Mathematics teacher education, Transformational geometry, Proof},
abstract = {This report explores pre-service teachers’ proficiency with concepts of transformational geometry at the end of a semester-long advanced geometry course. In the course, the instructor incorporated transformational geometry content, including congruence proofs, in an attempt to prepare the pre-service teachers to teach high school geometry in alignment with the Common Core State Standards for Mathematics. At the conclusion of the course, students expressed a preference for using traditional triangle congruence criteria (SAS, ASA, SSS, and AAS) over using transformations to complete proofs, but were nevertheless generally successful in completing proofs using transformations. Similarly, while the students often described thinking of transformations in terms of analytic forms, they were successfully able to prove triangle congruences in synthetic contexts. Finally, some evidence indicates that students may have motion or process conceptions of transformations, but not map or object conceptions, but this evidence is not conclusive.}
}
@article{DUKHANOV2016449,
title = {Big Data and Artificial Intelligence for Digital Humanities: An International Master Program via Trans-Eurasian Universities Network},
journal = {Procedia Computer Science},
volume = {101},
pages = {449-451},
year = {2016},
note = {5th International Young Scientist Conference on Computational Science, YSC 2016, 26-28 October 2016, Krakow, Poland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.11.052},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916327211},
author = {Alexey Dukhanov and Alexander Boukhanovsky and Tatyana Sidorova and Natalya Spitsyna},
keywords = {trans-Eurasian universities’ network, international Master's program, digital humanities, skills of contemporary professional},
abstract = {This paper presents an intention of two Russian universities located at opposite sides of Russia to build with partners – leading world educational centers (in the Top-100 Universities of Times Higher Education) – a trans-Eurasian international network with Master's program “Big Data and Artificial Intelligence for Digital Humanities.” This program significantly extends the area of fostering students’ talent. In addition, it allows students to develop valuable global skills of a contemporary professional: domain expertise, soft skills including creative and system thinking, self-development, working in an international and intercultural team on a research project, etc. After graduation, the alumni will have a wide choice of opportunities to continue their academic career or to get a well-paid job in developing and developed countries around the World.}
}
@incollection{JONATHANCOHEN1986597,
title = {Semantics and the Computational Metaphor},
editor = {Ruth {Barcan Marcus} and Georg J.W. Dorn and Paul Weingartner},
series = {Studies in Logic and the Foundations of Mathematics},
publisher = {Elsevier},
volume = {114},
pages = {597-621},
year = {1986},
booktitle = {Logic, Methodology and Philosophy of Science VII},
issn = {0049-237X},
doi = {https://doi.org/10.1016/S0049-237X(09)70715-3},
url = {https://www.sciencedirect.com/science/article/pii/S0049237X09707153},
author = {L. {Jonathan Cohen}},
abstract = {Publisher Summary
The computational hypothesis, irrespective of any particular experimental outcomes, has been claimed to carry some rather specific implications for the semantics of natural language. The purpose of the chapter is to criticize that claim. The chapter is occupied solely with the implications of the computational hypothesis and not at all with its merits as a strategy for scientific research. The computational hypothesis, which has come to dominate cognitive psychology, in the sense that it expects the computational analogy to be more successful than any other in generating a variety of theories that are not only testable, but also worthwhile testing, about how particular mental processes operate. Memory, visual imagery, concept formation, problem solving, speech comprehension, etc., are treated as fields of research in which experiments may be used to test theories that such-and-such a combination of iteration, recursion, chunking, horizontal searching, vertical searching, geometrical coding, linguistic coding or other mode of information-processing is at work. Commonly the researcher first constructs or sketches or surveys a suitably wide range of computer-programs (implementable on a suitably wide range of computer-architectures) that, when compared with the mental process in question, would provide analogous outputs for analogous inputs. He then devises experiments on the performance of human subjects to determine, which of these computer-simulations is closest to the kind of explanatory mechanism required by the results of the experiments.}
}
@article{IONESCU2014275,
title = {Embodied Cognition: Challenges for Psychology and Education},
journal = {Procedia - Social and Behavioral Sciences},
volume = {128},
pages = {275-280},
year = {2014},
note = {International Conference: EDUCATION AND PSYCHOLOGY CHALLENGES - TEACHERS FOR THE KNOWLEDGE SOCIETY – 2nd EDITION EPC – TKS 2013},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2014.03.156},
url = {https://www.sciencedirect.com/science/article/pii/S1877042814022472},
author = {Thea Ionescu and Dermina Vasc},
keywords = {embodiment, cognition, sensory-motor processes, action, education.},
abstract = {Embodied cognition considers that human cognition is fundamentally grounded in sensory-motor processes and in our body's morphology and internal states. In this paper, we discuss some of the features of this post-cognitivist approach and the challenges that follow for psychology and education. These challenges point to the need to reconsider cognition and the way we pursue education today. If we want to have an efficient educational system we have to look at fundamental research in cognitive science to have an accurate description of what cognition is. Only then can we design optimal educational settings for the development of thinking.}
}
@article{DAS2023100065,
title = {Informatics on a social view and need of ethical interventions for wellbeing via interference of artificial intelligence},
journal = {Telematics and Informatics Reports},
volume = {11},
pages = {100065},
year = {2023},
issn = {2772-5030},
doi = {https://doi.org/10.1016/j.teler.2023.100065},
url = {https://www.sciencedirect.com/science/article/pii/S2772503023000257},
author = {Kabita Das and Manaswini Pattanaik and Smitimayee Basantia and Radhashyam Mishra and Debashreemayee Das and Kanhucharan Sahoo and Biswaranjan Paital},
keywords = {Artificial intelligence, Ethical enquiry, Ethics in technology, Human conduct, Moral value, Social cognition, Human intelligence},
abstract = {The main focus of this paper was to discuss and appraise the attribution of intelligence and value judgement on Artificial Intelligence (AI) and its regulated use in society. Humans are tool-making creatures and AI is used for civilization via tools. During the time of pre-civilization, tools were simple in the form of crude construction, using hand skills but at present, the achievements are the substitution of machinery to relieve/replace human intellect. AI is the scientific technique of bringing learning, adaptation, and self-organization of machines. It encompasses various concepts and methods, deployed by researchers in many diverse fields of computation and cognition. This is the computational mode of a brain, based on artificial neural networks. The usefulness of AI ethically, initiates a big question i.e. if the human mind is not self-sufficient for any work without harming the moral sentiment of others then, how can people believe in a computational model of the mind, is a machine, morally responsible for any good or bad action. We highlight issues on the use of AI in the replacement of the human mind asking what is the value of humans in this age of AI? Can AI reciprocate and respect human values better than human beings? Can AI replace human intelligence? In the case of ethical enquiry, it is rather a herculean task to consider a machine's action to be moral or immoral, after all, it is just a machinery action devoid of any moral quality.}
}
@article{ORAN1992251,
title = {Reactive-flow computations on a massively parallel computer},
journal = {Fluid Dynamics Research},
volume = {10},
number = {4},
pages = {251-271},
year = {1992},
issn = {0169-5983},
doi = {https://doi.org/10.1016/0169-5983(92)90025-R},
url = {https://www.sciencedirect.com/science/article/pii/016959839290025R},
author = {Elaine S. Oran and Jay P. Boris and C.Richard DeVore},
abstract = {Results are described of recent research and model developments for performing large-scale multidimensional compressible reacting-flow computations on the Connection Machine, a very fine-grained, parallel computer capable of multigigaflop performance. We are interested both in having general-purpose computer programs for routine production computations and in evaluating the architecture of the computer for a wide range of computational fluid dynamics and reacting-flow applications. We describe the hurdles involved in rethinking the structure and the algorithms to best suit this kind of computer, provide some relative timings for different programs, and describe ways of dealing with special constraints (such as periodic boundary conditions) imposed by the architecture. Finally, representative results are presented for several reacting and nonreacting computations, including the development and propagation of a spark in a hydrogen-oxygen mixture, an imploding detonation, and the generation of beam-channel turbulence.}
}
@article{KOSCHINSKY2013172,
title = {The case for spatial analysis in evaluation to reduce health inequities},
journal = {Evaluation and Program Planning},
volume = {36},
number = {1},
pages = {172-176},
year = {2013},
note = {Special Section: Rethinking Evaluation of Health Equity Initiatives},
issn = {0149-7189},
doi = {https://doi.org/10.1016/j.evalprogplan.2012.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0149718912000237},
author = {Julia Koschinsky},
keywords = {Spatial analysis, Spatial perspective, Program evaluation, Evaluation, Health inequities, Realist evaluation, Randomized control trials (RCTs)},
abstract = {The article begins by giving an overview of spatial thinking concepts that are relevant to evaluation. The article relates the spatial perspective to both a realist evaluation and a randomized control trial perspective in evaluation to demonstrate the benefits of a spatialized program and evaluation perspective. The article mainly suggests that the adoption of a spatial perspective can add new insights to the theory and practice of evaluation in ways that helps evaluation move closer to reducing health inequities.}
}
@article{GAO2023103794,
title = {Developing virtual acoustic terrain for Urban Air Mobility trajectory planning},
journal = {Transportation Research Part D: Transport and Environment},
volume = {120},
pages = {103794},
year = {2023},
issn = {1361-9209},
doi = {https://doi.org/10.1016/j.trd.2023.103794},
url = {https://www.sciencedirect.com/science/article/pii/S1361920923001918},
author = {Zhenyu Gao and Alex Porcayo and John-Paul Clarke},
keywords = {Urban Air Mobility, Sustainable aviation, Noise modeling, Trajectory planning, Optimization},
abstract = {Urban Air Mobility (UAM) is a transformative concept that must operate harmoniously within the constraints imposed by societal impacts. Noise-aware flight trajectory planning can address UAM’s community noise concerns. However, the traditional trajectory optimization paradigm requires repetitive computations of a flight’s noise footprints in complex urban environments and is computationally expensive. In this work, we propose virtual acoustic terrain, a novel concept to enable an efficient trajectory optimization paradigm. By applying acoustic ray tracing and the principle of reciprocity in a complex urban environment, we convert different noise constraints into 3D exclusion zones which UAM operations should avoid to maintain limited noise impact. It combines with the physical urban terrain to define an acceptable fly zone for non-repetitive noise-aware trajectory optimization. This framework provides a new angle to future urban area airspace management and can also accommodate other forms of societal constraints.}
}
@article{BRAUND2013175,
title = {First steps in teaching argumentation: A South African study},
journal = {International Journal of Educational Development},
volume = {33},
number = {2},
pages = {175-184},
year = {2013},
issn = {0738-0593},
doi = {https://doi.org/10.1016/j.ijedudev.2012.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0738059312000417},
author = {Martin Braund and Zena Scholtz and Melanie Sadeck and Robert Koopman},
keywords = {Critical thinking, Argumentation, Student teachers, Science},
abstract = {South African student teachers were studied to see how they coped with requirements to teach science using argumentation. Lesson observations, plans, reflective logs, post-teaching interviews and assessment of pupils’ argumentation were used to compare student teachers’ preparedness and interactions with pupils. Two clusters of students were identified representing high preparedness and low interaction. A high degree of preparedness alone did not guarantee high levels of argumentation. Schools’ educational situations were independent of success in teaching argumentation. The outcomes and implications for further development of teaching critical thinking are discussed.}
}
@article{CASTANEDA2023102391,
title = {A simulation-based approach for assessing the innovation barriers in the manufacturing firms},
journal = {Technology in Society},
volume = {75},
pages = {102391},
year = {2023},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2023.102391},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X23001963},
author = {Monica Castaneda and Milton M. Herrera and Alberto Méndez-Morales},
keywords = {Product innovation, Process innovation, Manufacturing sector, System dynamics, Barriers to innovation, Innovation policy},
abstract = {One of the most important challenges organisations’ faces to innovate is dealing with different types of barriers. Particularly, the case of manufacturing firms confronts several barriers, such as demand uncertainty, product imitation, lack of employees, scarcity of government funding, absence of internal and external financing. This paper aims to provide new insights regarding to the innovation barriers faced by the manufacturing firms. To do this, we implemented a computational model for analysing the barriers to innovation in the Colombian case. In this model, product and processes innovation are studied. It was concluded that for the innovation of process, the highly important barrier is the shortcoming of internal financing, while for the innovation of product is the lack of employees. Results show that the government expenditure is scarce compared to private and external investment.}
}
@article{VAITESSWAR2024210,
title = {Machine learning based feature engineering for thermoelectric materials by design††Electronic supplementary information (ESI) available: Details on methodology, Box–Cox transformations, machine learning models, and inverse design. See DOI: https://doi.org/10.1039/d3dd00131h},
journal = {Digital Discovery},
volume = {3},
number = {1},
pages = {210-220},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d3dd00131h},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X24000305},
author = {U. S. Vaitesswar and Daniil Bash and Tan Huang and Jose Recatala-Gomez and Tianqi Deng and Shuo-Wang Yang and Xiaonan Wang and Kedar Hippalgaonkar},
abstract = {Availability of material datasets through high performance computing has enabled the use of machine learning to not only discover correlations and employ materials informatics to perform screening, but also to take the first steps towards materials by design. Computational materials databases are well-labelled and provide a fertile ground for predicting both ground-state and functional properties of materials. However, a clear design approach that allows prediction of materials with the desired functional performance does not yet exist. In this work, we train various machine learning models on a dataset curated from a combination of Materials Project as well as computationally calculated thermoelectric electronic power factor using a constant relaxation time Boltzmann transport equation (BoltzTrap). We show that simple random forest-based machine learning models outperform more complex neural network-based approaches on the moderately sized dataset and also allow for interpretability. In addition, when trained on only cubic material systems, the best performing machine learning model employs a perturbative scanning approach to find new candidates in Materials Project that it has never seen before, and automatically converges upon half-Heusler alloys as promising thermoelectric materials. We validate this prediction by performing density functional theory and BoltzTrap calculations to reveal accurate matching. One of those predicted to be a good material, NbFeSb, has been studied recently by the thermoelectric community; from this study, we propose four new half-Heusler compounds as promising thermoelectric materials – TiGePt, ZrInAu, ZrSiPd and ZrSiPt. Our approach is generalizable to extrapolate into previously unexplored material spaces and establishes an automated pipeline for the development of high-throughput functional materials.}
}
@article{STANCIU2015312,
title = {Embodied Creativity: A Critical Analysis of an Underdeveloped Subject},
journal = {Procedia - Social and Behavioral Sciences},
volume = {187},
pages = {312-317},
year = {2015},
note = {INTERNATIONAL CONFERENCE PSIWORLD 2014 - 5th edition},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.03.058},
url = {https://www.sciencedirect.com/science/article/pii/S1877042815018510},
author = {Marius M. Stanciu},
keywords = {Embodied, Creativity, Cognition, Research, Review},
abstract = {While the idea that cognition is embodied appeared in the literature more than four decades ago, studies concerned with how and to what degree might the body and the environment influence creative thinking represent a relatively recent scientific endeavor. In this paper we wish to provide a critical examination of the core ideas of this new field, suggesting new experimental paradigms for testing the more radical and often ignored assertions of the embodied cognition program. We conclude that given the extremely small number of papers that are produced on this subject, as well as its obscurity within the scientific community, future research will have to expand its theoretical considerations greatly if the field is to survive and flourish.}
}
@article{NAGLE2019100684,
title = {Using APOS theory as a framework for considering slope understanding},
journal = {The Journal of Mathematical Behavior},
volume = {54},
pages = {100684},
year = {2019},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2018.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0732312318301469},
author = {Courtney Nagle and Rafael Martínez-Planell and Deborah Moore-Russo},
keywords = {APOS, Slope, APOS levels between stages, Precalculus, Rate of change, Totality},
abstract = {In this paper a framework for slope is proposed using APOS (Action-Process-Object-Schema) Theory and conceptualizations of slope previously identified in research. The proposed APOS-slope framework allows for discussion of students’ cognitive development in relation to different conceptualizations of slope. As such, it may be adopted as a means to advance future research or as a way to plan instruction. In particular, the framework uses specific examples to consider interrelations between the ways of thinking about slope that have been reported to provide additional insight on how individuals understand this concept. The proposed framework contributes to the field by bringing together a number of past studies related to slope and providing a common ground under which these works might be interpreted.}
}
@article{CAO2020118,
title = {Computational parameter identification of strongest influence on the shear resistance of reinforced concrete beams by fiber reinforcement polymer},
journal = {Structures},
volume = {27},
pages = {118-127},
year = {2020},
issn = {2352-0124},
doi = {https://doi.org/10.1016/j.istruc.2020.05.031},
url = {https://www.sciencedirect.com/science/article/pii/S2352012420302435},
author = {Yan Cao and Qingming Fan and Sadaf {Mahmoudi Azar} and Rayed Alyousef and Salim T. Yousif and Karzan Wakil and Kittisak Jermsittiparsert and Lanh {Si Ho} and Hisham Alabduljabbar and Abdulaziz Alaskar},
keywords = {FRP: reinforced concrete, Shear resistance, Selection procedure, ANFIS},
abstract = {Bars made of fiber reinforcement polymer (FRP) are in common usage for concrete reinforcing instead of steel reinforcing since steel could be affected by corrosion. The concrete beams reinforced by FRP bars have been studied mostly in longitudinal direction without shear reinforcement. The primary objective of this investigation was to design and advance an algorithm for selection procedure of the parameters influence on prediction of shear resistance of reinforced concrete beams by FRP. Six input parameters were used which represent geometric and mechanical properties of the bars as well as shear features. These parameters are: web width, tensile reinforcement depth, ratio of shear and depth, concrete compressive strength, ratio of FRP reinforcement, FRP modulus of elasticity and beam shear resistance. The searching algorithm is based on combination of artificial neural network and fuzzy logic principle or adaptive neuro fuzzy inference system (ANFIS). Based on the obtained results ratio of shear and depth has the strongest influence on the prediction of shear resistance of reinforced concrete beams by FRP. Moreover, combination of tensile reinforcement depth and ratio of shear and depth is the most influential combination of two parameters on the prediction of shear resistance of reinforced concrete beams by FRP. Finally, combination of tensile reinforcement depth, ratio of shear and depth and FRP modulus of elasticity is the most influential combination of three parameters on the prediction of shear resistance of reinforced concrete beams by FRP.}
}
@article{SHU2025111052,
title = {Optimal power flow in hybrid AC-DC systems considering N-k security constraints in the preventive-corrective control stage},
journal = {Electric Power Systems Research},
volume = {238},
pages = {111052},
year = {2025},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2024.111052},
url = {https://www.sciencedirect.com/science/article/pii/S0378779624009349},
author = {Hongchun Shu and Hongfang Zhao and Mengli Liao},
keywords = {Flexible DC transmission, AC-DC hybrid system, -, Safety constraints, Optimal power flow},
abstract = {The optimal power flow methods for AC-DC systems containing VSC-HVDC generally only consider the economy during normal operation, overlooking the distribution of line transmission power in fault conditions. As a result, lines that continue to operate after a fault may experience overloading or operate at full capacity. Thus, a method for optimal power flow calculation is proposed that incorporates N-k security constraints in the preventive-corrective control stage for secure and economic operation of hybrid AC-DC systems. This method ensures that the line transmission power in the system meets the limits in the normal, short-term fault, and long-term fault states. In addition to the optimal power flow in the normal state, the method incorporates the system's imbalance as an indicator to evaluate system resilience. It combines this indicator with the economic, network loss, and performance metrics of the system, forming a two-stage bi-level multi-objective optimization model. Furthermore, to address the curse of dimensionality in anticipating system fault sets, a method for generating the anticipated fault set using non-sequential Monte Carlo simulation is proposed, along with a fault scenario search approach based on robust thinking to identify the most severe faults. Finally, the traditional IEEE 30-bus system was improved, and simulation verification was conducted using examples of an AC/DC system with a three-terminal DC network and a wind-solar-storage hybrid AC/DC system with a three-terminal DC network. The simulation results indicate that the proposed optimal power flow method considering the preventive-corrective control stage with N-k security constraints can effectively enhance system resilience. Furthermore, it improves the economic efficiency while ensuring the secure operation of the system.}
}
@article{LEE1993255,
title = {Interval computation as deduction in chip},
journal = {The Journal of Logic Programming},
volume = {16},
number = {3},
pages = {255-276},
year = {1993},
issn = {0743-1066},
doi = {https://doi.org/10.1016/0743-1066(93)90045-I},
url = {https://www.sciencedirect.com/science/article/pii/074310669390045I},
author = {J.H.M. Lee and M.H. {Van Emden}},
abstract = {Logic programming realizes the ideal of “computation is deduction,” but not when floating-point numbers are involved. In that respect logic programming languages are as careless as conventional computation: they ignore the fact that floating-point operations are only approximate and that it is not easy to tell how good the approximation is. It is our aim to extend the benefits of logic programming to computation involving floating-point arithmetic. Our starting points are the ideas of Cleary and the CHIP programming language. Cleary proposed a relational form of interval arithmetic that was incorporated in BNR Prolog in such a way that variables already bound can be bound again. In this way the usual logical interpretation of computation no longer holds. In this paper we develop a technique for narrowing intervals that we relate both to Cleary's work and to the constraint-satisfaction techniques of artificial intelligence. We then modify CHIP by allowing domains to be intervals of real numbers. To reduce arithmetic primitives with interval domains, we use our interval narrowing technique as an implementation of the looking-ahead inference rule. We show that the result is a system where answers are logical consequences of a declarative logic program, even when floating-point computations have been used. We believe ours is the first system with this property.}
}
@article{SHEARER2021,
title = {Foodborne Illness Outbreak Investigation for One Health Postsecondary Education},
journal = {Journal of Microbiology & Biology Education},
volume = {22},
number = {2},
year = {2021},
issn = {1935-7877},
doi = {https://doi.org/10.1128/jmbe.00129-21},
url = {https://www.sciencedirect.com/science/article/pii/S193578772100143X},
author = {Adrienne E. H. Shearer and Kalmia E. Kniel},
keywords = {food safety, investigation, One Health, education, microbiology, public health, escape room, problem-based learning, epidemiology, environment},
abstract = {One Health concepts were incorporated in a foodborne disease outbreak investigation with game features of data presented as visual and manipulative clues. Postsecondary pre-veterinary medicine and animal biosciences students and food science students (n = 319) enrolled in an introductory animal and food sciences course over a 3-year period received a brief introduction to foodborne illness, an outbreak scenario, and investigative tasks to complete individually or in groups.
ABSTRACT
One Health concepts were incorporated in a foodborne disease outbreak investigation with game features of data presented as visual and manipulative clues. Postsecondary pre-veterinary medicine and animal biosciences students and food science students (n = 319) enrolled in an introductory animal and food sciences course over a 3-year period received a brief introduction to foodborne illness, an outbreak scenario, and investigative tasks to complete individually or in groups. Tasks addressed epidemiology, laboratory, environment, traceback, recall, and prevention concepts. Gamification of the exercise involved generation of a numerical code to unlock a combination lock as an indication of successful organization, compilation, and interpretation of data. Students presented investigation findings and responses to critical thought questions on their roles. Student surveys on engagement and self-perceived change in conceptual understanding indicated that nearly all expressed increased understanding of outbreak investigations, safe food production, and environmental water as a transmission vehicle. Volunteered learned concepts indicated enhanced appreciation for the complexity of food safety and interdisciplinary connections. Students enjoyed the exercise (92%) and cited the clues and group interaction among the most enjoyable features. Objective assessment of student conceptual learning with the subset of students who conducted the investigation individually (n = 58) demonstrated significant increase in correct test responses (49% pretest; 76% posttest) after completion of the investigation for all questions combined and across all learning objectives. These data demonstrate the value of a foodborne disease investigation with escape room gamification features for engaging students in One Health concepts and exercising problem-solving, critical thinking, and skills for independent and collaborative work.}
}
@article{BLELLOCH199490,
title = {Parallel solutions to geometric problems in the scan model of computation},
journal = {Journal of Computer and System Sciences},
volume = {48},
number = {1},
pages = {90-115},
year = {1994},
issn = {0022-0000},
doi = {https://doi.org/10.1016/S0022-0000(05)80023-6},
url = {https://www.sciencedirect.com/science/article/pii/S0022000005800236},
author = {Guy E. Blelloch and James J. Little},
abstract = {This paper describes several parallel algorithms that solve geometric problems. The algorithms are based on a vector model of computation-the scan model. The purpose of this paper is both to show how the model can be used and to formulate a set of practical algorithms. The scan model is based on a small set of operations on vectors of atomic values. It differs from the P-RAM models both in that it includes a set of scan primitives, also called parallel prefix computations, and in that it is a strictly data-parallel model. A very useful abstraction in the scan model is the segment abstraction, the subdivision of a vector into a collection of independent smaller vectors. The segment abstraction permits a clean formulation of divide-and-conquer algorithms and is used heavily in the algorithms described in this paper. Within the scan model, using the operations and routines defined, the paper describes a k-D tree algorithm requiring O(lg n) calls to the primitives for n points, a closest-pair algorithm requiring O(lg n) calls to the primitives, a line-drawing algorithm requiring O(1) calls to the primitives, a line-of-sight algorithm requiring O(1) calls to the primitives, and finally, three different convex-hull algorithms. The last convex-hull algorithm, merge-hull, utilizes a generalized binary search technique using divide-and-conquer with the segment abstraction. The paper also describes how to implement the CREW version of Cole's merge sort in O(lg n) calls to the primitives. All these algorithms should be noted for their simplicity rather than their complexity; many of them are parallel versions of known serial algorithms. Most of the algorithms discussed in this paper have been implemented on the Connection Machine, a highly parallel single instruction multiple data computer.}
}
@article{LIU2018164,
title = {Neural and genetic determinants of creativity},
journal = {NeuroImage},
volume = {174},
pages = {164-176},
year = {2018},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2018.02.067},
url = {https://www.sciencedirect.com/science/article/pii/S1053811918301745},
author = {Zhaowen Liu and Jie Zhang and Xiaohua Xie and Edmund T. Rolls and Jiangzhou Sun and Kai Zhang and Zeyu Jiao and Qunlin Chen and Junying Zhang and Jiang Qiu and Jianfeng Feng},
abstract = {Creative thinking plays a vital role in almost all aspects of human life. However, little is known about the neural and genetic mechanisms underlying creative thinking. Based on a cross-validation based predictive framework, we searched from the whole-brain connectome (34,716 functional connectivities) and whole genome data (309,996 SNPs) in two datasets (all collected by Southwest University, Chongqing) consisting of altogether 236 subjects, for a better understanding of the brain and genetic underpinning of creativity. Using the Torrance Tests of Creative Thinking score, we found that high figural creativity is mainly related to high functional connectivity between the executive control, attention, and memory retrieval networks (strong top-down effects); and to low functional connectivity between the default mode network, the ventral attention network, and the subcortical and primary sensory networks (weak bottom-up processing) in the first dataset (consisting of 138 subjects). High creativity also correlates significantly with mutations of genes coding for both excitatory and inhibitory neurotransmitters. Combining the brain connectome and the genomic data we can predict individuals' creativity scores with an accuracy of 78.4%, which is significantly better than prediction using single modality data (gene or functional connectivity), indicating the importance of combining multi-modality data. Our neuroimaging prediction model built upon the first dataset was cross-validated by a completely new dataset of 98 subjects (r = 0.267, p = 0.0078) with an accuracy of 64.6%. In addition, the creativity–related functional connectivity network we identified in the first dataset was still significantly correlated with the creativity score in the new dataset (p<10−3). In summary, our research demonstrates that strong top-down control versus weak bottom-up processes underlie creativity, which is modulated by competition between the glutamate and GABA neurotransmitter systems. Our work provides the first insights into both the neural and the genetic bases of creativity.}
}
@article{FOWLER20245,
title = {Will variants of uncertain significance still exist in 2030?},
journal = {The American Journal of Human Genetics},
volume = {111},
number = {1},
pages = {5-10},
year = {2024},
issn = {0002-9297},
doi = {https://doi.org/10.1016/j.ajhg.2023.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0002929723004007},
author = {Douglas M. Fowler and Heidi L. Rehm},
abstract = {Summary
In 2020, the National Human Genome Research Institute (NHGRI) made ten “bold predictions,” including that “the clinical relevance of all encountered genomic variants will be readily predictable, rendering the diagnostic designation ‘variant of uncertain significance (VUS)’ obsolete.” We discuss the prospects for this prediction, arguing that many, if not most, VUS in coding regions will be resolved by 2030. We outline a confluence of recent changes making this possible, especially advances in the standards for variant classification that better leverage diverse types of evidence, improvements in computational variant effect predictor performance, scalable multiplexed assays of variant effect capable of saturating the genome, and data-sharing efforts that will maximize the information gained from each new individual sequenced and variant interpreted. We suggest that clinicians and researchers can realize a future where VUSs have largely been eliminated, in line with the NHGRI’s bold prediction. The length of time taken to reach this future, and thus whether we are able to achieve the goal of largely eliminating VUSs by 2030, is largely a consequence of the choices made now and in the next few years. We believe that investing in eliminating VUSs is worthwhile, since their predominance remains one of the biggest challenges to precision genomic medicine.}
}
@incollection{KUMAR20031,
title = {1 - An introduction to computational development},
editor = {Sanjeev Kumar and Peter J. Bentley},
booktitle = {On Growth, Form and Computers},
publisher = {Academic Press},
address = {London},
pages = {1-43},
year = {2003},
isbn = {978-0-12-428765-5},
doi = {https://doi.org/10.1016/B978-012428765-5/50034-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780124287655500347},
author = {Sanjeev Kumar and Peter J. Bentley}
}
@article{WILSON1989171,
title = {Grand challenges to computational science},
journal = {Future Generation Computer Systems},
volume = {5},
number = {2},
pages = {171-189},
year = {1989},
note = {Grand Challenges to Computational Science},
issn = {0167-739X},
doi = {https://doi.org/10.1016/0167-739X(89)90038-1},
url = {https://www.sciencedirect.com/science/article/pii/0167739X89900381},
author = {Kenneth G. Wilson},
abstract = {Computational Science is at the very beginning of centuries of growth, comparable to the four centuries of experimental advances since Galileo. The Grand Challenges to Computational Science are unsolved scientific problems of extraordinary breadth and importance which will demand continuing computational advances throughout the forthcoming computational era. Supercomputers can be used to see phenomena not directly accessible to experiment in key scientific and engineering areas such as atmospheric science, astronomy, materials science, molecular biology, aerodynamics, and elementary particle physics. However, the benefits of supercomputers will be greatly increased if some major difficulties are overcome. In this paper, I address some of the tougher requirements on current grand challenge research to ensure that it has enduring value. The problems of algorithm development, error control, software productivity, and the fostering of technological advances are especially important.}
}
@article{PIOLOPEZ2023103585,
title = {Morphoceuticals: Perspectives for discovery of drugs targeting anatomical control mechanisms in regenerative medicine, cancer and aging},
journal = {Drug Discovery Today},
volume = {28},
number = {6},
pages = {103585},
year = {2023},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2023.103585},
url = {https://www.sciencedirect.com/science/article/pii/S1359644623001010},
author = {Léo Pio-Lopez and Michael Levin},
keywords = {Biomedicine, Drug discovery, Morphogenesis},
abstract = {Morphoceuticals are a new class of interventions that target the setpoints of anatomical homeostasis for efficient, modular control of growth and form. Here, we focus on a subclass: electroceuticals, which specifically target the cellular bioelectrical interface. Cellular collectives in all tissues form bioelectrical networks via ion channels and gap junctions that process morphogenetic information, controlling gene expression and allowing cell networks to adaptively and dynamically control growth and pattern formation. Recent progress in understanding this physiological control system, including predictive computational models, suggests that targeting bioelectrical interfaces can control embryogenesis and maintain shape against injury, senescence and tumorigenesis. We propose a roadmap for drug discovery focused on manipulating endogenous bioelectric signaling for regenerative medicine, cancer suppression and antiaging therapeutics.}
}
@article{ISLAM2022100280,
title = {Industry 4.0: Skill set for employability},
journal = {Social Sciences & Humanities Open},
volume = {6},
number = {1},
pages = {100280},
year = {2022},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2022.100280},
url = {https://www.sciencedirect.com/science/article/pii/S2590291122000341},
author = {Md. Aminul Islam},
keywords = {Industry 4.0, Skills, Competencies, Graduates, A lower middle-income country, Bangladesh},
abstract = {This paper aims at finding whether students are ready to perform in the modern competitive business job arena. Most importantly, if they have the required skills and competencies to catch the opportunity offered by companies at the fourth industrial revolution where we notice the trend of automation and data exchange and IoT, cloud computing and cognitive computing have taken the lead. Our target participants in the survey include students from public and private universities in Bangladesh who will perform in the job market and who are already in the market. This is how we can bridge the gap between employers' expectations and students' perceptions of skills and competencies they acquire before entering the job market. After surveying and analyzing data collected from 361 undergraduate and graduate-level students, we found that both business and technology impact employment. Students are aware of the changing job market scenario, and they are trying to have those skills which will make them competent compared to the early years, but they are not prepared enough to accept the challenges faced in industry 4.0. This paper will be helpful for both the academicians to be aware of the future trend of the market so that they can prepare students to fight the challenges and do future research on them. At the same time, employers can get some ideas how students are thinking right now and how much training and development opportunity they should arrange for the newly recruited graduates who have lack expertise but if they are trained up, can be a source of strength for the companies.}
}
@incollection{MCKELVEY199687,
title = {Chapter 2 Computation of equilibria in finite games},
series = {Handbook of Computational Economics},
publisher = {Elsevier},
volume = {1},
pages = {87-142},
year = {1996},
issn = {1574-0021},
doi = {https://doi.org/10.1016/S1574-0021(96)01004-0},
url = {https://www.sciencedirect.com/science/article/pii/S1574002196010040},
author = {Richard D. McKelvey and Andrew McLennan},
abstract = {Publisher Summary
This chapter provides an overview of the latest state of the art of methods for numerical computation of Nash equilibria —and refinements of Nash equilibria —for general finite n-person games. The appropriate method for computing Nash equilibria for a game depends on a number of factors. The first and most important factor involves, whether it is required to simply find one equilibrium (a sample equilibrium), or find all equilibria. The problem of finding one equilibrium is a well studied problem, and there exist number of different methods for numerically computing a sample equilibrium. The problem of finding all equilibria has been addressed recently. While, there exist methods for computation of all equilibria, they are computationally intensive. With current methods, they are only feasible on small problems. The chapter overviews methods for computing sample equilibria in normal form games, and discusses the computation of equilibria on extensive form games.}
}
@article{ENGLAND2008163,
title = {Rattling the cage: computational models of chaperonin-mediated protein folding},
journal = {Current Opinion in Structural Biology},
volume = {18},
number = {2},
pages = {163-169},
year = {2008},
note = {Theory and simulation / Macromolecular assemblages},
issn = {0959-440X},
doi = {https://doi.org/10.1016/j.sbi.2007.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S0959440X08000067},
author = {Jeremy England and Del Lucent and Vijay Pande},
abstract = {Chaperonins are known to maintain the stability of the proteome by facilitating the productive folding of numerous misfolded or aggregation-prone proteins and are thus essential for cell viability. Despite their established importance, the mechanism by which chaperonins facilitate protein folding remains unknown. Computer simulation techniques are now being employed to complement experimental ones in order to shed light on this mystery. Here we review previous computational models of chaperonin-mediated protein folding in the context of the two main hypotheses for chaperonin function: iterative annealing and landscape modulation. We then discuss new results pointing to the importance of solvent (a previously neglected factor) in chaperonin activity. We conclude with our views on the future role of simulation in studying chaperonin activity as well as protein folding in other biologically relevant confined contexts.}
}
@article{GARAS2024100885,
title = {A data analytics case study analyzing IRS SOI migration data using no code, low code technologies},
journal = {Journal of Accounting Education},
volume = {66},
pages = {100885},
year = {2024},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2024.100885},
url = {https://www.sciencedirect.com/science/article/pii/S0748575124000010},
author = {Samy Garas and Susan L. Wright},
keywords = {Robotic process automation, UiPath, Alteryx, Tableau, Data automation, Data analytics, Data visualizations, Regional migration, Government planning, Business planning},
abstract = {Organizations generate and accumulate vast amounts of structured and unstructured data that have value for formulating and supporting strategic decisions. The advancement of no-code and low-code software has enabled the use of this data to provide significant data insights and business intelligence by employing multiple forms of data analytics. The imperative to cultivate a robust and proficient group of individuals with expertise in data analytics has led to a substantial increase in the number of educational programs focused on data science and analytics. Accounting educators can capitalize on these trends by integrating data analytics and software skills into the accounting curriculum. This case offers essential materials to aid in the development of the curriculum to support accounting and analytics educators. This case serves many objectives by providing a professional setting in which you take on the role of junior data analyst, offering necessary context and motivation for completing the tasks. The case allows you to analyze extensive data sets obtained from the IRS Statistics of Income (SOI) website in order to investigate migration patterns based on state, year, age, and income categories. UiPath-robotic process automation (RPA), Alteryx-based data analysis, and Tableau-based data visualization tools are employed to extract, generate, and present descriptive statistics and to conduct a simple times series analysis. These insights are highly valuable to decision makers in business and government organizations. You are encouraged to engage in critical thinking and to consider the potential impacts of migratory patterns on choices made by firm executives and public policy makers. Migration patterns have a significant impact on firm management decisions, influencing either to expand or reduce current operations and indicating the availability and expansion of new talent pools. Migration patterns have a significant impact on the decision made by public policy makers, particularly in relation to public utilities, infrastructure, and other services and benefits. You analyze temporal data to deduce the influence of changes in the tax code and shifts in the economy. You gain expertise in managing large data sets, exploring features of analytics software, and creating compelling visualizations to effectively communicate important discoveries. Instructors and students are given comprehensive instructions and videos to facilitate the efficient application of these technologies.}
}
@article{LI2024112467,
title = {Study on correlation between perioperative cognitive function and nutritional status in elderly patients with gastric cancer},
journal = {Experimental Gerontology},
volume = {193},
pages = {112467},
year = {2024},
issn = {0531-5565},
doi = {https://doi.org/10.1016/j.exger.2024.112467},
url = {https://www.sciencedirect.com/science/article/pii/S0531556524001098},
author = {Rong Li and Yuping Liu and Yingtao Meng and Xianlin Qu and Meimei Shang and Lihui Yang and Jie Chai},
keywords = {Elderly, Gastric cancer, Perioperative period, Cognitive function, Nutritional status, Correlation, Analysis},
abstract = {Objective: To investigate the cognitive function and nutritional status of elderly patients with gastric cancer during perioperative period, and to analyze their correlation. Methods: Aged patients undergoing gastric cancer surgery in The Affiliated Cancer Hospital of Shandong First Medical University from March to October 2021 were selected as the subjects of this study. The monitoring data of cognitive function and nutritional status were retrospectively analyzed from 1 to 3 days before surgery, 1 and 3 days after surgery, 7 days after surgery (before discharge) and 30 days after surgery to analyze the correlation between cognitive function and nutritional status in elderly patients with gastric cancer. Results: the incidence of mild cognitive impairment in elderly patients with gastric cancer was 52.43 %, the visual space of the two groups' (mild cognitive impairment) ability of execution, name, attention, language, abstract thinking, delayed memory and cognitive function scores were lower than 1 set of directional force (cognitive function in normal group), statistically significant difference (P < 0.05). The nutritional status of elderly patients with gastric cancer was lower than that of healthy elderly group at the same period (P < 0.05). The scores of visual spatial executive function, name, attention, delayed memory, orientation and total score of cognitive function in elderly gastric cancer patients were positively correlated with nutritional status (P < 0.05). Conclusions: The cognitive function and nutritional status of elderly patients with gastric cancer are both in a low state during treatment and a higher level of cognitive function can help patients maintain a more correct nutritional cognition, and the nutritional status of patients will be relatively better. There is a positive correlation between cognitive function and nutritional status in elderly patients with gastric cancer, which should be paid attention to in the treatment.}
}
@article{DEVINK20222744,
title = {Cooperativity as quantification and optimization paradigm for nuclear receptor modulators††Electronic supplementary information (ESI) available: Experimental details, supporting figures and tables. See DOI: 10.1039/d1sc06426f},
journal = {Chemical Science},
volume = {13},
number = {9},
pages = {2744-2752},
year = {2022},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d1sc06426f},
url = {https://www.sciencedirect.com/science/article/pii/S2041652023017297},
author = {Pim J. {de Vink} and Auke A. Koops and Giulia D'Arrigo and Gabriele Cruciani and Francesca Spyrakis and Luc Brunsveld},
abstract = {ABSTRACT
Nuclear Receptors (NRs) are highly relevant drug targets, for which small molecule modulation goes beyond a simple ligand/receptor interaction. NR–ligands modulate Protein–Protein Interactions (PPIs) with coregulator proteins. Here we bring forward a cooperativity mechanism for small molecule modulation of NR PPIs, using the Peroxisome Proliferator Activated Receptor γ (PPARγ), which describes NR–ligands as allosteric molecular glues. The cooperativity framework uses a thermodynamic model based on three-body binding events, to dissect and quantify reciprocal effects of NR–coregulator binding (KID) and NR–ligand binding (KIID), jointly recapitulated in the cooperativity factor (α) for each specific ternary ligand·NR·coregulator complex formation. These fundamental thermodynamic parameters allow for a conceptually new way of thinking about structure–activity-relationships for NR–ligands and can steer NR modulator discovery and optimization via a completely novel approach.}
}
@incollection{KRAAK2009468,
title = {Geovisualization},
editor = {Rob Kitchin and Nigel Thrift},
booktitle = {International Encyclopedia of Human Geography},
publisher = {Elsevier},
address = {Oxford},
pages = {468-480},
year = {2009},
isbn = {978-0-08-044910-4},
doi = {https://doi.org/10.1016/B978-008044910-4.00033-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008044910400033X},
author = {M.-J. Kraak},
keywords = {Alternative visualization, Cartography, Cognition, Coordinated-multiple-views, Geocomputation, Geoservices, Geovisualization, Information visualization, Interfaces, Maps, Representation, Spatiotemporal data, Usability, Visual exploration, Visual representation, Visual thinking},
abstract = {Recent developments in information and communication technology (ICT) have introduced many new opportunities, and have influenced many scientific disciplines in application of their methods and techniques. From a mapping perspective, this includes cartography and related disciplines like scientific visualization, image analysis and remote sensing, information visualization, exploratory data analysis, visual analytics, and GI Science. Interactivity and dynamics are prominent keywords and allow one not only to apply maps and diagrams to present-known facts but also to analyze and explore unknown data. The environment in which the maps and diagrams are used has also changed and often includes coordinated multiple views display via the Internet. This allows for simultaneous alternative views of the data and stimulates visual thinking, resulting in geovisualization.}
}
@article{NASSIRI201329,
title = {Computational modelling of long bone fractures fixed with locking plates – How can the risk of implant failure be reduced?},
journal = {Journal of Orthopaedics},
volume = {10},
number = {1},
pages = {29-37},
year = {2013},
issn = {0972-978X},
doi = {https://doi.org/10.1016/j.jor.2013.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0972978X13000020},
author = {M. Nassiri and B. MacDonald and J.M. O'Byrne},
keywords = {Modeling, Fracture, Locking, Plate, Failure},
abstract = {Background and purpose
The Locking Compression Plate (LCP) is part of a new plate generation requiring an adapted surgical technique and new thinking about commonly used concepts of internal fixation using plates. Knowledge of the fixation stability provided by these new plates is very limited and clarification is still necessary to determine how the mechanical stability and the risk of implant failure can best be controlled.
Methods
Upon validation, a finite element model of an LCP attached to a cylinder was developed to simulate and analyse the biomechanics of a transverse long bone fracture fixed with a locking plate. Of special interest were the factors influencing the mechanical conditions at the fracture site, the control of interfragmentary movement and implant failure.
Results
Several factors were shown to influence stability in compression. Increasing translation and/or fracture angle post fixation reduced construct stability. Axial stiffness was also influenced by the working length and plate-bone distance. The fracture gap had no effect on the construct stability when no bone contact occurred during loading. Stress analysis of the LCP demonstrated that the maximum Von Mises stresses were found in the innermost screws at the screw-head junction.
Interpretation
For the clinical use of the LCP as a locked internal fixator in fractures with an interfragmentary gap of 1 mm, at least two to four plate holes near the fracture gap should be omitted to allow fracture motion and bone contact to occur. This will also achieve a larger area of stress distribution on the plate and reduce the likelihood of fatigue failure due to cyclic loading.}
}
@article{GORMONG20231988,
title = {Neighboring Group Effects on the Rates of Cleavage of Si–O–Si-Containing Compounds},
journal = {The Journal of Organic Chemistry},
volume = {88},
number = {4},
pages = {1988-1995},
year = {2023},
issn = {0022-3263},
doi = {https://doi.org/10.1021/acs.joc.2c02126},
url = {https://www.sciencedirect.com/science/article/pii/S002232632300107X},
author = {Ethan A. Gormong and Dorian S. Sneddon and Theresa M. Reineke and Thomas R. Hoye},
abstract = {ABSTRACT
The presence of a nearby tethered functional group (G, G = tertiary amide or amine) can significantly impact the rate of cleavage of an Si–O bond. We report here an in situ1H NMR spectroscopic investigation of the relative rates of cleavage of model substrates containing two different Si–O substructures, namely alkoxydisiloxanes [GRO–Si­(Me2)–O–SiMe3] and carbodisiloxanes [GR–Si­(Me2)–O–SiMe3]. The trends in the relative rates (which slowed with increasing chain length, with a notable exception) of alkoxydisiloxane hydrolyses were probed via computation. The results correlated well with the experimental data. In contrast to the hydrolysis of the alkoxydisiloxanes, the carbodisiloxanes were not fully hydrolyzed, but rather formed an equilibrium mixture of starting asymmetric disiloxane, two silanols, and a new symmetrical disiloxane. We also uncovered a facile siloxy-metathesis reaction of an incoming silanol with the carbodisiloxane substrate [e.g., Me2NR–Si­(Me2)–O–SiMe3 + HOSiEt3 ⇋ Me2NR–Si­(Me2)–O–SiEt3 + HOSiMe3] facilitated by the pendant dimethylamino group, a process that was also probed by computation.}
}
@incollection{SUGHRUE2024205,
title = {Chapter 12 - Connectomic approaches to neurosurgical planning},
editor = {Michael E. Sughrue and Jacky T. Yeung and Nicholas B. Dadario},
booktitle = {Connectomic Medicine},
publisher = {Academic Press},
pages = {205-214},
year = {2024},
isbn = {978-0-443-19089-6},
doi = {https://doi.org/10.1016/B978-0-443-19089-6.00011-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443190896000112},
author = {Michael E. Sughrue and Jacky T. Yeung and Nicholas B. Dadario},
keywords = {Brain tumor surgery, Cerebral cortex, Cognitive deficits, fMRI, Graph theory, Neuro-Oncology, Onco-functional balance},
abstract = {In this chapter, we introduce how connectomics can provide an improved understanding of the structural and functional organization of the human brain which can be applied for intracerebral brain surgery. In particular, such connectomic thinking expands our ability to improve patient functional outcomes after surgery beyond mere motor and language functions by also considering the anatomy responsible for complex cognitive functions. We introduce the concept of “disconnection surgery,” where the surgical decisions when removing a tumor can be thought of a series of specific cuts that we plan to perform on the periphery of the tumor such that we can disconnect the tumor from the surrounding networks. Connectomics allows us to define the risks associated with specific tumors and surgical decisions, which can subsequently guide the operation but also tailor preoperative patient discussion. Novel mathematical concepts from the field of network neuroscience on graph theory are also introduced so as to better define truly eloquent brain regions on an individualized basis.}
}
@article{HERTENSTEIN20191213,
title = {Modulation of creativity by transcranial direct current stimulation},
journal = {Brain Stimulation},
volume = {12},
number = {5},
pages = {1213-1221},
year = {2019},
issn = {1935-861X},
doi = {https://doi.org/10.1016/j.brs.2019.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1935861X19302293},
author = {Elisabeth Hertenstein and Elena Waibel and Lukas Frase and Dieter Riemann and Bernd Feige and Michael A. Nitsche and Christoph P. Kaller and Christoph Nissen},
keywords = {Creativity, Flexibility, Transcranial direct current stimulation, Frontal cortex, Electroencephalography},
abstract = {Background
Creativity is the use of original ideas to accomplish something innovative. Previous research supports the notion that creativity is facilitated by an activation of the right and/or a deactivation of the left prefrontal cortex. In contrast, recent brain imaging studies suggest that creativity improves with left frontal activation.
Objective
The present study was designed to further elucidate the neural basis of and ways to modulate creativity, based on the modulation of prefrontal cortical activity through the non-invasive brain stimulation technique transcranial direct current stimulation (tDCS).
Methods
Ninety healthy University students performed three tasks on major aspects of creativity: conceptual expansion (Alternate Uses Task, AUT), associative thinking (Compound Remote Associate Task, CRA), and set shifting ability (Wisconsin Card Sorting Task, WCST). Simultaneously, they received cathodal stimulation of the left and anodal stimulation of the right inferior frontal gyrus (IFG), the reverse protocol, or sham stimulation.
Results
The main pattern of results was a superior performance with bilateral left cathodal/right anodal stimulation, and an inferior performance in the reversed protocol compared to sham stimulation. As a potential underlying physiological mechanism, resting state EEG beta power, indicative of enhanced cortical activity, in the right frontal area increased with anodal stimulation and was associated with better performance.
Conclusion
The findings provide new insights into ways of modulating creativity, whereby a deactivation of the left and an activation of the right prefrontal cortex with tDCS is associated with increased creativity. Potential future applications might include tDCS for patients with mental disorders and for healthy individuals in creative professions.}
}
@incollection{MAMATHA2024259,
title = {Chapter Eleven - Bio-intelligent computing and optimization techniques for developing computerized solutions},
editor = {Anupam Biswas and Alberto Paolo Tonda and Ripon Patgiri and Krishn Kumar Mishra},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {135},
pages = {259-288},
year = {2024},
booktitle = {Applications of Nature-Inspired Computing and Optimization Techniques},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2023.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S006524582300089X},
author = {G.S. Mamatha and Haripriya V. Joshi and R. Amith},
keywords = {Bio-intelligent, Bio-inspired, Computing, Optimization technique, Bio-engineering},
abstract = {Bio-inspired computing is a field of study that Lois lee knits together subfields related to the connectionism, social behavior and emergence. It is often closely related to the field of artificial intelligence as many of its pursuits can be linked to machine learning. It relies heavily on fields of biology, computer science and mathematics. Briefly it is the use of computers to model the living phenomena and simultaneously the study of life to improve the usage of computer. Biologically inspired computation is a major subset of natural computation areas of research. Some areas of study encompassed under the canon of biologically inspired computing and their biological counterparts are, genetic algorithms, evolution, biodegradability prediction, biodegradation, cellular automata, life emergent system ants, termites, bees, wasps, neural networks, artificial immune systems rendering patterning and animal skins, bird feathers, mollusk shells and bacterial colonies. Linder Mayer systems, plant structures, communication networks and protocol, epidemiology and the spared of disease, intra membrane molecular processes in living cells, excitable media forest fires the wave heart conditions axons and sensor networks sensory organs. Optimization techniques takes more bottom-up decentralized approach and often involves the methods of specifying a set of simple rules, a set of simple organisms which adhere to those rules and method of iteratively applying those rules for example, training virtual insect to investigate to an unknown terrain for finding food includes six simple rules which can be adopted. After several generations of rules application, it is usually the case where some forms of complex behavior get built upon complexity until the end results is something markedly complex and quite often completely counterintuitive from what the original rules would be expected to produce. For this reason, most technology-oriented solutions like neural network models, algorithms and other techniques came in to existence for accurate measurements and analysis that can be used to refine statistical inference and extrapolation as system complexity increases. The rules of nature inspired computing are the principle simple rules yet after being used for over millions of years have produced remarkably complex optimization techniques. All these techniques for developing software applications along with optimization techniques are discussed in the chapter.}
}
@incollection{MILLER2023203,
title = {Chapter 7 - The calculated uncertainty of scientific discovery: From Maths to Deep Maths},
editor = {Steven G. Krantz and Arni S.R. {Srinivasa Rao} and C.R. Rao},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {49},
pages = {203-226},
year = {2023},
booktitle = {Artificial Intelligence},
issn = {0169-7161},
doi = {https://doi.org/10.1016/bs.host.2023.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S016971612300024X},
author = {D. Douglas Miller},
keywords = {Mathematics, Philosophy, Statistics, Null hypothesis, Artificial intelligence, Data dimensionality, Machine learning, Algorithms, Deep learning, Stochastic gradient descent, Model optimization, Bias, Neural networks, Backpropagation, Large language models, Model generalizability},
abstract = {Throughout history, diverse Maths have underpinned numerous important natural and physical science discoveries. In their initial development and application, these Maths were often incompletely or imperfectly understood, with constants and “fudge factors” needed to account for statistical uncertainties to advance a scientific discipline. Some polymaths have acted as philosophers in support of new ways of thinking, based on their novel discoveries about the natural and physical world. Deep Maths integral to artificial intelligence (AI), machine learning and deep learning (DL), are also subject to human imperfections (i.e., computational errors, operator assumptions) and stochastic uncertainties (i.e., modeling biases, convergence optimizers). Mathematicians and domain experts can collaborate to increase AI model accuracy by improving training data quality (i.e., curating, reducing dimensionality), mitigating human and machine biases, and understanding data contexts prior to query. Since the advent of DL and through the design of multilayered feedforward neural networks then large language models, scientists have applied advanced AI computing capabilities to push the limits of this technology trend. Recently, AI's capacity to uncover newly modeled insights has been hyped beyond the proven limits of DL model accuracy. History has witnessed the acceptance of new knowledge (primarily by peers) based on the accuracy and/or reproducibility of empirical observations and on varied interpretations of mathematical proofs. Societal enthusiasm for science or technology insertion is often limited by the general public's understanding of the underlying Maths and Deep Maths, and related human fears and concerns of displacement (i.e., lost jobs, ecological impact, less privacy, etc.). Today's proponents of societal progress based on new discoveries and technologies are motivated by a range of influences (i.e., humanity, control, security, profit, etc.), creating additional uncertainties that can deflect initial scientific enthusiasm and/or delay widespread adoption.}
}
@article{RAMAMURTHY20124247,
title = {Design for sustainability: The role of CAD},
journal = {Renewable and Sustainable Energy Reviews},
volume = {16},
number = {6},
pages = {4247-4256},
year = {2012},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2012.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S1364032112001918},
author = {Sudhir {Rama Murthy} and Monto Mani},
keywords = {Design technology, Sustainability, Design process, Design tools, Creativity, Computer Aided Design},
abstract = {The term design in this paper particularly refers to the process (verb) and less to the outcome or product. Design comprises a complex set of activities today involving both man and machine. Sustainability is a fundamental paradigm and carries significance in any process, natural or manmade, and its outcome. In simple terms, sustainability implies a state of sustainable living, viz. health and continuity, nurtured by diversity and evolution (innovations) in an ever-changing world. Design, in a similar line, has been comprehensively investigated and its current manifestations including design-aids (Computer Aided Design) have been evaluated in terms of sustainability. The paper investigates the rationale of sustainability to design as a whole – its purpose, its adoption in the natural world, its relevance to humankind and the technologies involved. Throughout its history, technology has been used to aid design. But in the current context of advanced algorithms and computational capacity, design no longer remains an exclusively animate faculty. Given this scenario, investigating sustainability in the light of advanced design aids such as CAD becomes pertinent. Considering that technology plays a part in design activities, the paper explores where technology must play a part and to what degree amongst the various activities that comprise design. The study includes an examination of the morphology of design and the development of a systems-thinking integrated forecasting model to evaluate the implications of CAD tools in design and sustainability. The results of the study along with a broad range of recommendations have been presented.}
}
@article{READ200577,
title = {Early computational processing in binocular vision and depth perception},
journal = {Progress in Biophysics and Molecular Biology},
volume = {87},
number = {1},
pages = {77-108},
year = {2005},
note = {Biophysics of Excitable Tissues},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2004.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S007961070400063X},
author = {Jenny Read},
abstract = {Stereoscopic depth perception is a fascinating ability in its own right and also a useful model of perception. In recent years, considerable progress has been made in understanding the early cortical circuitry underlying this ability. Inputs from left and right eyes are first combined in primary visual cortex (V1), where many cells are tuned for binocular disparity. Although the observation of disparity tuning in V1, combined with psychophysical evidence that stereopsis must occur early in visual processing, led to initial suggestions that V1 was the neural correlate of stereoscopic depth perception, more recent work indicates that this must occur in higher visual areas. The firing of cells in V1 appears to depend relatively simply on the visual stimuli within local receptive fields in each retina, whereas the perception of depth reflects global properties of the stimulus. However, V1 neurons appear to be specialized in a number of respects to encode ecologically relevant binocular disparities. This suggests that they carry out essential pre-processing underlying stereoscopic depth perception in higher areas. This article reviews recent progress in developing accurate models of the computations carried out by these neurons. We seem close to achieving a mathematical description of the initial stages of the brain's stereo algorithm. This is important in itself––for instance, it may enable improved stereopsis in computer vision––and paves the way for a full understanding of how depth perception arises.}
}
@article{LIANG2019341,
title = {Is ecoregional scale precise enough for lake nutrient criteria? Insights from a novel relationship-based clustering approach},
journal = {Ecological Indicators},
volume = {97},
pages = {341-349},
year = {2019},
issn = {1470-160X},
doi = {https://doi.org/10.1016/j.ecolind.2018.10.034},
url = {https://www.sciencedirect.com/science/article/pii/S1470160X18308094},
author = {Zhongyao Liang and Yong Liu and Huili Chen and Yao Ji},
keywords = {Spatial scale, Nutrient criteria, Relationship-based clustering approach, Relationship mapping, Hierarchical clustering, Leave-one-out cross-validation},
abstract = {While the ecoregional lake nutrient criteria have been widely used in the past two decades, the overconfidence on their applicability may mislead the pollution management decisions, considering the spatial heterogeneity within the ecoregion. The exploration of applicability is thereby important, but is hindered by the difficulty in recognizing reliable relationship patterns between the nutrient and management endpoint. We propose a novel relationship-based clustering approach (RCA) to explore whether the ecoregional scale is precise enough for nutrient criteria. The approach (a) simulates relationships using Bayesian Linear Models, (b) clusters lakes according to relationship similarities via relationship mapping and hierarchical clustering, and (c) identifies reliable relationship patterns based on the leave-one-out cross-validation. The RCA is then employed to explore Chlorophyll a-total phosphorus relationships of 34 lakes in four Ecological Drainage Units (EDUs) in the U.S. Long-term water quality data is from a newly established database (LAGOS-NE). The results show that multiple relationship patterns exist in all the EDUs. The ecoregional relationships misestimate the nutrient effect in over a half of lakes. Therefore, we determine that the ecoregional scale is not precise enough for nutrient criteria and the sub-ecoregional scale is then recommended. Besides, the RCA provides a backward thinking for determining the spatial scale and can be used in some other fields where relationship-based clustering is needed.}
}